<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="7c13e77bc830b382c041eec0e3fcc1723f375e05" translate="yes" xml:space="preserve">
          <source>Homogeneity metric of a cluster labeling given a ground truth.</source>
          <target state="translated">给定地真值的聚类标签的同质性度量。</target>
        </trans-unit>
        <trans-unit id="3afc9b67230d985e8782525c7b58b615e013e8f9" translate="yes" xml:space="preserve">
          <source>Homogeneity, completeness and V-measure can be computed at once using &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt;&lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt;&lt;/a&gt; as follows:</source>
          <target state="translated">可以使用&lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt; &lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt; &lt;/a&gt;如下计算同质性，完整性和V度量：</target>
        </trans-unit>
        <trans-unit id="0878824f511837fc1a1c8d27240af19053ebdbd4" translate="yes" xml:space="preserve">
          <source>HouseAge median house age in block</source>
          <target state="translated">房龄 板块房龄中位数</target>
        </trans-unit>
        <trans-unit id="95f00bca96463f976c07bb46f71ce6b37ead0db0" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evaluate perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be45c283b4c54643c38f84bc65a4bfc525d6d30a" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="translated">多久评估一次困惑。仅用于 &lt;code&gt;fit&lt;/code&gt; 方法。将其设置为0或负数完全不会评估训练中的困惑。评估困惑度可以帮助您检查训练过程中的收敛性，但同时也会增加总的训练时间。在每次迭代中评估困惑度可能会使训练时间增加两倍。</target>
        </trans-unit>
        <trans-unit id="82dc6d28bb69b15075cb833b4ea7ee856b1fb8a5" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="060faa287065b4ad6ba6c00f598635b42adc21de" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">如何在分母中计算归一化。可能的选项是&amp;ldquo;最小&amp;rdquo;，&amp;ldquo;几何&amp;rdquo;，&amp;ldquo;算术&amp;rdquo;和&amp;ldquo;最大&amp;rdquo;。如果&amp;ldquo;警告&amp;rdquo;，将使用&amp;ldquo;几何&amp;rdquo;。默认值将在0.22版中更改为&amp;ldquo;算术&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b8efa217d0db9ce56ac60653645beebe151304f9" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;max&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">如何在分母中计算归一化。可能的选项是&amp;ldquo;最小&amp;rdquo;，&amp;ldquo;几何&amp;rdquo;，&amp;ldquo;算术&amp;rdquo;和&amp;ldquo;最大&amp;rdquo;。如果&amp;ldquo;警告&amp;rdquo;，将使用&amp;ldquo;最大&amp;rdquo;。默认值将在0.22版中更改为&amp;ldquo;算术&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="cf894bb3f8fceedab10cdd5da9a5edd37e00865d" translate="yes" xml:space="preserve">
          <source>How to construct the affinity matrix.</source>
          <target state="translated">如何构建亲和力矩阵。</target>
        </trans-unit>
        <trans-unit id="d34268ba2716d71aaaeee2c35e527ca55a46dd2f" translate="yes" xml:space="preserve">
          <source>However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).</source>
          <target state="translated">然而ARI也可以在纯粹的无监督环境中作为共识指数的构建模块,用于聚类模型的选择(TODO)。</target>
        </trans-unit>
        <trans-unit id="0121c2b22395d1a9db3fd77f24d0a9f0e41170e3" translate="yes" xml:space="preserve">
          <source>However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.</source>
          <target state="translated">然而,基于MI的测量方法在纯粹的无监督环境下也可以作为共识指数的构建模块,用于聚类模型的选择。</target>
        </trans-unit>
        <trans-unit id="117b6230e0e8ab3fcdc1b277507becef8a05f759" translate="yes" xml:space="preserve">
          <source>However care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.</source>
          <target state="translated">然而必须注意始终使亲和矩阵对称,这样特征向量分解才能达到预期效果。</target>
        </trans-unit>
        <trans-unit id="92447df8a934c98a63d3aa91a9c263efa88d6300" translate="yes" xml:space="preserve">
          <source>However let&amp;rsquo;s keep our high capacity random forest model for now so as to illustrate some pitfalls with feature importance on variables with many unique values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="925f5b77eb2888a89c04118c35bff0f0ace7255e" translate="yes" xml:space="preserve">
          <source>However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).</source>
          <target state="translated">然而RI得分并不能保证随机标签分配会得到一个接近于零的值(特别是当聚类数量与样本数量在同一数量级时)。</target>
        </trans-unit>
        <trans-unit id="11d179b15971b8eaae6270be9fce57c0bd0d2416" translate="yes" xml:space="preserve">
          <source>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</source>
          <target state="translated">然而,通过将可用数据划分为三组,我们大大减少了可用于学习模型的样本数量,结果可能取决于对(训练、验证)集的特定随机选择。</target>
        </trans-unit>
        <trans-unit id="56c680421b5fb07e56baa9a65f13a80fce385b54" translate="yes" xml:space="preserve">
          <source>However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">但是，普通最小二乘的系数估计取决于模型项的独立性。当项相关并且设计矩阵\（X \）的列具有近似线性相关性时，设计矩阵变得接近奇异，结果，最小二乘估计对观察到的响应中的随机误差变得高度敏感，产生很大的差异。例如，在没有实验设计的情况下收集数据时，可能会出现&lt;em&gt;多重共线性的&lt;/em&gt;情况。</target>
        </trans-unit>
        <trans-unit id="aca3ba038ade9dc36b01dc839f8a0cfb1c392a3d" translate="yes" xml:space="preserve">
          <source>However, dropping one category breaks the symmetry of the original representation and can therefore induce a bias in downstream models, for instance for penalized linear classification or regression models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf734282463bfc3a9cb343729f546342ec401691" translate="yes" xml:space="preserve">
          <source>However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.</source>
          <target state="translated">然而,如果相关训练规模的学习曲线很陡峭,那么5倍或10倍的交叉验证会高估泛化误差。</target>
        </trans-unit>
        <trans-unit id="fc162d85afa0b20b4064f40b16eb0e55ca89c629" translate="yes" xml:space="preserve">
          <source>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</source>
          <target state="translated">然而,有时绘制单个超参数对训练得分和验证得分的影响,以了解估计器对某些超参数值的拟合过度或拟合不足,是很有帮助的。</target>
        </trans-unit>
        <trans-unit id="5c8b24673bb3f660f66f90035a856044be6d6f9e" translate="yes" xml:space="preserve">
          <source>However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; to complete binary one-hot encoding.</source>
          <target state="translated">但是，请注意，当特征值的类型为string时，此转换器只会执行二进制一键编码。如果将分类特征表示为数字值（例如int），则DictVectorizer后面可以跟着&lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; ,&lt;/a&gt;以完成二进制一键编码。</target>
        </trans-unit>
        <trans-unit id="8b25d9ad009118aef0894664f601ac10786f8b49" translate="yes" xml:space="preserve">
          <source>However, this is not the most precise way of doing this computation, and the distance matrix returned by this function may not be exactly symmetric as required by, e.g., &lt;code&gt;scipy.spatial.distance&lt;/code&gt; functions.</source>
          <target state="translated">但是，这不是进行此计算的最精确方法，并且此函数返回的距离矩阵可能不像 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 函数所要求的那样完全对称。</target>
        </trans-unit>
        <trans-unit id="379cfb166aa26713fe1131478e9b37a4224780ad" translate="yes" xml:space="preserve">
          <source>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</source>
          <target state="translated">余祥福,黄芳兰,林志仁(2011).雙座標下降</target>
        </trans-unit>
        <trans-unit id="15d1d4b26d2ba06629e368bf6c8a860d8762ef89" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="translated">Huber（ &lt;code&gt;'huber'&lt;/code&gt; ）：另一个结合最小二乘和最小绝对偏差的鲁棒损失函数；使用 &lt;code&gt;alpha&lt;/code&gt; 控制离群值的敏感度（有关更多详细信息，请参见&lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="31a935f21354ade96cddb9bceb15816934445a99" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;model_evaluation#f2001&quot; id=&quot;id18&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ac7569be3812003aa580f2d415abbe085c935f8" translate="yes" xml:space="preserve">
          <source>Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when \(|y_i - f(x_i)| \leq \varepsilon\), and \(L(y_i, f(x_i)) = \varepsilon |y_i - f(x_i)| - \frac{1}{2} \varepsilon^2\) otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d12d436101cbc3212af50bf81000f6d78d4cf01" translate="yes" xml:space="preserve">
          <source>HuberRegressor vs Ridge on dataset with strong outliers</source>
          <target state="translated">HuberRegressor与Ridge在具有强离群值的数据集上的对比。</target>
        </trans-unit>
        <trans-unit id="7e58a6e8d89e8504ad31e135de9b485ad40f05f6" translate="yes" xml:space="preserve">
          <source>Hue</source>
          <target state="translated">Hue</target>
        </trans-unit>
        <trans-unit id="c7a8b2b20a9c45f674f17cd8ef7ece305e1c36eb" translate="yes" xml:space="preserve">
          <source>Hue:</source>
          <target state="translated">Hue:</target>
        </trans-unit>
        <trans-unit id="4e99bcdee413a9c98d317e0e8e4199a2bf582f90" translate="yes" xml:space="preserve">
          <source>Hugo Chavez</source>
          <target state="translated">乌戈-查韦斯</target>
        </trans-unit>
        <trans-unit id="27d175ec2bd32b6e89237e92741eaada2632ca6b" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="135e7e12c7ab5ea7496649e72ee134478ecf558e" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在alpha参数上的反比例参数(速率参数)。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="b2f7b3253a19f527d0298206233b561d7b41b5a7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="761054fe5bafcad49a16f057764235860452b8da" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的反比例参数(速率参数)。默认值为1.e-6</target>
        </trans-unit>
        <trans-unit id="6001ea6392d3003569381e7107254e88f75fd600" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的反比例参数(速率参数)。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="dc7fba2810913663c629f4f037b88df90cdd9978" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81e171654bf22a490946ec147c219e96694497ff" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter:Gamma分布在alpha参数上的形状参数。默认为1.e-6</target>
        </trans-unit>
        <trans-unit id="b07af48fd68aeaacb4df041ef30bae006150c237" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:Gamma分布在alpha参数上的形状参数,默认为1.e-6。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="532d93a63b09b5d558170a615d6e76799550e7c3" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1398aea0b1e181e76b6d9d73db4040ccf06ee2f7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的形状参数。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="7a5b8a439bb2492412d2944256add4dcdf337928" translate="yes" xml:space="preserve">
          <source>Hyper-parameter optimizers</source>
          <target state="translated">超参数优化器</target>
        </trans-unit>
        <trans-unit id="223bf115da53d3d9cdf837b624135b565596fd92" translate="yes" xml:space="preserve">
          <source>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; for Support Vector Classifier, &lt;code&gt;alpha&lt;/code&gt; for Lasso, etc.</source>
          <target state="translated">超参数是无法在估算器中直接学习的参数。在scikit-learn中，它们作为参数传递给estimator类的构造函数。典型示例包括 &lt;code&gt;C&lt;/code&gt; ，支持向量分类器的 &lt;code&gt;kernel&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; ，Lasso的 &lt;code&gt;alpha&lt;/code&gt; 等等。</target>
        </trans-unit>
        <trans-unit id="568b05951392672a52de0358537dd29fcafbe544" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="translated">通过&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params（）&lt;/a&gt;方法构造估算器的超参数后，可以对其进行更新。多次调用 &lt;code&gt;fit()&lt;/code&gt; 会覆盖以前的 &lt;code&gt;fit()&lt;/code&gt; 所学的内容：</target>
        </trans-unit>
        <trans-unit id="3320fca926b13c61acfba24014e8ac870169bd3f" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db8c072507305b4aa23189287be39423349b8f4" translate="yes" xml:space="preserve">
          <source>Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).</source>
          <target state="translated">学习反变换的岭回归的超参数(当fit_inverse_transform=True)。</target>
        </trans-unit>
        <trans-unit id="a15138d06876fc00149292405bf57e4204d00bbe" translate="yes" xml:space="preserve">
          <source>Hyperparameters</source>
          <target state="translated">Hyperparameters</target>
        </trans-unit>
        <trans-unit id="181eca8daf7aaeed93f61701c7eddb643dc6b36a" translate="yes" xml:space="preserve">
          <source>Hyperparameters:</source>
          <target state="translated">Hyperparameters:</target>
        </trans-unit>
        <trans-unit id="8bb86931be2a9d0449c3eec151da751cb88591f1" translate="yes" xml:space="preserve">
          <source>I. Guyon, &amp;ldquo;Design of experiments for the NIPS 2003 variable selection benchmark&amp;rdquo;, 2003.</source>
          <target state="translated">I. Guyon，&amp;ldquo; NIPS 2003变量选择基准的实验设计&amp;rdquo;，2003年。</target>
        </trans-unit>
        <trans-unit id="074b587a2df67fa7bdaebb29bf4f1381e49051c3" translate="yes" xml:space="preserve">
          <source>I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Maci&amp;agrave;, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7280767&quot;&gt;Design of the 2015 ChaLearn AutoML Challenge&lt;/a&gt;, IJCNN 2015.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="483f14c4d9fef04833d7508282eb9a08a8019931" translate="yes" xml:space="preserve">
          <source>I.K. Yeo and R.A. Johnson, &amp;ldquo;A new family of power transformations to improve normality or symmetry.&amp;rdquo; Biometrika, 87(4), pp.954-959, (2000).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a238a89365b9d0ce7f5fb26e189eb03cdc08fbe5" translate="yes" xml:space="preserve">
          <source>ICA can also be used as yet another non linear decomposition that finds components with some sparsity:</source>
          <target state="translated">ICA也可以作为另一种非线性分解,找到具有一定稀疏性的成分。</target>
        </trans-unit>
        <trans-unit id="57933c6e2d57c3e3911db47768687ccc98d16924" translate="yes" xml:space="preserve">
          <source>IDpol</source>
          <target state="translated">IDpol</target>
        </trans-unit>
        <trans-unit id="fcc34dd193c826ae2f0c8b804c532252b4a25480" translate="yes" xml:space="preserve">
          <source>INDUS proportion of non-retail business acres per town</source>
          <target state="translated">每镇非零售商业亩数的工业比例。</target>
        </trans-unit>
        <trans-unit id="44a4d7b7db7815be999da6a406f4dadd2c4327c5" translate="yes" xml:space="preserve">
          <source>Identification number of each sample, as ordered in dataset.data.</source>
          <target state="translated">每个样本的识别号,按dataset.data的顺序排列。</target>
        </trans-unit>
        <trans-unit id="4b5c601d48e24d2806952d270f88677fcd7cfb39" translate="yes" xml:space="preserve">
          <source>Identifying which category an object belongs to.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02d51b4f13558cbcfc807b53522b1ffb156ad7e7" translate="yes" xml:space="preserve">
          <source>Identity: d(x, y) = 0 if and only if x == y</source>
          <target state="translated">特征:如果且仅当x==y时,d(x,y)=0。</target>
        </trans-unit>
        <trans-unit id="35bd2069c37f2c6a308bc5401948b247d5bcfc02" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;all&amp;rdquo;, the imputer mask will represent all features.</source>
          <target state="translated">如果为&amp;ldquo;全部&amp;rdquo;，则防毒面具将代表所有功能。</target>
        </trans-unit>
        <trans-unit id="84934b5d658c0a370c458ee55fa3255bb65884a6" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo; (default), the imputer mask will be of same type as input.</source>
          <target state="translated">如果为&amp;ldquo;自动&amp;rdquo;（默认），则防毒面具与输入的类型相同。</target>
        </trans-unit>
        <trans-unit id="7cdc1bc49e801caf1ff212e1000d88bb13d7e93e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="164a2722286c1b34bc2df80a90c75397afce3e6b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="911a50d98b398312fa01572b5d7b864da542117b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dca169b413a6ec050ae0928eb38f43b00b9c08e0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;constant&amp;rdquo;, then replace missing values with fill_value. Can be used with strings or numeric data.</source>
          <target state="translated">如果为&amp;ldquo;常量&amp;rdquo;，则将缺失的值替换为fill_value。可以与字符串或数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="fed653e1ff76c14b62a8cb9c0f4474c620b2641e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;log2&amp;rdquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; log2&amp;rdquo;，则 &lt;code&gt;max_features=log2(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e799052bdd1932be1b28378fc91f87421f6d1065" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along each column. Can only be used with numeric data.</source>
          <target state="translated">如果是&amp;ldquo;平均值&amp;rdquo;，则使用每列中的平均值替换缺失值。只能与数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="8c28cbae695709f5ae6daaba6d2035fa26d4e040" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along the axis.</source>
          <target state="translated">如果为&amp;ldquo;平均值&amp;rdquo;，则使用沿轴的平均值替换缺失值。</target>
        </trans-unit>
        <trans-unit id="d5353b7f39f25231d62cbbc36fcd604e05d2faa0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along each column. Can only be used with numeric data.</source>
          <target state="translated">如果为&amp;ldquo;中位数&amp;rdquo;，则使用每列中的中位数替换缺失值。只能与数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="2c7bf0a70af62c9d1ff80c38810d3732da415b46" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along the axis.</source>
          <target state="translated">如果为&amp;ldquo;中位数&amp;rdquo;，则使用沿轴的中位数替换缺失值。</target>
        </trans-unit>
        <trans-unit id="b9dfb246debbef95e2bc6e78da2b0aca54b8e768" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;missing-only&amp;rdquo; (default), the imputer mask will only represent features containing missing values during fit time.</source>
          <target state="translated">如果为&amp;ldquo;仅缺失&amp;rdquo;（默认），则不当面罩将仅表示在拟合时间内包含缺失值的要素。</target>
        </trans-unit>
        <trans-unit id="fb9cd590a090a11e857ebbc7c5d49f19787d4a57" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.</source>
          <target state="translated">如果为&amp;ldquo; most_frequent&amp;rdquo;，则使用每一列中的最频繁值替换&amp;ldquo; missing&amp;rdquo;。可以与字符串或数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="a90ab2bff0e7c4a2db0c7d70bcb17fa44e1b8cb3" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along the axis.</source>
          <target state="translated">如果为&amp;ldquo; most_frequent&amp;rdquo;，则使用沿轴的最频繁值替换&amp;ldquo; missing&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8007580c79fb9470823eec0b7f7391f7011a44a8" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that &lt;code&gt;base_estimator&lt;/code&gt; has been fitted already and all data is used for calibration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d25972b438acba3aa495003bff5b880c3dc78f95" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration.</source>
          <target state="translated">如果通过&amp;ldquo; prefit&amp;rdquo;，则假定已经安装了base_estimator，并且所有数据都用于校准。</target>
        </trans-unit>
        <trans-unit id="ddc01f2adfc0b5da49bb0bea104c04055f37082b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; (same as &amp;ldquo;auto&amp;rdquo;).</source>
          <target state="translated">如果为&amp;ldquo; sqrt&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; （与&amp;ldquo; auto&amp;rdquo;相同）。</target>
        </trans-unit>
        <trans-unit id="050de520f25e08567f8dcb7bcdaa6887bd8ca53c" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; sqrt&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="274dd12ab1d70a7a9d00df8bbe2aa7f35f2ca3c4" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;SAMME.R&amp;rsquo; then use the SAMME.R real boosting algorithm. &lt;code&gt;base_estimator&lt;/code&gt; must support calculation of class probabilities. If &amp;lsquo;SAMME&amp;rsquo; then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</source>
          <target state="translated">如果为&amp;ldquo; SAMME.R&amp;rdquo;，则使用SAMME.R实际增强算法。 &lt;code&gt;base_estimator&lt;/code&gt; 必须支持类概率的计算。如果为&amp;ldquo; SAMME&amp;rdquo;，则使用SAMME离散提升算法。SAMME.R算法通常比SAMME收敛更快，从而以更少的提升迭代次数实现了更低的测试误差。</target>
        </trans-unit>
        <trans-unit id="51b26722f92fe40c28c6f4d36bb516d8181566a9" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ae3261c7ce3a365ccb1e46d21ef269f4ad371b0" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, the threshold is determined as in the original paper.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34bbe83eef64f8685433d192f4dd39b726225179" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;auto&amp;rsquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bac8214432dad215f7331c0af16dfd3868b9e19" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;balanced&amp;rsquo;, class weights will be given by &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.</source>
          <target state="translated">如果&amp;ldquo;平衡&amp;rdquo;，则类权重将由 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 。如果给出字典，则键为类，值为相应的类权重。如果未指定，则班级权重将是统一的。</target>
        </trans-unit>
        <trans-unit id="2ed37f73bd5305414a6ee47c4802aa22bc780ac0" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;diagram&amp;rsquo;, estimators will be displayed as a diagram in a Jupyter lab or notebook context. If &amp;lsquo;text&amp;rsquo;, estimators will be displayed as text. Default is &amp;lsquo;text&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="456401c87cfc2a15cdd408f2dd8be315dc3e833e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;english&amp;rsquo;, a built-in stop word list for English is used. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">如果为&amp;ldquo; english&amp;rdquo;，则使用内置的英语停用词列表。&amp;ldquo;英语&amp;rdquo;存在几个已知的问题，您应该考虑一种替代方法（请参阅&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;使用停用词&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="f2019bdbdfa8956b7b54e8a5677954f4996f6374" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;file&amp;rsquo;, the sequence items must have a &amp;lsquo;read&amp;rsquo; method (file-like object) that is called to fetch the bytes in memory.</source>
          <target state="translated">如果为&amp;ldquo;文件&amp;rdquo;，则序列项必须具有一个&amp;ldquo;读取&amp;rdquo;方法（类似文件的对象），该方法被调用以获取内存中的字节。</target>
        </trans-unit>
        <trans-unit id="3dd1f3ca74314afe1ddf15184f6ab04977d1a20b" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;filename&amp;rsquo;, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</source>
          <target state="translated">如果为'filename'，则作为参数传递给fit的序列应该是需要读取以获取原始内容进行分析的文件名列表。</target>
        </trans-unit>
        <trans-unit id="6381402e5f026c95872c614d3f284a846d432d3e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;hard&amp;rsquo;, uses predicted class labels for majority rule voting. Else if &amp;lsquo;soft&amp;rsquo;, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.</source>
          <target state="translated">如果为&amp;ldquo;硬&amp;rdquo;，则将预测的类别标签用于多数规则投票。否则，如果为&amp;ldquo; soft&amp;rdquo;，则根据预测的概率之和的argmax来预测类别标签，建议对经过良好校准的分类器进行组合。</target>
        </trans-unit>
        <trans-unit id="4a61aad0a278d4a0b23f699c1bb6bc9a25b8f483" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;log2&amp;rsquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="034a602ff18129b75a77961464f62c916fb178cb" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;precomputed&amp;rsquo;, the training input X is expected to be a distance matrix.</source>
          <target state="translated">如果&amp;ldquo;预先计算&amp;rdquo;，则训练输入X预期为距离矩阵。</target>
        </trans-unit>
        <trans-unit id="d286c5a17d9d64b09f62a6cd1cf2dc973056dbb7" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;sqrt&amp;rsquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efe2693abb0ad6fb0bb32fc7410403c6f8a6131c" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果'warm_start'为True，则最后一次拟合的解决方案将用作下一次fit（）的初始化。在类似问题上多次调用拟合时，这可以加快收敛速度​​。在这种情况下，&amp;ldquo; n_init&amp;rdquo;将被忽略，并且在第一次调用时仅发生一次初始化。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="65bcde64719b83bdeb345ca5fab269155ae4a753" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="245f671779646599b33075b7dcf37bf735b34555" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果'warm_start'为True，则最后一次拟合的解决方案将用作下一次fit（）的初始化。在类似问题上多次调用拟合时，这可以加快收敛速度​​。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="775000d0783a0c4a0cc36e649a7e52e403237e23" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bb29953be4032aa8bb2090970ba2fb09c57dc65" translate="yes" xml:space="preserve">
          <source>If 0, no progress messages will be printed. If 1, progress messages will be printed to stdout. If &amp;gt; 1, progress messages will be printed and the &lt;code&gt;disp&lt;/code&gt; parameter of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize&quot;&gt;&lt;code&gt;scipy.optimize.minimize&lt;/code&gt;&lt;/a&gt; will be set to &lt;code&gt;verbose - 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a2ad0e9541d6795a0339e5a9c03b37f24ea5bc7" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; has not been called before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e311856e2dbc16a358c30263eb21c62e3f976c09" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; is given an explicit &lt;code&gt;feature_range=(min, max)&lt;/code&gt; the full formula is:</source>
          <target state="translated">如果给&lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; &lt;/a&gt;一个明确的 &lt;code&gt;feature_range=(min, max)&lt;/code&gt; 则完整公式为：</target>
        </trans-unit>
        <trans-unit id="6f352ac5787c6e0994736f1793f840aefef2eb74" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.</source>
          <target state="translated">如果 &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; 并且 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; ，则选择组件数，以使需要解释的方差量大于n_components指定的百分比。</target>
        </trans-unit>
        <trans-unit id="6154e481a1bfebf053da4021c41ed6b15075ac75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;Gram&lt;/code&gt; is overwritten.</source>
          <target state="translated">如果 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;Gram&lt;/code&gt; 被覆盖。</target>
        </trans-unit>
        <trans-unit id="c8ea59a59509714d84c6c3be2a8959e87ca2c339" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; is overwritten.</source>
          <target state="translated">如果 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;X&lt;/code&gt; 被覆盖。</target>
        </trans-unit>
        <trans-unit id="ecd953eee019b7cf39fa95c1745e9486ce5fb903" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回正确分类的样本数。否则，返回正确分类的样本的分数。</target>
        </trans-unit>
        <trans-unit id="8dd52da2b8b6d856acbbc6b969b86fd0a4246941" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of misclassifications. Otherwise, return the fraction of misclassifications.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回错误分类的数量。否则，返回错误分类的分数。</target>
        </trans-unit>
        <trans-unit id="85fcfc531652a8814592a07e791b2030fbc9598e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回样本集上的Jaccard相似系数之和。否则，返回Jaccard相似系数的平均值。</target>
        </trans-unit>
        <trans-unit id="c21045a17e85201e2f77134fc96d9edd698a8ef9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;cv_results_&lt;/code&gt; 属性将不包括训练分数。</target>
        </trans-unit>
        <trans-unit id="363d7b7bc89b4fe7d745ba13b3bf107700064544" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f7a2b9af6d7b5ad533a302e51ca948f564886c6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="translated">如果为 &lt;code&gt;None&lt;/code&gt; ，则使用估算器的默认计分器。</target>
        </trans-unit>
        <trans-unit id="ab9a136f2eebf764f09dd9b08d3d9d7a3daec6b9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8dec298ccc565c5c7f632ae827694845b31aa21" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;estimator&lt;/code&gt; is considered fitted if there exist an attribute that ends with a underscore and does not start with double underscore.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89a2c07a59e8a597581d7c4accbf8ade7624601a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;init_size= 3 * batch_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0206caad9c301767e28c1eb37b72a3a7f7598e94" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">如果为 &lt;code&gt;None&lt;/code&gt; ，则返回每个班级的分数。否则，这将确定对数据执行的平均类型：</target>
        </trans-unit>
        <trans-unit id="de8d1676c9bb98aea236fe73b6992134925cbda9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: Note: multiclass ROC AUC currently only handles the &amp;lsquo;macro&amp;rsquo; and &amp;lsquo;weighted&amp;rsquo; averages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1170dbf5a5618e80add067200212cb5804a58cd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ,则完整路径将存储在 &lt;code&gt;coef_path_&lt;/code&gt; 属性中。如果您针对一个大问题或许多目标计算解决方案， &lt;code&gt;fit_path&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 将导致加速，尤其是对于较小的alpha而言。</target>
        </trans-unit>
        <trans-unit id="97edef35821191bb6c4443dade8924b80d003e19" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; then features with missing values during &lt;code&gt;transform&lt;/code&gt; which did not have any missing values during &lt;code&gt;fit&lt;/code&gt; will be imputed with the initial imputation method only. Set to &lt;code&gt;True&lt;/code&gt; if you have many features with no missing values at both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; time to save compute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e703de20e5680ee264e2b1b950a8b1ca587cd24f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，将复制X；否则为X。否则，它可能会被覆盖。</target>
        </trans-unit>
        <trans-unit id="8e26b0bb501133486ba99496e311f44a49ce472b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, perform metric MDS; otherwise, perform nonmetric MDS.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则执行度量MDS；否则，执行非度量MDS。</target>
        </trans-unit>
        <trans-unit id="a2b129bca8e38a348fd53d1896c7796acded2f57" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return a sparse feature matrix</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则返回一个稀疏特征矩阵</target>
        </trans-unit>
        <trans-unit id="887d26ef7077238a636d223d3985225a211c8d82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则返回给定类的特征的先验类概率和条件概率，并从中得出数据。</target>
        </trans-unit>
        <trans-unit id="0b7bef40bad08d9d2c4e67da6c9b56ea79751005" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, some instances might not belong to any class.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则某些实例可能不属于任何类。</target>
        </trans-unit>
        <trans-unit id="e223ba26a8622e808f0df02e86007844177282d6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; or &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; 或 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 是应用于L1范数的惩罚。如果 &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 是阈值的绝对值，低于该阈值时系数将被压缩为零。如果 &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 为容差参数：目标重建误差的值。在这种情况下，它将覆盖 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c8adb20b3b91095a2c077330ab11f60b186c2818" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm='lasso_lars'&lt;/code&gt; or &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm='threshold'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm='omp'&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ef12938b73ad4fc11883a02f290e9f35cac58e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;axes_[i, j]&lt;/code&gt; is the axes on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;axes_[i]&lt;/code&gt; is the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes in that position.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd2bdf4031503663b9e9168a5bd9a1faa112780b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;contours_[i, j]&lt;/code&gt; is the partial dependence plot on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;contours_[i]&lt;/code&gt; is the partial dependence plot corresponding to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a contour plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16daa78b71c8fbb0a04353048c9be677f4bd1a67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;lines_[i, j]&lt;/code&gt; is the partial dependence curve on the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;lines_[i]&lt;/code&gt; is the partial dependence curve corresponding to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a line plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b7c522014bc873fbd61bfb64d47b7833096d1b1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;vlines_[i, j]&lt;/code&gt; is the line collection representing the x axis deciles of the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;vlines_[i]&lt;/code&gt; corresponds to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a PDP plot. .. versionadded:: 0.23</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03e758847856246528ffcca83ba803cf5d3e9c24" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, &lt;code&gt;vlines_[i, j]&lt;/code&gt; is the line collection representing the y axis deciles of the i-th row and j-th column. If &lt;code&gt;ax&lt;/code&gt; is a list of axes, &lt;code&gt;vlines_[i]&lt;/code&gt; corresponds to the i-th item in &lt;code&gt;ax&lt;/code&gt;. Elements that are None correspond to a nonexisting axes or an axes that does not include a 2-way plot. .. versionadded:: 0.23</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e5d3c60fbe26a21b3685b7cd50d8157796cb85c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;ax&lt;/code&gt; is an axes or None, the &lt;code&gt;bounding_ax_&lt;/code&gt; is the axes where the grid of partial dependence plots are drawn. If &lt;code&gt;ax&lt;/code&gt; is a list of axes or a numpy array of axes, &lt;code&gt;bounding_ax_&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a88caf8f6b6232395c9c1524315c6ed672bcf763" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt; and X is encoded as a CSR matrix;</source>
          <target state="translated">如果 &lt;code&gt;axis=0&lt;/code&gt; 且X编码为CSR矩阵；</target>
        </trans-unit>
        <trans-unit id="160d044408c23f7840586e9cc7d8cab0e43ba9b5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, boolean and integer array-like, integer slice, and scalar integer are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b74f02ef0c3e3aceaf2040e39764c8bf5d153fee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, then impute along columns.</source>
          <target state="translated">如果 &lt;code&gt;axis=0&lt;/code&gt; ，则沿列插补。</target>
        </trans-unit>
        <trans-unit id="231cba4e2ed9eee1fca5c3a6b02c0c6f66c47550" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt; and X is encoded as a CSC matrix.</source>
          <target state="translated">如果 &lt;code&gt;axis=1&lt;/code&gt; 并且X编码为CSC矩阵。</target>
        </trans-unit>
        <trans-unit id="4405a4c1e894889993d89bb6694cef1a6a8f7db3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;, then impute along rows.</source>
          <target state="translated">如果 &lt;code&gt;axis=1&lt;/code&gt; ，则沿行插补。</target>
        </trans-unit>
        <trans-unit id="afe718681bc04a6b175f3e0dbacf56b2e22d3277" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf71a4a70c1ff1b9076f02c43d89e78c4b0ffc27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;backend&lt;/code&gt; is a string it must match a previously registered implementation using the &lt;code&gt;register_parallel_backend&lt;/code&gt; function.</source>
          <target state="translated">如果 &lt;code&gt;backend&lt;/code&gt; 是字符串，则必须使用 &lt;code&gt;register_parallel_backend&lt;/code&gt; 函数匹配先前注册的实现。</target>
        </trans-unit>
        <trans-unit id="6456a4494f2ba1f052aff4cf6d35ef66e787bc14" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;base_estimator&lt;/code&gt; is None, then &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; is used for target values of dtype float.</source>
          <target state="translated">如果 &lt;code&gt;base_estimator&lt;/code&gt; 为None，则 &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; 用于dtype float的目标值。</target>
        </trans-unit>
        <trans-unit id="898731158b64382ef9aad2307b39d22b5cd2a315" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dense&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the dense binary indicator format. If &lt;code&gt;'sparse'&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the sparse binary indicator format. &lt;code&gt;False&lt;/code&gt; returns a list of lists of labels.</source>
          <target state="translated">如果为 &lt;code&gt;dense&lt;/code&gt; ，则以密集二进制指示符格式返回 &lt;code&gt;Y&lt;/code&gt; 。如果为 &lt;code&gt;'sparse'&lt;/code&gt; ，则以稀疏二进制指示符格式返回 &lt;code&gt;Y&lt;/code&gt; 。 &lt;code&gt;False&lt;/code&gt; 返回标签列表的列表。</target>
        </trans-unit>
        <trans-unit id="b1213caecc623f2a5139e82ba5db339edb5f6265" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="translated">如果 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False，则截距设置为零。当给定问题为二进制时， &lt;code&gt;intercept_&lt;/code&gt; 的形状为（1，）。特别是，当 &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt; ， &lt;code&gt;intercept_&lt;/code&gt; 对应于结果1（真），而 &lt;code&gt;-intercept_&lt;/code&gt; 对应于结果0（假）。</target>
        </trans-unit>
        <trans-unit id="5adb2b902dec2303b1749b6ba9f10123fe5ab03c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class='multinomial'&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4504cae87026fef1f6989cfa20e2e5bc171d37e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape(1,) when the problem is binary.</source>
          <target state="translated">如果 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False，则截距设置为零。问题为二进制时， &lt;code&gt;intercept_&lt;/code&gt; 的形状为（1，）。</target>
        </trans-unit>
        <trans-unit id="646836188c841e4fea39e4e4200d1f27e6191986" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;loss&lt;/code&gt; is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on &lt;code&gt;X[i]&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;loss&lt;/code&gt; 是可调用的，则它应该是一个将两个数组作为输入的真实值和预测值，并返回一维数组的函数，该数组的第i个值对应于 &lt;code&gt;X[i]&lt;/code&gt; 上的损失。</target>
        </trans-unit>
        <trans-unit id="c42d62680a26d66fc0f439c634f24ee01c670c77" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;memory&lt;/code&gt; is not joblib.Memory-like.</source>
          <target state="translated">如果 &lt;code&gt;memory&lt;/code&gt; 不是joblib。类似内存。</target>
        </trans-unit>
        <trans-unit id="e14dd7c153267d74f6b2214ce66bcf041482d792" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_bins&lt;/code&gt; is an array, and there is an ignored feature at index &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;n_bins[i]&lt;/code&gt; will be ignored.</source>
          <target state="translated">如果 &lt;code&gt;n_bins&lt;/code&gt; 是一个数组，并且在索引 &lt;code&gt;i&lt;/code&gt; 处有被忽略的功能，则 &lt;code&gt;n_bins[i]&lt;/code&gt; 将被忽略。</target>
        </trans-unit>
        <trans-unit id="20ab457ec31da79f104a0f7a337ba6bfe94b5438" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.</source>
          <target state="translated">如果 &lt;code&gt;n_clusters&lt;/code&gt; 设置为None，则数据从100,000个样本减少到158个群集。这可以看作是最终（全局）群集步骤之前的预处理步骤，该步骤进一步将这158个群集减少到100个群集。</target>
        </trans-unit>
        <trans-unit id="1769c2fe615105013ff722090827f85ac960dff9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components == 'mle'&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, Minka&amp;rsquo;s MLE is used to guess the dimension. Use of &lt;code&gt;n_components == 'mle'&lt;/code&gt; will interpret &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; as &lt;code&gt;svd_solver == 'full'&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 并且 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; ，则使用Minka的MLE来猜测尺寸。使用 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 会将 &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; 为 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="959758e689ea656dd0e72e3bda18b0a45bbef2e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is not set then all components are stored and the sum of the ratios is equal to 1.0.</source>
          <target state="translated">如果未设置 &lt;code&gt;n_components&lt;/code&gt; ,则将存储所有分量，并且比率之和等于1.0。</target>
        </trans-unit>
        <trans-unit id="712e62a0190856257c5f6877b0d11f259bd408b5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is strictly smaller than the dimensionality of the inputs passed to &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, the identity matrix will be truncated to the first &lt;code&gt;n_components&lt;/code&gt; rows.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4aaa2df3fa37c88b24d06638ef7188d7e8ebe112" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each parameter setting(and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_jobs&lt;/code&gt; 的值设置为大于1，则将为每个参数设置复制数据（而不是 &lt;code&gt;n_jobs&lt;/code&gt; 次）。出于效率原因，如果单个作业花费的时间很少，则执行此操作，但是如果数据集很大且没有足够的可用内存，则可能会引发错误。在这种情况下，一种解决方法是设置 &lt;code&gt;pre_dispatch&lt;/code&gt; 。然后，仅将 &lt;code&gt;pre_dispatch&lt;/code&gt; 复制多次内存。一个合理的值 &lt;code&gt;pre_dispatch&lt;/code&gt; 是 &lt;code&gt;2 * n_jobs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="53682a81a25d0884d79ca09b064b0fc6e7cabd67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each point in the grid (and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_jobs&lt;/code&gt; 的值设置为大于1，则将为网格中的每个点复制数据（而不是 &lt;code&gt;n_jobs&lt;/code&gt; 次）。出于效率原因，如果单个作业花费的时间很少，则执行此操作，但是如果数据集很大且没有足够的可用内存，则可能会引发错误。在这种情况下，一种解决方法是设置 &lt;code&gt;pre_dispatch&lt;/code&gt; 。然后，仅将 &lt;code&gt;pre_dispatch&lt;/code&gt; 复制多次内存。一个合理的值 &lt;code&gt;pre_dispatch&lt;/code&gt; 是 &lt;code&gt;2 * n_jobs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c1cc035dd2ff12188f95601d6fe5c4679ad6bb78" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_samples == 10000&lt;/code&gt;, storing &lt;code&gt;X&lt;/code&gt; as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = &lt;strong&gt;4GB in RAM&lt;/strong&gt; which is barely manageable on today&amp;rsquo;s computers.</source>
          <target state="translated">如果 &lt;code&gt;n_samples == 10000&lt;/code&gt; ，将 &lt;code&gt;X&lt;/code&gt; 存储为float32类型的NumPy数组将需要10000 x 100000 x 4字节= &lt;strong&gt;4GB RAM&lt;/strong&gt;，这在当今的计算机上几乎无法管理。</target>
        </trans-unit>
        <trans-unit id="dd40778e7a383f2053d98b9a07e6219944c6316f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;needs_proba=False&lt;/code&gt; and &lt;code&gt;needs_threshold=False&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. If &lt;code&gt;needs_proba=True&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; (For binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept probability of the positive class). If &lt;code&gt;needs_threshold=True&lt;/code&gt;, the score function is supposed to accept the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e65ba558868cb56b0c8a76632ea67901b254afe" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回平均Jaccard相似系数，否则返回样本集上Jaccard相似系数的总和。</target>
        </trans-unit>
        <trans-unit id="c606413521700e073d3e669024415faa2300a113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回正确分类的样本的分数（浮点数），否则返回正确分类的样本数（整数）。</target>
        </trans-unit>
        <trans-unit id="cd9dc36a4d7167817c2823908b4ce633913b9765" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回错误分类的分数（浮点数），否则返回错误分类的数目（整数）。</target>
        </trans-unit>
        <trans-unit id="4260d2abf4eeb10994306d99a6424e7eb5ca19c8" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;order='random'&lt;/code&gt;, determines random number generation for the chain order. In addition, it controls the random seed given at each &lt;code&gt;base_estimator&lt;/code&gt; at each chaining iteration. Thus, it is only used when &lt;code&gt;base_estimator&lt;/code&gt; exposes a &lt;code&gt;random_state&lt;/code&gt;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0b860961bc5b4a4c45189c0fd4de5c2d61f1ab4" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;out=None&lt;/code&gt;, returns a new array containing the mean values, otherwise a reference to the output array is returned.</source>
          <target state="translated">如果 &lt;code&gt;out=None&lt;/code&gt; ，则返回一个包含平均值的新数组，否则返回对输出数组的引用。</target>
        </trans-unit>
        <trans-unit id="a1e72f87e91fc5bb6f8882ece59a46bf9ee089e2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;pos_label is None&lt;/code&gt; and in binary classification, this function returns the average precision, recall and F-measure if &lt;code&gt;average&lt;/code&gt; is one of &lt;code&gt;'micro'&lt;/code&gt;, &lt;code&gt;'macro'&lt;/code&gt;, &lt;code&gt;'weighted'&lt;/code&gt; or &lt;code&gt;'samples'&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;pos_label is None&lt;/code&gt; 并且在二进制分类中，则该函数将返回平均精度，查全率和F度量，如果 &lt;code&gt;average&lt;/code&gt; 是 &lt;code&gt;'micro'&lt;/code&gt; ， &lt;code&gt;'macro'&lt;/code&gt; ， &lt;code&gt;'weighted'&lt;/code&gt; 或 &lt;code&gt;'samples'&lt;/code&gt; 之一。</target>
        </trans-unit>
        <trans-unit id="933b8be58a37dcefc6ca9fd3f4735aba59bace4c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;probability=True&lt;/code&gt;, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If &lt;code&gt;probability=False&lt;/code&gt;, it&amp;rsquo;s an empty array. Platt scaling uses the logistic function &lt;code&gt;1 / (1 + exp(decision_value * probA_ + probB_))&lt;/code&gt; where &lt;code&gt;probA_&lt;/code&gt; and &lt;code&gt;probB_&lt;/code&gt; are learned from the dataset &lt;a href=&quot;#r20c70293ef72-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt;. For more information on the multiclass case and training procedure see section 8 of &lt;a href=&quot;#r20c70293ef72-1&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afcc3cf20f075d3a281dbe1f0f610f65f9ef38a7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;probability=True&lt;/code&gt;, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If &lt;code&gt;probability=False&lt;/code&gt;, it&amp;rsquo;s an empty array. Platt scaling uses the logistic function &lt;code&gt;1 / (1 + exp(decision_value * probA_ + probB_))&lt;/code&gt; where &lt;code&gt;probA_&lt;/code&gt; and &lt;code&gt;probB_&lt;/code&gt; are learned from the dataset &lt;a href=&quot;#r9709ce4a60d3-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt;. For more information on the multiclass case and training procedure see section 8 of &lt;a href=&quot;#r9709ce4a60d3-1&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bb4c6eca31ede3ca3e8fe5a9b41ecd9a55b266b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_path==True&lt;/code&gt; returns the entire path, else returns only the last point of the path.</source>
          <target state="translated">如果 &lt;code&gt;return_path==True&lt;/code&gt; 返回整个路径，否则仅返回路径的最后一点。</target>
        </trans-unit>
        <trans-unit id="9e477bb8072307702a812f869b8166fe31bf9ca0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</source>
          <target state="translated">如果 &lt;code&gt;smooth_idf=True&lt;/code&gt; （默认值），则将常数&amp;ldquo; 1&amp;rdquo;添加到idf的分子和分母，就好像看到一个额外的文档恰好包含一次集合中的每个术语一样，这防止了零除：idf（d，t ）= log [（1 + n）/（1 + df（d，t））] + 1。</target>
        </trans-unit>
        <trans-unit id="7e7ad1036fa8cee418b26fc730608087f9208f2c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f772487671a5560a2a2bb3464b54cf0bad5b348" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt;, the number of components must be strictly less than the minimum of n_features and n_samples.</source>
          <target state="translated">如果 &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt; ，则组件的数量必须严格小于n_features和n_samples的最小值。</target>
        </trans-unit>
        <trans-unit id="abdb8ed2b871d03510343cf9ee77736674d298ab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;validate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; will be checked.</source>
          <target state="translated">如果 &lt;code&gt;validate&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，将检查 &lt;code&gt;X&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5cfb594032bd50fcef2738a25df520e95867f411" translate="yes" xml:space="preserve">
          <source>If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:</source>
          <target state="translated">如果C是一个地道的类赋值,K是聚类,让我们把\(a\)和\(b\)定义为。</target>
        </trans-unit>
        <trans-unit id="40e72ab25b1921db07187a1c526cc9080a10eaea" translate="yes" xml:space="preserve">
          <source>If False, X will be overwritten. &lt;code&gt;copy=False&lt;/code&gt; can be used to save memory but is unsafe for general use.</source>
          <target state="translated">如果为False，则X将被覆盖。 &lt;code&gt;copy=False&lt;/code&gt; 可以用来节省内存，但是对于一般用途来说是不安全的。</target>
        </trans-unit>
        <trans-unit id="b5379fd8e8700833a560e6ab84ea58c40e10b6a8" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</source>
          <target state="translated">如果False,传递给fit的数据会被覆盖,运行fit(X).transform(X)不会得到预期的结果,请使用fit_transform(X)代替。</target>
        </trans-unit>
        <trans-unit id="3d545281a4ef31d03ffb244079b728a3c5cc8b18" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten. Defaults to True.</source>
          <target state="translated">如果为False,则覆盖传递给拟合的数据。默认值为True。</target>
        </trans-unit>
        <trans-unit id="4bf616e8d9d604d2525c59d889782525e410270c" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned</source>
          <target state="translated">如果为 &quot;False&quot;,将不返回距离。</target>
        </trans-unit>
        <trans-unit id="b4145c6e6cc098818614b51aca0cae30eca8ed94" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8cdf8e9cb326e6212f67c80aca3a4f04326fc4c" translate="yes" xml:space="preserve">
          <source>If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">如果False,如果数据在本地不可用,则引发一个IOError,而不是试图从源站点下载数据。</target>
        </trans-unit>
        <trans-unit id="707f36c34b2f81eacbaf143a8e62cb9371b1332e" translate="yes" xml:space="preserve">
          <source>If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">如果False,如果数据在本地不可用,则引发一个IOError,而不是试图从源站点下载数据。</target>
        </trans-unit>
        <trans-unit id="d32001af2bcb0806daa431ab8cf432f700c0bb79" translate="yes" xml:space="preserve">
          <source>If False, the imputer mask will be a numpy array.</source>
          <target state="translated">如果False,imputer mask将是一个numpy数组。</target>
        </trans-unit>
        <trans-unit id="2823ebb07c9bdb5cae0bfca227f5db48d585ba5a" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and dictionary will not be checked.</source>
          <target state="translated">如果False,则不会检查输入数组X和字典。</target>
        </trans-unit>
        <trans-unit id="bd8e933f9aa74b9b27da566d4ffb96f4e62218cf" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and y will not be checked.</source>
          <target state="translated">如果False,则不检查输入数组X和y。</target>
        </trans-unit>
        <trans-unit id="85aa52dd8c7d5d29b6bebfd616a7ca3fe91cde14" translate="yes" xml:space="preserve">
          <source>If False, the projected data uses a sparse representation if the input is sparse.</source>
          <target state="translated">如果False,如果输入是稀疏的,则投影数据使用稀疏表示。</target>
        </trans-unit>
        <trans-unit id="7bfec8f3204bdf713e2d3557ec53ea6f3960ad24" translate="yes" xml:space="preserve">
          <source>If False, there is no input validation.</source>
          <target state="translated">如果为False,则没有输入验证。</target>
        </trans-unit>
        <trans-unit id="a67320198a0b746d35fcc941198f1221ee73c87b" translate="yes" xml:space="preserve">
          <source>If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</source>
          <target state="translated">如果为False,则尽量避免复制,而是进行原地缩放。这并不能保证总是在原地缩放;例如,如果数据不是 NumPy 数组或 scipy.sparse CSR 矩阵,仍然可能返回一个副本。</target>
        </trans-unit>
        <trans-unit id="fcd08eda0bca1685e28de89ae046095006b92653" translate="yes" xml:space="preserve">
          <source>If None (default), load all the categories. If not None, list of category names to load (other categories ignored).</source>
          <target state="translated">如果为 &quot;无&quot;(默认),加载所有类别。如果不为 &quot;无&quot;,则加载类别名称列表(其他类别忽略)。</target>
        </trans-unit>
        <trans-unit id="7c0ab0b638c1cad0f1492e60796eecd4f321a731" translate="yes" xml:space="preserve">
          <source>If None (default), then draw &lt;code&gt;X.shape[0]&lt;/code&gt; samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7cbd99fe721a3cd173045eddaf688b83e7620c1" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s default scorer, if available, is used.</source>
          <target state="translated">如果为None，则使用估算器的默认评分器（如果有）。</target>
        </trans-unit>
        <trans-unit id="8c0f950a52950ceff35d04e0ab42f83a05b3adb9" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96ad58db5ee1b903109c173fcab72b0955bb0408" translate="yes" xml:space="preserve">
          <source>If None, defaults to 1.0 / n_features</source>
          <target state="translated">如果无,则默认为1.0/n_features。</target>
        </trans-unit>
        <trans-unit id="e5ce9a9046a52014390758ba790166ae01779c1f" translate="yes" xml:space="preserve">
          <source>If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.</source>
          <target state="translated">如果为 &quot;无&quot;,不要尝试对文件的内容进行解码(例如图像或其他非文本内容)。如果不是None,如果load_content为True,则使用编码将文本文件解码为Unicode。</target>
        </trans-unit>
        <trans-unit id="3e5e8d666168a7a15a80edb16364256ae0a379e4" translate="yes" xml:space="preserve">
          <source>If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</source>
          <target state="translated">max_df可以设置为一个范围[0.7,1.0)的值,以根据语料库内术语的文档频率自动检测和过滤停止词。</target>
        </trans-unit>
        <trans-unit id="02542a43a2f09f5328657402d69f49ce442cb6c2" translate="yes" xml:space="preserve">
          <source>If None, pairwise_distances_chunked returns a generator of vertical chunks of the distance matrix.</source>
          <target state="translated">如果为None,则pairwise_distances_chunked返回距离矩阵垂直分块的生成器。</target>
        </trans-unit>
        <trans-unit id="eb5d73cb83520641b0e2c815c109159135d569cc" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s default scorer (if available) is used.</source>
          <target state="translated">如果为None，则使用估算器的默认计分器（如果有）。</target>
        </trans-unit>
        <trans-unit id="43ea051b06405f0316e7696e80296e11d93edbf6" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s score method is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="651c3653c15c90a6722a00465ba57c19c30adb9b" translate="yes" xml:space="preserve">
          <source>If None, the threshold is assumed to be half way between neg_label and pos_label.</source>
          <target state="translated">如果None,则假定阈值为neg_label和pos_label之间的一半。</target>
        </trans-unit>
        <trans-unit id="ac652d29bc285e4e46f5aaf2fe5415c63aee1f09" translate="yes" xml:space="preserve">
          <source>If None, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">如果为None，则 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2f9e5ee96434f57529c2481b71d631d9dd0cb5e7" translate="yes" xml:space="preserve">
          <source>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</source>
          <target state="translated">如果为真(默认),平方误差标准值除以n_features。如果为False,则不会重新调整平方误差标准值。</target>
        </trans-unit>
        <trans-unit id="da82574bb396bf8045c493d20398be74e4e9ef51" translate="yes" xml:space="preserve">
          <source>If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).</source>
          <target state="translated">如果为True(默认),则包含一个偏置列,即所有多项式幂为零的特征(即一列1--作为线性模型中的截距项)。</target>
        </trans-unit>
        <trans-unit id="d150b2a4c21e929dfd726f6463d03ca9f005e91a" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="translated">如果为True（默认值），则当变换中的要素缺失值且拟合中没有缺失值时，变换将引发错误。仅当 &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt; 时适用。</target>
        </trans-unit>
        <trans-unit id="9da3cf1a0e0153fc6c646a1aa71f68e34d8d27f5" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit. This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4fe165b3f600e13152308080cc2a736269c867b" translate="yes" xml:space="preserve">
          <source>If True and &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; has been called before, the solution of the previous call to &lt;a href=&quot;#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is used as the initial linear transformation (&lt;code&gt;n_components&lt;/code&gt; and &lt;code&gt;init&lt;/code&gt; will be ignored).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d91ad850130f94be79fb668041bf3eecd01a29b0" translate="yes" xml:space="preserve">
          <source>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to &amp;lsquo;sag&amp;rsquo;. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</source>
          <target state="translated">如果为True且X为稀疏，则该方法还将返回截距，并且求解器会自动更改为'sag'。这只是将截距与稀疏数据拟合的临时解决方案。对于密集数据，请在回归之前使用sklearn.linear_model._preprocess_data。</target>
        </trans-unit>
        <trans-unit id="d1eef608e634bc22aa5f2e22e802ae927e355666" translate="yes" xml:space="preserve">
          <source>If True returns MSE value, if False returns RMSE value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a68108e0ea5bf983075f127e077ee80517d6dfdd" translate="yes" xml:space="preserve">
          <source>If True the covariance matrices are computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="translated">如果为True，则计算协方差矩阵并将其存储在 &lt;code&gt;self.covariance_&lt;/code&gt; 属性中。</target>
        </trans-unit>
        <trans-unit id="5c5d5facc126a265032a533a4664c8926339ade0" translate="yes" xml:space="preserve">
          <source>If True the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">如果为True，则完整路径将存储在 &lt;code&gt;coef_path_&lt;/code&gt; 属性中。如果您针对一个大问题或许多目标计算解决方案， &lt;code&gt;fit_path&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 将导致加速，尤其是对于较小的alpha而言。</target>
        </trans-unit>
        <trans-unit id="53e960778922c6ba257a9c66f18d0e260655f043" translate="yes" xml:space="preserve">
          <source>If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs.</source>
          <target state="translated">如果为真,函数返回对偶距离矩阵,否则返回分量L1对偶距离。不支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="2546c89362b151bbba35dab463b810d1f7c0a359" translate="yes" xml:space="preserve">
          <source>If True the order of the dataset is shuffled to avoid having images of the same person grouped.</source>
          <target state="translated">如果为True,数据集的顺序会被洗牌,以避免同一人的图像被分组。</target>
        </trans-unit>
        <trans-unit id="618a67ac95fc4ccc3385ae319143bf344e1ffb63" translate="yes" xml:space="preserve">
          <source>If True then raise a warning if conversion is required.</source>
          <target state="translated">如果为True,则在需要转换时发出警告。</target>
        </trans-unit>
        <trans-unit id="19362eed638b2dc6d204e12092075aedd87e6e93" translate="yes" xml:space="preserve">
          <source>If True then raise an exception if array is not symmetric.</source>
          <target state="translated">如果为True,那么如果数组不是对称的,则引发异常。</target>
        </trans-unit>
        <trans-unit id="baf82faf959595f525e5d5a94c6b8526ad942779" translate="yes" xml:space="preserve">
          <source>If True, X will be copied; else, it may be overwritten.</source>
          <target state="translated">如果为True,X将被复制;否则,可能被覆盖。</target>
        </trans-unit>
        <trans-unit id="e25c58f63a5736150718d46a3a4c05d31a44c0e6" translate="yes" xml:space="preserve">
          <source>If True, a &lt;a href=&quot;sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transform will stack onto output of the imputer&amp;rsquo;s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won&amp;rsquo;t appear on the missing indicator even if there are missing values at transform/test time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df1778fbc0c3701b8b17966bdd64201d1f2550fe" translate="yes" xml:space="preserve">
          <source>If True, a &lt;a href=&quot;sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transform will stack onto the output of the imputer&amp;rsquo;s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won&amp;rsquo;t appear on the missing indicator even if there are missing values at transform/test time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa91f074d713faca021e35ddd5331805b9848f9e" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, a copy may still be returned if X&amp;rsquo;s dtype is not a floating point type.</source>
          <target state="translated">如果为True，将创建X的副本。如果为False，则如果X的dtype不是浮点类型，则仍可能返回副本。</target>
        </trans-unit>
        <trans-unit id="45f6e4d313f4bf3abc200106ce518a942e07ab23" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="054b374d036034be5d7258a6a975038eb8fec935" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if &lt;code&gt;copy=False&lt;/code&gt;:</source>
          <target state="translated">如果为True，将创建X的副本。如果为False，则插补将在任何可能的地方进行。请注意，在以下情况下，即使 &lt;code&gt;copy=False&lt;/code&gt; ，也将始终创建一个新副本：</target>
        </trans-unit>
        <trans-unit id="f237ade75e04520befb53fc36267d58be41dc5ae" translate="yes" xml:space="preserve">
          <source>If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.</source>
          <target state="translated">如果为真,则在对象中存储训练数据的持久化副本。否则,只是存储了一个对训练数据的引用,如果从外部修改数据,可能会导致预测结果改变。</target>
        </trans-unit>
        <trans-unit id="5b4ca3bdeb594ab34289df8cfc7fa70c9919ad95" translate="yes" xml:space="preserve">
          <source>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</source>
          <target state="translated">如果True,则所有非零计数都设置为1,这对于模拟二进制事件而不是整数计数的离散概率模型非常有用。</target>
        </trans-unit>
        <trans-unit id="95f1a98443a6aa5501cfb81c289c266bb7baf4d6" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0beec2b2d6b910456e155063f83ec1e59c6c0df1" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)</source>
          <target state="translated">如果为True,所有非零的项数都被设置为1,这并不意味着输出只有0/1的值,只是tf-idf中的tf项是二进制的。(将idf和归一化设置为False,得到0/1的输出。)</target>
        </trans-unit>
        <trans-unit id="b69242de00e60526a79082b37846a2a724d7b3dd" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling.</source>
          <target state="translated">如果为真,则在缩放前将数据居中。</target>
        </trans-unit>
        <trans-unit id="6c9a4d2da449884c97015be12ce056a53dc04757" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">如果为True,在缩放前将数据居中。这在稀疏矩阵上是行不通的(并且会引发一个异常),因为将数据居中需要建立一个密集矩阵,而这个密集矩阵在一般的使用情况下可能会太大而无法放入内存。</target>
        </trans-unit>
        <trans-unit id="9d7135e468914be214009091b1fc11f6721afd0a" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This will cause &lt;code&gt;transform&lt;/code&gt; to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">如果为True，则在缩放之前将数据居中。尝试使用稀疏矩阵时，这将导致 &lt;code&gt;transform&lt;/code&gt; 引发异常，因为将它们居中需要构建密集的矩阵，在常见的用例中，该矩阵可能太大而无法容纳在内存中。</target>
        </trans-unit>
        <trans-unit id="694a00b6963c573c0a68b328feb15483853b44c0" translate="yes" xml:space="preserve">
          <source>If True, compute the log marginal likelihood at each iteration of the optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c24f1c3d35266b322135ac3270540ea5ef40a09d" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20839df17b3bca6b55533337f17b7c17f0881d1a" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False</source>
          <target state="translated">如果为真,在模型的每一步计算目标函数。默认为False</target>
        </trans-unit>
        <trans-unit id="afb80999f635ad0619301e31bb4cd6b353188af0" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False.</source>
          <target state="translated">如果为真,在模型的每一步计算目标函数。默认为False。</target>
        </trans-unit>
        <trans-unit id="9df36ef1b24eb49a93a9d932c395b3324554689c" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">如果为真,计算前数据不居中。用于处理平均数显著等于零但不完全为零的数据。如果为假,数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="cdd266c276f30f1ed8fec5e418bed1790d829f9f" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.</source>
          <target state="translated">如果为真,数据在计算前不居中。在处理平均数几乎为零但不完全为零的数据时很有用。如果为False(默认),数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="b4e1adfc1374b7a62eee31a2ac4be08eea958149" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">如果为真,数据在计算前不居中。在处理平均数几乎为零但不完全为零的数据时很有用。如果为假,数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="4b7584f328528f9b2a426f648be6c8534ecc99c4" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="899541250010cf3e75fa34e54556cce0c0296945" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d925d680717ba978e6037a9fa61e005a509ad73c" translate="yes" xml:space="preserve">
          <source>If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2902b07926590508dee697d1e97c541f35cd531" translate="yes" xml:space="preserve">
          <source>If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.</source>
          <target state="translated">如果为True,即使输入和随机投影矩阵都是稀疏的,也要保证随机投影的输出是一个密集的numpy数组。在实际应用中,如果分量较少,投影数据中零分量的数量会非常少,使用密实的表示方式会更节省CPU和内存。</target>
        </trans-unit>
        <trans-unit id="41679ec27e720c4445a3d18e34d01eeadf44de13" translate="yes" xml:space="preserve">
          <source>If True, explicitely compute the weighted within-class covariance matrix when solver is &amp;lsquo;svd&amp;rsquo;. The matrix is always computed and stored for the other solvers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5a6d396b2b3b1ae647f1eed557078cef4ea31ec" translate="yes" xml:space="preserve">
          <source>If True, for binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept a 1D &lt;code&gt;y_pred&lt;/code&gt; (i.e., probability of the positive class or the decision function, shape &lt;code&gt;(n_samples,)&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92550063b64465fcb2c6a46cd5346c9970dc1bb3" translate="yes" xml:space="preserve">
          <source>If True, for binary &lt;code&gt;y_true&lt;/code&gt;, the score function is supposed to accept a 1D &lt;code&gt;y_pred&lt;/code&gt; (i.e., probability of the positive class, shape &lt;code&gt;(n_samples,)&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3946c2202800dbed0f15da39a81b563f2054e41" translate="yes" xml:space="preserve">
          <source>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">如果为真,则在训练数据的随机子集上对各个树进行替换采样拟合。如果为 &quot;False&quot;,则执行无替换采样。</target>
        </trans-unit>
        <trans-unit id="47a8f3ebe1bf3e97122b0404e375e9c09f1cef3f" translate="yes" xml:space="preserve">
          <source>If True, input X is copied and stored by the model in the &lt;code&gt;X_fit_&lt;/code&gt; attribute. If no further changes will be done to X, setting &lt;code&gt;copy_X=False&lt;/code&gt; saves memory by storing a reference.</source>
          <target state="translated">如果为True，则模型将输入X复制并存储在 &lt;code&gt;X_fit_&lt;/code&gt; 属性中。如果对X不再做任何更改，则将 &lt;code&gt;copy_X=False&lt;/code&gt; 设置为通过存储引用来节省内存。</target>
        </trans-unit>
        <trans-unit id="396eb3a1b9a656b43a14e28fce4ff1f92f4bae42" translate="yes" xml:space="preserve">
          <source>If True, normalizes each document&amp;rsquo;s feature vector to unit norm using &lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f3a3bba5e0b6c545bdd5d5451a2769f0ab6a17d" translate="yes" xml:space="preserve">
          <source>If True, only the parameters that were set to non-default values will be printed when printing an estimator. For example, &lt;code&gt;print(SVC())&lt;/code&gt; while True will only print &amp;lsquo;SVC()&amp;rsquo; while the default behaviour would be to print &amp;lsquo;SVC(C=1.0, cache_size=200, &amp;hellip;)&amp;rsquo; with all the non-changed parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e2f5f725dbeca38c6f078963224eb5885ad4abb" translate="yes" xml:space="preserve">
          <source>If True, only the parameters that were set to non-default values will be printed when printing an estimator. For example, &lt;code&gt;print(SVC())&lt;/code&gt; while True will only print &amp;lsquo;SVC()&amp;rsquo;, but would print &amp;lsquo;SVC(C=1.0, cache_size=200, &amp;hellip;)&amp;rsquo; with all the non-changed parameters when False. Default is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10ab7d9c4ef9751f0861c3cfbcd363da08ee1196" translate="yes" xml:space="preserve">
          <source>If True, return a sparse CSR continency matrix. If &lt;code&gt;eps is not None&lt;/code&gt;, and &lt;code&gt;sparse is True&lt;/code&gt;, will throw ValueError.</source>
          <target state="translated">如果为True，则返回一个稀疏的CSR连续性矩阵。如果 &lt;code&gt;eps is not None&lt;/code&gt; ，而 &lt;code&gt;sparse is True&lt;/code&gt; ，将抛出ValueError。</target>
        </trans-unit>
        <trans-unit id="bbadcb21277fb2fd7500e5e018984adc0515f847" translate="yes" xml:space="preserve">
          <source>If True, return output as dict</source>
          <target state="translated">如果为True,返回dict的输出</target>
        </trans-unit>
        <trans-unit id="5d0bfe964a0a56bae92b114342901e57452f609f" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c0f661b8fac7f22363a5a1e55327c6967bb56d8" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.21, to correspond to the standard definition of cross-validation.</source>
          <target state="translated">如果为真,返回各褶皱的平均得分,按每个测试集的样本数加权。在这种情况下,假设数据在各个褶皱中是完全相同的分布,最小化的损失是每个样本的总损失,而不是各个褶皱的平均损失。如果为False,则返回各褶皱的平均分数。默认为True,但在0.21版本中将改为False,以符合交叉验证的标准定义。</target>
        </trans-unit>
        <trans-unit id="cd7031ac688b02e25258c5831c7d3ea164486db6" translate="yes" xml:space="preserve">
          <source>If True, return the distance between the clusters.</source>
          <target state="translated">如果为True,返回簇之间的距离。</target>
        </trans-unit>
        <trans-unit id="a6e36ec8e19c4760ce9d4d884ef8627513b82b97" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a &lt;code&gt;Bunch&lt;/code&gt; object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48eeb388f7c432fd1b00c136703cc691939b77aa" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data, target)&lt;/code&gt; 而不是Bunch对象。有关 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 对象的更多信息，请参见下文。</target>
        </trans-unit>
        <trans-unit id="b8ef976b3bf0a8c5fe0d328bf69ca77595314915" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; objects.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data, target)&lt;/code&gt; 而不是Bunch对象。有关 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 对象的更多信息，请参见下文。</target>
        </trans-unit>
        <trans-unit id="d467c22a2bd71ab649cbad26364f06a31ce58ac1" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data.data, data.target)&lt;/code&gt; instead of a Bunch object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data.data, data.target)&lt;/code&gt; 而不是Bunch对象。</target>
        </trans-unit>
        <trans-unit id="248b6d0d481e47f352d5f3203242e6800072c658" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;dataset.data&lt;/code&gt; and &lt;code&gt;dataset.target&lt;/code&gt; object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; 而不是Bunch对象。请参阅下文，以获取有关 &lt;code&gt;dataset.data&lt;/code&gt; 和 &lt;code&gt;dataset.target&lt;/code&gt; 对象的更多信息。</target>
        </trans-unit>
        <trans-unit id="c220af25fde2fda1e9985d78b63166a331905d63" translate="yes" xml:space="preserve">
          <source>If True, scale the data to interquartile range.</source>
          <target state="translated">如果为 &quot;True&quot;,将数据放大到四分位数范围。</target>
        </trans-unit>
        <trans-unit id="0f395f8257fe0bdfb8a823c88f3be9843583f8a6" translate="yes" xml:space="preserve">
          <source>If True, scale the data to unit variance (or equivalently, unit standard deviation).</source>
          <target state="translated">如果为 &quot;True&quot;,则将数据缩放为单位方差(或等价的单位标准差)。</target>
        </trans-unit>
        <trans-unit id="df95a4486aac1e164d23cfb2a72a9a3a02411ccb" translate="yes" xml:space="preserve">
          <source>If True, the class covariance matrices are explicitely computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a491ca8d8797fa01293c195b8787d725c30e073a" translate="yes" xml:space="preserve">
          <source>If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.</source>
          <target state="translated">如果为真,簇被放在超立方体的顶点上,如果为假,簇被放在随机多角体的顶点上。如果为 &quot;False&quot;,则聚类被放在随机多角体的顶点上。</target>
        </trans-unit>
        <trans-unit id="dc426ca785aa82bc3726faf833e38673875ff1ab" translate="yes" xml:space="preserve">
          <source>If True, the coefficients of the underlying linear model are returned.</source>
          <target state="translated">如果为True,则返回底层线性模型的系数。</target>
        </trans-unit>
        <trans-unit id="5724be8fac97575b0a1ba6783afc1c2fbe0fcac8" translate="yes" xml:space="preserve">
          <source>If True, the covariance of the joint predictive distribution at the query points is returned along with the mean</source>
          <target state="translated">如果为True,则返回查询点的联合预测分布的协方差和平均值。</target>
        </trans-unit>
        <trans-unit id="dbcfae5acdcda56eec77c1115971fac37cdadcbc" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric). The target is a pandas DataFrame or Series depending on the number of target columns. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then (&lt;code&gt;data&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt;) will be pandas DataFrames or Series as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e36608088c739a911a649d964516a72573ccb05" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target columns. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then (&lt;code&gt;data&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt;) will be pandas DataFrames or Series as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b39e4230ac891358bd0af7c3eedd9e07bde06b43" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="132ae12ba4f9038c17051f7e128fd887c18e17fe" translate="yes" xml:space="preserve">
          <source>If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns. The Bunch will contain a &lt;code&gt;frame&lt;/code&gt; attribute with the target and the data. If &lt;code&gt;return_X_y&lt;/code&gt; is True, then &lt;code&gt;(data, target)&lt;/code&gt; will be pandas DataFrames or Series as describe above.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab63854b4cbc92de5ee21a5ee4815065271902d4" translate="yes" xml:space="preserve">
          <source>If True, the distances and indices will be sorted before being returned. If False, the results will not be sorted. If return_distance == False, setting sort_results = True will result in an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91db79e9bb9248c604fdbf115bede6d2bc2e0f43" translate="yes" xml:space="preserve">
          <source>If True, the distances and indices will be sorted before being returned. If False, the results will not be sorted. Only used with mode=&amp;rsquo;distance&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d28765388f526f443e8440713d9f120592d23759" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.</source>
          <target state="translated">如果为True,则额外返回theta位置上相对于核超参数的对数边际似然的梯度。如果为True,则theta不能为None。</target>
        </trans-unit>
        <trans-unit id="2ecaf6f4ca3e741011335b25b38b91313c972ea3" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.</source>
          <target state="translated">如果为True,则额外返回在位置theta处相对于内核超参数的对数边际似然的梯度。注意,梯度计算不支持非二进制分类。如果为True,theta不能为None。</target>
        </trans-unit>
        <trans-unit id="65ddb79c8d6a76beebeeb976d10455d7eb1fb46d" translate="yes" xml:space="preserve">
          <source>If True, the imputer mask will be a sparse matrix.</source>
          <target state="translated">如果为True,imputer mask将是一个稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="eb2cac4cae6a6c8ef92320b8c850d0341c9b187b" translate="yes" xml:space="preserve">
          <source>If True, the kernel attribute is copied. If False, the kernel attribute is modified, but may result in a performance improvement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a134a28236267fd097c12ab6f4f7b85d957aa9ca" translate="yes" xml:space="preserve">
          <source>If True, the method also returns &lt;code&gt;n_iter&lt;/code&gt;, the actual number of iteration performed by the solver.</source>
          <target state="translated">如果为True，则该方法还返回 &lt;code&gt;n_iter&lt;/code&gt; ，即求解器执行的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="af50e40087f45610f2fbcc323b88ccd93dcf9324" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">如果为True，则在回归之前将对回归变量X进行归一化。当 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False 时，将忽略此参数。在对回归值进行归一化时，请注意，这会使学习到的超参数更健壮，并且几乎与样本数无关。相同的属性对于标准化数据无效。但是，如果您希望标准化，请在使用 &lt;code&gt;normalize=False&lt;/code&gt; 的估算器上调用 &lt;code&gt;fit&lt;/code&gt; 之前，使用 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="016e8fefc2c3108a2b7a4351de86dada7ecbd68e" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">如果为True，则在回归之前将对回归变量X进行归一化。当 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False 时，将忽略此参数。在对回归值进行归一化时，请注意，这会使学习到的超参数更健壮，并且几乎与样本数无关。相同的属性对于标准化数据无效。但是，如果您希望标准化，请在使用 &lt;code&gt;normalize=False&lt;/code&gt; 的估算器上调用 &lt;code&gt;fit&lt;/code&gt; 之前，使用 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c3fdb7e36f654834d666d698b8cb7219451a4481" translate="yes" xml:space="preserve">
          <source>If True, the return value will be an array of integers, rather than a boolean mask.</source>
          <target state="translated">如果为True,返回值将是一个整数数组,而不是一个布尔掩码。</target>
        </trans-unit>
        <trans-unit id="d17d4bbcdf2df4ef33c01a7114516c2e4e90d7d8" translate="yes" xml:space="preserve">
          <source>If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.</source>
          <target state="translated">如果为真,则返回查询点的预测分布的标准差和平均值。</target>
        </trans-unit>
        <trans-unit id="3f294130716f356d91654999eee9757bd2ba6c1c" translate="yes" xml:space="preserve">
          <source>If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">如果True,计算鲁棒位置和协方差估计的支持度,并从中重新计算协方差估计,而不将数据居中。对于平均数显著等于零但不完全为零的数据很有用。如果False,则直接用FastMCD算法计算鲁棒位置和协方差,而不做额外处理。</target>
        </trans-unit>
        <trans-unit id="667a36b1a1b94aa0d760aa610f277fcd1918ae0a" translate="yes" xml:space="preserve">
          <source>If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">如果为True,则计算稳健位置和协方差估计的支持度,并从中重新计算协方差估计,而不将数据居中。对于处理均值显著等于零但不完全为零的数据很有用。如果False,则直接用FastMCD算法计算鲁棒位置和协方差,而不做额外处理。</target>
        </trans-unit>
        <trans-unit id="77487b230b7ba030d96d8037319b1c202fdf89dc" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting each step will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c5c205f98c6f7e0686c6b1dad61d3840345a50a" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting each transformer will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af3eeb64eec0b2d56ce55e625f484e757480a63a" translate="yes" xml:space="preserve">
          <source>If True, the time elapsed while fitting will be printed as it is completed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28346b69fb6f1a64d688b2ef42aabb524b6563d8" translate="yes" xml:space="preserve">
          <source>If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised.</source>
          <target state="translated">如果为True,那么X将被转换为一个二维的NumPy数组或稀疏矩阵。如果转换不可能,则会引发异常。</target>
        </trans-unit>
        <trans-unit id="5c0fef9c1e6748fc4d3cb84e62459147a039a4fd" translate="yes" xml:space="preserve">
          <source>If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be &amp;lt; n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.</source>
          <target state="translated">如果为True，则将删除所有具有零特征值的分量，以使输出中的分量数可能小于n_components（有时由于数值不稳定性甚至为零）。当n_components为None时，将忽略此参数，并删除特征值为零的组件。</target>
        </trans-unit>
        <trans-unit id="42874c4e7adb064c98a0fb44835161462f985220" translate="yes" xml:space="preserve">
          <source>If True, then compute normalized Laplacian.</source>
          <target state="translated">如果为True,则计算归一化的拉普拉斯系数。</target>
        </trans-unit>
        <trans-unit id="ba532aca0424d37b34ea4248f58728a030b07d2d" translate="yes" xml:space="preserve">
          <source>If True, then return the centers of each cluster</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7818bd11ce52f2496690f2a19701e1fdc3bace9d" translate="yes" xml:space="preserve">
          <source>If True, transpose the downloaded data array.</source>
          <target state="translated">如果为True,则对下载的数据阵列进行转置。</target>
        </trans-unit>
        <trans-unit id="9b28aadc7e361758f4f56436eeab711f856e9105" translate="yes" xml:space="preserve">
          <source>If True, use a breadth-first search. If False (default) use a depth-first search. Breadth-first is generally faster for compact kernels and/or high tolerances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06e972e2019f64a904722b2934f5b7cf1e54e236" translate="yes" xml:space="preserve">
          <source>If True, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09f8cdcb36703e0413a45867eb062d48bc42e4a4" translate="yes" xml:space="preserve">
          <source>If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error. Global default: False.</source>
          <target state="translated">如果为真,将跳过有限性验证,节省时间,但会导致潜在的崩溃。如果为False,则会执行有限性验证,避免错误。全局默认。False.</target>
        </trans-unit>
        <trans-unit id="f770d204acb3934762188e63b6bd0977cfe619aa" translate="yes" xml:space="preserve">
          <source>If True, will return the parameters for this estimator and contained subobjects that are estimators.</source>
          <target state="translated">如果为True,将返回这个估计器和包含的子对象的估计器的参数。</target>
        </trans-unit>
        <trans-unit id="edc518974aa9f8ea115d0f36469bc2b1e2cd15a2" translate="yes" xml:space="preserve">
          <source>If True, will return the query_id array for each file.</source>
          <target state="translated">如果为True,将返回每个文件的query_id数组。</target>
        </trans-unit>
        <trans-unit id="80fdba1026cc980884a84dfa72ad1747c034afc6" translate="yes" xml:space="preserve">
          <source>If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.</source>
          <target state="translated">如果X和y不是C-ordered且连续的np.float64数组,并且X不是scipy.sparse.csr_matrix,那么X和/或y可以被复制。</target>
        </trans-unit>
        <trans-unit id="a601440183ce5a872479c357e441563196aab652" translate="yes" xml:space="preserve">
          <source>If X is a dense array, then the other methods will not support sparse matrices as input.</source>
          <target state="translated">如果X是一个密集数组,那么其他方法将不支持稀疏矩阵作为输入。</target>
        </trans-unit>
        <trans-unit id="efab9063b47a2afb85758f067069188b21b34f3e" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix.</source>
          <target state="translated">如果X被编码为CSR矩阵。</target>
        </trans-unit>
        <trans-unit id="f4e2537cdb42d2bfe76e858b065192a8758650ef" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da96e9fabf18fc39905756390120e4a7a1e09f97" translate="yes" xml:space="preserve">
          <source>If X is not a C-ordered contiguous array it is copied.</source>
          <target state="translated">如果X不是一个C顺序的连续数组,就会被复制。</target>
        </trans-unit>
        <trans-unit id="9cc8f34afbd30e04cc65d91f1b233abc1c382996" translate="yes" xml:space="preserve">
          <source>If X is not an array of floating values;</source>
          <target state="translated">如果X不是一个浮动值数组。</target>
        </trans-unit>
        <trans-unit id="e7ca7ce1d419c3d60265041304210343f2e8b91d" translate="yes" xml:space="preserve">
          <source>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that &lt;em&gt;X = L C&lt;/em&gt;. Different criteria exist to choose the components</source>
          <target state="translated">如果X是我们的多元数据，那么我们试图解决的问题是在不同的观察基础上重写它：我们想学习载荷L和一组分量C，使得&lt;em&gt;X = LC&lt;/em&gt;。存在选择组件的不同标准</target>
        </trans-unit>
        <trans-unit id="4691b6eeb6f44a63af5f24ac30dc066ab023aa61" translate="yes" xml:space="preserve">
          <source>If X is sparse and &lt;code&gt;missing_values=0&lt;/code&gt;;</source>
          <target state="translated">如果X稀疏并且 &lt;code&gt;missing_values=0&lt;/code&gt; ;</target>
        </trans-unit>
        <trans-unit id="5bf131136f9283115170d3f032466f07678834a5" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.</source>
          <target state="translated">如果给定Y(默认为None),那么返回的矩阵是X和Y两个数组之间的对距离。</target>
        </trans-unit>
        <trans-unit id="a6e7fe3e345be28d5e67984fa49ad01aeaa444dd" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.</source>
          <target state="translated">如果给定Y(默认为None),那么返回的矩阵是X和Y两个数组之间的对核。</target>
        </trans-unit>
        <trans-unit id="15e0fd61e3b85d8ebad2fc2135f6d5822723ce41" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;, the square of the standard deviation, then the explained variance is estimated as follow:</source>
          <target state="translated">如果\（\ hat {y} \）是估计的目标输出，\（y \）是相应的（正确）目标输出，并且\（Var \）是&lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;，即标准偏差的平方，则解释的方差为估计如下：</target>
        </trans-unit>
        <trans-unit id="1d8ad99bdd9bde4f8f4c5eb30616979db9d7983c" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value for total \(n\) samples, the estimated R&amp;sup2; is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8fb389b8a60a2b57fc22c9161412c484a73dd18" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:</source>
          <target state="translated">如果/(hat{y}_i/)是第i个样本的预测值,而/(y_i/)是相应的真实值,那么0-1损失/(L_{0-1}/)定义为:。</target>
        </trans-unit>
        <trans-unit id="01fda7f04ce93ad5d6b5843c80c53ee91e04866d" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第1个样本的预测值,而(y_i)是相应的真实值,那么正确预测的分数对(n_text{样本})的定义为:1.</target>
        </trans-unit>
        <trans-unit id="af46aeec0a0c654990b43c84ec26ca8a3817bbd3" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{text{samples}})估计的绝对误差中位数(MedAE)定义为</target>
        </trans-unit>
        <trans-unit id="27ab05c62dcfc0b1ca98228a2c106c2bd25d72f6" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the score R&amp;sup2; estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果\（\ hat {y} _i \）是第\（i \）个样本的预测值，而\（y_i \）是相应的真实值，则在\（n _ {\ text {样品}} \）定义为</target>
        </trans-unit>
        <trans-unit id="2057e5fd21ea6e3999b964ff2858a1023496793a" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the max error is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b2139578cc75e1715d428f9c389fc66abbdc391" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean Tweedie deviance error (D) for power \(p\), estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9be139031431f33624d9549cf24272bbec27cad" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第1个样本的预测值,而(y_i)是相应的真实值,那么对(n_{文本{样本})估计的平均绝对误差(MAE)被定义为</target>
        </trans-unit>
        <trans-unit id="b7836e2114e345225a74c22cbe0d3b5d52c8f253" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{样本})估计的均方误差(MSE)定义如下</target>
        </trans-unit>
        <trans-unit id="1cf47fc0a0aaaffd1c0a818b8704218b8ae722c1" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{text{samples}})估计的均值平方对数误差(MSLE)定义如下</target>
        </trans-unit>
        <trans-unit id="f295fa8851d145ac326bc63b92809e2fbf3d1f72" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_j\) is the predicted value for the \(j\)-th label of a given sample, \(y_j\) is the corresponding true value, and \(n_\text{labels}\) is the number of classes or labels, then the Hamming loss \(L_{Hamming}\) between two samples is defined as:</source>
          <target state="translated">如果\(\hat{y}_j/)是给定样本的第1个标签的预测值,\(y_j/)是相应的真实值,\(n_text{labels}/)是类或标签的数量,那么两个样本之间的汉明损失(L_{Hamming}/)定义为。</target>
        </trans-unit>
        <trans-unit id="7fa4bf510f83c73a55e8dec038abaacf960351ca" translate="yes" xml:space="preserve">
          <source>If \(c_0 = 0\) the kernel is said to be homogeneous.</source>
          <target state="translated">如果(c_0=0),则说内核是同质的。</target>
        </trans-unit>
        <trans-unit id="c7d07701826b4f4c8efa455b46a49990993930f2" translate="yes" xml:space="preserve">
          <source>If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:</source>
          <target state="translated">如果给定(h_i\),上述等式自动意味着以下的概率解释。</target>
        </trans-unit>
        <trans-unit id="9d9af8bc9b90a6fbf0d539b126efbd694bdaddf6" translate="yes" xml:space="preserve">
          <source>If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:</source>
          <target state="translated">如果(y_i\)是第i个样本的真实值,而(w_i\)是对应的样本权重,那么我们将样本权重调整为。</target>
        </trans-unit>
        <trans-unit id="4c76ddad0ef7a743f202685a0861880cbc2062e2" translate="yes" xml:space="preserve">
          <source>If \(y_w\) is the predicted decision for true label and \(y_t\) is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</source>
          <target state="translated">如果(y_w\)是真标签的预测决策,而(y_t\)是所有其他标签的预测决策的最大值,其中预测决策是由决策函数输出的,那么多类铰链损失的定义为:。</target>
        </trans-unit>
        <trans-unit id="b8371056060d53aef38082b274a12c6e92dd981b" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">如果提供了一个CSR、CSC、COO或BSR稀疏矩阵并被accept_sparse接受,那么accept_large_sparse将导致只有当它的索引以32位dtype存储时才会被接受。</target>
        </trans-unit>
        <trans-unit id="ca2f554a4272574081b19f205bd8db66223aa9f8" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse=False will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">如果提供了一个CSR、CSC、COO或BSR稀疏矩阵并被accept_sparse接受,accept_large_sparse=False将导致只有当它的索引以32位dtype存储时才会被接受。</target>
        </trans-unit>
        <trans-unit id="b0090a224443cfea422c2167734e98ec705e71a5" translate="yes" xml:space="preserve">
          <source>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</source>
          <target state="translated">如果传递了一个callable,它被用来从原始的、未处理的输入中提取特征序列。</target>
        </trans-unit>
        <trans-unit id="dcd8eacb988e0fa27afa1a7692943530b07747f6" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, k and and a random state and return an initialization.</source>
          <target state="translated">如果传递一个callable,它应该接受参数X、k和一个随机状态,并返回一个初始化。</target>
        </trans-unit>
        <trans-unit id="cf28b181c12fdc4001f26b46f656bc18426882b0" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15b5ddf5d35e7b6d7436896e31247316b726ba30" translate="yes" xml:space="preserve">
          <source>If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If &lt;code&gt;None&lt;/code&gt;, nothing is adjusted.</source>
          <target state="translated">如果是浮点型，则将该值添加到列联矩阵中的所有值。这有助于阻止NaN传播。如果为 &lt;code&gt;None&lt;/code&gt; ，则不进行任何调整。</target>
        </trans-unit>
        <trans-unit id="aec58020de2b924f9656034ee47c95a7ace302db" translate="yes" xml:space="preserve">
          <source>If a list is passed it&amp;rsquo;s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">如果传递了一个列表，则它应该是n_targets这样的数组之一。沿路径的系数的变化值。如果 &lt;code&gt;fit_path&lt;/code&gt; 参数为 &lt;code&gt;False&lt;/code&gt; 则不存在。</target>
        </trans-unit>
        <trans-unit id="8e3b6cd9422926a607fefd39c3e9bd3020c06d14" translate="yes" xml:space="preserve">
          <source>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if &lt;code&gt;analyzer == 'word'&lt;/code&gt;.</source>
          <target state="translated">如果是列表，则假定该列表包含停用词，所有停用词将从结果标记中删除。仅在 &lt;code&gt;analyzer == 'word'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3a891d1ec4d966c2171dc7ddf73d952ce675676b" translate="yes" xml:space="preserve">
          <source>If a single axis is passed in, it is treated as a bounding axes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58d1b02436a7aa9160e580f582400827e1ad046d" translate="yes" xml:space="preserve">
          <source>If a string, it is passed to _check_stop_list and the appropriate stop list is returned. &amp;lsquo;english&amp;rsquo; is currently the only supported string value. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">如果是字符串，则将其传递到_check_stop_list并返回适当的停止列表。&amp;ldquo; english&amp;rdquo;是当前唯一受支持的字符串值。&amp;ldquo;英语&amp;rdquo;存在几个已知的问题，您应该考虑一种替代方法（请参阅&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;使用停用词&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="564b43bc82acf22a1de3b85bb28ac591ff97b4bf" translate="yes" xml:space="preserve">
          <source>If a string, this may be one of &amp;lsquo;nearest_neighbors&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo; or one of the kernels supported by &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt;.</source>
          <target state="translated">如果是字符串，则可以是'nearest_neighbors'，'precomputed'，'rbf'之一或 &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt; 支持的内核之一。</target>
        </trans-unit>
        <trans-unit id="675cbfc234c52633edd36cba3388cd72c1b8e2d3" translate="yes" xml:space="preserve">
          <source>If a target is a classification outcome taking on values 0,1,&amp;hellip;,K-1, for node \(m\), representing a region \(R_m\) with \(N_m\) observations, let</source>
          <target state="translated">如果目标是分类结点，对节点\（m \）取值0,1，&amp;hellip;，K-1，表示具有\（N_m \）个观测值的区域\（R_m \），则令</target>
        </trans-unit>
        <trans-unit id="b39b95bbd18e310b00daaab2152e27da5d834f6a" translate="yes" xml:space="preserve">
          <source>If add_indicator=True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="373500a68bbbd934744d157d24ba37240f790a20" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="translated">如果亲和力是&amp;ldquo;预先计算的&amp;rdquo; X：类似数组的形状（n_samples，n_samples），则将X解释为根据样本计算得出的预先计算的邻接图。</target>
        </trans-unit>
        <trans-unit id="86d4d97c781f9d1a441e0d9197ea013d3820b489" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : {array-like, sparse matrix}, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c306493486d7abaed871db69a1b0f3a0d3e230f4" translate="yes" xml:space="preserve">
          <source>If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.</source>
          <target state="translated">如果亲和力是一个图形的邻接矩阵,那么这个方法可以用来寻找归一化图形切割。</target>
        </trans-unit>
        <trans-unit id="fef3ba1186af33eef8e244a6a4bb530d43ffdf84" translate="yes" xml:space="preserve">
          <source>If all examples are from the same class, it uses a one-class SVM.</source>
          <target state="translated">如果所有的例子都是来自同一个类,它使用的是一个类的SVM。</target>
        </trans-unit>
        <trans-unit id="b53805960d76925767243aca9c19243fc1b08d06" translate="yes" xml:space="preserve">
          <source>If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.</source>
          <target state="translated">如果所有的参数都以列表的形式呈现,则进行无替换抽样。如果至少有一个参数是以分布形式给出的,则使用替换抽样。强烈建议对连续参数使用连续分布。</target>
        </trans-unit>
        <trans-unit id="44ee4928f6faf842a93c2259e4d1b6db16c4073a" translate="yes" xml:space="preserve">
          <source>If all the coordinates are missing or if there are no common present coordinates then NaN is returned for that pair.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f219d1953670922fbbfe88159427df7222c9397" translate="yes" xml:space="preserve">
          <source>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</source>
          <target state="translated">如果一个算法,如线性支持向量机或PCA,只依赖于数据点的标量乘积(x_i/),那么我们可以使用(k(x_i,x_j)\)的值,这相当于将算法应用于映射的数据点(\phi(x_i)\)。使用/(k/)的好处是映射/(\phi/)永远不需要明确计算,允许任意大的特征(甚至是无限的)。</target>
        </trans-unit>
        <trans-unit id="22b4c02685073d2c5c3e900f3a57456658da8b22" translate="yes" xml:space="preserve">
          <source>If an array-like of axes are passed in, the partial dependence</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fea95f95d0577b3d2b8dde1c99a4f5f11240b1d" translate="yes" xml:space="preserve">
          <source>If an exception is triggered, use &lt;code&gt;%debug&lt;/code&gt; to fire-up a post mortem ipdb session.</source>
          <target state="translated">如果触发了异常，请使用 &lt;code&gt;%debug&lt;/code&gt; 启动事后ipdb会话。</target>
        </trans-unit>
        <trans-unit id="5c4a826b768bf0ea44b0de0cf9a279c59410da1d" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.</source>
          <target state="translated">如果给定一个整数,它固定了要使用的alpha网格上的点数。如果给定一个列表,它给出了要使用的网格。更多细节请参见类 docstring 中的注释。</target>
        </trans-unit>
        <trans-unit id="9e0d496b83e5a4c02138b5662acc0d0357377648" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details. Range is (0, inf] when floats given.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f54c86c80b29ba93fdb4405121f2a742f42412e" translate="yes" xml:space="preserve">
          <source>If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</source>
          <target state="translated">如果传递一个ndarray,它应该是形状(n_clusters,n_features)并给出初始中心。</target>
        </trans-unit>
        <trans-unit id="ce1d1dc15735f50591b21078e51cada592338767" translate="yes" xml:space="preserve">
          <source>If arpack :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57a89ce6b3eb175b37e97fdc8660396b5ef203e5" translate="yes" xml:space="preserve">
          <source>If auto :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bf20b6ab9e24e0313be1f29e5bd87350c62fc72" translate="yes" xml:space="preserve">
          <source>If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently.</source>
          <target state="translated">如果没有给定带宽,则使用基于所有对偶距离中值的启发式方法来确定。这将耗费样本数的二次方时间。可以使用sklearn.cluster.eestimate_bandwidth函数来更有效地完成这一工作。</target>
        </trans-unit>
        <trans-unit id="307d133eac653119e68c3f800803dcc4776573b9" translate="yes" xml:space="preserve">
          <source>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If &amp;lsquo;auto&amp;rsquo;, it is assigned to False for dense &lt;code&gt;X&lt;/code&gt; and to True for sparse &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">如果是布尔值，则确定是考虑所有特征是离散特征还是连续特征。如果是数组，则它应该是具有形状（n_features）的布尔蒙版，或者是具有离散特征索引的数组。如果为'auto'，则对于密集 &lt;code&gt;X&lt;/code&gt; ，将其分配为False；对于稀疏 &lt;code&gt;X&lt;/code&gt; ，将其分配为True 。</target>
        </trans-unit>
        <trans-unit id="14d26c2cb6ccf4f84a440b5eee94360749d30490" translate="yes" xml:space="preserve">
          <source>If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.</source>
          <target state="translated">如果是布尔值,是否拟合y递增或递减的同调回归。</target>
        </trans-unit>
        <trans-unit id="34ddc69f78e9ae21f32c4048eb992eb5ea37253a" translate="yes" xml:space="preserve">
          <source>If bootstrap is True, the number of samples to draw from X to train each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7380002883f78b7443a4cf144369e2cdf9c7dd5" translate="yes" xml:space="preserve">
          <source>If bytes or files are given to analyze, this encoding is used to decode.</source>
          <target state="translated">如果给定字节或文件进行分析,则使用此编码进行解码。</target>
        </trans-unit>
        <trans-unit id="b1cd46fc8b5b18d3d8ec258c92a5832fe49ecb69" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,则赋值完全不完整,因此AMI为空。</target>
        </trans-unit>
        <trans-unit id="e6f2dbc2c288fdff952bc5d6d0612fad044f50c2" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,则赋值完全不完整,因此NMI为空。</target>
        </trans-unit>
        <trans-unit id="60b93fba5d2befe30dad173ef2989a32caf2707d" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,那么分配是完全不完整的,因此ARI很低。</target>
        </trans-unit>
        <trans-unit id="e02bb35b2a969fdf2ff25b865a0b3ba938fb92a9" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的簇中,那么分配是完全不完整的,因此V-Measure是空的。</target>
        </trans-unit>
        <trans-unit id="d0974a75f074fb11fd0a08f495fdb803227dd0c6" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null:</source>
          <target state="translated">如果班级成员在不同的簇中完全分裂,那么分配是完全随机的,因此FMI为空。</target>
        </trans-unit>
        <trans-unit id="3a4c44f6cadbe5141305e79bc3f8068062652c4d" translate="yes" xml:space="preserve">
          <source>If classes members are split across different clusters, the assignment cannot be complete:</source>
          <target state="translated">如果班级成员分散在不同的群组中,则无法完成任务。</target>
        </trans-unit>
        <trans-unit id="a2787535a9b0772f98dac284faca0d2184e54d74" translate="yes" xml:space="preserve">
          <source>If coefficients vary significantly when changing the input dataset their robustness is not guaranteed, and they should probably be interpreted with caution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52c8ac55b5c80763168c6804bd2b4b595db63ef9" translate="yes" xml:space="preserve">
          <source>If computed_score is True, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baf96abe9df5cd386826eafcd47454c9dcc36819" translate="yes" xml:space="preserve">
          <source>If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency</source>
          <target state="translated">如果copy为False,则算法会在原地修改亲和矩阵,以提高内存效率</target>
        </trans-unit>
        <trans-unit id="0070f2c19699b732c372ba0e363293b507463ed1" translate="yes" xml:space="preserve">
          <source>If decision_function_shape=&amp;rsquo;ovo&amp;rsquo;, the function values are proportional to the distance of the samples X to the separating hyperplane. If the exact distances are required, divide the function values by the norm of the weight vector (&lt;code&gt;coef_&lt;/code&gt;). See also &lt;a href=&quot;https://stats.stackexchange.com/questions/14876/interpreting-distance-from-hyperplane-in-svm&quot;&gt;this question&lt;/a&gt; for further details. If decision_function_shape=&amp;rsquo;ovr&amp;rsquo;, the decision function is a monotonic transformation of ovo decision function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671e4e16873255449d2ba54f06975c272f73c34d" translate="yes" xml:space="preserve">
          <source>If density = &amp;lsquo;auto&amp;rsquo;, the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).</source>
          <target state="translated">如果density ='auto'，则按Ping Li等人的建议将值设置为最小密度：1 / sqrt（n_features）。</target>
        </trans-unit>
        <trans-unit id="391517cb3cfce3ac9c6c32b7ceac049807282afc" translate="yes" xml:space="preserve">
          <source>If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass &lt;code&gt;analyzer=str.split&lt;/code&gt;</source>
          <target state="translated">如果文档是由外部软件包预先加标记的，则将它们存储在文件（或字符串）中，其标记由空格分隔，并通过 &lt;code&gt;analyzer=str.split&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="80416b24b5f24b22b80d50d90aa942e77a2dadfc" translate="yes" xml:space="preserve">
          <source>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</source>
          <target state="translated">如果每行每列正好属于一个双簇,那么重新排列数据矩阵的行和列,就会发现对角线上的双簇。下面是这种结构的一个例子,双簇的平均值高于其他行和列。</target>
        </trans-unit>
        <trans-unit id="78ba0badd104b3ed95bd2061747613a1b6f84ce9" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of groups to include in the test split (rounded up). If int, represents the absolute number of test groups. If None, the value is set to the complement of the train size. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db9c10662b6d49b6b84ca8b7b7ce2a9580afa10" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the parameter is unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下（未指定参数），该值设置为0.1。默认值将在0.21版中更改。这将保持0.1只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="73ce93bb920d84bef49715afdf02f771938f8206" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.1。默认值将在0.21版中更改。这将保持0.1只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aa74ab3ce21513c4e7c83e9b91dad63582c835b8" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.2。默认值将在0.21版中更改。这将保持0.2只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dfe0bc4ba0825c6b9e54b3177711e714d6e28a2b" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.25。默认值将在0.21版中更改。它将保持0.25只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="09a40f5654bce0b9dd81c58e4db01bd35e373391" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If &lt;code&gt;train_size&lt;/code&gt; is also None, it will be set to 0.1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c86f8c8782626d870a975f8a368ba48b84e1e80" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If &lt;code&gt;train_size&lt;/code&gt; is also None, it will be set to 0.25.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3b7da8f21403a8e0fce55a369b8d82b4da5bfd1" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">如果是float,应该在0.0和1.0之间,代表训练分割中要包含的数据集的比例。如果是int,代表训练样本的绝对数量。如果为None,则该值自动设置为测试大小的补数。</target>
        </trans-unit>
        <trans-unit id="62b47f7a89d7c976d2813299381cdc4f4f5b3cad" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">如果是float,应该在0.0和1.0之间,代表列车分割中要包含的组数比例。如果是int,代表训练组的绝对数量。如果为None,则该值自动设置为测试大小的补数。</target>
        </trans-unit>
        <trans-unit id="561d4c125db4b741d015190537a6b19d8f8e55c1" translate="yes" xml:space="preserve">
          <source>If float, the contamination should be in the range [0, 0.5].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0ffbda1c59db43809cf9245f33efa45a65a5c05" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;max_features&lt;/code&gt; is a fraction and &lt;code&gt;int(max_features * n_features)&lt;/code&gt; features are considered at each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;max_features&lt;/code&gt; 是一个分数，并且在每个分割处均考虑 &lt;code&gt;int(max_features * n_features)&lt;/code&gt; 特征。</target>
        </trans-unit>
        <trans-unit id="47d0c1ebc9dc44a7018a0ce88d0d45201068f385" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; are the minimum number of samples for each node.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_leaf&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 是每个节点的最小样本数。</target>
        </trans-unit>
        <trans-unit id="c217707834c84f95b745c6fd735e46ef1d5cb29d" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; is the minimum number of samples for each node.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_leaf&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 是每个节点的最小样本数。</target>
        </trans-unit>
        <trans-unit id="f5813e9c1656f619ed6234eb86815e1c76ec9071" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; are the minimum number of samples for each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_split&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 是每个拆分的最小样本数。</target>
        </trans-unit>
        <trans-unit id="c32957ea85344bbb6b41aa874b4a5e7763ff6b76" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; is the minimum number of samples for each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_split&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 是每个拆分的最小样本数。</target>
        </trans-unit>
        <trans-unit id="81fe24c94c96599e85080c0cc195542bdb1ce722" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; features.</source>
          <target state="translated">如果为float，则绘制 &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; 特征。</target>
        </trans-unit>
        <trans-unit id="a2b1676fcae8577852e20614ce418906e2f79102" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples.</source>
          <target state="translated">如果为float，则绘制 &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; 样本。</target>
        </trans-unit>
        <trans-unit id="271d97216c612b23c04f108b3da94d4db7dcfcec" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples. Thus, &lt;code&gt;max_samples&lt;/code&gt; should be in the interval &lt;code&gt;(0, 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cf469ccb5c1129883a42e4f4a3183401e643c51" translate="yes" xml:space="preserve">
          <source>If full :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f234188a9694365b66e83538b40de1fa4059a074" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration.</source>
          <target state="translated">如果大于或等于1，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的（整数）个特征。如果在（0.0，1.0）之内，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的要素的百分比（向下舍入）。</target>
        </trans-unit>
        <trans-unit id="d3aeb6a39c457b69bdde12a943780461d08c388d" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than &lt;code&gt;step&lt;/code&gt; features in order to reach &lt;code&gt;min_features_to_select&lt;/code&gt;.</source>
          <target state="translated">如果大于或等于1，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的（整数）个特征。如果在（0.0，1.0）之内，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的要素的百分比（向下舍入）。请注意，为了达到 &lt;code&gt;min_features_to_select&lt;/code&gt; ，最后一次迭代可能会删除少于 &lt;code&gt;step&lt;/code&gt; 要素。</target>
        </trans-unit>
        <trans-unit id="1351e34960b08f78869e356e9d9129a529a17168" translate="yes" xml:space="preserve">
          <source>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果在QDA模型中假设协方差矩阵是对角线，则假定输入在每个类别中是有条件独立的，并且所得分类器等效于Gaussian Naive Bayes分类器&lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="47884bf7f490577d7025ceb970813cb997acfc82" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution.</source>
          <target state="translated">如果init ='custom'，则用作解决方案的初始猜测。</target>
        </trans-unit>
        <trans-unit id="25a1f9fc4e56498f0b6e355b29cc8aeb5e3daeb3" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b9c788e52ff7adb618d2b8cf3dd396e088800c8" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points equally divided among clusters. If array-like, each element of the sequence indicates the number of samples per cluster.</source>
          <target state="translated">如果是int,则是各簇平分的总点数。如果是类数组,序列的每个元素表示每个簇的样本数。</target>
        </trans-unit>
        <trans-unit id="424c096f1eaf5892378c66d2235ced4e356f327c" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points generated. For odd numbers, the inner circle will have one point more than the outer circle. If two-element tuple, number of points in outer circle and inner circle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2403d0bd3d2ac8c8a9756c4cde9cd9202cbe9926" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="676c2454734bf9216c5797d643595f0b850b4e7a" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Note that different initializations might result in different local minima of the cost function.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。注意，不同的初始化可能导致成本函数的局部最小值。</target>
        </trans-unit>
        <trans-unit id="a1933900181bd8e24f0237ebecc7a06b2ef8b486" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Only used when &lt;code&gt;svd_method&lt;/code&gt; equals &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。仅在 &lt;code&gt;svd_method&lt;/code&gt; 等于&amp;ldquo;随机化&amp;rdquo;时使用。</target>
        </trans-unit>
        <trans-unit id="4914440c8828885c0b1efea7679daefd5f1e4eaf" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;eigen_solver&lt;/code&gt; =='arpack'时使用。</target>
        </trans-unit>
        <trans-unit id="90657492e3c46d11fb3a1c799a4569e4854211ed" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; == True.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;shuffle&lt;/code&gt; == True时使用。</target>
        </trans-unit>
        <trans-unit id="7e39c8ed74a38762200c178da772671eaa6b3f52" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; is True.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;shuffle&lt;/code&gt; 为True时使用。</target>
        </trans-unit>
        <trans-unit id="636d95a8f3edcd08bb7a122de05f8944c1a330ee" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;solver&lt;/code&gt; =='arpack'时使用。</target>
        </trans-unit>
        <trans-unit id="d8bb54edf7a318cb4f3734b3f7721b6c840db8b1" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;svd_solver&lt;/code&gt; =='arpack'或'randomized'时使用。</target>
        </trans-unit>
        <trans-unit id="49fc99a0f8ec79ab5cf6633c8b91ad397447b0ae" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.</source>
          <target state="translated">如果是int,random_state是随机数发生器使用的种子;如果是RandomState实例,random_state是随机数发生器;如果是None,随机数发生器是np.random使用的RandomState实例。注意,这是由子采样和平滑噪声使用的。</target>
        </trans-unit>
        <trans-unit id="20ecb73824a9c787c46458c94b754eb343f23997" translate="yes" xml:space="preserve">
          <source>If int, the total number of points generated. If two-element tuple, number of points in each of two moons.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8faba5e55a8f0e899120109354364cac8c2354b" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;max_features&lt;/code&gt; features at each split.</source>
          <target state="translated">如果为int，则在每个拆分中考虑 &lt;code&gt;max_features&lt;/code&gt; 功能。</target>
        </trans-unit>
        <trans-unit id="79438cfe8b8de1684467307814da6af61cdfe6ba" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_leaf&lt;/code&gt; as the minimum number.</source>
          <target state="translated">如果为int，则将 &lt;code&gt;min_samples_leaf&lt;/code&gt; 视为最小值。</target>
        </trans-unit>
        <trans-unit id="69e04ca78560d3ef445be4d724f5c0cc8198187a" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_split&lt;/code&gt; as the minimum number.</source>
          <target state="translated">如果为int，则将 &lt;code&gt;min_samples_split&lt;/code&gt; 视为最小值。</target>
        </trans-unit>
        <trans-unit id="a8d276c242fbe315ce14903af35e7ebf9a0c3619" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_features&lt;/code&gt; features.</source>
          <target state="translated">如果为int，则绘制 &lt;code&gt;max_features&lt;/code&gt; 功能。</target>
        </trans-unit>
        <trans-unit id="0771ca4ef29dd427aac0ffda56943aa541e3af54" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_samples&lt;/code&gt; samples.</source>
          <target state="translated">如果为int，则绘制 &lt;code&gt;max_samples&lt;/code&gt; 样本。</target>
        </trans-unit>
        <trans-unit id="4430c154e22158c0d6435f75a2d640312ee73ab2" translate="yes" xml:space="preserve">
          <source>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the &amp;ldquo;first&amp;rdquo; singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.</source>
          <target state="translated">如果使用对数归一化，则所有奇异向量都是有意义的。但是，如果使用独立归一化或双稳态，则第一个奇异矢量\（u_1 \）和\（v_1 \）。被丢弃。从现在开始，除对数归一化的情况外，&amp;ldquo;第一个&amp;rdquo;奇异向量是指\（u_2 \ dots u_ {p + 1} \）和\（v_2 \ dots v_ {p + 1} \）。</target>
        </trans-unit>
        <trans-unit id="f38434d38fce86523bd80aac7625c65019a5f868" translate="yes" xml:space="preserve">
          <source>If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).</source>
          <target state="translated">如果max_samples大于所提供的样本数,则所有的样本将用于所有的树(无采样)。</target>
        </trans-unit>
        <trans-unit id="0512919782ceb898f5e82137605d37f1918f7fe8" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;auto&amp;rdquo;, the ratio of n_samples / n_population is used to determine which algorithm to use: If ratio is between 0 and 0.01, tracking selection is used. If ratio is between 0.01 and 0.99, numpy.random.permutation is used. If ratio is greater than 0.99, reservoir sampling is used. The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">如果method ==&amp;ldquo; auto&amp;rdquo;，则使用n_samples / n_population的比率来确定要使用的算法：如果比率在0到0.01之间，则使用跟踪选择。如果比率在0.01到0.99之间，则使用numpy.random.permutation。如果比率大于0.99，则使用储层采样。所选整数的顺序不确定。如果需要随机顺序，则应将选定的子集混洗。</target>
        </trans-unit>
        <trans-unit id="aa3ae5990e2e00fef99c00cc07049cdbc9d8e931" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;pool&amp;rdquo;, a pool based algorithm is particularly fast, even faster than the tracking selection method. Hovewer, a vector containing the entire population has to be initialized. If n_samples ~ n_population, the reservoir sampling method is faster.</source>
          <target state="translated">如果method ==&amp;ldquo; pool&amp;rdquo;，则基于池的算法特别快，甚至比跟踪选择方法快。但是，必须初始化包含整个种群的向量。如果n_samples〜n_population，则储层采样方法更快。</target>
        </trans-unit>
        <trans-unit id="6916dcd6d6c8f00e1ac0ef865ab4c75ff38ebedf" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;reservoir_sampling&amp;rdquo;, a reservoir sampling algorithm is used which is suitable for high memory constraint or when O(&lt;code&gt;n_samples&lt;/code&gt;) ~ O(&lt;code&gt;n_population&lt;/code&gt;). The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">如果方法==&amp;ldquo; reservoir_sampling&amp;rdquo;，则使用适合于高内存约束或当O（ &lt;code&gt;n_samples&lt;/code&gt; ） &lt;code&gt;n_population&lt;/code&gt; （n_population）时使用的储层采样算法。所选整数的顺序不确定。如果需要随机顺序，则应将选定的子集混洗。</target>
        </trans-unit>
        <trans-unit id="0dea8a6c91cef90e0430014d895bb3954c8fb19c" translate="yes" xml:space="preserve">
          <source>If method ==&amp;rdquo;tracking_selection&amp;rdquo;, a set based implementation is used which is suitable for &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt;.</source>
          <target state="translated">如果方法==&amp;ldquo; tracking_selection&amp;rdquo;，则使用基于集合的实现，适用于 &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b83a3c2dac1d5c17a6e580230ebdf136c940fa15" translate="yes" xml:space="preserve">
          <source>If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79ae0bba548597b9902c41c70fbd6bf9602e8534" translate="yes" xml:space="preserve">
          <source>If metric is &amp;lsquo;precomputed&amp;rsquo;, Y is ignored and X is returned.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则忽略Y并返回X。</target>
        </trans-unit>
        <trans-unit id="ca8cb47e72e166fd730a116349e050254e6876d5" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string.</source>
          <target state="translated">如果metric是可调用的函数，则会在每对实例（行）上调用它，并记录结果值。可调用对象应将两个数组作为输入并返回一个值，指示它们之间的距离。这适用于Scipy的指标，但效率不如将指标名称作为字符串传递。</target>
        </trans-unit>
        <trans-unit id="574d42008b369aefc553aab20dba12b6d233789b" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27305db22802f1bb3d3e2d60db076f6f0d275369" translate="yes" xml:space="preserve">
          <source>If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.</source>
          <target state="translated">如果使用mini-batch k-means,则选择最佳初始化,算法运行一次。否则,每次初始化都要运行算法,并选择最佳解。</target>
        </trans-unit>
        <trans-unit id="16e6684b95c26e373af21b6c0d2ea50b705c0505" translate="yes" xml:space="preserve">
          <source>If multioutput is &amp;lsquo;raw_values&amp;rsquo;, then mean absolute error is returned for each output separately. If multioutput is &amp;lsquo;uniform_average&amp;rsquo; or an ndarray of weights, then the weighted average of all output errors is returned.</source>
          <target state="translated">如果多输出为&amp;ldquo; raw_values&amp;rdquo;，则分别为每个输出返回平均绝对错误。如果multioutput是'uniform_average'或权重的ndarray，则返回所有输出错误的加权平均值。</target>
        </trans-unit>
        <trans-unit id="5304821b08fca43ab85cd7593997416e8f601261" translate="yes" xml:space="preserve">
          <source>If neighbors_algorithm=&amp;rsquo;precomputed&amp;rsquo;, X is assumed to be a distance matrix or a sparse graph of shape (n_queries, n_samples_fit).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62bc17472717b60146346d5aed7f5c0278bbe8ae" translate="yes" xml:space="preserve">
          <source>If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caf3d42023b133b9efdfbb493fc66df503092e71" translate="yes" xml:space="preserve">
          <source>If no scoring is specified and the estimator has no score function, we can either return None or raise an exception.</source>
          <target state="translated">如果没有指定评分,并且估计器没有评分函数,我们可以返回None或者引发异常。</target>
        </trans-unit>
        <trans-unit id="5519c1c6825bf59bfd06c08f68bb64b64ee1a0ae" translate="yes" xml:space="preserve">
          <source>If no valid consensus set could be found. This occurs if &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; return False for all &lt;code&gt;max_trials&lt;/code&gt; randomly chosen sub-samples.</source>
          <target state="translated">如果找不到有效的共识集。如果 &lt;code&gt;is_data_valid&lt;/code&gt; 和 &lt;code&gt;is_model_valid&lt;/code&gt; 对所有 &lt;code&gt;max_trials&lt;/code&gt; 随机选择的子样本返回False，则会发生这种情况。</target>
        </trans-unit>
        <trans-unit id="e48a960f323664c14eed43108cfafa1159796090" translate="yes" xml:space="preserve">
          <source>If normalize is &lt;code&gt;True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.</source>
          <target state="translated">如果normalize为 &lt;code&gt;True&lt;/code&gt; ，则返回错误分类的分数（浮点数），否则返回错误分类的数目（整数）。最佳性能为0。</target>
        </trans-unit>
        <trans-unit id="66ebd47239b72bc82239183dacc1b3e58bfa41bb" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-2&quot; id=&quot;id1&quot;&gt;[2]&lt;/a&gt; over the range [0, max_fpr] is returned. For the multiclass case, &lt;code&gt;max_fpr&lt;/code&gt;, should be either equal to &lt;code&gt;None&lt;/code&gt; or &lt;code&gt;1.0&lt;/code&gt; as AUC ROC partial computation currently is not supported for multiclass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d93355ca0c397a92c0eb63483bbae0b531d00cf1" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; over the range [0, max_fpr] is returned.</source>
          <target state="translated">如果不为 &lt;code&gt;None&lt;/code&gt; ，则返回范围为[0，max_fpr] 的标准化部分AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="954d968337062d6fae676f5915fb0dc48db9ccef" translate="yes" xml:space="preserve">
          <source>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</source>
          <target state="translated">如果不是None,建立一个只考虑整个语料库中按词频排序的top max_features的词汇。</target>
        </trans-unit>
        <trans-unit id="6b6dff5f6d294c2bdbfe5ee6b0ee56319193880c" translate="yes" xml:space="preserve">
          <source>If not None, data is split in a stratified fashion, using this as the class labels.</source>
          <target state="translated">如果不是None,则以这个作为类标签,将数据进行分层分割。</target>
        </trans-unit>
        <trans-unit id="d9fe4271c08ca870db7143f08e0938aa49f2d1d0" translate="yes" xml:space="preserve">
          <source>If not None, set the highest value of the fit to y_max.</source>
          <target state="translated">如果不是None,将拟合度的最高值设置为y_max。</target>
        </trans-unit>
        <trans-unit id="3c138b5d1ed12eddb3226ed7535814059b7a615c" translate="yes" xml:space="preserve">
          <source>If not None, set the lowest value of the fit to y_min.</source>
          <target state="translated">如果不是None,将拟合的最低值设置为y_min。</target>
        </trans-unit>
        <trans-unit id="ebcf44116da09ed76a723aed5cadbe6d4ed2530d" translate="yes" xml:space="preserve">
          <source>If not None, this argument is passed as &lt;code&gt;sample_weight&lt;/code&gt; keyword argument to the &lt;code&gt;score&lt;/code&gt; method of the final estimator.</source>
          <target state="translated">如果不是None，则将此参数作为 &lt;code&gt;sample_weight&lt;/code&gt; 关键字参数传递给最终估算器的 &lt;code&gt;score&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="3798f9f768af1129609b1d811ed41c3721cfae7d" translate="yes" xml:space="preserve">
          <source>If not None, this function is called after every iteration of the optimizer, taking as arguments the current solution (flattened transformation matrix) and the number of iterations. This might be useful in case one wants to examine or store the transformation found after each iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f7d0b7263b16cf926e8314af8096b9ae6c9066" translate="yes" xml:space="preserve">
          <source>If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).</source>
          <target state="translated">如果没有给定,则使用sklearn.cluster.eestimate_bandwidth估计带宽;关于可扩展性的提示,请参见该函数的文档(也请参见下面的注释)。</target>
        </trans-unit>
        <trans-unit id="e77fe01e6cae9364473d1714c8219315178ecad2" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fc57ade66d3b29b2e5dacfcee394aac7f4ec951" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;. .. versionadded:: 0.18</source>
          <target state="translated">如果未提供，将从y_true推断标签。如果 &lt;code&gt;labels&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; 且 &lt;code&gt;y_pred&lt;/code&gt; 具有形状（n_samples），则假定标签为二进制，并从 &lt;code&gt;y_true&lt;/code&gt; 推论得出。..版本添加：： 0.18</target>
        </trans-unit>
        <trans-unit id="d3a1f4e96f04c6f8dfd4835d50de53587903b2e7" translate="yes" xml:space="preserve">
          <source>If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones.</source>
          <target state="translated">如果对分类特征应用one-of-K编码,这将包括构建的特征名,但不包括原始特征名。</target>
        </trans-unit>
        <trans-unit id="c3b8faf61102e14148418b48bf3dbb3389d54ef3" translate="yes" xml:space="preserve">
          <source>If only the diagonal of the auto-covariance is being used, the method &lt;code&gt;diag()&lt;/code&gt; of a kernel can be called, which is more computationally efficient than the equivalent call to &lt;code&gt;__call__&lt;/code&gt;: &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</source>
          <target state="translated">如果仅使用自协方差的对角线，则可以调用内核的 &lt;code&gt;diag()&lt;/code&gt; 方法，其计算效率比对 &lt;code&gt;__call__&lt;/code&gt; 的等效调用高： &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="db7f35e5fc1dd73c10c86bdccb4a2449d5a89ec7" translate="yes" xml:space="preserve">
          <source>If order is &amp;lsquo;random&amp;rsquo; a random ordering will be used.</source>
          <target state="translated">如果订单是&amp;ldquo;随机的&amp;rdquo;，将使用随机排序。</target>
        </trans-unit>
        <trans-unit id="f42275492b00fc14b5861ea85e0f4944992d0324" translate="yes" xml:space="preserve">
          <source>If passed, include the name of the estimator in warning messages.</source>
          <target state="translated">如果通过,在警告信息中包含估计器的名称。</target>
        </trans-unit>
        <trans-unit id="908cd551a5eab6201799122746b2ad3d99f4a3d2" translate="yes" xml:space="preserve">
          <source>If positive, restrict regression coefficients to be positive</source>
          <target state="translated">如果为正,则限制回归系数为正。</target>
        </trans-unit>
        <trans-unit id="66637d66644751acb1ce04342cdfce0be0ef5495" translate="yes" xml:space="preserve">
          <source>If provided, this parameter will override the choice of copy_X made at instance creation. If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71021bee801a334b18f6587cd74d122911551ceb" translate="yes" xml:space="preserve">
          <source>If query_id is set to True, this will return instead [X1, y1, q1,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54d5c9aca2dfc54fbbbdb98327375a6f374bdf8f" translate="yes" xml:space="preserve">
          <source>If randomized :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5484f943f94af044829e2453a87ea1beff675d6" translate="yes" xml:space="preserve">
          <source>If return_costs is True, the objective function and dual gap at each iteration are returned.</source>
          <target state="translated">如果return_costs为True,则返回目标函数和每次迭代时的双倍差距。</target>
        </trans-unit>
        <trans-unit id="3a07f641c209d2556442bcd652915ffb4ab857db" translate="yes" xml:space="preserve">
          <source>If safe is false, clone will fall back to a deep copy on objects that are not estimators.</source>
          <target state="translated">如果safe为false,clone会在不是估计器的对象上回落到深度复制。</target>
        </trans-unit>
        <trans-unit id="179d83839b7c246b21dd4fad6260ec3c338cc783" translate="yes" xml:space="preserve">
          <source>If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError.</source>
          <target state="translated">如果seed是None,返回np.random使用的RandomState单体。如果seed是int,返回一个新的RandomState实例。如果 seed 已经是一个 RandomState 实例,则返回它。否则会引发ValueError。</target>
        </trans-unit>
        <trans-unit id="7108bbb3c9ecad70c2ad038e49ece7ce906a1c8f" translate="yes" xml:space="preserve">
          <source>If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be &amp;lt; len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.</source>
          <target state="translated">如果seq [i]是一个int或具有一个int值的元组，则创建一个单向PDP；否则，将创建一个单向PDP。如果seq [i]是两个整数的元组，则会创建一个双向PDP。如果指定了feature_names并且seq [i]是一个int，则seq [i]必须小于len（feature_names）。如果seq [i]是字符串，则必须指定feature_names，并且seq [i]必须在feature_names中。</target>
        </trans-unit>
        <trans-unit id="4c1ce6df0b81e7680bbf5092a9535a5fa0cb37a8" translate="yes" xml:space="preserve">
          <source>If set to &amp;ldquo;warn&amp;rdquo;, this acts as 0, but warnings are also raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b01ea458e6ed79ec076f21dd79564eecc3f7a882" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4</source>
          <target state="translated">如果设置为&amp;ldquo;随机&amp;rdquo;，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为&amp;ldquo;随机&amp;rdquo;）通常会导致收敛更快，尤其是当tol高于1e-4时</target>
        </trans-unit>
        <trans-unit id="7a4374896942a67a58d05d59133607fc7483d7a2" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4.</source>
          <target state="translated">如果设置为&amp;ldquo;随机&amp;rdquo;，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为&amp;ldquo;随机&amp;rdquo;）通常会导致收敛更快，尤其是当tol高于1e-4时。</target>
        </trans-unit>
        <trans-unit id="b3732982402454957d1d44e2700684220ba9b532" translate="yes" xml:space="preserve">
          <source>If set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to &lt;code&gt;fit&lt;/code&gt; as initialization for &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d15f7a009cd3b6e81a757534913f0edc7a2b7947" translate="yes" xml:space="preserve">
          <source>If set to True, forces coefficients to be positive. (Only allowed when &lt;code&gt;y.ndim == 1&lt;/code&gt;).</source>
          <target state="translated">如果设置为True，则强制系数为正。（仅当 &lt;code&gt;y.ndim == 1&lt;/code&gt; 时允许）。</target>
        </trans-unit>
        <trans-unit id="6bd31584b0a279bb6a357ab195ba9534cb5cad4e" translate="yes" xml:space="preserve">
          <source>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</source>
          <target state="translated">如果设置为True,则对所有折线的分数进行平均,并取对应于最佳分数的coefs和C,并使用这些参数进行最后的调整。否则,则取各折线中与最佳分数对应的coefs、intercepts和C的平均值。</target>
        </trans-unit>
        <trans-unit id="f3d43f9f7c9e3af1ca6eddc0268b8ee91bb07ee3" translate="yes" xml:space="preserve">
          <source>If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.</source>
          <target state="translated">如果设置了,scikit-learn会尝试将临时数组的大小限制在这个MiB的数量(并行化时的每个作业),通常会在可以分块执行的昂贵操作上节省计算时间和内存。全局默认:1024。</target>
        </trans-unit>
        <trans-unit id="cd9d66e1ab8be1fe689482ddb0cbca43b44a3950" translate="yes" xml:space="preserve">
          <source>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</source>
          <target state="translated">如果严格来说是正值,一旦文件中的位置达到(偏移量+长度)字节阈值,就停止读取任何新的数据行。</target>
        </trans-unit>
        <trans-unit id="af99c20b0f1015ebcecc8bfb6a50ca848ab08d15" translate="yes" xml:space="preserve">
          <source>If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.</source>
          <target state="translated">如果是字符串,指定包含数据的路径。如果是类文件,数据将被写入f,f应该以二进制模式打开。</target>
        </trans-unit>
        <trans-unit id="d25cbbfb18995acebbdc8e788e3994e16b21b8ae" translate="yes" xml:space="preserve">
          <source>If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.</source>
          <target state="translated">如果sum_over_features为False,则shape为(n_samples_X*n_samples_Y,n_features),D包含分量的L1对偶距离(即绝对差),否则shape为(n_samples_X,n_samples_Y),D包含L1对偶距离。</target>
        </trans-unit>
        <trans-unit id="1979731cc29c616c5ac5593ab888192599b2d46b" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;loss&lt;/code&gt; does not support probabilities.</source>
          <target state="translated">如果 &lt;code&gt;loss&lt;/code&gt; 不支持概率。</target>
        </trans-unit>
        <trans-unit id="41f96f9118cd39448d94e68aca8aa6d327b368ef" translate="yes" xml:space="preserve">
          <source>If the algorithm is &amp;ldquo;deflation&amp;rdquo;, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.</source>
          <target state="translated">如果算法是&amp;ldquo;放气&amp;rdquo;，则n_iter是所有组件之间运行的最大迭代次数。否则，它们只是收敛的迭代次数。</target>
        </trans-unit>
        <trans-unit id="b72ab6a8a780a6da86f798b7381c54dc2236b80c" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; of &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;means_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;means_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="translated">如果算法停止之前完全融合（因为 &lt;code&gt;tol&lt;/code&gt; 的 &lt;code&gt;max_iter&lt;/code&gt; ）， &lt;code&gt;labels_&lt;/code&gt; 和 &lt;code&gt;means_&lt;/code&gt; 不会是一致的，即 &lt;code&gt;means_&lt;/code&gt; 不会在每个集群点的手段。同样，估算器将在最后一次迭代后重新分配 &lt;code&gt;labels_&lt;/code&gt; ，以使 &lt;code&gt;labels_&lt;/code&gt; 与训练集上的 &lt;code&gt;predict&lt;/code&gt; 一致。</target>
        </trans-unit>
        <trans-unit id="12008b7aa6dad452411115d8236f3a855fd0fea9" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; or &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;cluster_centers_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;cluster_centers_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4043c787702cc081c41f25b031d69ea98cf35c42" translate="yes" xml:space="preserve">
          <source>If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.</source>
          <target state="translated">如果数组不是对称的,那么将返回一个对称的版本。如果矩阵不是对称的,可以选择发出警告或异常。</target>
        </trans-unit>
        <trans-unit id="d4238935d45ce51eea0be6c47146a609e05c26ed" translate="yes" xml:space="preserve">
          <source>If the attributes are not found.</source>
          <target state="translated">如果没有找到属性。</target>
        </trans-unit>
        <trans-unit id="10f86bc0a8ef8d94dd88200305e21d6ac290743f" translate="yes" xml:space="preserve">
          <source>If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).</source>
          <target state="translated">如果分类器在任何一个类上的表现都一样好,这个项就降为传统的准确率(即正确预测数除以总预测数)。</target>
        </trans-unit>
        <trans-unit id="9d0651dbf433477af9dfe8c9482b03c0b28a7aea" translate="yes" xml:space="preserve">
          <source>If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.</source>
          <target state="translated">如果数据排序不是任意的(例如,具有相同类标签的样本是连续的),先洗牌可能是获得有意义的交叉验证结果的关键。然而,如果样本不是独立和相同分布,则情况可能相反。例如,如果样本对应的是新闻文章,并按其发表时间排序,那么洗牌数据很可能会导致模型过拟合和验证分数夸大:它将在与训练样本人为相似(时间上接近)的样本上进行测试。</target>
        </trans-unit>
        <trans-unit id="4fdc5debb409dcec7a673c288152f0ef6e4738ef" translate="yes" xml:space="preserve">
          <source>If the default value is passed, then &lt;code&gt;keepdims&lt;/code&gt; will not be passed through to the &lt;code&gt;mean&lt;/code&gt; method of sub-classes of &lt;code&gt;ndarray&lt;/code&gt;, however any non-default value will be. If the sub-class&amp;rsquo; method does not implement &lt;code&gt;keepdims&lt;/code&gt; any exceptions will be raised.</source>
          <target state="translated">如果传递了默认值，则 &lt;code&gt;keepdims&lt;/code&gt; 不会传递给 &lt;code&gt;ndarray&lt;/code&gt; 的子类的 &lt;code&gt;mean&lt;/code&gt; 方法，但是任何非默认值都将传递。如果子类的方法未实现 &lt;code&gt;keepdims&lt;/code&gt; ,则将引发任何异常。</target>
        </trans-unit>
        <trans-unit id="322da3aec4cc8f30cb7592a5692203180007e581" translate="yes" xml:space="preserve">
          <source>If the degree is 2 or 3, the method described in &amp;ldquo;Leveraging Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices Using K-Simplex Numbers&amp;rdquo; by Andrew Nystrom and John Hughes is used, which is much faster than the method used on CSC input. For this reason, a CSC input will be converted to CSR, and the output will be converted back to CSC prior to being returned, hence the preference of CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca5777d1057fb92ff835301c12f93dc71bd51069" translate="yes" xml:space="preserve">
          <source>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</source>
          <target state="translated">如果当前预测与正确标签之间的差异低于这个阈值,则不更新模型。</target>
        </trans-unit>
        <trans-unit id="54f187a0c12dbeb2b22455f8308653334a568512" translate="yes" xml:space="preserve">
          <source>If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.</source>
          <target state="translated">如果估计器支持增量学习,这将用于加快不同训练集大小的拟合速度。</target>
        </trans-unit>
        <trans-unit id="28446974a089033b0f005a14dd2e5e7cde0d019d" translate="yes" xml:space="preserve">
          <source>If the file does not exist yet, it is downloaded from mldata.org .</source>
          <target state="translated">如果文件还不存在,可从mldata.org下载。</target>
        </trans-unit>
        <trans-unit id="3377386ec971b5f97505ad0b0efacd641307a6b2" translate="yes" xml:space="preserve">
          <source>If the folder does not already exist, it is automatically created.</source>
          <target state="translated">如果文件夹还不存在,则会自动创建。</target>
        </trans-unit>
        <trans-unit id="bcf86cd76452a354a39a384d6cc008f0521fadc0" translate="yes" xml:space="preserve">
          <source>If the gradient norm is below this threshold, the optimization will be stopped.</source>
          <target state="translated">如果梯度规范低于这个阈值,优化将被停止。</target>
        </trans-unit>
        <trans-unit id="aaea0ac91de1101ebb5583d72a39edadc546a9ed" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt;&lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt;&lt;/a&gt;) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:</source>
          <target state="translated">如果不知道地面真相标签，则必须使用模型本身进行评估。轮廓系数（&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt; &lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt; &lt;/a&gt;）是这种评估的一个示例，其中较高的轮廓系数得分与具有更好定义的群集的模型有关。为每个样本定义了轮廓系数，该系数由两个分数组成：</target>
        </trans-unit>
        <trans-unit id="f86f2c18ff62eced8e4c69ce5c4a60d3487f70e2" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabasz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabasz_score#sklearn.metrics.calinski_harabasz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabasz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb3f5944370bdcf362ff4bffb46e8bf1ded41ef1" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabaz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.</source>
          <target state="translated">如果不知道地面真相标签，则可以使用Calinski-Harabaz指数（&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt; &lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt; &lt;/a&gt;）（也称为方差比标准）来评估模型，其中较高的Calinski-Harabaz得分与具有以下特征的模型相关：定义更好的集群。</target>
        </trans-unit>
        <trans-unit id="a4a519d35f95c18e319df7e8878b98f013f9bd44" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Davies-Bouldin index (&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt;&lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt;&lt;/a&gt;) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.</source>
          <target state="translated">如果不知道地面真相标签，则可以使用Davies-Bouldin索引（&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt; &lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt; &lt;/a&gt;）评估模型，其中较低的Davies-Bouldin索引与具有更好群集分离度的模型相关。</target>
        </trans-unit>
        <trans-unit id="7ce6861d9ec6948a6bc8aef858e97abae7ed0654" translate="yes" xml:space="preserve">
          <source>If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.</source>
          <target state="translated">如果输入是一个稀疏矩阵,那么只有非零值才会被Binarizer类更新。</target>
        </trans-unit>
        <trans-unit id="c10a8d9d8c40f9f50dafe36b727f5b47e7075f19" translate="yes" xml:space="preserve">
          <source>If the input matrix X is very sparse, it is recommended to convert to sparse &lt;code&gt;csc_matrix&lt;/code&gt; before calling fit and sparse &lt;code&gt;csr_matrix&lt;/code&gt; before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.</source>
          <target state="translated">如果输入矩阵X是非常稀疏，建议转换为稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 调用配合和稀疏之前 &lt;code&gt;csr_matrix&lt;/code&gt; 调用预测之前。当特征在大多数样本中为零时，与稀疏矩阵输入相比，稀疏矩阵输入的训练时间可以快几个数量级。</target>
        </trans-unit>
        <trans-unit id="661cb29a3f5fe68fda8ea5f8c53273efb7753ce1" translate="yes" xml:space="preserve">
          <source>If the labels are encoded with +1 and -1, \(y\): is the true value, and \(w\) is the predicted decisions as output by &lt;code&gt;decision_function&lt;/code&gt;, then the hinge loss is defined as:</source>
          <target state="translated">如果标签用+1和-1编码，则\（y \）：是真实值，\（w \）是Decision_function输出的预测 &lt;code&gt;decision_function&lt;/code&gt; ，则铰链损耗定义为：</target>
        </trans-unit>
        <trans-unit id="30e7605353fedb22ff0c25c7418abbab28137e70" translate="yes" xml:space="preserve">
          <source>If the loss on a sample is greater than the &lt;code&gt;residual_threshold&lt;/code&gt;, then this sample is classified as an outlier.</source>
          <target state="translated">如果样本的损失大于 &lt;code&gt;residual_threshold&lt;/code&gt; 阈值，则将该样本分类为离群值。</target>
        </trans-unit>
        <trans-unit id="fd47c1f065810b1b45f0d8d994aa0a4ff29505d4" translate="yes" xml:space="preserve">
          <source>If the metric constructor parameter is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be the distance matrix between the data to be predicted and &lt;code&gt;self.centroids_&lt;/code&gt;.</source>
          <target state="translated">如果度量构造函数参数是&amp;ldquo;预先计算的&amp;rdquo;，则将X假定为要预测的数据与 &lt;code&gt;self.centroids_&lt;/code&gt; 之间的距离矩阵。</target>
        </trans-unit>
        <trans-unit id="c8476d320358236ab03a9b2e5775d6207658d4f7" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则X必须是平方距离矩阵。否则，它每行包含一个样本。</target>
        </trans-unit>
        <trans-unit id="ed2846275337b6c05f70ce353bc177c3fd2fc1b0" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则X必须是平方距离矩阵。否则，它每行包含一个样本。如果方法是&amp;ldquo;精确&amp;rdquo;，则X可以是类型为&amp;ldquo; csr&amp;rdquo;，&amp;ldquo; csc&amp;rdquo;或&amp;ldquo; coo&amp;rdquo;的稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="3a27e115b0c223dd4eebc69d8c1ee49334684c4e" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;. If the method is &amp;lsquo;barnes_hut&amp;rsquo; and the metric is &amp;lsquo;precomputed&amp;rsquo;, X may be a precomputed sparse graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be2b4ccc2ee21bcc622b72ad8a09e29905f86d22" translate="yes" xml:space="preserve">
          <source>If the number of features is \(p\), you now require \(n \sim 1/d^p\) points. Let&amp;rsquo;s say that we require 10 points in one dimension: now \(10^p\) points are required in \(p\) dimensions to pave the \([0, 1]\) space. As \(p\) becomes large, the number of training points required for a good estimator grows exponentially.</source>
          <target state="translated">如果要素数量为\（p \），则现在需要\（n \ sim 1 / d ^ p \）点。假设我们在一维中需要10个点：现在，在\（p \）维度中需要\（10 ^ p \）个点才能铺平\（[0，1] \）空间。随着\（p \）变大，一个好的估计量所需的训练点数将呈指数增长。</target>
        </trans-unit>
        <trans-unit id="31f6fadae8fdb5e25318685f0dd36c90a3234b90" translate="yes" xml:space="preserve">
          <source>If the number of features is much greater than the number of samples, avoid over-fitting in choosing &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; and regularization term is crucial.</source>
          <target state="translated">如果特征数量远大于样本数量，则在选择&lt;a href=&quot;#svm-kernels&quot;&gt;内核函数&lt;/a&gt;时应避免过度拟合，并且正则化项至关重要。</target>
        </trans-unit>
        <trans-unit id="421f2017551080d266c05ad587f0ba1ecff6bff8" translate="yes" xml:space="preserve">
          <source>If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.</source>
          <target state="translated">如果需要减少数据的实例数量,或者想要大量的子群作为预处理步骤或其他,Birch比MiniBatchKMeans更有用。</target>
        </trans-unit>
        <trans-unit id="0169ea68b458a3b315ac7acca32e943f8bdb1bed" translate="yes" xml:space="preserve">
          <source>If the option chosen is &amp;lsquo;ovr&amp;rsquo;, then a binary problem is fit for each label. For &amp;lsquo;multinomial&amp;rsquo; the loss minimised is the multinomial loss fit across the entire probability distribution, &lt;em&gt;even when the data is binary&lt;/em&gt;. &amp;lsquo;multinomial&amp;rsquo; is unavailable when solver=&amp;rsquo;liblinear&amp;rsquo;. &amp;lsquo;auto&amp;rsquo; selects &amp;lsquo;ovr&amp;rsquo; if the data is binary, or if solver=&amp;rsquo;liblinear&amp;rsquo;, and otherwise selects &amp;lsquo;multinomial&amp;rsquo;.</source>
          <target state="translated">如果选择的选项是&amp;ldquo; ovr&amp;rdquo;，则每个标签都适合二进制问题。对于&amp;ldquo;多项式&amp;rdquo;，&lt;em&gt;即使数据是二进制的&lt;/em&gt;，最小化的损失也就是整个概率分布中的多项式损失拟合。当solver ='liblinear'时，'multinomial'不可用。如果数据是二进制的，或者如果Solver ='liblinear'，则'auto'选择'ovr'，否则选择'multinomial'。</target>
        </trans-unit>
        <trans-unit id="e56253ed1e137739144617c8f91af1433e314b57" translate="yes" xml:space="preserve">
          <source>If the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ff404c7c7b751e78382edbf95c1647019d00b38" translate="yes" xml:space="preserve">
          <source>If the parameter&amp;rsquo;s type does not match the desired type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74c8a6dddf7307f96841c8e0079e93341eb05526" translate="yes" xml:space="preserve">
          <source>If the parameter&amp;rsquo;s value violates the given bounds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb7bde2a134f67333f646adb3cb422fdb3b0a619" translate="yes" xml:space="preserve">
          <source>If the prediction task is to classify the observations in a set of finite labels, in other words to &amp;ldquo;name&amp;rdquo; the objects observed, the task is said to be a &lt;strong&gt;classification&lt;/strong&gt; task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a &lt;strong&gt;regression&lt;/strong&gt; task.</source>
          <target state="translated">如果预测任务是将观察结果分类为一组有限标签，换句话说就是&amp;ldquo;命名&amp;rdquo;观察到的对象，则该任务称为&lt;strong&gt;分类&lt;/strong&gt;任务。另一方面，如果目标是预测连续目标变量，则称其为&lt;strong&gt;回归&lt;/strong&gt;任务。</target>
        </trans-unit>
        <trans-unit id="c305135e1b17987e86652a7910deafacf0f5dc9d" translate="yes" xml:space="preserve">
          <source>If the pyamg package is installed, it is used: this greatly speeds up computation.</source>
          <target state="translated">如果安装了pyamg包,就会使用它:这大大加快了计算速度。</target>
        </trans-unit>
        <trans-unit id="04f07265ff7d7b70145be5aec52784e1b24d6f09" translate="yes" xml:space="preserve">
          <source>If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.</source>
          <target state="translated">如果新样本与最近的子群合并得到的子群半径大于阈值的平方,且子群数量大于分支系数,则暂时给这个新样本分配一个空间。取最远的两个子簇,根据这些子簇之间的距离,将子簇分为两组。</target>
        </trans-unit>
        <trans-unit id="41eef1b8a501219131b2619b097a82294e78c28a" translate="yes" xml:space="preserve">
          <source>If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.</source>
          <target state="translated">如果对样本进行加权，则使用基于权重的预修剪标准（例如 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; )可以更轻松地优化树结构，该标准可确保叶节点至少包含样本权重总和的一小部分。</target>
        </trans-unit>
        <trans-unit id="0c1baaebbfab363e25ace0c6b6ee91539fab116b" translate="yes" xml:space="preserve">
          <source>If the selected solver is &amp;lsquo;L-BFGS&amp;rsquo;, training does not support online nor mini-batch learning.</source>
          <target state="translated">如果选择的求解器为&amp;ldquo; L-BFGS&amp;rdquo;，则培训不支持在线学习或小批量学习。</target>
        </trans-unit>
        <trans-unit id="6b79b273c5ccf0e85d810ff5a4ad56e9102a13be" translate="yes" xml:space="preserve">
          <source>If the target is a continuous value, then for node \(m\), representing a region \(R_m\) with \(N_m\) observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.</source>
          <target state="translated">如果目标是一个连续的值,那么对于节点 \(m\),代表一个区域 \(R_m\)与 \(N_m\)的观测值,在确定未来分割的位置时,常见的最小化标准是均值平方误差,它利用终端节点的平均值将L2误差最小化,以及均值绝对误差,它利用终端节点的中值将L1误差最小化。</target>
        </trans-unit>
        <trans-unit id="43d2fac91a90af78192c1e9cb5f608e633304262" translate="yes" xml:space="preserve">
          <source>If the target values \(y\) are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson deviance with log-link.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25b233fcf730d262746a1d2c5c4f20ca63e89053" translate="yes" xml:space="preserve">
          <source>If the target values are positive valued and skewed, you might try a Gamma deviance with log-link.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73b808b1b5912c4df231c6d6d8790651299cc5e0" translate="yes" xml:space="preserve">
          <source>If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian deviance (or even higher variance powers of the Tweedie family).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd360b0d17d9758c91116a373249c96c3c493365" translate="yes" xml:space="preserve">
          <source>If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as &lt;code&gt;latin-1&lt;/code&gt;. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.</source>
          <target state="translated">如果文本的编码杂乱无章，难以整理（这是20个新闻组数据集的情况），则可以使用简单的单字节编码，如 &lt;code&gt;latin-1&lt;/code&gt; 。某些文本可能显示不正确，但是至少相同的字节序列将始终表示相同的功能。</target>
        </trans-unit>
        <trans-unit id="21286b430a50cd4c4f50df67c996d4b5b475416f" translate="yes" xml:space="preserve">
          <source>If the text you are loading is not actually encoded with UTF-8, however, you will get a &lt;code&gt;UnicodeDecodeError&lt;/code&gt;. The vectorizers can be told to be silent about decoding errors by setting the &lt;code&gt;decode_error&lt;/code&gt; parameter to either &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; or &lt;code&gt;&quot;replace&quot;&lt;/code&gt;. See the documentation for the Python function &lt;code&gt;bytes.decode&lt;/code&gt; for more details (type &lt;code&gt;help(bytes.decode)&lt;/code&gt; at the Python prompt).</source>
          <target state="translated">但是，如果要加载的文本实际上未使用UTF-8编码，则将收到 &lt;code&gt;UnicodeDecodeError&lt;/code&gt; 。该vectorizers可以告诉保持沉默被设定解码错误 &lt;code&gt;decode_error&lt;/code&gt; 参数设置为 &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; 或 &lt;code&gt;&quot;replace&quot;&lt;/code&gt; 。有关更多详细信息，请参见Python函数 &lt;code&gt;bytes.decode&lt;/code&gt; 的文档 &lt;code&gt;help(bytes.decode)&lt;/code&gt; 在Python提示符下键入help（bytes.decode））。</target>
        </trans-unit>
        <trans-unit id="13a9f813159d9e6258a164870cb1dc6a301dddd5" translate="yes" xml:space="preserve">
          <source>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter \(\gamma\) of an SVM on the digits dataset.</source>
          <target state="translated">如果训练得分和验证得分都很低,估计器将是欠拟合的。如果训练得分高而验证得分低,则估计器过拟合,否则工作得很好。训练得分低,验证得分高,通常是不可能的。这三种情况都可以在下面的图中找到,我们在数字数据集上改变一个SVM的参数\(\gamma\)。</target>
        </trans-unit>
        <trans-unit id="80e789647361ff21671194a00300fe312dc530d2" translate="yes" xml:space="preserve">
          <source>If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored.</source>
          <target state="translated">如果转换后的输出包含稀疏和密集数据的混合，则如果密度低于此值，则它将作为稀疏矩阵堆叠。使用 &lt;code&gt;sparse_threshold=0&lt;/code&gt; 总是返回密集值。当转换后的输出包含所有稀疏数据或所有密集数据时，堆叠结果将分别为稀疏或密集数据，并且将忽略此关键字。</target>
        </trans-unit>
        <trans-unit id="efca83041c066057e65d83989c4190b74b69dba6" translate="yes" xml:space="preserve">
          <source>If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.</source>
          <target state="translated">如果底层图的节点的连接数比一般节点多得多,算法就会漏掉其中的一些连接。</target>
        </trans-unit>
        <trans-unit id="27f470c8c1d74aa01f92e63860f4d2b9149dd0bb" translate="yes" xml:space="preserve">
          <source>If there are few data points per dimension, noise in the observations induces high variance:</source>
          <target state="translated">如果每个维度的数据点很少,观测值中的噪声会诱发高方差。</target>
        </trans-unit>
        <trans-unit id="fbaec65eed1139944f6f906201326eaef7bc4d08" translate="yes" xml:space="preserve">
          <source>If there are more than two classes, \(f(x)\) itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,</source>
          <target state="translated">如果有两个以上的类,/(f(x)/)本身将是一个大小为(n_classes,)的向量。它不通过logistic函数,而是通过softmax函数,写成。</target>
        </trans-unit>
        <trans-unit id="e2c9b002eac60ce02e4a3cafb47196266855c43e" translate="yes" xml:space="preserve">
          <source>If there are more than two labels, &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; uses a multiclass variant due to Crammer &amp;amp; Singer. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;Here&lt;/a&gt; is the paper describing it.</source>
          <target state="translated">如果标签不止两个，&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;由于Crammer＆Singer的缘故，铰链损耗会使用多类变体。&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;这&lt;/a&gt;是描述它的论文。</target>
        </trans-unit>
        <trans-unit id="362b6e0ad023f937918b9ce5c294c8243f2c8ea1" translate="yes" xml:space="preserve">
          <source>If there is a possibility that the training data might have missing categorical features, it can often be better to specify &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; instead of setting the &lt;code&gt;categories&lt;/code&gt; manually as above. When &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (&lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is only supported for one-hot encoding):</source>
          <target state="translated">如果训练数据有可能缺少分类特征，则通常最好指定 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 而不是如上所述手动设置 &lt;code&gt;categories&lt;/code&gt; 。当指定 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 且在转换过程中遇到未知类别时，不会引发错误，但此功能生成的一键编码列将全为零（ &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 仅支持一键编码）：</target>
        </trans-unit>
        <trans-unit id="76c3aee0f8dcd7757eeddd2a91b271bc2108fb17" translate="yes" xml:space="preserve">
          <source>If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.</source>
          <target state="translated">如果有多个这样的值,只返回第一个。同时返回模态仓的仓数。</target>
        </trans-unit>
        <trans-unit id="e4599c6e53b844db2376ed9e56ed7b49e63c6ec3" translate="yes" xml:space="preserve">
          <source>If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.</source>
          <target state="translated">如果这是一个ints的元组,则在多个轴上执行平均数,而不是像以前那样在一个轴或所有轴上执行。</target>
        </trans-unit>
        <trans-unit id="015e500928e7c3f86f2c9b5121c746246fcd7a9f" translate="yes" xml:space="preserve">
          <source>If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</source>
          <target state="translated">如果设置为True,被缩小的轴将作为尺寸为1的尺寸留在结果中。使用这个选项,结果将正确地在输入数组中广播。</target>
        </trans-unit>
        <trans-unit id="1d47783a4de427039b4db643f305b3dd195e7ba2" translate="yes" xml:space="preserve">
          <source>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.</source>
          <target state="translated">如果这个被拆分的节点有一个父子簇,并且有空间容纳一个新的子簇,那么父子簇就被一分为二。如果没有空间,那么这个节点又被一分为二,这个过程继续递归,直到到达根节点。</target>
        </trans-unit>
        <trans-unit id="012f5a7e85e6e4a424dae20b5f0c103c4fa516ee" translate="yes" xml:space="preserve">
          <source>If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.</source>
          <target state="translated">如果为true(默认),使用广度优先的方法来解决这个问题。否则使用深度优先的方法。</target>
        </trans-unit>
        <trans-unit id="5f57fd46a52f081b17c9c11ebb2ef30582e94549" translate="yes" xml:space="preserve">
          <source>If true the classification weights will be exported on each leaf. The classification weights are the number of samples each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cec7c6586dbaf8eb8cda6633d2caddbd361cde5b" translate="yes" xml:space="preserve">
          <source>If true, &lt;code&gt;decision_function_shape='ovr'&lt;/code&gt;, and number of classes &amp;gt; 2, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt; will break ties according to the confidence values of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8002efc50268110232cc0ffbdad97c50440caadd" translate="yes" xml:space="preserve">
          <source>If true, X and y will be centered.</source>
          <target state="translated">如果为真,X和y将居中。</target>
        </trans-unit>
        <trans-unit id="de73b79cb6d725781d21d3fce59be150e3f40a4c" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None.</source>
          <target state="translated">如果为真,初始核的位置不是所有点的位置,而是点的离散化版本的位置,在这个版本中,点被分层到一个网格上,其粗度与带宽相对应。将这个选项设置为True会加快算法的速度,因为需要初始化的种子较少。如果 seeds 参数不是 None,则忽略。</target>
        </trans-unit>
        <trans-unit id="8e21ac3d3be6bbf299e61e387ddc28e4aa0e4b76" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. The default value is False. Ignored if seeds argument is not None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="382a329c7d03e3dc0838d8f846116bb309725e41" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None.</source>
          <target state="translated">如果为真,初始核位置不是所有点的位置,而是点的离散化版本的位置,点被分层到一个网格上,其粗度与带宽相对应。将此选项设置为True会加快算法的速度,因为会有更少的种子被初始化。 默认值。False 如果 seeds 参数不是 None,则忽略该选项。</target>
        </trans-unit>
        <trans-unit id="5f8f1503f4ad4447aabcfddb9ab2a5dcd7bfde79" translate="yes" xml:space="preserve">
          <source>If true, only interaction features are produced: features that are products of at most &lt;code&gt;degree&lt;/code&gt;&lt;em&gt;distinct&lt;/em&gt; input features (so not &lt;code&gt;x[1] ** 2&lt;/code&gt;, &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt;, etc.).</source>
          <target state="translated">如果为真，只有相互作用特征产生：即至多产品特征 &lt;code&gt;degree&lt;/code&gt; &lt;em&gt;不同&lt;/em&gt;的输入功能（因此不是 &lt;code&gt;x[1] ** 2&lt;/code&gt; ， &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt; 等）。</target>
        </trans-unit>
        <trans-unit id="2d6ce18ffe19be253728c95b7f8882b85215b418" translate="yes" xml:space="preserve">
          <source>If true, randomize the order of coordinates in the CD solver.</source>
          <target state="translated">如果为真,则随机化CD解算器中的坐标顺序。</target>
        </trans-unit>
        <trans-unit id="c9d7c7ecbdd08be425848806cc6f9d68c29d7323" translate="yes" xml:space="preserve">
          <source>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</source>
          <target state="translated">如果为真,返回每个样本的平均损失。否则,返回每个样本损失的总和。</target>
        </trans-unit>
        <trans-unit id="6d32e9cabd3e10e0279ad9dd2b7c71514057bf52" translate="yes" xml:space="preserve">
          <source>If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.</source>
          <target state="translated">如果为真,那么所有的点都会被聚类,甚至是那些不在任何核内的孤核。孤儿被分配给最近的核。如果为假,那么孤儿被赋予簇标签-1。</target>
        </trans-unit>
        <trans-unit id="87535a59e28d16a49b32c83a6882f2aefd67503d" translate="yes" xml:space="preserve">
          <source>If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="translated">如果为真,则使用双树算法,否则,使用单树算法。否则,使用单树算法。双树算法对于大的N来说,可以有更好的伸缩性。</target>
        </trans-unit>
        <trans-unit id="1f1b5d26ff8a3e05067cc01bd3cc7a0f425c7062" translate="yes" xml:space="preserve">
          <source>If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a62f22814f97612f53d5c62d8ea50a484acea3fc" translate="yes" xml:space="preserve">
          <source>If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="translated">如果两个变量与响应的相关性几乎相等,那么它们的系数应该以大致相同的速度增加。因此,该算法的表现与直觉预期的一样,也比较稳定。</target>
        </trans-unit>
        <trans-unit id="c6d813716240a58cb1e341ee096ed78ff4d17824" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are plotted at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时绘制目标函数和双倍差距。</target>
        </trans-unit>
        <trans-unit id="c845cf733c967b921d034159fbd6e6d551e8f5a1" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are printed at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时打印目标函数和双倍差距。</target>
        </trans-unit>
        <trans-unit id="d04bb65f95446e17ac813ad52d517cc52bee8bef" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and duality gap are printed at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时打印目标函数和二元性差距。</target>
        </trans-unit>
        <trans-unit id="c297e2c2dea63aea70529d05594e08d33ba95930" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果启用了热启动，则后验模式的拉普拉斯近似上的最后一次牛顿迭代的解将用作下一个_posterior_mode（）调用的初始化。在与超参数优化中类似的问题上多次调用_posterior_mode时，这可以加快收敛速度​​。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="250e3ab5ccbd8ff9f61320148d394ace048df253" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0beda1307d8fba8bf455ad043421d0e1b5743334" translate="yes" xml:space="preserve">
          <source>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</source>
          <target state="translated">如果我们认为损失函数是每个样本的单个误差,那么数据拟合项,或每个样本的误差总和,将随着我们增加更多的样本而增加。然而,惩罚项不会增加。</target>
        </trans-unit>
        <trans-unit id="232b0b83965c86185ba5cb4047fca7ca1eb2e5e8" translate="yes" xml:space="preserve">
          <source>If we define &lt;code&gt;s = 1 / density&lt;/code&gt;, the elements of the random matrix are drawn from</source>
          <target state="translated">如果我们定义 &lt;code&gt;s = 1 / density&lt;/code&gt; ，则随机矩阵的元素将从</target>
        </trans-unit>
        <trans-unit id="de1968292a81bf57ac7d922cf400e8863c649aea" translate="yes" xml:space="preserve">
          <source>If we increase &lt;code&gt;power&lt;/code&gt; to 1,:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1427e0ae27c6ada10257117ddf3e602836e812e6" translate="yes" xml:space="preserve">
          <source>If we note &lt;code&gt;s = 1 / density&lt;/code&gt; the components of the random matrix are drawn from:</source>
          <target state="translated">如果我们注意到 &lt;code&gt;s = 1 / density&lt;/code&gt; ，则随机矩阵的分量来自：</target>
        </trans-unit>
        <trans-unit id="f9cbc4d2964857d1865d4f91b696f12d3cce3cff" translate="yes" xml:space="preserve">
          <source>If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果我们注意到\（n _ {\ max} = \ max（n _ {\ mathrm {samples}}，n _ {\ mathrm {features}}）\）\）和\（n _ {\ min} = \ min（n _ {\ mathrm {samples}}，n _ {\ mathrm {features}}）\），则随机&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;的时间复杂度为\（O（n _ {\ max} ^ 2 \ cdot n _ {\ mathrm {components}}）\）代替\（O（n _ {\ max} ^ 2 \ cdot n _ {\ min}）\）的值，以实现在&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 中&lt;/a&gt;实现的确切方法。</target>
        </trans-unit>
        <trans-unit id="483854d9b52bcc6a7814b6c99e197398a7767c0e" translate="yes" xml:space="preserve">
          <source>If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot. As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal, but the off-diagonal structure is lost.</source>
          <target state="translated">如果我们使用l2收缩,就像Ledoit-Wolf估计器一样,由于样本数量很少,我们需要收缩很多。结果,Ledoit-Wolf的精度相当接近于地真精度,也就是离对角线不远,但失去了非对角线结构。</target>
        </trans-unit>
        <trans-unit id="c048d2d806d4fc664d0d49e2240da1f9038d7165" translate="yes" xml:space="preserve">
          <source>If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:</source>
          <target state="translated">如果我们想给数据拟合一个抛物线而不是一个平面,我们可以用二阶多项式来结合特征,这样的模型就像这样。</target>
        </trans-unit>
        <trans-unit id="9cd2bdd13889db844fd297c5019d226d5f2214ec" translate="yes" xml:space="preserve">
          <source>If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain &lt;code&gt;PPCA&lt;/code&gt;.</source>
          <target state="translated">如果我们进一步限制模型，则通过假设高斯噪声是各向同性的（所有对角线入口都相同），我们将获得 &lt;code&gt;PPCA&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="722d8ecf13f293f3aeb1f206f30063d8afe3b1aa" translate="yes" xml:space="preserve">
          <source>If whiten is false, the data is already considered to be whitened, and no whitening is performed.</source>
          <target state="translated">如果whiten为假,则已经认为数据已经被白化,不进行白化。</target>
        </trans-unit>
        <trans-unit id="7d7146f3cf5f6ee3a12daad9561636b3070c19dc" translate="yes" xml:space="preserve">
          <source>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</source>
          <target state="translated">如果启用了白化,inverse_transform将计算精确的反操作,其中包括反白化。</target>
        </trans-unit>
        <trans-unit id="d6701cdf426d6a22381b3dd206332eab0defeb4b" translate="yes" xml:space="preserve">
          <source>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant &lt;code&gt;c&lt;/code&gt; such that the average L2 norm of the training data equals one.</source>
          <target state="translated">如果将SGD应用于使用PCA提取的特征，我们发现通常明智的做法是将特征值按某个常数 &lt;code&gt;c&lt;/code&gt; 进行缩放，以使训练数据的平均L2范数等于1。</target>
        </trans-unit>
        <trans-unit id="88e7b5f8ea1b769958767cd4c4986cb0d84b69d4" translate="yes" xml:space="preserve">
          <source>If you are having trouble decoding text, here are some things to try:</source>
          <target state="translated">如果你在解码文本时遇到困难,这里有一些事情可以尝试。</target>
        </trans-unit>
        <trans-unit id="79c8dbf5a9dd8e0bcb21dbc0b3d21d4a002af1f5" translate="yes" xml:space="preserve">
          <source>If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:</source>
          <target state="translated">如果你对分别控制L1和L2的处罚感兴趣,请记住,这相当于。</target>
        </trans-unit>
        <trans-unit id="fda5569b1e927ca58d1243554ef8331577e43162" translate="yes" xml:space="preserve">
          <source>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.</source>
          <target state="translated">如果你没有提供一个先验词典,也没有使用分析器进行某种特征选择,那么特征的数量将等于分析数据发现的词汇量。</target>
        </trans-unit>
        <trans-unit id="44906a85511569286aafb87826155b3c2905ddf9" translate="yes" xml:space="preserve">
          <source>If you don&amp;rsquo;t have labels, try using &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;Clustering&lt;/a&gt; on your problem.</source>
          <target state="translated">如果没有标签，请尝试对问题使用&lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;群集&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="951c73020178956033a4088912e01e49fd5d4ad8" translate="yes" xml:space="preserve">
          <source>If you encounter a bug with &lt;code&gt;scikit-learn&lt;/code&gt; or something that needs clarification in the docstring or the online documentation, please feel free to ask on the &lt;a href=&quot;http://scikit-learn.org/stable/support.html&quot;&gt;Mailing List&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e3c9a76f4703e92f09f761bb01632ba0994c7fc" translate="yes" xml:space="preserve">
          <source>If you experience hanging subprocesses with &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; or &lt;code&gt;n_jobs=-1&lt;/code&gt;, make sure you have a single-threaded BLAS library, or set &lt;code&gt;n_jobs=1&lt;/code&gt;, or upgrade to Python 3.4 which has a new version of &lt;code&gt;multiprocessing&lt;/code&gt; that should be immune to this problem.</source>
          <target state="translated">如果您遇到 &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 或 &lt;code&gt;n_jobs=-1&lt;/code&gt; 的子进程挂起，请确保您具有单线程BLAS库，或者将 &lt;code&gt;n_jobs=1&lt;/code&gt; 设置，或者升级到Python 3.4，该版本应具有新版本的 &lt;code&gt;multiprocessing&lt;/code&gt; ，因此对此不受影响。问题。</target>
        </trans-unit>
        <trans-unit id="01bbb2c6066f4ee0c3cd8eec64a157d2ed58c8df" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(\phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(\phi\) followed by removal of the mean in that space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4d3f940390acd5b7c567c108aba27222ddceb7d" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(phi\) followed by removal of the mean in that space.</source>
          <target state="translated">如果您有一个内核\（K \）的内核矩阵，可以在函数\（phi \）定义的特征空间中计算点积，则&lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt; &lt;code&gt;KernelCenterer&lt;/code&gt; &lt;/a&gt;可以转换内核矩阵，以便它在定义的特征空间中包含内部乘积通过\（phi \），然后去除该空间中的均值。</target>
        </trans-unit>
        <trans-unit id="2615ef2dcc8005e7f8f1fe1a8b864f8c5c8b40ba" translate="yes" xml:space="preserve">
          <source>If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel:</source>
          <target state="translated">如果你有一个亲和矩阵,比如距离矩阵,对于这个矩阵,0代表相同的元素,高值代表非常不相似的元素,可以应用高斯(RBF,热)核将其转化为非常适合算法的相似度矩阵。</target>
        </trans-unit>
        <trans-unit id="fe35ae96a69c356c4c790a684b706493b567f769" translate="yes" xml:space="preserve">
          <source>If you have multiple labels per document, e.g categories, have a look at the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel section&lt;/a&gt;.</source>
          <target state="translated">如果每个文档有多个标签，例如类别，请查看&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel部分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4c5f38a361bcbafd12cc29508483a444dfcf9728" translate="yes" xml:space="preserve">
          <source>If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.</source>
          <target state="translated">如果你有好几个类要预测,经常使用的一个选项是拟合一个与所有分类器,然后使用投票启发式进行最终决策。</target>
        </trans-unit>
        <trans-unit id="f97615967054f5695c197161c38be5160cb380e9" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots you can use the &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt;&lt;code&gt;partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">如果您需要局部依赖函数的原始值而不是绘图，则可以使用&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt; &lt;code&gt;partial_dependence&lt;/code&gt; &lt;/a&gt;函数：</target>
        </trans-unit>
        <trans-unit id="89ce73b51d6a3d39879546007326b93ba776d729" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots, you can use the &lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt;&lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57ce2ca8cd47ab25ffa05da2880829913ab0ea8d" translate="yes" xml:space="preserve">
          <source>If you really want to use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator. In this case, &lt;code&gt;fit_predict&lt;/code&gt; is not available.</source>
          <target state="translated">如果您确实要使用&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;进行新颖性检测，即预测标签或计算新的看不见数据的异常分数，则可以在拟合估算器之前将 &lt;code&gt;novelty&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 来实例化估算器。在这种情况下， &lt;code&gt;fit_predict&lt;/code&gt; 不可用。</target>
        </trans-unit>
        <trans-unit id="d5eafa493c5ec5c70bb915830d883a4547e5cefd" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;a href=&quot;../classes#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f2c392b35ce2be9956cf3b6740e21a9cb0e7eb8" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;.</source>
          <target state="translated">如果设置load_content = True，则还应该使用'encoding'参数指定文本的编码。对于许多现代文本文件，&amp;ldquo; utf-8&amp;rdquo;将是正确的编码。如果保留等于None的编码，则内容将由字节而不是Unicode组成，并且您将无法使用 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 中的大多数功能。</target>
        </trans-unit>
        <trans-unit id="bc2de8e7176de3a7ec5f2ba66eebe7612bd92fb1" translate="yes" xml:space="preserve">
          <source>If you specify &lt;code&gt;max_depth=h&lt;/code&gt; then complete binary trees of depth &lt;code&gt;h&lt;/code&gt; will be grown. Such trees will have (at most) &lt;code&gt;2**h&lt;/code&gt; leaf nodes and &lt;code&gt;2**h - 1&lt;/code&gt; split nodes.</source>
          <target state="translated">如果指定 &lt;code&gt;max_depth=h&lt;/code&gt; ，则将增长深度为 &lt;code&gt;h&lt;/code&gt; 的完整二叉树。这样的树将（最多）具有 &lt;code&gt;2**h&lt;/code&gt; 叶节点和 &lt;code&gt;2**h - 1&lt;/code&gt; 分裂节点。</target>
        </trans-unit>
        <trans-unit id="b5e591b33a0bd24a7e9f3db1117e493d465fb600" translate="yes" xml:space="preserve">
          <source>If you use sparse data (i.e. data represented as sparse matrices), &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt; will deal with the data without making it dense.</source>
          <target state="translated">如果你使用稀疏数据（即表示为稀疏矩阵数据），&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;将处理数据，而使其密集。</target>
        </trans-unit>
        <trans-unit id="a1ac2d367786359c2f555cec450b280865094e6b" translate="yes" xml:space="preserve">
          <source>If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using &lt;code&gt;warm_start=True&lt;/code&gt; and &lt;code&gt;max_iter=1&lt;/code&gt; and iterating yourself can be helpful:</source>
          <target state="translated">如果您想更好地控制SGD中的停止条件或学习率，或者想进行其他监视，请使用 &lt;code&gt;warm_start=True&lt;/code&gt; 和 &lt;code&gt;max_iter=1&lt;/code&gt; 并自己进行迭代可能会有所帮助：</target>
        </trans-unit>
        <trans-unit id="5572f4380789e92c52811040d71f90082bdb6315" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="translated">如果您想更多地了解这些问题并探索其他可能的序列化方法，请参阅&lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;Alex Gaynor的演讲&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f3aeb01a6e584fc1ad67fbb7aa6de9be209b24eb" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;https://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73e4bcb37920038eaa4bbe1483d8a08f411face1" translate="yes" xml:space="preserve">
          <source>If you want to model a relative frequency, i.e. counts per exposure (time, volume, &amp;hellip;) you can do so by using a Poisson distribution and passing \(y=\frac{\mathrm{counts}}{\mathrm{exposure}}\) as target values together with \(\mathrm{exposure}\) as sample weights. For a concrete example see e.g. &lt;a href=&quot;../auto_examples/linear_model/plot_tweedie_regression_insurance_claims#sphx-glr-auto-examples-linear-model-plot-tweedie-regression-insurance-claims-py&quot;&gt;Tweedie regression on insurance claims&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9d28c176517636a408517ab464ebf5a8ed78a10" translate="yes" xml:space="preserve">
          <source>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</source>
          <target state="translated">如果你的属性有一个内在的尺度(如词频或指标特征),就不需要缩放。</target>
        </trans-unit>
        <trans-unit id="1c54fdc8274e03b04e776296dd28d5ff9fd81085" translate="yes" xml:space="preserve">
          <source>If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt;&lt;code&gt;RobustScaler&lt;/code&gt;&lt;/a&gt; as drop-in replacements instead. They use more robust estimates for the center and range of your data.</source>
          <target state="translated">如果您的数据包含许多离群值，则使用数据的均值和方差进行缩放可能效果不佳。在这些情况下，您可以改用&lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt; &lt;code&gt;RobustScaler&lt;/code&gt; &lt;/a&gt;作为替代产品。他们对数据的中心和范围使用更可靠的估计。</target>
        </trans-unit>
        <trans-unit id="a10bfba29a759d89e81956a541262290109cf294" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="translated">如果您的功能数量很多，则在进行有监督的步骤之前先通过无监督的步骤来减少功能可能会很有用。许多&lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;无监督学习&lt;/a&gt;方法都实现了可用于降低维度的 &lt;code&gt;transform&lt;/code&gt; 方法。下面，我们讨论大量使用此模式的两个特定示例。</target>
        </trans-unit>
        <trans-unit id="7fd7c263c0ce2fcb7f46b0e79b01f4b5e3878456" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;https://scikit-learn.org/0.23/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="933c257247173f2464c3cf8d4de4af2d678e5a7c" translate="yes" xml:space="preserve">
          <source>If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.</source>
          <target state="translated">如果你的观测数与底层图中的边数相比并不多,你将无法恢复。</target>
        </trans-unit>
        <trans-unit id="4f34c772816074a373c0d5e918e2e9ac136448b7" translate="yes" xml:space="preserve">
          <source>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</source>
          <target state="translated">通过向前寻求忽略偏移的第一个字节,然后丢弃下面的字节,直到下一个新的行字符。</target>
        </trans-unit>
        <trans-unit id="78fee1435d74666b84850cd5e82c18229351da5d" translate="yes" xml:space="preserve">
          <source>Ignored</source>
          <target state="translated">Ignored</target>
        </trans-unit>
        <trans-unit id="9b02e8c10d5a363337d6fcee177ec3e9cae9f1ce" translate="yes" xml:space="preserve">
          <source>Ignored in binary classification or classical regression settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce03a847a845250be1b2b174971c463802e32813" translate="yes" xml:space="preserve">
          <source>Ignored variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e65bb4eca2d3c71529c96890a4b735eb7dafeac" translate="yes" xml:space="preserve">
          <source>Ignored.</source>
          <target state="translated">Ignored.</target>
        </trans-unit>
        <trans-unit id="d0c592be2a6267cc2802c86a5116a8ba2d4b6ff9" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e417badfc4d52f79664b451110854e41b4a0daf" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.</source>
          <target state="translated">忽略。这个参数只存在于与sklearn.pipeline.Pipeline的兼容性。</target>
        </trans-unit>
        <trans-unit id="2d34b7c897f7b41a0f0625575a2c9cc21b1078a7" translate="yes" xml:space="preserve">
          <source>Illustration of &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 和 &lt;code&gt;GridSearchCV&lt;/code&gt; 的插图</target>
        </trans-unit>
        <trans-unit id="643998f34944846c305de7d49de1c3e80f814d2d" translate="yes" xml:space="preserve">
          <source>Illustration of Gaussian process classification (GPC) on the XOR dataset</source>
          <target state="translated">XOR数据集上的高斯过程分类(GPC)图解</target>
        </trans-unit>
        <trans-unit id="c2cd661f8089fd4df71dfb566ea137083aa22024" translate="yes" xml:space="preserve">
          <source>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.</source>
          <target state="translated">说明估计器在未见数据(测试数据)上的性能与在训练数据上的性能如何不同。随着正则化程度的提高,训练数据的性能会下降,而测试数据的性能在正则化参数的范围内是最佳的。带有弹性网络回归模型的例子,性能是用解释方差也就是R^2来衡量的。</target>
        </trans-unit>
        <trans-unit id="5790a5aaa3a6c4543a820b9b12ce6d261eeb0581" translate="yes" xml:space="preserve">
          <source>Illustration of prior and posterior Gaussian process for different kernels</source>
          <target state="translated">不同核的前高斯过程和后高斯过程的说明。</target>
        </trans-unit>
        <trans-unit id="71618836e7c136eb1b3d1aef884b22f68af959aa" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27c062ea4e410688effe23ac51313a8ab9a70f1c" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">说明了不同的正则化策略对梯度提升的影响。这个例子取自Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5beb1d257bbb65ddb7ec568445a1c3acdcb42d37" translate="yes" xml:space="preserve">
          <source>Image denoising using dictionary learning</source>
          <target state="translated">利用字典学习进行图像去噪</target>
        </trans-unit>
        <trans-unit id="5ab7decf36c80b04aff06a11c0e8ef068c85a1b9" translate="yes" xml:space="preserve">
          <source>Image histogram</source>
          <target state="translated">图像直方图</target>
        </trans-unit>
        <trans-unit id="2c151a57b190c9b9e70046810542a00e0344b5af" translate="yes" xml:space="preserve">
          <source>Image representing the confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c328038b14054033bab1147ef5d1ad234b3373d" translate="yes" xml:space="preserve">
          <source>Imagine you have three subjects, each with an associated number from 1 to 3:</source>
          <target state="translated">想象一下,你有三个科目,每个科目都有一个相关的数字,从1到3。</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="8d522809f4125f5930c1f4f77ec91f8735a003d8" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</source>
          <target state="translated">基于 &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1f57545a08e425cccaf152a77959fb187cad06cb" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;em&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6a1ab1256c83ccf4dbd316f43007b044bc691a2" translate="yes" xml:space="preserve">
          <source>Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights. Note that the binning stage (specifically the quantiles computation) does not take the weights into account.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6ac8df85fe47d2d00b4a78e1facdef4fbcae73b" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</source>
          <target state="translated">支持向量机分类器使用libsvm的实现：内核可以是非线性的，但其SMO算法不能像LinearSVC那样扩展到大量样本。此外，SVC多类模式使用一种对一种方案实现，而LinearSVC使用一种对另一种方案。通过使用&lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;包装器，可以用SVC来实现一个。最后，如果输入是C连续的，则SVC可以适合密集数据而无需复制内存。稀疏数据仍然会导致内存复制。</target>
        </trans-unit>
        <trans-unit id="78b58091d5da65fb36aa747d9975841d97302dec" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</source>
          <target state="translated">支持向量机分类器的实现,使用与该类相同的库(liblinear)。</target>
        </trans-unit>
        <trans-unit id="0faf8832b17d93a1b230f7a6ca4feb36d15cc4ac" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</source>
          <target state="translated">使用libsvm实现支持向量机回归:内核可以是非线性的,但其SMO算法不能像LinearSVC那样扩展到大量的样本。</target>
        </trans-unit>
        <trans-unit id="adae10003f16f5885f71700e866f2cc76e2c6af9" translate="yes" xml:space="preserve">
          <source>Implements feature hashing, aka the hashing trick.</source>
          <target state="translated">实现特征哈希,也就是哈希技巧。</target>
        </trans-unit>
        <trans-unit id="d98e09b894119d4a2d55bf3e2f04052ec103359a" translate="yes" xml:space="preserve">
          <source>Implements resampling with replacement. If False, this will implement (sliced) random permutations.</source>
          <target state="translated">实现重采样与替换。如果为False,将实现(切片)随机排列。</target>
        </trans-unit>
        <trans-unit id="9f9d0b6a3b9dbc770ff8e17c3a6979d6ebb5425d" translate="yes" xml:space="preserve">
          <source>Implements the Birch clustering algorithm.</source>
          <target state="translated">实施Birch聚类算法。</target>
        </trans-unit>
        <trans-unit id="82028db75262dc1a82a0dc4cf2e6f254032ff9f7" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="translated">从以下方面实现增量PCA模型： &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; 请参阅&lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eddd9fc5411550a8b67e601c8cf138c977f02e42" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;em&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/em&gt; See &lt;a href=&quot;https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06be3bf25c44efb35fdd12e8816c051b94a6e5d6" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: &lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt;Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">通过以下方式实现概率PCA模型：&lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt; Tipping，ME和Bishop，CM（1999）。&amp;ldquo;概率主成分分析&amp;rdquo;。皇家统计学会杂志：B系列（统计方法），61（3），611-622。通过score和score_samples方法。参见&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5153c1832189653f3d836c4da5e485c3d1b56671" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="496d8573358dc0bbf8fad0d466b95c37d153e5fd" translate="yes" xml:space="preserve">
          <source>Importance of Feature Scaling</source>
          <target state="translated">特征缩放的重要性</target>
        </trans-unit>
        <trans-unit id="dee0fbd7a096536203f3e083c7a95f20ef772057" translate="yes" xml:space="preserve">
          <source>Important members are fit, predict.</source>
          <target state="translated">重要成员适合,预测。</target>
        </trans-unit>
        <trans-unit id="34aace9b4c119f775f92304ec637d19c91f28ab6" translate="yes" xml:space="preserve">
          <source>Importantly, this tabular dataset has very different dynamic ranges for its features. Neural networks tend to be very sensitive to features with varying scales and forgetting to preprocess the numeric feature would lead to a very poor model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7f6e12fca9c93deac0f3e9ce5406887ec818c03" translate="yes" xml:space="preserve">
          <source>Improvements to the histogram-based Gradient Boosting estimators</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eed74af8ff28262ccadec0b3ca567aeb04ce5592" translate="yes" xml:space="preserve">
          <source>Imputation for completing missing values using k-Nearest Neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0004bf233145469d6159f141af0ae0b05f3c5e9a" translate="yes" xml:space="preserve">
          <source>Imputation transformer for completing missing values.</source>
          <target state="translated">用于完成缺失值的推演变压器。</target>
        </trans-unit>
        <trans-unit id="8154b566118976ff2097cfffb2c92470797b0a69" translate="yes" xml:space="preserve">
          <source>Impute all missing values in X.</source>
          <target state="translated">输入X中的所有缺失值。</target>
        </trans-unit>
        <trans-unit id="ad8e498cb05e98f21bd0f42774d74dde4c2bb7ea" translate="yes" xml:space="preserve">
          <source>Impute missing values with mean</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4715d3875af4e3819958eff35e6816030a9c89e" translate="yes" xml:space="preserve">
          <source>Impute the missing data and score</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="689f1aca2f66f40afe86b2a1257542980cceee50" translate="yes" xml:space="preserve">
          <source>Imputer used to initialize the missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74ee388ccf36d172dba88b767c11eeb31f63d971" translate="yes" xml:space="preserve">
          <source>Imputes all missing values in X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="510c592fb9a4fd828788fc0bdd902c165ca78889" translate="yes" xml:space="preserve">
          <source>Imputing missing values before building an estimator</source>
          <target state="translated">在建立估计器之前,计算缺失值。</target>
        </trans-unit>
        <trans-unit id="87c53ba7fd85032a63ff707cca98951b5852e72d" translate="yes" xml:space="preserve">
          <source>Imputing missing values with variants of IterativeImputer</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5d148df74ab3f703a9d283fda0c99f4936ff674" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt;, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in &lt;code&gt;ElasticNet&lt;/code&gt;, we control the combination of L1 and L2 with the &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) parameter, and the intensity of the regularization with the &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) parameter. Then the priors terms are:</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; 中&lt;/a&gt;，可以将L1和L2先验值添加到损失函数中，以便对模型进行正则化。L2先验使用Frobenius范数，而L1先验使用元素级L1范数。就像在 &lt;code&gt;ElasticNet&lt;/code&gt; 中一样，我们使用 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））参数控制L1和L2的组合，并使用 &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））参数控制正则化的强度。然后，先验条件为：</target>
        </trans-unit>
        <trans-unit id="eee03375c59654f18a55a7961e4f26a36fbc2cee" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt;&lt;code&gt;OutputCodeClassifier&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;code_size&lt;/code&gt; attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt; &lt;code&gt;OutputCodeClassifier&lt;/code&gt; 中&lt;/a&gt;， &lt;code&gt;code_size&lt;/code&gt; 属性允许用户控制将使用的分类器的数量。它是课程总数的百分比。</target>
        </trans-unit>
        <trans-unit id="92869ea1268ab85728d32cc145b2fc2a3cf98201" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if data for classification are unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;中，如果用于分类数据是不平衡（例如许多积极的和几个阴性），组 &lt;code&gt;class_weight='balanced'&lt;/code&gt; 和/或尝试不同的惩罚参数 &lt;code&gt;C&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3c0402c73702f19dd4de5bea870fc6f39e9426a9" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if the data is unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4a2dd93e9c8bc18123ea577d336109c88e8c2c3" translate="yes" xml:space="preserve">
          <source>In &lt;strong&gt;averaging methods&lt;/strong&gt;, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</source>
          <target state="translated">在&lt;strong&gt;平均方法中&lt;/strong&gt;，驱动原理是独立建立多个估计量，然后平均其预测。平均而言，由于组合估计量的方差减小，因此组合估计量通常比任何单个基本估计量都要好。</target>
        </trans-unit>
        <trans-unit id="ca51c131377adedf53770a869ad18245bd6bd115" translate="yes" xml:space="preserve">
          <source>In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baeac0931c0e4b4385579000935f2bb52ceb9f07" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;lsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="translated">在二元分类任务中，术语``正''和``负''指分类器的预测，术语``真''和``假''指该预测是否对应于外部判断（有时也称为&amp;ldquo;观测&amp;rdquo;）。给定这些定义，我们可以制定下表：</target>
        </trans-unit>
        <trans-unit id="ccc920586f4d3de666e5e909b4599881349f812c" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;rsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="995a1ae8b5be72e5d8cbdf391052b665e77e9964" translate="yes" xml:space="preserve">
          <source>In a first step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it&amp;rsquo;s a hierarchical clustering with structure prior.</source>
          <target state="translated">第一步，在不对结构进行连接限制的情况下执行层次聚类，并且仅基于距离，而在第二步中，聚类仅限于k最近邻居图：它是具有结构优先级的层次聚类。</target>
        </trans-unit>
        <trans-unit id="0336cb4d8e7c9adb72abdea9417802db49cccd1f" translate="yes" xml:space="preserve">
          <source>In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</source>
          <target state="translated">在大型文本语料库中，某些单词会非常出现（例如英语中的&amp;ldquo; the&amp;rdquo;，&amp;ldquo; a&amp;rdquo;，&amp;ldquo; is&amp;rdquo;），因此几乎没有关于文档实际内容的有意义的信息。如果我们将直接计数数据直接提供给分类器，那么那些非常频繁的术语会掩盖稀有但更有趣的术语的频率。</target>
        </trans-unit>
        <trans-unit id="0caaa107286ab067ca8115855386dd04703bccc4" translate="yes" xml:space="preserve">
          <source>In a multiclass setting, specifies the class for which the PDPs should be computed. Note that for binary classification, the positive class (index 1) is always used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52eb2197195ece26c6612081f2107f907b3e4099" translate="yes" xml:space="preserve">
          <source>In a multioutput setting, specifies the task for which the PDPs should be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0ed349168abd35a1fce87e6ae9e8ccc5ff58f4" translate="yes" xml:space="preserve">
          <source>In a nutshell, the following table summarizes the solvers characteristics:</source>
          <target state="translated">简而言之,下表总结了解算器的特点。</target>
        </trans-unit>
        <trans-unit id="b4ec4bcaff3e86d4d99c938f5623ab4b737e65c7" translate="yes" xml:space="preserve">
          <source>In a real world setting, the &lt;code&gt;n_features&lt;/code&gt; parameter can be left to its default value of &lt;code&gt;2 ** 20&lt;/code&gt; (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as &lt;code&gt;2 **
18&lt;/code&gt; might help without introducing too many additional collisions on typical text classification tasks.</source>
          <target state="translated">在实际设置中， &lt;code&gt;n_features&lt;/code&gt; 参数可以保留为其默认值 &lt;code&gt;2 ** 20&lt;/code&gt; （大约一百万个可能的特征）。如果内存或下游模型的大小成为问题，则选择一个较低的值（例如 &lt;code&gt;2 ** 18&lt;/code&gt; 可能会有所帮助，而不会在典型的文本分类任务上引入过多的其他冲突。</target>
        </trans-unit>
        <trans-unit id="c75e295f24e05d06cacc1a2f9bb570a61bdd802e" translate="yes" xml:space="preserve">
          <source>In a similar manner, the boston housing data set is used to show the impact of transforming the targets before learning a model. In this example, the targets to be predicted corresponds to the weighted distances to the five Boston employment centers.</source>
          <target state="translated">以类似的方式,波士顿住房数据集被用来显示在学习模型之前转换目标的影响。在这个例子中,要预测的目标对应于到五个波士顿就业中心的加权距离。</target>
        </trans-unit>
        <trans-unit id="c210295417ef2f29cc593be46bbc5efed5892a5f" translate="yes" xml:space="preserve">
          <source>In addition of using an imputing method, we can also keep an indication of the missing information using &lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt;&lt;/a&gt; which might carry some information.</source>
          <target state="translated">除了使用插补方法外，我们还可以使用&lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt; 保留&lt;/a&gt;可能包含某些信息的丢失信息的指示。</target>
        </trans-unit>
        <trans-unit id="67d21513f58775c1e37b37d0e7fc556492bd761a" translate="yes" xml:space="preserve">
          <source>In addition to imputing the missing values, the imputers have an &lt;code&gt;add_indicator&lt;/code&gt; parameter that marks the values that were missing, which might carry some information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45e3263783ee36dc90c5821ae7ffc8d210750151" translate="yes" xml:space="preserve">
          <source>In addition to its current contents, this module will eventually be home to refurbished versions of Pipeline and FeatureUnion.</source>
          <target state="translated">除了目前的内容,这个模块最终将成为Pipeline和FeatureUnion的翻新版本。</target>
        </trans-unit>
        <trans-unit id="3dda0db479e61a3dc2539c918fe85f072a3cc4a4" translate="yes" xml:space="preserve">
          <source>In addition to standard scikit-learn estimator API, GaussianProcessRegressor:</source>
          <target state="translated">除了标准的scikit-learn估计器API,GaussianProcessRegressor。</target>
        </trans-unit>
        <trans-unit id="6c259b1081efe473add72a6c01ee26dbc96ee486" translate="yes" xml:space="preserve">
          <source>In addition to the mean of the predictive distribution, also its standard deviation can be returned.</source>
          <target state="translated">除了预测分布的平均值外,还可以返回其标准差。</target>
        </trans-unit>
        <trans-unit id="f5f01da0b407208bd57121f71ed7408cbbce3fe6" translate="yes" xml:space="preserve">
          <source>In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi partition of the graph.</source>
          <target state="translated">此外,由于图像的强度或其梯度中没有有用的信息,我们选择在一个仅由梯度提供弱信息的图上执行光谱聚类。这接近于执行图的Voronoi分割。</target>
        </trans-unit>
        <trans-unit id="8be2789c3e2f5a1d410c34b62e20c28c8adc9fef" translate="yes" xml:space="preserve">
          <source>In addition, if the &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;distributed&lt;/code&gt; Python packages are installed, it is possible to use the &amp;lsquo;dask&amp;rsquo; backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</source>
          <target state="translated">此外，如果安装了 &lt;code&gt;dask&lt;/code&gt; 和 &lt;code&gt;distributed&lt;/code&gt; Python软件包，则可以使用&amp;ldquo; dask&amp;rdquo;后端更好地调度嵌套并行调用，而不会超额预订，并且有可能在多个主机的网络群集上分配并行调用。</target>
        </trans-unit>
        <trans-unit id="1068dbee0dd3c16e2fc93e44b0ce455f5b052f8b" translate="yes" xml:space="preserve">
          <source>In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.</source>
          <target state="translated">此外,scikit-learn还包括各种随机样本生成器,可以用来构建规模和复杂度可控的人工数据集。</target>
        </trans-unit>
        <trans-unit id="fc65b58b68e776527046619280b83dca96b85eef" translate="yes" xml:space="preserve">
          <source>In addition, some of the numpy routines that are used internally by scikit-learn may also be parallelized if numpy is installed with specific numerical libraries such as MKL, OpenBLAS, or BLIS.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a9dc36feb47a20be69368bf3755ac289f3cd578" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67ed28f1d0cd9fef0560dd0bbfe2b568680ea5a6" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellanous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="translated">此外，还有其他工具可用于加载其他格式或从其他位置&lt;a href=&quot;#loading-other-datasets&quot;&gt;加载数据集&lt;/a&gt;，如&amp;ldquo; 加载其他数据集&amp;rdquo;部分所述。</target>
        </trans-unit>
        <trans-unit id="5f2d75a19b27e52127d76b03616ce749746e4308" translate="yes" xml:space="preserve">
          <source>In addition, we show two different ways to dispatch the columns to the particular pre-processor: by column names and by column data types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="846c6c3d11b49bd5243a8b72c066c7aae06dcfe3" translate="yes" xml:space="preserve">
          <source>In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are interested in separating the objects one from the other, and not from the background.</source>
          <target state="translated">此外,我们使用对象的掩码将图形限制在对象的轮廓上。在这个例子中,我们感兴趣的是将对象一个个分开,而不是与背景分开。</target>
        </trans-unit>
        <trans-unit id="b490744f01019d4237b3a8568465e031a5ae6e1f" translate="yes" xml:space="preserve">
          <source>In all these strategies, the &lt;code&gt;predict&lt;/code&gt; method completely ignores the input data.</source>
          <target state="translated">在所有这些策略中， &lt;code&gt;predict&lt;/code&gt; 方法完全忽略了输入数据。</target>
        </trans-unit>
        <trans-unit id="450c8a41f7c5da3bb2b7da9a15306b194b36c681" translate="yes" xml:space="preserve">
          <source>In an &lt;strong&gt;unsupervised setting&lt;/strong&gt; it can be used to group similar documents together by applying clustering algorithms such as &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt;:</source>
          <target state="translated">在&lt;strong&gt;无人监督的情况下&lt;/strong&gt;，可以通过应用聚类算法（例如&lt;a href=&quot;clustering#k-means&quot;&gt;K-means）&lt;/a&gt;将相似的文档分组在一起：</target>
        </trans-unit>
        <trans-unit id="e5af8b183011d6bf7238d58f71b94384a5a96f84" translate="yes" xml:space="preserve">
          <source>In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.</source>
          <target state="translated">在任何情况下都要注意,如上所述,降低模型的复杂度会影响精度。例如,一个非线性可分离的问题可以用一个快速的线性模型来处理,但预测能力很可能在这个过程中受到影响。</target>
        </trans-unit>
        <trans-unit id="8c17ce8abfedb506f7ed46ebde27121f581c8bb7" translate="yes" xml:space="preserve">
          <source>In applications where a high false positive rate is not tolerable the parameter &lt;code&gt;max_fpr&lt;/code&gt; of &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; can be used to summarize the ROC curve up to the given limit.</source>
          <target state="translated">在 &lt;code&gt;max_fpr&lt;/code&gt; 高误报率的应用中，可以使用roc_auc_score的&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;参数总结ROC曲线，直至达到给定极限。</target>
        </trans-unit>
        <trans-unit id="bbcc07c440f8193b3e7ecf64eaaca0e386adcbad" translate="yes" xml:space="preserve">
          <source>In bin edges for feature &lt;code&gt;i&lt;/code&gt;, the first and last values are used only for &lt;code&gt;inverse_transform&lt;/code&gt;. During transform, bin edges are extended to:</source>
          <target state="translated">在特征 &lt;code&gt;i&lt;/code&gt; 的 bin边缘中，第一个和最后一个值仅用于 &lt;code&gt;inverse_transform&lt;/code&gt; 。在转换期间，bin边缘将扩展为：</target>
        </trans-unit>
        <trans-unit id="9d083c0b55e1a00133d4b27dd85f5ea26aca3922" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.</source>
          <target state="translated">在二元分类和多类分类中,Jaccard相似度系数得分等于分类精度。</target>
        </trans-unit>
        <trans-unit id="834fbb10f3b1dd0752ef9b7f9aa530ac5202b2db" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_score&lt;/code&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0b30958f6975dadd3d7eaf24f5447254d56c629" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_similarity_score&lt;/code&gt; function.</source>
          <target state="translated">在二进制和多类分类中，此函数等于 &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 函数。</target>
        </trans-unit>
        <trans-unit id="e731c9654f26a8d7255165d2c0d5f78e47c3bd9a" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equivalent to the &lt;code&gt;accuracy_score&lt;/code&gt;. It differs in the multilabel classification problem.</source>
          <target state="translated">在二进制和多类分类中，此函数等效于 &lt;code&gt;accuracy_score&lt;/code&gt; 。在多标签分类问题上有所不同。</target>
        </trans-unit>
        <trans-unit id="c4cb57bb3e2c0bb1485829473f6ca6362cee5b90" translate="yes" xml:space="preserve">
          <source>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; is always negative (since the signs disagree), implying &lt;code&gt;1 - margin&lt;/code&gt; is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">在二元类情况下，假设y_true中的标签用+1和-1编码，则在发生预测错误时， &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; 始终为负（因为符号不同），这意味着 &lt;code&gt;1 - margin&lt;/code&gt; 始终大于1。因此，累积的铰链损耗是分类器所犯错误数量的上限。</target>
        </trans-unit>
        <trans-unit id="f2c8a5d61695d64c32adbdec8051b4ecc7f404cb" translate="yes" xml:space="preserve">
          <source>In binary classification settings</source>
          <target state="translated">在二进制分类设置中</target>
        </trans-unit>
        <trans-unit id="0e8524872beef3003e748e1d9b4f90c7ce280313" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with a tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="translated">在这两种情况下，该标准都会由一个时期进行一次评估，并且当该标准没有连续提高 &lt;code&gt;n_iter_no_change&lt;/code&gt; 时间时，算法就会停止。改进以容差 &lt;code&gt;tol&lt;/code&gt; 进行评估，并且在最大迭代次数 &lt;code&gt;max_iter&lt;/code&gt; 之后，算法无论如何都将停止。</target>
        </trans-unit>
        <trans-unit id="e7dad232dab7fc7fe4bb0ec13d7d0a07fef6ce88" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with absolute tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f03bffae8069d1108a85252dc34a4485434865b8" translate="yes" xml:space="preserve">
          <source>In both cases, the kernel&amp;rsquo;s parameters are estimated using the maximum likelihood principle.</source>
          <target state="translated">在两种情况下，均使用最大似然原理估算内核参数。</target>
        </trans-unit>
        <trans-unit id="dc8004c8d5b437fc6c479e2e5882eb635fa97592" translate="yes" xml:space="preserve">
          <source>In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.</source>
          <target state="translated">在下面的两个例子中,主要结果是,作为非稳健估计的经验协方差估计,受观测值的异质结构影响很大。虽然鲁棒协方差估计能够关注数据分布的主模式,但它坚持了数据应该是高斯分布的假设,得出的数据结构估计有一定的偏差,但却在一定程度上准确。One-Class SVM不假设数据分布的任何参数形式,因此能够更好地模拟数据的复杂形状。</target>
        </trans-unit>
        <trans-unit id="7f2b051010be4e679b8556fa182a1724fa1e49b9" translate="yes" xml:space="preserve">
          <source>In case the file contains a pairwise preference constraint (known as &amp;ldquo;qid&amp;rdquo; in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</source>
          <target state="translated">如果文件包含成对的首选项约束（在svmlight格式中称为&amp;ldquo; qid&amp;rdquo;），除非query_id参数设置为True，否则将忽略这些约束。当使用成对损失函数时（如某些学习排序问题的情况），这些成对偏好约束可用于约束样本组合，从而仅考虑具有相同query_id值的对。</target>
        </trans-unit>
        <trans-unit id="b2d3bbc7ab1d1edcd850a10f6fb01a251c14a28b" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zero&amp;rsquo;s in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="translated">如果遇到未知类别（在&amp;ldquo;一键编码&amp;rdquo;中全为零）， &lt;code&gt;None&lt;/code&gt; ）代表该类别。</target>
        </trans-unit>
        <trans-unit id="d452896d1312cac0434176b68ca8ca9a2bb1195e" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zeros in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca499264726caa99e06bab43fc3d4644ea84df01" translate="yes" xml:space="preserve">
          <source>In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in &lt;code&gt;working_memory&lt;/code&gt;-sized chunks. If &lt;code&gt;reduce_func&lt;/code&gt; is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.</source>
          <target state="translated">在并非所有成对距离矩阵都需要一次存储的情况下，这用于计算 &lt;code&gt;working_memory&lt;/code&gt; 大小的块中的成对距离。如果给出了 &lt;code&gt;reduce_func&lt;/code&gt; ，它将在每个块上运行，并将其返回值连接到列表，数组或稀疏矩阵中。</target>
        </trans-unit>
        <trans-unit id="261de18f8066fcaced5cb3f145cb26c170301e09" translate="yes" xml:space="preserve">
          <source>In cases where the data is not uniformly sampled, radius-based neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt;&lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt;&lt;/a&gt; can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;.</source>
          <target state="translated">如果没有统一采样数据，则&lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt; &lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt; 中&lt;/a&gt;基于半径的邻居分类可能是更好的选择。用户指定一个固定半径\（r \），这样稀疏邻域中的点将较少的最近邻用于分类。对于高维参数空间，此方法由于所谓的&amp;ldquo;维数诅咒&amp;rdquo;而变得无效。</target>
        </trans-unit>
        <trans-unit id="46149a533d1136e96a72fc2595f06ccb02814862" translate="yes" xml:space="preserve">
          <source>In certain cases Theil-Sen performs better than &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the &lt;code&gt;residual_threshold&lt;/code&gt; parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the &lt;code&gt;max_subpopulation&lt;/code&gt; parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset.</source>
          <target state="translated">在某些情况下，Theil-Sen的性能优于&lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;，这也是一种可靠的方法。在下面的第二个示例中对此进行了说明，其中第二个示例相对于x轴扰动RANSAC。调整RANSAC 的 &lt;code&gt;residual_threshold&lt;/code&gt; 参数可解决此问题，但通常需要有关数据和异常值性质的先验知识。由于Theil-Sen的计算复杂性，建议仅将其用于样本数量和特征较小的问题。对于较大的问题， &lt;code&gt;max_subpopulation&lt;/code&gt; 参数将p个子采样点的所有可能组合的大小限制为随机选择的子集，因此也限制了运行时间。因此，Theil-Sen适用于较大的问题，因为它随后会在随机子集上工作，因此会失去一些数学特性。</target>
        </trans-unit>
        <trans-unit id="08403787ed9849b402f6d04f68a0bae46063dfaf" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="translated">与&lt;a href=&quot;#id13&quot;&gt;贝叶斯岭回归&lt;/a&gt;相反，\（w_ {i} \）的每个坐标都有其自己的标准偏差\（\ lambda_i \）。所有\（\ lambda_i \）的优先级被选择为由超参数\（\ lambda_1 \）和\（\ lambda_2 \）给出的相同伽玛分布。</target>
        </trans-unit>
        <trans-unit id="f00603c6aeddee02bf1dc7fdebe82c8fea70d634" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id9&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="741dc2ca1b0b96b753a4293cdc66da483cb961b9" translate="yes" xml:space="preserve">
          <source>In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</source>
          <target state="translated">与GridSearchCV不同的是,并不是所有的参数值都被试过,而是从指定的分布中抽取固定数量的参数设置。试用的参数设置的数量由n_iter给出。</target>
        </trans-unit>
        <trans-unit id="f7007cbebb915951a7329a621ec59e7bfd3c1528" translate="yes" xml:space="preserve">
          <source>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</source>
          <target state="translated">与多数投票(硬投票)相比,软投票将类标签返回为预测概率之和的argmax。</target>
        </trans-unit>
        <trans-unit id="a5222e41535c7e60d0bed8020d5a39a4cdb9c58d" translate="yes" xml:space="preserve">
          <source>In contrast to the original publication &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</source>
          <target state="translated">与原始出版物&lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]相比&lt;/a&gt;，scikit-learn实现通过平均分类器的概率预测来组合分类器，而不是让每个分类器对单个分类投票。</target>
        </trans-unit>
        <trans-unit id="092465bd0b61837459fb29bf14c2dda6ed20e949" translate="yes" xml:space="preserve">
          <source>In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">与回归设置相反，即使对于GP先验，潜在函数\（f \）的后验也不是高斯，因为高斯似然不适用于离散类标签。而是，使用与逻辑链接函数（logit）相对应的非高斯似然。 GaussianProcessClassifier基于Laplace逼近，以高斯近似非高斯后验。可以在&lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]的&lt;/a&gt;第3章中找到更多详细信息。</target>
        </trans-unit>
        <trans-unit id="8da23c4fea95dfb9e1a4723525c1617ef732103e" translate="yes" xml:space="preserve">
          <source>In contrast, for small amounts of data, the training score of the SVM is much greater than the validation score. Adding more training samples will most likely increase generalization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d7f12a42ea8277b24625f1aff8d53cb363a14e0" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{\text{n\_classes}}\).</source>
          <target state="translated">相反,如果传统的准确率高于偶然性,只是因为分类器利用了不平衡的测试集,那么平衡的准确率就会酌情降到/(frac{1}{text{n_classes}})。</target>
        </trans-unit>
        <trans-unit id="8343cdcc0bd2973a4149cb63a31822f5be571a78" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{n\_classes}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89611c1358b346353d5469c5670ea64fb02fcdd7" translate="yes" xml:space="preserve">
          <source>In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored using 10 fold cross validation, we see:</source>
          <target state="translated">按照质量降序,当使用 30 个估计器对所有 4 个特征进行训练(本例之外),并使用 10 倍交叉验证进行评分时,我们看到。</target>
        </trans-unit>
        <trans-unit id="0732cca6c2251b860da4c331fa5748d479b14945" translate="yes" xml:space="preserve">
          <source>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</source>
          <target state="translated">在合集算法中,袋法构成了一类算法,它在原始训练集的随机子集上建立多个黑盒估计器实例,然后将它们的各个预测汇总起来,形成最终的预测。这些方法被用作减少基础估计器(如决策树)方差的一种方法,通过在其构建过程中引入随机化,然后将其做成一个集合。在许多情况下,袋法构成了一种非常简单的方法来改进单一模型,而不需要调整基础算法。由于袋法提供了一种减少过拟合的方法,因此,袋法对强而复杂的模型(例如,完全开发的决策树)效果最好,而升压法通常对弱模型(例如,浅层决策树)效果最好。</target>
        </trans-unit>
        <trans-unit id="5305d1e9b70806a8391e61e804a0df6abd8f6cc5" translate="yes" xml:space="preserve">
          <source>In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">在将二进制度量标准扩展到多类或多标签问题时，数据被视为二进制问题的集合，每个类一个。然后，有多种方法可以对一组类的平均二进制度量计算取平均值，每种方法在某些情况下可能很有用。如果可用，您应该使用 &lt;code&gt;average&lt;/code&gt; 参数从中选择。</target>
        </trans-unit>
        <trans-unit id="e87cfc9dff0fe670bd40ebf7e26edaa15ca842ad" translate="yes" xml:space="preserve">
          <source>In extremely randomized trees (see &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;随机化的树中（请参见ExtraTreesClassifier和&lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt;类），随机性在计算拆分的方式上进一步向前迈进了一步。像在随机森林中一样，使用候选特征的随机子集，但不是寻找最有区别的阈值，而是为每个候选特征随机绘制阈值，并选择这些随机生成的阈值中的最佳阈值作为划分规则。这通常可以更大程度地减少模型的方差，但要以更大的偏差增加为代价：</target>
        </trans-unit>
        <trans-unit id="5c1305e3ce4cbb99adc8d313e42a43efab81ea5c" translate="yes" xml:space="preserve">
          <source>In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:</source>
          <target state="translated">事实上,这个数据集只有一个版本。而虹膜数据集则有多个版本。</target>
        </trans-unit>
        <trans-unit id="63493dde535d33b43819cf48666bb2a9620c2476" translate="yes" xml:space="preserve">
          <source>In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">法文,但仍可作为参考。Tenenhaus,M.(1998年)。La regression PLS:theorie et pratique。巴黎。Editions Technic。</target>
        </trans-unit>
        <trans-unit id="6e95c3ada3b2525ed5f608da19594b4a42ad3dc4" translate="yes" xml:space="preserve">
          <source>In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:</source>
          <target state="translated">一般来说,由于一些原因(分支可预测性、CPU缓存、线性代数库优化等),批量做预测(许多实例同时进行)更有效率。在这里,我们看到在少数特征的设置上,独立于估计器的选择,批量模式总是更快,而且对其中一些来说,快了1到2个数量级。</target>
        </trans-unit>
        <trans-unit id="73d5a0649f1537ceaa4a43b2819de8ab34f4f95d" translate="yes" xml:space="preserve">
          <source>In general, &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5f14cdf8cb9c0df1b6ffce69bd866cdeffd9355" translate="yes" xml:space="preserve">
          <source>In general, a learning problem considers a set of n &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;samples&lt;/a&gt; of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;multivariate&lt;/a&gt; data), it is said to have several attributes or &lt;strong&gt;features&lt;/strong&gt;.</source>
          <target state="translated">通常，学习问题会考虑一组n 个数据&lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;样本&lt;/a&gt;，然后尝试预测未知数据的属性。如果每个样本都大于一个数字，例如是一个多维条目（也称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;多元&lt;/a&gt;数据），则称其具有多个属性或&lt;strong&gt;特征&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="9cf7334c38597a2189c7af702ab9abdbe9f10093" translate="yes" xml:space="preserve">
          <source>In general, is a technique used for analyzing similarity or dissimilarity data. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="translated">通常，是一种用于分析相似性或不相似性数据的技术。&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;尝试将相似性或不相似性数据建模为几何空间中的距离。数据可以是对象之间的相似性等级，分子的相互作用频率或国家之间的贸易指数。</target>
        </trans-unit>
        <trans-unit id="71aab6786f00490669e72ac36911ce2d2486dab4" translate="yes" xml:space="preserve">
          <source>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</source>
          <target state="translated">一般来说,它是关于学习一个粗略的、接近的边界限定初始观测分布的轮廓,绘制在嵌入/(p)-维空间中。然后,如果进一步的观测值位于边界限定的子空间内,则认为它们来自于与初始观测值相同的群体。否则,如果它们位于边界之外,我们可以说它们是异常的,我们的评估具有给定的信心。</target>
        </trans-unit>
        <trans-unit id="c9bca25ec918e4e036ec8a37ec502896ec56d542" translate="yes" xml:space="preserve">
          <source>In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;Compare the effect of different scalers on data with outliers&lt;/a&gt;.</source>
          <target state="translated">通常，学习算法受益于数据集的标准化。如果集合中存在一些异常值，则更适合使用健壮的缩放器或转换器。在&lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;比较不同缩放器对数据与异常值的影响中，&lt;/a&gt;着重强调了不同缩放器，转换器和规范化器在包含边缘异常值的数据集上的行为。</target>
        </trans-unit>
        <trans-unit id="baeb2b7a43c2bc0dd04675c021d6ed663a58bf2d" translate="yes" xml:space="preserve">
          <source>In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in entropy. This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).</source>
          <target state="translated">一般来说,构建平衡二叉树的运行时间成本为/(O(n_{samples}n_{features}log(n_{samples}))/),查询时间为/(O(\log(n_{samples}))/)。虽然树的构造算法试图生成平衡的树,但它们不会总是平衡的。假设子树保持近似平衡,每个节点的成本包括通过搜索(O(n_{features}))来寻找能提供最大熵减少的特征。这在每个节点上的成本为 \(O(n_{features}n_{samples}/log(n_{samples}))\),导致整个树的总成本(通过将每个节点的成本相加)为 \(O(n_{features}n_{samples}^{2}/log(n_{samples}))\)。</target>
        </trans-unit>
        <trans-unit id="144a3925f6b19404e9d474c272482fb04a69a6ff" translate="yes" xml:space="preserve">
          <source>In general, when fitting a curve with a polynomial by Bayesian ridge regression, the selection of initial values of the regularization parameters (alpha, lambda) may be important. This is because the regularization parameters are determined by an iterative procedure that depends on initial values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dba314b8268f3eec306fec03d7cfe13e8e090ace" translate="yes" xml:space="preserve">
          <source>In general, when the problem isn&amp;rsquo;t linearly separable, the support vectors are the samples &lt;em&gt;within&lt;/em&gt; the margin boundaries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="635895acc09f2d99381585bc2d144c9a66a85f3a" translate="yes" xml:space="preserve">
          <source>In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,</source>
          <target state="translated">在梯度下降中,计算出损失相对于权重的梯度(Loss_{W}),并从(W)中扣除。更正式地说,它表示为:</target>
        </trans-unit>
        <trans-unit id="2c51a2af5a19ac0ce7e4fb04fd6d887c03b6fecb" translate="yes" xml:space="preserve">
          <source>In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data.</source>
          <target state="translated">在高维空间中,线性分类器通常能达到很好的精度。对于稀疏的二进制数据,BernoulliNB尤其适合。底部一行比较了BernoulliNB在变换空间中获得的决策边界与在原始数据上学习的ExtraTreesClassifier森林。</target>
        </trans-unit>
        <trans-unit id="26c26ee3d75b66c7f22fed706da52f459434240f" translate="yes" xml:space="preserve">
          <source>In linear models, the target value is modeled as a linear combination of the features (see the &lt;a href=&quot;../../modules/linear_model#linear-model&quot;&gt;Linear Models&lt;/a&gt; User Guide section for a description of a set of linear models available in scikit-learn). Coefficients in multiple linear models represent the relationship between the given feature, \(X_i\) and the target, \(y\), assuming that all the other features remain constant (&lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_dependence&quot;&gt;conditional dependence&lt;/a&gt;). This is different from plotting \(X_i\) versus \(y\) and fitting a linear relationship: in that case all possible values of the other features are taken into account in the estimation (marginal dependence).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2367cf553e95efae790eac559ef2be19cd28f503" translate="yes" xml:space="preserve">
          <source>In machine-learning practice, Ridge Regression is more often used with non-negligible regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b577c96674cf299faa19ce0d11e2224d3c2c813" translate="yes" xml:space="preserve">
          <source>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</source>
          <target state="translated">在多数投票中,特定样本的预测类标签是代表每个单独分类器预测的类标签的多数(模式)的类标签。</target>
        </trans-unit>
        <trans-unit id="589394183aec0e7af2afe4b456559f6baedc9992" translate="yes" xml:space="preserve">
          <source>In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.</source>
          <target state="translated">因此,在许多情况下,建议仔细地对你的特征提取代码进行时间和配置文件,因为当你的整体延迟对你的应用程序来说太慢时,它可能是一个很好的开始优化的地方。</target>
        </trans-unit>
        <trans-unit id="aeae04273a5ed1fc88f796de718e3c2190c04f0d" translate="yes" xml:space="preserve">
          <source>In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.</source>
          <target state="translated">在许多建模场景中,数据集中特征的正态性是可取的。功率变换是一系列参数化的单调变换,其目的是将来自任何分布的数据映射到尽可能接近高斯分布的数据中,以稳定方差并最大限度地减少偏斜。</target>
        </trans-unit>
        <trans-unit id="c82f65d47c3f4e11ad468a4165bdc787c51720a5" translate="yes" xml:space="preserve">
          <source>In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use &lt;code&gt;FeatureUnion&lt;/code&gt; to combine features obtained by PCA and univariate selection.</source>
          <target state="translated">在许多实际示例中，有许多方法可以从数据集中提取要素。通常，组合几种方法以获得良好的性能是有益的。本示例说明如何使用 &lt;code&gt;FeatureUnion&lt;/code&gt; 组合通过PCA和单变量选择获得的特征。</target>
        </trans-unit>
        <trans-unit id="9c0b7f3861d3fe001968b978c49f3447d1233fa3" translate="yes" xml:space="preserve">
          <source>In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</source>
          <target state="translated">在数学中,约翰逊-林登斯特劳斯定律是一个关于高维空间中的点嵌入低维欧氏空间的低失真结果。该定律指出,高维空间中的一小部分点可以被嵌入到一个低维空间中,而点与点之间的距离几乎被保存下来。用于嵌入的映射至少是Lipschitz的,甚至可以认为是正交投影。</target>
        </trans-unit>
        <trans-unit id="35a3805825da50966c5f8cb649b1d2ea852b8f59" translate="yes" xml:space="preserve">
          <source>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</source>
          <target state="translated">在最大化对数似然的过程中,正梯度使得模型更倾向于与观察到的训练数据兼容的隐藏状态。由于RBMs的二段式结构,它可以有效地计算。然而,负梯度是难以解决的。它的目标是降低模型偏好的联合状态的能量,因此使其保持对数据的真实性。它可以通过马尔科夫链蒙特卡洛使用块吉布斯抽样来近似,通过迭代抽样每一个给定的/(v/)和/(h/)/),直到链混合。以这种方式生成的样本有时被称为幻想粒子。这种方式效率很低,而且很难确定马尔科夫链是否混合。</target>
        </trans-unit>
        <trans-unit id="54db7da5f1b2e2f16e8f4dc3a375dac661b78213" translate="yes" xml:space="preserve">
          <source>In multi-label classification, the &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function is extended by averaging over the labels as &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;.</source>
          <target state="translated">在多标签分类，&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;功能是通过平均在标签为扩展&lt;a href=&quot;#average&quot;&gt;以上&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d9be5dcb267dcb84c278d12d7b1a881ada760886" translate="yes" xml:space="preserve">
          <source>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</source>
          <target state="translated">在多标签分类中,这是子集准确率,这是一个苛刻的指标,因为你要求每个样本的每个标签集都能正确预测。</target>
        </trans-unit>
        <trans-unit id="9ff5420b9cd3095ee44bf9941c38c72dce6d517a" translate="yes" xml:space="preserve">
          <source>In multi-label settings</source>
          <target state="translated">在多标签设置中</target>
        </trans-unit>
        <trans-unit id="cf7a69d811fd496380ea6a3966d13bf17ca83f43" translate="yes" xml:space="preserve">
          <source>In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the &lt;code&gt;average&lt;/code&gt; argument to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; (multilabel only), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; functions, as described &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;. Note that if all labels are included, &amp;ldquo;micro&amp;rdquo;-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy. Also note that &amp;ldquo;weighted&amp;rdquo; averaging may produce an F-score that is not between precision and recall.</source>
          <target state="translated">在多类和多标签分类任务中，可以将精度，召回率和F量度的概念分别应用于每个标签。有几种方法可以横跨标签结果组合，由指定的 &lt;code&gt;average&lt;/code&gt; 参数来&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;（多标记只），&lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt;功能，如所描述的&lt;a href=&quot;#average&quot;&gt;上述&lt;/a&gt;。请注意，如果包括所有标签，则在多类设置中进行&amp;ldquo;微&amp;rdquo;平均将产生与精度相同的精度，查全率和\（F \）。另请注意，&amp;ldquo;加权&amp;rdquo;平均可能会产生介于精度和召回率之间的F分数。</target>
        </trans-unit>
        <trans-unit id="afc91520f5287da47360dcd6fd00b4fb446bcf96" translate="yes" xml:space="preserve">
          <source>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer&amp;rsquo;s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">在多类情况下，该函数期望所有标签都包含在y_true中，或者提供一个包含所有标签的可选标签参数。多标签边距是根据Crammer-Singer方法计算的。与二元情况一样，累积的铰链损失是分类器所犯错误数量的上限。</target>
        </trans-unit>
        <trans-unit id="a7ec36140af641cfb5e4e5e11dec536798cfb2f8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss correspond to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function.</source>
          <target state="translated">在多类分类中，汉明损失对应于 &lt;code&gt;y_true&lt;/code&gt; 和 &lt;code&gt;y_pred&lt;/code&gt; 之间的汉明距离，该距离等效于子集 &lt;code&gt;zero_one_loss&lt;/code&gt; 函数。</target>
        </trans-unit>
        <trans-unit id="a514b0b14d02249930d02d183e261b474a100dbd" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff1916ae5265c4d87d1472e5cc3e0c2594a22de8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is similar to the &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.</source>
          <target state="translated">在多类分类中，汉明损失对应于 &lt;code&gt;y_true&lt;/code&gt; 和 &lt;code&gt;y_pred&lt;/code&gt; 之间的汉明距离，类似于&lt;a href=&quot;#zero-one-loss&quot;&gt;零一损失&lt;/a&gt;函数。但是，虽然零一损失惩罚了不严格匹配真实集的预测集，但汉明损失惩罚了各个标签。因此，以零一损失为上限的汉明损失始终在零和一之间（包括零）。并预测正确标签的正确子集或超集将使汉明损失介于0和1之间（不包括）。</target>
        </trans-unit>
        <trans-unit id="cf7ce831a18d046dad4e38dc2cae92648b792778" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">在多标签分类中，&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt;如果其标签严格匹配预测，则将子集评分为1，如果存在任何错误，则评分为0。默认情况下，该函数返回不完全预测的子集的百分比。要获取此类子集的计数，请将 &lt;code&gt;normalize&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2cdc777c3fd9aacea19e984339f1423c55608098" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.</source>
          <target state="translated">在多标签分类中,汉明损失与子集零一损失不同。零一损失认为,如果给定样本的整个标签集确实与真实的标签集完全匹配,那么它的标签集就不正确。汉明损失更宽容,它对单个标签进行惩罚。</target>
        </trans-unit>
        <trans-unit id="602aeb7c2d89b27ea6d03c59146d4b4fecde4c31" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does not entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes only the individual labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00e9bece59054d08c4ac787e06eeb4fc8070bdab" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.</source>
          <target state="translated">在多标签分类中,该函数返回子集精度。如果一个样本的整个预测标签集与真实标签集严格匹配,那么子集准确率为1.0;否则为0.0。</target>
        </trans-unit>
        <trans-unit id="7cd1b88a6c55666089bdc7543f7e259d70d5898d" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.</source>
          <target state="translated">在多标签分类中,zero_one_loss函数对应的是子集零一损失:对于每个样本,必须正确预测整个标签集,否则该样本的损失等于一。</target>
        </trans-unit>
        <trans-unit id="c56a96e702a01557c0cb1c7c6c5d254cdaebcc8b" translate="yes" xml:space="preserve">
          <source>In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must &lt;em&gt;exactly&lt;/em&gt; match the corresponding set of labels in y_true.</source>
          <target state="translated">在多标签分类中，此函数计算子集准确性：为样本预测的标签集必须与y_true中的相应标签集&lt;em&gt;完全&lt;/em&gt;匹配。</target>
        </trans-unit>
        <trans-unit id="00440d1e0316ae49b10a616cf581f0acff1a935a" translate="yes" xml:space="preserve">
          <source>In multilabel confusion matrix \(MCM\), the count of true negatives is \(MCM_{:,0,0}\), false negatives is \(MCM_{:,1,0}\), true positives is \(MCM_{:,1,1}\) and false positives is \(MCM_{:,0,1}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fad4287dcc0210ad8169708b233947ca706f077" translate="yes" xml:space="preserve">
          <source>In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.</source>
          <target state="translated">在多标签学习中,每个样本可以有任意数量的地真标签与之相关联。其目标是给地真标签以高分和更好的排名。</target>
        </trans-unit>
        <trans-unit id="9d6449537c42279d12e406059563c338784d06f3" translate="yes" xml:space="preserve">
          <source>In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.</source>
          <target state="translated">在多标签学习中，二进制分类任务的联合集用标签二进制指示符数组表示：每个样本是具有二进制值的二维形状数组（n_samples，n_classes）的一行：一个（即非零元素）对应于标签的子集。诸如 &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; 类的数组在第一个样本中表示标签0，在第二个样本中表示标签1和2。 ，而第三个样本中没有标签。</target>
        </trans-unit>
        <trans-unit id="65f6ef4e3d2b7a1359958abf87c802c2de77e1d9" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c2c0f769c8a98dc6df3f2e7afe566ac80c0f339" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.</source>
          <target state="translated">在通常情况下,Calinski-Harabaz指数适用于聚类分析的结果。</target>
        </trans-unit>
        <trans-unit id="5f0c7d20ec265094d1673fd625fd38165b384452" translate="yes" xml:space="preserve">
          <source>In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:</source>
          <target state="translated">在通常情况下,Davies-Bouldin指数应用于聚类分析的结果,具体如下。</target>
        </trans-unit>
        <trans-unit id="0488e7351783ef8ef785f4bdea49af8c75724adf" translate="yes" xml:space="preserve">
          <source>In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.</source>
          <target state="translated">在通常情况下,剪影系数被应用于聚类分析的结果。</target>
        </trans-unit>
        <trans-unit id="af7916eabb756a4304309b1e18ceea097a7a5071" translate="yes" xml:space="preserve">
          <source>In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as &amp;ldquo;Structured output&amp;rdquo; problems which are currently outside of the scope of scikit-learn.</source>
          <target state="translated">为了解决自然语言理解的更广泛任务，因此应考虑句子和段落的局部结构。因此，许多这样的模型将被视为&amp;ldquo;结构化输出&amp;rdquo;问题，目前不在scikit-learn的范围内。</target>
        </trans-unit>
        <trans-unit id="819693d214fc959100941f9c2bf3cb570fc069ec" translate="yes" xml:space="preserve">
          <source>In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:</source>
          <target state="translated">为了解决这个问题,scikit-learn提供了从文本内容中提取数字特征的最常见方法的实用程序,即。</target>
        </trans-unit>
        <trans-unit id="5bdd52099ccc039c40b609f18b326c63aea62fae" translate="yes" xml:space="preserve">
          <source>In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the &lt;code&gt;scipy.sparse&lt;/code&gt; package.</source>
          <target state="translated">为了能够将这样的矩阵存储在内存中并且还可以加快代数运算矩阵/向量，实现通常会使用稀疏表示，例如 &lt;code&gt;scipy.sparse&lt;/code&gt; 包中可用的实现。</target>
        </trans-unit>
        <trans-unit id="af0531a207de85560d0c6e1dcc4e5a478aa65d8d" translate="yes" xml:space="preserve">
          <source>In order to build histograms, the input data &lt;code&gt;X&lt;/code&gt; needs to be binned into integer-valued bins. This binning procedure does require sorting the feature values, but it only happens once at the very beginning of the boosting process (not at each node, like in &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0bf98f40bc311f4824763dea8c552bc0812d861" translate="yes" xml:space="preserve">
          <source>In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; as demonstrated in the following example that extract &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; vectors of unigram tokens from a subset of 20news:</source>
          <target state="translated">为了向预测或聚类模型提供文本数据，首先需要将文本转换为适合统计分析的数值向量。可以使用 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 的实用程序来实现此目的，如以下示例所示，该示例从20news的子集中提取unigram令牌的&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;向量：</target>
        </trans-unit>
        <trans-unit id="c3614fb1e15f18200960459d2e1c203458a6eae2" translate="yes" xml:space="preserve">
          <source>In order to fit linear models with those predictors it is therefore necessary to perform standard feature transformations as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a439a73e36b65ee0a94b3f1d9d89e3ac154697cf" translate="yes" xml:space="preserve">
          <source>In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:</source>
          <target state="translated">为了让第一个例子获得更快的执行时间,我们将在部分数据集上工作,在数据集的20个可用类别中,只有4个类别。</target>
        </trans-unit>
        <trans-unit id="da7edac191ef2f2a6bab6d167570c5dc3d626b83" translate="yes" xml:space="preserve">
          <source>In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.</source>
          <target state="translated">为了从一个小的数据集中学习良好的潜伏表征,我们通过对训练数据进行每个方向1个像素的线性移动,人工生成更多的标签数据。</target>
        </trans-unit>
        <trans-unit id="6983d2c6ff1cbf277ea5d9522b128070bfd0a615" translate="yes" xml:space="preserve">
          <source>In order to make the vectorizer =&amp;gt; transformer =&amp;gt; classifier easier to work with, &lt;code&gt;scikit-learn&lt;/code&gt; provides a &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; class that behaves like a compound classifier:</source>
          <target state="translated">为了使矢量化器=&amp;gt;变压器=&amp;gt;分类器更易于使用， &lt;code&gt;scikit-learn&lt;/code&gt; 提供了行为类似于复合分类器的&lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;类：</target>
        </trans-unit>
        <trans-unit id="5257e11193f291f6f81d5d2347e3cbb71ec9f310" translate="yes" xml:space="preserve">
          <source>In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.</source>
          <target state="translated">为了对文本文档进行机器学习,我们首先需要将文本内容转化为数字特征向量。</target>
        </trans-unit>
        <trans-unit id="7b973d24b18f4331d1cc68b945953f9c40c766fe" translate="yes" xml:space="preserve">
          <source>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support &lt;code&gt;predict_proba&lt;/code&gt; method):</source>
          <target state="translated">为了基于预测的类概率来预测类标签（VotingClassifier中的scikit-learn估计器必须支持 &lt;code&gt;predict_proba&lt;/code&gt; 方法）：</target>
        </trans-unit>
        <trans-unit id="a7ffbb7849ad7a74935991324e062c6b6722378d" translate="yes" xml:space="preserve">
          <source>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf&amp;ndash;idf transform.</source>
          <target state="translated">为了将计数特征重新加权为适合分类器使用的浮点值，通常使用tf&amp;ndash;idf变换。</target>
        </trans-unit>
        <trans-unit id="4707665df8a323c1a68b209bc6166b3798e4ea75" translate="yes" xml:space="preserve">
          <source>In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:</source>
          <target state="translated">为了用未来版本的scikit-learn重建一个类似的模型,应该沿着腌制的模型保存额外的元数据。</target>
        </trans-unit>
        <trans-unit id="168239ecf279021917cbfef805f1d7d711ae1c44" translate="yes" xml:space="preserve">
          <source>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.</source>
          <target state="translated">为了测试一个分类分数是否是有意义的,一种技术是在随机、换位、标签后重复分类程序。然后通过所获得的分数大于最初获得的分类分数的运行百分比给出p值。</target>
        </trans-unit>
        <trans-unit id="fdc8e1656ba1332f0933f9f656403151b15252d2" translate="yes" xml:space="preserve">
          <source>In other words, return an input X_original whose transform would be X.</source>
          <target state="translated">换句话说,返回一个输入X_original,其变换将是X。</target>
        </trans-unit>
        <trans-unit id="f84fbaf022a2c87e2f72b92c7b8059751d7f8963" translate="yes" xml:space="preserve">
          <source>In other words, we &lt;em&gt;decomposed&lt;/em&gt; matrix \(\mathbf{X}\).</source>
          <target state="translated">换句话说，我们&lt;em&gt;分解了&lt;/em&gt;矩阵\（\ mathbf {X} \）。</target>
        </trans-unit>
        <trans-unit id="573ad5780d66d8749d635925a4f90732aa002652" translate="yes" xml:space="preserve">
          <source>In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:</source>
          <target state="translated">特别是Rosenberg和Hirschberg(2007)为任何集群分配定义了以下两个理想目标:</target>
        </trans-unit>
        <trans-unit id="dafd8fff090495231531a6dce6a0d9bf23cd3c87" translate="yes" xml:space="preserve">
          <source>In particular in a &lt;strong&gt;supervised setting&lt;/strong&gt; it can be successfully combined with fast and scalable linear models to train &lt;strong&gt;document classifiers&lt;/strong&gt;, for instance:</source>
          <target state="translated">特别是在有&lt;strong&gt;监督的环境中，&lt;/strong&gt;它可以成功地与快速且可扩展的线性模型组合以训练&lt;strong&gt;文档分类器&lt;/strong&gt;，例如：</target>
        </trans-unit>
        <trans-unit id="b70db829e86f8b0b87ee4b4ea9165e2800cb135e" translate="yes" xml:space="preserve">
          <source>In particular the interrogative form &amp;ldquo;Is this&amp;rdquo; is only present in the last document:</source>
          <target state="translated">特别是疑问句形式&amp;ldquo;这是&amp;rdquo;仅出现在最后的文档中：</target>
        </trans-unit>
        <trans-unit id="48a72aaef5f57348c3c02ddfbd84f34663c56133" translate="yes" xml:space="preserve">
          <source>In particular we name:</source>
          <target state="translated">我们特别指出:</target>
        </trans-unit>
        <trans-unit id="32bae48b70c29501503578b88b61dfab45b0637c" translate="yes" xml:space="preserve">
          <source>In particular, \(\nu = 3/2\):</source>
          <target state="translated">特别是,(3/2)。</target>
        </trans-unit>
        <trans-unit id="204dd46cfb26952328568f02a630bc8ec2809e56" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in &lt;a href=&quot;../classes#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt;. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff34c019cd4067dd0b8a1a6d1536db31ff351b58" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="translated">特别是,截断的SVD工作在术语计数/tf-idf矩阵上,这些矩阵由sklearn.feature_extraction.text中的向量器返回。在这种情况下,它被称为潜在语义分析(LSA)。</target>
        </trans-unit>
        <trans-unit id="74d856933005d819af2a4b7d2ce24317b5186453" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.</source>
          <target state="translated">在实践中,当单个簇的结构是高度非凸的时候,或者更一般的情况下,当测量簇的中心和分布并不适合描述完整的簇时,光谱聚类是非常有用的。例如,当聚类是二维平面上的嵌套圆时。</target>
        </trans-unit>
        <trans-unit id="316ef03a3b1d8845e6fcfccde0af625da5037900" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21d1a0735c97d36c546ea3246065facc34b4f5a9" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance, when clusters are nested circles on the 2D plane.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5662b7bb73c6d21ae298513382716bd3fe526ba2" translate="yes" xml:space="preserve">
          <source>In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.</source>
          <target state="translated">在实践中,局部密度是由k个最近的邻居获得的。一个观测值的LOF分数等于他的k个最近邻居的平均局部密度和它自己的局部密度之比:一个正常的实例预计会有一个类似于其邻居的局部密度,而异常数据的局部密度会小得多。</target>
        </trans-unit>
        <trans-unit id="7ab811a62d63edef5c9695fca6cafd8cc266404c" translate="yes" xml:space="preserve">
          <source>In practice those estimates are stored as an attribute named &lt;code&gt;feature_importances_&lt;/code&gt; on the fitted model. This is an array with shape &lt;code&gt;(n_features,)&lt;/code&gt; whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</source>
          <target state="translated">实际上，这些估计值被存储为拟合模型上名为 &lt;code&gt;feature_importances_&lt;/code&gt; 的属性。这是一个形状为 &lt;code&gt;(n_features,)&lt;/code&gt; 的数组，其值是正数，总和为1.0。值越高，匹配特征对预测函数的贡献就越重要。</target>
        </trans-unit>
        <trans-unit id="2f91c327e74b19930bc0b8d2d9c2f5d99fe44af7" translate="yes" xml:space="preserve">
          <source>In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.</source>
          <target state="translated">在实际工作中,我们常常忽略分布的形状,只需将数据进行变换,将每个特征的均值去掉,然后用非常态特征除以其标准差,将数据居中。</target>
        </trans-unit>
        <trans-unit id="4c632dd9d37d8e850afe2fbbbdbddfedb108d119" translate="yes" xml:space="preserve">
          <source>In practice, \(\mu\) and \(\Sigma\) are replaced by some estimates. The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set and therefor, the corresponding Mahalanobis distances are. One would better have to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set and that the associated Mahalanobis distances accurately reflect the true organisation of the observations.</source>
          <target state="translated">实际上，\（\ mu \）和\（\ Sigma \）被一些估计值所代替。通常的协方差最大似然估计对数据集中异常值的存在非常敏感，因此相应的马氏距离是。最好使用健壮的协方差估计器来确保估计值可以抵抗数据集中的&amp;ldquo;错误&amp;rdquo;观测值，并且相关的Mahalanobis距离可以准确地反映观测值的真实组织。</target>
        </trans-unit>
        <trans-unit id="7f19bfe5f66f3783151b8147191d95599d9b587d" translate="yes" xml:space="preserve">
          <source>In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That&amp;rsquo;s why it can be useful to restart it several times.</source>
          <target state="translated">实际上，k-means算法非常快（是可用的最快的聚类算法之一），但它属于局部最小值。这就是为什么多次重新启动它会很有用。</target>
        </trans-unit>
        <trans-unit id="514529e761d628932a46cfab06e171128c270c12" translate="yes" xml:space="preserve">
          <source>In practice, whether parallelism is helpful at improving runtime depends on many factors. It is usually a good idea to experiment rather than assuming that increasing the number of workers is always a good thing. In some cases it can be highly detrimental to performance to run multiple copies of some estimators or functions in parallel (see oversubscription below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f131bcc125dc920f59e2c48cc0ec0622c3531f86" translate="yes" xml:space="preserve">
          <source>In practice, you will have to handle yourself the column data type. If you want some columns to be considered as &lt;code&gt;category&lt;/code&gt;, you will have to convert them into categorical columns. If you are using pandas, you can refer to their documentation regarding &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html&quot;&gt;Categorical data&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c017831824360a69856c6ccfffcd3f1b73574d4" translate="yes" xml:space="preserve">
          <source>In practise, a stacking predictor predict as good as the best predictor of the base layer and even sometimes outputperform it by combining the different strength of the these predictors. However, training a stacking predictor is computationally expensive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0aad0cfd8ffb7884222ad0979279d2b7ce81ef15" translate="yes" xml:space="preserve">
          <source>In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9809378b0338436bd7bbe8f2e2070a6272b570d" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples keywords &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="translated">在希望更加重视某些类或某些单独样本的 &lt;code&gt;class_weight&lt;/code&gt; ，可以使用class_weight和 &lt;code&gt;sample_weight&lt;/code&gt; 关键字。</target>
        </trans-unit>
        <trans-unit id="274191e11959a25ec702f3eb0728b40adf181870" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c88cb15e9453fb980c7d19e00322e852632f7bf2" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cae7f41bfdb577edc832687cbceee9865d3220c5" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.</source>
          <target state="translated">在随机森林中（请参见&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt;类），集合中的每棵树都是根据训练集中替换后的样本（即引导样本）构建的。此外，在树的构造过程中拆分节点时，选择的拆分不再是所有要素中的最佳拆分。取而代之的是，选取的分割是要素的随机子集中最佳的分割。由于这种随机性，森林的偏见通常会略有增加（相对于单个非随机树的偏见），但是由于求平均值，其方差也会减少，通常大于补偿偏见的增加，因此产生了一个整体更好的模型。</target>
        </trans-unit>
        <trans-unit id="eee63fdeeb00f1867cdf7e3f336a4276e1d12ae2" translate="yes" xml:space="preserve">
          <source>In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.</source>
          <target state="translated">在回归中,估计器的预期均方误差可以用偏差、方差和噪声来分解。在回归问题的平均数据集上,偏差项衡量估计器的预测与问题的最佳估计器(即贝叶斯模型)的预测的平均差异量。方差项衡量估计器的预测在问题的不同实例LS上拟合时的变异性。最后,噪声衡量误差中不可重复的部分,这是由于数据的变异性造成的。</target>
        </trans-unit>
        <trans-unit id="3eae88b0a075df5d4090eee16e911849fedcc7b3" translate="yes" xml:space="preserve">
          <source>In regression, the output remains as \(f(x)\); therefore, output activation function is just the identity function.</source>
          <target state="translated">在回归中,输出仍为/(f(x)/);因此,输出激活函数只是身份函数。</target>
        </trans-unit>
        <trans-unit id="253be6f032627aec5a3b2c4240659e2190b9fba2" translate="yes" xml:space="preserve">
          <source>In scikit-learn a random split into training and test sets can be quickly computed with the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; helper function. Let&amp;rsquo;s load the iris data set to fit a linear support vector machine on it:</source>
          <target state="translated">在scikit-learn中，可以使用&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;辅助函数快速将随机分为训练和测试集的部分进行计算。让我们加载虹膜数据集以使其适合线性支持向量机：</target>
        </trans-unit>
        <trans-unit id="bdcdb5bf0b220e10633a04e3b3d7b23fb832fcf9" translate="yes" xml:space="preserve">
          <source>In scikit-learn, an estimator for classification is a Python object that implements the methods &lt;code&gt;fit(X, y)&lt;/code&gt; and &lt;code&gt;predict(T)&lt;/code&gt;.</source>
          <target state="translated">在scikit-learn中，用于分类的估计量是一个Python对象，该对象实现了 &lt;code&gt;fit(X, y)&lt;/code&gt; 和 &lt;code&gt;predict(T)&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="a15700735758e7e1606b13cd4f276e5fe1e0ef96" translate="yes" xml:space="preserve">
          <source>In scikit-learn, bagging methods are offered as a unified &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt;&lt;code&gt;BaggingClassifier&lt;/code&gt;&lt;/a&gt; meta-estimator (resp. &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt;&lt;code&gt;BaggingRegressor&lt;/code&gt;&lt;/a&gt;), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, &lt;code&gt;max_samples&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt; control the size of the subsets (in terms of samples and features), while &lt;code&gt;bootstrap&lt;/code&gt; and &lt;code&gt;bootstrap_features&lt;/code&gt; control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting &lt;code&gt;oob_score=True&lt;/code&gt;. As an example, the snippet below illustrates how to instantiate a bagging ensemble of &lt;code&gt;KNeighborsClassifier&lt;/code&gt; base estimators, each built on random subsets of 50% of the samples and 50% of the features.</source>
          <target state="translated">在scikit-learn中，装袋方法以统一的&lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt; &lt;code&gt;BaggingClassifier&lt;/code&gt; &lt;/a&gt;元估计器（即&lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt; &lt;code&gt;BaggingRegressor&lt;/code&gt; &lt;/a&gt;）的形式提供，将用户指定的基本估计器以及指定绘制随机子集的策略的参数作为输入。特别地， &lt;code&gt;max_samples&lt;/code&gt; 和 &lt;code&gt;max_features&lt;/code&gt; 控制子集的大小（就样本和特征而言），而 &lt;code&gt;bootstrap&lt;/code&gt; 和 &lt;code&gt;bootstrap_features&lt;/code&gt; 控制是否绘制或替换时绘制样本和特征。当使用可用样本的子集时，可以通过设置 &lt;code&gt;oob_score=True&lt;/code&gt; 来估计袋外样本的泛化精度。例如，下面的代码段说明了如何实例化 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 基本估计器的装袋集合，每个估计器都基于50％的样本和50％的特征的随机子集。</target>
        </trans-unit>
        <trans-unit id="0848ab34fddbc88dcbfdd395d0519986e8d182eb" translate="yes" xml:space="preserve">
          <source>In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt;&lt;code&gt;shrunk_covariance&lt;/code&gt;&lt;/a&gt; method. Also, a shrunk estimator of the covariance can be fitted to data with a &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt;&lt;code&gt;ShrunkCovariance&lt;/code&gt;&lt;/a&gt; object and its &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt;&lt;code&gt;ShrunkCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Again, results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately.</source>
          <target state="translated">在scikit-learn中，可以使用&lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt; &lt;code&gt;shrunk_covariance&lt;/code&gt; &lt;/a&gt;方法将该转换（具有用户定义的收缩系数）直接应用于预先计算的协方差。同样，可以使用&lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt; &lt;code&gt;ShrunkCovariance&lt;/code&gt; &lt;/a&gt;对象及其&lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt; &lt;code&gt;ShrunkCovariance.fit&lt;/code&gt; &lt;/a&gt;方法将协方差的缩小估算器拟合到数据。同样，结果取决于数据是否居中，因此可能需要准确地使用 &lt;code&gt;assume_centered&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="77c9e3634b9d256acc307751b68c900b0b833a29" translate="yes" xml:space="preserve">
          <source>In single precision, &lt;code&gt;mean&lt;/code&gt; can be inaccurate:</source>
          <target state="translated">以单精度表示， &lt;code&gt;mean&lt;/code&gt; 可能不准确：</target>
        </trans-unit>
        <trans-unit id="640c5c337251b299605fe0c1704cd43d50fcda84" translate="yes" xml:space="preserve">
          <source>In some cases it&amp;rsquo;s not necessary to include higher powers of any single feature, but only the so-called &lt;em&gt;interaction features&lt;/em&gt; that multiply together at most \(d\) distinct features. These can be gotten from &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;.</source>
          <target state="translated">在某些情况下，没有必要包含任何单个特征的较高幂，而​​只需包含最多可将\（d \）个不同特征相乘的所谓&lt;em&gt;交互&lt;/em&gt;特征。这些可以从&lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; 中&lt;/a&gt;获得，设置为 &lt;code&gt;interaction_only=True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4976a8ab8df4ced9a6ceb9c8368b484dbc953550" translate="yes" xml:space="preserve">
          <source>In some cases, only interaction terms among features are required, and it can be gotten with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;:</source>
          <target state="translated">在某些情况下，仅需要要素之间的交互项，并且可以通过设置 &lt;code&gt;interaction_only=True&lt;/code&gt; 来获得：</target>
        </trans-unit>
        <trans-unit id="fc4d9d9d36cc2fd0e96fb6fded728410f07d4165" translate="yes" xml:space="preserve">
          <source>In some specific cases (when the code that is run in parallel releases the GIL), scikit-learn will indicate to &lt;code&gt;joblib&lt;/code&gt; that a multi-threading backend is preferable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b40a5cc0ca592bd5c1306138f251a81fe534579" translate="yes" xml:space="preserve">
          <source>In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)</source>
          <target state="translated">尽管它们的假设显然过于简单,但奈夫贝叶斯分类器在许多真实世界的情况下工作得相当好,著名的是文档分类和垃圾邮件过滤。它们需要少量的训练数据来估计必要的参数。(关于为什么天真贝叶斯工作得很好,以及在哪些类型的数据上工作得很好的理论原因,请参见下面的参考文献)。</target>
        </trans-unit>
        <trans-unit id="389828ed3005eb646a2fbe827835555cbdc74437" translate="yes" xml:space="preserve">
          <source>In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \(n - 1\) of the \(n\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.</source>
          <target state="translated">在精度方面,LOO作为测试误差的估计器,往往会导致高方差。直观地讲,由于每一个模型的建立都要用到折线样本中的(n-1/)个样本,所以由折线构建的模型彼此之间以及由整个训练集构建的模型实际上是相同的。</target>
        </trans-unit>
        <trans-unit id="36132aafc76511f4279f2a1765dcbaeb9d7a44b1" translate="yes" xml:space="preserve">
          <source>In terms of time and space complexity, Theil-Sen scales according to</source>
          <target state="translated">在时间和空间的复杂性方面,Theil-Sen的尺度是根据</target>
        </trans-unit>
        <trans-unit id="8cac8320893acecd46013a1cd740f5237cabb213" translate="yes" xml:space="preserve">
          <source>In that case, the model with 2 components and full covariance (which corresponds to the true generative model) is selected.</source>
          <target state="translated">在这种情况下,选择具有2个分量和全协方差的模型(相当于真正的生成模型)。</target>
        </trans-unit>
        <trans-unit id="56b0989d4ac400367ce631898b8b32a7aa114deb" translate="yes" xml:space="preserve">
          <source>In that way, we emphasize that the greater the variance of a feature, the larger the weight of the corresponding coefficient on the output, all else being equal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df23be83a828beae97b01644c9cebfd6ec568f81" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;smooth_idf=False&lt;/code&gt;, the &amp;ldquo;1&amp;rdquo; count is added to the idf instead of the idf&amp;rsquo;s denominator:</source>
          <target state="translated">在带有 &lt;code&gt;smooth_idf=False&lt;/code&gt; 的&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 中&lt;/a&gt;，将&amp;ldquo; 1&amp;rdquo;计数添加到idf而不是idf的分母：</target>
        </trans-unit>
        <trans-unit id="5bd53e1aa867b99daf4cb138797f33e24d9cfda1" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren&amp;rsquo;t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; 中&lt;/a&gt;，所有单位都是二进制随机单位。这意味着输入数据应该是二进制的，或者是介于0和1之间的实值，表示可见单位将打开或关闭的可能性。这是一个很好的字符识别模型，关注的是哪些像素处于活动状态，哪些像素未处于活动状态。对于自然场景的图像，由于背景，深度和相邻像素采用相同值的趋势而不再适合。</target>
        </trans-unit>
        <trans-unit id="729860dcb0b5963ed7d873f5d8718ecbded39d56" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the &lt;code&gt;l1&lt;/code&gt;. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling &lt;code&gt;C1&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;l1&lt;/code&gt; 的情况下，理论认为，由于 &lt;code&gt;l1&lt;/code&gt; 的偏差，不可能实现预测一致性（即在给定的假设下，所学习的估计量可以预测以及知道真实分布的模型）。但是，它确实说，通过缩放 &lt;code&gt;C1&lt;/code&gt; 可以在找到正确的非零参数集及其符号方面实现模型一致性。</target>
        </trans-unit>
        <trans-unit id="3ac113ba1d5ea8579f40e00fa885b5d980c4ba43" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; penalty case, the cross-validation-error correlates best with the test-error, when scaling our &lt;code&gt;C&lt;/code&gt; with the number of samples, &lt;code&gt;n&lt;/code&gt;, which can be seen in the first figure.</source>
          <target state="translated">在 &lt;code&gt;l1&lt;/code&gt; 罚金的情况下，当用样本数 &lt;code&gt;n&lt;/code&gt; 缩放 &lt;code&gt;C&lt;/code&gt; 时，交叉验证误差与测试误差最相关，这可以从第一个图中看出。</target>
        </trans-unit>
        <trans-unit id="4495b7082e60ec7cb3a7d3cf1946536697a0e6b3" translate="yes" xml:space="preserve">
          <source>In the above case, the classifier is fit on a 1d array of multiclass labels and the &lt;code&gt;predict()&lt;/code&gt; method therefore provides corresponding multiclass predictions. It is also possible to fit upon a 2d array of binary label indicators:</source>
          <target state="translated">在上述情况下，分类器适合一维多类标签数组，因此 &lt;code&gt;predict()&lt;/code&gt; 方法提供了相应的多类预测。也有可能适合二维标签指示符的二维数组：</target>
        </trans-unit>
        <trans-unit id="41d2181ce120b72b14a943b5e6f5608fe64d404d" translate="yes" xml:space="preserve">
          <source>In the above example, &lt;code&gt;char_wb&lt;/code&gt; analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The &lt;code&gt;char&lt;/code&gt; analyzer, alternatively, creates n-grams that span across words:</source>
          <target state="translated">在上面的示例中，使用了 &lt;code&gt;char_wb&lt;/code&gt; 分析器，该分析器仅从单词边界内的字符（每边填充空格）创建n-gram。所述 &lt;code&gt;char&lt;/code&gt; 分析器，可替代地，将创建的n-gram跨单词跨度：</target>
        </trans-unit>
        <trans-unit id="02dd6b844a6f6c7bb7b63318a0172b18e25d4984" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'city'&lt;/code&gt;). However, other transformers generally expect 2D data, and in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="translated">在上面的示例中，&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;期望将一维数组作为输入，因此将列指定为字符串（ &lt;code&gt;'city'&lt;/code&gt; ）。但是，其他转换器通常需要2D数据，在这种情况下，您需要将列指定为字符串列表（ &lt;code&gt;['city']&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="af32f30ac84780c46ec03071f0807a02a06be37e" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'title'&lt;/code&gt;). However, &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8be3ffb93abc4e32a08a3f61b709f6ed3e6124c4" translate="yes" xml:space="preserve">
          <source>In the above example-code, we firstly use the &lt;code&gt;fit(..)&lt;/code&gt; method to fit our estimator to the data and secondly the &lt;code&gt;transform(..)&lt;/code&gt; method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the &lt;code&gt;fit_transform(..)&lt;/code&gt; method as shown below, and as mentioned in the note in the previous section:</source>
          <target state="translated">在上面的示例代码中，我们首先使用 &lt;code&gt;fit(..)&lt;/code&gt; 方法使估计量适合数据，其次使用 &lt;code&gt;transform(..)&lt;/code&gt; 方法将计数矩阵转换为tf-idf表示形式。通过跳过冗余处理，可以将这两个步骤结合起来以更快地达到相同的最终结果。这是通过使用 &lt;code&gt;fit_transform(..)&lt;/code&gt; 方法完成的，如下所示，并且如上一节中的注释所述：</target>
        </trans-unit>
        <trans-unit id="370df873906104ebc3c5f7c3ad3752f50f2bb258" translate="yes" xml:space="preserve">
          <source>In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the &lt;a href=&quot;#nca-mathematical-formulation&quot;&gt;mathematical formulation&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc1fc9feffcc3ab7062a5968abf403f994be456d" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is more than 2, and that the document length is never zero. Likewise, we reject classes which have already been chosen. The documents that are assigned to both classes are plotted surrounded by two colored circles.</source>
          <target state="translated">在上述过程中,拒绝采样是用来确保n大于2,并且文档长度永远不会为零。同样,我们也会拒绝那些已经被选择的类。被分配到这两个类的文档被两个彩色圆圈包围绘制出来。</target>
        </trans-unit>
        <trans-unit id="35ac86e1f976c024d854c94dc1a07d9250df373b" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is never zero or more than &lt;code&gt;n_classes&lt;/code&gt;, and that the document length is never zero. Likewise, we reject classes which have already been chosen.</source>
          <target state="translated">在上述过程中，使用拒绝采样来确保n永远不为零或大于 &lt;code&gt;n_classes&lt;/code&gt; ，并且文档长度永远不为零。同样，我们拒绝已经选择的类。</target>
        </trans-unit>
        <trans-unit id="35b3eed71c5956697e4e941c9abda7fa7875d908" translate="yes" xml:space="preserve">
          <source>In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and \(fn\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as</source>
          <target state="translated">在二元(两类)情况下,\(tp/)、\(tn/)、\(fp/)和\(fn/)分别是真阳性、真阴性、假阳性和假阴性的数量,MCC定义为:1.</target>
        </trans-unit>
        <trans-unit id="8cf754386b9e93bff61012cc7eebd891fe098125" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores.</source>
          <target state="translated">在二进制情况下，平衡精度等于&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;灵敏度&lt;/a&gt;（真阳性率）和&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;特异性&lt;/a&gt;（真阴性率）的算术平均值，或者说是用二进制预测而不是分数在ROC曲线下的面积。</target>
        </trans-unit>
        <trans-unit id="43cd3b6bd763c03854427d73b603fdca48b2b30e" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a998ef5238ae7921fe9486295ae64f00deab3566" translate="yes" xml:space="preserve">
          <source>In the binary case, we can extract true positives, etc as follows:</source>
          <target state="translated">在二进制的情况下,我们可以提取真阳性等,如下。</target>
        </trans-unit>
        <trans-unit id="3e94daaa4e8fed019552e1789dc3caf5c267c82f" translate="yes" xml:space="preserve">
          <source>In the binary case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="193e8f9e646cb769a122ba34f156f35dc1f6d79e" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;(n_classes * (n_classes - 1) / 2, n_features)&lt;/code&gt; and &lt;code&gt;(n_classes *
(n_classes - 1) / 2)&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2482e7f51309cef70ec85538824011f95f0813b1" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of having a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; and &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="translated">在&amp;ldquo;一对一&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;的情况下，属性的布局要稍微复杂一些。在具有线性核的情况下，属性 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 分别具有 &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; 和 &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; 。这类似于上述用于&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;的布局，现在每一行都对应一个二进制分类器。类0到n的顺序是&amp;ldquo; 0对1&amp;rdquo;，&amp;ldquo; 0对2&amp;rdquo;，&amp;hellip;&amp;hellip;&amp;ldquo; 0对n&amp;rdquo;，&amp;ldquo; 1对2&amp;rdquo;，&amp;ldquo; 1对3&amp;rdquo;，&amp;ldquo; 1对n&amp;rdquo;，...。 。 。 &amp;ldquo; n-1 vs n&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="7befa9fe69dc29e17ce8c14ce1f24dcd596f25dc" translate="yes" xml:space="preserve">
          <source>In the case of Gaussian process classification, &amp;ldquo;one_vs_one&amp;rdquo; might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that &amp;ldquo;one_vs_one&amp;rdquo; does not support predicting probability estimates but only plain predictions. Moreover, note that &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.</source>
          <target state="translated">在高斯过程分类的情况下，&amp;ldquo; one_vs_one&amp;rdquo;在计算上可能更便宜，因为它必须解决仅涉及整个训练集的一个子集的许多问题，而不是解决整个数据集上较少的问题。由于高斯过程分类与数据集的大小成三次方缩放，因此这可能会更快。但是，请注意，&amp;ldquo; one_vs_one&amp;rdquo;不支持预测概率估计，而仅支持简单预测。此外，请注意，&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; &lt;/a&gt;尚未（尚未）在内部实现真正的多类Laplace逼近，但如上所述，它基于内部解决几个二进制分类任务的方法，这些任务使用&amp;ldquo;一对多&amp;rdquo;或&amp;ldquo;一对一&amp;rdquo;进行组合。</target>
        </trans-unit>
        <trans-unit id="35a5ada3d16c18d2778299b423e5960182d45740" translate="yes" xml:space="preserve">
          <source>In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios \(\log[P(y=k | X) / P(y=l | X)]\):</source>
          <target state="translated">在LDA的情况下,假设每个类的高斯指数共享相同的协方差矩阵。\对于所有的高斯马(k)来说,(高斯马_k=高斯马)。这就导致了线性决策面,这可以通过比较对数概率比率 \(\log[P(y=k | X)/P(y=l | X)]\)看出。</target>
        </trans-unit>
        <trans-unit id="9191afe7181654f4c123a5274615786e30f48b2b" translate="yes" xml:space="preserve">
          <source>In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">在QDA的情况下，对高斯的协方差矩阵\（\ Sigma_k \）没有假设，从而导致二次决策面。有关更多详细信息，请参见&lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="be50ffb98df62eb79c354d934aa76c5587bf8cba" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of &lt;code&gt;shape=[n_classes]&lt;/code&gt;. The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="translated">在多类分类的情况下 &lt;code&gt;coef_&lt;/code&gt; 是一个二维阵列 &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 是一个一维阵列 &lt;code&gt;shape=[n_classes]&lt;/code&gt; 。 &lt;code&gt;coef_&lt;/code&gt; 的第i行保存了第i类的OVA分类器的权重向量；类按升序索引（请参阅属性 &lt;code&gt;classes_&lt;/code&gt; ）。注意，原则上，由于它们允许创建概率模型，所以 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 和 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 更适用于一对多分类。</target>
        </trans-unit>
        <trans-unit id="a5109fe6449c3757593d6d14759e9857d15d95c4" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of shape (n_classes, n_features) and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of shape (n_classes,). The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1959581346965192dc91c946ef9926bceb1c51a" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.</source>
          <target state="translated">在多类分类的情况下,返回单类与其余分类器的平均对数边际似然。</target>
        </trans-unit>
        <trans-unit id="a45d03e82085c1e194b2b19d0700767439a7ac42" translate="yes" xml:space="preserve">
          <source>In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than the original ones.</source>
          <target state="translated">在one-hot/one-of-K编码的情况下,返回的是构建的特征名和值,而不是原始的。</target>
        </trans-unit>
        <trans-unit id="108b9e0576d5a78538771fb415a46ae76d1a6e26" translate="yes" xml:space="preserve">
          <source>In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. &lt;code&gt;BernoulliNB&lt;/code&gt; might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.</source>
          <target state="translated">在文本分类的情况下，可以使用单词出现向量（而不是单词计数向量）来训练和使用此分类器。 &lt;code&gt;BernoulliNB&lt;/code&gt; 可能在某些数据集上表现更好，尤其是那些文档较短的数据集。如果时间允许，建议评估两个模型。</target>
        </trans-unit>
        <trans-unit id="1ee37ddaa2e2b7fb0103fdddd162d9ad76a8f2dd" translate="yes" xml:space="preserve">
          <source>In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits zero through nine) on which we &lt;em&gt;fit&lt;/em&gt; an &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimator&lt;/a&gt; to be able to &lt;em&gt;predict&lt;/em&gt; the classes to which unseen samples belong.</source>
          <target state="translated">对于数字数据集，任务是在给定图像的情况下预测其代表的数字。我们为10种可能的类别（数字从零到九）中的每一个提供了样本，我们在其上&lt;em&gt;拟合&lt;/em&gt;了一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;估计量&lt;/a&gt;，以能够&lt;em&gt;预测&lt;/em&gt;未见样本所属的类别。</target>
        </trans-unit>
        <trans-unit id="0484e6facaecfeba6a4ff8e0552fa9a5ac1c52dd" translate="yes" xml:space="preserve">
          <source>In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if &lt;code&gt;method&lt;/code&gt; produces columns per class, as in {&amp;lsquo;decision_function&amp;rsquo;, &amp;lsquo;predict_proba&amp;rsquo;, &amp;lsquo;predict_log_proba&amp;rsquo;}. For &lt;code&gt;predict_proba&lt;/code&gt; this value is 0. In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases.</source>
          <target state="translated">如果训练部分中缺少一个或多个类，则如果 &lt;code&gt;method&lt;/code&gt; 为每个类生成列，则需要为该类的所有实例分配默认分数，例如{'decision_function'，'predict_proba'，'predict_log_proba'} 。对于 &lt;code&gt;predict_proba&lt;/code&gt; ,此值为0。为了确保有限的输出，在其他情况下，我们用dtype的最小有限浮点值来近似负无穷大。</target>
        </trans-unit>
        <trans-unit id="74ff5bfdda6b3e93f59169f7c22fc2d68fa3fcf3" translate="yes" xml:space="preserve">
          <source>In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear model&amp;rsquo;s decision_function method directly as the input of inverse_transform.</source>
          <target state="translated">在二进制标签是小数（概率）的情况下，inverse_transform选择具有最大值的类。通常，这允许将线性模型的Decision_function方法的输出直接用作inverse_transform的输入。</target>
        </trans-unit>
        <trans-unit id="e087c17fe61957d86c5cc0e9905f0323cd8dea87" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d852af4ac330c07eec96a31a9559a88b3b70655" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;code&gt;VotingClassifier&lt;/code&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="translated">在平局的情况下， &lt;code&gt;VotingClassifier&lt;/code&gt; 将根据升序排列选择类别。例如，在以下情况下</target>
        </trans-unit>
        <trans-unit id="84352a0fa93e8d3c09e63ba562819b09bff22e0e" translate="yes" xml:space="preserve">
          <source>In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:</source>
          <target state="translated">在棋盘的情况下,每行属于所有列簇,每列属于所有行簇。下面是这种结构的一个例子,每个双簇内的值的方差很小。</target>
        </trans-unit>
        <trans-unit id="6a06bacf0f95acd504bad8493dc228833ae72576" translate="yes" xml:space="preserve">
          <source>In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised.</source>
          <target state="translated">如果基于Fisher变换的95%置信区间跨度为零,则会发出警告。</target>
        </trans-unit>
        <trans-unit id="2da0fe066fd806cee05903c8f41b8c38bb726d66" translate="yes" xml:space="preserve">
          <source>In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.</source>
          <target state="translated">在下面的例子中,使用一个小的收缩阈值将模型的精度从0.81提高到0.82。</target>
        </trans-unit>
        <trans-unit id="ae3527cc8009043f3459062f8b3ac5f4c7cdc080" translate="yes" xml:space="preserve">
          <source>In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.</source>
          <target state="translated">在下图中,颜色表示簇成员资格,大圆圈表示算法发现的核心样本。较小的圆圈为非核心样本,它们仍然是聚类的一部分。此外,离群值用下面的黑点表示。</target>
        </trans-unit>
        <trans-unit id="cfa64cd986932f750b1c68de33ab3971d444bf3d" translate="yes" xml:space="preserve">
          <source>In the first column, first row the learning curve of a naive Bayes classifier is shown for the digits dataset. Note that the training score and the cross-validation score are both not very good at the end. However, the shape of the curve can be found in more complex datasets very often: the training score is very high at the beginning and decreases and the cross-validation score is very low at the beginning and increases. In the second column, first row we see the learning curve of an SVM with RBF kernel. We can see clearly that the training score is still around the maximum and the validation score could be increased with more training samples. The plots in the second row show the times required by the models to train with various sizes of training dataset. The plots in the third row show how much time was required to train the models for each training sizes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbb41405aaf08b233f5d5a9cb37143ee4833765d" translate="yes" xml:space="preserve">
          <source>In the first figure, we visualize the value of the kernel, i.e. the similarity of the sequences, using a colormap. Brighter color here indicates higher similarity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a915ebedbc9fc712ae336eb7409727fc370425dc" translate="yes" xml:space="preserve">
          <source>In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only.</source>
          <target state="translated">在第一行中,只使用萼片宽度和萼片长度特征建立分类器,在第二行中只使用花瓣长度和萼片长度特征建立分类器,在第三行中只使用花瓣宽度和花瓣长度特征建立分类器。</target>
        </trans-unit>
        <trans-unit id="974efdfc7b27c9e04d6e731c42ba40fb3d593ff5" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NearestNeighbors class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd9366b471cf174a5dc26260b1ee3e4775bd8a95" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1, 1, 1]:</source>
          <target state="translated">在下面的示例中，我们从一个代表数据集的数组构造一个NeighborsClassifier类，并询问谁是最接近[1，1，1]的点：</target>
        </trans-unit>
        <trans-unit id="f873541d5e32ccd97b454877a7265b9e862eeb9a" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="translated">在下面的示例中，我们从代表我们的数据集的数组构造NeighborsClassifier类，并询问谁是最接近[1,1,1]的点</target>
        </trans-unit>
        <trans-unit id="65f9f2f58e8b0fd298381aa88835a40b1607c17f" translate="yes" xml:space="preserve">
          <source>In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:</source>
          <target state="translated">在下图中,从双峰分布中抽取100个点,并显示了三种核的选择的核密度估计。</target>
        </trans-unit>
        <trans-unit id="a08df9eebc31ca3edc4756b37a4fdadef1454fe3" translate="yes" xml:space="preserve">
          <source>In the following plot, the maximum effective alpha value is removed, because it is the trivial tree with only one node.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24263a7351535bc4435386a661d4637b721eb5a0" translate="yes" xml:space="preserve">
          <source>In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).</source>
          <target state="translated">在下面的图中,我们看到一个函数/(f(x)=\cos (\frac{3}{2}\pi x)\)和该函数的一些噪声样本。我们使用三种不同的估计器来拟合该函数:具有1、4和15度多项式特征的线性回归。我们看到,第一个估计器最多只能对样本和真实函数提供较差的拟合,因为它太简单了(高偏差),第二个估计器几乎完美地拟合了它,最后一个估计器完美地拟合了训练数据,但不能很好地拟合真实函数,即它对变化的训练数据非常敏感(高方差)。</target>
        </trans-unit>
        <trans-unit id="605a0ccca31d97f6c29fd62c728a36908756654e" translate="yes" xml:space="preserve">
          <source>In the following section, we will interpret the coefficients of the model. While we do so, we should keep in mind that any conclusion we draw is about the model that we build, rather than about the true (real-world) generative process of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de54ef0bf525631d31eb0623dcd55f8a9ddc120b" translate="yes" xml:space="preserve">
          <source>In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.</source>
          <target state="translated">在下面的小节中,我们将对这些函数进行逐一描述,前面还有一些关于常见API和度量衡定义的说明。</target>
        </trans-unit>
        <trans-unit id="7549668dfe247eb9e0e172cc89f620a63604992b" translate="yes" xml:space="preserve">
          <source>In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; function by pointing it to the &lt;code&gt;20news-bydate-train&lt;/code&gt; sub-folder of the uncompressed archive folder.</source>
          <target state="translated">在下面的内容中，我们将使用内置的数据集加载器加载来自scikit-learn的20个新闻组。另外，也可以从网站手动下载数据集，并通过将&lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; &lt;/a&gt;函数指向未压缩存档文件夹的 &lt;code&gt;20news-bydate-train&lt;/code&gt; 子文件夹来使用它。</target>
        </trans-unit>
        <trans-unit id="2cb0b9817ecf09ea4893bb9df9d328ce75ef2d1b" translate="yes" xml:space="preserve">
          <source>In the following, &amp;ldquo;city&amp;rdquo; is a categorical attribute while &amp;ldquo;temperature&amp;rdquo; is a traditional numerical feature:</source>
          <target state="translated">在下文中，&amp;ldquo;城市&amp;rdquo;是分类属性，而&amp;ldquo;温度&amp;rdquo;是传统的数字特征：</target>
        </trans-unit>
        <trans-unit id="3e019b4cbe3ce7f1554fa08ce08898c560cb8a3b" translate="yes" xml:space="preserve">
          <source>In the following, we start a Python interpreter from our shell and then load the &lt;code&gt;iris&lt;/code&gt; and &lt;code&gt;digits&lt;/code&gt; datasets. Our notational convention is that &lt;code&gt;$&lt;/code&gt; denotes the shell prompt while &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; denotes the Python interpreter prompt:</source>
          <target state="translated">接下来，我们从外壳启动Python解释器，然后加载 &lt;code&gt;iris&lt;/code&gt; 和 &lt;code&gt;digits&lt;/code&gt; 数据集。我们的符号约定是 &lt;code&gt;$&lt;/code&gt; 表示shell提示符，而 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 表示Python解释器提示符：</target>
        </trans-unit>
        <trans-unit id="fe30ab8cfcc104ee94b5fb01793274570a377034" translate="yes" xml:space="preserve">
          <source>In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:</source>
          <target state="translated">在上式中,\(mathbf{b}\)和\(mathbf{c}\)分别是可见层和隐藏层的截距向量。模型的联合概率用能量来定义。</target>
        </trans-unit>
        <trans-unit id="e448c91ec6db1584eec64770c66e9fad7266b47b" translate="yes" xml:space="preserve">
          <source>In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66070e8da21856ec34fc0b507e5d723e9475a567" translate="yes" xml:space="preserve">
          <source>In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">在多类别和多标签的情况下，这是每个类别的F1分数的平均值，其权重取决于 &lt;code&gt;average&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="47604d7ff8f733fb868ecbca5a77b859a57fe5aa" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the Matthews correlation coefficient can be &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;defined&lt;/a&gt; in terms of a &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt;\(C\) for \(K\) classes. To simplify the definition consider the following intermediate variables:</source>
          <target state="translated">在多类情况下，可以根据con （K \）类的&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; &lt;/a&gt; \（C \）&lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;定义&lt;/a&gt; Matthews相关系数。为了简化定义，请考虑以下中间变量：</target>
        </trans-unit>
        <trans-unit id="b8d01a57cb617acafda7dfb6fdd570d7cb46be7c" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross- entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="translated">在多类情况下，如果将&amp;ldquo; multi_class&amp;rdquo;选项设置为&amp;ldquo; ovr&amp;rdquo;，则训练算法将使用&amp;ldquo;一对多休息&amp;rdquo;（OvR）方案；如果将&amp;ldquo; multi_class&amp;rdquo;选项设置为&amp;ldquo;多项式&amp;rdquo;，则将使用交叉熵损失。 '。（当前，只有&amp;ldquo; lbfgs&amp;rdquo;，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; newton-cg&amp;rdquo;求解器支持&amp;ldquo;多项式&amp;rdquo;选项。）</target>
        </trans-unit>
        <trans-unit id="df1f72c5b54cf7ce11968ba990ade83d8b80475a" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross-entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc004eb44e3b89b80f2b6796f35d04551e921cc" translate="yes" xml:space="preserve">
          <source>In the multiclass case:</source>
          <target state="translated">在多类情况下。</target>
        </trans-unit>
        <trans-unit id="d77598124fa8bad46b51e89decd127c4c83bc1e3" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators, where the first label set [0,1] has an error:</source>
          <target state="translated">在有二进制标签指示器的多标签情况下,第一个标签集[0,1]有错误。</target>
        </trans-unit>
        <trans-unit id="4557110f167a92f3d0af48823270c458eb95839b" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators:</source>
          <target state="translated">在多标签与二元标签指标的情况下。</target>
        </trans-unit>
        <trans-unit id="e898613ef6934f2809451f3fa3e427c4a4bd49ce" translate="yes" xml:space="preserve">
          <source>In the multilabel case, this calculates a confusion matrix per sample</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b2a464789db8978cf9674eef13e9bc3f4a4cf79" translate="yes" xml:space="preserve">
          <source>In the multilabel case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dbb497f0422701e359a6bab16a477019a81aa92" translate="yes" xml:space="preserve">
          <source>In the multilabel learning literature, OvR is also known as the binary relevance method.</source>
          <target state="translated">在多标签学习文献中,OvR也被称为二元相关性方法。</target>
        </trans-unit>
        <trans-unit id="b098a0ed402e179dee6b5de05c6210f893507735" translate="yes" xml:space="preserve">
          <source>In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by &lt;code&gt;transform&lt;/code&gt; will typically be dense.</source>
          <target state="translated">在新空间中，每个维度都是到群集中心的距离。请注意，即使X是稀疏的， &lt;code&gt;transform&lt;/code&gt; 返回的数组通常也会很密集。</target>
        </trans-unit>
        <trans-unit id="a90aa674a36fbb7c4f65ac18f5e6a5a0eb4c7bc3" translate="yes" xml:space="preserve">
          <source>In the official &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt&lt;/a&gt; this task is described as the &amp;ldquo;Restricted&amp;rdquo; task. As I am not sure as to implement the &amp;ldquo;Unrestricted&amp;rdquo; variant correctly, I left it as unsupported for now.</source>
          <target state="translated">在官方的&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt中，&lt;/a&gt;此任务称为&amp;ldquo;受限&amp;rdquo;任务。由于我不确定如何正确实施&amp;ldquo;不受限制&amp;rdquo;变体，因此暂时不支持该变体。</target>
        </trans-unit>
        <trans-unit id="5f046ff1a0210873f4b93ca532a15f1f001fc328" translate="yes" xml:space="preserve">
          <source>In the second figure, we show some regression result on a dataset of 6 sequences. Here we use the 1st, 2nd, 4th, and 5th sequences as the training set to make predictions on the 3rd and 6th sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="901c760a214576d30b4ced7bbcce771984b42233" translate="yes" xml:space="preserve">
          <source>In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.</source>
          <target state="translated">在我们在例子中看到的简单的一维问题中,很容易看到估计器是否存在偏差或方差。然而,在高维空间中,模型可能变得非常难以可视化。出于这个原因,使用下面描述的工具通常是有帮助的。</target>
        </trans-unit>
        <trans-unit id="bada0a0c8458a65354b2c23e7134e865cc4bf85c" translate="yes" xml:space="preserve">
          <source>In the single label multiclass case, the rows of the returned matrix sum to 1.</source>
          <target state="translated">在单标签多类的情况下,返回矩阵的行数相加为1。</target>
        </trans-unit>
        <trans-unit id="696912c12d134eed0fdc2e472302634288905dc5" translate="yes" xml:space="preserve">
          <source>In the small-samples situation, in which &lt;code&gt;n_samples&lt;/code&gt; is on the order of &lt;code&gt;n_features&lt;/code&gt; or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.</source>
          <target state="translated">在小样本情况下，其中 &lt;code&gt;n_samples&lt;/code&gt; 为 &lt;code&gt;n_features&lt;/code&gt; 或更小数量级，稀疏逆协方差估计器往往比收缩的协方差估计器更好。但是，在相反的情况下，或者对于非常相关的数据，它们可能在数值上不稳定。另外，与收缩估计器不同，稀疏估计器能够恢复非对角线结构。</target>
        </trans-unit>
        <trans-unit id="1c648bbbf8ddd269e7c36de7c1822a2705968fac" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;dump&lt;/code&gt; &amp;amp; &lt;code&gt;load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5f33dda5b96ece3c041650e9fa8db02e62bfd46" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="translated">在scikit-learn的特定情况下，最好使用joblib代替pickle（ &lt;code&gt;joblib.dump&lt;/code&gt; 和 &lt;code&gt;joblib.load&lt;/code&gt; ），这在内部携带大型numpy数组的对象上效率更高，这通常适合于scikit-学习估计器，但只能将其腌制到磁盘而不是字符串中：</target>
        </trans-unit>
        <trans-unit id="4d7c6f4a78fb7d42f5b6b076760e4c0fb27e052c" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be more interesting to use joblib&amp;rsquo;s replacement for pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on big data but it can only pickle to the disk and not to a string:</source>
          <target state="translated">在scikit-learn的特定情况下，使用joblib代替pickle（ &lt;code&gt;joblib.dump&lt;/code&gt; 和 &lt;code&gt;joblib.load&lt;/code&gt; ）可能会更有趣，这在大数据上效率更高，但只能在磁盘而不是字符串中进行酸洗。 ：</target>
        </trans-unit>
        <trans-unit id="6bc18b31ba734fa9a1f609c8898b12e640d531fb" translate="yes" xml:space="preserve">
          <source>In the statistics community, it is common practice to perform multiple imputations, generating, for example, &lt;code&gt;m&lt;/code&gt; separate imputations for a single feature matrix. Each of these &lt;code&gt;m&lt;/code&gt; imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The &lt;code&gt;m&lt;/code&gt; final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3fa57071e687f0bc6b84bf63b957df19fb1d89e" translate="yes" xml:space="preserve">
          <source>In the third figure, we demonstrate a classification model by training on 6 sequences and make predictions on another 5 sequences. The ground truth here is simply whether there is at least one &amp;lsquo;A&amp;rsquo; in the sequence. Here the model makes four correct classifications and fails on one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e8940d2e745f9a24bd23b0a1547dcf715a870bb" translate="yes" xml:space="preserve">
          <source>In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification.</source>
          <target state="translated">在全部特征集中,只有前4个特征是显著的。我们可以看到,它们的单变量特征选择得分最高。SVM对其中的一个特征赋予了很大的权重,但同时也选择了很多非信息特征。在SVM之前应用单变量特征选择,可以增加SVM归属于重要特征的权重,从而将提高分类效果。</target>
        </trans-unit>
        <trans-unit id="5cab08ab26978fd8cb0ee3262a21c03baca4d379" translate="yes" xml:space="preserve">
          <source>In the transformed &lt;code&gt;X&lt;/code&gt;, the first column is the encoding of the feature with categories &amp;ldquo;male&amp;rdquo;/&amp;rdquo;female&amp;rdquo;, while the remaining 6 columns is the encoding of the 2 features with respectively 3 categories each.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3dd53135ff49dbe7e421a248bf614a3ea8f0d5e" translate="yes" xml:space="preserve">
          <source>In the vector quantization literature, &lt;code&gt;cluster_centers_&lt;/code&gt; is called the code book and each value returned by &lt;code&gt;predict&lt;/code&gt; is the index of the closest code in the code book.</source>
          <target state="translated">在矢量量化文献中， &lt;code&gt;cluster_centers_&lt;/code&gt; 被称为代码簿， &lt;code&gt;predict&lt;/code&gt; 返回的每个值都是代码簿中最接近的代码的索引。</target>
        </trans-unit>
        <trans-unit id="71ea1d0e6870ae14110d577f25dbcd29b63431c3" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a63d5086bf9614df56a7612405271e0122a8145" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="translated">O. Ledoit和M. Wolf 在其2004年的论文中&lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;提出了一个计算最佳收缩系数\（\ alpha \）的公式，该公式使估计和实际协方差矩阵之间的均方误差最小。</target>
        </trans-unit>
        <trans-unit id="ee9767309b3df05ebf7c392a0bb4e8915eee3d46" translate="yes" xml:space="preserve">
          <source>In these settings, the &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; approach solves the problem know as &amp;lsquo;normalized graph cuts&amp;rsquo;: the image is seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts defining regions while minimizing the ratio of the gradient along the cut, and the volume of the region.</source>
          <target state="translated">在这些设置中，&lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;光谱聚类&lt;/a&gt;方法解决了称为&amp;ldquo;归一化图割&amp;rdquo;的问题：将图像视为已连接体素的图，并且光谱聚类算法相当于选择定义区域的图割，同时将沿切割，以及该区域的体积。</target>
        </trans-unit>
        <trans-unit id="e2e4475ec0999dd975ba681e19179d817d6681ae" translate="yes" xml:space="preserve">
          <source>In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.</source>
          <target state="translated">在这种情况下,我们想知道在特定组上训练的模型是否能很好地泛化到未见组。为了衡量这一点,我们需要确保验证折中的所有样本都来自于配对训练折中完全没有代表的群体。</target>
        </trans-unit>
        <trans-unit id="8e6586aaac37d3a887b7276aee6797fdd6471b11" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;X_test&lt;/code&gt; are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:</source>
          <target state="translated">在这种情况下，保证 &lt;code&gt;X_train&lt;/code&gt; 和 &lt;code&gt;X_test&lt;/code&gt; 具有相同数量的功能。获得相同结果的另一种方法是修复功能数量：</target>
        </trans-unit>
        <trans-unit id="c94c921d6a6a8582b29da8ef5a3a44fe1ea80a89" translate="yes" xml:space="preserve">
          <source>In this case, the classifier is fit upon instances each assigned multiple labels. The &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt; is used to binarize the 2d array of multilabels to &lt;code&gt;fit&lt;/code&gt; upon. As a result, &lt;code&gt;predict()&lt;/code&gt; returns a 2d array with multiple predicted labels for each instance.</source>
          <target state="translated">在这种情况下，分类器适合每个分配了多个标签的实例。所述&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; &lt;/a&gt;用于multilabels的2D阵列二进制化到 &lt;code&gt;fit&lt;/code&gt; 时。结果， &lt;code&gt;predict()&lt;/code&gt; 返回一个二维数组，每个实例带有多个预测标签。</target>
        </trans-unit>
        <trans-unit id="4367a423584d0b6ba622189fa17b21bb3e206f2c" translate="yes" xml:space="preserve">
          <source>In this case, the cross-validation retained the same ratio of classes across each CV split. Next we&amp;rsquo;ll visualize this behavior for a number of CV iterators.</source>
          <target state="translated">在这种情况下，交叉验证在每个CV划分中保留相同的类比例。接下来，我们将可视化许多CV迭代器的行为。</target>
        </trans-unit>
        <trans-unit id="d121f450bc55250670235f93c8cd2083eb40a561" translate="yes" xml:space="preserve">
          <source>In this context, we can define the notions of precision, recall and F-measure:</source>
          <target state="translated">在这种情况下,我们可以定义精确性、召回率和F-measure的概念。</target>
        </trans-unit>
        <trans-unit id="308ef10a0e6fa4feab74c9fff8eae0756aa171db" translate="yes" xml:space="preserve">
          <source>In this dataset, each sample corresponds to an insurance policy, i.e. a contract within an insurance company and an individual (policyholder). Available features include driver age, vehicle age, vehicle power, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b69830262e95394b14f5a754f7290cc7372dc2b1" translate="yes" xml:space="preserve">
          <source>In this dataset, each sample corresponds to an insurance policy. Available features include driver age, vehicle age, vehicle power, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b7e5d4a758a26d1b659ba54387246d5cebcf12f" translate="yes" xml:space="preserve">
          <source>In this example the dependent variable Y is set as a function of the input features: y = X*w + c. The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant.</source>
          <target state="translated">在这个例子中,因变量Y被设置为输入特征的函数:y=X*w+c。系数向量w是从正态分布中随机抽取的,而偏置项c被设置为一个常数。</target>
        </trans-unit>
        <trans-unit id="5d530c717737ac885c81ddc70c9c4fe51f2f2e42" translate="yes" xml:space="preserve">
          <source>In this example the silhouette analysis is used to choose an optimal value for &lt;code&gt;n_clusters&lt;/code&gt;. The silhouette plot shows that the &lt;code&gt;n_clusters&lt;/code&gt; value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.</source>
          <target state="translated">在此示例中，轮廓分析用于为 &lt;code&gt;n_clusters&lt;/code&gt; 选择最佳值。轮廓图显示，对于给定的数据， &lt;code&gt;n_clusters&lt;/code&gt; 值3、5和6是一个不好的选择，这是因为存在轮廓轮廓分数低于平均水平的聚类，并且轮廓图的大小波动很大。在确定2到4之间时，轮廓分析更为矛盾。</target>
        </trans-unit>
        <trans-unit id="0b7aa73d7d4561b0e25989b4fff6f5d53474b724" translate="yes" xml:space="preserve">
          <source>In this example we compare some estimators for the purpose of missing feature imputation with &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="550c896ceafe31d2e76547c4031642097a79581f" translate="yes" xml:space="preserve">
          <source>In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.</source>
          <target state="translated">在这个例子中,我们从运行时间和结果质量方面比较了K-means的各种初始化策略。</target>
        </trans-unit>
        <trans-unit id="d9bed364e1f96090d42e72d8ad4e31b8f81dfc1d" translate="yes" xml:space="preserve">
          <source>In this example we prefer the &lt;code&gt;elasticnet&lt;/code&gt; penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the &lt;code&gt;l1_ratio&lt;/code&gt; parameter (in combination with the regularization strength &lt;code&gt;alpha&lt;/code&gt;) to control this tradeoff.</source>
          <target state="translated">在此示例中，我们首选 &lt;code&gt;elasticnet&lt;/code&gt; 惩罚，因为它通常是模型紧凑性和预测能力之间的良好折衷。还可以进一步调整 &lt;code&gt;l1_ratio&lt;/code&gt; 参数（与正则化强度 &lt;code&gt;alpha&lt;/code&gt; 组合）以控制这一折衷。</target>
        </trans-unit>
        <trans-unit id="5f4ca84332e1fc2168e90c43c86e1d784ebd5f8c" translate="yes" xml:space="preserve">
          <source>In this example we see how to robustly fit a linear model to faulty data using the RANSAC algorithm.</source>
          <target state="translated">在这个例子中,我们将看到如何使用RANSAC算法对有问题的数据进行稳健的线性模型拟合。</target>
        </trans-unit>
        <trans-unit id="8b2f8ba713590c7461bca4df745b9c8578a9ee4a" translate="yes" xml:space="preserve">
          <source>In this example we will illustrate both approaches. We start by defining a few helper functions for loading the data and visualizing results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cb7bd690b6a99c0206d146b05cd9c20c5a47dca" translate="yes" xml:space="preserve">
          <source>In this example we will investigate different imputation techniques:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38fc37287fc51222e73dd7c83e6c92e563107ff6" translate="yes" xml:space="preserve">
          <source>In this example you might try to:</source>
          <target state="translated">在这个例子中,你可以尝试:</target>
        </trans-unit>
        <trans-unit id="8a61e0fa0737723bbfe9d0174ce3aad285419f4d" translate="yes" xml:space="preserve">
          <source>In this example, &lt;code&gt;X&lt;/code&gt; is &lt;code&gt;float32&lt;/code&gt;, which is cast to &lt;code&gt;float64&lt;/code&gt; by &lt;code&gt;fit_transform(X)&lt;/code&gt;.</source>
          <target state="translated">在此示例中， &lt;code&gt;X&lt;/code&gt; 为 &lt;code&gt;float32&lt;/code&gt; ，它由 &lt;code&gt;fit_transform(X)&lt;/code&gt; 强制转换为 &lt;code&gt;float64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2812da8873763c11175cae962f9ab9000ab381c4" translate="yes" xml:space="preserve">
          <source>In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.</source>
          <target state="translated">在这个例子中,生成了一个具有连接圆的图像,并使用光谱聚类来分离圆。</target>
        </trans-unit>
        <trans-unit id="79a59d7ef51f3f34cd5dc94073ebe194acbc66c6" translate="yes" xml:space="preserve">
          <source>In this example, both modeling approaches yield comparable performance metrics. For implementation reasons, the percentage of explained variance \(D^2\) is not available for the product model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69af0b849be70a0524a821dde21a609feb16811a" translate="yes" xml:space="preserve">
          <source>In this example, pixels are represented in a 3D-space and K-means is used to find 64 color clusters. In the image processing literature, the codebook obtained from K-means (the cluster centers) is called the color palette. Using a single byte, up to 256 colors can be addressed, whereas an RGB encoding requires 3 bytes per pixel. The GIF file format, for example, uses such a palette.</source>
          <target state="translated">在这个例子中,像素在3D空间中表示,K-means被用来寻找64个颜色簇。在图像处理文献中,由K-means(簇中心)得到的代码本称为调色板。使用一个字节,最多可寻址256种颜色,而RGB编码每像素需要3个字节。例如,GIF文件格式就使用了这样的调色板。</target>
        </trans-unit>
        <trans-unit id="2f0fb947da0f2bfc5faf32b771a3cb10ff049eda" translate="yes" xml:space="preserve">
          <source>In this example, the numeric data is standard-scaled after mean-imputation, while the categorical data is one-hot encoded after imputing missing values with a new category (&lt;code&gt;'missing'&lt;/code&gt;).</source>
          <target state="translated">在此示例中，数字数据在均值输入后进行标准缩放，而分类数据在使用新类别（ &lt;code&gt;'missing'&lt;/code&gt; ）插入缺失值后进行一次热编码。</target>
        </trans-unit>
        <trans-unit id="354a556d83cef273107f176ebec9db1b19a5c757" translate="yes" xml:space="preserve">
          <source>In this example, the sinusoid is approximated by a polynomial using different pairs of initial values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d88656bc2e040320cf9595554acac12be98f916c" translate="yes" xml:space="preserve">
          <source>In this example, we compare the estimation errors that are made when using various types of location and covariance estimates on contaminated Gaussian distributed data sets:</source>
          <target state="translated">在这个例子中,我们比较了在污染的高斯分布数据集上使用各种类型的位置和协方差估计时的估计误差。</target>
        </trans-unit>
        <trans-unit id="26a8e8ae6383655dcfc18bd00f71528218aa360e" translate="yes" xml:space="preserve">
          <source>In this example, we compute the permutation importance on the Wisconsin breast cancer dataset using &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; can easily get about 97% accuracy on a test dataset. Because this dataset contains multicollinear features, the permutation importance will show that none of the features are important. One approach to handling multicollinearity is by performing hierarchical clustering on the features&amp;rsquo; Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b7bcaf87ef3b0730f7083836942b0b038810927" translate="yes" xml:space="preserve">
          <source>In this example, we give an overview of the &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt;. Two examples illustrate the benefit of transforming the targets before learning a linear regression model. The first example uses synthetic data while the second example is based on the Boston housing data set.</source>
          <target state="translated">在此示例中，我们概述了&lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt;。两个示例说明了在学习线性回归模型之前转换目标的好处。第一个示例使用合成数据，而第二个示例基于Boston住房数据集。</target>
        </trans-unit>
        <trans-unit id="5f4a1f2d68c8f41cbf437ea7e001b2f1285b3f2f" translate="yes" xml:space="preserve">
          <source>In this example, we illustrate the use case in which different regressors are stacked together and a final linear penalized regressor is used to output the prediction. We compare the performance of each individual regressor with the stacking strategy. Stacking slightly improves the overall performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd9410f53a0f1d1aa2f5ff77c7bafaf9751d4c08" translate="yes" xml:space="preserve">
          <source>In this example, we set the value of &lt;code&gt;gamma&lt;/code&gt; manually. To find good values for these parameters, we can use tools such as &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; and &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;.</source>
          <target state="translated">在此示例中，我们手动设置了 &lt;code&gt;gamma&lt;/code&gt; 值。要为这些参数找到合适的值，我们可以使用诸如&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;网格搜索&lt;/a&gt;和&lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;交叉验证之类的工具&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="be5803bd91839824804bc4aadf2784b6ed10723a" translate="yes" xml:space="preserve">
          <source>In this example, we will compare the impurity-based feature importance of &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; with the permutation importance on the titanic dataset using &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt;. We will show that the impurity-based feature importance can inflate the importance of numerical features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f66533d22104292e30bad75824b906ee4fc1acd9" translate="yes" xml:space="preserve">
          <source>In this example, we will construct display objects, &lt;a href=&quot;../../modules/generated/sklearn.metrics.confusionmatrixdisplay#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.metrics.roccurvedisplay#sklearn.metrics.RocCurveDisplay&quot;&gt;&lt;code&gt;RocCurveDisplay&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.metrics.precisionrecalldisplay#sklearn.metrics.PrecisionRecallDisplay&quot;&gt;&lt;code&gt;PrecisionRecallDisplay&lt;/code&gt;&lt;/a&gt; directly from their respective metrics. This is an alternative to using their corresponding plot functions when a model&amp;rsquo;s predictions are already computed or expensive to compute. Note that this is advanced usage, and in general we recommend using their respective plot functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e0782ea6d7859077c07d60aaa0a30b1c4373f50" translate="yes" xml:space="preserve">
          <source>In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.</source>
          <target state="translated">在这个图中,你可以看到一个SVM在不同的内核参数gamma值下的训练得分和验证得分。对于非常低的gamma值,你可以看到训练得分和验证得分都很低。这就是所谓的欠拟合。中等的gamma值会导致两个分数都很高,即分类器的性能相当好。如果gamma值过高,分类器就会过拟合,这意味着训练得分很好,但验证得分很差。</target>
        </trans-unit>
        <trans-unit id="2c39a03080473177a8509645110953edafebbd76" translate="yes" xml:space="preserve">
          <source>In this scheme, features and samples are defined as follows:</source>
          <target state="translated">在本方案中,特征和样本定义如下。</target>
        </trans-unit>
        <trans-unit id="5941fbb58c226f551ff80660bcd51a84bcc2bae1" translate="yes" xml:space="preserve">
          <source>In this section we will see how to:</source>
          <target state="translated">在本节中,我们将看到如何。</target>
        </trans-unit>
        <trans-unit id="6ce5845b6414a0cfccffc603f3efdd4b47c7ce4b" translate="yes" xml:space="preserve">
          <source>In this section, we introduce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; vocabulary that we use throughout scikit-learn and give a simple learning example.</source>
          <target state="translated">在本节中，我们介绍了在整个scikit-learn中使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;机器学习&lt;/a&gt;词汇，并给出了一个简单的学习示例。</target>
        </trans-unit>
        <trans-unit id="e7b54ae8e73f20fa370a273bbb52814367b82582" translate="yes" xml:space="preserve">
          <source>In this snippet we make use of a &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; coupled with &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; to evaluate feature importances and select the most relevant features. Then, a &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt;&lt;/a&gt; is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt; examples for more details.</source>
          <target state="translated">在这个片段中，我们使用一个的&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;加上&lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt;评估功能重要度，并选择最相关的功能。然后，在转换后的输出上训练&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt; &lt;/a&gt;，即仅使用相关特征。您可以使用其他功能选择方法以及分类器执行类似的操作，这些分类器提供了一种评估功能的重要性的方法。有关更多详细信息，请参见&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;示例。</target>
        </trans-unit>
        <trans-unit id="87aee0924ee2a28e2d573efd8e050c2a5c1632a3" translate="yes" xml:space="preserve">
          <source>In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n \}\). How can this dataset be described mathematically? A very simple &lt;code&gt;continuous latent variable&lt;/code&gt; model for \(X\) is</source>
          <target state="translated">在无监督学习中，我们只有一个数据集\（X = \ {x_1，x_2，\ dots，x_n \} \）。如何用数学方式描述此数据集？\（X \）的一个非常简单的 &lt;code&gt;continuous latent variable&lt;/code&gt; 模型是</target>
        </trans-unit>
        <trans-unit id="a1efe7e891f38316d324e0fe7b8b483308bcbebf" translate="yes" xml:space="preserve">
          <source>Includes values in confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c89a6ca6f29b888687c7afd577a282b5b95a2be5" translate="yes" xml:space="preserve">
          <source>Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as &lt;em&gt;data leakage&lt;/em&gt;), for example in the case of scalers or imputing missing values.</source>
          <target state="translated">将测试数据中的统计信息合并到预处理器中，会使交叉验证得分不可靠（称为&lt;em&gt;数据泄漏&lt;/em&gt;），例如在定标器或估算缺失值的情况下。</target>
        </trans-unit>
        <trans-unit id="4c33f1e1286c254eaae3fbe03197d5578f08f56a" translate="yes" xml:space="preserve">
          <source>Increasing &lt;code&gt;max_depth&lt;/code&gt; for AdaBoost lowers the standard deviation of the scores (but the average score does not improve).</source>
          <target state="translated">增加AdaBoost的 &lt;code&gt;max_depth&lt;/code&gt; 会降低分数的标准偏差（但平均分数不会提高）。</target>
        </trans-unit>
        <trans-unit id="e79b1358981354168a853701629e2643ba45bf93" translate="yes" xml:space="preserve">
          <source>Increasing false positive rates such that element i is the false positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">增加假阳性率，使得元素i是得分&amp;gt; =阈值[i]的预测的假阳性率。</target>
        </trans-unit>
        <trans-unit id="3ca08d3a2216068596512fa76cc1f85e2464a3a8" translate="yes" xml:space="preserve">
          <source>Increasing thresholds on the decision function used to compute precision and recall.</source>
          <target state="translated">提高用于计算精度和召回的决策函数的阈值。</target>
        </trans-unit>
        <trans-unit id="7ae5f53b337e575381bac1d47d2d4a4d2e4839b6" translate="yes" xml:space="preserve">
          <source>Increasing true positive rates such that element i is the true positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">增加真实肯定率，使得元素i是得分&amp;gt; =阈值[i]的预测的真实肯定率。</target>
        </trans-unit>
        <trans-unit id="54206634ab03f8962d59d7c24e12c85ebd45b5e1" translate="yes" xml:space="preserve">
          <source>Incremental PCA</source>
          <target state="translated">增量PCA</target>
        </trans-unit>
        <trans-unit id="0254682de6b7d13bd44669747bb093c1dca18b21" translate="yes" xml:space="preserve">
          <source>Incremental Principal Component Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acaf3165fc4e0ddec759b9648ee12eed48089691" translate="yes" xml:space="preserve">
          <source>Incremental fit on a batch of samples.</source>
          <target state="translated">对一批样品进行增量拟合。</target>
        </trans-unit>
        <trans-unit id="a79a34aec33f8c8b084e4316cf8e243a4d6601e6" translate="yes" xml:space="preserve">
          <source>Incremental fit with X.</source>
          <target state="translated">与X的增量配合。</target>
        </trans-unit>
        <trans-unit id="87210470540ea5af2ee40f330fdeea4017f1c0aa" translate="yes" xml:space="preserve">
          <source>Incremental fit with X. All of X is processed as a single batch.</source>
          <target state="translated">与X进行增量拟合,将X全部作为单批处理。</target>
        </trans-unit>
        <trans-unit id="5b9d567927b0a80924b0a28fdea6cf19b23d2e57" translate="yes" xml:space="preserve">
          <source>Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.</source>
          <target state="translated">增量主成分分析(IPCA)通常被用来替代主成分分析(PCA),当需要分解的数据集太大而无法放入内存时。IPCA使用与输入数据样本数量无关的内存量为输入数据建立一个低阶近似。它仍然依赖于输入数据特征,但改变批次大小可以控制内存的使用。</target>
        </trans-unit>
        <trans-unit id="66088e706ece2d903ed2071fa2d42be9315774b6" translate="yes" xml:space="preserve">
          <source>Incremental principal components analysis (IPCA).</source>
          <target state="translated">增量主成分分析(IPCA)。</target>
        </trans-unit>
        <trans-unit id="ef7722207a6c2343d08e45f401cd00ccd19381c7" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data.</source>
          <target state="translated">增加模型与数据的拟合度。</target>
        </trans-unit>
        <trans-unit id="505bf67b8aa7cec37d64a9ce9b03d73f70b38b8b" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">增量拟合模型与数据。为每个输出变量拟合一个单独的模型。</target>
        </trans-unit>
        <trans-unit id="e7353e5363cf13ee1c3efdc144e75ff650f6c2d1" translate="yes" xml:space="preserve">
          <source>Incrementally trained logistic regression (when given the parameter &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e12e704fa3eb16be83d58c2167c9c4f83379a1d" translate="yes" xml:space="preserve">
          <source>Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.</source>
          <target state="translated">事实上,许多估计器在设计时都假设每个特征的值接近于零,或者更重要的是,所有特征的变化都在可比的范围内。特别是,基于度量和基于梯度的估计器通常假设近似于标准化的数据(单位方差的中心特征)。一个明显的例外是基于决策树的估计器,它对数据的任意缩放是稳健的。</target>
        </trans-unit>
        <trans-unit id="9cde6c3dae7189a85444fb86f682cb4ecd226cef" translate="yes" xml:space="preserve">
          <source>Indeed, from the plot above the most important factor in determining WAGE appears to be the variable UNION, even if our intuition might tell us that variables like EXPERIENCE should have more impact.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7933c6d72de999f40e22d3286d9c782fd636305b" translate="yes" xml:space="preserve">
          <source>Independent Component Analysis: ICA</source>
          <target state="translated">独立成分分析。ICA</target>
        </trans-unit>
        <trans-unit id="d170598045cdc9e2df037718a96d1706ed03e640" translate="yes" xml:space="preserve">
          <source>Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;Fast ICA&lt;/code&gt;&lt;/a&gt; algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.</source>
          <target state="translated">独立分量分析将多变量信号分离为最大独立的加性子分量。它是使用scikit-learn使用&lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;Fast ICA&lt;/code&gt; &lt;/a&gt;算法实现的。通常，ICA并不用于降低尺寸，而是用于分离叠加的信号。由于ICA模型不包含噪声项，因此对于正确的模型，必须应用白化。这可以在内部使用whiten参数完成，也可以使用PCA变体之一手动完成。</target>
        </trans-unit>
        <trans-unit id="420bdf88d0c37e0c49c684e7be83be0c3065749b" translate="yes" xml:space="preserve">
          <source>Independent component analysis, a latent variable model with non-Gaussian latent variables.</source>
          <target state="translated">独立成分分析,是一种非高斯潜变量的潜变量模型。</target>
        </trans-unit>
        <trans-unit id="e81fd2ba1ed4351b51becec6b3e044be03272d29" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel.</source>
          <target state="translated">多/西格慕核的独立参数。</target>
        </trans-unit>
        <trans-unit id="75b2e172573134b992419919380eaa4d379125c8" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel. 0 by default.</source>
          <target state="translated">在 poly/sigmoid 内核中的独立参数,默认为 0。默认为0。</target>
        </trans-unit>
        <trans-unit id="93ccb8f475af3ead0828a4d704d80fd7c1bcca2a" translate="yes" xml:space="preserve">
          <source>Independent term in decision function.</source>
          <target state="translated">决策函数中的独立项:</target>
        </trans-unit>
        <trans-unit id="d60b4ce63cb13d9b546e71bb468305b122e1ff12" translate="yes" xml:space="preserve">
          <source>Independent term in decision function. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="translated">决策功能中的独立术语。如果 &lt;code&gt;fit_intercept = False&lt;/code&gt; 则设置为0.0 。</target>
        </trans-unit>
        <trans-unit id="62ca36e367478a373b265bf6692ac1dd0b87c736" translate="yes" xml:space="preserve">
          <source>Independent term in kernel function. It is only significant in &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">内核函数中的独立术语。它仅在&amp;ldquo; poly&amp;rdquo;和&amp;ldquo; Sigmoid&amp;rdquo;中有意义。</target>
        </trans-unit>
        <trans-unit id="498514c5789196d4e7f38be6d2ccefad9084f660" translate="yes" xml:space="preserve">
          <source>Independent term in poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">多核和sigmoid核中的独立项。被其他核忽略。</target>
        </trans-unit>
        <trans-unit id="c818388cffefe0c1449b9099e6b8c434f2466b05" translate="yes" xml:space="preserve">
          <source>Independent term in the decision function.</source>
          <target state="translated">决定函数中的独立项:</target>
        </trans-unit>
        <trans-unit id="b8069da00c91cf6e966b739b57f9cbe347e859d2" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model.</source>
          <target state="translated">线性模型中的独立项。</target>
        </trans-unit>
        <trans-unit id="191e8d234bde29edd43499e6f75fe115a0a775a9" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c78017fad7dc4be13e21b61b07a09cb5931df33" translate="yes" xml:space="preserve">
          <source>Index of the cluster each sample belongs to.</source>
          <target state="translated">每个样本所属群的索引。</target>
        </trans-unit>
        <trans-unit id="79bbc01c7afbd569e88078c06011f6a142dc18ba" translate="yes" xml:space="preserve">
          <source>Index of the column of X to be swapped.</source>
          <target state="translated">要交换的X列的索引。</target>
        </trans-unit>
        <trans-unit id="ed1d58c02de7a13d74564b832a9effc7dd7512f7" translate="yes" xml:space="preserve">
          <source>Index of the row of X to be swapped.</source>
          <target state="translated">要交换的X行的索引。</target>
        </trans-unit>
        <trans-unit id="ec76f2d92b2be403363a104dc4a87849c9c331a9" translate="yes" xml:space="preserve">
          <source>Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.</source>
          <target state="translated">可索引的数据结构可以是数组、列表、数据框或第一维一致的scipy稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="436737ead6b730ec05aa4979d3ab186eb46b0b4a" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above.</source>
          <target state="translated">在第二个轴上索引数据。整数被解释为位置列，而字符串可以按名称引用DataFrame列。标量字符串或整数应在 &lt;code&gt;transformer&lt;/code&gt; 期望X像一维数组（矢量）的情况下使用，否则会将二维数组传递给转换器。可调用对象将传递输入数据 &lt;code&gt;X&lt;/code&gt; ,并且可以返回上述任何一种。</target>
        </trans-unit>
        <trans-unit id="14c06502f716a74d640158c7430747c2d65e82bc" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above. To select multiple columns by name or dtype, you can use &lt;a href=&quot;sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33324894ea0a98c1ef1f880dc97967e00c913318" translate="yes" xml:space="preserve">
          <source>Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised.</source>
          <target state="translated">表示func接受一个稀疏矩阵作为输入。如果validate为False,则没有任何影响,否则,如果accept_sparse为false,稀疏矩阵的输入将导致异常发生。否则,如果accept_sparse为false,稀疏矩阵的输入将导致一个异常被引发。</target>
        </trans-unit>
        <trans-unit id="eb7cd0d8cb7fae1e80a3e28d3771772ae8884b48" translate="yes" xml:space="preserve">
          <source>Indicate that the input X array should be checked before calling &lt;code&gt;func&lt;/code&gt;. The possibilities are:</source>
          <target state="translated">指示在调用 &lt;code&gt;func&lt;/code&gt; 之前应检查输入X数组。可能是：</target>
        </trans-unit>
        <trans-unit id="ae286e88bfa268243cfd38132ccb40e58428bfed" translate="yes" xml:space="preserve">
          <source>Indicate that transform should forward the y argument to the inner callable.</source>
          <target state="translated">表示transform应该将y参数转发给内部的可调用对象。</target>
        </trans-unit>
        <trans-unit id="9d241566a403a6506d3449cf17d407da2b6e2613" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels</source>
          <target state="translated">表示类标签的排序</target>
        </trans-unit>
        <trans-unit id="5daac4dde04b0b12288306e9a52dc06ec04c0c8f" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels. All entries should be unique (cannot contain duplicate classes).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1159dbae1b402de01888d0798e1ed328834a2b28" translate="yes" xml:space="preserve">
          <source>Indicates the monotonic constraint to enforce on each feature. -1, 1 and 0 respectively correspond to a positive constraint, negative constraint and no constraint. Read more in the &lt;a href=&quot;../ensemble#monotonic-cst-gbdt&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61bd4246c5310fe8fe14b7816d8991bfb310307e" translate="yes" xml:space="preserve">
          <source>Indicator used to add binary indicators for missing values. &lt;code&gt;None&lt;/code&gt; if add_indicator is False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f992c130abda366b807271662ae2ae17c7305e7" translate="yes" xml:space="preserve">
          <source>Indices according to which X will be subsampled.</source>
          <target state="translated">据此对X进行子抽样的指数。</target>
        </trans-unit>
        <trans-unit id="13d9e5d82e7cab8325fb7843fa12831d259a69f1" translate="yes" xml:space="preserve">
          <source>Indices of &lt;code&gt;components_&lt;/code&gt; in the training set.</source>
          <target state="translated">训练集中的 &lt;code&gt;components_&lt;/code&gt; 的索引。</target>
        </trans-unit>
        <trans-unit id="eeb5984b85169d88759ac10f7fe9d10f5246bc74" translate="yes" xml:space="preserve">
          <source>Indices of active variables at the end of the path.</source>
          <target state="translated">处于路径末端的活动变量的指数。</target>
        </trans-unit>
        <trans-unit id="fff8cc57ffbca371ebcb1bd938597e8d339ff509" translate="yes" xml:space="preserve">
          <source>Indices of cluster centers</source>
          <target state="translated">集群中心指数</target>
        </trans-unit>
        <trans-unit id="2c68ceeb78b6311d290c266259420efe85138435" translate="yes" xml:space="preserve">
          <source>Indices of columns in the dataset that belong to the bicluster.</source>
          <target state="translated">数据集中属于双簇的列的指数。</target>
        </trans-unit>
        <trans-unit id="d4d2ad6637f8190da67b396b7e452baac4f7558f" translate="yes" xml:space="preserve">
          <source>Indices of core samples.</source>
          <target state="translated">核心样品的指数:</target>
        </trans-unit>
        <trans-unit id="82cd4a5510ba380bec1e554cdeb5d721222207d6" translate="yes" xml:space="preserve">
          <source>Indices of features for a given plot. A tuple of one integer will plot a partial dependence curve of one feature. A tuple of two integers will plot a two-way partial dependence curve as a contour plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c71c298055da26ecac09942b9115f5fc73f7640d" translate="yes" xml:space="preserve">
          <source>Indices of rows in the dataset that belong to the bicluster.</source>
          <target state="translated">数据集中属于双簇的行的指数。</target>
        </trans-unit>
        <trans-unit id="d9c7ee7b4f1a89fde0903f47c0111e0985a4d274" translate="yes" xml:space="preserve">
          <source>Indices of samples used when training the estimators. &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;estimator&lt;/code&gt; does not have &lt;code&gt;_pairwise&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5ecd973c55633f90c97a971f74a80fe11af3310" translate="yes" xml:space="preserve">
          <source>Indices of support vectors.</source>
          <target state="translated">支持向量的指数。</target>
        </trans-unit>
        <trans-unit id="9ccd80ce2c5e0529264583d000f7c9651892271d" translate="yes" xml:space="preserve">
          <source>Indices of the approximate nearest points in the population matrix.</source>
          <target state="translated">人口矩阵中近似最近的点的指数。</target>
        </trans-unit>
        <trans-unit id="94147abfa5127a12fe3c3b0c7153a32b171d401a" translate="yes" xml:space="preserve">
          <source>Indices of the nearest points in the population matrix.</source>
          <target state="translated">人口矩阵中最近的点的指数。</target>
        </trans-unit>
        <trans-unit id="3db97f5586a1b88a52d6998bdcb68ce6424bd44e" translate="yes" xml:space="preserve">
          <source>Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.</source>
          <target state="translated">单个决策树可以很容易地通过简单的可视化树结构来解释。然而,梯度提升模型由数百棵回归树组成,因此不能简单地通过对单个树的视觉检查来解释。幸运的是,已经有一些技术被提出来总结和解释梯度提升模型。</target>
        </trans-unit>
        <trans-unit id="dda28c621b6ebb6a75808a25ba823af15c148423" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="translated">各个决策树通过选择适当的分割点本质上执行特征选择。此信息可用于衡量每个功能的重要性。基本思想是：在树的拆分点中使用功能的频率越高，该功能越重要。可以通过简单地平均每棵树的特征重要性来将重要性的概念扩展到决策树集合（有关更多详细信息，请参见&lt;a href=&quot;#random-forest-feature-importance&quot;&gt;特征重要性评估&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="a4ca1ad4d7f055ed4989788bfa5efeb8e8f28cc3" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the impurity-based feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3bee3019e098ad6b65e2ff793a0e712b529b8f1" translate="yes" xml:space="preserve">
          <source>Individual samples are assumed to be files stored a two levels folder structure such as the following:</source>
          <target state="translated">单个样本假设为文件,存储两级文件夹结构,如以下。</target>
        </trans-unit>
        <trans-unit id="675958cddf4afedd9b3304a64c0ebf41aefd317c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;'passthrough'&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0fc01010c6e24d626cecdd62fd33ec75c0c929c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="translated">也可以将各个步骤替换为参数，并将非最终步骤设置为 &lt;code&gt;None&lt;/code&gt; 可以忽略它们：</target>
        </trans-unit>
        <trans-unit id="1005c12f11beba0f3b6f9dd6a7112c31b922588d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample</source>
          <target state="translated">每个样本的单独权重</target>
        </trans-unit>
        <trans-unit id="1fda26bba39629c5adc019bfeebf23b6ea1cae92" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.</source>
          <target state="translated">如果传递了sample_weight,并且base_estimator拟合方法不支持,那么每个样本的单独权重会引发错误。</target>
        </trans-unit>
        <trans-unit id="fa75f1e4d13933a19ded3aa860129fc7cab3ed7d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample, ignored if None is passed.</source>
          <target state="translated">每个样本的单独权重,如果通过 &quot;无 &quot;则忽略。</target>
        </trans-unit>
        <trans-unit id="00c4a167764cbf2ab25348f070ffdca6c4a1b160" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If given a float, every sample will have the same weight.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f445327eb6fc51f2a25624b484ae69f8950143fb" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If given a float, every sample will have the same weight. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="718e842694bb04faf5cc021c83e29c576f98c12d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="translated">每个样品的重量。如果sample_weight不为None且solver ='auto'，则求解器将设置为'cholesky'。</target>
        </trans-unit>
        <trans-unit id="080f186426dd365e06ebdb64bb4da000ce313de8" translate="yes" xml:space="preserve">
          <source>Inductive Clustering</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="390c151b41ae038aa81f8d4fccaa7e2c366dd521" translate="yes" xml:space="preserve">
          <source>Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4facccaf3ac8930794a0b5d15e4cd633a166965" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;decomposition#pca&quot;&gt;Principal component analysis (PCA)&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f221e5d04a99f60098722e2605a369c8541e6c9c" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="translated">惯性不是标准化的度量标准：我们只知道较低的值更好，而零是最佳。但是在高维空间中，欧几里得距离趋于膨胀（这是所谓的&amp;ldquo;维数诅咒&amp;rdquo;的一个实例）。在k均值聚类之前运行降维算法（例如&lt;a href=&quot;pca&quot;&gt;PCA）&lt;/a&gt;可以缓解此问题并加快计算速度。</target>
        </trans-unit>
        <trans-unit id="e010b0c9058bbaf9975d3f14818f3861029f83a1" translate="yes" xml:space="preserve">
          <source>Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.</source>
          <target state="translated">惯性做出的假设是,集群是凸的和各向同性的,但事实并非总是如此。它对拉长的集群或形状不规则的表层反应较差。</target>
        </trans-unit>
        <trans-unit id="2d57b1c4d1f958efe3710f23677dd552a8a1384c" translate="yes" xml:space="preserve">
          <source>Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="translated">惯性,或集群内的平方和标准,可以被认为是对集群内部一致性程度的衡量。它存在着各种缺点。</target>
        </trans-unit>
        <trans-unit id="1b9b7d4cd56309d7954eee8c5d88a8d58559a22d" translate="yes" xml:space="preserve">
          <source>Inference of the model can be time consuming.</source>
          <target state="translated">模型的推断可能会很耗时。</target>
        </trans-unit>
        <trans-unit id="52570031388abf13077117eb25231bb2412ec6ba" translate="yes" xml:space="preserve">
          <source>Inferred batch size from &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3406f3eade82db35711ae0ecbcd4dea59f7e1be" translate="yes" xml:space="preserve">
          <source>Inferred value for &lt;code&gt;increasing&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f5d2c4b74b9f6de8f1a369f5793fd5ad3db1bd0" translate="yes" xml:space="preserve">
          <source>Influence of outliers on location and covariance estimates</source>
          <target state="translated">离群值对位置和协方差估计的影响。</target>
        </trans-unit>
        <trans-unit id="b8c700f6663aab653644d35fb6aa9a0e53919c01" translate="yes" xml:space="preserve">
          <source>Information on how to contribute. This also contains useful information for advanced users, for example how to build their own estimators.</source>
          <target state="translated">关于如何作出贡献的信息。这也包含对高级用户有用的信息,例如如何建立自己的估算器。</target>
        </trans-unit>
        <trans-unit id="c4fbdb7aab44015fbed39936864f9255a99074c1" translate="yes" xml:space="preserve">
          <source>Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">基于信息准则的模型选择速度非常快,但它依赖于自由度的正确估计,是针对大样本得出的(渐近结果),并且假设模型是正确的,即数据实际上是由这个模型产生的。当问题的条件不好(特征多于样本)时,它们也往往会被打破。</target>
        </trans-unit>
        <trans-unit id="132062fafb54cb7d1e8b42d922906ff561bc3d9c" translate="yes" xml:space="preserve">
          <source>Inherits from SGDClassifier. &lt;code&gt;Perceptron()&lt;/code&gt; is equivalent to &lt;code&gt;SGDClassifier(loss=&quot;perceptron&quot;, eta0=1, learning_rate=&quot;constant&quot;, penalty=None)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="897de0e7da29095e5d730f2c7102743d023bc056" translate="yes" xml:space="preserve">
          <source>Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5584ccc82484d2c61a85300c7acd35cb5205fef6" translate="yes" xml:space="preserve">
          <source>Initial value for lambda (precision of the weights). If not set, lambda_init is 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="168bf67b39abe0e5515698a4ec915c75e07ca524" translate="yes" xml:space="preserve">
          <source>Initial value for the dictionary for warm restart scenarios.</source>
          <target state="translated">暖式重启场景下字典的初始值。</target>
        </trans-unit>
        <trans-unit id="64b2b9f72c239478fc1b9e586ac8147218ca87bd" translate="yes" xml:space="preserve">
          <source>Initial value for the sparse code for warm restart scenarios.</source>
          <target state="translated">暖式重启方案的稀疏代码的初始值。</target>
        </trans-unit>
        <trans-unit id="1a6282c9a9baf1c9231fe6132d9510c0860835e2" translate="yes" xml:space="preserve">
          <source>Initial values for the components for warm restart scenarios.</source>
          <target state="translated">暖式重启情景下各组件的初始值。</target>
        </trans-unit>
        <trans-unit id="ff1f0a80e07dd6cd648bd3a5e935a909ec2cb299" translate="yes" xml:space="preserve">
          <source>Initial values for the loadings for warm restart scenarios.</source>
          <target state="translated">暖式重启情况下负荷的初始值。</target>
        </trans-unit>
        <trans-unit id="e597ad1e68022a1929058718fd92959e281b3619" translate="yes" xml:space="preserve">
          <source>Initialization of embedding. Possible options are &amp;lsquo;random&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.</source>
          <target state="translated">嵌入的初始化。可能的选项是&amp;ldquo; random&amp;rdquo;，&amp;ldquo; pca&amp;rdquo;和形状的numpy数组（n_samples，n_components）。PCA初始化不能与预先计算的距离一起使用，并且通常比随机初始化更全局稳定。</target>
        </trans-unit>
        <trans-unit id="726483459f359a8ededc78286c7835b11430e40a" translate="yes" xml:space="preserve">
          <source>Initialization of the linear transformation. Possible options are &amp;lsquo;auto&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, &amp;lsquo;lda&amp;rsquo;, &amp;lsquo;identity&amp;rsquo;, &amp;lsquo;random&amp;rsquo;, and a numpy array of shape (n_features_a, n_features_b).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e644154875d3e41452e4b87177ec7a98e4e47540" translate="yes" xml:space="preserve">
          <source>Initialization value for coefficients of logistic regression. Useless for liblinear solver.</source>
          <target state="translated">对数回归系数的初始化值。对liblinear求解器无用。</target>
        </trans-unit>
        <trans-unit id="ed7011971816dd0f1903ad54a411facedaee4ded" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">稀疏代码的初始化值。仅在 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8989f9ab8650a480a58e2f03b20f1b5428df4549" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f66672d1a211feccb2663e524389cc7bbdc9544" translate="yes" xml:space="preserve">
          <source>Initialize self. See help(type(self)) for accurate signature.</source>
          <target state="translated">初始化self。参见 help(type(self))以获得准确的签名。</target>
        </trans-unit>
        <trans-unit id="ccac906d8789727d67e839b8a0f34db2f9002a71" translate="yes" xml:space="preserve">
          <source>Initializing components, sampling from layers during fit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4694fddfebcd07057574f3bedbe073aecef8cbe" translate="yes" xml:space="preserve">
          <source>Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the &lt;code&gt;score_samples&lt;/code&gt; method, while the threshold can be controlled by the &lt;code&gt;contamination&lt;/code&gt; parameter.</source>
          <target state="translated">异常值标记为1，而异常值标记为-1。预测方法利用了由估算器计算的原始评分函数的阈值。可通过 &lt;code&gt;score_samples&lt;/code&gt; 方法访问此评分功能，而阈值可由 &lt;code&gt;contamination&lt;/code&gt; 参数控制。</target>
        </trans-unit>
        <trans-unit id="2495be2cc170e277a5d9d5dd99a3f779ff0175c9" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">内在的充分统计,由算法保存。在初始化时传递它们在在线设置中很有用,可以避免丢失演化的历史。A(n_components,n_components)是字典协方差矩阵。B (n_features,n_components)是数据近似矩阵。</target>
        </trans-unit>
        <trans-unit id="06dcf06c32dda54c31aa74eee694be9763d822e4" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid losing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cb705151cf2a5dcfea8029a0e74d39c67469fa5" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSC/CSR matrix.</source>
          <target state="translated">CSC/CSR矩阵的列内缩放。</target>
        </trans-unit>
        <trans-unit id="15ad21d3f40808b965488683b0faaff07ad4b5e9" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSR matrix.</source>
          <target state="translated">输入CSR矩阵的列标尺。</target>
        </trans-unit>
        <trans-unit id="75290bdbc975498c98c7572ec067e43ddceb0c68" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l1 norm</source>
          <target state="translated">使用L1法线进行行归一化。</target>
        </trans-unit>
        <trans-unit id="bc0180825da8415aba8c76f27c29d7e471b70c2a" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l2 norm</source>
          <target state="translated">使用 l2 norm 进行嵌位行归一化。</target>
        </trans-unit>
        <trans-unit id="d91ae689358e283ef6d343e24e55244f1fb1cad2" translate="yes" xml:space="preserve">
          <source>Inplace row scaling of a CSR or CSC matrix.</source>
          <target state="translated">输入CSR或CSC矩阵的行比例。</target>
        </trans-unit>
        <trans-unit id="16ca749420dd58126c1f3f4ccf95ce2fc49387c9" translate="yes" xml:space="preserve">
          <source>Input array.</source>
          <target state="translated">输入阵列。</target>
        </trans-unit>
        <trans-unit id="f6fcca00499ce6b21e6114d56b450f6d3d43ccb2" translate="yes" xml:space="preserve">
          <source>Input checker utility for building a cross-validator</source>
          <target state="translated">用于建立交叉验证器的输入检查器实用程序。</target>
        </trans-unit>
        <trans-unit id="3f43a2e4863dbf39da8cfbd5e4e557f3052ec269" translate="yes" xml:space="preserve">
          <source>Input data</source>
          <target state="translated">输入数据</target>
        </trans-unit>
        <trans-unit id="a2e2ac083ce3186e216de5448418ffc50e83a494" translate="yes" xml:space="preserve">
          <source>Input data for prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41aa04ac5754100b497c803419573444b7e1d42b" translate="yes" xml:space="preserve">
          <source>Input data representation and sparsity</source>
          <target state="translated">输入数据的表示和稀疏性</target>
        </trans-unit>
        <trans-unit id="66a8a4e34fe15bd5eafb5d984d77b9c0e3866728" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed.</source>
          <target state="translated">将要转换的输入数据。</target>
        </trans-unit>
        <trans-unit id="fbb05f66a147e8520f226c078931507f60d0caac" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed. It cannot be sparse.</source>
          <target state="translated">将被转换的输入数据。不能是稀疏的。</target>
        </trans-unit>
        <trans-unit id="1e3e0a570b83c9cbe639106afeb9bedd23e3fcfd" translate="yes" xml:space="preserve">
          <source>Input data to be transformed.</source>
          <target state="translated">要转换的输入数据。</target>
        </trans-unit>
        <trans-unit id="071feefe9143dba47a473de169ba49367bfce443" translate="yes" xml:space="preserve">
          <source>Input data to be transformed. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csr_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">要转换的输入数据。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。还支持稀疏矩阵，请使用稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 以获得最大效率。</target>
        </trans-unit>
        <trans-unit id="688f24dc1e25fac1dc510cf29e6e51a5cdee42ba" translate="yes" xml:space="preserve">
          <source>Input data used to build forests. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">输入用于构建林的数据。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。</target>
        </trans-unit>
        <trans-unit id="ece9df27fcdc2d8935842ef4ed6ac1e8d53f8b6c" translate="yes" xml:space="preserve">
          <source>Input data, of which specified subsets are used to fit the transformers.</source>
          <target state="translated">输入数据,其中指定的子集被用来匹配变压器。</target>
        </trans-unit>
        <trans-unit id="8c020a67c0398f86d112987222d02c36283701ba" translate="yes" xml:space="preserve">
          <source>Input data, target values.</source>
          <target state="translated">输入数据,目标值。</target>
        </trans-unit>
        <trans-unit id="0d15fc28721eb2bcf2d53de59215da674d786463" translate="yes" xml:space="preserve">
          <source>Input data, used to fit transformers.</source>
          <target state="translated">输入数据,用于配合变压器。</target>
        </trans-unit>
        <trans-unit id="6db69e17f58030ae15478a3e504e982203a8ead7" translate="yes" xml:space="preserve">
          <source>Input data, where &amp;ldquo;n_samples&amp;rdquo; is the number of samples and &amp;ldquo;n_features&amp;rdquo; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aee2f5203193bb3069f6e2f7b08e833e91d53841" translate="yes" xml:space="preserve">
          <source>Input data, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">输入数据，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数， &lt;code&gt;n_features&lt;/code&gt; 是要素数。</target>
        </trans-unit>
        <trans-unit id="8f9c683e36e31c7c38c7e8d6a2daeccf181d4c94" translate="yes" xml:space="preserve">
          <source>Input data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">输入数据,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="260753716624ad2077f13f38ada17a33c0f48cba" translate="yes" xml:space="preserve">
          <source>Input data.</source>
          <target state="translated">输入数据:</target>
        </trans-unit>
        <trans-unit id="c414149f534c72aa4d2016c93404604f17aa41fd" translate="yes" xml:space="preserve">
          <source>Input data. Columns are assumed to have unit norm.</source>
          <target state="translated">输入数据。假设各列有单位法线。</target>
        </trans-unit>
        <trans-unit id="fc921091c020b61d18864b21129c4eb989ef6d69" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;None&lt;/code&gt;, the output will be the pairwise similarities between all samples in &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">输入数据。如果为 &lt;code&gt;None&lt;/code&gt; ，则输出将为 &lt;code&gt;X&lt;/code&gt; 中所有样本之间的成对相似性。</target>
        </trans-unit>
        <trans-unit id="afaf08d18a1423dc1a78bf3492e5ce0eb50d8638" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt;, the input should be the dissimilarity matrix.</source>
          <target state="translated">输入数据。如果 &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt; ，则输入应为不相似矩阵。</target>
        </trans-unit>
        <trans-unit id="c7e2acaca5145e47486c9c2928ab5532aee98fd4" translate="yes" xml:space="preserve">
          <source>Input data. If X is not provided, only the global clustering step is done.</source>
          <target state="translated">输入数据。如果没有提供X,只进行全局聚类步骤。</target>
        </trans-unit>
        <trans-unit id="609eafdfc25268bf81369faed57dc8b7d067fce7" translate="yes" xml:space="preserve">
          <source>Input data. Note that if X is None then the Gram matrix must be specified, i.e., cannot be None or False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d39fe8b9c923e952612de77f70ad5aff60c34542" translate="yes" xml:space="preserve">
          <source>Input object to check / convert.</source>
          <target state="translated">要检查/转换的输入对象。</target>
        </trans-unit>
        <trans-unit id="ca7fd27e447d5e335b2e80d1b2bb1406dceac239" translate="yes" xml:space="preserve">
          <source>Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised.</source>
          <target state="translated">要检查/转换的输入对象。必须是二维和正方形,否则将引发一个ValueError。</target>
        </trans-unit>
        <trans-unit id="f24bf584af83fad1f47a5035ff05cdc62fd1daef" translate="yes" xml:space="preserve">
          <source>Input points.</source>
          <target state="translated">输入点:</target>
        </trans-unit>
        <trans-unit id="71b2d21e1f2e71c0dcf88bd09dfe9ef6fd499ea3" translate="yes" xml:space="preserve">
          <source>Input targets</source>
          <target state="translated">输入目标</target>
        </trans-unit>
        <trans-unit id="b58e4bce1b7ebb41f970a06dbe933522f9ad2c46" translate="yes" xml:space="preserve">
          <source>Input targets multiplied by X: X.T * y</source>
          <target state="translated">输入目标乘以X:X.T*y。</target>
        </trans-unit>
        <trans-unit id="69a0e9f3a009e8ccbba64367545e9fbe88306b19" translate="yes" xml:space="preserve">
          <source>Input targets.</source>
          <target state="translated">输入目标:</target>
        </trans-unit>
        <trans-unit id="51c9f9fb1fac83429c51404b5e9b2caee57cc2f2" translate="yes" xml:space="preserve">
          <source>Input validation for standard estimators.</source>
          <target state="translated">标准估计器的输入验证。</target>
        </trans-unit>
        <trans-unit id="346da7aa7d7e4eb884706a35a69404b7d3fc9daa" translate="yes" xml:space="preserve">
          <source>Input validation on an array, list, sparse matrix or similar.</source>
          <target state="translated">在数组、列表、稀疏矩阵或类似情况下进行输入验证。</target>
        </trans-unit>
        <trans-unit id="4191049318f63b6c3a85211b91a5c1cecb649bdb" translate="yes" xml:space="preserve">
          <source>Input values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8344beaf285df55c120907e8b74a9a1f87253895" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are 4 independent features uniformly distributed on the intervals:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是在区间上均匀分布的4个独立特征：</target>
        </trans-unit>
        <trans-unit id="bcbcb56a88ddeef69ce01bcc9a78a456071e2cf0" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are independent features uniformly distributed on the interval [0, 1]. The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是独立的特征，均匀分布在间隔[0，1]上。根据以下公式创建输出 &lt;code&gt;y&lt;/code&gt; ：</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
