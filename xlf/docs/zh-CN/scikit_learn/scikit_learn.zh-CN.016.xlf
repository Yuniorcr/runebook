<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">每幅图像要提取的最大补丁数。如果max_patches是(0,1)中的浮点数,则表示占总补丁数的比例。</target>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">要提取的补丁的最大数量。如果max_patches是介于0和1之间的浮点数,则取其占补丁总数的比例。</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">用于计算交叉验证中残差的路径上的最大点数。</target>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">全数据集的均值和经验协方差,一旦数据集出现异常值,就会被分解掉。</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">观测值的均值和经验协方差是好的。这可以被认为是&amp;ldquo;完美的&amp;rdquo; MCD估计，因此通过与这种情况进行比较，可以相信我们的实施。</target>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">每个混合物分量的平均值。</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">多维正态分布的均值。如果为None，则使用原点（0，0，&amp;hellip;）。</target>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">每个bin的平均预测概率。</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">因此,得分的平均值和95%的置信区间由以下公式表示。</target>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">训练集中每个特征的平均值。当 &lt;code&gt;with_mean=False&lt;/code&gt; 时等于 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">为每个图像计算这些特征的平均值，标准误以及&amp;ldquo;最差&amp;rdquo;或最大（三个最大值的平均值），从而得到30个特征。例如，字段3是平均半径，字段13是半径SE，字段23是最差半径。</target>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">给定一棵树,衡量一个观测值的规范性的标准是包含这个观测值的叶子的深度,相当于隔离这个点所需的分裂次数。如果叶中有多个观测点n_left,则要加上n_left样本隔离树的平均路径长度。</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">用与非腐败新数据的绝对偏差的中位数来判断预测的质量。</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">训练集中每个特征的中值。</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">从缓存加载numpy数组时使用的memmapping模式。参数的含义参见numpy.load。</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">从缓存加载numpy数组时使用的memmapping模式。参数的含义参见numpy.load。默认情况下,使用的是内存对象的模式。</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">随机&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;的内存占用也与\（2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \）而不是\（n _ {\ max} \ cdot n _ {\ min} \）成正比对于确切的方法。</target>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">该方法假设输入来自一个二进制分类器。</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">该方法拟合模型 &lt;code&gt;n_init&lt;/code&gt; 次，并设置模型具有最大似然性或下界的参数。在每次试验中，该方法都会在E步和M步之间迭代 &lt;code&gt;max_iter&lt;/code&gt; 次，直到似然性或下界的变化小于 &lt;code&gt;tol&lt;/code&gt; 为止，否则会引发 &lt;code&gt;ConvergenceWarning&lt;/code&gt; 。如果 &lt;code&gt;warm_start&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，那么将忽略 &lt;code&gt;n_init&lt;/code&gt; ，并且在第一次调用时执行一次初始化。连续通话时，训练从中断的地方开始。</target>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">该方法拟合模型n_init次，并设置模型具有最大似然性或下界的参数。在每个试验中，该方法都会在E步和M步之间迭代 &lt;code&gt;max_iter&lt;/code&gt; 次，直到似然性或下界的变化小于 &lt;code&gt;tol&lt;/code&gt; 为止，否则会引发 &lt;code&gt;ConvergenceWarning&lt;/code&gt; 。拟合后，它将为输入数据点预测最可能的标签。</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">该方法在用独立RBM的权重初始化深度神经网络时得到了普及。这种方法被称为无监督预训练。</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">支持向量分类的方法可以扩展到解决回归问题。这种方法称为支持向量回归法。</target>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">用于校准的方法。可以是对应于普拉特方法的&amp;ldquo; S形&amp;rdquo;，也可以是非参数方法的&amp;ldquo;等渗&amp;rdquo;。不建议对等渗校准使用很少的校准样品 &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; ,因为它倾向于过拟合。在这种情况下，请使用S型曲线（Platt的校准）。</target>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">用于初始化权重、平均值和协方差的方法。必须是以下两种方法之一:</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">用于初始化权数、平均值和精确度的方法。必须是以下两种方法之一:</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">该方法适用于简单的估计器以及嵌套对象（例如管道）。后者具有 &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; 形式的参数，以便可以更新嵌套对象的每个组件。</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">该方法适用于简单内核以及嵌套内核。后者具有 &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; 形式的参数，以便可以更新嵌套对象的每个组件。</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">基于F检验的方法可以估计两个随机变量之间的线性依赖程度。另一方面,互信息方法可以捕捉任何类型的统计依赖性,但由于是非参数化的,它们需要更多的样本来进行准确的估计。</target>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串或可调用，则它必须是&lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt;为其metric参数允许的选项之一。如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则将X假定为距离矩阵，并且必须为平方。 X可能是一个稀疏矩阵，在这种情况下，只有&amp;ldquo;非零&amp;rdquo;元素可以被视为DBSCAN的邻居。</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串或可调用，则必须是metric参数的metrics.pairwise.pairwise_distances允许的选项之一。对应于每个类别的样本的质心是一个点，从该点可以使属于该特定类别的所有样本的距离之和（根据度量）最小化。如果提供了&amp;ldquo;曼哈顿&amp;rdquo;度量标准，则此质心为中位数，对于所有其他度量标准，现在将质心设置为均值。</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串，则它必须是 &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; 允许的选项之一。如果X是距离数组本身，请使用 &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串，则它必须是 &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; 允许的选项之一。如果X是距离数组本身，则使用&amp;ldquo;预先计算&amp;rdquo;作为度量。</target>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串，则它必须是scipy.spatial.distance.pdist为其metric参数允许的选项之一，或者是pairwise.PAIRWISE_DISTANCE_FUNCTIONS中列出的度量。如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则假定X为距离矩阵。或者，如果metric是可调用的函数，则在每对实例（行）上调用它，并记录结果值。可调用对象应将X的两个数组作为输入，并返回一个指示它们之间距离的值。</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串，则它必须是scipy.spatial.distance.pdist为其metric参数允许的选项之一，或者是pairwise.PAIRWISE_DISTANCE_FUNCTIONS中列出的度量。如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则假定X为距离矩阵。或者，如果metric是可调用的函数，则在每对实例（行）上调用它，并记录结果值。可调用对象应将X的两个数组作为输入，并返回一个指示它们之间距离的值。默认值为&amp;ldquo;欧几里得距离&amp;rdquo;，它被解释为平方的欧几里得距离。</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">计算要素阵列中实例之间的距离时使用的度量。如果metric是字符串，则它必须是PAIRED_DISTANCES中指定的选项之一，包括&amp;ldquo; euclidean&amp;rdquo;，&amp;ldquo; manhattan&amp;rdquo;或&amp;ldquo; cosine&amp;rdquo;。或者，如果metric是可调用的函数，则在每对实例（行）上调用它，并记录结果值。可调用对象应将X的两个数组作为输入，并返回一个指示它们之间距离的值。</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">计算要素数组中的实例之间的内核时使用的度量。如果metric是字符串，则它必须是pairwise.PAIRWISE_KERNEL_FUNCTIONS中的一个度量。如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则假定X为内核矩阵。或者，如果metric是可调用的函数，则在每对实例（行）上调用它，并记录结果值。可调用对象应将X的两个数组作为输入，并返回一个指示它们之间距离的值。</target>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">为保证有良好概率的eps-embedding的n_samples的最小元件数。</target>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">最小共识得分,0,发生在所有双簇完全不同的时候。最高分,1,出现在两组完全相同的情况下。</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">保证eps嵌入的最小元件数量由以下公式给出。</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">保证eps嵌入的最小元件数量为:。</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">最少要选择的功能。即使原始特征计数与 &lt;code&gt;min_features_to_select&lt;/code&gt; 之间的差异不能被 &lt;code&gt;step&lt;/code&gt; 整除，也会始终对这个特征数进行评分。</target>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">在叶节点处所需的最小样本数。仅在任何深度的分割点在左分支和右分支中的每个分支上至少 &lt;code&gt;min_samples_leaf&lt;/code&gt; 训练样本时，才考虑该点。这可能具有平滑模型的效果，尤其是在回归中。</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">拆分一个内部节点所需的最小样本数。</target>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">在叶子节点上所需的权重总和(所有输入样本)的最小加权分数。当没有提供sample_weight时,样本的权重相等。</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">输入数据缺少指示符。 &lt;code&gt;Xt&lt;/code&gt; 的数据类型将为布尔值。</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">用于初始化算法的混合矩阵。</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">混合矩阵;</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">假设所有的类都有相同的协方差矩阵,模型对每个类都拟合一个高斯密度。</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">该模型对每个班级都拟合了一个高斯密度。</target>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">该模型对投入的分布做出假设。目前，scikit-learn仅提供&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;，它假定输入是二进制值或0到1之间的值，每个值都将编码打开特定功能的可能性。</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">该模型需要在训练时计算概率信息：将属性 &lt;code&gt;probability&lt;/code&gt; 设置为True。</target>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">可以通过成员 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 访问模型参数：</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">由支持向量分类产生的模型(如上所述)只依赖于训练数据的一个子集,因为建立模型的成本函数不关心位于边际之外的训练点。类似地,支持向量回归产生的模型只依赖于训练数据的子集,因为建立模型的成本函数忽略了任何接近模型预测的训练数据。</target>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">模型从最强正则化到最小正则化排序。收集模型的4个系数并将其绘制为&amp;ldquo;正则化路径&amp;rdquo;：在图的左侧（强正则化器），所有系数都正好为0。当正则化变得越来越宽松时，系数可能会变为非零值一个接一个。</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块包含流行的增强算法AdaBoost，该算法由Freund和Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;于1995年引入。</target>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块提供了通过梯度增强的回归树进行分类和回归的方法。</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;模块还公开了一组简单的函数，这些函数在给定地面真实性和预测的情况下测量预测误差：</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">模块 &lt;code&gt;partial_dependence&lt;/code&gt; 提供了一个便捷函数&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; ,&lt;/a&gt;用于创建单向和双向的部分依赖图。在下面的示例中，我们演示如何创建偏相关图的网格：特征 &lt;code&gt;0&lt;/code&gt; 和 &lt;code&gt;1&lt;/code&gt; 的两个单向PDP，两个特征之间的两个PDP：</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">该模块包含公共属性 &lt;code&gt;coefs_&lt;/code&gt; 和 &lt;code&gt;intercepts_&lt;/code&gt; 。 &lt;code&gt;coefs_&lt;/code&gt; 是权重矩阵的列表，其中索引\（i \）处的权重矩阵表示层\（i \）和层\（i + 1 \）之间的权重。 &lt;code&gt;intercepts_&lt;/code&gt; 是偏差向量的列表，其中索引\（i \）处的向量表示添加到图层\（i + 1 \）的偏差值。</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">模块： &lt;code&gt;random_projection&lt;/code&gt; 提供了几种用于通过随机投影进行数据缩减的工具。请参阅文档的相关部分：&lt;a href=&quot;random_projection#random-projection&quot;&gt;随机投影&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">在每次迭代后使用当前迭代调用监视器，该迭代器是对估计器的引用和 &lt;code&gt;_fit_stages&lt;/code&gt; 的局部变量作为关键字参数 &lt;code&gt;callable(i, self, locals())&lt;/code&gt; 。如果可调用对象返回 &lt;code&gt;True&lt;/code&gt; ,则拟合过程将停止。该监视器可用于各种事情，例如计算暂定的估计，提前停止，模型自省和快照。</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">适用于此策略的最常见参数是编码正则化器强度的参数。在这种情况下，我们说我们计算估计量的&lt;strong&gt;正则化路径&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">最直观的方法就是用袋字表示。</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">使用这种缩放的动机包括对特征的极小标准差的鲁棒性,以及保留稀疏数据中的零条目。</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">多任务套索允许共同拟合多个回归问题,强制要求所选特征在不同任务中相同。本例模拟顺序测量,每个任务是一个时间瞬时,相关特征在相同的同时随时间变化幅度。多任务套索规定,在一个时间点上选择的特征,所有时间点都要选择。这使得Lasso的特征选择更加稳定。</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">这里的多类定义似乎是二元分类中使用的度量的最合理的扩展,尽管文献中并没有一定的共识。</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">多类支持按照一比一的方案处理。</target>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">多项式奈夫贝叶斯分类器适用于具有离散特征的分类(例如,文本分类的字数)。多项分布通常需要整数的特征数。然而,在实践中,小数计数,如tf-idf也可以工作。</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">多个指标可以指定为列表、元组或一组预定义的评分器名称。</target>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">装入的样本图像的名称</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">名称 &lt;code&gt;vect&lt;/code&gt; ， &lt;code&gt;tfidf&lt;/code&gt; 和 &lt;code&gt;clf&lt;/code&gt; （分类）是任意的。我们将在下面使用它们对合适的超参数执行网格搜索。现在，我们可以使用一个命令来训练模型：</target>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">数据集列的名称</target>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">新的dtype将是np.float32或np.float64,取决于原始类型。该函数可以创建一个副本或修改参数,取决于参数的副本。</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">新条目(d(u,v)/)的计算方法如下:</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">下图比较了针对不同类型的 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; ，针对不同类型的先验重量浓度（参数weight_concentration_prior_type）获得的 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 。在这里，我们可以看到 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 参数的值对获得的有效成分的有效数量有很大的影响。我们还可以注意到，当先验类型为'dirichlet_distribution'时，先验浓度权重的较大值会导致权重更均匀，而'dirichlet_process'类型（默认情况下使用）不一定是这种情况。</target>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">下图比较了针对不同大小的训练集的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;SVR&lt;/code&gt; 的拟合和预测时间。对于中等规模的训练集（少于1000个样本），拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;的速度比 &lt;code&gt;SVR&lt;/code&gt; 快；但是，对于较大的训练集， &lt;code&gt;SVR&lt;/code&gt; 的缩放比例更好。关于预测时间，由于学习到的稀疏解决方案，对于所有大小的训练集， &lt;code&gt;SVR&lt;/code&gt; 都比&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;更快。注意，稀疏程度和预测时间取决于 &lt;code&gt;SVR&lt;/code&gt; 的参数\（\ epsilon \）和\（C \）； \（\ epsilon = 0 \）将对应于一个密集模型。</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">下图比较了不同大小训练集的KRR和SVR的拟合和预测时间。对于中等大小的训练集(小于1000个样本),拟合KRR比SVR快;但对于较大的训练集,SVR的伸缩性更好。在预测时间方面,由于学习了稀疏解,对于所有大小的训练集,SVR都比KRR快。请注意,稀疏程度,从而预测时间取决于SVR的参数epsilon和C。</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">下一张图片说明了sigmoid校准如何改变3类分类问题的预测概率。图中是标准的2-Simplex,其中三个角对应三个类。箭头从未校准分类器预测的概率向量指向同一分类器在保留验证集上进行sigmoid校准后预测的概率向量。颜色表示一个实例的真实等级(红色:等级1,绿色:等级2,蓝色:等级3)。</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">节点是随机变量,其状态取决于它们所连接的其他节点的状态。因此,模型的参数化是由连接的权重,以及每个可见单元和隐藏单元的一个截距(偏置)项,为了简单起见,图像中省略了。</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">可以通过参数 &lt;code&gt;alpha&lt;/code&gt; 传递目标中的噪声级别，该噪声级别可以全局作为标量或每个数据点传递。请注意，适度的噪声水平也可以在拟合期间处理数值问题，因为它可以有效地实现为Tikhonov正则化，即通过将其添加到核矩阵的对角线来实现。显式指定噪声级别的另一种方法是在内核中包含WhiteKernel组件，该组件可以从数据中估计全局噪声级别（请参见下面的示例）。</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">内核的非固定的、对数变换的超参数。</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">非计量算法在计算应力之前,增加了一个单调的回归步骤。</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">归一化每个非零样本的标准(如果轴为0,则每个非零特征)。</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">用来归一化每个非零样本的标准。</target>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">归一化的相互信息定义为</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">然后,归一化实例可以像任何变换器一样用于样本向量。</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">考虑的邻居数k(别名参数n_neighbors)通常选择1)大于一个簇必须包含的最小对象数,这样其他对象相对于这个簇就可以成为局部离群值,2)小于可能成为局部离群值的邻近对象的最大数量。在实际工作中,这样的信息一般无法获得,取n_neighbors=20在一般情况下似乎效果不错。当离群值的比例较高时(即大于10%,如下面的例子),n_neighbors应该更大(下面的例子中n_neighbors=35)。</target>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">用于执行OVA（对于多类问题，为1对所有）的CPU数量。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">用于进行计算的CPU数量。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">要执行的EM迭代次数。</target>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">一次分派给每个工作人员的原子任务数。当单个评估非常快时，由于开销大，向工作者分派调用可能比顺序计算慢。将快速计算分批处理可以减轻这种情况。在 &lt;code&gt;'auto'&lt;/code&gt; 战略跟踪花费了一批完整的时间，并动态调整批量大小，以保持半秒的顺序时，使用启发式。初始批处理大小为 &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; 和 &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; 将分派单个任务的批处理，因为线程化后端的开销很小，而使用较大的批处理大小并没有带来任何好处案件。</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">合集中基础估计器的数量。</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">要预先分派的（任务的）批次数。默认值为&amp;ldquo; 2 * n_jobs&amp;rdquo;。当batch_size =&amp;ldquo; auto&amp;rdquo;时，这是合理的默认值，并且工作人员永远不应挨饿。</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">要找到的双子星数量。</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">双胞胎的数量。</target>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">产生的垃圾箱数量。仓位的间隔由输入数据的最小值和最大值确定。如果 &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; 则引发ValueError 。</target>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">要执行的提升阶段的数量。梯度提升对过度拟合的影响相当大,所以通常一个大的数字会带来更好的性能。</target>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">班级数量</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">类的数量(对于单输出问题),或者包含每个输出的类的数量的列表(对于多输出问题)。</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">分类问题的类数(或标签)。</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">类的数量(单输出问题),或包含每个输出的类的数量的列表(多输出问题)。</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">训练数据中的类数</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">分类问题的类数。</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">要返回的班级数量。</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">班的数量。</target>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">每个班级的集群数量。</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">簇的数量来寻找。</target>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">形成的聚类数量以及生成的中心体数量。</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">网格图的列数(默认:3)。</target>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">图中连接的组件数量。</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">交叉验证分裂的次数(折/次)。</target>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">模型中各分量的自由度数。</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">重复特征的数量,从信息特征和冗余特征中随机抽取。</target>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 上等距点的数量。</target>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">轴上等间距点的数量。</target>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">通过提前停止选择的估计量（如果指定了 &lt;code&gt;n_iter_no_change&lt;/code&gt; ）。否则将其设置为 &lt;code&gt;n_estimators&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">输出矩阵中特征(列)的数量。小数量的特征很可能导致哈希碰撞,但大数量的特征会导致线性学习器的系数维度变大。</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">每个样本的特征数量。</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">功能数量必须&amp;gt; = 5。</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">在寻找最好的分体时,需要考虑的功能数量。</target>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">从X中抽取的特征数量,以训练每个基础估计器。</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">要选择的功能数量。如果为 &lt;code&gt;None&lt;/code&gt; ，则选择一半功能。</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">要使用的特征数量。如果无,将从任何文件中出现的最大列索引推断。</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">要使用的功能数量。如果为None，则将其推断出来。此参数对于加载作为更大切片数据集的子集的几个文件很有用：每个子集可能没有每个特征的示例，因此推断的形状可能在一个切片之间有所不同。仅当将 &lt;code&gt;offset&lt;/code&gt; 或 &lt;code&gt;length&lt;/code&gt; 传递为非默认值时才需要n_features 。</target>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">执行 &lt;code&gt;fit&lt;/code&gt; 时的特征数量。</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">特征的数量。</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">特征的数量。应至少为5个。</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">信息特征的数量,即用于建立用于生成输出的线性模型的特征数量。</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">信息功能的数量。每个类都由多个高斯群集组成，每个高斯群集都位于维度 &lt;code&gt;n_informative&lt;/code&gt; 的子空间中超立方体的顶点周围。对于每个聚类，独立于N（0，1）绘制信息特征，然后在每个聚类内随机线性组合以增加协方差。然后将簇放置在超立方体的顶点上。</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">要执行的初始化次数。保留最佳结果。</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">要执行的初始化次数。保留可能性下限值最高的结果。</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">要取样的整数。</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">在这次调用 partial_fit 之前,对数据批次进行迭代的次数。这是可选的:如果没有传递数字,则使用对象的内存。</target>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">对应于最佳应力的迭代次数。仅当 &lt;code&gt;return_n_iter&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; 时才返回。</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">lars_path为每个目标寻找字母网格的迭代次数。</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">坐标下降优化器为达到每个α的指定容差而进行的迭代次数。</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">坐标下降优化器为达到每个alpha的指定公差所进行的迭代次数。（当 &lt;code&gt;return_n_iter&lt;/code&gt; 设置为True 时返回）。</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">解算器运行的迭代次数。</target>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">并行运行以 &lt;code&gt;fit&lt;/code&gt; 工作的数量。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">同时进行 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 的作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">同时进行 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 的作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None`&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">用于计算的作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">用于计算的作业数。如果使用多个初始化（ &lt;code&gt;n_init&lt;/code&gt; ），则算法的每次运行都是并行计算的。</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">用于计算的作业数。它并行执行y中的每个目标变量。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">用于计算的作业数。这只会为n_targets&amp;gt; 1和足够大的问题提供加速。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">用于计算的作业数量。它的工作原理是将对偶矩阵分解成n_job偶数片,并进行并行计算。</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">用于计算的作业数量。通过并行计算每一个n_init运行。</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">E步骤中要使用的作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">树上的叶子数量</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">网格中经度(x)和纬度(y)的数量。</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">混合物成分的数量;</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">混合成分的数量。根据数据和 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 的值，模型可以通过将某些组件 &lt;code&gt;weights_&lt;/code&gt; 设置为非常接近零的值来决定不使用所有组件。因此，有效分量的数量小于n_components。</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">返回的最近的邻居数量</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考虑的邻域数(参数n_neighbors)通常设置为:1)大于一个簇必须包含的最小样本数,这样其他样本相对于这个簇就可以成为局部离群值;2)小于可能成为局部离群值的邻近样本的最大数量。在实践中,这样的信息一般是无法获得的,取n_neighbors=20似乎在一般情况下效果不错。</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">考虑的邻域数(参数n_neighbors)通常设置为:1)大于一个簇必须包含的最小样本数,这样其他样本相对于这个簇就可以成为局部离群值;2)小于可能成为局部离群值的邻近样本的最大数量。在实践中,这样的信息一般是无法获得的,取n_neighbors=20似乎在一般情况下效果不错。</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 中每个标签的出现次数。</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">离群点的数量很重要,但也要看离群点的多少。</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">执行 &lt;code&gt;fit&lt;/code&gt; 时的输出数量。</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">产出的数量;</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">邻居搜索的并行作业数量。</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">为邻居搜索运行的并行作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">为邻居搜索运行的并行作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。仅影响&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt;方法。</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">为邻居搜索运行的并行作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。不影响&lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法。</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">为邻居搜索运行的并行作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。不影响&lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法。</target>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">要运行的并行作业数。除非在&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; &lt;/a&gt;上下文中，否则 &lt;code&gt;None&lt;/code&gt; 表示1 。 &lt;code&gt;-1&lt;/code&gt; 表示使用所有处理器。有关更多详细信息，请参见&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">训练数据的传递次数(也就是epochs)。默认值为None。已废弃,将在0.21中删除。</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">冗余特征的数量。这些特征是作为信息特征的随机线性组合生成的。</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">回归目标的数量,即与样本相关的y输出向量的维度。默认情况下,输出是一个标量。</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">优化程序的重新启动次数，用于查找最大化对数边际可能性的内核参数。优化程序的第一次运行是从内核的初始参数执行的，其余的（如果有的话）是从允许的theta值空间中随机抽取的theta采样对数均匀性进行的。如果大于0，则所有边界必须是有限的。请注意，n_restarts_optimizer == 0表示执行了一次运行。</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">优化程序的重新启动次数，用于查找最大化对数边际可能性的内核参数。优化程序的第一次运行是从内核的初始参数执行的，其余的（如果有的话）是从允许的theta值空间中随机抽取的theta采样对数均匀性进行的。如果大于0，则所有边界必须是有限的。请注意，n_restarts_optimizer = 0表示执行了一次运行。</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">棋盘结构中的行、列簇数。</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">行和列群的数量。</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">S曲线上的样本点数量。</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">一个点被视为核心点的邻域中的样本数(或总权重)。这包括该点本身。</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">从高斯过程中抽取的样本数量。</target>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">估计器为每个要素处理的样本数。如果不缺少样本，则 &lt;code&gt;n_samples_seen&lt;/code&gt; 将为整数，否则将为数组。将在新的调用中重置为fit，但在 &lt;code&gt;partial_fit&lt;/code&gt; 调用中递增。</target>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">估计器处理的样本数。将在新的调用中重置为fit，但在 &lt;code&gt;partial_fit&lt;/code&gt; 调用中递增。</target>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">训练每个基础估计器时,从X中抽取的样本数。</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">每批取样的数量。</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">每个批次要使用的样本数。仅在调用 &lt;code&gt;fit&lt;/code&gt; 时使用。如果 &lt;code&gt;batch_size&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; ，则从数据中推断 &lt;code&gt;batch_size&lt;/code&gt; 并将其设置为 &lt;code&gt;5 * n_features&lt;/code&gt; ，以在近似精度和内存消耗之间取得平衡。</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">要使用的样本数量。如果没有给定,则使用所有样本。</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">样品的数量;</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">delta中包含的秒数</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">选取的特征数量与交叉验证。</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">所选特征的数量;</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">最终模型的阶段数在属性 &lt;code&gt;n_estimators_&lt;/code&gt; 中可用。</target>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">细化网格的次数。如果传递了明确的字母值,则不使用。</target>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">森林中的树木数量。</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">弱学习者（即回归树）的数量由参数 &lt;code&gt;n_estimators&lt;/code&gt; 控制；&lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;每个树的大小&lt;/a&gt;可以通过设定经由树的深度来控制 &lt;code&gt;max_depth&lt;/code&gt; 或者通过经由设定叶节点的数目 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 。该 &lt;code&gt;learning_rate&lt;/code&gt; 是在范围（0.0，1.0]经由过度拟合控制一个超参数&lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;收缩&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">弱学习者的数量由参数 &lt;code&gt;n_estimators&lt;/code&gt; 控制。该 &lt;code&gt;learning_rate&lt;/code&gt; 参数控制弱学习的最终组合的贡献。默认情况下，学习能力差的人是决策树桩。可以通过 &lt;code&gt;base_estimator&lt;/code&gt; 参数指定不同的弱学习者。要调整以获得良好结果的主要参数是 &lt;code&gt;n_estimators&lt;/code&gt; 和基本估计量的复杂性（例如，其深度 &lt;code&gt;max_depth&lt;/code&gt; 或考虑分割 &lt;code&gt;min_samples_split&lt;/code&gt; 所需的最小样本数）。</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">该对象解决的问题与LassoCV对象相同。但是,与LassoCV不同的是,它自己寻找相关的alphas值。一般来说,由于这个特性,它将更加稳定。但是,对于严重的多线性数据集来说,它是比较脆弱的。</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">用于拟合数据的对象。</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">对象的 &lt;code&gt;best_score_&lt;/code&gt; 和 &lt;code&gt;best_params_&lt;/code&gt; 属性存储最佳平均得分和与该得分相对应的参数设置：</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">目标函数的最小化是W和H的交替最小化。</target>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">目标函数是:</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">在这种情况下,要最小化的目标函数是</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">假设观测值是由低维潜伏因子的线性变换和添加的高斯噪声引起的。在不失一般性的前提下,因子按照零均值和单位协方差的高斯分布。噪声也是零均值,并具有一个任意的对角协方差矩阵。</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">要聚类的观测值。必须注意的是,数据将被转换为C级排序,如果给定的数据不是C级连续的,将导致内存拷贝。</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">观测值,我们计算其中的马哈兰诺比斯距离。假设观测值的分布与拟合中使用的数据相同。</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">所得分数总是严格大于0,最佳值为1。</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">相对于其余的元分类器还实现了 &lt;code&gt;predict_proba&lt;/code&gt; 方法，只要该方法由基本分类器实现即可。此方法在单标签和多标签情况下都返回类成员资格的概率。请注意，在多标签情况下，概率是给定样本属于给定类别的边际概率。这样，在多标签情况下，给定样本的所有可能标签上的这些概率之和&lt;em&gt;将不会&lt;/em&gt;像在单标签情况下那样合计为一。</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">训练样本的相反LOF。越高，越正常。离群值的LOF得分往往接近1（ &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 接近-1），而离群值的LOF得分往往更高。</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">与每个输入样本的局部异常因子相反。越低,越不正常。</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">对于给定数据集的最佳算法是一个复杂的选择,取决于很多因素。</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">使用最大似然对每个特征独立估计最小化偏斜度的最优lambda参数。</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">Lasso的优化目标是。</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">MultiTaskElasticNet的优化目标是。</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">MultiTaskLasso的优化目标是。</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">case method ='lasso'的优化目标是：</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">可选的extra参数将被附加到废弃信息和docstring中。注意:如果要使用默认的extra参数,请在括号中填一个空。</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">可选参数 &lt;code&gt;whiten=True&lt;/code&gt; 使得可以将数据投影到奇异空间上，同时将每个分量缩放到单位方差。如果下游模型对信号的各向同性有很强的假设，这通常很有用：例如，带有RBF内核和K-Means聚类算法的支持向量机就是这种情况。</target>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">分类器链中标签的顺序。</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">链的顺序可以通过提供一个整数列表来明确设置。例如,对于长度为5的链...。</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">转换后的特征矩阵中的列顺序遵循在 &lt;code&gt;transformers&lt;/code&gt; 列表中指定列的顺序。除非在 &lt;code&gt;passthrough&lt;/code&gt; 关键字中指定，否则未指定的原始要素矩阵的列将从生成的转换后的要素矩阵中删除。用 &lt;code&gt;passthrough&lt;/code&gt; 指定的那些列将添加到变压器输出的右侧。</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">原始数据</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">原始数据集由92 x 112组成,而这里的版本由64x64的图像组成。</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">Weinberger等人对散列技巧的最初表述是使用两个独立的散列函数\(h\)和\(\xi\)来分别确定一个特征的列索引和符号。本实施例的工作原理是假设MurmurHash3的符号位与其他位无关。</target>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">原始图像的像素为250 x 250,但默认的slice和resize参数将其缩小到62 x 47。</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">其他内核</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">行、列标签向量的外积显示了棋盘结构的表示。</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">根据以下公式创建输出 &lt;code&gt;y&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">通过将具有 &lt;code&gt;n_informative&lt;/code&gt; 非零回归变量的（可能有偏差的）随机线性回归模型应用于先前生成的输入以及一些具有一定可调比例的高斯中心噪声来生成输出。</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">奇异值分解的输出仅在奇异矢量的符号置换之前是唯一的。如果 &lt;code&gt;flip_sign&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; ，则通过使左奇异向量中每个分量的最大负载为正，来解决符号歧义。</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">3个模型的输出组合成一个二维图,其中节点代表股票,边缘是。</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">变换的输出有时被称为1-of-K编码方案。</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">变换的输出有时被一些作者称为1-of-K编码方案。</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">的输出值。</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">Isomap的总体复杂度为/(O[D \log(k)N \log(N)]+O[N^2(k+\log(N))]+O[d N^2]\)。</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">MLLE的总体复杂度为\(O[D \log(k)N \log(N)]+O[D N k^3]+O[N (k-D)k^2]+O[d N^2]\)。</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">频谱嵌入的总体复杂度为/(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\)。</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">标准HLLE的总体复杂度为:/(O[D log(k)N log(N)]+O[D N k^3]+O[N d^6]+O[d N^2])。</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">标准LLE的总体复杂度为/(O[D \log(k)N \log(N)]+O[D N k^3]+O[d N^2]\)。</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">标准LTSA的总体复杂度为:\(O[D \log(k)N \log(N)]+O[D N k^3]+O[k^2 d]+O[d N^2]\)。</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">p值,近似于偶然得到分数的概率。其计算方法为:</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">当 &lt;code&gt;numpy&lt;/code&gt; 使用 &lt;code&gt;Accelerate&lt;/code&gt; Framework 时，并行版本的K-Means在OS X上已损坏。这是预期的行为：可以在派生后调用 &lt;code&gt;Accelerate&lt;/code&gt; ,但是您需要使用Python二进制文件（在posix下不能执行多处理）来执行子进程。</target>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">参数 &lt;code&gt;learning_rate&lt;/code&gt; 与参数 &lt;code&gt;n_estimators&lt;/code&gt; （要适应的弱学习者的数量）强烈交互。较小的 &lt;code&gt;learning_rate&lt;/code&gt; 值需要大量弱学习者，以保持恒定的训练错误。经验证据表明， &lt;code&gt;learning_rate&lt;/code&gt; 的较小值有利于更好的测试错误。&lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt;建议将学习速率设置为一个较小的常数（例如 &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; ），并通过尽早停止选择 &lt;code&gt;n_estimators&lt;/code&gt; 。有关 &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;n_estimators&lt;/code&gt; 之间的交互的更详细讨论，请参见&lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">需要参数 &lt;code&gt;memory&lt;/code&gt; 以缓存变压器。 &lt;code&gt;memory&lt;/code&gt; 可以是包含用于存放转换器的目录的字符串，也可以是&lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt;对象：</target>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">参数\（\ nu \）也称为&lt;strong&gt;学习率，&lt;/strong&gt;因为它缩放梯度下降过程的步长；可以通过 &lt;code&gt;learning_rate&lt;/code&gt; 参数进行设置。</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">参数epsilon控制了应被归类为离群值的样本数量。epsilon越小,对离群值的影响越大。</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">要探索的参数网格,作为一个将估计器参数映射到允许值序列的字典。</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">参数l1_ratio对应于glmnet R包中的alpha,而alpha对应于glmnet中的lambda参数。更具体地说,优化目标是:</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">参数l1_ratio对应于glmnet R包中的alpha，而alpha对应于glmnet中的lambda参数。具体来说，l1_ratio = 1是套索惩罚。当前，除非您提供自己的alpha序列，否则l1_ratio &amp;lt;= 0.01不可靠。</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">控制学习函数平滑度的参数nu。nu越小,近似函数越不平滑。当nu=inf时,内核相当于RBF内核,nu=0.5时相当于绝对指数内核。重要的中间值是nu=1.5(一次可微分函数)和nu=2.5(两次可微分函数)。需要注意的是,不在[0.5,1.5,2.5,inf]的nu值会产生相当高的计算成本(大约10倍),因为它们需要评估修改后的Bessel函数。此外,与l相反,nu被固定在其初始值上,没有进行优化。</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">参数/(sigma_y/)和/(mu_y/)采用最大似然法估算。</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">参数\(\theta_y\)由最大似然的平滑版本估计,即相对频率计数。</target>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt;类的参数实现提出了权重分布的两种先验类型：具有Dirichlet分布的有限混合模型和具有Dirichlet过程的无限混合模型。在实践中，Dirichlet Process推理算法是近似的，并使用具有固定最大组件数的截断分布（称为&amp;ldquo;折断表示&amp;rdquo;）。实际使用的组件数量几乎总是取决于数据。</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">用于应用这些方法的估计器的参数是通过在参数网格上的交叉验证网格搜索来优化的。</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">通过对参数设置的交叉验证搜索,优化应用这些方法的估计器的参数。</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">所选特征的功率变换参数。</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">根据评分参数,选择的是能使憋出的数据得分最大化的参数。</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">所选择的参数是那些能使留出的数据得分最大化的参数,除非传递了一个显式的得分,在这种情况下,会使用它来代替。</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">已评估的参数;</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">每个节点的父级。仅在指定连接矩阵时返回，否则返回&amp;ldquo; None&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;grid&lt;/code&gt; 上评估偏相关函数。对于回归和二进制分类， &lt;code&gt;n_classes==1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">被动进取算法是用于大规模学习的一系列算法。它们与感知器相似，因为它们不需要学习率。然而，这违背了感知，它们包括调整参数 &lt;code&gt;C&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">要用作数据存储的基础目录的路径或None。如果给定None,则不进行缓存,Memory对象是完全透明的。这个选项从0.12版本开始取代了cachedir。</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">scikit-learn data dir的路径。</target>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">要使用的惩罚（又称正则化术语）。默认值为&amp;ldquo; l2&amp;rdquo;，这是线性SVM模型的标准正则化器。&amp;ldquo; l1&amp;rdquo;和&amp;ldquo; elasticnet&amp;rdquo;可能会给模型带来稀疏性（功能选择），而&amp;ldquo; l2&amp;rdquo;是无法实现的。</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">要使用的惩罚(也就是正则化术语)。默认值为无。</target>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">随机化搜索的性能稍差,不过这很可能是一种噪声效应,不会延续到憋屈的测试集上。</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">然后，通过&lt;em&gt;k&lt;/em&gt;倍交叉验证报告的性能指标是循环中计算的值的平均值。这种方法在计算上可能会很昂贵，但不会浪费太多数据（固定任意验证集时就是这种情况），这在诸如样本数量非常少的逆推断之类的问题中具有主要优势。</target>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">比较了SAMME和SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;算法的性能。SAMME.R使用概率估计来更新加性模型，而SAMME仅使用分类。如示例所示，SAMME.R算法通常比SAMME收敛更快，从而以更少的提升迭代次数实现了更低的测试误差。每次提升迭代后，测试集上每种算法的误差显示在左侧，中间显示每棵树的测试集上的分类误差，右边显示每棵树的提升权重。在SAMME.R算法中，所有树的权重均为1，因此未显示。</target>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">然后在模型选择步骤中未使用的专用评估集上测量所选超参数和训练模型的性能。</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">核的周期性。</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">混乱度定义为:(k=2^{(S)}/)其中(S)是条件概率分布的香农熵。一个边模的迷惑性为/(k),因此/(k)实际上就是t-SNE在生成条件概率时考虑的最近邻居的数量。复杂度越大,最近邻数越多,对小结构的敏感度越低。相反,较低的plexity考虑的邻域数量较少,因此忽略了更多的全局信息,而偏重于局部邻域。随着数据集规模的增大,需要更多的点来获得合理的局部邻域样本,因此可能需要更大的迷惑性。同样,噪声较大的数据集将需要更大的混淆度值,以涵盖足够的局部邻域,从而超越背景噪声。</target>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">迷惑性与其他歧路学习算法中使用的最近邻居数量有关。较大的数据集通常需要较大的困惑度。考虑选择一个5到50之间的值。这个选择并不是非常关键,因为t-SNE对这个参数相当不敏感。</target>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">缺失值的占位符。所有出现的 &lt;code&gt;missing_values&lt;/code&gt; 都将被估算。</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">缺失值的占位符。所有出现的 &lt;code&gt;missing_values&lt;/code&gt; 都将被估算。对于编码为np.nan的缺失值，请使用字符串值&amp;ldquo; NaN&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">该图代表了分类器的学习曲线:分类准确率在迷你批次过程中的变化。准确度是在前1000个样本上测量的,作为一个验证集。</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">该图显示了线性判别分析和二次判别分析的决策边界。最下面一行显示线性判别分析只能学习线性边界,而二次判别分析可以学习二次边界,因此更加灵活。</target>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">该图显示了四个单向和两个双向偏倚图。单向PDP的目标变量为：平均收入（ &lt;code&gt;MedInc&lt;/code&gt; ），平均。每户居住人数（ &lt;code&gt;AvgOccup&lt;/code&gt; ），平均房屋年龄（ &lt;code&gt;HouseAge&lt;/code&gt; ）和平均 每户房间数（ &lt;code&gt;AveRooms&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">图中显示的是离散化编码不变的区域。</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">下图显示了参数 &lt;code&gt;C&lt;/code&gt; 对分隔线的影响。很大的 &lt;code&gt;C&lt;/code&gt; 值基本上告诉我们的模型，我们对数据的分布没有太多的信心，并且仅考虑接近分隔线的点。</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">图中首先显示了使用三个聚类的K-means算法的结果。然后显示初始化不好对分类过程的影响。通过将n_init设置为1(默认值为10),减少了算法使用不同中心种子的运行次数。接下来的图显示了使用8个簇会带来什么,最后是地面真相。</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">图中以boxplot表示预测延迟的分布。</target>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">图中训练点为纯色,测试点为半透明。右下角显示了测试集的分类精度。</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">上面观察到的点云在一个方向上非常平坦：三个单变量特征之一几乎可以使用另外两个来精确计算。PCA查找其中的数据不是方向&lt;em&gt;平坦&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">分。</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">多项式核定义为:</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">每个特征簇的集合值。</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">可能的选项是&amp;ldquo;铰链&amp;rdquo;，&amp;ldquo;对数&amp;rdquo;，&amp;ldquo; modified_huber&amp;rdquo;，&amp;ldquo; squared_hinge&amp;rdquo;，&amp;ldquo; perceptron&amp;rdquo;或回归损失：&amp;ldquo; squared_loss&amp;rdquo;，&amp;ldquo; huber&amp;rdquo;，&amp;ldquo; epsilon_insensitive&amp;rdquo;或&amp;ldquo; squared_epsilon_insensitive&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">用来计算点间距离的Minkowski度量的功率。</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">幂级变换在建模问题中是很有用的,因为在这些问题中需要同质性和正态性。下面是Box-Cox和Yeo-Johnwon应用于六种不同的概率分布的例子:对数正态、Chi-squared、Weibull、高斯、均匀和双模。对数正态,Chi-squared,Weibull,Gaussian,Uniform,and Bimodal.</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">功率变换法。可用的方法有:</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">功率变换方法。当前，&amp;ldquo; box-cox&amp;rdquo;（Box-Cox转换）是唯一可用的选项。</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">精度是比率 &lt;code&gt;tp / (tp + fp)&lt;/code&gt; ，其中 &lt;code&gt;tp&lt;/code&gt; 是真实肯定的数目， &lt;code&gt;fp&lt;/code&gt; 是错误肯定的数目。直观上讲，精度是分类器不将阴性样本标记为阳性的能力。</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">混合物中每种成分的精密度矩阵。精度矩阵是协方差矩阵的逆矩阵。协方差矩阵是对称正定的，因此可以通过精度矩阵等效地对高斯的混合进行参数化。存储精度矩阵而不是协方差矩阵使在测试时计算新样本的对数似然更有效率。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">与当前协方差对象相关的精度矩阵。</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">各分量在平均分布(高斯)上的精度。</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">均值分布上的先验精度（高斯）。控制扩展到可以放置均值的位置。较小的值将每个聚类的均值集中在 &lt;code&gt;mean_prior&lt;/code&gt; 周围。</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">均值分布上的先验精度（高斯）。控制扩展到可以放置均值的位置。较小的值将每个聚类的均值集中在 &lt;code&gt;mean_prior&lt;/code&gt; 周围。参数的值必须大于0。如果为None，则将其设置为1。</target>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">精度-召回曲线显示了不同阈值下精度和召回率之间的权衡。曲线下的高区域代表着高召回和高精确度,其中高精确度与低假阳性率有关,高召回与低假阴性率有关。两者得分高,说明分类器返回的结果准确(高精度),同时也返回了大部分的阳性结果(高召回率)。</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">返回X中每个样本的预测类C。</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">输入样本的预测类对数概率计算为集合中基础估计器的平均预测类概率的对数。</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">输入样本的预测类对数概率计算为森林中树木的平均预测类概率的对数。</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">输入样本的预测类对数概率被计算为集合中分类器的加权平均预测类对数概率。</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">输入样本的预测类是由森林中的树木根据它们的概率估计加权投票。也就是说,预测的类是所有树木的平均概率估计值最高的一类。</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">输入样本的预测类别被计算为具有最高平均预测概率的类别。如果基本估算器未实现 &lt;code&gt;predict_proba&lt;/code&gt; 方法，则它会诉诸表决。</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">输入样本的预测类是以集合中分类器的加权平均预测值来计算的。</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">输入样本的预测类概率计算为森林中树木的平均预测类概率。一棵树的类概率是叶子中同一类样本的分数。</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">输入样本的预测类别概率被计算为集合中基本估计量的平均预测类别概率。如果基本估计量未实现 &lt;code&gt;predict_proba&lt;/code&gt; 方法，则该方法将诉诸表决，并且输入样本的预测类概率表示预测每个类的估计量的比例。</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">输入样本的预测类概率计算为集合中分类器的加权平均预测类概率。</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">预测类概率是指叶子中同一类样本的分数。</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">预测类。</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">预测类,或预测值。</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">预测类。</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">模型中每个类的样本的预计对数概率，其中类按照它们在 &lt;code&gt;self.classes_&lt;/code&gt; 中的顺序进行排序。相当于log（predict_proba（X））</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">模型中每个类的样本的预测概率，其中类按照它们在 &lt;code&gt;self.classes_&lt;/code&gt; 中的顺序进行排序。</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">预测的概率。</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">输入样本的预测回归目标被计算为集合中估计器的平均预测回归目标。</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">输入样本的预测回归目标计算为森林中树木的平均预测回归目标。</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">输入样本的预测回归值被计算为集合中分类器的加权中值预测。</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">预测的回归值。</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">预测的目标值;</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">输入样本的预测值。</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">预测值;</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">预测对观测值进行插值(至少对常规内核而言)。</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">预测是概率的(高斯),因此人们可以计算经验置信区间,并根据这些区间决定是否应该在某个感兴趣的区域重新进行预测(在线拟合、自适应拟合)。</target>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">先前引入的度量标准&lt;strong&gt;并未针对随机标记进行规范化&lt;/strong&gt;：这意味着，根据样本，聚类和基本事实类别的数量，完全随机标记的均匀性，完整性和v度量不一定总会产生相同的值。特别&lt;strong&gt;是当簇数很大时，随机标记不会产生零分&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">最近邻方法的原理是找到距离新点最近的预定义数量的训练样本，并从中预测标签。样本数可以是用户定义的常数（k近邻学习），也可以基于点的局部密度而变化（基于半径的邻居学习）。距离通常可以是任何度量标准：标准欧几里德距离是最常见的选择。基于邻居的方法被称为&lt;em&gt;非通用&lt;/em&gt;机器学习方法，因为它们仅&amp;ldquo;记住&amp;rdquo;其所有训练数据（可能转换为快速索引结构，例如&lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt;或&lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">下图显示了由&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt;内核生成的GP的先验和后验：</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">ExpSineSquared核产生的GP的前值和后值如下图所示。</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">协方差分布上自由度数的先验(Wishart)。</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">协方差分布上的自由度数的先验（Wishart）。如果为None，则将其设置为 &lt;code&gt;n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">协方差分布的先验（Wishart）。如果为None，则使用X的协方差初始化半协方差先验。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">协方差分布的先验（Wishart）。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">平均值分布的先验(高斯)。</target>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">均值上的先验分布（高斯）。如果为None，则将其设置为X的均值。</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">\（\ alpha \）和\（\ lambda \）的先验被选择为&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma分布&lt;/a&gt;，这是高斯精度的共轭先验。</target>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">概率模型是使用交叉验证创建的,所以结果可能与预测得到的结果略有不同。另外,它在很小的数据集上会产生无意义的结果。</target>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">绘制每个类的概率。仅在 &lt;code&gt;return_distributions=True&lt;/code&gt; 时返回。</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">给每个类别绘制每个要素的概率。仅在 &lt;code&gt;return_distributions=True&lt;/code&gt; 时返回。</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">系数为零的概率(见注释)。数值越大,稀疏性越大。</target>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">众所周知,学习最优决策树的问题在最优性的几个方面下是NP-完备的,甚至对于简单的概念也是如此。因此,实用的决策树学习算法都是基于启发式算法,如贪婪算法,在每个节点上进行局部最优决策。这种算法不能保证返回全局最优的决策树。可以通过在集合学习器中训练多棵树来缓解这种情况,其中特征和样本是随机采样与替换的。</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">聚类中解决的问题</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">监督学习中解决的问题</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">进度表： &lt;code&gt;verbose&lt;/code&gt; 的值越高，消息越多：</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">项目邮件列表</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">预计数据:</target>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">在原始MCD估计的支持中包含的点的比例。默认值是None,这意味着算法中会使用support_fraction的最小值。[n_sample+n_features+1]/2。</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">支持原始MCD估算的点数比例。如果为None，则将在算法内使用support_fraction的最小值： &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">预留的训练数据比例作为早期停止的验证集。必须在0到1之间。仅在 &lt;code&gt;n_iter_no_change&lt;/code&gt; 设置为整数时使用。</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">训练数据的比例,作为早期停止的验证集。必须在0和1之间,只有当early_stopping为真时才使用。</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">训练数据的比例,作为早期停止的验证集。必须在0和1之间,只有当early_stopping为True时才使用。</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">分配给每个类别的样本比例。如果为None，则类是平衡的。请注意，如果 &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; ，则会自动推断最后一个类的权重。如果 &lt;code&gt;weights&lt;/code&gt; 之和超过1，则可能返回多于 &lt;code&gt;n_samples&lt;/code&gt; 个样本。</target>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">流水线的目的是组装几个步骤，这些步骤可以在设置不同参数的同时交叉验证。为此，它可以使用各个步骤的名称和以&amp;ldquo; __&amp;rdquo;分隔的参数名称来设置各个步骤的参数，如下例所示。可以通过将参数的名称设置为另一个估计器来完全替换步骤的估计器，或者通过将其设置为&amp;ldquo;无&amp;rdquo;来删除变形器。</target>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">用于生成模型的python源代码</target>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">使用&amp;ldquo;分位数&amp;rdquo;策略进行预测的分位数。0.5的分位数对应于中位数，而0.0的分位数对应于最小值，而1.0的分位数对应于最大值。</target>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">我们使用的量是报价的日变化:一天内联动的报价往往会出现同流合污的情况。</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">查询点。如果没有提供,将返回每个索引点的邻居。在这种情况下,查询点不被认为是自己的邻居。</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">查询样本,以计算训练样本的局部离群因子。</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">查询样本,以计算训练样本的局部离群因子。</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">通过合并一个新样本和最近的子群得到的子群半径应该小于阈值。否则将启动一个新的子簇。把这个值设置得很低,会促进分裂,反之亦然。</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">随机森林回归器只能预测每个目标的观测值范围内或接近零的数值。因此,预测值会偏向圆心。</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">随机数生成器用于生成随机链订单。</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">随机对称的正定矩阵。</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">随机化搜索和网格搜索探索的参数空间完全相同。在参数设置上的结果十分相似,而随机化搜索的运行时间却大大降低。</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">原始(未经调整)兰德指数则由以下公式给出:</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">然后，使用以下方案将原始RI分数&amp;ldquo;根据机会调整&amp;rdquo;为ARI分数：</target>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">修正和重新加权前的原始稳健估计协方差。</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">修正和重新加权前的原始稳健估计位置。</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">实际数据位于 &lt;code&gt;filenames&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 属性中。目标属性是类别的整数索引：</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">召回率是比率 &lt;code&gt;tp / (tp + fn)&lt;/code&gt; ，其中 &lt;code&gt;tp&lt;/code&gt; 是真实肯定的数目， &lt;code&gt;fn&lt;/code&gt; 是错误否定的数目。直观上，召回是分类器找到所有阳性样本的能力。</target>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">使用公制MDS和非公制MDS重建的点会稍微移动以避免重叠。</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">每个例程计算出的重建误差可用于选择最佳输出尺寸。对于嵌入在\（D \）维参数空间中的\（d \）维流形，重构误差将随着 &lt;code&gt;n_components&lt;/code&gt; 的增加而减小，直到 &lt;code&gt;n_components == d&lt;/code&gt; 为止。</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">即使将噪声添加到投影中，使用L1惩罚进行的重构也会产生零误差的结果（所有像素均成功标记为0或1）。相比之下，L2惩罚（&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt;）对像素产生大量标记错误。与L1惩罚相反，在重建的图像上观察到重要的伪影。特别要注意的是，圆角伪影将角落中的像素分开，与中央磁盘相比，投影造成的投影更少。</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">对于某些度量来说,缩小的距离是一种计算效率更高的度量,它保留了真实距离的等级。例如,在欧几里得距离度量中,缩小的距离是平方欧几里得距离。</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">减少的样品。</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">可以在 &lt;code&gt;best_estimator_&lt;/code&gt; 属性中使用经过调整的估计器，并允许直接在此 &lt;code&gt;GridSearchCV&lt;/code&gt; 实例上使用 &lt;code&gt;predict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">可以在 &lt;code&gt;best_estimator_&lt;/code&gt; 属性中使用经过调整的估计器，并允许直接在此 &lt;code&gt;RandomizedSearchCV&lt;/code&gt; 实例上使用 &lt;code&gt;predict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">回归目标或分类标签(如果适用)。如果是数字型,Dtype为浮动型,如果是分类型,则为对象型。</target>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">回归变量用于预测，并在返回预测之前应用 &lt;code&gt;inverse_func&lt;/code&gt; 或 &lt;code&gt;inverse_transform&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">正则化协方差为。</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1的正则化混合参数。对于l1_ratio = 0，惩罚是元素级L2惩罚（又称为Frobenius范数）。对于l1_ratio = 1，这是元素级的L1损失。对于0 &amp;lt;l1_ratio &amp;lt;1，惩罚是L1和L2的组合。</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">LogisticRegression中的正则化参数C。当C为数组时，fit将为LogisticRegression逐一获取C中的每个正则化参数，并将每个参数的结果存储在 &lt;code&gt;all_scores_&lt;/code&gt; 中，其中列和行代表相应的reg_parameters和功能。</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">Lasso中的正则化参数alpha参数。警告:这不是稳定性选择文章中的alpha参数,它是缩放的。</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">正则化参数:α越高,正则化程度越高,反协方差越稀疏。</target>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">正则化(缩小)后的协方差由下列方式给出:</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">正则化(缩小)后的协方差为:。</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">正则化协方差由下列公式给出:</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">正则化器是添加到损失函数中的一个惩罚,它使用平方欧氏法则L2或绝对法则L1或两者的组合(Elastic Net)将模型参数向零向量收缩。如果因为正则器的原因,参数更新越过0.0值,则更新被截断为0.0,以便学习稀疏模型,实现在线特征选择。</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">在图中的阶梯区域可以观察到召回率和精度之间的关系--在这些阶梯的边缘,阈值的微小变化大大降低了精度,而召回率仅有微小的提高。</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">如果 &lt;code&gt;effective_rank&lt;/code&gt; 等级不为&amp;ldquo;无&amp;rdquo; ，则奇异值配置文件中胖尾巴的相对重要性。</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">单值曲线的胖吵尾相对重要。</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">在宣布收敛之前,结果的相对增量。</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">用作树中决策节点的特征的相对等级（即深度）可用于评估该特征相对于目标变量的可预测性的相对重要性。在树的顶部使用的特征有助于更大比例的输入样本的最终预测决策。他们所贡献&lt;strong&gt;的样本&lt;/strong&gt;的&lt;strong&gt;预期比例&lt;/strong&gt;因此可以用作&lt;strong&gt;特征相对重要性&lt;/strong&gt;的估计。在scikit-learn中，将特征贡献的样本比例与杂质分解所导致的杂质减少相结合，以创建对该特征的预测能力的归一化估计。</target>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">其余奇异值的尾巴是胖的，减少为：</target>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">报告的平均值包括微观平均值（对真阳性，假阴性和假阳性的总数进行平均），宏观平均值（对每个标签的未加权平均值进行平均），加权平均值（对每个标签的支持加权平均值进行平均）和样品平均值（仅对于多标签）分类）。有关平均值的更多详细信息，另请参见&lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">X(Xk+1)块的残差矩阵由当前X分数上的放缩得到:x_score。</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">通过对当前X分数的放缩,得到Y(Yk+1)块的残差矩阵。这就执行了PLS回归,称为PLS2。这种模式是面向预测的。</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">通过对当前Y分的放缩,得到Y(Yk+1)块的残差矩阵。</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">通过对当前Y得分的放缩,得到Y(Yk+1)块的残差矩阵。这就执行了一个规范的对称版本的PLS回归。但与CCA略有不同。这主要用于建模。</target>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">&lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;的结果有更大的偏倚：差异使人联想到原始图像的局部强度值。</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;的结果可能与使用&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;获得的结果不同，因为元素以不同的方式分组。函数&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;对交叉验证折叠取平均值，而&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;只是简单地返回来自几个不同模型的标签（或概率）。因此，&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;并不是对泛化误差的适当度量。</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">在 &lt;code&gt;GridSearchCV&lt;/code&gt; 对象上调用 &lt;code&gt;fit&lt;/code&gt; 的结果是一个分类器，我们可以用来 &lt;code&gt;predict&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">这个方法的结果与np.diag(self(X))相同;但是,由于只评估对角线,所以可以更有效地评估。</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">结果点&lt;em&gt;并不&lt;/em&gt;一定是按距离排序与他们的查询点。</target>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">由此产生的卡林斯基-哈拉巴兹分。</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">由此产生的戴维斯-博尔丁得分。</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">由此可见,福克斯-马洛斯的得分。</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">由此产生的双簇结构是块状对角线,因为每行每列正好属于一个双簇。</target>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">结果数据集包含序数属性，可以在&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; 中&lt;/a&gt;进一步使用。</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">所得的核定义为k_exp(X,Y)=k(X,Y)**指数。</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">得出的核定义为k_prod(X,Y)=k1(X,Y)*k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">得出的核定义为k_sum(X,Y)=k1(X,Y)+k2(X,Y)</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">生成的模型称为&lt;em&gt;贝叶斯岭回归&lt;/em&gt;，与经典的&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;相似。在模型拟合期间，共同估算参数\（w \），\（\ alpha \）和\（\ lambda \）。其余的超参数是\（\ alpha \）和\（\ lambda \）上的伽玛先验参数。这些通常被选择&lt;em&gt;为非信息性的&lt;/em&gt;。通过最大化&lt;em&gt;边际对数似然&lt;/em&gt;来估计参数。</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">由此产生的补丁会被分配到一个专用数组中。</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">然后,产生的变换器已经学会了数据的有监督的、稀疏的、高维的分类嵌入。</target>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">返回值是一个交叉验证器，该交叉验证器通过 &lt;code&gt;split&lt;/code&gt; 方法生成训练/测试的拆分。</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">返回的数据集是 &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo; bunch&amp;rdquo;：一个简单的holder对象，其字段可以方便地用作python &lt;code&gt;dict&lt;/code&gt; 键或 &lt;code&gt;object&lt;/code&gt; 属性来访问，例如 &lt;code&gt;target_names&lt;/code&gt; 包含所请求类别名称的列表：</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">所有班级的返回估价按班级的标签排序。</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">所有班级的返回估价是按班级的标签排序的。</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的对象是一个MemorizedFunc对象，可以调用（行为类似于函数），但是提供了用于缓存查找和管理的其他方法。请参阅&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt;的文档。</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">右边更丰富的字典并不是尺寸越大越好,为了保持在同一数量级上,会进行更重的子采样。</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">正确的图对应于相同的图，但改用决策树的装袋合奏。在这两个图中，我们可以观察到偏差项大于前一种情况。在右上方的图中，平均预测值（青色）与最佳可能模型之间的差异较大（例如，请注意 &lt;code&gt;x=2&lt;/code&gt; 附近的偏移量）。在右下图中，偏差曲线也略高于左下图中。但是，就方差而言，预测范围较窄，这表明方差较低。确实，如右下图所示，方差项（绿色）低于单个决策树。总体而言，偏差方差分解因此不再相同。权衡更适合装袋：对数据集的引导副本上拟合的多个决策树进行平均会稍微增加偏差项，但允许方差的较大减少，从而导致较低的总体均方差（将红色曲线与较低的红色曲线进行比较）数据）。脚本输出也证实了这种直觉。套袋集成的总误差低于单个决策树的总误差，实际上，这种差异主要源于方差的减少。</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">右图显示了模型找到的系数与所选向量w之间的平均平方误差,较少的正则化模型能检索到精确的系数(误差等于0),较强的正则化模型会增加误差。</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">健壮的MCD，提供了较低的错误\（n_ \ text {samples}&amp;gt; 5n_ \ text {features} \）</target>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">行是样品,列是。萼片长度,萼片宽度,花瓣长度和花瓣宽度。</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">s参数,用于随机缩放不同特征的惩罚。应该在0和1之间。</target>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">同一组不会出现在两个不同的折页中(不同组的数量必须至少等于折页的数量)。</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">然后,可以将同一个变换器实例应用到一些在拟合调用期间未见的新测试数据上:将应用相同的缩放和移位操作,以与在列车数据上执行的变换保持一致。</target>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">显示的样本数是用任何可能存在的sample_weights加权的。</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">样本权重重新调整了C参数的大小,这意味着分类器更加强调把这些点做对。这种效果可能往往很微妙。为了强调这里的效果,我们特别对离群值进行了加权,使得决策边界的变形非常明显。</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">整数的采样子集。被选中的整数子集可能不是随机的,参见方法参数。</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">该数据集中的样本对应于美国30&amp;times;30m的森林斑块，是为了预测每个斑块的覆盖类型（即树木的主要树种）而收集的。有七个Covertype，这使它成为多类分类问题。每个样本都有54个功能，在&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;数据集的首页&lt;/a&gt;上进行了介绍。其中一些功能是布尔指标，而其他功能则是离散或连续测量。</target>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">样品。</target>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">然后,缩放器实例可以用于新的数据,以同样的方式对训练集进行转换。</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">scikit-learn项目提供了一套机器学习工具,既可以用于新颖性检测,也可以用于异常值检测。这种策略是通过对象以无监督的方式从数据中学习来实现的。</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">scikit-learn提供一个对象&lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt;。EllipticEnvelope使稳健的协方差估计适合数据，从而使椭圆适合中心数据点，而忽略中心模式之外的点。</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">分数以上应选择哪些功能。</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">每份简历拆分的测试成绩的分数阵列。</target>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">每个简历上火车分数的分数数组。仅当 &lt;code&gt;return_train_score&lt;/code&gt; 参数为 &lt;code&gt;True&lt;/code&gt; 时,此选项才可用。</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">分数在-1之间为不正确聚类,+1为高度密集聚类。零分左右的分数表示重叠的聚类。</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">分数被定义为群内分散度与群间分散度之比。</target>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">分数被定义为群内距离与群间距离的比率。</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">分数计算速度快</target>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">当集群密集且分离良好时,得分较高,这与集群的标准概念有关。</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">分数范围从0到1，或者当使用Adjusted &lt;code&gt;adjusted=True&lt;/code&gt; 时，分数重新调整为\（\ frac {1} {1-\ text {n \ _classes}} \）到1（含）范围内，表现随机得分0。</target>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">分值范围从0到1。分值越高,说明两个群组之间的相似度越高。</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">计分器可调用对象/函数必须具有其作为计 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 签名（estimator，X，y）。</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">射手。</target>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">沿途每个特征的得分。</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">每种组合的得分。</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">HuberRegressor的分数可能无法直接与TheilSen和RANSAC进行比较,因为它并不试图完全过滤异常值,而是减少其影响。</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">在 &lt;code&gt;cv_results_&lt;/code&gt; dict中，以 &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; 结尾的键（ &lt;code&gt;'mean_test_precision'&lt;/code&gt; ， &lt;code&gt;'rank_test_precision'&lt;/code&gt; 等）中可获得所有得分手的得分。</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">最佳惩罚参数(α)的搜索是在迭代细化的网格上进行的:首先计算网格上的交叉验证分数,然后以最大值为中心建立新的细化网格,以此类推。</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">搜索的参数。</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">产品内核的第二个基核。</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">和核的第二基核。</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">第二个例子显示了协方差的最小协方差确定器鲁棒估计器集中在数据分布的主模式上的能力:位置似乎很好估计,尽管由于香蕉形分布,协方差很难估计。无论如何,我们可以摆脱一些外围观测值。One-Class SVM能够捕捉到真实的数据结构,但困难的是调整其内核带宽参数,以便在数据散点矩阵的形状和数据过度拟合的风险之间获得一个良好的折中。</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">第二张图是线性支持向量分类器(LinearSVC)的校准曲线。LinearSVC表现出与高斯奈夫贝叶斯相反的行为:校准曲线有一条sigmoid曲线,这是一个信心不足的分类器的典型特征。在LinearSVC的情况下,这是由于铰链损失的边际属性造成的,它让模型专注于接近决策边界(支持向量)的硬样本。</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">第二个图显示了内核超参数的不同选择的对数边际似然性，并用黑点突出显示了第一个图中使用的超参数的两个选择。</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">第二个加载器通常用于人脸验证任务:每个样本是一对属于或不属于同一个人的两张图片。</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">第二个模型是贝叶斯高斯混合模型，该模型具有先验Dirichlet过程，并具有变分推断。先验浓度的低值使模型倾向于使用较少数量的活性成分。该模型&amp;ldquo;决定&amp;rdquo;将其建模能力集中在数据集结构的整体上：由非对角协方差矩阵建模的具有交替方向的点组。这些交替方向大致捕获了原始正弦信号的交替性质。</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">第二种具有较小的噪声水平和较短的长度尺度,它可以通过无噪声函数关系解释大部分的变化。第二种模型具有较高的似然性;然而,根据超参数的初始值,基于梯度的优化也可能收敛到高噪声解。因此,对于不同的初始化,多次重复优化是很重要的。</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">第二个图使用 &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 和 &lt;code&gt;n_init=1&lt;/code&gt; 演示了 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 估计器的一次运行。此运行会导致收敛不良（局部最优），估计中心陷入地面真相群集之间。</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">第二个图是分类器的交叉验证准确性作为 &lt;code&gt;C&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; 的函数的热图。在此示例中，为了说明目的，我们探索了一个相对较大的网格。实际上，从\（10 ^ {-3} \）到\（10 ^ 3 \）的对数网格通常就足够了。如果最佳参数位于网格的边界上，则可以在后续搜索中沿该方向扩展。</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">第二曲线显示，在允许的失真的增加 &lt;code&gt;eps&lt;/code&gt; 允许大大减少尺寸的最小数目 &lt;code&gt;n_components&lt;/code&gt; 用于样品的给定数量的 &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">第二张图可视化了RBF内核SVM和近似内核图的线性SVM的决策面。该图显示了分类器的决策面投射到数据的前两个主成分上。这种可视化应该带着盐分,因为它只是64维决策面的一个有趣的切片。特别要注意的是,一个数据点(用点表示)不一定会被分类到它所在的区域,因为它不会位于前两个主成分所跨越的平面上。</target>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">第二个用例是使用&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;从一个简单的python函数构建一个完全自定义的Scorer对象，该函数可以采用多个参数：</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">伪随机数生成器的种子，用于将小噪声添加到连续变量中以删除重复值。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">选择随机特征进行更新的伪随机数生成器的种子。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。当 &lt;code&gt;selection&lt;/code&gt; =='random'时使用</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">选择随机特征进行更新的伪随机数生成器的种子。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。当 &lt;code&gt;selection&lt;/code&gt; =='random'时使用。</target>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">伪随机数生成器的种子，用于对双坐标下降数据进行混洗（如果 &lt;code&gt;dual=True&lt;/code&gt; ）。当 &lt;code&gt;dual=False&lt;/code&gt; 时，&lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;的基础实现不是随机的， &lt;code&gt;random_state&lt;/code&gt; 对结果没有影响。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">在对数据进行混洗时使用的伪随机数生成器的种子。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">在对数据进行混洗时使用的伪随机数生成器的种子。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。当 &lt;code&gt;solver&lt;/code&gt; =='sag'或'liblinear'时使用。</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">在对数据进行混洗时使用的伪随机数生成器的种子。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;solver&lt;/code&gt; =='sag'时使用。</target>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">伪随机数生成器的种子，在对数据进行混洗以进行概率估计时使用。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">F值的集合。</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">每个输出变量的标签集可以不同。例如，可以为一个输出变量分配一个样本&amp;ldquo;梨&amp;rdquo;，该变量在诸如&amp;ldquo;梨&amp;rdquo;，&amp;ldquo;苹果&amp;rdquo;的一组有限种类中取可能的值。和&amp;ldquo;蓝色&amp;rdquo;或&amp;ldquo;绿色&amp;rdquo;表示第二个输出变量，该变量采用有限的一组颜色（例如&amp;ldquo;绿色&amp;rdquo;，&amp;ldquo;红色&amp;rdquo;，&amp;ldquo;蓝色&amp;rdquo;，&amp;ldquo;黄色&amp;rdquo;）中的可能值...</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">每个样本的标签集，使得 &lt;code&gt;y[i]&lt;/code&gt; 由每个 &lt;code&gt;yt[i, j] == 1&lt;/code&gt; 的 &lt;code&gt;classes_[j]&lt;/code&gt; 组成。</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">当 &lt;code&gt;average != 'binary'&lt;/code&gt; 时要包括的一组标签，如果 &lt;code&gt;average is None&lt;/code&gt; ，则标签的顺序。可以排除数据中存在的标签，例如，以忽略多数否定类别的方式计算多类平均值，而数据中不存在的标签将导致宏平均值中的0成分。对于多标签目标，标签是列索引。默认情况下， &lt;code&gt;y_true&lt;/code&gt; 和 &lt;code&gt;y_pred&lt;/code&gt; 中的所有标签均按排序顺序使用。</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">p值的集合。</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">将依次测试的回归者集合。</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">形状(Nx,Ny)数组中X、Y两点之间的对距离。</target>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">&lt;code&gt;dual_coef_&lt;/code&gt; 的形状为 &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; ，布局有些难以掌握。这些列对应于 &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;一对一&amp;rdquo;分类器中涉及的支持向量。每个支持向量都在 &lt;code&gt;n_class - 1&lt;/code&gt; 分类器中使用。每行中的 &lt;code&gt;n_class - 1&lt;/code&gt; 个条目对应于这些分类器的对偶系数。</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">结果的形状。</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">移位偏移允许零阈值作为异常值。仅可用于新颖性检测（新颖性设置为True时）。参数X应该包含&lt;em&gt;新数据&lt;/em&gt;：如果X包含来自训练的点，则它将在其自己的邻域中考虑后者。同样，在任何点附近都不会考虑X中的样本。</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">每个输入样本的局部异常因子的移位相反。越低,越不正常。负分代表离群值,正分代表异常值。</target>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">两组双联体的相似度。</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">最简单的度量&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;模型称为&lt;em&gt;绝对MDS&lt;/em&gt;，视差由\（\ hat {d} _ {ij} = S_ {ij} \）定义。对于绝对MDS，值\（S_ {ij} \）应该恰好对应于嵌入点中点\（i \）和\（j \）之间的距离。</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">最简单的分类器是&lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;最接近的邻居&lt;/a&gt;：给定新的观测值 &lt;code&gt;X_test&lt;/code&gt; ，在训练集中找到具有最接近特征向量的观测值（即用于训练估计器的数据）。（有关这种类型的分类器的更多信息，请参见在线Scikit学习文档的&lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors部分&lt;/a&gt;。）</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">完成这种维度降低的最简单方法是采取数据的随机投影。虽然这样可以在一定程度上实现数据结构的可视化,但选择的随机性还有很多不足之处。在随机投影中,很可能会丢失数据内比较有趣的结构。</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">使用交叉验证的最简单方法是在估算器和数据集上调用&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;帮助器函数。</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">奇异值分解,(A_n=U Σ V^\top\),提供了行和列的分区。左边奇异向量的子集给出了行的分区,右边奇异向量的子集给出了列的分区。</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">对应于每个所选组件的奇异值。奇异值等于低维空间中 &lt;code&gt;n_components&lt;/code&gt; 变量的2范数。</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">&lt;code&gt;grid_scores_&lt;/code&gt; 的大小等于 &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; ，其中step是在每次迭代中删除的要素数量。</target>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">具有默认参数的模型的大小为\（O（M * N * log（N））\），其中\（M \）是树数，\（N \）是样本数。为了减小模型的大小，您可以更改以下参数： &lt;code&gt;min_samples_split&lt;/code&gt; ， &lt;code&gt;max_leaf_nodes&lt;/code&gt; ， &lt;code&gt;max_depth&lt;/code&gt; 和 &lt;code&gt;min_samples_leaf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">要生成的随机矩阵的大小。</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">回归树基础学习者的人数定义了梯度增强模型可以捕获的变量交互作用的级别。通常，深度为 &lt;code&gt;h&lt;/code&gt; 的树可以捕获 &lt;code&gt;h&lt;/code&gt; 阶的交互。有两种方法可以控制各个回归树的大小。</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">在数据的随机子集上计算轮廓系数时要使用的样本大小。如果 &lt;code&gt;sample_size is None&lt;/code&gt; ，则不使用采样。</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">要取样的集合大小。</target>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">在初始化后,簇的大小、距离和形状可能会有变化,迷惑值,并不总是传达一种意义。</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">歪曲的chi平方核由以下公式给出。</target>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">平滑先验\（\ alpha \ ge 0 \）解决了学习样本中不存在的特征，并防止了进一步计算中的零概率。设置\（\ alpha = 1 \）称为拉普拉斯平滑，而\（\ alpha &amp;lt;1 \）称为Lidstone平滑。</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">求解器&amp;ldquo; liblinear&amp;rdquo;使用坐标下降（CD）算法，并依赖于scikit-learn附带的出色的C ++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR库&lt;/a&gt;。但是，以liblinear实施的CD算法无法学习真正的多项式（多类）模型。取而代之的是，优化问题以&amp;ldquo;一对多&amp;rdquo;的方式分解，因此针对所有类别训练了单独的二进制分类器。这是在后台进行的，因此使用此求解器的&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;实例的行为类似于多类分类器。对于L1惩罚，&lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt;允许计算C的下限，以便获得非&amp;ldquo;空&amp;rdquo;（所有特征权重都为零）模型。</target>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">权重优化的求解器。</target>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;类中实现的求解器是&amp;ldquo; liblinear&amp;rdquo;，&amp;ldquo; newton-cg&amp;rdquo;，&amp;ldquo; lbfgs&amp;rdquo;，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; saga&amp;rdquo;：</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">临时距离矩阵块所需的最大内存。如果为None（默认），则使用 &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">来源也可以&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;在Github&lt;/a&gt;上找到。</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">本教程的来源可以在您的 scikit-learn 文件夹中找到。</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">网格点之间的间距,以度为单位。</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">矩阵分解中的稀码因子。</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">稀码,使这个矩阵的每一列正好有n_nonzero_coefs非零项(X)。</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">稀疏码</target>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">由于截距的学习率缩小,稀疏实现与密集实现产生的结果略有不同。</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">稀疏向量</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">稀疏性实际上是强加在矩阵的cholesky因子上的。因此α并不直接转化为矩阵本身的填充分数。</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">当很少有训练样本可用时，稀疏诱导\（\ ell_1 \）范本也可以防止学习组件受到噪音的干扰。可以通过超参数 &lt;code&gt;alpha&lt;/code&gt; 来调整惩罚程度（以及稀疏程度）。较小的值会导致逐渐正规化的因式分解，而较大的值会将许多系数缩小为零。</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">单个样本的拆分代码的长度为 &lt;code&gt;2 * n_components&lt;/code&gt; 并使用以下规则构造：首先，计算长度为 &lt;code&gt;n_components&lt;/code&gt; 的常规代码。然后，第一 &lt;code&gt;n_components&lt;/code&gt; 所述的条目 &lt;code&gt;split_code&lt;/code&gt; 填充有常规代码矢量的正的部分。拆分代码的后半部分用代码矢量的负数部分填充，只用一个正号填充。因此，split_code是非负的。</target>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">标准的LLE算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">簇的标准差。</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">应用于输出的高斯噪声的标准偏差。</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">高斯噪声的标准差。</target>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">覆盖输入域 &lt;code&gt;X&lt;/code&gt; 的逐步插值函数。</target>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">停止标准。如果不是None，则迭代将在（loss&amp;gt; previous_loss-tol）时停止。默认为无。从0.21默认为1e-3。</target>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">在嵌入空间中使用的策略来分配标签。有两种方法来分配标签后的laplacian嵌入。 k-means可以应用,是一个流行的选择。但它也会对初始化敏感。Discretization是另一种对随机初始化不太敏感的方法。</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">用于在嵌入空间中分配标签的策略。拉普拉斯嵌入后，有两种分配标签的方法。可以应用k均值，它是一种流行的选择。但是它也可能对初始化敏感。离散化是另一种对随机初始化不太敏感的方法。有关离散化方法的更多详细信息，请参见下面引用的&amp;ldquo;多类谱聚类&amp;rdquo;论文。</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">用于在每个节点上选择拆分的策略。支持的策略是&amp;ldquo;最佳&amp;rdquo;选择最佳拆分，&amp;ldquo;随机&amp;rdquo;选择最佳随机拆分。</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">F-score的召回强度与精度。</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">LOF算法的优势在于它同时考虑了数据集的局部和全局属性:即使在异常样本具有不同基础密度的数据集中,它也能表现良好。问题不在于,样本有多孤立,而在于它相对于周围邻域的孤立程度。</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">要解码的字符串</target>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">字符串值&amp;ldquo; auto&amp;rdquo;根据Spearman相关估计的符号确定y是增加还是减少。</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">双簇i对应的子矩阵。</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">每个基础估计器的绘制特征子集。</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">每个基础估计器的抽样子集。</target>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">特征的总和(如果文档的字数)是从一个泊松分布中抽取的,这个期望值。</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">支持的是 &lt;code&gt;y_true&lt;/code&gt; 中每个类的出现次数。</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">scikit-learn中的支持向量机既支持密集（ &lt;code&gt;numpy.ndarray&lt;/code&gt; ，也可以通过 &lt;code&gt;numpy.asarray&lt;/code&gt; 转换为支持）和稀疏（任何 &lt;code&gt;scipy.sparse&lt;/code&gt; ）样本向量作为输入。但是，要使用SVM对稀疏数据进行预测，它必须已经适合此类数据。为了获得最佳性能，请使用C排序的 &lt;code&gt;numpy.ndarray&lt;/code&gt; （密集）或 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; （稀疏）和dtype &lt;code&gt;dtype=float64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">计算部分依赖性的目标特征(对于视觉渲染,大小应小于3)。</target>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">通过对训练集中最近邻居关联的目标进行局部插值预测。</target>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">目标值(分类中的类标签,回归中的实数)。</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">目标值(类标签)为整数或字符串。</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">目标值(类标签)。</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">目标值(分类中对应类的整数,回归中对应实数)。</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">目标值(实数);</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">目标值（实数）。使用 &lt;code&gt;dtype=np.float64&lt;/code&gt; 和 &lt;code&gt;order='C'&lt;/code&gt; 以获得最大效率。</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">目标值;</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">监督学习问题的目标变量。</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">监督学习问题的目标变量。根据y标签进行分层。</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">目标变量是加州各区的房屋价值中位数。</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">在监督学习的情况下,要尝试预测的目标变量。</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">目前的任务是从生理变量中预测疾病的进展。</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">这十个特征是标准独立的高斯，目标 &lt;code&gt;y&lt;/code&gt; 定义为：</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">使用术语&amp;ldquo;离散特征&amp;rdquo;而不是将它们命名为&amp;ldquo;分类&amp;rdquo;，因为它可以更准确地描述要素。例如，图像的像素强度是离散特征（但很难分类），如果将其标记为这样，您将获得更好的结果。还要注意，将连续变量视为离散变量，反之亦然通常会产生错误的结果，因此请注意这一点。</target>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">数据的测试点。与训练数据的格式相同。</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">该拆分的测试集指数。</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">scikit-learn中的文本特征提取器知道如何解码文本文件，但&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;是您告诉他们文件的 &lt;code&gt;encoding&lt;/code&gt; 。CountVectorizer为此采用了一个编码参数。对于现代文本文件，正确的编码可能是UTF-8，因此它是默认 &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; （encoding =&amp;ldquo; utf-8&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">理论上说,为了达到预测的一致性,随着样本数量的增长,惩罚参数应保持不变。</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">第三张图比较了100个样本的一维分布的核密度估计。虽然这个例子使用的是一维分布,但核密度估计也可以轻松有效地扩展到更高维度。</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">第三个模型也是一个带有Dirichlet过程先验的贝叶斯高斯混合物模型,但这次浓度先验的值更高,使模型更自由地模拟数据的细粒度结构。结果是一个具有较多活性成分的混合物,这与第一个模型类似,我们任意决定将成分数量固定为10。</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">用于特征选择的阈值。保留重要性更高或相等的要素，而其他要素则被丢弃。如果为&amp;ldquo;中位数&amp;rdquo;（分别为&amp;ldquo;平均值&amp;rdquo;），则 &lt;code&gt;threshold&lt;/code&gt; 值为特征重要性的中位数（分别为平均值）。也可以使用缩放因子（例如，&amp;ldquo; 1.25 *平均值&amp;rdquo;）。如果为None（无），并且估计器的参数惩罚显式或隐式设置为l1（例如Lasso），则使用的阈值为1e-5。否则，默认使用&amp;ldquo;均值&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">用于特征选择的阈值。</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">假设d〜n_features〜n_components ，此实现的时间复杂度为 &lt;code&gt;O(d ** 2)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">每个cv拆分的训练集拟合估计器的时间。</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">在每个cv分割的测试集上对估算器评分的时间。（请注意，即使将 &lt;code&gt;return_train_score&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; ,也不包括在火车上评分的时间</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">用于计算下降方向的弹性网求解器的公差。该参数控制给定列更新的搜索方向的准确性，而不是整体参数估计的准确性。仅用于mode ='cd'。</target>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">优化的容限：如果更新小于 &lt;code&gt;tol&lt;/code&gt; ，则优化代码检查对偶间隙的最优性，并继续进行直到其小于 &lt;code&gt;tol&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">宣布收敛的容忍度:如果双间隙低于这个值,则停止迭代。</target>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">层析成像投影操作是线性变换。除了对应于线性回归的数据保真度项之外，我们还对图像的L1范数进行了惩罚，以考虑其稀疏性。由此产生的优化问题称为&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;套索&lt;/a&gt;。我们使用&lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt;类，该类使用坐标下降算法。重要的是，与在此使用的投影运算符相比，该实现在稀疏矩阵上的计算效率更高。</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">特征的总数量。</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">功能总数。这些包括 &lt;code&gt;n_informative&lt;/code&gt; 信息性信息特征， &lt;code&gt;n_redundant&lt;/code&gt; 冗余冗余特征， &lt;code&gt;n_repeated&lt;/code&gt; 重复特征和 &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; 无用特征。</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">输入特征的总数。</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">各班平分的总分。</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">产生的总点数。</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">产生的总点数。如果是奇数,内圈将比外圈多一个点。</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">多项式输出特征的总数。输出特征的数量是通过迭代所有适当大小的输入特征组合来计算的。</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">传统的计算主特征向量的方法是使用功率迭代法。</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; 中&lt;/a&gt;实现的训练算法称为随机最大似然（SML）或持久对比散度（PCD）。由于数据可能性的形式，直接优化最大可能性是不可行的：</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">训练数据</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">训练数据中包含离群值,离群值被定义为远离其他观测值。因此,离群值检测估计器试图拟合训练数据最集中的区域,而忽略偏差观测值。</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">训练数据不受异常值的污染，我们有兴趣检测&lt;strong&gt;新&lt;/strong&gt;观察值是否是异常值。在这种情况下，离群值也称为新颖性。</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">训练数据,例如对一个不可改变的快照的引用。</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">训练输入样本。</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">训练输入样本。在内部，它将被转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; ,并且如果将稀疏矩阵提供给稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">训练输入样本。在内部，其dtype将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; 。如果提供了稀疏矩阵，它将被转换为稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">训练输入样本。稀疏矩阵只有在基础估计器支持的情况下才会被接受。</target>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">训练输入样本。稀疏矩阵可以是CSC、CSR、COO、DOK或LIL。DOK和LIL被转换为CSR。</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">数据的训练点。每个点有三个领域:</target>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">训练集在第 &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; 具有大小i * n_samples //（n_splits + 1）+ n_samples％（n_splits + 1）， &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; ，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数。</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">该分割的训练集指数。</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">可以通过设置 &lt;code&gt;transformer&lt;/code&gt; 或函数对 &lt;code&gt;func&lt;/code&gt; 和 &lt;code&gt;inverse_func&lt;/code&gt; 来触发转换。但是，同时设置这两个选项将引发错误。</target>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">变换是独立地应用在每个特征上。一个特征的累积密度函数被用来预测原始值。低于或高于拟合范围的新/未见数据的特征值将被映射到输出分布的边界。注意,这种变换是非线性的。它可能会扭曲在相同尺度下测量的变量之间的线性相关性,但会使在不同尺度下测量的变量更直接地进行比较。</target>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">转变的方式是:</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">转换后的数据</target>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">转换后的数据被用于训练一个天真的贝叶斯分类器,并观察到预测精度的明显差异,其中在PCA之前缩放的数据集大大优于未缩放的版本。</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">转换后的数据。</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">要使用的树算法。有效选项为['kd_tree'|'ball_tree'|'auto']。默认为&amp;ldquo;自动&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">树状数据结构由节点组成,每个节点由若干个子簇组成。一个节点中子簇的最大数量由分支因子决定。每个子簇都保持着线性和、平方和以及该子簇中的样本数。此外,每个子簇还可以有一个节点作为它的子节点,如果该子簇不是叶子节点的成员。</target>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">每个bin中的真实概率(阳性的分数)。</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">无换位目标的真实分数。</target>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">教程文件夹应包含以下子文件夹。</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">下面的两个图绘制的值 &lt;code&gt;C&lt;/code&gt; 在 &lt;code&gt;x-axis&lt;/code&gt; 和相应的交叉验证的分数上的 &lt;code&gt;y-axis&lt;/code&gt; ，用于生成数据集的多个不同的级分。</target>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">这两个品种是:</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">使用的标准类型;</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">特征值的类型。作为dtype参数传递给Numpy array/scipy.sparse matrix构造函数。</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">特征值的类型。作为 dtype 参数传递给 scipy.sparse 矩阵构造函数。不要将其设置为bool、np.boolean或任何无符号整数类型。</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">用于计算误差的规范类型。可用的错误类型：-'frobenius'（默认）：sqrt（tr（A ^ tA））-'spectral'：sqrt（max &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; （A ^ tA））其中A是错误（comp_cov-self.covariance_）。</target>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">未更改的字典原子</target>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">底层&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;实现使用一个随机数生成器选择功能与双坐标下降拟合模型时（即当 &lt;code&gt;dual&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; ）。因此，对于相同的输入数据具有略微不同的结果并不罕见。如果发生这种情况，请尝试使用较小的tol参数。也可以使用 &lt;code&gt;random_state&lt;/code&gt; 参数控制此随机性。当 &lt;code&gt;dual&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 时，&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;的基础实现不是随机的， &lt;code&gt;random_state&lt;/code&gt; 对结果没有影响。</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">底层的C实现在拟合模型时使用随机数生成器选择特征。因此，对于相同的输入数据具有略有不同的结果并不罕见。如果发生这种情况，请尝试使用较小的 &lt;code&gt;tol&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">底层的C实现在拟合模型时使用随机数生成器来选择特征。因此,对于相同的输入数据,结果略有不同是很正常的。如果出现这种情况,可以尝试使用较小的tol参数。</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">基础的Tree对象。请参阅 &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; 获取Tree对象的属性，并&lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;了解决策树结构&lt;/a&gt;以了解这些属性的基本用法。</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">底层实现是MurmurHash3_x86_32,生成低延迟的32bits哈希,适用于实现查找表、Bloom过滤器、计数最小草图或特征哈希。</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">底层实现liblinear使用稀疏的内部表示法来表示将招致内存拷贝的数据。</target>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">根据分式中各点的主维度,样本的单变量位置。</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">未混合矩阵。</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">无监督的数据约简和监督的估计量可以一步一步地链接在一起。请参阅&lt;a href=&quot;compose#pipeline&quot;&gt;管道：链接估算器&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">左上方的图显示了在玩具1d回归问题的随机数据集LS（蓝点）上训练的单个决策树的预测（深红色）。它还说明了在问题的其他（和不同）随机绘制实例LS上训练的其他单个决策树的预测（浅红色）。直观上，此处的变化项对应于各个估算器的预测光束的宽度（浅红色）。方差越大，对 &lt;code&gt;x&lt;/code&gt; 的预测越敏感训练集上的小变化。偏差项对应于估计量的平均预测值（青色）和最佳模型（深蓝色）之间的差。因此，在这个问题上，我们可以观察到偏差很低（青色和蓝色曲线彼此接近），而方差很大（红光束相当宽）。</target>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;内核近似&lt;/a&gt;中详细介绍了&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;的用法。</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">中心距的使用将距离度量限制在欧几里得空间。</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">所述的使用&lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; &lt;/a&gt;是与上述相同的用于描述的使用&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。唯一的区别在于free参数，即\（c \）。有关此映射的动机和数学细节，请参见&lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">使用多&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;输出估计量的人脸完成&lt;/a&gt;演示了多输出最近邻的回归使用。在此示例中，输入X是脸部上半部分的像素，输出Y是脸部下半部分的像素。</target>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">使用多&lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;输出估计器&lt;/a&gt;在人脸完成中演示了如何使用多输出树进行分类。在此示例中，输入X是脸部上半部分的像素，输出Y是脸部下半部分的像素。</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">在&lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;多输出决策树&lt;/a&gt;回归中证明了使用多输出树进行回归。在此示例中，输入X是单个实数值，而输出Y是X的正弦和余弦。</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">可以在 &lt;code&gt;categories_&lt;/code&gt; 属性中找到使用的类别。</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">用户提供的初始均值，默认为None，如果为None，则使用 &lt;code&gt;init_params&lt;/code&gt; 方法初始化均值。</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">用户提供的初始精度（协方差矩阵的倒数），默认为无。如果为None，则使用'init_params'方法初始化精度。形状取决于'covariance_type'：</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">用户提供的初始权重，默认为无。如果为None，则使用 &lt;code&gt;init_params&lt;/code&gt; 方法初始化权重。</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">通常的协方差最大似然估计可以使用收缩进行正则化。Ledoit和Wolf提出了一个近似的公式来计算渐进最优的收缩参数(最小化MSE准则),得到Ledoit-Wolf协方差估计。</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">通常的协方差最大似然估计对数据集中异常值的存在非常敏感。在这种情况下，最好使用鲁棒的协方差估计器来保证该估计能够抵抗数据集中的&amp;ldquo;错误&amp;rdquo;观察。</target>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">实用程序功能&lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt;是构造管道的简写。它使用可变数量的估算器并返回管道，并自动填充名称：</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">有效的距离度量,以及它们所映射的函数是:</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">值2的分值最高:它出现了两次,权重分别为1.5和2:两者之和为3。</target>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">值4出现了三次:在权重均匀的情况下,结果只是分布的模式。</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">该值由 &lt;code&gt;|y - X'w - c|&lt;/code&gt; 缩小比例。</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">与所选分区相关联的惯性标准的值(如果compute_labels被设置为True)。惯性被定义为样本到最近邻居的平方距离之和。</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">所有Alpha信息标准（&amp;ldquo; aic&amp;rdquo;，&amp;ldquo; bic&amp;rdquo;）的值。选择具有最小信息标准的alpha。与 &lt;code&gt;n_samples&lt;/code&gt; 相比，该值大n_samples倍。2.15和2.16 in（Zou et al，2007）。</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">最大系数的值。</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">最小系数的值。</target>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">参考量子值对应的数值。</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">ValueError异常列出的值与以下各节中描述的测量预测准确性的功能相对应。这些功能的计分器对象存储在字典 &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">要评估的参数值。</target>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">分配给每个样本群的数值。</target>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">训练集中每个要素的方差。用于计算 &lt;code&gt;scale_&lt;/code&gt; 。当 &lt;code&gt;with_std=False&lt;/code&gt; 时等于 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">通过对每个分量的投影变换的训练样本的方差。</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">沿路径的系数的变化值。如果 &lt;code&gt;fit_path&lt;/code&gt; 参数为 &lt;code&gt;False&lt;/code&gt; 则不存在。</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">向量\（h_i \）被称为&amp;ldquo;潜在&amp;rdquo;，因为它未被观察到。\（\ epsilon \）被认为是根据均值0和协方差\（\ Psi \）（即\（\ epsilon \ sim \ mathcal {N}（0，\ Psi）\））高斯分布的噪声项， \（\ mu \）是一些任意的偏移向量。这样的模型称为&amp;ldquo;生成式&amp;rdquo;，因为它描述了如何从\（h_i \）生成\（x_i \）。如果我们将所有\（x_i \）用作矩阵\（\ mathbf {X} \）并将所有\（h_i \）用作矩阵\（\ mathbf {H} \ ），然后我们可以编写（使用适当定义的\（\ mathbf {M} \）和\（\ mathbf {E} \））：</target>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">MiniBatchKMeans上的verbose设置使我们能够看到,在连续调用部分拟合的过程中,一些簇被重新分配。这是因为它们所代表的补丁数量已经变得太少,最好选择一个随机的新簇。</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">语速水平</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">词性水平。</target>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">缄默等级。默认值为0,表示无声模式。</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">verbosity级别:如果非零,则打印进度信息。超过50,输出被发送到stdout。消息的频率随着verbosity级别的增加而增加。如果超过10,则报告所有的迭代。</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">函数的verbosity模式。默认情况下,使用内存对象的模式。</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">scikit-learn的版本及其依赖性</target>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">因此,这个向量器提取的词汇量要大得多,现在可以解决局部定位模式中编码的模糊性。</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">然后,一个样本的加权平均概率将计算如下:</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">加权杂质减少公式如下:</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">模型的权重可以访问。</target>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">X中每个观测值的权重,如果为 &quot;无&quot;,则所有观测值的权重相等(默认:无)。</target>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">由 &lt;code&gt;fit&lt;/code&gt; 方法调用计算的每个特征的权重存储在model属性中：</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">各混合物组分的权重;</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">葡萄酒数据集是一个经典且非常简单的多类分类数据集。</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">&amp;ldquo;文章&amp;rdquo;一词是一个重要的功能，它基于人们经常引用以前的帖子的方式：&amp;ldquo;在文章[文章ID]中，[名称] &amp;lt;[电子邮件地址]&amp;gt;写道：&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">字&lt;em&gt;限制&lt;/em&gt;指的是模型，禁止隐藏单元之间的直接相互作用，或可见单元之间的二分结构。这意味着假定以下条件独立性：</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">单词边界感知变体 &lt;code&gt;char_wb&lt;/code&gt; 对于使用空格进行单词分离的语言特别有趣，因为在这种情况下，它产生的噪声特征比原始 &lt;code&gt;char&lt;/code&gt; 变体少得多。对于此类语言，它可以提高使用此类功能训练的分类器的预测准确性和收敛速度，同时保留有关拼写错误和单词派生的鲁棒性。</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">最坏情况下的复杂度由O（n ^（k + 2 / p））给出，其中n = n_samples，p = n_features。（D. Arthur和S. Vassilvitskii，&amp;ldquo; k均值方法有多慢？&amp;rdquo; SoCG2006）</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Theil-Sen估计器:稳健的多元回归模型。</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">多重线性回归模型中的Theil-Sen估计量，2009年：当当，彭汉祥，王学勤和张和平（&lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf）&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">Theil-Sen回归</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">TheilSen对于小的离群值,无论是X方向还是Y方向都有很好的效果,但是有一个断点,超过这个断点,它的表现就会比OLS差。</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">它们的谐波平均值&lt;strong&gt;V-measure&lt;/strong&gt;由&lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; &lt;/a&gt;计算：</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">然后启动一个ipython shell并运行正在进行中的脚本。</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">那么我们可以证明,对一个数据点进行缩放后的分类,相当于找到估计的类均值(mu^*_k\),它在欧氏距离上最接近数据点。但这也可以在投影到所有类的由所有的类均值(mu^*_k\)生成的亲和子空间(H_K\)上后同样可以完成。这表明,在LDA分类器中,通过线性投影到一个/(K-1/)维度空间,可以减少维度。</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">那么Davies-Bouldin指数定义为:</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">那么指标定义为:</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">那么多类MCC的定义为:。</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">然后\（Z \）的行使用&lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;聚类。前 &lt;code&gt;n_rows&lt;/code&gt; 个标签提供行分区，其余 &lt;code&gt;n_columns&lt;/code&gt; 个标签提供列分区。</target>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">然后,应用欧氏(L2)规范,我们得到文档1的以下tf-idfs。</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">然后， &lt;code&gt;raw_X&lt;/code&gt; 要被馈送到 &lt;code&gt;FeatureHasher.transform&lt;/code&gt; 可使用被构造：</target>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">理论界限</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">有3种不同的API可用于评估模型预测的质量：</target>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">在距离度量和相似性度量之间进行转换的方法有很多种，例如内核。令 &lt;code&gt;D&lt;/code&gt; 为距离， &lt;code&gt;S&lt;/code&gt; 为内核：</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">也有几个缺点(与使用内存词汇的CountVectorizer相比)。</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">有一些概念是很难学的,因为决策树不容易表达,比如XOR、奇偶性或多路复用器问题。</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">在处理被异常值破坏的数据时,有不同的事情需要注意。</target>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">有许多学习例程都依赖于最近的邻居。一个示例是&lt;a href=&quot;density#kernel-density&quot;&gt;核密度估计&lt;/a&gt;，将在&lt;a href=&quot;density#density-estimation&quot;&gt;密度估计&lt;/a&gt;部分讨论。</target>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">我们提供的&amp;ldquo;英语&amp;rdquo;停用词列表中有几个已知问题。参见&lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">40个不同的对象各有10张不同的图像。对于一些被摄者来说,这些图像是在不同的时间拍摄的,改变了光线、面部表情(睁眼/闭眼、微笑/不微笑)和面部细节(戴眼镜/不戴眼镜)。所有的图像都是在深色均匀的背景下拍摄的,被试者处于直立的正面位置(可以容忍一些侧面的移动)。</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">支持向量回归有三种不同的实现方式：&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;。&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;提供了比&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;更快的实现，但是仅考虑了线性内核，而&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt;实现了与&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt;略有不同的表达方式。有关更多详细&lt;a href=&quot;#svm-implementation-details&quot;&gt;信息&lt;/a&gt;，请参见实现细节。</target>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">根据所需的数据集类型,主要有三种数据集接口可以用来获取数据集。</target>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">有两个选项来分配标签。</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">评价双聚类结果有两种方法:内部和外部。内部测量,比如聚类稳定性,只依赖于数据和结果本身。目前scikit-learn中没有内部的双聚类测量方法。外部测量指的是外部信息源,比如真解。在处理真实数据时,真实解通常是未知的,但双聚类人工数据可能对评估算法有用,正是因为真实解是已知的。</target>
        </trans-unit>
        <trans-unit id="21ade7db23177cbafdee1241d820ab218814e247" translate="yes" xml:space="preserve">
          <source>There are two ways to specify multiple scoring metrics for the &lt;code&gt;scoring&lt;/code&gt; parameter:</source>
          <target state="translated">有两种方法可以为 &lt;code&gt;scoring&lt;/code&gt; 参数指定多个评分指标：</target>
        </trans-unit>
        <trans-unit id="cdf4947857438b0c51097ba679415b8309e576f2" translate="yes" xml:space="preserve">
          <source>There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.</source>
          <target state="translated">MDS算法有两种：公制和非公制。在scikit-learn中，&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;类同时实现了这两种方法。在度量MDS中，输入相似性矩阵来自度量（因此考虑了三角形不等式），然后将输出两点之间的距离设置为尽可能接近相似性或不相似性数据。在非度量版本中，算法将尝试保留距离的顺序，并因此在嵌入式空间中的距离与相似度/相异度之间寻求单调关系。</target>
        </trans-unit>
        <trans-unit id="9e945ec56932f8495741411aac1ef0391f41ea70" translate="yes" xml:space="preserve">
          <source>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</source>
          <target state="translated">绝对不能保证恢复地道。首先,选择合适的集群数量是很难的。其次,算法对初始化很敏感,可能会陷入局部最小值,尽管scikit-learn采用了几个技巧来缓解这个问题。</target>
        </trans-unit>
        <trans-unit id="a5fa0b690cccf03ea1d32deadc1c15c3039ee96d" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="translated">内置支持以&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;支持的格式在任何矩阵中提供的稀疏数据。但是，为了获得最大效率，请使用&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix中&lt;/a&gt;定义的CSR矩阵格式。</target>
        </trans-unit>
        <trans-unit id="7bf90eed7a42877d9e674dd4cae64f5ef3af07a9" translate="yes" xml:space="preserve">
          <source>There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (&lt;code&gt;LassoCV&lt;/code&gt; or &lt;code&gt;LassoLarsCV&lt;/code&gt;), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (&lt;code&gt;LassoLarsIC&lt;/code&gt;) tends, on the opposite, to set high values of alpha.</source>
          <target state="translated">没有一般规则来选择用于恢复非零系数的alpha参数。可以通过交叉验证（ &lt;code&gt;LassoCV&lt;/code&gt; 或 &lt;code&gt;LassoLarsCV&lt;/code&gt; ）进行设置，尽管这可能会导致模型欠缺惩罚：包括少量无关变量不会损害预测得分。相反，BIC（ &lt;code&gt;LassoLarsIC&lt;/code&gt; ）倾向于设置较高的alpha值。</target>
        </trans-unit>
        <trans-unit id="ff0ab54a4c7a8a1cf35484dbae4d9dce95d5c026" translate="yes" xml:space="preserve">
          <source>There might be a difference in the scores obtained between &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;solver=liblinear&lt;/code&gt; or &lt;code&gt;LinearSVC&lt;/code&gt; and the external liblinear library directly, when &lt;code&gt;fit_intercept=False&lt;/code&gt; and the fit &lt;code&gt;coef_&lt;/code&gt; (or) the data to be predicted are zeroes. This is because for the sample(s) with &lt;code&gt;decision_function&lt;/code&gt; zero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;LinearSVC&lt;/code&gt; predict the negative class, while liblinear predicts the positive class. Note that a model with &lt;code&gt;fit_intercept=False&lt;/code&gt; and having many samples with &lt;code&gt;decision_function&lt;/code&gt; zero, is likely to be a underfit, bad model and you are advised to set &lt;code&gt;fit_intercept=True&lt;/code&gt; and increase the intercept_scaling.</source>
          <target state="translated">当 &lt;code&gt;fit_intercept=False&lt;/code&gt; 和fit &lt;code&gt;coef_&lt;/code&gt; （或）要预测的数据为零时，使用 &lt;code&gt;solver=liblinear&lt;/code&gt; 或 &lt;code&gt;LinearSVC&lt;/code&gt; 的&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;与外部liblinear库之间获得的分数可能会有所不同。这是因为对于 &lt;code&gt;decision_function&lt;/code&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 为零的样本，&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;和LinearSVC预测负类，而liblinear预测正类。请注意，与模型 &lt;code&gt;fit_intercept=False&lt;/code&gt; ，并具有许多样品 &lt;code&gt;decision_function&lt;/code&gt; 为零，很可能是一个underfit，坏的模式，建议您设置 &lt;code&gt;fit_intercept=True&lt;/code&gt; 并增加intercept_scaling。</target>
        </trans-unit>
        <trans-unit id="4a4216d2986ca1c413a45927f7f8dc07ca811204" translate="yes" xml:space="preserve">
          <source>Therefore, a logarithmic (&lt;code&gt;np.log1p&lt;/code&gt;) and an exponential function (&lt;code&gt;np.expm1&lt;/code&gt;) will be used to transform the targets before training a linear regression model and using it for prediction.</source>
          <target state="translated">因此，在训练线性回归模型并将其用于预测之前，将使用对数（ &lt;code&gt;np.log1p&lt;/code&gt; ）和指数函数（ &lt;code&gt;np.expm1&lt;/code&gt; ）转换目标。</target>
        </trans-unit>
        <trans-unit id="6912f2dbecf0d873a7ad1021b00fc015a0232427" translate="yes" xml:space="preserve">
          <source>These are transformers that are not intended to be used on features, only on supervised learning targets. See also &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transforming target in regression&lt;/a&gt; if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.</source>
          <target state="translated">这些是不打算在功能上使用的变压器，仅在有监督的学习目标上使用。如果要转换预测目标以进行学习，但请在原始（未转换）空间中评估模型，另请参见&lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;在回归中转换目标&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9c615abfdf912d61f49e70314e0bacb6d3789d48" translate="yes" xml:space="preserve">
          <source>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</source>
          <target state="translated">这些分类器之所以吸引人,是因为它们具有易于计算的闭式解,本质上是多类的,在实践中已经被证明是行之有效的,而且没有超参数可以调整。</target>
        </trans-unit>
        <trans-unit id="fde55c65b8197fd0cbaa58300d107768af9ed389" translate="yes" xml:space="preserve">
          <source>These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.</source>
          <target state="translated">这些约束对于强加一定的局部结构是很有用的,但也会使算法的速度更快,特别是当样本数量较多时。</target>
        </trans-unit>
        <trans-unit id="ebd210a6b7ae21f6cafc3c41203edaaa46e25728" translate="yes" xml:space="preserve">
          <source>These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.</source>
          <target state="translated">这些数据集对于快速说明scikit-learn中实现的各种算法的行为非常有用。然而,它们往往太小,不能代表真实世界的机器学习任务。</target>
        </trans-unit>
        <trans-unit id="8d2ab26191aa60fb366e6a03a6fce9c7d01208ba" translate="yes" xml:space="preserve">
          <source>These environment variables should be set before importing scikit-learn.</source>
          <target state="translated">这些环境变量应该在导入 scikit-learn 之前进行设置。</target>
        </trans-unit>
        <trans-unit id="b821932825baa77bb11f8b1f95c8cc38b1d75bf5" translate="yes" xml:space="preserve">
          <source>These estimators are called similarly to their counterparts, with &amp;lsquo;CV&amp;rsquo; appended to their name.</source>
          <target state="translated">这些估算器的名称与对应估算器的名称类似，后缀&amp;ldquo; CV&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8bef6d4f1ba3e716c219b98d63998eb428a7438d" translate="yes" xml:space="preserve">
          <source>These families of algorithms are useful to find linear relations between two multivariate datasets: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; arguments of the &lt;code&gt;fit&lt;/code&gt; method are 2D arrays.</source>
          <target state="translated">这些算法系列可用于查找两个多元数据集之间的线性关系： &lt;code&gt;fit&lt;/code&gt; 方法的 &lt;code&gt;X&lt;/code&gt; 和 &lt;code&gt;Y&lt;/code&gt; 参数是2D数组。</target>
        </trans-unit>
        <trans-unit id="e90c7c20b495d718d6ee1761b52661a07d31f2ac" translate="yes" xml:space="preserve">
          <source>These figures aid in illustrating how a point cloud can be very flat in one direction&amp;ndash;which is where PCA comes in to choose a direction that is not flat.</source>
          <target state="translated">这些图有助于说明点云如何在一个方向上非常平坦，这就是PCA选择不平坦方向的原因。</target>
        </trans-unit>
        <trans-unit id="5b3c06102d71e9aa21ce3635f214fb7544bfe93e" translate="yes" xml:space="preserve">
          <source>These functions have an &lt;code&gt;multioutput&lt;/code&gt; keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is &lt;code&gt;'uniform_average'&lt;/code&gt;, which specifies a uniformly weighted mean over outputs. If an &lt;code&gt;ndarray&lt;/code&gt; of shape &lt;code&gt;(n_outputs,)&lt;/code&gt; is passed, then its entries are interpreted as weights and an according weighted average is returned. If &lt;code&gt;multioutput&lt;/code&gt; is &lt;code&gt;'raw_values'&lt;/code&gt; is specified, then all unaltered individual scores or losses will be returned in an array of shape &lt;code&gt;(n_outputs,)&lt;/code&gt;.</source>
          <target state="translated">这些函数具有一个 &lt;code&gt;multioutput&lt;/code&gt; 关键字参数，该参数指定应平均每个单个目标的得分或损失的方式。默认值为 &lt;code&gt;'uniform_average'&lt;/code&gt; ，它指定输出的均匀加权平均值。如果 &lt;code&gt;ndarray&lt;/code&gt; 形状为 &lt;code&gt;(n_outputs,)&lt;/code&gt; 的ndarray,则将其条目解释为权重，并返回相应的加权平均值。如果指定了 &lt;code&gt;multioutput&lt;/code&gt; 为 &lt;code&gt;'raw_values'&lt;/code&gt; ，则所有未更改的单个得分或损失将以形状数组 &lt;code&gt;(n_outputs,)&lt;/code&gt; 返回。</target>
        </trans-unit>
        <trans-unit id="8f744234c4fef479ca52103694dddbbb39569d67" translate="yes" xml:space="preserve">
          <source>These functions return a tuple &lt;code&gt;(X, y)&lt;/code&gt; consisting of a &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; and an array of length &lt;code&gt;n_samples&lt;/code&gt; containing the targets &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">这些函数返回一个元组 &lt;code&gt;(X, y)&lt;/code&gt; 该元组由 &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; 个 numpy数组 &lt;code&gt;X&lt;/code&gt; 和一个长度为 &lt;code&gt;n_samples&lt;/code&gt; 的包含目标 &lt;code&gt;y&lt;/code&gt; 的数组组成。</target>
        </trans-unit>
        <trans-unit id="7ea4b087a47ece8eb67eda1c13d069b3e5f40d70" translate="yes" xml:space="preserve">
          <source>These generators produce a matrix of features and corresponding discrete targets.</source>
          <target state="translated">这些生成器会产生一个特征矩阵和相应的离散目标。</target>
        </trans-unit>
        <trans-unit id="7870342f67a34d74383fe781d3e91593569275ef" translate="yes" xml:space="preserve">
          <source>These images how similar features are merged together using feature agglomeration.</source>
          <target state="translated">这些图像如何使用特征集聚将相似的特征合并在一起。</target>
        </trans-unit>
        <trans-unit id="2f6550a6d877a222d311c7f7d2e4efbab68b15f3" translate="yes" xml:space="preserve">
          <source>These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;), but also to build precomputed kernels, or similarity matrices.</source>
          <target state="translated">这些矩阵可用于在使用连通性信息的估计器中强加连通性，例如Ward聚类（&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;），也可用于构建预先计算的内核或相似性矩阵。</target>
        </trans-unit>
        <trans-unit id="f1476efa19922a5a7b34be362fc1e06a3ae81118" translate="yes" xml:space="preserve">
          <source>These metrics &lt;strong&gt;require the knowledge of the ground truth classes&lt;/strong&gt; while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).</source>
          <target state="translated">这些度量标准&lt;strong&gt;需要了解基本事实类别，&lt;/strong&gt;而&lt;strong&gt;实际上&lt;/strong&gt;却几乎不可用，或者需要人工注释者手动分配（如在有监督的学习环境中）。</target>
        </trans-unit>
        <trans-unit id="1b78634dcd7b938ef7f64bb3d2756751845f06a1" translate="yes" xml:space="preserve">
          <source>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt;&lt;code&gt;SelectKBest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt;&lt;code&gt;SelectPercentile&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">这些对象将一个得分函数作为输入，该函数返回单变量得分和p值（或仅返回&lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt; &lt;code&gt;SelectKBest&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt; &lt;code&gt;SelectPercentile&lt;/code&gt; 的&lt;/a&gt;得分）：</target>
        </trans-unit>
        <trans-unit id="3403571e1d1dd77b054e08b8e749a4d9b5c4d316" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\)</source>
          <target state="translated">这些参数可以通过成员 &lt;code&gt;dual_coef_&lt;/code&gt; （具有差异\（\ alpha_i- \ alpha_i ^ * \），具有支持向量的 &lt;code&gt;support_vectors_&lt;/code&gt; 和具有独立项\（\ rho \）的 &lt;code&gt;intercept_&lt;/code&gt; 来访问。</target>
        </trans-unit>
        <trans-unit id="9254aef96f1c8727db185406da2727e74822dda9" translate="yes" xml:space="preserve">
          <source>These quantities are also related to the (\(F_1\)) score, which is defined as the harmonic mean of precision and recall.</source>
          <target state="translated">这些数量也与((F_1/1))分值有关,它被定义为精度和召回的谐波平均值。</target>
        </trans-unit>
        <trans-unit id="8ec6209edcb97e57d934d74900289c4f7467ca16" translate="yes" xml:space="preserve">
          <source>These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.</source>
          <target state="translated">这些代表了在地图网格的每一点上测量的14个特征。网格的经纬度值在下面讨论。缺少的数据用数值-9999表示。</target>
        </trans-unit>
        <trans-unit id="e11ef00d62661e429b4b798a345a9c8d62a348e6" translate="yes" xml:space="preserve">
          <source>These steps are performed either a maximum number of times (&lt;code&gt;max_trials&lt;/code&gt;) or until one of the special stop criteria are met (see &lt;code&gt;stop_n_inliers&lt;/code&gt; and &lt;code&gt;stop_score&lt;/code&gt;). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.</source>
          <target state="translated">这些步骤将执行最大次数（ &lt;code&gt;max_trials&lt;/code&gt; ），或者直到满足特殊停止条件之一为止（请参见 &lt;code&gt;stop_n_inliers&lt;/code&gt; 和 &lt;code&gt;stop_score&lt;/code&gt; ）。使用先前确定的最佳模型的所有内部样本（共识集）估计最终模型。</target>
        </trans-unit>
        <trans-unit id="eff89650d9905b662c6df80228e926f2f144c91b" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="translated">这三个距离是&amp;beta;-散度族的特例，分别具有\（\ beta = 2，1，0 \）&lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;。&amp;beta;散度定义为：</target>
        </trans-unit>
        <trans-unit id="100dafc268c3e9d0b628da1715aed2440b14ba32" translate="yes" xml:space="preserve">
          <source>These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt;) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.</source>
          <target state="translated">这些吞吐量是在单个过程中实现的。增加应用程序吞吐量的一个显而易见的方法是生成共享同一模型的其他实例（通常是由于&lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL而&lt;/a&gt;在Python中进行处理）。可能还会增加一台机器来分散负载。但是，有关如何实现此目的的详细说明超出了本文档的范围。</target>
        </trans-unit>
        <trans-unit id="e63971597c4df5f2dc150f90b566e1219b6a632a" translate="yes" xml:space="preserve">
          <source>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</source>
          <target state="translated">它们不是稀疏的,即使用整个样本/特征信息来进行预测。</target>
        </trans-unit>
        <trans-unit id="26f84bd6fe103542912ebb1fb588d29515a2fd6d" translate="yes" xml:space="preserve">
          <source>They can be loaded using the following functions:</source>
          <target state="translated">可以使用以下函数加载它们:</target>
        </trans-unit>
        <trans-unit id="f7ddbd9f45ee3f137b5eb656135dce7584351da0" translate="yes" xml:space="preserve">
          <source>They expose a &lt;code&gt;split&lt;/code&gt; method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</source>
          <target state="translated">他们公开了一个 &lt;code&gt;split&lt;/code&gt; 方法，该方法接受要拆分的输入数据集，并为所选交叉验证策略的每次迭代生成训练/测试集索引。</target>
        </trans-unit>
        <trans-unit id="03bbcc98e1d567dd0afc112fe378a99851c98226" translate="yes" xml:space="preserve">
          <source>They lose efficiency in high dimensional spaces &amp;ndash; namely when the number of features exceeds a few dozens.</source>
          <target state="translated">它们会在高维空间中失去效率，也就是说，当特征数量超过几十个时。</target>
        </trans-unit>
        <trans-unit id="a673690dbc33eee17ee6f3995f817097918876ff" translate="yes" xml:space="preserve">
          <source>This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</source>
          <target state="translated">此缩放器移除中位数,并根据分位数范围对数据进行缩放(默认为IQR:四分位数范围)。IQR是第1个四分位数(25分位数)和第3个四分位数(75分位数)之间的范围。</target>
        </trans-unit>
        <trans-unit id="d9994b645c162ae9e80a2c6bc1c81390f2e92325" translate="yes" xml:space="preserve">
          <source>This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation helper function cross_val_score to warn when there is an error while fitting the estimator.</source>
          <target state="translated">这个Warning用于元估计器GridSearchCV和RandomizedSearchCV以及交叉验证辅助函数cross_val_score中,当拟合估计器时出现错误时,会发出警告。</target>
        </trans-unit>
        <trans-unit id="7f5e3d23312509826c13f1663d34b7e7912ac4af" translate="yes" xml:space="preserve">
          <source>This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by &lt;code&gt;n_clusters&lt;/code&gt;. If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</source>
          <target state="translated">该算法可以视为一种实例或数据约简方法，因为它将输入数据缩减为一组直接从CFT的叶获得的子簇。减少的数据可以通过将其馈送到全局群集器中进行进一步处理。可以由 &lt;code&gt;n_clusters&lt;/code&gt; 设置此全局群集器。如果 &lt;code&gt;n_clusters&lt;/code&gt; 设置为None，则直接读取叶子中的子集群，否则全局聚类步骤将这些子集群标记为全局集群（标签），并将样本映射到最近的子集群的全局标签。</target>
        </trans-unit>
        <trans-unit id="895ec6bb1e64f58ea59b9ae781fa620e703125a9" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该算法涵盖了文献中的几篇著作。当将数据集的随机子集绘制为样本的随机子集时，该算法称为&amp;ldquo;粘贴&amp;rdquo; &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。如果抽取样本进行替换，则该方法称为Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;。当将数据集的随机子集绘制为要素的随机子集时，该方法称为随机子空间&lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;。最后，当基于样本和特征的子集建立基础估计量时，该方法称为随机补丁&lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="870122c945d57891e89c2ba931542331f1b4afdb" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该算法涵盖了文献中的几篇著作。当将数据集的随机子集绘制为样本的随机子集时，该算法称为&amp;ldquo;粘贴&amp;rdquo; &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。如果抽取样本进行替换，则该方法称为Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;。当将数据集的随机子集绘制为要素的随机子集时，该方法称为随机子空间&lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;。最后，当基于样本和特征的子集建立基础估计量时，该方法称为随机补丁&lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="866e774dfaaba96a720c3090067c49c13cac935f" translate="yes" xml:space="preserve">
          <source>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, &lt;code&gt;n_iter&lt;/code&gt; can be set &amp;lt;=2 (at the cost of loss of precision).</source>
          <target state="translated">该算法使用随机化找到（通常非常好）近似的截断奇异值分解，以加快计算速度。对于只希望提取少量成分的大型矩阵，该方法特别快。为了获得进一步的加速，可以将 &lt;code&gt;n_iter&lt;/code&gt; 设置为&amp;lt;= 2（以牺牲精度为代价）。</target>
        </trans-unit>
        <trans-unit id="77977d31f5357c834112cbb27bbc98b5c64c10d7" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory.</source>
          <target state="translated">该算法具有恒定的内存复杂性，大约为 &lt;code&gt;batch_size&lt;/code&gt; ，从而可以使用np.memmap文件，而无需将整个文件加载到内存中。</target>
        </trans-unit>
        <trans-unit id="9586aed3b1a5b6a2c44b32af5cc0558b6ad496a6" translate="yes" xml:space="preserve">
          <source>This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.</source>
          <target state="translated">该算法解决了k=2的归一化切割:是一种归一化谱聚类。</target>
        </trans-unit>
        <trans-unit id="01c65a021c885e4e00baaaa5c6652a314a97daa3" translate="yes" xml:space="preserve">
          <source>This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.</source>
          <target state="translated">这种算法总是会使用它能获得的所有组件,在没有外部线索的情况下,需要持出数据或信息理论标准来决定使用多少组件。</target>
        </trans-unit>
        <trans-unit id="ddf04fe856314e7dd4ddddf49f4086029e04d842" translate="yes" xml:space="preserve">
          <source>This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:</source>
          <target state="translated">这使得在存在异质噪声的情况下,模型选择比概率PCA更好。</target>
        </trans-unit>
        <trans-unit id="3307a2458ebbefda8ea7fe8b898077ec4cadcf2c" translate="yes" xml:space="preserve">
          <source>This also works where final estimator is &lt;code&gt;None&lt;/code&gt;: all prior transformations are applied.</source>
          <target state="translated">这在最终估计器为 &lt;code&gt;None&lt;/code&gt; 的情况下也适用：所有先前的转换都将应用。</target>
        </trans-unit>
        <trans-unit id="c63b80512d8853cd76b24210ee07543da5c60fc4" translate="yes" xml:space="preserve">
          <source>This assumption is the base of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;Vector Space Model&lt;/a&gt; often used in text classification and clustering contexts.</source>
          <target state="translated">这个假设是&lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;向量空间模型&lt;/a&gt;的基础，向量模型通常用于文本分类和聚类上下文中。</target>
        </trans-unit>
        <trans-unit id="8142b653ecd3322ad782e8a8807635d6c4c0325e" translate="yes" xml:space="preserve">
          <source>This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">这种校准的结果是降低对数损失。请注意,一个替代办法是增加基础估计器的数量,这将导致对数损失的类似减少。</target>
        </trans-unit>
        <trans-unit id="3ba422e075fa10216e381bf46530abda4e719fab" translate="yes" xml:space="preserve">
          <source>This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.</source>
          <target state="translated">此调用需要估计一个p x q矩阵,这在高维空间中可能是一个问题。</target>
        </trans-unit>
        <trans-unit id="345a3bd30c8553d3251f728b344d1bd100958670" translate="yes" xml:space="preserve">
          <source>This can be confirmed on a independent testing set with similar remarks:</source>
          <target state="translated">这一点可以在独立的测试集上用类似的说法来证实。</target>
        </trans-unit>
        <trans-unit id="e60927eda9d0f07135f56fa39eaa1a8f1f9c2813" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="translated">这可以通过在模型的超参数上引入无信息的&lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;先验&lt;/a&gt;来完成。&lt;a href=&quot;#id2&quot;&gt;Ridge回归中&lt;/a&gt;使用的\（\ ell_ {2} \）正则化等效于在精度为\（\ lambda ^ {-1} \）的参数\（w \）下，在高斯条件下找到最大后验估计。代替手动设置 &lt;code&gt;lambda&lt;/code&gt; ，可以将其视为要从数据中估计的随机变量。</target>
        </trans-unit>
        <trans-unit id="aa181018cfeb3219e1e67073f9c58ca90a0c4faa" translate="yes" xml:space="preserve">
          <source>This can be done by using the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; utility function.</source>
          <target state="translated">这可以通过使用&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;实用程序函数来完成。</target>
        </trans-unit>
        <trans-unit id="821e5435b62e56a883478da64d6731f6479103d5" translate="yes" xml:space="preserve">
          <source>This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised.</source>
          <target state="translated">这个值可以设置为比任何一个输入文件中的实际特征数更高的值,但设置为较低的值会导致异常发生。</target>
        </trans-unit>
        <trans-unit id="4a0bd36c1ccd6d51b38a900236d58695657d5aff" translate="yes" xml:space="preserve">
          <source>This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.</source>
          <target state="translated">这类方法可以推断出高斯混合分布参数的近似后验分布。可以从数据中推断出有效的成分数。</target>
        </trans-unit>
        <trans-unit id="6130cf2c7564234b715447525f16ff596ebe842e" translate="yes" xml:space="preserve">
          <source>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</source>
          <target state="translated">该类可用于交叉验证以固定时间间隔观测的时间序列数据样本。</target>
        </trans-unit>
        <trans-unit id="88afb4091eb2a25b2e2017ee8508b1ff5c5ae125" translate="yes" xml:space="preserve">
          <source>This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</source>
          <target state="translated">该类实现了一个元估计器,它可以在数据集的不同子样本上拟合一些随机化决策树(也就是额外树),并使用平均法来提高预测精度和控制过度拟合。</target>
        </trans-unit>
        <trans-unit id="dd60f6580be8e1908408c4fbfd8d3915f46e55b1" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">该类使用liblinear、newton-cg、sag和lbfgs优化器实现逻辑回归。newton-cg、sag和lbfgs求解器只支持L2正则化,采用原始公式。liblinear解算器同时支持L1和L2正则化,仅对L2罚则采用双公式。</target>
        </trans-unit>
        <trans-unit id="350693d0493245dcbd676a8f10e001d5020f745e" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="translated">此类使用'liblinear'库，'newton-cg'，'sag'和'lbfgs'求解器实现正则逻辑回归。它可以处理密集和稀疏输入。使用C排序数组或包含64位浮点数的CSR矩阵可获得最佳性能；其他任何输入格式将被转换（并复制）。</target>
        </trans-unit>
        <trans-unit id="2c107aea4a3526193efe1f31d97ccab92891d87f" translate="yes" xml:space="preserve">
          <source>This class implements the Graphical Lasso algorithm.</source>
          <target state="translated">该类实现了图形化的Lasso算法。</target>
        </trans-unit>
        <trans-unit id="456e7e6f7be68de425401d6f66fe28d8a1090538" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost-SAMME [2].</source>
          <target state="translated">这个类实现了被称为AdaBoost-SAMME的算法[2]。</target>
        </trans-unit>
        <trans-unit id="003b6bf538c58eeda3c6fd28b21967bfd8b6c40e" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost.R2 [2].</source>
          <target state="translated">这个类实现了被称为AdaBoost.R2的算法[2]。</target>
        </trans-unit>
        <trans-unit id="63c4b52b10df12140782204ea7cf58fe0edab9de" translate="yes" xml:space="preserve">
          <source>This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">该类实现了两种类型的权重分布的先验:一种是Dirichlet分布的有限混合模型,另一种是Dirichlet过程的无限混合模型。在实践中Dirichlet Process推理算法是近似的,并使用一个固定的最大分量数的截断分布(称为棒破表示)。实际使用的分量数几乎总是取决于数据。</target>
        </trans-unit>
        <trans-unit id="e15e7e7d8d04d41fdd2a4f34183baa0366e86dea" translate="yes" xml:space="preserve">
          <source>This class inherits from PLS with mode=&amp;rdquo;A&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;, norm_y_weights=True and algorithm=&amp;rdquo;nipals&amp;rdquo;, but svd should provide similar results up to numerical errors.</source>
          <target state="translated">此类从PLS继承，其模式为&amp;ldquo; A&amp;rdquo;且deflation_mode =&amp;ldquo; canonical&amp;rdquo;，norm_y_weights = True，algorithm =&amp;ldquo; nipals&amp;rdquo;，但svd应该提供类似的结果，直至出现数值错误。</target>
        </trans-unit>
        <trans-unit id="a0a1d1daa83e6ad727cb13fd589f28f37b44df20" translate="yes" xml:space="preserve">
          <source>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</source>
          <target state="translated">该类继承自ValueError和AttributeError,以帮助异常处理和向后兼容。</target>
        </trans-unit>
        <trans-unit id="d34b4e0a14d3a494edeba399329a47fb49b3ee6c" translate="yes" xml:space="preserve">
          <source>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</source>
          <target state="translated">该类是DictVectorizer和CountVectorizer的低内存替代品,适用于大规模(在线)学习和内存紧张的情况,例如在嵌入式设备上运行预测代码时。</target>
        </trans-unit>
        <trans-unit id="8e34d865883b535f68990f01a240eaab9f87f080" translate="yes" xml:space="preserve">
          <source>This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">因此，此类适合在&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;的早期步骤中使用：</target>
        </trans-unit>
        <trans-unit id="eef0760dd6d25fd731b9abce61656453bb690cee" translate="yes" xml:space="preserve">
          <source>This class is useful when the behavior of &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt; is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; provides a random sample (with replacement) of the train / test splits generated by &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">当需要使用&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt;的行为时，此类非常有用，但是组的数量足够大，以至于生成所有可能的分区并保留\（P \）组将非常昂贵。在这种情况下，&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt;提供了由&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt;生成的训练/测试拆分的随机样本（带有替换）。</target>
        </trans-unit>
        <trans-unit id="edf4c29bfa2afe43016dc0b6660ad50132bd51ec" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;code&gt;get_metric&lt;/code&gt; class method and the metric string identifier (see below). For example, to use the Euclidean distance:</source>
          <target state="translated">此类为快速距离度量功能提供统一的接口。可以通过 &lt;code&gt;get_metric&lt;/code&gt; 类方法和度量标准字符串标识符（请参见下文）访问各种度量标准。例如，使用欧几里得距离：</target>
        </trans-unit>
        <trans-unit id="336f533fd7cf96bc18f597ff7211d31f0cd62386" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.</source>
          <target state="translated">该类支持密集和稀疏输入,多类支持按照onevstherest方案处理。</target>
        </trans-unit>
        <trans-unit id="aebcf6792861578bf4b4a69a0be71898d4023b6a" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input.</source>
          <target state="translated">该类支持密集和稀疏输入。</target>
        </trans-unit>
        <trans-unit id="4042c6697e9df3310aa51f4f2bbe289c3d222c6e" translate="yes" xml:space="preserve">
          <source>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">该类将符号特征名称(字符串)序列转化为scipy.sparse矩阵,使用哈希函数计算名称对应的矩阵列。采用的哈希函数是Murmurhash3的签名32位版本。</target>
        </trans-unit>
        <trans-unit id="afb2ca6e635ea8a6abf9d5cec38428c8ecdc147c" translate="yes" xml:space="preserve">
          <source>This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">这个分类数据集的构建方法是采用一个多维标准正态分布,并通过嵌套的多维同心球来定义类,使每个类中的样本数量大致相等(量子数的分布)。</target>
        </trans-unit>
        <trans-unit id="a35e593acabc67ebdf8fae8d0484d5f85056d4a7" translate="yes" xml:space="preserve">
          <source>This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.</source>
          <target state="translated">这个分类器可以作为一个简单的基线与其他(真实)分类器进行比较。不要用它来解决实际问题。</target>
        </trans-unit>
        <trans-unit id="4ceac36d1efc9bb429dd84350330a84101bb5c8c" translate="yes" xml:space="preserve">
          <source>This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:</source>
          <target state="translated">这个分类器损失了很多F-score,只是因为我们去掉了与主题分类关系不大的元数据。如果我们把这些元数据也从训练数据中剥离出来,它的损失就更大了。</target>
        </trans-unit>
        <trans-unit id="064e5da463cfecd3ca166c4023a79d0a6500160d" translate="yes" xml:space="preserve">
          <source>This combination is implementing in &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt;, a transformer class that is mostly API compatible with &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is stateless, meaning that you don&amp;rsquo;t have to call &lt;code&gt;fit&lt;/code&gt; on it:</source>
          <target state="translated">这种结合是在&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 中&lt;/a&gt;实现的，HashingVectorizer是一种转换器类，主要与&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; API兼容。&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;是无状态的，这意味着你不必调用 &lt;code&gt;fit&lt;/code&gt; 就可以了：</target>
        </trans-unit>
        <trans-unit id="1d56591f4aa7f0ebb864c2d8835af7dfb7f8f26b" translate="yes" xml:space="preserve">
          <source>This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument &lt;code&gt;axis=1&lt;/code&gt;, and reduce it to an array of size [M].</source>
          <target state="translated">它将聚集的要素的值合并为一个值，并应接受形状为[M，N]且关键字参数 &lt;code&gt;axis=1&lt;/code&gt; 的数组，并将其减小为大小[M]的数组。</target>
        </trans-unit>
        <trans-unit id="26c788fe31e79bd1bbf24fbba8a1b8342ff15ce1" translate="yes" xml:space="preserve">
          <source>This consumes less memory than shuffling the data directly.</source>
          <target state="translated">这比直接洗牌数据消耗的内存更少。</target>
        </trans-unit>
        <trans-unit id="9c8cf431c4f1299ae41612f84a50a597aa4a1059" translate="yes" xml:space="preserve">
          <source>This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.</source>
          <target state="translated">该函数通过获取输入点的点积和hash_function,然后根据投影的符号(正/负)将投影转化为二进制字符串数组,从而创建输入数据点的二进制哈希。一个经过排序的二进制哈希数组就创建完成了。</target>
        </trans-unit>
        <trans-unit id="75f340063df2a6996986c297d7d4423dc4417e05" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">这个交叉验证对象是StratifiedKFold和ShuffleSplit的合并,它返回的是分层随机化的褶皱。褶皱是通过保留每类样本的百分比来进行的。</target>
        </trans-unit>
        <trans-unit id="ab90d891a9b7f61040a0bd2fc78eae2488d53a3a" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.</source>
          <target state="translated">此交叉验证对象是&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;的变体。在第k次拆分中，它返回前k个折叠作为训练集，第（k + 1）个折叠作为测试集。</target>
        </trans-unit>
        <trans-unit id="c3d6a6171342c06e134b7087a7e83e3f80ba8a79" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">这个交叉验证对象是KFold的一个变体,它可以返回分层褶皱。褶皱是通过保留每类样本的百分比来制作的。</target>
        </trans-unit>
        <trans-unit id="2c34ca372157ddad0d3d18ffb66a8717775276f6" translate="yes" xml:space="preserve">
          <source>This data sets consists of 3 different types of irises&amp;rsquo; (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray</source>
          <target state="translated">此数据集由3种不同类型的虹膜（Setosa，Versicolour和Virginica）的花瓣和萼片长度组成，存储在150x4 numpy中。</target>
        </trans-unit>
        <trans-unit id="158d23b76a72fb850a277110200a4b319b51d7f5" translate="yes" xml:space="preserve">
          <source>This database is also available through the UW CS ftp server:</source>
          <target state="translated">这个数据库也可以通过UW CS ftp服务器获得。</target>
        </trans-unit>
        <trans-unit id="8f0ed5f875aa21e65ca9d6116dc0e918ff34c0ff" translate="yes" xml:space="preserve">
          <source>This dataset consists of 20,640 samples and 9 features.</source>
          <target state="translated">该数据集由20,640个样本和9个特征组成。</target>
        </trans-unit>
        <trans-unit id="2e15d3adbea56bdf31e76cb4ee55fcf6dfb7c05f" translate="yes" xml:space="preserve">
          <source>This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:</source>
          <target state="translated">这个数据集是通过网络收集的名人JPEG图片,所有细节都可以在官方网站上找到。</target>
        </trans-unit>
        <trans-unit id="1c0b9129c637e05601004735cb7bfdbdba431796" translate="yes" xml:space="preserve">
          <source>This dataset is described in Celeux et al [1]. as:</source>
          <target state="translated">这个数据集在Celeux等人[1]中描述为。</target>
        </trans-unit>
        <trans-unit id="e92704f97c6e77c413a0405e7cf7dd2e32191660" translate="yes" xml:space="preserve">
          <source>This dataset is described in Friedman [1] and Breiman [2].</source>
          <target state="translated">这个数据集在Friedman[1]和Breiman[2]中描述。</target>
        </trans-unit>
        <trans-unit id="473d3b557a1c23b381f633c2c2acf0c38c2a328f" translate="yes" xml:space="preserve">
          <source>This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&amp;rsquo;d have to first transform it into a feature vector with length 64.</source>
          <target state="translated">该数据集由1797张8x8图像组成。每个图像（如下图所示）都是手写数字。为了利用这样的8x8图形，我们必须首先将其转换为长度为64的特征向量。</target>
        </trans-unit>
        <trans-unit id="73cdbbbbdb25af126933f658f4b065e86b9c1a55" translate="yes" xml:space="preserve">
          <source>This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).</source>
          <target state="translated">该数据集代表了物种的地理分布。该数据集由Phillips等人(2006)提供。(2006).</target>
        </trans-unit>
        <trans-unit id="e72397d5f7691c5c60df49442bb007cee4717786" translate="yes" xml:space="preserve">
          <source>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</source>
          <target state="translated">这个数据集来自1990年美国人口普查,每个人口普查区组使用一行。街区组是美国人口普查局公布样本数据的最小地理单位(一个街区组的人口通常为600至3 000人)。</target>
        </trans-unit>
        <trans-unit id="47bf559cf1abc9fb33a96a120f583ad0bcd0f9f8" translate="yes" xml:space="preserve">
          <source>This dataset was obtained from the StatLib repository. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</source>
          <target state="translated">该数据集是从StatLib存储库获得的。&lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4668c093fae272aec3196fc6d39e4e5a4eede980" translate="yes" xml:space="preserve">
          <source>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="translated">这个数据集来自卡内基梅隆大学维护的StatLib库。</target>
        </trans-unit>
        <trans-unit id="9b1b90be23e0adb200134bcc2570822a79ae6e88" translate="yes" xml:space="preserve">
          <source>This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.</source>
          <target state="translated">这证明了Label Propagation即使在少量标签数据的情况下也能学习到良好的边界。</target>
        </trans-unit>
        <trans-unit id="33d36bf521beb70b7b2f4b7e5d24d58f96f46c86" translate="yes" xml:space="preserve">
          <source>This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; for normalization):</source>
          <target state="translated">可以将该描述矢量化为适合输入分类器的稀疏二维矩阵（也许在将其通过管道&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;text.TfidfTransformer&lt;/code&gt; &lt;/a&gt;到text.TfidfTransformer中进行归一化之后）：</target>
        </trans-unit>
        <trans-unit id="12f7e332bc936dbb460a17349dda07780cab7782" translate="yes" xml:space="preserve">
          <source>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</source>
          <target state="translated">这决定了在这个函数被用来只返回其中一个指标的情况下,会发出哪些警告。</target>
        </trans-unit>
        <trans-unit id="a1a66d0ad9255f63c11b93170b95da4e6eeaea4f" translate="yes" xml:space="preserve">
          <source>This downscaling is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; for &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;.</source>
          <target state="translated">这种缩减称为&amp;ldquo;术语频率乘以文档的倒数频率&amp;rdquo;的&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="edeebc499fdf886cf2b1fe82f9cc25a148384f70" translate="yes" xml:space="preserve">
          <source>This early stopping strategy is activated if &lt;code&gt;early_stopping=True&lt;/code&gt;; otherwise the stopping criterion only uses the training loss on the entire input data. To better control the early stopping strategy, we can specify a parameter &lt;code&gt;validation_fraction&lt;/code&gt; which set the fraction of the input dataset that we keep aside to compute the validation score. The optimization will continue until the validation score did not improve by at least &lt;code&gt;tol&lt;/code&gt; during the last &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations. The actual number of iterations is available at the attribute &lt;code&gt;n_iter_&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;early_stopping=True&lt;/code&gt; ，则激活此提前停止策略；否则，停止标准仅对整个输入数据使用训练损失。为了更好地控制提前停止策略，我们可以指定一个参数 &lt;code&gt;validation_fraction&lt;/code&gt; ，该参数设置输入数据集的一部分，我们将其保留来计算验证分数。优化将继续进行，直到在最后的 &lt;code&gt;n_iter_no_change&lt;/code&gt; 迭代期间验证得分至少提高了 &lt;code&gt;tol&lt;/code&gt; 为止。实际的迭代次数在属性 &lt;code&gt;n_iter_&lt;/code&gt; 处可用。</target>
        </trans-unit>
        <trans-unit id="6528dcf2523991bd357ca56474b42d537ada09b8" translate="yes" xml:space="preserve">
          <source>This embedding can also &amp;lsquo;work&amp;rsquo; even if the &lt;code&gt;adjacency&lt;/code&gt; variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).</source>
          <target state="translated">即使该 &lt;code&gt;adjacency&lt;/code&gt; 变量并非严格地是图的邻接矩阵，而更通常是样本之间的亲和度或相似度矩阵（例如，欧几里得距离矩阵或k-NN矩阵的热核），该嵌入也可以&amp;ldquo;起作用&amp;rdquo; 。</target>
        </trans-unit>
        <trans-unit id="662188aeeffeee289aab2f0d97150266d90c022d" translate="yes" xml:space="preserve">
          <source>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</source>
          <target state="translated">这种编码是许多scikit-learn估计器,特别是线性模型和标准核的SVM所需要的。</target>
        </trans-unit>
        <trans-unit id="752036d9bd5ae374e975c046f504e0b38de39538" translate="yes" xml:space="preserve">
          <source>This estimator</source>
          <target state="translated">该估算器</target>
        </trans-unit>
        <trans-unit id="36ceeb9f387752a577aeb048b3f21cd319ecfb48" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="translated">该估计器允许对输入的不同列或列子集分别进行变换,并将结果组合成一个单一的特征空间。这对于异构数据或列式数据很有用,可以将几种特征提取机制或变换组合成一个变换器。</target>
        </trans-unit>
        <trans-unit id="02199e2b9b2bd941c7464261eedf68a6fd2d82e2" translate="yes" xml:space="preserve">
          <source>This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.</source>
          <target state="translated">该估计器将一个变压器对象列表并行应用到输入数据中,然后将结果连在一起。这对于将几种特征提取机制结合到一个变换器中非常有用。</target>
        </trans-unit>
        <trans-unit id="a93ab3d6ae360c030a56bd4fabca46c42abdaf54" translate="yes" xml:space="preserve">
          <source>This estimator approximates a slightly different version of the additive chi squared kernel then &lt;code&gt;metric.additive_chi2&lt;/code&gt; computes.</source>
          <target state="translated">此估算器近似近似加和卡方内核的版本，然后由 &lt;code&gt;metric.additive_chi2&lt;/code&gt; 计算。</target>
        </trans-unit>
        <trans-unit id="57f1dab8dd3e838e06f9128461ff865667eb2891" translate="yes" xml:space="preserve">
          <source>This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">该估计器内置了对多变量回归的支持(即当y是一个形状为[n_samples,n_targets]的二维数组时)。</target>
        </trans-unit>
        <trans-unit id="ab45bcef3fd31ebffe9c4724334c2d64921dae44" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="translated">这个估计器用随机梯度下降(SGD)学习实现了正则化线性模型:每次每个样本都会估计损失的梯度,并在途中以递减的强度计划(也就是学习率)更新模型。SGD允许minibatch(在线/核心外)学习,参见partial_fit方法。为了使用默认的学习率时间表获得最佳效果,数据的均值和单位方差应该为零。</target>
        </trans-unit>
        <trans-unit id="5fdb17bfdb14f498d9377f8ca9b7cc2199a41d7f" translate="yes" xml:space="preserve">
          <source>This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.</source>
          <target state="translated">这个估计器是无状态的(除了构造函数参数),拟合方法什么都不做,但在管道中使用时很有用。</target>
        </trans-unit>
        <trans-unit id="42d08f109b32ce107e9f058e7449521b0b6eab28" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.</source>
          <target state="translated">这个估计器对每个特征进行单独的缩放和翻译,使其在训练集上处于给定的范围内,即在零和一之间。</target>
        </trans-unit>
        <trans-unit id="ce7850baf5a7a3e7ef75716db2d2e4af71c92137" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.</source>
          <target state="translated">这个估计器对每个特征进行单独的缩放和翻译,使训练集中每个特征的最大绝对值为1.0。它不会移动/中心数据,因此不会破坏任何稀疏性。</target>
        </trans-unit>
        <trans-unit id="0ff92b3f8e56701f386ccbc5279ec5fdb2974bc0" translate="yes" xml:space="preserve">
          <source>This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</source>
          <target state="translated">该估计器对每个特征进行单独缩放,使训练集中每个特征的最大绝对值将为1.0。</target>
        </trans-unit>
        <trans-unit id="354214ed410106228bbb593cd82a49f32a2508f8" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.</source>
          <target state="translated">该估计器支持两种算法：快速随机SVD求解器和&amp;ldquo;天真&amp;rdquo;算法，该算法使用ARPACK作为（X * XT）或（XT * X）上的本征求解器，以效率较高的一种为准。</target>
        </trans-unit>
        <trans-unit id="3ed5861f671a7b7a1c102cd9c37c8bf2e273de13" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="translated">该估计器将运行一个广泛的测试套件,用于输入验证、形状等。如果Estimator类继承了sklearn.base的相应mixin,则将运行分类器、回归器、聚类或变换器的额外测试。</target>
        </trans-unit>
        <trans-unit id="7011f5e2b484f266188acd0aba4f3d3175f11ed0" translate="yes" xml:space="preserve">
          <source>This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).</source>
          <target state="translated">这个例子还显示了将Ridge回归应用于高度不符合条件的矩阵的有用性。对于这样的矩阵,目标变量的轻微变化都会导致计算出的权重出现巨大的变异。在这种情况下,设置一定的正则化(alpha)来减少这种变异(噪声)是很有用的。</target>
        </trans-unit>
        <trans-unit id="7f82721c4674480adff4241dbcac8e543f16c868" translate="yes" xml:space="preserve">
          <source>This example applies to olivetti_faces different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="translated">此示例适用于olivetti_faces从模块不同的无监督矩阵分解（降维）方法&lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; &lt;/a&gt;（参见文档章节&lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;中部件分解的信号（矩阵因式分解问题）&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="33924f5409489cd3edd1b22f28ee011b17a585da" translate="yes" xml:space="preserve">
          <source>This example compares 2 dimensionality reduction strategies:</source>
          <target state="translated">这个例子比较了2个维度的减少策略。</target>
        </trans-unit>
        <trans-unit id="1ba8c26b14d0dc5555ed6b18d75cbed15c385668" translate="yes" xml:space="preserve">
          <source>This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.</source>
          <target state="translated">这个例子比较了虹膜数据集分类器上的非嵌套和嵌套交叉验证策略。嵌套交叉验证(CV)通常用于训练一个模型,其中超参数也需要优化。嵌套CV估计基础模型及其(超)参数搜索的泛化误差。选择最大化非嵌套CV的参数会使模型对数据集产生偏差,产生过度优化的分数。</target>
        </trans-unit>
        <trans-unit id="7ef1cb506f6769f9ea8f57cc850c6090d62898eb" translate="yes" xml:space="preserve">
          <source>This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.</source>
          <target state="translated">这个例子比较了Birch(有和没有全局聚类步骤)和MiniBatchKMeans在一个有100,000个样本和2个特征的合成数据集上使用make_blobs生成的时间。</target>
        </trans-unit>
        <trans-unit id="8901e1f5225dc1b7e06d2fabb8f06f19a3753c45" translate="yes" xml:space="preserve">
          <source>This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;Pipeline&lt;/code&gt; to optimize over different classes of estimators in a single CV run &amp;ndash; unsupervised &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;NMF&lt;/code&gt; dimensionality reductions are compared to univariate feature selection during the grid search.</source>
          <target state="translated">本示例构建了一个进行降维处理，然后使用支持向量分类器进行预测的管道。它演示了如何使用 &lt;code&gt;GridSearchCV&lt;/code&gt; 和 &lt;code&gt;Pipeline&lt;/code&gt; 在单个CV运行中针对不同类别的估计量进行优化-将无监督的 &lt;code&gt;PCA&lt;/code&gt; 和 &lt;code&gt;NMF&lt;/code&gt; 降维与在网格搜索过程中的单变量特征进行比较。</target>
        </trans-unit>
        <trans-unit id="b084d11db0bc218128b35a7afe55ac9fa7c4daf1" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:</source>
          <target state="translated">本例演示了如何利用岭回归来逼近一个度数为n_degree的多项式函数。具体来说,从n_samples 1d点出发,建立Vandermonde矩阵就可以了,该矩阵为n_samples x n_degree+1,其形式如下。</target>
        </trans-unit>
        <trans-unit id="8509d7895b98e9b3d2d07ed03eae90baa68f1c72" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.</source>
          <target state="translated">这个例子演示了如何使用Spectral Biclustering算法生成一个棋盘数据集并进行双聚类。</target>
        </trans-unit>
        <trans-unit id="fb9a20a0b6ffdb624c38c357324f211d180af27f" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.</source>
          <target state="translated">这个例子演示了如何使用Spectral Co-Clustering算法生成一个数据集并进行双聚类。</target>
        </trans-unit>
        <trans-unit id="4e024e96a6e8109f327c2d890a3a85ee374e03bd" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.</source>
          <target state="translated">本示例演示如何在包含不同类型&lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;的数据集上使用sklearn.compose.ColumnTransformer。我们使用20个新闻组数据集，并在单独的管道中为主题行和正文计算标准的词袋功能，以及正文上的临时功能。我们使用ColumnTransformer组合它们（与权重），最后在组合的特征集上训练分类器。</target>
        </trans-unit>
        <trans-unit id="76d491fec0fed042e6d7927f2dc124b698f9bded" translate="yes" xml:space="preserve">
          <source>This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &amp;lsquo;comp.os.ms-windows.misc&amp;rsquo; category is excluded because it contains many posts containing nothing but data.</source>
          <target state="translated">本示例演示了二十个新闻组数据集上的频谱共聚算法。&amp;ldquo; comp.os.ms-windows.misc&amp;rdquo;类别被排除在外，因为它包含许多帖子，其中仅包含数据。</target>
        </trans-unit>
        <trans-unit id="6186f51b55d8cba756254a15fe651e5e8504791a" translate="yes" xml:space="preserve">
          <source>This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components.</source>
          <target state="translated">这个例子展示了高斯混合模型在数据上拟合的行为,这些数据不是从高斯随机变量的混合物中采样的。数据集是由100个点沿着一条有噪声的正弦曲线松散地排列而成。因此,高斯分量的数量没有基本真值。</target>
        </trans-unit>
        <trans-unit id="a1949f51dde2d60d7d4d1e707d145c1301537434" translate="yes" xml:space="preserve">
          <source>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.</source>
          <target state="translated">这个例子展示了半监督学习的力量,通过训练一个Label Spreading模型来对具有极少标签集的手写数字进行分类。</target>
        </trans-unit>
        <trans-unit id="c6020c6a7334e89ced3b2c5f4b02f4ed9989e37f" translate="yes" xml:space="preserve">
          <source>This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called &lt;strong&gt;underfitting&lt;/strong&gt;. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will &lt;strong&gt;overfit&lt;/strong&gt; the training data, i.e. it learns the noise of the training data. We evaluate quantitatively &lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.</source>
          <target state="translated">此示例演示了拟合不足和拟合过度的问题，以及如何使用具有多项式特征的线性回归来近似非线性函数。该图显示了我们要近似的函数，它是余弦函数的一部分。此外，还将显示来自实函数的样本以及不同模型的近似值。这些模型具有不同程度的多项式特征。我们可以看到线性函数（阶数为1的多项式）不足以拟合训练样本。这称为&lt;strong&gt;欠拟合&lt;/strong&gt;。 4次多项式几乎完美地逼近了真函数。但是，对于更高的程度，模型将&lt;strong&gt;过度拟合&lt;/strong&gt;训练数据，即模型会学习训练数据的噪声。我们定量评估&lt;strong&gt;&lt;/strong&gt;通过使用交叉验证来&lt;strong&gt;过度拟合&lt;/strong&gt; / &lt;strong&gt;不足&lt;/strong&gt;&lt;strong&gt;拟合&lt;/strong&gt;。我们在验证集上计算均方误差（MSE），数值越高，模型从训练数据中正确推广的可能性就越小。</target>
        </trans-unit>
        <trans-unit id="ab8d51c9ac9762c2930c84a5a03c1c12345a6627" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; to map data from various distributions to a normal distribution.</source>
          <target state="translated">本示例演示了通过 &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; 使用Box-Cox和Yeo-Johnson变换将数据从各种分布映射到正态分布。</target>
        </trans-unit>
        <trans-unit id="9b62ef0be0bf7ce49acab00a23e04164c0becf9d" translate="yes" xml:space="preserve">
          <source>This example does not perform any learning over the data (see &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of classification based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in geospatial coordinates.</source>
          <target state="translated">本示例不对数据进行任何学习（有关基于此数据集中属性的分类示例，请参阅&lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;物种分布建模&lt;/a&gt;）。它仅显示了地理空间坐标中观察到的数据点的核密度估计。</target>
        </trans-unit>
        <trans-unit id="03eea75ae69c05ca0f9dc1a13d89bbd210d48145" translate="yes" xml:space="preserve">
          <source>This example doesn&amp;rsquo;t show it, as we&amp;rsquo;re in a low-dimensional space, but another advantage of the Dirichlet process model is that it can fit full covariance matrices effectively even when there are less examples per cluster than there are dimensions in the data, due to regularization properties of the inference algorithm.</source>
          <target state="translated">这个示例没有显示出来，因为我们处于低维空间，但是Dirichlet过程模型的另一个优点是，即使每个聚类中的示例少于维中的维，它也可以有效地拟合完整的协方差矩阵。数据，归因于推理算法的正则化属性。</target>
        </trans-unit>
        <trans-unit id="7b398c0b1dbb0f3edb9cc17097a9c9f8696a24cc" translate="yes" xml:space="preserve">
          <source>This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.</source>
          <target state="translated">本例采用了几种无监督学习技术,从历史行情的变化中提取股票市场结构。</target>
        </trans-unit>
        <trans-unit id="41937f256baaea1198c4d043d4bb81b61df0d50b" translate="yes" xml:space="preserve">
          <source>This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.</source>
          <target state="translated">本例拟合了一个最小二乘损失的梯度提升模型和500棵深度为4的回归树。</target>
        </trans-unit>
        <trans-unit id="e8121408498cf6fb8c886d50eef2580465ff3307" translate="yes" xml:space="preserve">
          <source>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &amp;ldquo;Gaussian quantiles&amp;rdquo; clusters (see &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.</source>
          <target state="translated">本示例将AdaBoosted决策树桩拟合到由两个&amp;ldquo;高斯分位数&amp;rdquo;集群组成的非线性可分离分类数据集中（请参阅&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt; &lt;/a&gt;），并绘制决策边界和决策得分。分别显示了A类和B类样本的决策分数分布。每个样本的预测类别标签由决策分数的符号确定。决策得分大于零的样本归为B，否则归为A。决策得分的大小决定了与预测类别标签的相似度。另外，例如，仅通过选择决策得分高于某个值的样本，就可以构建一个包含所需纯度B类的新数据集。</target>
        </trans-unit>
        <trans-unit id="02c230a18f98a72777c5f2652062014b16511fe8" translate="yes" xml:space="preserve">
          <source>This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the nearest neighbor along each axis.</source>
          <target state="translated">这个例子有相当多的可视化相关的代码,因为在这里可视化是显示图形的关键。其中一个挑战是如何将标签的位置最小化重叠。为此,我们使用了一个基于沿每个轴的最近邻方向的启发式方法。</target>
        </trans-unit>
        <trans-unit id="259139974bd9ca7304e763dff02af979bb7908e6" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;code&gt;DotProduct&lt;/code&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="translated">此示例说明了XPC数据上的GPC。比较的是固定的各向同性内核（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;）和非固定的内核（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;）。在此特定数据集上，由于类边界是线性的并且与坐标轴重合，因此 &lt;code&gt;DotProduct&lt;/code&gt; 内核获得了更好的结果。但是，实际上，诸如&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 之类的&lt;/a&gt;固定内核通常会获得更好的结果。</target>
        </trans-unit>
        <trans-unit id="a467b781e30c272f4f5e4645f1261bd726b14e8d" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results.</source>
          <target state="translated">这个例子说明了XOR数据的GPC。比较的是一个固定的、各向同性的内核(RBF)和一个非固定的内核(DotProduct)。在这个特定的数据集上,DotProduct内核获得了更好的结果,因为类边界是线性的,并且与坐标轴重合。一般来说,静止核往往能得到更好的结果。</target>
        </trans-unit>
        <trans-unit id="b43f5231cb55a1a95a64bd8afe5472e7955fc98b" translate="yes" xml:space="preserve">
          <source>This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.</source>
          <target state="translated">这个例子说明并比较了单个估计器与袋装集合的预期均方误差的偏方差分解。</target>
        </trans-unit>
        <trans-unit id="0a3450a632d656139c95a4f138b569cec22ba6ef" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The first figure compares the learned model of KRR and SVR when both complexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are very similar; however, fitting KRR is approx. seven times faster than fitting SVR (both with grid-search). However, prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">这个例子说明了在人工数据集上的两种方法,该数据集由一个正弦目标函数和强噪声组成,每五个数据点都会添加强噪声。第一个图比较了使用网格搜索优化RBF内核的复杂性/正则化和带宽时KRR和SVR的学习模型。学习到的函数非常相似;然而,拟合KRR比拟合SVR快约7倍(均采用网格搜索)。然而,由于SVR只使用了100个训练数据点中的约1/3作为支持向量来学习稀疏模型,因此预测100000个目标值的速度比SVR快一树倍以上。</target>
        </trans-unit>
        <trans-unit id="bdbaaa805869c3c13d157f267aeffaf332ff1284" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">此示例说明了人工数据集上的两种方法，该方法由正弦目标函数和强噪声组成。该图比较了基于ExpSineSquared内核的KRR和GPR的学习模型，该模型适合于学习周期性函数。内核的超参数控制内核的平滑度（l）和周期性（p）。此外，数据的噪声级别由GPR通过内核中的其他WhiteKernel组件以及KRR的正则化参数alpha明确学习。</target>
        </trans-unit>
        <trans-unit id="745a420beb7d4cfe3dcb516bc28a36f2576e5395" translate="yes" xml:space="preserve">
          <source>This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">这个例子说明了sigmoid校准如何改变3类分类问题的预测概率。图中是标准的2-Simplex,其中三个角对应三个类。箭头从未校准的分类器预测的概率向量指向同一分类器在保留验证集上进行sigmoid校准后预测的概率向量。颜色表示一个实例的真实等级(红色:等级1,绿色:等级2,蓝色:等级3)。</target>
        </trans-unit>
        <trans-unit id="12c733d8527ccd2c1862324a52d9d453fa3717b9" translate="yes" xml:space="preserve">
          <source>This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications are outliers detection, observations ranking, clustering, &amp;hellip; For visualization purpose, the cubic root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]</source>
          <target state="translated">此示例说明了边远数据如何影响马氏距离：从污染性分布中得出的观测值与可能要使用的真实高斯分布中的观测值没有区别。使用基于MCD的Mahalanobis距离，这两个种群变得可区分。关联的应用是离群值检测，观察值排名，聚类...&amp;hellip;为了可视化目的，马氏距离的立方根在方框图中表示，如Wilson和Hilferty所建议的[2]。</target>
        </trans-unit>
        <trans-unit id="e6c2656adbcd3c5e59b375bc18300061f72c931d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping using many fewer estimators. This can significantly reduce training time, memory usage and prediction latency.</source>
          <target state="translated">此示例说明了如何在&lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;模型中使用提早停止，以实现与不使用更少估计量而无需提早停止构建的模型几乎相同的精度。这可以显着减少训练时间，内存使用量和预测延迟。</target>
        </trans-unit>
        <trans-unit id="c9534999032b13d85edbba90f8420279af14435d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping. This can significantly reduce training time. Note that scores differ between the stopping criteria even from early iterations because some of the training data is held out with the validation stopping criterion.</source>
          <target state="translated">此示例说明了如何在&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;模型中使用提前停止，与未提前停止构建的模型相比，该方法几乎可以达到相同的精度。这样可以大大减少培训时间。请注意，即使是早期迭代，停止准则之间的分数也会有所不同，因为某些训练数据会与验证停止准则保持一致。</target>
        </trans-unit>
        <trans-unit id="c861b8fa50d5165e1e9cdc8044114c0ba76be7f1" translate="yes" xml:space="preserve">
          <source>This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt;. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.</source>
          <target state="translated">本示例说明了如何使用&lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;将不同的预处理和特征提取管道应用于不同的特征子集。对于包含异构数据类型的数据集，这尤其方便，因为我们可能要缩放数字特征并对分类特征进行一次热编码。</target>
        </trans-unit>
        <trans-unit id="7688b615fac5133028d275260a50b4a9e7d6a213" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</source>
          <target state="translated">这个例子说明了GPR与包括WhiteKernel在内的和核可以估计数据的噪声水平。对数边际似然(LML)景观的说明表明,LML存在两个局部最大值。</target>
        </trans-unit>
        <trans-unit id="aab3b4258aabcd6bfe55c7c5adf04622c59229dc" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">这个例子说明了GPR与包括WhiteKernel在内的和核可以估计数据的噪声水平。对数边际似然(LML)景观的说明表明,LML存在两个局部最大值。第一个对应的是一个具有高噪声水平和大长度尺度的模型,它可以通过噪声解释数据的所有变化。第二个具有较小的噪声水平和较短的长度尺度,它能通过无噪声的函数关系解释大部分的变化。第二种模型具有较高的似然性;然而,根据超参数的初始值,基于梯度的优化也可能收敛到高噪声解。因此,对于不同的初始化,多次重复优化是很重要的。</target>
        </trans-unit>
        <trans-unit id="d19a1528c92e37a37fb4fd9cad2cad1ac3d91123" translate="yes" xml:space="preserve">
          <source>This example illustrates the differences between univariate F-test statistics and mutual information.</source>
          <target state="translated">这个例子说明了单变量F检验统计和相互信息的区别。</target>
        </trans-unit>
        <trans-unit id="b411e019157b9b0953b37eb36e27bc6a8ffb3f5f" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of the parameters &lt;code&gt;gamma&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; of the Radial Basis Function (RBF) kernel SVM.</source>
          <target state="translated">此示例说明了径向基函数（RBF）内核SVM 的参数 &lt;code&gt;gamma&lt;/code&gt; 和 &lt;code&gt;C&lt;/code&gt; 的影响。</target>
        </trans-unit>
        <trans-unit id="0f49634dcf1598fde3c403bd7ddd702e3816c634" translate="yes" xml:space="preserve">
          <source>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.</source>
          <target state="translated">这个例子说明了对真实数据集进行稳健的协方差估计的必要性。它对于检测异常值和更好地理解数据结构都很有用。</target>
        </trans-unit>
        <trans-unit id="dc09ff23a3cae32be328d47d0a0a7e64185cd75a" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</source>
          <target state="translated">这个例子说明了不同超参数选择下RBF内核的GPC预测概率。第一张图显示了任意选择的超参数和与最大对数边际似然(LML)对应的超参数下的GPC预测概率。</target>
        </trans-unit>
        <trans-unit id="d49b62c58d1a1ad4b52cfdb4df97043fcb25dfc7" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">这个例子说明了在虹膜数据集的二维版本上,各向同性和各向异性RBF内核的GPC预测概率。各向异性的RBF内核通过给两个特征维度分配不同的长度尺度来获得略高的对数边际似然。</target>
        </trans-unit>
        <trans-unit id="f8e8b4dfa6bc8bbc237c34d26328609c3561c0d3" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">这个例子说明了在虹膜数据集的二维版本上,同向性和异向性RBF内核的GPC预测概率。这说明了GPC对非二元分类的适用性。各向异性RBF核通过对两个特征维度分配不同的长度尺度,获得了略高的对边似然。</target>
        </trans-unit>
        <trans-unit id="d60d503f722a9b87495f756d071794c2e2c52164" translate="yes" xml:space="preserve">
          <source>This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10 samples are shown for both prior and posterior.</source>
          <target state="translated">这个例子说明了不同内核的GPR的前值和后值。平均值、标准差和10个样本都显示了前值和后值。</target>
        </trans-unit>
        <trans-unit id="7e4f09a45ae58596e6f6f82a5f372f88442c0679" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator to perform multi-output regression. A random forest regressor is used, which supports multi-output regression natively, so the results can be compared.</source>
          <target state="translated">此示例说明了使用&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt;元估计器执行多输出回归。使用了随机森林回归器，该回归器本身支持多输出回归，因此可以比较结果。</target>
        </trans-unit>
        <trans-unit id="8a4413b8994df2fb3a9be31d12e5c3eff453a657" translate="yes" xml:space="preserve">
          <source>This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.</source>
          <target state="translated">这个例子直观地说明了在特征空间中使用两种不同的成分分析技术的结果比较。</target>
        </trans-unit>
        <trans-unit id="2cd2616385e5968100e838cea35843639b5d4649" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="translated">此示例基于Hastie等人2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]的&lt;/a&gt;图10.2 ，说明了离散SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;增强算法和实际SAMME.R增强算法之间的性能差异。两种算法都在二进制分类任务上进行评估，其中目标Y是10个输入要素的非线性函数。</target>
        </trans-unit>
        <trans-unit id="f74f72b73b7f5fc12f50525ee3a073668f796ed6" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &amp;ldquo;Gaussian Processes for Machine Learning&amp;rdquo; [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">本示例基于第5.4.3节&amp;ldquo;用于机器学习的高斯过程&amp;rdquo; [RW2006]。它说明了使用对数边际似然法上的梯度上升的复杂内核工程和超参数优化的示例。数据由1958年至2001年在夏威夷的莫纳罗阿天文台收集的每月平均大气CO2浓度（百万分之一体积（ppmv））组成。目标是对随时间t变化的CO2浓度进行建模。 。</target>
        </trans-unit>
        <trans-unit id="b3b5741f90375b259727a574f2a0488e7637e5bc" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt;. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">该示例基于&lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]的&lt;/a&gt; 5.4.3节。它说明了使用对数边际似然法上的梯度上升的复杂内核工程和超参数优化的示例。数据由1958年至1997年之间在夏威夷的莫纳罗阿天文台收集的每月平均大气CO2浓度（以体积百万分之一（ppmv）为单位）组成。目标是对随时间t变化的CO2浓度进行建模。 。</target>
        </trans-unit>
        <trans-unit id="9ecc1c02a68f38d70271669af08df8c1497b1d98" translate="yes" xml:space="preserve">
          <source>This example is commented in the &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;tutorial section of the user manual&lt;/a&gt;.</source>
          <target state="translated">该示例&lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;在用户手册&lt;/a&gt;的教程部分中进行了注释。</target>
        </trans-unit>
        <trans-unit id="9143eb314f05280f157b4081755ef4be5d0d1857" translate="yes" xml:space="preserve">
          <source>This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.</source>
          <target state="translated">这个例子是为了说明k-means会产生不直观和可能意外的聚类的情况。在前三张图中,输入数据不符合k-means做出的一些隐含假设,因此产生了不理想的聚类。在最后一张图中,尽管blubs大小不均,k-means还是会返回直观的聚类。</target>
        </trans-unit>
        <trans-unit id="fb84e7488212d58818869cb1a848aafb46dc6573" translate="yes" xml:space="preserve">
          <source>This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.</source>
          <target state="translated">这个例子绘制了由LDA和QDA学习的每个类和决策边界的协方差椭圆。椭圆显示的是每个类的双标准差。在LDA中,所有类的标准差都是一样的,而QDA中每个类都有自己的标准差。</target>
        </trans-unit>
        <trans-unit id="61cf8846c08926de131cab630666b0d7a1ff4033" translate="yes" xml:space="preserve">
          <source>This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class models with a Dirichlet distribution prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt;) and a Dirichlet process prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt;). On each figure, we plot the results for three different values of the weight concentration prior.</source>
          <target state="translated">此示例绘制从 &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; GaussianMixture 类模型拟合的玩具数据集（三个高斯的混合物）获得的椭圆体，该模型具有Dirichlet分布先验（ &lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt; ）和Dirichlet加工先验（ &lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt; ）。在每个图上，我们绘制了三个不同重量浓度值的结果。</target>
        </trans-unit>
        <trans-unit id="b0d46d4faf4d4d45c7ba844c05b8ff931d890d6c" translate="yes" xml:space="preserve">
          <source>This example presents the different strategies implemented in KBinsDiscretizer:</source>
          <target state="translated">这个例子介绍了KBinsDiscretizer中实现的不同策略。</target>
        </trans-unit>
        <trans-unit id="aff95617011fc0f39a1d4573107679df90fa3c83" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">该示例复制了Zhu等人的图1 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]，&lt;/a&gt;并显示了增强如何改善多类问题的预测准确性。分类数据集是通过采用十维标准正态分布并定义三个由嵌套同心十维球体分隔的类来构造的，从而每个类中的样本数量大致相等（\（\ chi ^ 2 \）分布的分位数）。</target>
        </trans-unit>
        <trans-unit id="f0193d5eae04ea4c45d1272eb8f24ceb76514e1e" translate="yes" xml:space="preserve">
          <source>This example serves as a visual check that IPCA is able to find a similar projection of the data to PCA (to a sign flip), while only processing a few samples at a time. This can be considered a &amp;ldquo;toy example&amp;rdquo;, as IPCA is intended for large datasets which do not fit in main memory, requiring incremental approaches.</source>
          <target state="translated">此示例用作视觉检查，以确保IPCA能够找到与PCA相似的数据投影（至符号翻转），而一次仅处理几个样本。这可以被视为&amp;ldquo;玩具示例&amp;rdquo;，因为IPCA适用于不适合主存储器的大型数据集，需要增量方法。</target>
        </trans-unit>
        <trans-unit id="3dfa9a56451c107730b94ff094ad060fe6660b05" translate="yes" xml:space="preserve">
          <source>This example should be taken with a grain of salt, as the intuition conveyed does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly. Moreover, using feature discretization and one-hot encoding increases the number of features, which easily lead to overfitting when the number of samples is small.</source>
          <target state="translated">这个例子应该带着盐分,因为所传达的直觉不一定适用于真实的数据集。特别是在高维空间中,数据更容易被线性分离。此外,使用特征离散化和一热编码会增加特征数量,当样本数量较少时,容易导致过拟合。</target>
        </trans-unit>
        <trans-unit id="ad21e470ff2bffe875ca12e50c6f8a883eaa0515" translate="yes" xml:space="preserve">
          <source>This example shows an example usage of the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">本示例显示了 &lt;code&gt;split&lt;/code&gt; 方法的示例用法。</target>
        </trans-unit>
        <trans-unit id="05321a1b8964aa5eaada463be8f8b6ab44686817" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.</source>
          <target state="translated">这个例子展示了不同的异常检测算法在二维数据集上的特点。数据集包含一个或两个模式(高密度区域),以说明算法处理多模态数据的能力。</target>
        </trans-unit>
        <trans-unit id="9671740bdc9e0f010272719df08d61d30b070724" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different clustering algorithms on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.</source>
          <target state="translated">此示例显示了&amp;ldquo;有趣&amp;rdquo;但仍在2D模式下的数据集上不同聚类算法的特征。除最后一个数据集外，每个数据集算法对的参数都已调整以产生良好的聚类结果。一些算法比其他算法对参数值更敏感。</target>
        </trans-unit>
        <trans-unit id="408c25df8162bc85c75adf89aefb6c4283aab313" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D.</source>
          <target state="translated">此示例显示了&amp;ldquo;有趣&amp;rdquo;但仍在2D模式下用于数据集上层次聚类的不同链接方法的特征。</target>
        </trans-unit>
        <trans-unit id="ee904b77cbf769dbe7d1093eb852f864329f4bb5" translate="yes" xml:space="preserve">
          <source>This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.</source>
          <target state="translated">这个例子展示了内核密度估计(KDE)--一种强大的非参数密度估计技术--如何被用来学习一个数据集的生成模型。有了这个生成模型,就可以抽取新的样本。这些新样本反映了数据的基本模型。</target>
        </trans-unit>
        <trans-unit id="54102d8f78c42d496181e5bcdf5a40bdaee3e42d" translate="yes" xml:space="preserve">
          <source>This example shows how quantile regression can be used to create prediction intervals.</source>
          <target state="translated">这个例子展示了如何使用量化回归来创建预测区间。</target>
        </trans-unit>
        <trans-unit id="682ea376dc5fd413204f119c7903367cc1b71149" translate="yes" xml:space="preserve">
          <source>This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.</source>
          <target state="translated">本示例说明如何使用BernoulliRBM特征提取器和&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;分类器构建分类管道。整个模型的超参数（学习率，隐藏层大小，正则化）已通过网格搜索进行了优化，但由于运行时的限制，此处未复制搜索。</target>
        </trans-unit>
        <trans-unit id="e6287a37f5ab2f7e58b3aa65bfc9b371f8d2e434" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">本示例说明如何从在加利福尼亚住房数据集中训练的&lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; 中&lt;/a&gt;获得部分依赖图。该示例取自&lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dd4b844c3488b502b915a6b11d22b13911beaa74" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores.</source>
          <target state="translated">这个例子展示了如何在运行SVC(支持向量分类器)之前进行单变量特征选择,以提高分类得分。</target>
        </trans-unit>
        <trans-unit id="47a26e628df959c3e5ed3ffe4f4f3490e8927a8d" translate="yes" xml:space="preserve">
          <source>This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.</source>
          <target state="translated">这个例子展示了如何绘制在MNIST数据集上训练的MLPClassifier中的一些第一层权重。</target>
        </trans-unit>
        <trans-unit id="dd07bd7e7afed03f3c2a3c74458c42dc14028dfe" translate="yes" xml:space="preserve">
          <source>This example shows how to plot the decision surface for four SVM classifiers with different kernels.</source>
          <target state="translated">这个例子展示了如何绘制四个具有不同内核的SVM分类器的决策面。</target>
        </trans-unit>
        <trans-unit id="52dcf2b8bf47a153b3ba3beca30c9af85b850fad" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;code&gt;cross_val_predict&lt;/code&gt; to visualize prediction errors.</source>
          <target state="translated">本示例说明如何使用 &lt;code&gt;cross_val_predict&lt;/code&gt; 可视化预测错误。</target>
        </trans-unit>
        <trans-unit id="351c02b1031f149a08df70cbeed39cd2a6bb9ec7" translate="yes" xml:space="preserve">
          <source>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.</source>
          <target state="translated">这个例子表明,Kernel PCA能够找到数据的投影,使数据可以线性分离。</target>
        </trans-unit>
        <trans-unit id="607fdb6fda0285694fd1dd982f83082f2f9a6687" translate="yes" xml:space="preserve">
          <source>This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.</source>
          <target state="translated">这个例子表明,推算缺失值比丢弃包含任何缺失值的样本可以得到更好的结果。归入并不总是能改善预测结果,所以请通过交叉验证进行检查。有时放弃行或使用标记值更有效。</target>
        </trans-unit>
        <trans-unit id="b6045a3110197ccfd64402c3c879acac73bdaadb" translate="yes" xml:space="preserve">
          <source>This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.</source>
          <target state="translated">这个例子表明,可以使用信息理论标准(BIC)对高斯混合模型进行模型选择。模型选择既涉及到协方差类型,也涉及到模型中成分的数量。在这种情况下,AIC也能提供正确的结果(未显示节省时间),但如果问题是确定正确的模型,BIC更适合。与贝叶斯程序不同,这种推论是无先验的。</target>
        </trans-unit>
        <trans-unit id="7ed4db944a196b1cb5ab23d8834c95cd5421c757" translate="yes" xml:space="preserve">
          <source>This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces.</source>
          <target state="translated">这个例子表明,你可以用线性模型做非线性回归,使用管道来添加非线性特征。内核方法扩展了这个想法,可以诱导出非常高(甚至是无限)维度的特征空间。</target>
        </trans-unit>
        <trans-unit id="48dcc848c2d6e1561f6c336d60b0fb518f9ab59a" translate="yes" xml:space="preserve">
          <source>This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.</source>
          <target state="translated">这个例子显示了不同数据集的ROC响应,由K倍交叉验证创建。将所有这些曲线,可以计算出曲线下的平均面积,并看到训练集被分割成不同子集时的曲线方差。这大致可以看出分类器的输出是如何受到训练数据变化的影响,以及K-fold交叉验证所产生的分裂数据之间的差异。</target>
        </trans-unit>
        <trans-unit id="7b92e840bf44fca60c7edc394c5ccf3da0546857" translate="yes" xml:space="preserve">
          <source>This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.</source>
          <target state="translated">这个例子显示了强加一个连接图来捕捉数据中局部结构的效果。该图只是20个最近的邻居的图。</target>
        </trans-unit>
        <trans-unit id="a122bd5b47879a72d10715b2e2741901d74ebd5f" translate="yes" xml:space="preserve">
          <source>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in &lt;strong&gt;computed tomography&lt;/strong&gt; (CT).</source>
          <target state="translated">此示例显示了从一组沿不同角度获取的平行投影重建图像的方法。此类数据集是在&lt;strong&gt;计算机断层扫描&lt;/strong&gt;（CT）中获取的。</target>
        </trans-unit>
        <trans-unit id="277c7e399c7f521a9367c3279ba6605ffa32bb5b" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="translated">这个例子显示了使用树之林来评估图像分类任务(人脸)中像素的重要性。像素越热,越重要。</target>
        </trans-unit>
        <trans-unit id="181df9c721e88b620fbcd3038af372d2bc17958d" translate="yes" xml:space="preserve">
          <source>This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.</source>
          <target state="translated">这个例子展示了使用多输出估计器来完成图像。目标是预测一个人脸的下半部分,给定其上半部分。</target>
        </trans-unit>
        <trans-unit id="c194d9ad3fd820ff97a5b60a54a77ad5e096ac46" translate="yes" xml:space="preserve">
          <source>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:</source>
          <target state="translated">本例模拟了一个多标签文档分类问题。数据集是根据以下过程随机生成的。</target>
        </trans-unit>
        <trans-unit id="beca62f6aeebe1ac71c16bdf04b277689fc51da6" translate="yes" xml:space="preserve">
          <source>This example uses &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.</source>
          <target state="translated">本示例在根据图像上体素之间的差异创建的图上使用&amp;ldquo; &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;光谱&amp;rdquo;聚类&lt;/a&gt;，以将该图像分为多个部分均匀的区域。</target>
        </trans-unit>
        <trans-unit id="b1b3f16a0bb367262a72b24da0008ca16f86a096" translate="yes" xml:space="preserve">
          <source>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.</source>
          <target state="translated">这个例子使用一个大型的人脸数据集来学习构成人脸的一组20×20的图像补丁。</target>
        </trans-unit>
        <trans-unit id="0ad2a4281006cf2ce3833b3619a37abf58d678e0" translate="yes" xml:space="preserve">
          <source>This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.</source>
          <target state="translated">这个例子使用不同的标量器、变换器和归一化器,使数据在预定义的范围内。</target>
        </trans-unit>
        <trans-unit id="e1bfbae38c6acd2b8b362eacb6ca0227cf12cdc6" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; class to demonstrate the principles of Kernel Density Estimation in one dimension.</source>
          <target state="translated">本示例使用&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt;类在一个维度上演示内核密度估计的原理。</target>
        </trans-unit>
        <trans-unit id="1c91a9c343e9d52fecff06520d2432c55d78e2a0" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;. In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="translated">此示例使用 &lt;code&gt;scipy.stats&lt;/code&gt; 模块，该模块包含许多有用的采样参数分布，例如 &lt;code&gt;expon&lt;/code&gt; ， &lt;code&gt;gamma&lt;/code&gt; ， &lt;code&gt;uniform&lt;/code&gt; 或 &lt;code&gt;randint&lt;/code&gt; 。原则上，可以传递提供 &lt;code&gt;rvs&lt;/code&gt; （随机变量样本）方法以采样值的任何函数。调用 &lt;code&gt;rvs&lt;/code&gt; 函数应从连续调用的可能参数值中提供独立的随机样本。</target>
        </trans-unit>
        <trans-unit id="d9381762b80079a0275cf5384c50652951b2c3b8" translate="yes" xml:space="preserve">
          <source>This example uses the only the first feature of the &lt;code&gt;diabetes&lt;/code&gt; dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.</source>
          <target state="translated">本示例仅使用 &lt;code&gt;diabetes&lt;/code&gt; 数据集的第一个特征，以说明此回归技术的二维图。可以在图中看到直线，显示了线性回归如何尝试绘制一条直线，该直线将最大程度地减少数据集中观察到的响应与通过线性近似预测的响应之间的残差平方和。</target>
        </trans-unit>
        <trans-unit id="c6bb5d76743a81844f0fc5afc16345d399cae103" translate="yes" xml:space="preserve">
          <source>This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.</source>
          <target state="translated">这个例子可视化了不同随机学习策略的一些训练损失曲线,包括 SGD 和 Adam。由于时间限制,我们使用了几个小数据集,对于这些数据集,L-BFGS可能更适合。然而,这些例子中显示的一般趋势似乎可以延续到更大的数据集。</target>
        </trans-unit>
        <trans-unit id="65646a35859e04e16667e59a0e282463725f9c9e" translate="yes" xml:space="preserve">
          <source>This example visualizes the behavior of several common scikit-learn objects for comparison.</source>
          <target state="translated">这个例子将几个常见的scikit-learn对象的行为可视化,以便比较。</target>
        </trans-unit>
        <trans-unit id="49dcb9492cd2c3de6ca468ca869fddbd4adf1109" translate="yes" xml:space="preserve">
          <source>This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification.</source>
          <target state="translated">这个例子将几个树给出的分区可视化,并展示了如何将变换也用于非线性维度减少或非线性分类。</target>
        </trans-unit>
        <trans-unit id="6888e16176d5ba984d30e5b2aecac9e36e3202eb" translate="yes" xml:space="preserve">
          <source>This example will also work by replacing &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; with &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt;. Setting the &lt;code&gt;loss&lt;/code&gt; parameter of the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; equal to &lt;code&gt;hinge&lt;/code&gt; will yield behaviour such as that of a SVC with a linear kernel.</source>
          <target state="translated">通过将 &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; 替换为 &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt; 此示例也可以工作。设置 &lt;code&gt;loss&lt;/code&gt; 的参数&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;等于 &lt;code&gt;hinge&lt;/code&gt; 将产生的行为，例如，与线性核一个SVC的。</target>
        </trans-unit>
        <trans-unit id="08633b59361c5b4332dffd09b9ac681bbe920080" translate="yes" xml:space="preserve">
          <source>This example, inspired from Chen&amp;rsquo;s publication [1], shows a comparison of the estimated MSE of the LW and OAS methods, using Gaussian distributed data.</source>
          <target state="translated">这个例子的灵感来自Chen的出版物[1]，显示了使用高斯分布数据对LW和OAS方法的估计MSE的比较。</target>
        </trans-unit>
        <trans-unit id="f9260a90e6c35e520398765702d07497fe04f1a8" translate="yes" xml:space="preserve">
          <source>This examples shows how a classifier is optimized by cross-validation, which is done using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; object on a development set that comprises only half of the available labeled data.</source>
          <target state="translated">此示例显示了如何通过交叉验证来优化分类器，交叉验证是通过在仅包含可用标记数据的一半的开发集上使用&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;对象完成的。</target>
        </trans-unit>
        <trans-unit id="e16048c7f7cf75798d53fe7e675cc81cd1d6af8b" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.</source>
          <target state="translated">这个例子展示了使用树木森林来评估特征对人工分类任务的重要性。红条是森林的特征重要性,以及它们的树间变化。</target>
        </trans-unit>
        <trans-unit id="4fb4f14900902539137295018be0a0c7a07b1094" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Cross-validated estimators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">在&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;模型选择&lt;/a&gt;的&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;交叉验证估计器&lt;/a&gt;部分中使用此练习：&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;科学数据统计学习指南的&lt;/a&gt;选择估计器及其参数部分。</target>
        </trans-unit>
        <trans-unit id="621b0c8349abb129c7bc150bd9745f46f49af3ab" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Cross-validation generators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">本练习在&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;模型选择&lt;/a&gt;的&lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;交叉验证生成器&lt;/a&gt;部分中使用：&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;《科学数据处理统计学习指南》的&amp;ldquo; &lt;/a&gt;选择估计量及其参数&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="6915ba6643fea2f9e387885428a6e6189c7df3bf" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Classification&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">此练习在&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;监督学习&lt;/a&gt;的&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;分类&lt;/a&gt;部分中使用：从&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;用于科学数据处理的统计学习指南的&lt;/a&gt;高维观测部分预测输出变量。</target>
        </trans-unit>
        <trans-unit id="421684adc1a996556d28fe46ff4c101c2f3063ef" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Using kernels&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">本练习在&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;监督学习&lt;/a&gt;的&lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;使用内核&lt;/a&gt;部分中使用：从&lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;用于科学数据处理的统计学习指南的&lt;/a&gt;高维观测部分预测输出变量。</target>
        </trans-unit>
        <trans-unit id="dadb46eeaf7a2bf2f8f61dc107ba2d3f5d55d33a" translate="yes" xml:space="preserve">
          <source>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)\). Then the log loss of the whole set is</source>
          <target state="translated">这扩展到多类情况如下。让一组样本的真实标签被编码为一个1-of-K的二进制指标矩阵 \(Y\),即,如果样本 \(i)\具有从一组 \(K\)标签中抽取的标签 \(k)\,则 \(y_{i,k}=1\)。让\(P\)是一个概率估计的矩阵,其中\(p_{i,k}=\operatorname{Pr}(t_{i,k}=1)\)。那么整个集合的对数损失为</target>
        </trans-unit>
        <trans-unit id="826a67cf49f96f56e23921af61e52712fab61d33" translate="yes" xml:space="preserve">
          <source>This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt;, &lt;code&gt;adjusted_rand_index&lt;/code&gt; or &lt;code&gt;average_precision&lt;/code&gt; and returns a callable that scores an estimator&amp;rsquo;s output.</source>
          <target state="translated">此工厂函数包装评分函数，以用于GridSearchCV和cross_val_score。它需要一个得分的功能，如 &lt;code&gt;accuracy_score&lt;/code&gt; ， &lt;code&gt;mean_squared_error&lt;/code&gt; ， &lt;code&gt;adjusted_rand_index&lt;/code&gt; 或 &lt;code&gt;average_precision&lt;/code&gt; 并返回一个可调用的是分数的估计的输出。</target>
        </trans-unit>
        <trans-unit id="5eb7a4eb6e2161d3d9f2a7b4c7010506ccf35ad1" translate="yes" xml:space="preserve">
          <source>This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:</source>
          <target state="translated">这个特征对应于以厘米为单位的萼片长度。一旦应用分位数变换,这些地标就会接近之前定义的百分数。</target>
        </trans-unit>
        <trans-unit id="18a1d2c5a41fd4d57af7a6bb802060cade230322" translate="yes" xml:space="preserve">
          <source>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</source>
          <target state="translated">这种特征选择算法只看特征(X),而不是所需的输出(y),因此可以用于无监督学习。</target>
        </trans-unit>
        <trans-unit id="677c582ff4a458e9dc8e636909bbbb985fe5cce6" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; preprocessor. This preprocessor transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="translated">该图是使用&lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt;预处理器创建的。该预处理器将输入数据矩阵转换为给定程度的新数据矩阵。可以如下使用：</target>
        </trans-unit>
        <trans-unit id="adbd1df9acbf84e51fe5dc83e34aca6a9423eabf" translate="yes" xml:space="preserve">
          <source>This figure shows an example of such an ROC curve:</source>
          <target state="translated">该图显示了这样一条ROC曲线的例子。</target>
        </trans-unit>
        <trans-unit id="e1207da0df5038f5f29891db83b7d5022ead8471" translate="yes" xml:space="preserve">
          <source>This folder is used by some large dataset loaders to avoid downloading the data several times.</source>
          <target state="translated">这个文件夹被一些大型数据集加载器使用,以避免多次下载数据。</target>
        </trans-unit>
        <trans-unit id="697a12fdadac01e298b3e16a8634659c2b054014" translate="yes" xml:space="preserve">
          <source>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</source>
          <target state="translated">这种格式是一种基于文本的格式,每行有一个样本,它不存储零值特征,因此适合于稀疏数据集。它不存储零值特征,因此适用于稀疏数据集。</target>
        </trans-unit>
        <trans-unit id="a2fe6f0ee6734c2a60bcbb1d0d95dc9dfd886002" translate="yes" xml:space="preserve">
          <source>This format is used as the default format for both svmlight and the libsvm command line programs.</source>
          <target state="translated">这种格式被用作 svmlight 和 libsvm 命令行程序的默认格式。</target>
        </trans-unit>
        <trans-unit id="a2e505f490185afab8e1242d5832b6872eb9a667" translate="yes" xml:space="preserve">
          <source>This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then &lt;code&gt;dot(x, x)&lt;/code&gt; and/or &lt;code&gt;dot(y, y)&lt;/code&gt; can be pre-computed.</source>
          <target state="translated">与其他计算距离的方式相比，此公式具有两个优点。首先，在处理稀疏数据时它在计算上是有效的。其次，如果一个参数变化而另一个参数保持不变，则可以预先计算 &lt;code&gt;dot(x, x)&lt;/code&gt; 和/或 &lt;code&gt;dot(y, y)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fd76e45c139161a6c2340aa524dcf0429afb583e" translate="yes" xml:space="preserve">
          <source>This function computes Cohen&amp;rsquo;s kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</source>
          <target state="translated">此函数计算Cohen的kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;，该分数表示两个注释者在分类问题上的一致程度。定义为</target>
        </trans-unit>
        <trans-unit id="1a26f30c64bc915159ba37349551edb033f8db68" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).</source>
          <target state="translated">这个函数为X中的每一行计算最接近Y行的索引(根据指定的距离)。</target>
        </trans-unit>
        <trans-unit id="40ca8ec3788866f2480b1619115207e644d05cac" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.</source>
          <target state="translated">这个函数为X中的每一行计算最接近Y行的索引(根据指定的距离)。同时返回最小距离。</target>
        </trans-unit>
        <trans-unit id="6e14f241e1c03d6b67a6c0f22515d375ae432487" translate="yes" xml:space="preserve">
          <source>This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.</source>
          <target state="translated">这个函数抓取模块并获取所有继承自BaseEstimator的类。不包括在测试模块中定义的类。默认情况下,元估计器(如GridSearchCV)也不包括在内。</target>
        </trans-unit>
        <trans-unit id="311d27372cf5315019acfac7480657777c623446" translate="yes" xml:space="preserve">
          <source>This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.</source>
          <target state="translated">这个函数不会尝试将特征提取到numpy数组或scipy稀疏矩阵中。此外,如果 load_content 为 false,它不会尝试加载内存中的文件。</target>
        </trans-unit>
        <trans-unit id="28042729acf4bf75d343c82f013b112dad91f4c8" translate="yes" xml:space="preserve">
          <source>This function generates a GraphViz representation of the decision tree, which is then written into &lt;code&gt;out_file&lt;/code&gt;. Once exported, graphical renderings can be generated using, for example:</source>
          <target state="translated">此函数生成决策树的GraphViz表示形式，然后将其写入 &lt;code&gt;out_file&lt;/code&gt; 。导出后，可以使用以下方式生成图形渲染：</target>
        </trans-unit>
        <trans-unit id="90af5dc07a0d6b1b987fbe6284966c35b8f7dbed" translate="yes" xml:space="preserve">
          <source>This function implements Test 1 in:</source>
          <target state="translated">该函数实现了测试1中。</target>
        </trans-unit>
        <trans-unit id="51510777ab8c25d02b45243629e172250d646e40" translate="yes" xml:space="preserve">
          <source>This function is called with the estimated model and the randomly selected data: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with &lt;code&gt;is_data_valid&lt;/code&gt;. &lt;code&gt;is_model_valid&lt;/code&gt; should therefore only be used if the estimated model is needed for making the rejection decision.</source>
          <target state="translated">使用估计的模型和随机选择的数据调用此函数： &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt; 。如果其返回值为False，则跳过当前随机选择的子样本。与 &lt;code&gt;is_data_valid&lt;/code&gt; 相比，使用此函数拒绝样本的计算开销更大。因此，仅在需要估计模型来做出拒绝决策时才应使用 &lt;code&gt;is_model_valid&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a4283f593950d3e9c84617d07a78fd011e78bfa4" translate="yes" xml:space="preserve">
          <source>This function is called with the randomly selected data before the model is fitted to it: &lt;code&gt;is_data_valid(X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped.</source>
          <target state="translated">在对模型进行拟合之前，将使用随机选择的数据调用此函数： &lt;code&gt;is_data_valid(X, y)&lt;/code&gt; 。如果其返回值为False，则跳过当前随机选择的子样本。</target>
        </trans-unit>
        <trans-unit id="7289fd594a0de96a89a572bf0a7bd6e9501fda52" translate="yes" xml:space="preserve">
          <source>This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.</source>
          <target state="translated">这个函数相当于将 load_svmlight_file 映射到一个文件列表上,只是结果会被连接成一个单一的、平坦的列表,而且样本向量被限制为都具有相同数量的特征。</target>
        </trans-unit>
        <trans-unit id="828fa7414b6e6676bd49f6624bf5ec1232d13777" translate="yes" xml:space="preserve">
          <source>This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.</source>
          <target state="translated">这个函数可以对事先已知的一组固定的类标签计算这种变换。</target>
        </trans-unit>
        <trans-unit id="910452d7cd5e91358a13a185cadee882cea17632" translate="yes" xml:space="preserve">
          <source>This function modifies the estimator in-place.</source>
          <target state="translated">该函数对估计器进行就地修改。</target>
        </trans-unit>
        <trans-unit id="3e0bd9f3948e27380d0112f277598d80d17269d2" translate="yes" xml:space="preserve">
          <source>This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">此函数需要真实的二进制值和目标分数，它们可以是正类的概率估计值，置信度值或二进制决策。这是一个如何使用&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="2ddc8c678c75b432a0f0fafa6490a9a69a784bf8" translate="yes" xml:space="preserve">
          <source>This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.</source>
          <target state="translated">这个函数返回实际结果和可能结果的预测概率之间的均值平方差的分数,实际结果必须是1或0(真或假),而实际结果的预测概率可以是0和1之间的值。实际结果必须是1或0(真或假),而实际结果的预测概率可以是0和1之间的数值。</target>
        </trans-unit>
        <trans-unit id="fbdeef434a34fee928d2d9974f81cfc54768558d" translate="yes" xml:space="preserve">
          <source>This function returns posterior probabilities of classification according to each class on an array of test vectors X.</source>
          <target state="translated">这个函数根据测试向量数组X上的每个类返回分类的后验概率。</target>
        </trans-unit>
        <trans-unit id="f7adc46ef6325367984cfe5d6cabca879706d70d" translate="yes" xml:space="preserve">
          <source>This function returns the Silhouette Coefficient for each sample.</source>
          <target state="translated">该函数返回每个样本的轮廓系数。</target>
        </trans-unit>
        <trans-unit id="30221178098fdd3682a8c91454092d6226b25e4f" translate="yes" xml:space="preserve">
          <source>This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt;&lt;code&gt;silhouette_samples&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此函数返回所有样本的平均轮廓系数。要获取每个样本的值，请使用&lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt; &lt;code&gt;silhouette_samples&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bd812dc35d867d7152375e5009e2698c8db08fc0" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.</source>
          <target state="translated">这个函数只是返回有效的对偶距离度量。它的存在是为了允许对每个有效字符串的映射进行描述。</target>
        </trans-unit>
        <trans-unit id="fa4705a70e55596dcf2ace89a6d2a8d09a9fcccf" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.</source>
          <target state="translated">这个函数只是返回有效的对偶距离度量。然而,它的存在是为了允许对每个有效字符串的映射进行详细描述。</target>
        </trans-unit>
        <trans-unit id="d1a7a45215b31f0644d6e686c87b329e59419299" translate="yes" xml:space="preserve">
          <source>This function won&amp;rsquo;t compute the intercept.</source>
          <target state="translated">此函数不会计算截距。</target>
        </trans-unit>
        <trans-unit id="a808622a520f852134a2d8734b9e29ce0a669efe" translate="yes" xml:space="preserve">
          <source>This function works with dense 2D arrays only.</source>
          <target state="translated">这个函数只适用于密集的二维数组。</target>
        </trans-unit>
        <trans-unit id="7541ac358f5bb3bc5dcdfc1193ff72d6ac233a66" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.</source>
          <target state="translated">这种生成器方法在每次迭代提升后都能得到集合预测的类概率,因此可以进行监控,比如在每次提升后确定测试集上的预测类概率。</target>
        </trans-unit>
        <trans-unit id="7c1ad29f5d19940cf714626cd821b0934b5bc400" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.</source>
          <target state="translated">这种生成器方法在每次迭代升压后都会产生合集预测,因此可以进行监控,比如在每次升压后确定对测试集的预测。</target>
        </trans-unit>
        <trans-unit id="acc74c06c673308a3e484c230b5ff2c8348cfe79" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.</source>
          <target state="translated">这种生成器方法在每次迭代升压后都能得到合集得分,因此可以进行监控,比如在每次升压后确定测试集的得分。</target>
        </trans-unit>
        <trans-unit id="cb7462acd1f1e763247c87d170997bea5c436272" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="translated">这说明了 &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; 数据集生成器。每个样本都包含两个特征的计数（总共最多50个），这两个特征在两个类别的每个类别中的分布不同。</target>
        </trans-unit>
        <trans-unit id="b80113eed9b4b9668cc4e8b638bede5d1f2cf638" translate="yes" xml:space="preserve">
          <source>This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). It may attract a higher memory complexity when querying these nearest neighborhoods, depending on the &lt;code&gt;algorithm&lt;/code&gt;.</source>
          <target state="translated">此实现批量计算所有邻居查询，这将内存复杂度提高到O（nd），其中d是邻居的平均数量，而原始DBSCAN的内存复杂度为O（n）。在查询这些最近邻域时，它可能会吸引更高的内存复杂性，具体取决于 &lt;code&gt;algorithm&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="92eb61902d4dfd6571a464d87feff72fe7b32901" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="translated">此实现基于R. Rubinstein，M. Zibulevsky，M.和Elad，M.，《使用批量正交匹配追踪技术报告有效实现K-SVD算法》，CS Technion，2008年4月&lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;。http：//www.cs。 technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6fb112845601277f8931b295b857e73c1428c8fb" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="translated">这个实现默认情况下是不节省内存的,因为它在不能使用kd-树或球树的情况下(如稀疏矩阵),构建了一个完整的配对相似度矩阵。这个矩阵将消耗n^2个浮点数。绕过这个问题的机制有以下几种。</target>
        </trans-unit>
        <trans-unit id="cc51c30dcd01f51cada4be15777f17eb95eb7cbd" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="translated">此实现不适用于大规模应用。特别是，scikit-learn不提供GPU支持。有关更快的基于GPU的实现以及提供更多灵活性来构建深度学习架构的框架，请参见&lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0e30a130e6449e6025aca5fcf59ecb737e97cb91" translate="yes" xml:space="preserve">
          <source>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at:</source>
          <target state="translated">这个实现是用Cython编写的,速度相当快。然而,一个更快的API兼容的加载器也可以在以下网站获得。</target>
        </trans-unit>
        <trans-unit id="b488dd9d3cb1238d47d93805595214963db6dd0c" translate="yes" xml:space="preserve">
          <source>This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</source>
          <target state="translated">这个实现使用scipy.sparse.csr_matrix产生了一个稀疏的计数表示。</target>
        </trans-unit>
        <trans-unit id="42bc7bbc3f5bd0df8efbfbad62976f6ec6db583b" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that 3 PLS packages provided in the R language (R-project):</source>
          <target state="translated">本实施例提供的结果与R语言中提供的3个PLS包(R-项目)相同。</target>
        </trans-unit>
        <trans-unit id="09c013dbb84b7e3d406ea0732bc3a8236ed37cbd" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that the &amp;ldquo;plspm&amp;rdquo; package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; of the &amp;ldquo;mixOmics&amp;rdquo; package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.</source>
          <target state="translated">使用函数plsca（X，Y），此实现提供与R语言（R-project）中提供的&amp;ldquo; plspm&amp;rdquo;包相同的结果。结果与&amp;ldquo; mixOmics&amp;rdquo;包的函数 &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; 相等或共线。区别在于，由于mixOmics实现未将y_weights归一化，因此它并未完全实现Wold算法。</target>
        </trans-unit>
        <trans-unit id="36e4a374c505873717456a086d5c9ed44e5157f6" translate="yes" xml:space="preserve">
          <source>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</source>
          <target state="translated">这个实现将拒绝将scipy.sparse矩阵居中,因为这会使它们变成非稀疏的,并有可能因内存耗尽问题而使程序崩溃。</target>
        </trans-unit>
        <trans-unit id="6684c1532df5f751b6b61c242ea952621dc3f4e8" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense and sparse numpy arrays of floating point values.</source>
          <target state="translated">这个实现可以处理以密集和稀疏的浮点值numpy数组表示的数据。</target>
        </trans-unit>
        <trans-unit id="b0df3cb22108e4cd0ed0fcd534ed03e427412c64" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays of floating point values for the features.</source>
          <target state="translated">这个实现是用密集的numpy数组浮点值来表示特征的数据。</target>
        </trans-unit>
        <trans-unit id="4abb3ee00da8e0ef45c7b10f884f8d272712ca81" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.</source>
          <target state="translated">这个实现可以处理以密集numpy数组或稀疏scipy数组浮点值表示的数据。</target>
        </trans-unit>
        <trans-unit id="c3c22c958df17cff584f8c572beeb762a9a0290e" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).</source>
          <target state="translated">这个实现可以处理以密集或稀疏的浮点值数组表示的特征数据。它所拟合的模型可以用损失参数来控制;默认情况下,它拟合的是线性支持向量机(SVM)。</target>
        </trans-unit>
        <trans-unit id="c43a7d8bb7931a79100804db2f074a29d45e4b6b" translate="yes" xml:space="preserve">
          <source>This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called &amp;ldquo;Concentration of Measure&amp;rdquo; or &amp;ldquo;Curse of Dimensionality&amp;rdquo; for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.</source>
          <target state="translated">这种改善在轮廓系数中不明显，这对于两者来说都是很小的，因为对于像文本数据这样的高维数据集，此度量似乎遭受称为&amp;ldquo;度量集中&amp;rdquo;或&amp;ldquo;维数诅咒&amp;rdquo;的现象。其他度量（例如V度量和调整后的兰德指数）都是基于信息理论的评估得分：因为它们仅基于聚类分配而不是距离，因此不受维度诅咒的影响。</target>
        </trans-unit>
        <trans-unit id="a0d4ffe805942e66e32866a4eb458e728074d78e" translate="yes" xml:space="preserve">
          <source>This initially creates clusters of points normally distributed (std=1) about vertices of an &lt;code&gt;n_informative&lt;/code&gt;-dimensional hypercube with sides of length &lt;code&gt;2*class_sep&lt;/code&gt; and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.</source>
          <target state="translated">最初，这将创建一个长度为 &lt;code&gt;2*class_sep&lt;/code&gt; 且边长为2 * class_sep的正态分布于（n = &lt;code&gt;n_informative&lt;/code&gt; 维超立方体的顶点周围的点簇（std = 1），并为每个类分配相等数量的簇。它引入了这些功能之间的相互依赖性，并为数据增加了各种类型的进一步噪声。</target>
        </trans-unit>
        <trans-unit id="5b14c6be212126cf2e3bdc1fe1c6fe8f3c7dc46d" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; as at version 0.20 and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="translated">此接口从0.20版开始处于&lt;strong&gt;试验状态&lt;/strong&gt;，后续版本可能会更改属性，恕不另行通知（尽管对 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 的更改应该很小）。</target>
        </trans-unit>
        <trans-unit id="7f6be37b4617684744b3ccc169d2c583b6e3ddc1" translate="yes" xml:space="preserve">
          <source>This is a convenience alias to &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; to do random permutations of the collections.</source>
          <target state="translated">这是 &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; 以便对集合进行随机排列的便利别名。</target>
        </trans-unit>
        <trans-unit id="4632bc2ee98a17257db1d248b06f38b79a53d4ef" translate="yes" xml:space="preserve">
          <source>This is a convenience function; the transformation is done using the default settings for &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这是一种便利功能；转换使用&lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;的默认设置完成。要进行更高级的使用（停用词过滤，n-gram提取等），请将fetch_20newsgroups与自定义&lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2b3dbf5e1c5e08d66c77786f2bcbc632afc0312a" translate="yes" xml:space="preserve">
          <source>This is a convenience routine for the sake of testing. For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.</source>
          <target state="translated">这是一个方便的例程,用于测试。对于许多指标,scipy.spatial.distance.cdist 和 scipy.spatial.distance.pdist 中的实用程序会更快。</target>
        </trans-unit>
        <trans-unit id="7a67e0a846a2ab3c88cb06fc2b7950cd30914abe" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</source>
          <target state="translated">这是UCI ML乳腺癌威斯康星州（诊断）数据集的副本。&lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22bae61d9be3213577df5087cb01b12cfdf8dff4" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Wine recognition datasets. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</source>
          <target state="translated">这是UCI ML Wine识别数据集的副本。&lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c52f45448ee0e84b694b7c38bdbed7fd0e586461" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML housing dataset. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</source>
          <target state="translated">这是UCI ML住房数据集的副本。&lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3dee2146876159a5a0b048cd24a612fae4a810ca" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="translated">这是UCI ML手写数字数据集测试集的副本，网址为&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handheld+Digits&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd4a60c08b29c6d6af237f8cfa14740252c7d04a" translate="yes" xml:space="preserve">
          <source>This is a general function, given points on a curve. For computing the area under the ROC-curve, see &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;. For an alternative way to summarize a precision-recall curve, see &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">给定曲线上的点，这是一项常规功能。要计算ROC曲线下的面积，请参阅&lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;。有关汇总精确调用曲线的另一种方法，请参见&lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="37510c6c60985c0ea76b5bcc4364db965e5a12fd" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">这是ColumnTransformer构造函数的简写;它不需要也不允许为变换器命名。取而代之的是,将根据它们的类型自动给它们命名。它也不允许加权。</target>
        </trans-unit>
        <trans-unit id="022d95ed0540e35ea0bdce867e9a502603bc5f51" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">这是 FeatureUnion 构造函数的简写;它不需要也不允许为变换器命名。取而代之的是,将根据它们的类型自动给它们命名。它也不允许加权。</target>
        </trans-unit>
        <trans-unit id="52a890ca0cc5d284d366294db21e8c380349733e" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</source>
          <target state="translated">这是管道构造函数的简写;它不需要也不允许对估计器进行命名。取而代之的是,它们的名称将被自动设置为其类型的小写。</target>
        </trans-unit>
        <trans-unit id="bcbf4cb6d3eb7ea12d02241a9a60f7a4e6044f4d" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.predict(X)&lt;/code&gt;.</source>
          <target state="translated">这是 &lt;code&gt;estimator_.predict(X)&lt;/code&gt; 的包装。</target>
        </trans-unit>
        <trans-unit id="17ebe8027dbbfba976bb150f0e9172e06d0c02ec" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.score(X, y)&lt;/code&gt;.</source>
          <target state="translated">这是 &lt;code&gt;estimator_.score(X, y)&lt;/code&gt; 的包装。</target>
        </trans-unit>
        <trans-unit id="602675ab661ad893c615b29b13c1d56146fcfa0b" translate="yes" xml:space="preserve">
          <source>This is an alternative to passing a &lt;code&gt;backend='backend_name'&lt;/code&gt; argument to the &lt;code&gt;Parallel&lt;/code&gt; class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</source>
          <target state="translated">这是将 &lt;code&gt;backend='backend_name'&lt;/code&gt; 参数传递给 &lt;code&gt;Parallel&lt;/code&gt; 类构造函数的替代方法。当调用在内部使用joblib但未在其自己的API中公开后端参数的库代码时，此功能特别有用。</target>
        </trans-unit>
        <trans-unit id="47f9c3947e84bb8a914aa6f2f19cb2c5e42e970f" translate="yes" xml:space="preserve">
          <source>This is an example of &lt;strong&gt;bias/variance tradeoff&lt;/strong&gt;: the larger the ridge &lt;code&gt;alpha&lt;/code&gt; parameter, the higher the bias and the lower the variance.</source>
          <target state="translated">这是&lt;strong&gt;偏差/方差折衷的&lt;/strong&gt;一个示例：脊 &lt;code&gt;alpha&lt;/code&gt; 参数越大，偏差越大，方差越小。</target>
        </trans-unit>
        <trans-unit id="d75c7c933c17fefbabe7c2e292b885d0ecac3a21" translate="yes" xml:space="preserve">
          <source>This is an example of applying &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).</source>
          <target state="translated">这是在文档语料库上应用&lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt;并提取该语料库主题结构的附加模型的示例。输出是主题列表，每个主题都表示为术语列表（未显示权重）。</target>
        </trans-unit>
        <trans-unit id="53176f2993974522405fa17c9a80a84e38a4969c" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&amp;rsquo;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch.</source>
          <target state="translated">这是一个示例，显示了如何使用核心方法将scikit-learn用于分类：从不适合主内存的数据中学习。我们利用一种在线分类器，即一种支持partial_fit方法的分类器，该分类器将提供大量示例。为了确保要素空间随着时间的推移保持不变，我们利用了HashingVectorizer，它将每个示例投影到相同的要素空间中。这在文本分类的情况下特别有用，在文本分类中，每批中都可能出现新功能（单词）。</target>
        </trans-unit>
        <trans-unit id="1620bf9fc1f7795235eabc8e2a67657eca16390d" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</source>
          <target state="translated">这是一个展示如何使用 scikit-learn 使用单词袋方法按主题对文档进行分类的例子。这个例子使用scipy.sparse矩阵来存储特征,并展示了各种可以有效处理稀疏矩阵的分类器。</target>
        </trans-unit>
        <trans-unit id="687fdb042e4ef171de769c4722977550577ec678" translate="yes" xml:space="preserve">
          <source>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.</source>
          <target state="translated">这个例子展示了如何使用 scikit-learn 通过使用词袋方法按主题进行文档聚类。这个例子使用 scipy.sparse 矩阵来存储特征,而不是标准的 numpy 数组。</target>
        </trans-unit>
        <trans-unit id="c505c2f7b70a5aa0d5582bdc56a7d9627b32a4d8" translate="yes" xml:space="preserve">
          <source>This is an example showing the prediction latency of various scikit-learn estimators.</source>
          <target state="translated">这是一个显示各种scikit-learn估计器的预测延迟的例子。</target>
        </trans-unit>
        <trans-unit id="9e4f7a05490ee1267f03d9980bace7147baa0b76" translate="yes" xml:space="preserve">
          <source>This is an extension of the algorithm in scipy.stats.mode.</source>
          <target state="translated">这是scipy.stats.mode中算法的扩展。</target>
        </trans-unit>
        <trans-unit id="375819c22c211b4c7fc97205acd724c3a575f620" translate="yes" xml:space="preserve">
          <source>This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that there will be no speedup with liblinear solver, since it does not handle warm-starting.</source>
          <target state="translated">这是一个使用前一个模型的结果沿着解集加速计算的实现,使得它比连续调用LogisticRegression来处理不同的参数要快。需要注意的是,使用liblinear求解器不会有任何加速,因为它不处理暖启动。</target>
        </trans-unit>
        <trans-unit id="89098058da4c55a1db96b87aadaa162a0a15baba" translate="yes" xml:space="preserve">
          <source>This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">假定这样做是为了实现scikit-learn估计器接口。估算者需要提供 &lt;code&gt;score&lt;/code&gt; 函数，或者必须通过 &lt;code&gt;scoring&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a9f1a5b0fa7d00ad69170d1ab81bf1031dee11a2" translate="yes" xml:space="preserve">
          <source>This is called a &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation.</source>
          <target state="translated">这称为&lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;交叉验证。</target>
        </trans-unit>
        <trans-unit id="2e974743bc0fdffbf7238debaf0ee76bb5a5d9b2" translate="yes" xml:space="preserve">
          <source>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</source>
          <target state="translated">这就是所谓的余弦相似性,因为欧几里得(L2)归一化将向量投射到单位球面上,它们的点积就是向量所表示的点之间角度的余弦。</target>
        </trans-unit>
        <trans-unit id="9b01365512b47448f649e450ddd111360a73cfc3" translate="yes" xml:space="preserve">
          <source>This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and is a core problem that machine learning addresses.</source>
          <target state="translated">这称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;维数诅咒，&lt;/a&gt;是机器学习解决的核心问题。</target>
        </trans-unit>
        <trans-unit id="25e5e11d6a0e13a60841f1cb72db59989d03472f" translate="yes" xml:space="preserve">
          <source>This is currently implemented in the following classes:</source>
          <target state="translated">目前在以下类中实现。</target>
        </trans-unit>
        <trans-unit id="a774a1be5070f83615f896d6d2ec16ebfbe92e4e" translate="yes" xml:space="preserve">
          <source>This is done in 2 steps:</source>
          <target state="translated">这是通过2个步骤完成的。</target>
        </trans-unit>
        <trans-unit id="0c564a0d4cfad247ff47792f5a12558130a84f0c" translate="yes" xml:space="preserve">
          <source>This is equivalent to fit followed by transform, but more efficiently implemented.</source>
          <target state="translated">这相当于先拟合后变换,但实现效率更高。</target>
        </trans-unit>
        <trans-unit id="831022bba18e9ed70a7a762cd8243e7523afeddb" translate="yes" xml:space="preserve">
          <source>This is especially useful when the whole dataset is too big to fit in memory at once.</source>
          <target state="translated">当整个数据集太大而无法一次放入内存时,这一点特别有用。</target>
        </trans-unit>
        <trans-unit id="75c0bef753e28aecf63e47221c52fe362a027981" translate="yes" xml:space="preserve">
          <source>This is implemented as &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.</source>
          <target state="translated">这以 &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; 形式实现，该函数将通过估计器为每个可能的类对预测决策结果的方法，返回投票最多的类的标签。</target>
        </trans-unit>
        <trans-unit id="9b2a6723fed7b2d139e18e341020f963dc7f8450" translate="yes" xml:space="preserve">
          <source>This is implemented by linking the points X into the graph of geodesic distances of the training data. First the &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.</source>
          <target state="translated">这是通过将点X链接到训练数据的测地线距离图中来实现的。首先，在训练数据中找到X 的 &lt;code&gt;n_neighbors&lt;/code&gt; 个最近邻居，然后从中计算出X中每个点到训练数据中每个点的最短测地距离，以构造核。X的嵌入是此内核在训练集的嵌入向量上的投影。</target>
        </trans-unit>
        <trans-unit id="51ae8a82b3eff6fb295f70aa28171d41c73ac3db" translate="yes" xml:space="preserve">
          <source>This is implemented in &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt;. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; constructor parameter. This parameter has no influence on &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这在&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; 中&lt;/a&gt;实现。可以使用 &lt;code&gt;n_components&lt;/code&gt; 构造函数参数设置所需的尺寸。此参数对&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt; &lt;/a&gt;没有影响。</target>
        </trans-unit>
        <trans-unit id="de8220f3c95931fc4241bdd5334e66fca04da2f0" translate="yes" xml:space="preserve">
          <source>This is known as &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这称为&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1d1ef16c8ffe6df8c7a1a81b132f67cdda1b92ea" translate="yes" xml:space="preserve">
          <source>This is more efficient than calling fit followed by transform.</source>
          <target state="translated">这比调用fit后再进行transform更有效率。</target>
        </trans-unit>
        <trans-unit id="1dfb3afc660617ced3002fefa24847b5bb1a14dd" translate="yes" xml:space="preserve">
          <source>This is mostly equivalent to calling:</source>
          <target state="translated">这主要相当于打电话。</target>
        </trans-unit>
        <trans-unit id="bbd51f304b678a157464ff7ab03c52123d04218d" translate="yes" xml:space="preserve">
          <source>This is not a symmetric function.</source>
          <target state="translated">这不是一个对称函数。</target>
        </trans-unit>
        <trans-unit id="0b5c42967b0e34c52656dd80a4e659c6f0fa2181" translate="yes" xml:space="preserve">
          <source>This is not exactly the same as &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt;. The authors of &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</source>
          <target state="translated">这与 &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt; 并不完全相同。&lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt;的作者更喜欢上面的版本，因为它始终是肯定的。由于内核是可加性的，因此可以单独处理所有组件\（x_i \）进行嵌入。这样就可以以规则的间隔对傅立叶变换进行采样，而不是使用蒙特卡洛采样进行近似。</target>
        </trans-unit>
        <trans-unit id="f7f802fcac19c1c8ed1c55f673312d761a2c94a7" translate="yes" xml:space="preserve">
          <source>This is not the case for &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt;: both are bound by the relationship:</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt;并非如此：两者都受以下关系约束：</target>
        </trans-unit>
        <trans-unit id="5d73f5b087f2ecb2dadb8778d3d18cfc19507034" translate="yes" xml:space="preserve">
          <source>This is not true for &lt;code&gt;mutual_info_score&lt;/code&gt;, which is therefore harder to judge:</source>
          <target state="translated">对于 &lt;code&gt;mutual_info_score&lt;/code&gt; 并非如此，因此很难判断：</target>
        </trans-unit>
        <trans-unit id="982101a3d677907e48e034c807cde26531a0468b" translate="yes" xml:space="preserve">
          <source>This is only available if no vocabulary was given.</source>
          <target state="translated">只有在没有给定词汇的情况下才能使用。</target>
        </trans-unit>
        <trans-unit id="0ca1a333516e3b52907835d092958662636ea528" translate="yes" xml:space="preserve">
          <source>This is particularly important for doing grid searches:</source>
          <target state="translated">这对于做网格搜索尤为重要。</target>
        </trans-unit>
        <trans-unit id="2413be66af5e312ff97e484aff33c56868971fbe" translate="yes" xml:space="preserve">
          <source>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&amp;rsquo;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp;amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</source>
          <target state="translated">这也许是模式识别文献中最著名的数据库。费舍尔的论文是该领域的经典之作，至今一直被引用。（例如，请参见Duda＆Hart。）数据集包含3个类，每个类有50个实例，其中每个类都涉及一种虹膜植物。一类与另一类可线性分离；另一类可线性分离。后者不能线性分离。</target>
        </trans-unit>
        <trans-unit id="32ef6d8e689e0cbccf726fb7a94339c2864c487f" translate="yes" xml:space="preserve">
          <source>This is present only if &lt;code&gt;refit&lt;/code&gt; is not False.</source>
          <target state="translated">仅当 &lt;code&gt;refit&lt;/code&gt; 不为False 时才存在。</target>
        </trans-unit>
        <trans-unit id="717414c2af196799a3d1dc1aec1269a995318377" translate="yes" xml:space="preserve">
          <source>This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.</source>
          <target state="translated">这与错误集大小类似,但由相关和不相关标签的数量加权。排名损失为零时,性能最佳。</target>
        </trans-unit>
        <trans-unit id="00034266cafd87d919959c8134650d981f2c4466" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="translated">这是scikit-learn的类和函数参考。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;完整的用户指南&lt;/a&gt;以获取更多详细信息，因为类和函数的原始规范可能不足以提供有关其用法的完整指南。有关在API上重复的概念的参考，请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;&amp;ldquo;通用术语表和API元素&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9c0b38fb17b983574b86706f2172c4c4bac1c6a5" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier&amp;rsquo;s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="translated">这是在（多项式）逻辑回归及其扩展（例如神经网络）中使用的损失函数，定义为给定概率分类器预测的真实标签的负对数似然性。仅为两个或多个标签定义对数丢失。对于在{0,1}中具有真实标签yt且估计概率yp为yt = 1的单个样本，对数损失为</target>
        </trans-unit>
        <trans-unit id="a9111de5c7f4d9db0e2dc4faf926c5c47ae0eb12" translate="yes" xml:space="preserve">
          <source>This is the result of calling &lt;code&gt;method&lt;/code&gt;</source>
          <target state="translated">这是调用 &lt;code&gt;method&lt;/code&gt; 的结果</target>
        </trans-unit>
        <trans-unit id="611492c50f944d397ce592f12eabee4801e28de1" translate="yes" xml:space="preserve">
          <source>This is the structured version, that takes into account some topological structure between samples.</source>
          <target state="translated">这是结构化的版本,它考虑到了样本之间的一些拓扑结构。</target>
        </trans-unit>
        <trans-unit id="eb0a6c68cdbb70ef7265210b8b8760243faa6912" translate="yes" xml:space="preserve">
          <source>This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.</source>
          <target state="translated">这对于拟合截距项的实现很有用,否则无法直接拟合它。</target>
        </trans-unit>
        <trans-unit id="1c420e62ce6697ac415dbca5798c0153080b025c" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果必须重用先前使用的模型的存储属性，这将很有用。如果设置为False，则每次调用时系数都会被改写。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="42f2c0052d88e51f5df6c26752c192f81040ec01" translate="yes" xml:space="preserve">
          <source>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; accepts &lt;code&gt;scipy.sparse&lt;/code&gt; matrices. (Note that the tf-idf functionality in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; can produce normalized vectors, in which case &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; is equivalent to &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt;, only slower.)</source>
          <target state="translated">该内核是用于计算以tf-idf向量表示的文档的相似度的流行选择。&lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt;接受 &lt;code&gt;scipy.sparse&lt;/code&gt; 矩阵。（请注意， &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 中的tf-idf功能可以生成规范化的矢量，在这种情况下，&lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt;等效于&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;，只是速度较慢。）</target>
        </trans-unit>
        <trans-unit id="f1fbcea40cf4aba903be6eddf36c859a1718e3b8" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth.</source>
          <target state="translated">这个核是无限可微分的,这意味着以这个核作为协方函数的GP具有所有阶数的均方导数,因此是非常平稳的。</target>
        </trans-unit>
        <trans-unit id="6ad60f9f3ef06c9a3898414b0afc593eaff20c1d" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</source>
          <target state="translated">这个核是无限可微分的,这意味着以这个核作为协方函数的GP具有所有阶数的均方导数,因此是非常平稳的。由RBF核产生的GP的前、后值如下图所示。</target>
        </trans-unit>
        <trans-unit id="6cbded70a18b870dfff7fda8d59e204ebd77900f" translate="yes" xml:space="preserve">
          <source>This kind of singular profiles is often seen in practice, for instance:</source>
          <target state="translated">这种单一的型材在实践中经常见到,比如。</target>
        </trans-unit>
        <trans-unit id="1df4414d1e47e9ea41e98b8c6e13b5eeb49e06ac" translate="yes" xml:space="preserve">
          <source>This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes &amp;ldquo;for free&amp;rdquo; as no additional data is needed and can be used for model selection.</source>
          <target state="translated">这个遗漏的部分可以用来估计泛化误差，而不必依赖单独的验证集。该估计是&amp;ldquo;免费&amp;rdquo;的，因为不需要其他数据，可以用于模型选择。</target>
        </trans-unit>
        <trans-unit id="4f52c7809ab985057ba93af9b88459b0d10b33a2" translate="yes" xml:space="preserve">
          <source>This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.</source>
          <target state="translated">这就保证了损失函数不会受到离群值的严重影响,同时也不会完全忽略其影响。</target>
        </trans-unit>
        <trans-unit id="4826ff4b754b03a246d73c38aa2c971ffdf335e1" translate="yes" xml:space="preserve">
          <source>This means each weight \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="translated">这意味着每个权重(w_{i})都是从高斯分布中抽取的,以零为中心,精度为(lambda_{i})。</target>
        </trans-unit>
        <trans-unit id="de2308f740624c092eea038ac13deb5af2b1fa80" translate="yes" xml:space="preserve">
          <source>This means that any classifiers handling multi-output multiclass or multi-task classification tasks, support the multi-label classification task as a special case. Multi-task classification is similar to the multi-output classification task with different model formulations. For more information, see the relevant estimator documentation.</source>
          <target state="translated">这意味着,任何处理多输出多类或多任务分类任务的分类器,都会支持多标签分类任务这一特殊情况。多任务分类与多输出分类任务类似,但模型公式不同。更多信息,请参见相关估计器文档。</target>
        </trans-unit>
        <trans-unit id="6157bc0b8c2f67c8a593bf2d12852c000be43e72" translate="yes" xml:space="preserve">
          <source>This measure is not adjusted for chance. Therefore &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; might be preferred.</source>
          <target state="translated">这项措施没有调整的机会。因此，&lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt;可能是首选。</target>
        </trans-unit>
        <trans-unit id="cca48ea1404a91d2df7b18cac656df30067cb12b" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.</source>
          <target state="translated">这种方法可以在每次升压迭代后进行监控(即确定测试集的误差)。</target>
        </trans-unit>
        <trans-unit id="778da8cdceb825f45e49260c13130972c415ba18" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each stage.</source>
          <target state="translated">这种方法可以在每个阶段后进行监测(即确定测试集的误差)。</target>
        </trans-unit>
        <trans-unit id="893e20b8eaafff147aad0ee519c22b2be0783798" translate="yes" xml:space="preserve">
          <source>This method allows to generalize prediction to &lt;em&gt;new observations&lt;/em&gt; (not in the training set). Only available for novelty detection (when novelty is set to True).</source>
          <target state="translated">这种方法可以将预测推广到&lt;em&gt;新的观测值&lt;/em&gt;（不在训练集中）。仅可用于新颖性检测（新颖性设置为True时）。</target>
        </trans-unit>
        <trans-unit id="e13bbab31202d773dbd5b155d7e0b7799ca54f84" translate="yes" xml:space="preserve">
          <source>This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n, p) this method has a cost of \(O(n p^2)\), assuming that \(n \geq p\).</source>
          <target state="translated">如果X是一个大小为(n,p)的矩阵,这种方法的成本为(O(n p^2)\),假设(n geq p\)。</target>
        </trans-unit>
        <trans-unit id="dc0919b0c811a79ed295c00492df459fa5ab93e8" translate="yes" xml:space="preserve">
          <source>This method doesn&amp;rsquo;t do anything. It exists purely for compatibility with the scikit-learn transformer API.</source>
          <target state="translated">此方法没有任何作用。纯粹为了与scikit-learn转换器API兼容而存在。</target>
        </trans-unit>
        <trans-unit id="89035c0aee87e0125ffcf7bf5f0dbe56fcfb74a9" translate="yes" xml:space="preserve">
          <source>This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">这种方法有一定的性能和数值稳定性开销,因此最好在尽可能大的数据块上调用partial_fit(只要在内存预算中拟合),以隐藏开销。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
