<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="51c9f9fb1fac83429c51404b5e9b2caee57cc2f2" translate="yes" xml:space="preserve">
          <source>Input validation for standard estimators.</source>
          <target state="translated">标准估计器的输入验证。</target>
        </trans-unit>
        <trans-unit id="346da7aa7d7e4eb884706a35a69404b7d3fc9daa" translate="yes" xml:space="preserve">
          <source>Input validation on an array, list, sparse matrix or similar.</source>
          <target state="translated">在数组、列表、稀疏矩阵或类似情况下进行输入验证。</target>
        </trans-unit>
        <trans-unit id="4191049318f63b6c3a85211b91a5c1cecb649bdb" translate="yes" xml:space="preserve">
          <source>Input values.</source>
          <target state="translated">输入值:</target>
        </trans-unit>
        <trans-unit id="8344beaf285df55c120907e8b74a9a1f87253895" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are 4 independent features uniformly distributed on the intervals:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是在区间上均匀分布的4个独立特征：</target>
        </trans-unit>
        <trans-unit id="bcbcb56a88ddeef69ce01bcc9a78a456071e2cf0" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are independent features uniformly distributed on the interval [0, 1]. The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是独立的特征，均匀分布在间隔[0，1]上。根据以下公式创建输出 &lt;code&gt;y&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="20a9e2ae79f377ec69f0ec221a6bcb99fe892698" translate="yes" xml:space="preserve">
          <source>Inputs: fitted predictive model \(m\), tabular dataset (training or validation) \(D\).</source>
          <target state="translated">输入:拟合的预测模型(m\),表格数据集(训练或验证)(D\)。</target>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">将新数据插入到已经拟合的LSH森林中。</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">在已经拟合的LSH森林中插入新数据。成本与新的总规模成正比,所以添加时应分批进行。</target>
        </trans-unit>
        <trans-unit id="aa15440a6446ecaf7603f9c0287316507f4328a1" translate="yes" xml:space="preserve">
          <source>Inspecting coefficients across the folds of a cross-validation loop gives an idea of their stability.</source>
          <target state="translated">检查交叉验证环路褶皱处的系数可以了解其稳定性。</target>
        </trans-unit>
        <trans-unit id="d3ce4618efaae8bf391d0768eaf2c4b834adfb9b" translate="yes" xml:space="preserve">
          <source>Inspection</source>
          <target state="translated">Inspection</target>
        </trans-unit>
        <trans-unit id="4c0fbc7b0ca330086776985f409e7f037b2f9494" translate="yes" xml:space="preserve">
          <source>Instance of the estimator.</source>
          <target state="translated">估计器的实例。</target>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">与其使用基数为&amp;ldquo; n select k&amp;rdquo;的集合（其中n是样本数，k是子样本数（至少特征数））进行计算，如果选择&amp;ldquo; n select&amp;rdquo;，则仅考虑给定最大大小的随机子种群k'大于max_subpopulation。对于较小的问题，如果不更改n_subsamples，则此参数将确定内存使用情况和运行时间。</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">LARS解决方案没有给出矢量结果，而是由一条曲线表示，该曲线表示针对参数矢量L1范数的每个值的解决方案。完整系数路径存储在数组 &lt;code&gt;coef_path_&lt;/code&gt; 中，该数组的大小为（n_features，max_features + 1）。第一列始终为零。</target>
        </trans-unit>
        <trans-unit id="848ef7b85f04c1e0179836725b124a8c68948c34" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \(\ell_1\) norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">LARS解决方案没有给出矢量结果，而是由一条曲线表示，该曲线表示针对参数矢量\（\ ell_1 \）范数的每个值的解决方案。完整系数路径存储在数组 &lt;code&gt;coef_path_&lt;/code&gt; 中，该数组的大小为（n_features，max_features + 1）。第一列始终为零。</target>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">而不是调整链上各个组件的参数,而是可以在一个可能值的网格上运行一个最佳参数的详尽搜索。我们在单词或大词组上试用所有分类器,有无idf,线性SVM的惩罚参数为0.01或0.001。</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">取而代之的是，调用者应该显式设置 &lt;code&gt;with_centering=False&lt;/code&gt; （在这种情况下，将仅对CSR矩阵的特征执行方差缩放），或者如果调用者 &lt;code&gt;X.toarray()&lt;/code&gt; 化的密集数组适合，则调用X.toarray（）。在记忆中。</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">取而代之的是，预期调用者要么显式设置 &lt;code&gt;with_mean=False&lt;/code&gt; （在这种情况下，仅对CSC矩阵的特征执行方差缩放），或者如果调用 &lt;code&gt;X.toarray()&lt;/code&gt; 希望物化密集数组适合，则调用X.toarray（）。在记忆中。</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">取而代之的是,假设对/(w/)的分布是一个轴平行的椭圆高斯分布。</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">如果给出一个字节序列进行分析，但其中包含的字符不是给定 &lt;code&gt;encoding&lt;/code&gt; 字符，该怎么办。默认情况下，它是&amp;ldquo;严格的&amp;rdquo;，这意味着将引发UnicodeDecodeError。其他值是&amp;ldquo;忽略&amp;rdquo;和&amp;ldquo;替换&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">如果给出一个字节序列进行分析，但其中包含的字符不是给定 &lt;code&gt;encoding&lt;/code&gt; 字符，该怎么办。作为关键字参数&amp;ldquo;错误&amp;rdquo;传递给bytes.decode。</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">整数个标签数组,如果没有提供,标签将由y_true和y_pred推断。如果没有提供,标签将由y_true和y_pred推断。</target>
        </trans-unit>
        <trans-unit id="e031a894709099be1ecbe448974105f94db94157" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to linear predictor.</source>
          <target state="translated">截距(也就是偏置)加到线性预测器上。</target>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">在决策函数中加入截距(也就是偏置)。</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">拦截项。</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">该算法保留的内部足够的统计信息。保留它们在联机设置中很有用，以避免丢失演化历史，但对于最终用户而言，它们不应有任何用处。A（n_components，n_components）是字典协方差矩阵。B（n_features，n_components）是数据近似矩阵</target>
        </trans-unit>
        <trans-unit id="e1895bccbde849f2ce31dc6715c34549e1152575" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid losing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">该算法保留的内部足够的统计信息。保留它们在联机设置中很有用，以避免丢失演化历史，但对于最终用户而言，它们不应有任何用处。 A（n_components，n_components）是字典协方差矩阵。 B（n_features，n_components）是数据近似矩阵</target>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">在内部,Laplace近似用于用高斯来近似非高斯后验。</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">在内部，目标 &lt;code&gt;y&lt;/code&gt; 始终会转换为二维数组，以供scikit-learn转换器使用。在预测时，输出将被整形为具有与 &lt;code&gt;y&lt;/code&gt; 相同数量的维。</target>
        </trans-unit>
        <trans-unit id="465a9fa03a440d5f1b8441512ea129ccebe5933c" translate="yes" xml:space="preserve">
          <source>Internally, this method uses &lt;code&gt;max_iter = 1&lt;/code&gt;. Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence and early stopping should be handled by the user.</source>
          <target state="translated">在内部，此方法使用 &lt;code&gt;max_iter = 1&lt;/code&gt; 。因此，不能保证在调用一次cost函数之后就达到了最小值。诸如目标收敛和提前停止之类的问题应由用户处理。</target>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">在内部，我们使用&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;和&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;处理所有计算。这些库使用C和Cython包装。</target>
        </trans-unit>
        <trans-unit id="921b6b42e9e212246385b90b6e2081ffae4bdd4d" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt; and &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers.</source>
          <target state="translated">在内部，我们使用&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm &lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt;和&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear &lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt;来处理所有计算。这些库使用C和Cython包装。有关实现的描述和所用算法的详细信息，请参阅其各自的论文。</target>
        </trans-unit>
        <trans-unit id="a02157db035ff864370a2c436b6c81a38e8d8a3c" translate="yes" xml:space="preserve">
          <source>Interpreting coefficients: scale matters</source>
          <target state="translated">解释系数:比例尺很重要</target>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">使用&amp;ldquo; Ctrl-C&amp;rdquo;中断多进程作业</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">直观的解释：与劣V-措施聚类可以&lt;strong&gt;定性分析的同质性和完整性的条款&lt;/strong&gt;，以更好地感受什么错误的&amp;ldquo;类&amp;rdquo;是由分配完成。</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">直觉上，&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;精度&lt;/a&gt;是分类器不将阴性样本标记为正样本的能力，&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;召回率&lt;/a&gt;是分类器查找所有阳性样本的能力。</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">直观上,我们也可以把直方图看成是一个块的堆叠,每个点一个块。通过在适当的网格空间中堆叠区块,我们就可以恢复直方图。但是,如果我们不把块堆叠在规则的网格上,而是把每个块集中在它所代表的点上,并对每个位置的总高度进行求和呢?这个想法导致了左下角的可视化。它也许没有直方图那么干净,但数据驱动块位置的事实意味着它能更好地表达底层数据。</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">直观地， &lt;code&gt;gamma&lt;/code&gt; 参数定义单个训练示例的影响达到的程度，低值表示&amp;ldquo;远&amp;rdquo;，高值表示&amp;ldquo;接近&amp;rdquo;。的 &lt;code&gt;gamma&lt;/code&gt; 参数可以被看作是由模型支持向量选择的样本的影响的半径的倒数。</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">直观地讲,这个矩阵可以解释为一个伪特征的矩阵(点提高到某个幂级)。该矩阵类似于(但又不同于)多项式核所引起的矩阵。</target>
        </trans-unit>
        <trans-unit id="d0136f60343b9ddfe4e95ff298a3f11e6a44a13d" translate="yes" xml:space="preserve">
          <source>Intuitively, we&amp;rsquo;re trying to maximize the margin (by minimizing \(||w||^2 = w^Tw\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \(y_i (w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from their correct margin boundary. The penalty term &lt;code&gt;C&lt;/code&gt; controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below).</source>
          <target state="translated">直观地，我们试图最大化边距（通过最小化\（|| w || ^ 2 = w ^ Tw \）），同时在样本分类错误或在边距边界内时会受到惩罚。理想情况下，所有样本的值\（y_i（w ^ T \ phi（x_i）+ b）\）将为\（\ geq 1 \），这表明预测是理想的。但是问题通常并不总是可以与超平面完全分开，因此我们允许一些样本与它们的正确边界相距\（\ zeta_i \）。罚分项 &lt;code&gt;C&lt;/code&gt; 控制此罚分的强度，因此，它起反正则化参数的作用（请参见下面的注释）。</target>
        </trans-unit>
        <trans-unit id="fcf37d79a0d7f3a40e6e7bdc86aa285b256f5c04" translate="yes" xml:space="preserve">
          <source>Inverse Gaussian</source>
          <target state="translated">逆高斯</target>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">正则化强度的倒数;必须是正数浮点数。与支持向量机一样,数值越小,正则化程度越强。</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">逆向变换。</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">反向变换。返回一个大小为nb_features的向量,其中包含分配给每组特征的Xred值。</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">逆变换矩阵。仅在 &lt;code&gt;fit_inverse_transform&lt;/code&gt; 为True 时可用。</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">逆向变换阵列。</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">调用传递的估计量的传递的方法名称。对于method ='predict_proba'，列对应于已排序的类。</target>
        </trans-unit>
        <trans-unit id="93ce645e781a1eaea74d358c1f7aa54ffb25426b" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;VotingClassifier&lt;/code&gt; 上调用 &lt;code&gt;fit&lt;/code&gt; 方法将对将存储在类属性 &lt;code&gt;self.estimators_&lt;/code&gt; 中的那些原始估算器的克隆进行适配。可以使用 &lt;code&gt;set_params&lt;/code&gt; 将估算器设置为 &lt;code&gt;'drop'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;VotingClassifier&lt;/code&gt; 上调用 &lt;code&gt;fit&lt;/code&gt; 方法将对将存储在类属性 &lt;code&gt;self.estimators_&lt;/code&gt; 中的那些原始估计量的克隆进行调整。可以使用 &lt;code&gt;set_params&lt;/code&gt; 将估算器设置为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0f0eee1c2a0f3878912e931a58a1d92e46d13c5a" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingRegressor&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;VotingRegressor&lt;/code&gt; 上调用 &lt;code&gt;fit&lt;/code&gt; 方法将对将存储在类属性 &lt;code&gt;self.estimators_&lt;/code&gt; 中的那些原始估算器的克隆进行拟合。可以使用 &lt;code&gt;set_params&lt;/code&gt; 将估算器设置为 &lt;code&gt;'drop'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">高斯过程分类(GPC)的等概率线。</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">隔离林算法</target>
        </trans-unit>
        <trans-unit id="90b7e1d9dae263e13bf54b6eb5bbce295b25c458" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm.</source>
          <target state="translated">隔离林算法。</target>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">隔离林示例</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">等值线图嵌入</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">R中的同调优化:池-相邻-违约者算法(PAVA)和主动集方法 Leeuw,Hornik,Mair 统计软件杂志 2009年</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">同调中值回归。Nilotpal Chakravarti,《运筹学》第14卷第2期(1989年5月),第303-308页。</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">同位素回归</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">y的同调配合。</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">同位素回归模型。</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">当函数被调用/类被实例化时发出警告,并在docstring中添加一个警告。</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">它能适应手头的数据。</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">它允许指定多个评估指标。</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">也可以用集卡式表达。</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">它也可以直接用作 &lt;code&gt;kernel&lt;/code&gt; 参数：</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">它也可以作为考虑布尔随机变量的估计器的预处理步骤(例如,在贝叶斯环境中使用伯努利分布建模)。</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">它也可以用来将非数字标签(只要它们是可哈希和可比较的)转换为数字标签。</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">它也可以用来将非数字标签(只要它们是可哈希和可比较的)转换为数字标签。</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">它还可以在损失函数中加入一个正则化项,缩小模型参数以防止过拟合。</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">它也可以使用scipy.sparse.linalg ARPACK实现截断的SVD。</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">可以使用参数 &lt;code&gt;(estimator, X, y)&lt;/code&gt; 进行调用，其中 &lt;code&gt;estimator&lt;/code&gt; 是应评估的模型， &lt;code&gt;X&lt;/code&gt; 是验证数据， &lt;code&gt;y&lt;/code&gt; 是 &lt;code&gt;X&lt;/code&gt; （在有监督的情况下）或 &lt;code&gt;None&lt;/code&gt; （在无监督的情况下）的真实目标。案件）。</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">可以使用&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt;函数下载/加载。</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">可以理解为每个条目的加权差额。</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">可以注意到，k均值（和小批量k均值）对特征缩放非常敏感，在这种情况下，IDF加权有助于将聚类的质量提高很多，这是针对20个新闻组数据集的类标签分配。</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">从图中可以看出，具有两个非零系数的&lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;正交匹配追踪（OMP）的结果&lt;/a&gt;与仅保留一个系数时（边缘看起来不太明显）相比，偏差较小。此外，它离Frobenius规范中的地面真理更近。</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">可以看出,/(C/nu/)-SVC公式是/(C/C/)-SVC的重新参数化,因此在数学上是等价的。</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">它可用于单变量特征选择，请参阅《&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;用户指南》中的更多内容&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">它可以用来在估计程序中加入正则化参数。</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">它不需要学习率。</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">它没有给出一个单一的指标来用作集群优化的目标。</target>
        </trans-unit>
        <trans-unit id="53127b98db145a1107f55ea45dbcbf9eb40fb387" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">在[Hoyer，2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt;中已经观察到，经过仔细约束，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;可以生成基于零件的数据集表示形式，从而产生可解释的模型。以下示例显示了与PCA特征脸相比，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;从Olivetti人脸数据集中的图像中发现的16个稀疏分量。</target>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">在[Hoyer，2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt;中已经观察到，经过仔细约束，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;可以产生基于零件的数据集表示，从而产生可解释的模型。以下示例显示了与PCA特征脸相比，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;从Olivetti人脸数据集中的图像中发现的16个稀疏分量。</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">它的特性类似于计算机视觉中经常使用的指数化chi平方核,但允许对特征图进行简单的蒙特卡洛近似。</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">它被证明对应用于无噪声数据的机器学习很有用。&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;简而言之，&lt;/a&gt;参见例如机器学习中的量子力学。</target>
        </trans-unit>
        <trans-unit id="bbe0780585e0153715a866cbcbfa3d1d5e8429c4" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">它已被证明对应用于无噪声数据的机器学习很有用。&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;简而言之，&lt;/a&gt;参见例如机器学习中的量子力学。</target>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">它实现了随机厨房水槽的一个变种[1]。</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">它是一个线性模型,以L1先验作为正则化器进行训练。</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">它是一种内存有效的在线学习算法，可以替代&lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt;。它构造一个树数据结构，其中簇质心从叶中读取。这些可以是最终的聚类质心，也可以作为其他聚类算法（例如&lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; )的&lt;/a&gt;输入提供。</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">它是控制在线学习方法中学习率的参数。该值应设置在（0.5，1.0]之间，以确保渐近收敛。当值为0.0且batch_size为 &lt;code&gt;n_samples&lt;/code&gt; 时，更新方法与批处理学习相同，在文献中，这称为kappa。</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">建议将参数 &lt;code&gt;epsilon&lt;/code&gt; 设置为1.35，以达到95％的统计效率。</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">在文本处理界,使用二进制特征值也是很常见的(可能是为了简化概率推理),即使归一化计数(也就是术语频率)或TF-IDF值特征在实践中往往表现得稍好。</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">它也被称为方差比标准。</target>
        </trans-unit>
        <trans-unit id="657bf821e2dc05fc87b192deecf1d4c429b7d563" translate="yes" xml:space="preserve">
          <source>It is also possible to compute the permutation importances on the training set. This reveals that &lt;code&gt;random_num&lt;/code&gt; gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use that random numerical feature to overfit. You can further confirm this by re-running this example with constrained RF with min_samples_leaf=10.</source>
          <target state="translated">还可以计算训练集上的排列重要性。这表明与在测试集上计算时相比， &lt;code&gt;random_num&lt;/code&gt; 的重要性排名明显更高。这两幅图之间的差异证实了RF模型具有足够的能力来使用该随机数值特征进行过拟合。您可以通过在min_samples_leaf = 10的受限RF下重新运行此示例来进一步确认这一点。</target>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">也可以对字典和/或代码进行正向约束,以匹配数据中可能存在的约束。下面是应用了不同正性约束的面。红色表示负值,蓝色表示正值,白色表示零。</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">也可以有效地生成一个稀疏图,显示相邻点之间的连接。</target>
        </trans-unit>
        <trans-unit id="0396c69b921d6f190c09b79532ebbdc31b35e115" translate="yes" xml:space="preserve">
          <source>It is also possible to encode each column into &lt;code&gt;n_categories - 1&lt;/code&gt; columns instead of &lt;code&gt;n_categories&lt;/code&gt; columns by using the &lt;code&gt;drop&lt;/code&gt; parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;), since co-linearity would cause the covariance matrix to be non-invertible. When this parameter is not None, &lt;code&gt;handle_unknown&lt;/code&gt; must be set to &lt;code&gt;error&lt;/code&gt;:</source>
          <target state="translated">也可以使用 &lt;code&gt;drop&lt;/code&gt; 参数将每一列编码为 &lt;code&gt;n_categories - 1&lt;/code&gt; 列而不是 &lt;code&gt;n_categories&lt;/code&gt; 列。该参数允许用户为每个要删除的功能指定一个类别。在某些分类器中，这有助于避免输入矩阵中的共线性。例如，当使用非正则回归（&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt; &lt;code&gt;LinearRegression&lt;/code&gt; &lt;/a&gt;）时，此类功能很有用，因为共线性会导致协方差矩阵不可逆。当此参数不为None时，必须将 &lt;code&gt;handle_unknown&lt;/code&gt; 设置为 &lt;code&gt;error&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">通过设置 &lt;code&gt;output_distribution='normal'&lt;/code&gt; 还可以使用&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;将数据映射到正态分布。将先前的示例与虹膜数据集结合使用：</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">也可以通过传递一个交叉验证迭代器来代替使用其他交叉验证策略,例如。</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">传统上，它用于分离混合信号（称为&lt;em&gt;盲源分离&lt;/em&gt;的问题），如下例所示：</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">它的计算速度与正向选择一样快,复杂程度与普通最小二乘法相同。</target>
        </trans-unit>
        <trans-unit id="eaad2537722d5ddb17252eb65683de60a4e9ec00" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares.</source>
          <target state="translated">它的计算速度与正向选择一样快,复杂度与普通最小二乘法相同。</target>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">它被转换为F分,然后再转换为P值。</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">它很容易修改,以产生其他估计器的解决方案,如Lasso。</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">分类器很容易过度拟合出现在20个新闻组数据中的特定内容，例如新闻组标题。许多分类器获得很高的F分数，但是其结果不会推广到不在此时间范围内的其他文档。</target>
        </trans-unit>
        <trans-unit id="f3493e2a2c4e4ad9e265942ad2fd137cc9804a32" translate="yes" xml:space="preserve">
          <source>It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time.</source>
          <target state="translated">一般建议避免使用明显多于机器上CPU数量的进程或线程。当一个程序同时运行过多的线程时,就会出现过度订阅的情况。</target>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">如果特征数量非常多，强烈建议使用另一种降维方法（例如，对于密集数据使用PCA或对于稀疏数据使用TruncatedSVD）将尺寸数量减少到合理的数量（例如50个）。这将抑制一些噪声并加快样本之间成对距离的计算。有关更多技巧，请参阅Laurens van der Maaten的常见问题解答[2]。</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">使用 &lt;code&gt;fit&lt;/code&gt; 方法训练模型时，将标识符与标记的数据一起分配给未标记的点很重要。此实现使用的标识符是整数值\（-1 \）。</target>
        </trans-unit>
        <trans-unit id="643f8f6ee250eb138ea3f0ff80df853cb01ed9c1" translate="yes" xml:space="preserve">
          <source>It is important to keep in mind that the coefficients that have been dropped may still be related to the outcome by themselves: the model chose to suppress them because they bring little or no additional information on top of the other features. Additionnaly, this selection is unstable for correlated features, and should be interpreted with caution.</source>
          <target state="translated">重要的是要记住,被剔除的系数本身可能仍然与结果有关:模型选择抑制它们,因为它们在其他特征之外带来的额外信息很少或没有。另外,这种选择对于相关特征来说是不稳定的,应谨慎解释。</target>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">需要注意的是,当样本数量远大于特征数量时,人们会期望不需要缩减。这背后的直觉是,如果种群协方差是全秩的,当样本数量增长时,样本协方差也会变成正定值。因此,没有必要进行缩减,方法应该自动进行缩减。</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">它由150个虹膜观察组成，每个虹膜都有4个特征：它们的萼片和花瓣的长度和宽度，详细信息请参见 &lt;code&gt;iris.DESCR&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">如果只选取与总特征数量相比较少的特征,例如与特征数量相比样本非常少的情况下,它比LassoCV更有效率。</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">将这些预测传递到评估指标中是不合适的。使用&lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt;来衡量泛化错误。</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">不推荐在库中调用Parallel时对后端名称进行硬编码。相反,建议设置软提示(prefer)或硬约束(require),这样可以让库用户使用parallel_backend上下文管理器从外部改变后端。</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">它没有被规范化(处罚)。</target>
        </trans-unit>
        <trans-unit id="3023247377e7882a0cbda1c2d8280926be6aa8ba" translate="yes" xml:space="preserve">
          <source>It is now possible to prune most tree-based estimators once the trees are built. The pruning is based on minimal cost-complexity. Read more in the &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;User Guide&lt;/a&gt; for details.</source>
          <target state="translated">树木建成后，现在可以修剪大多数基于树的估计量。修剪基于最小的成本复杂性。有关详细信息，请阅读《&lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;用户指南》&lt;/a&gt;中的更多内容。</target>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">在p &amp;gt;&amp;gt; n的情况下（即，当维数明显大于点数时），在数值上有效</target>
        </trans-unit>
        <trans-unit id="3387956abcbcbc8c7f2e1b04a07247bbce69a743" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where the number of features is significantly greater than the number of samples.</source>
          <target state="translated">在特征数量明显大于样本数量的情况下,它在数值上是有效的。</target>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">将数据投射到一个保留大部分方差的低维空间,通过放弃与较低的奇异值相关的分量的奇异向量,通常是有趣的。</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">可能并建议在超参数空间中搜索最佳&lt;a href=&quot;cross_validation#cross-validation&quot;&gt;交叉验证&lt;/a&gt;得分。</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">可以调整二值机的阈值。</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">可以计算每个标签的精确度、召回、F1分数和支持率,而不是平均。</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">通过显式 &lt;code&gt;random_state&lt;/code&gt; 伪随机数生成器，可以控制结果可重复性的随机性。</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">可以通过传递一个可调用的向量器构造函数来定制行为。</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通过将 &lt;code&gt;with_mean=False&lt;/code&gt; 或 &lt;code&gt;with_std=False&lt;/code&gt; 传递给&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;的构造函数，可以禁用居中或缩放。</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">可以通过以下方式取回类别名称。</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">可以对标尺属性进行反省,找到在训练数据上学习到的转化的确切性质。</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">通过将要加载的类别列表传递给&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt;函数，可以仅加载类别的子选择：</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">可以在同一运行中混合使用稀疏数组和密集数组。</target>
        </trans-unit>
        <trans-unit id="54adadb321f18cc462f6fa41bc0c4a25f1f6b29d" translate="yes" xml:space="preserve">
          <source>It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The &lt;code&gt;statsmodels
package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; natively supports this. Within sklearn, one could use bootstrapping instead as well.</source>
          <target state="translated">在不惩罚的情况下，有可能获得系数的p值和置信区间。所述 &lt;code&gt;statsmodels package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; 本身支持这一点。在sklearn中，也可以使用自举。</target>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">它可以通过组合的&amp;ldquo;散列绝招&amp;rdquo;（克服这些局限性&lt;a href=&quot;#feature-hashing&quot;&gt;特点哈希&lt;/a&gt;通过实施）&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; &lt;/a&gt;类和文本预处理和符号化的特征&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">通过使用Python的内置持久性模型&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;，可以在scikit-learn中保存模型：</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">使用Python的内置持久性模型&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;可以将模型保存在scikit-learn中：</target>
        </trans-unit>
        <trans-unit id="c1dfe5304fca32594b4f7b15a0ed1671355448d1" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">通过使用Python的内置持久性模型&lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;，可以在scikit-learn中保存模型：</target>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">可以使用参数 &lt;code&gt;categories&lt;/code&gt; 来明确指定。我们的数据集中有两种性别，四个可能的大洲和四个网络浏览器：</target>
        </trans-unit>
        <trans-unit id="7291e604dadcc649d0d77ccb4ebf5e3c457ba713" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">建议使用&lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt; &lt;code&gt;plot_confusion_matrix&lt;/code&gt; &lt;/a&gt;创建一个&lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt; &lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt; &lt;/a&gt;。所有参数都存储为属性。</target>
        </trans-unit>
        <trans-unit id="ab8a32e7f4d09197b24de455e36e8e5fe3df1e84" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">建议使用&lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt; &lt;code&gt;plot_precision_recall_curve&lt;/code&gt; &lt;/a&gt;创建可视化器。所有参数都存储为属性。</target>
        </trans-unit>
        <trans-unit id="80b150a43cd5aba26116528fb4f30933db56b582" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">建议使用&lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt; &lt;code&gt;plot_roc_curve&lt;/code&gt; &lt;/a&gt;创建可视化器。所有参数都存储为属性。</target>
        </trans-unit>
        <trans-unit id="f18c0fc60439524a8f745b0ef99ce87a71814897" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">建议使用&lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt;创建&lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; &lt;/a&gt;。所有参数都存储为属性。</target>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">有时独立地对特征进行居中和缩放是不够的,因为下游模型可以进一步对特征的线性独立性做一些假设。</target>
        </trans-unit>
        <trans-unit id="b49f552a6383ab2c79a28eb8ae358eb905c8df59" translate="yes" xml:space="preserve">
          <source>It is sometimes tedious to find the model which will best perform on a given dataset. Stacking provide an alternative by combining the outputs of several learners, without the need to choose a model specifically. The performance of stacking is usually close to the best model and sometimes it can outperform the prediction performance of each individual model.</source>
          <target state="translated">有时要找到在给定数据集上表现最好的模型是很乏味的。堆叠提供了一个替代方案,将几个学习器的输出结合起来,而不需要专门选择一个模型。堆叠的性能通常接近于最佳模型,有时它可以超越每个单独模型的预测性能。</target>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">有时值得存储特定变压器的状态，因为它可以再次使用。在 &lt;code&gt;GridSearchCV&lt;/code&gt; 中使用管道会触发这种情况。因此，我们使用参数 &lt;code&gt;memory&lt;/code&gt; 来启用缓存。</target>
        </trans-unit>
        <trans-unit id="9d1619fcc011ef5a461992b135770b43d5982f12" translate="yes" xml:space="preserve">
          <source>It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.</source>
          <target state="translated">当用户对测量由于缺失值导致的不确定性不感兴趣时,单次与多次推算在预测和分类的背景下有多大用处,这仍然是一个未决问题。</target>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">它是学习混合物模型的最快算法。</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">恰恰相反,因为越大越好,即大值对应离群值。</target>
        </trans-unit>
        <trans-unit id="eec2b41e384c85c1e4587a1c1f6fa007f24ac337" translate="yes" xml:space="preserve">
          <source>It is the opposite as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">恰恰相反,因为越大越好,即大值对应离群值。</target>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">值得注意的是，RandomForests和ExtraTrees可以并行安装在许多内核上，因为每棵树都是独立于其他树构建的。AdaBoost的样本是按顺序构建的，因此请勿使用多个内核。</target>
        </trans-unit>
        <trans-unit id="9876d3b6328cf0e41b8f18a6a35d45d86ad7b5f1" translate="yes" xml:space="preserve">
          <source>It is worth noting that more than 93% of policyholders have zero claims. If we were to convert this problem into a binary classification task, it would be significantly imbalanced, and even a simplistic model that would only predict mean can achieve an accuracy of 93%.</source>
          <target state="translated">值得注意的是,93%以上的保户都是零索赔。如果我们将这一问题转化为二元分类任务,就会明显失衡,即使是只预测均值的简单模型也能达到93%的准确率。</target>
        </trans-unit>
        <trans-unit id="86441e9f0bfca4823b62f4a6d4cecce7c1b80a8e" translate="yes" xml:space="preserve">
          <source>It might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting &lt;code&gt;min_samples_leaf=5&lt;/code&gt; or &lt;code&gt;min_samples_leaf=10&lt;/code&gt;) so as to limit overfitting while not introducing too much underfitting.</source>
          <target state="translated">通过限制树的容量（例如，通过设置 &lt;code&gt;min_samples_leaf=5&lt;/code&gt; 或 &lt;code&gt;min_samples_leaf=10&lt;/code&gt; ），可以在训练集上获得一些精度，而在测试集上获得更高的精度，从而在不引入的情况下限制过度拟合很不合身。</target>
        </trans-unit>
        <trans-unit id="c1e3cdc828409a9c2610db18343fc252165aed81" translate="yes" xml:space="preserve">
          <source>It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt; allows for a very different choice of the numerical solvers with distinct computational performance profiles.</source>
          <target state="translated">使用（罚分）最小二乘损失来拟合分类模型而不是更传统的逻辑或铰链损失似乎是有问题的。但是在实践中，所有这些模型都可以在准确性或精确度/召回率方面得出相似的交叉验证得分，而&lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt; &lt;code&gt;RidgeClassifier&lt;/code&gt; &lt;/a&gt;使用的惩罚最小二乘损失允许对具有不同计算性能配置文件的数值求解器进行非常不同的选择。</target>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">它能产生一个完整的片状线性解路径,这在交叉验证或类似的调整模型的尝试中很有用。</target>
        </trans-unit>
        <trans-unit id="dec67b5f65557893043d8c253cdd2dab65f3a96a" translate="yes" xml:space="preserve">
          <source>It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.</source>
          <target state="translated">它代表模型中独立变量所解释的方差(y)的比例。它提供了拟合度的指标,因此,通过解释方差的比例来衡量未见过的样本被模型预测的可能性有多大。</target>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">它返回一个dict,除了测试分数外,还包含拟合时间、得分时间(以及可选的训练分数以及拟合估计器)。</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">它返回一个类似字典的对象,具有以下属性。</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">它返回一个浮点数，该浮点数参考 &lt;code&gt;y&lt;/code&gt; 量化 &lt;code&gt;X&lt;/code&gt; 上的 &lt;code&gt;estimator&lt;/code&gt; 预测质量。同样，按照惯例，数字越大越好，因此，如果您的计分员返回亏损，则该值应为负。</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">需要注意的是,Johnson-Lindenstrauss lemma可以得出非常保守的所需分量的估计值,因为它没有对数据集的结构做出假设。</target>
        </trans-unit>
        <trans-unit id="af7347a7c717add0101a2649bad5550dc47a184a" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">它显示了如何使用&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;来近似RBF内核的特征图，以便在数字数据集上使用SVM进行分类。比较了使用原始空间中的线性SVM，使用近似映射的线性SVM和使用内核化SVM的结果。显示了不同数量的蒙特卡洛采样的时间和精度（在&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;的情况下，它使用随机傅里叶特征）和训练集的不同大小的子集（对于&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;）进行了近似映射。</target>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">它显示了如何使用&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;来近似RBF内核的特征图，以便在数字数据集上使用SVM进行分类。比较了使用原始空间中的线性SVM，使用近似映射和使用内核化SVM的结果。显示了不同数量的蒙特卡洛采样的时间和精度（在&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;的情况下，它使用随机傅立叶特征）和训练集的不同大小的子集（对于&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;）进行了近似映射。</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">它将文本文档的集合转换为包含令牌出现次数（或二进制出现信息）的稀疏矩阵，如果norm ='l1'，则可能归一化为令牌频率，如果norm ='l2'，则投影到欧几里得单位球上。</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">它只在错误的时候更新其模型。</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">它采用LAPACK实现的全SVD或Halko等人2009年方法的随机截断SVD,这取决于输入数据的形状和提取成分的数量。</target>
        </trans-unit>
        <trans-unit id="74ae47bdcf2723d7a82146ada9f167c02a150388" translate="yes" xml:space="preserve">
          <source>It will plot the class decision boundaries given by a Nearest Neighbors classifier when using the Euclidean distance on the original features, versus using the Euclidean distance after the transformation learned by Neighborhood Components Analysis. The latter aims to find a linear transformation that maximises the (stochastic) nearest neighbor classification accuracy on the training set.</source>
          <target state="translated">它将绘制最近邻分类器在原始特征上使用欧氏距离时给出的类决策边界,与使用通过邻域成分分析学习的变换后的欧氏距离。后者的目的是找到一种线性变换,使训练集上的(随机)最近邻分类精度最大化。</target>
        </trans-unit>
        <trans-unit id="80b76c72ce07f72ff1bcc8279eceffb68f3a73b2" translate="yes" xml:space="preserve">
          <source>It would be possible to get even higher predictive performance with a larger neural network but the training would also be significantly more expensive.</source>
          <target state="translated">如果使用更大的神经网络,就有可能获得更高的预测性能,但训练的成本也会大大增加。</target>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">通过将 &lt;code&gt;return_X_y&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; ，几乎所有这些函数还可以将输出约束为仅包含数据和目标的元组。</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">很明显，内核形状如何影响结果分布的平滑度。scikit-learn内核密度估计器可以按以下方式使用：</target>
        </trans-unit>
        <trans-unit id="35600165765d17d14f9a53e3a40d6c087a8e15cc" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes.</source>
          <target state="translated">可以将表示群集的层次合并的树可视化为树状图。目视检查通常对于理解数据结构很有用，尽管在小样本量情况下更是如此。</target>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">迭代2和3,直到收敛。</target>
        </trans-unit>
        <trans-unit id="e39adff24d3659cea88912062bf2425d11890a07" translate="yes" xml:space="preserve">
          <source>Iterative imputation of the missing values</source>
          <target state="translated">缺失值的反复推算。</target>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">证据最大化的迭代程序</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">它的双重性是</target>
        </trans-unit>
        <trans-unit id="ce6398892ce7bfa57c8075a52d29f534a23469a6" translate="yes" xml:space="preserve">
          <source>Its validation performance, measured via the \(R^2\) score, is significantly larger than the chance level. This makes it possible to use the &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt; function to probe which features are most predictive:</source>
          <target state="translated">通过\（R ^ 2 \）分数衡量的验证性能显着大于机会水平。这样就可以使用&lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt; &lt;code&gt;permutation_importance&lt;/code&gt; &lt;/a&gt;函数来探查哪些功能最可预测：</target>
        </trans-unit>
        <trans-unit id="07da5fcab12f57a49e864df0a2dcb43ec8cd118b" translate="yes" xml:space="preserve">
          <source>J&amp;oslash;rgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matem&amp;aacute;tica, no. 51. See also &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;Exponential dispersion model.&lt;/a&gt;</source>
          <target state="translated">J&amp;oslash;rgensen，B.（1992年）。指数色散模型理论和偏差分析。Matematica Monografias，没有。51.另请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;指数弥散模型。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d7e5d74ebe16b65b422b57dd9089de563aa7d4b5" translate="yes" xml:space="preserve">
          <source>J. Cohen (1960). &amp;ldquo;A coefficient of agreement for nominal scales&amp;rdquo;. Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.</source>
          <target state="translated">J.科恩（1960）。&amp;ldquo;公称规模的协议系数&amp;rdquo;。教育和心理测量20（1）：37-46。doi：10.1177 / 001316446002000104。</target>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J.Davis，M.Goadrich，&lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;《精确召回与ROC曲线之间的关系》&lt;/a&gt;，ICML 2006。</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J. Friedman，&amp;ldquo;多元自适应回归样条&amp;rdquo;，《统计年鉴》 19（1），第1-67页，1991年。</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J.Friedman,Greedy Function Approximation:A Gradient Boosting Machine,The Annals of Statistics,Vol.29,No.5,2001.</target>
        </trans-unit>
        <trans-unit id="f06167e7b529cb39087bd6b97f521b456419a7cd" translate="yes" xml:space="preserve">
          <source>J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov. &amp;ldquo;Neighbourhood Components Analysis&amp;rdquo;. Advances in Neural Information Processing Systems. 17, 513-520, 2005. &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</source>
          <target state="translated">J. Goldberger，G。Hinton，S。Roweis，R。Salakhutdinov。&amp;ldquo;邻里成分分析&amp;rdquo;。神经信息处理系统的进展。第17卷，第513-520页，2005年&lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;。http：//www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal，F.Bach，J.Ponce，G.Sapiro，2009年：用于稀疏编码的在线词典学习（&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="8417a497b98a8d480d1cb9818d1f322bb7565268" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J.Mairal，F.Bach，J.Ponce，G.Sapiro，2009年：用于稀疏编码的在线词典学习（&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">诺思曼（J. Nothman），秦宏（H. Qin）和尤尔查克（R.Yurchak）（2018）。&lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;免费开源软件包中的停止单词列表&amp;rdquo;&lt;/a&gt;。在过程中&lt;em&gt;。NLP开源软件研讨会&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="40bd5fef8dd5af699d79639063d7832cf1e45e49" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">诺思曼（J. Nothman），秦宏（H. Qin）和尤尔恰克（R.Yurchak）（2018）。&lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;免费开源软件包中的禁止单词列表&amp;rdquo;&lt;/a&gt;。在过程中&lt;em&gt;。NLP开源软件研讨会&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu，H。Zou，S。Rosset，T。Hastie。&amp;ldquo;多类AdaBoost&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">J.R.Quinlan.C4.5:机器学习的程序。Morgan Kaufmann,1993.</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;对偏最小二乘（PLS）方法的调查，着重于两块案例&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">贾卡德相似性系数得分</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">Jacob A.Wegelin。部分最小二乘法的调查,重点是两块情况。技术报告371,华盛顿大学统计系,西雅图,2000年。</target>
        </trans-unit>
        <trans-unit id="b9a5b6145a558ec82725430f200fe46fa35f6aac" translate="yes" xml:space="preserve">
          <source>Jarvelin, K., &amp;amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446.</source>
          <target state="translated">Jarvelin，K.和Kekalainen，J.（2002）。基于增益的IR技术的累积评估。ACM信息系统交易（TOIS），20（4），422-446。</target>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">Jesse Read,Bernhard Pfahringer,Geoff Holmes,Yew Frank,</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read，Bernhard Pfahringer，Geoff Holmes，Eibe Frank，&amp;ldquo;多标签分类的分类器链&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="43121fdb3391941437c9f79e4fe2a93d4f69bed7" translate="yes" xml:space="preserve">
          <source>Joblib also tries to limit the oversubscription by limiting the number of threads usable in some third-party library threadpools like OpenBLAS, MKL or OpenMP. The default limit in each worker is set to &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; but this limit can be overwritten with the &lt;code&gt;inner_max_num_threads&lt;/code&gt; argument which will be used to set this limit in the child processes.</source>
          <target state="translated">Joblib还尝试通过限制某些第三方库线程池（例如OpenBLAS，MKL或OpenMP）中可用的线程数来限制超额订阅。每个工作程序中的默认限制设置为 &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; 但是可以使用 &lt;code&gt;inner_max_num_threads&lt;/code&gt; 参数覆盖此限制，该参数将用于在子进程中设置此限制。</target>
        </trans-unit>
        <trans-unit id="bbd429df4e4127b1956a8217cca140ac1f4576de" translate="yes" xml:space="preserve">
          <source>Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the &lt;strong&gt;backend&lt;/strong&gt; that it&amp;rsquo;s using.</source>
          <target state="translated">Joblib能够支持多处理和多线程。joblib是选择生成线程还是进程取决于它使用的&lt;strong&gt;后端&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="dbbc7351b3326fa09a2af62a2a8c482a7f498e4a" translate="yes" xml:space="preserve">
          <source>Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the &lt;code&gt;loky&lt;/code&gt; backend (which spawns processes).</source>
          <target state="translated">Joblib当前无法避免在多线程上下文中出现超额订购。它只能 &lt;code&gt;loky&lt;/code&gt; 后端（生成进程）来做到这一点。</target>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">Johanna Hardin,David M Rocke.The distribution of robust distances.Journal of Computational and Graphical Statistics.December 1,2005,14(4):928-946.</target>
        </trans-unit>
        <trans-unit id="3cd3820aa7670cf9157f83a7b518b28ca957a3fc" translate="yes" xml:space="preserve">
          <source>John K. Dixon, &amp;ldquo;Pattern Recognition with Partly Missing Data&amp;rdquo;, IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue: 10, pp. 617 - 621, Oct. 1979. &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;http://ieeexplore.ieee.org/abstract/document/4310090/&lt;/a&gt;</source>
          <target state="translated">John K. Dixon，&amp;ldquo;使用部分丢失的数据进行模式识别&amp;rdquo;，关于系统，人与控制论的IEEE事务，第9卷，第10期，第617-621页，1979年10月&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;。http://ieeexplore.ieee。 org / abstract / document / 4310090 /&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">约翰。D.Kelleher，Brian Mac Namee，Aoife D'Arcy，（2015年）。&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;预测数据分析的机器学习基础：算法，有效示例和案例研究&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">约翰。D.Kelleher，Brian Mac Namee，Aoife D'Arcy，《&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;用于预测数据分析的机器学习基础：算法，实例和案例研究》&lt;/a&gt;，2015年。</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">用多任务Lasso进行联合特征选择</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">联合参数选择</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">杂志的机器学习研究15（10月）的：3221-3245，2014年&lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7eaac587d1f40d409b66183976f554af82049338" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">杂志的机器学习研究15（10月）的：3221-3245，2014年&lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">1988年7月</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">1998年7月</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman,Riccadonna,Furlanello,(2012)。多类预测中MCC和CEN误差测量的比较。</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">正如测试预测器对训练，预处理（例如，标准化，特征选择等）所保留的数据非常重要，类似的&lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;数据转换&lt;/a&gt;也应类似地从训练集中学习并应用于保留的数据以进行预测：</target>
        </trans-unit>
        <trans-unit id="bb93e8a53cb9de4ad35209178664abb04cf0e5f7" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">正如测试预测器对训练，预处理（例如，标准化，特征选择等）所保留的数据非常重要，类似的&lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;数据转换&lt;/a&gt;也应类似地从训练集中学习，并应用于保留的数据以进行预测：</target>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">就像 self.assertTrue(a in b)一样,但有一个更好的默认信息。</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">就像 self.assertTrue(a not in b)一样,但默认信息更好。</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inen和S.&amp;Auml;yr&amp;auml;m&amp;ouml;：&lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;关于稳健数据挖掘的空间中位数计算。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K（X，Y）= &amp;lt;X，Y&amp;gt; /（|| X || * || Y ||）</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">K-Folds交叉验证器</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">K-Means聚类</target>
        </trans-unit>
        <trans-unit id="57bc3e1ca5db2c1c7f140e0c9654a7e1bd0c4cd4" translate="yes" xml:space="preserve">
          <source>K-Means clustering.</source>
          <target state="translated">K-Means聚类。</target>
        </trans-unit>
        <trans-unit id="4dcab3f446cd66684df43251a7a52921a5b665f1" translate="yes" xml:space="preserve">
          <source>K-dimensional tree for fast generalized N-point problems.</source>
          <target state="translated">快速广义N点问题的K维树。</target>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">具有非重叠组的K-fold迭代器变体。</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K-means Clustering</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">使用K均值算法。经典的EM风格算法是&amp;ldquo;完整&amp;rdquo;的。通过使用三角形不等式，&amp;ldquo; elkan&amp;rdquo;变体更为有效，但目前不支持稀疏数据。&amp;ldquo;自动&amp;rdquo;为密集数据选择&amp;ldquo; elkan&amp;rdquo;，为稀疏数据选择&amp;ldquo;完整&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="c00998c8eabed1e931fa81c6f69faf59aad07506" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient on data with well-defined clusters, by using the triangle inequality. However it&amp;rsquo;s more memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).</source>
          <target state="translated">使用K均值算法。经典的EM风格算法是&amp;ldquo;完整&amp;rdquo;的。通过使用三角形不等式，&amp;ldquo; elkan&amp;rdquo;变异对于定义良好的聚类的数据更有效。但是，由于分配了额外的形状数组（n_samples，n_clusters），因此需要更多的内存。</target>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">K均值可用于矢量量化。这是通过使用&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;训练模型的变换方法来实现的。</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K-means聚类</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K-means聚类算法。</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K-means相当于期望最大化算法,其协方差矩阵很小,全等,对角线。</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K均值通常被称为劳埃德算法。从根本上讲，该算法分为三个步骤。第一步选择初始质心，最基本的方法是从数据集\（X \）中选择\（k \）个样本。初始化后，K-means包括其他两个步骤之间的循环。第一步将每个样本分配给其最近的质心。第二步通过获取分配给每个先前质心的所有样本的平均值来创建新质心。计算新旧质心之间的差异，算法重复最后两个步骤，直到该值小于阈值。换句话说，它会重复直到质心不明显移动为止。</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K-means量化</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">快速广义N点问题的KDT树</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree（X，leaf_size = 40，metric ='minkowski'，** kwargs）</target>
        </trans-unit>
        <trans-unit id="e6f48940c5e34202cb03c9567fde221973b85eb8" translate="yes" xml:space="preserve">
          <source>KNN Based Imputation</source>
          <target state="translated">基于KNN的推断</target>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">二元或多类问题可以计算Kappa分数,但多标签问题不能计算Kappa分数(除了手动计算每个标签的分数),也不能计算两个以上注释者的分数。</target>
        </trans-unit>
        <trans-unit id="7642aa421288e310d04c4d2f1c88ac24e935cd02" translate="yes" xml:space="preserve">
          <source>Ke et. al. &lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&amp;ldquo;LightGBM: A Highly Efficient Gradient BoostingDecision Tree&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">柯等 al。&lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&amp;ldquo; LightGBM：高效的梯度Boosting决策树&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">保留3个RGB通道,而不是将它们平均到一个灰度通道。如果color为True,则数据的形状比color=False的形状多一个维度。</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">Ken Tang and Ponnuthurai N.Suganthan and Xi Yao and A.Kai Qin.Linear dimensionalityreduction using relevance weighted LDA.School of Electrical and Electronic Engineering Nanyang Technological University.2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">物种分布的核密度估计</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">核心密度估计</target>
        </trans-unit>
        <trans-unit id="9837c7505d0f3a8c028c3a0430177597bb56e3e2" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation.</source>
          <target state="translated">核密度估计。</target>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">PCA内核</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">核PCA被引入在。</target>
        </trans-unit>
        <trans-unit id="2064482c4c2332e23a8df06a915448e7e780cd46" translate="yes" xml:space="preserve">
          <source>Kernel Principal Component Analysis.</source>
          <target state="translated">核主成分分析。</target>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">内核主成分分析(KPCA)</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">核心带宽。</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">&amp;ldquo; rbf&amp;rdquo;，&amp;ldquo; poly&amp;rdquo;和&amp;ldquo; Sigmoid&amp;rdquo;的内核系数。</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">rbf核的核系数。</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">rbf、poly和sigmoid核的核系数。其他核忽略。</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">rbf，poly，Sigmoid，laplacian和chi2内核的内核系数。被忽略为 &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">scikit-learn中的内核密度估计是在&lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt;估计器中实现的，该估计器使用Ball Tree或KD Tree进行有效查询（有关这些内容的讨论，请参阅&lt;a href=&quot;neighbors#neighbors&quot;&gt;最近邻居&lt;/a&gt;）。尽管为简化起见，上面的示例使用一维数据集，但是可以在任意多个维度上执行内核密度估计，尽管在实践中，维度的诅咒会导致其性能在高维度上下降。</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">对数边际似然被评估的内核超参数。如果为None，则返回 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 的预先计算的log_marginal_likelihood。</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">对数边际似然被评估的内核超参数。在多类分类的情况下，theta可能是复合内核或单个内核的超参数。在后一种情况下，将为所有单个内核分配相同的theta值。如果为None，则返回 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 的预先计算的log_marginal_likelihood。</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">核k(X,Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">要近似的内核映射。一个可调用的对象应该接受两个参数和作为kernel_params传递给这个对象的关键字参数,并且应该返回一个浮点数。</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">内部使用的内核映射。一个callable应该接受两个参数和作为参数传递给该对象的关键字参数kernel_params，并且应该返回一个浮点数。设置为&amp;ldquo;预先计算&amp;rdquo;以便将预先计算的内核矩阵传递给估计器方法，而不是样本。</target>
        </trans-unit>
        <trans-unit id="438349cc7f220c5c2d499eba1d3d6c5414057ba4" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. This parameter is directly passed to &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is a string, it must be one of the metrics in &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if &lt;code&gt;kernel&lt;/code&gt; is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="translated">内部使用的内核映射。此参数直接传递到 &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt; 。如果 &lt;code&gt;kernel&lt;/code&gt; 是字符串，则它必须是 &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt; 中的指标之一。如果 &lt;code&gt;kernel&lt;/code&gt; 是&amp;ldquo;预先计算的&amp;rdquo;，则假定X为内核矩阵。或者，如果 &lt;code&gt;kernel&lt;/code&gt; 是可调用的函数，则在每对实例（行）上调用它，并记录结果值。可调用对象应将X的两行作为输入，并以单个数字返回相应的内核值。这意味着不允许来自&lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; 的&lt;/a&gt;可调用对象，因为它们在矩阵而不是单个样本上运行。请使用标识内核的字符串代替。</target>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">核心矩阵。</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">像支持向量机或核化PCA这样的核方法依赖于重现核希尔伯特空间的特性。对于任何一个正定核函数(k)(一个所谓的Mercer核),保证存在一个映射(phi)到一个Hilbert空间(mathcal{H})的映射,因此</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">将数据投射到交替维度空间的内核方法</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">内核运算符采用一个或两个基本内核，并将它们组合成一个新内核。的&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;内核需要两个内核\（K1 \）和\（K2 \），并通过将它们组合\（{K_总和}（X，Y）= K1（X，Y）+ K2（X，Y）\）。该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;的内核需要两个内核\（K1 \）和\（K2 \），并通过将它们组合\（{K_产物}（X，Y）= K1（X，Y）* K2（X，Y）\）。所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt;内核需要一个基本内核，并通过一个标量参数\（指数\）并且将它们组合\（{K_ EXP}（X，Y）= K（X，Y）^ \ {文本指数} \）。</target>
        </trans-unit>
        <trans-unit id="6d4564c4032221fcc796901a96d1a6cf81d6aa2f" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(p\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^p\). Note that magic methods &lt;code&gt;__add__&lt;/code&gt;, &lt;code&gt;__mul___&lt;/code&gt; and &lt;code&gt;__pow__&lt;/code&gt; are overridden on the Kernel objects, so one can use e.g. &lt;code&gt;RBF() + RBF()&lt;/code&gt; as a shortcut for &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt;.</source>
          <target state="translated">内核运算符采用一个或两个基本内核，并将它们组合成一个新内核。的&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;内核需要两个内核\（K_1 \）和\（K_2 \），并通过将它们组合\（{K_总和}（X，Y）= K_1（X，Y）+ K_2（X，Y）\）。该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;的内核需要两个内核\（K_1 \）和\（K_2 \），并通过将它们组合\（{K_产物}（X，Y）= K_1（X，Y）* K_2（X，Y）\）。所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt;内核需要一个基本内核和经由\一个标量参数\（P \），并将其组合（K_ {EXP}（X，Y）= K（X，Y）^ P \）。注意，魔术方法 &lt;code&gt;__add__&lt;/code&gt; ， &lt;code&gt;__mul___&lt;/code&gt; 和 &lt;code&gt;__pow__&lt;/code&gt; 在内核对象上被覆盖，因此可以使用 &lt;code&gt;RBF() + RBF()&lt;/code&gt; 作为 &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt; 的快捷方式。</target>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">内核岭回归（KRR）&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt;结合了&lt;a href=&quot;linear_model#ridge-regression&quot;&gt;岭回归&lt;/a&gt;（具有L2-范数正则化的线性最小二乘）和内核技巧。因此，它学习了由各个内核和数据产生的空间中的线性函数。对于非线性内核，这对应于原始空间中的非线性函数。</target>
        </trans-unit>
        <trans-unit id="3fcbd037e0e31c66e26fa73fca7d9588d56b4cd8" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge regression and classification&lt;/a&gt; (linear least squares with l2-norm regularization) with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernel trick&lt;/a&gt;. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">核岭回归（KRR）&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt;将核&lt;a href=&quot;linear_model#ridge-regression&quot;&gt;回归和分类&lt;/a&gt;（具有l2-范数正则化的线性最小二乘）与&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;核技巧&lt;/a&gt;结合在一起。因此，它学习了由各个内核和数据产生的空间中的线性函数。对于非线性内核，这对应于原始空间中的非线性函数。</target>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">内核岭回归(KRR)将岭回归(线性最小二乘法与l2-正则化)与内核技巧相结合。因此,它在各自内核和数据引起的空间中学习一个线性函数。对于非线性内核,这相当于原始空间中的非线性函数。</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">内核岭回归将岭回归与内核技巧相结合</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">核脊回归。</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">模型中使用的内核:线性、多项式、RBF、sigmoid或预计算。</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">模型中使用的内核：线性，多项式，RBF，S形或预先计算。默认情况下为&amp;ldquo; rbf&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">由一组其他内核组成的内核。</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">核心。默认值=&amp;ldquo;线性&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">核（在GP中也称为&amp;ldquo;协方差函数&amp;rdquo;）是GP的重要组成部分，它决定了GP前后的形状。他们通过定义两个数据点的&amp;ldquo;相似性&amp;rdquo;，并结合相似数据点应具有相似目标值的假设，对正在学习的函数的假设进行编码。可以区分两类内核：固定内核仅取决于两个数据点的距离，而不取决于它们的绝对值\（k（x_i，x_j）= k（d（x_i，x_j））\），因此不变于转换在输入空间中，非平稳内核也取决于数据点的特定值。固定核可以进一步细分为各向同性和各向异性的核，其中各向同性核也对输入空间中的旋转不变。更多细节，我们参考第4章&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98cdf159fe1dcd2fc6aef984c01878bfe7d52c10" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;. For guidance on how to best combine different kernels, we refer to &lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;.</source>
          <target state="translated">核（在GP中也称为&amp;ldquo;协方差函数&amp;rdquo;）是GP的重要组成部分，它决定GP前后的形状。他们通过定义两个数据点的&amp;ldquo;相似性&amp;rdquo;以及相似数据点应具有相似目标值的假设，对正在学习的函数的假设进行编码。可以区分两类内核：固定内核仅取决于两个数据点的距离，而不取决于它们的绝对值\（k（x_i，x_j）= k（d（x_i，x_j））\），因此不变于转换在输入空间中，非平稳内核也取决于数据点的特定值。固定核可以进一步细分为各向同性和各向异性的核，其中各向同性的核对于输入空间中的旋转也是不变的。更多细节，我们指的是第4章&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;。有关如何最佳组合不同内核的指导，请参考&lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">内核是相似的措施，即， &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; 如果对象 &lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 被认为是&amp;ldquo;更类似于&amp;rdquo;比对象 &lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;c&lt;/code&gt; 。内核还必须是正半定数。</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">内核通过超参数向量\（\ theta \）进行参数化。这些超参数可以例如控制内核的长度尺度或周期性（请参见下文）。通过在 &lt;code&gt;__call__&lt;/code&gt; 方法中设置 &lt;code&gt;eval_gradient=True&lt;/code&gt; ，所有内核都支持针对\（\ theta \）计算内核自协方差的解析梯度。高斯过程（回归器和分类器）都使用此梯度来计算对数边际似然率的梯度，而对数边际似然率的梯度又用于确定\（\ theta \）的值，从而使对数边际最大化。通过梯度上升的可能性。对于每个超参数，在创建内核实例时都需要指定初始值和界限。 \（\ theta \）的当前值可以通过该属性获取和设置 &lt;code&gt;theta&lt;/code&gt; 内核对象的theta。而且，超参数的界限可以通过内核的属性 &lt;code&gt;bounds&lt;/code&gt; 来访问。请注意，这两个属性（&amp;theta;和范围）都返回内部使用值的对数转换后的值，因为这些值通常更适合基于梯度的优化。每个超参数的规范被存储在的实例的形式&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt;在相应的内核。注意，使用名称为&amp;ldquo; x&amp;rdquo;的超参数的内核必须具有self.x和self.x_bounds属性。</target>
        </trans-unit>
        <trans-unit id="4aed51cbfb6629b3c22c46504375ed74bc60a033" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">内核通过超参数向量\（\ theta \）进行参数化。这些超参数可以例如控制内核的长度尺度或周期性（请参见下文）。通过在 &lt;code&gt;__call__&lt;/code&gt; 方法中设置 &lt;code&gt;eval_gradient=True&lt;/code&gt; ，所有内核都支持针对\（\ theta \）计算内核自协方差的解析梯度。高斯过程（回归器和分类器）都使用此梯度来计算对数边际似然率的梯度，而对数边际似然率的梯度又用于确定\（\ theta \）的值，该值会最大化对数边际可能性。通过梯度上升的可能性。对于每个超参数，在创建内核实例时都需要指定初始值和界限。 \（\ theta \）的当前值可以通过该属性获取和设置 &lt;code&gt;theta&lt;/code&gt; 内核对象的theta。而且，超参数的界限可以通过内核的属性 &lt;code&gt;bounds&lt;/code&gt; 来访问。请注意，这两个属性（&amp;theta;和范围）都返回内部使用值的对数转换后的值，因为这些值通常更适合基于梯度的优化。每个超参数的规范被存储在的实例的形式&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt;在相应的内核。请注意，使用名称为&amp;ldquo; x&amp;rdquo;的超参数的内核必须具有属性self.x和self.x_bounds。</target>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">凯文&amp;middot;墨菲（Kevin P. Murphy），《机器学习：概率论》，麻省理工学院出版社第14.4.3章，第492-493页</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">键是可以传递给&lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; 的&lt;/a&gt;参数名称。</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">关键字参数允许将这些默认值修改为特定的数据集（请参阅参数 &lt;code&gt;target_name&lt;/code&gt; ， &lt;code&gt;data_name&lt;/code&gt; ， &lt;code&gt;transpose_data&lt;/code&gt; 和以下示例）。</target>
        </trans-unit>
        <trans-unit id="6a687df4f73e66be23d8d5cd9810da872c9b92e2" translate="yes" xml:space="preserve">
          <source>Keyword arguments passed to the coordinate descent solver.</source>
          <target state="translated">传递给坐标下降解算器的关键字参数。</target>
        </trans-unit>
        <trans-unit id="ee035295632767669037b1fd1546556e8af6cebd" translate="yes" xml:space="preserve">
          <source>Keyword arguments to be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="translated">要传递给matplotlib的 &lt;code&gt;plot&lt;/code&gt; 的关键字参数。</target>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">要传递给指定的度量函数的关键字参数。</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger，Anirban Dasgupta，John Langford，Alex Smola和Josh Attenberg（2009）。&lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;特征哈希用于大规模多任务学习&lt;/a&gt;。进程 ICML。</target>
        </trans-unit>
        <trans-unit id="aaf2909b07b71367a7207c2f93060ee37cc58e6c" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger，Anirban Dasgupta，John Langford，Alex Smola和Josh Attenberg（2009）。&lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;特征哈希用于大规模多任务学习&lt;/a&gt;。程序。ICML。</target>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma，Diederik和Jimmy Ba。&amp;ldquo;亚当：一种随机的方法</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger，Y.，Basri，R.，Chang，JT，＆Gerstein，M.（2003年）。芯片数据的光谱二聚化：基因和条件的聚类。基因组研究，13（4），703-716。</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">Kluger，Yuval等。等人，2003年。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;微阵列数据的光谱二聚化：基因和条件的聚聚&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt;仅知道样本数，因此保守估计随机子空间的最小大小，以确保随机投影引入的有界失真：</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">Koby Crammer,Yoram Singer.On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines.Journal of Machine Learning Research 2,(2001),265-292。</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">优化后的Kullback-Leibler分歧。</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS是一种求解器，它近似表示代表函数二阶偏导数的黑森州矩阵。此外，它近似于Hessian矩阵的逆矩阵以执行参数更新。该实现使用&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;的Scipy版本。</target>
        </trans-unit>
        <trans-unit id="554ef38240e48c4335936815621409310d3aac71" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS是近似于代表函数二阶偏导数的Hessian矩阵的求解器。此外，它近似于Hessian矩阵的逆矩阵以执行参数更新。该实现使用&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;的Scipy版本。</target>
        </trans-unit>
        <trans-unit id="7bcf28acc035a046a1884f5a68a7e0642aed3a3f" translate="yes" xml:space="preserve">
          <source>L-BFGS-B &amp;ndash; Software for Large-scale Bound-constrained Optimization</source>
          <target state="translated">L-BFGS-B &amp;ndash;大规模约束优化软件</target>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L. Breiman，&amp;ldquo;装袋预测器&amp;rdquo;，机器学习24，第123-140页，1996年。</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L. Breiman，&amp;ldquo;装袋预测变量&amp;rdquo;，机器学习，24（2），123-140，1996年。</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L. Breiman，&amp;ldquo;粘贴小票以在大型数据库中在线分类&amp;rdquo;，Machine Learning，36（1），85-103，1999年。</target>
        </trans-unit>
        <trans-unit id="97e482bcc046e44b1e543a4a852a328e802cd962" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001. &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</source>
          <target state="translated">L. Breiman，&amp;ldquo;随机森林&amp;rdquo;，机器学习，45（1），5-32，2001年&lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman，J。Friedman，R。Olshen和C.Stone，&amp;ldquo;分类和回归树&amp;rdquo;，沃兹沃思，贝尔蒙特，加利福尼亚，1984年。</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L.Breiman,J.Friedman,R.Olshen,and C.Stone.分类和回归树。Wadsworth,Belmont,CA,1984.</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L. Breiman，P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;回归中的子模型选择和评估：X随机案例&lt;/a&gt;，国际统计评论1992；</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman和A. Cutler，&amp;ldquo;随机森林&amp;rdquo;，&lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http：//www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c982df17d29f8aba32aa6a03bfa726201c066e60" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman和A. Cutler，&amp;ldquo;随机森林&amp;rdquo;，&lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https：//www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ecce2961bb8a3f0ec10fc321ad8811dfc6312ac" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector&amp;rdquo;, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko，NN Leonenko，&amp;ldquo;随机向量熵的样本估计&amp;rdquo;，Probl。Peredachi Inf。，23：2（1987），9-16</target>
        </trans-unit>
        <trans-unit id="d97aac3a80efb6d42fc11cefc484a2c681583627" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko，NN Leonenko，&amp;ldquo;随机向量熵的样本估计：，Probl。Peredachi Inf。，23：2（1987），9-16</target>
        </trans-unit>
        <trans-unit id="f170f61c9bead94cf287d881c88071c0e3a5501e" translate="yes" xml:space="preserve">
          <source>L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</source>
          <target state="translated">L.Hubert和P.Arabie，比较分区，《分类杂志》 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9026b644be7a2edc54521dca8f44e2af501befa2" translate="yes" xml:space="preserve">
          <source>L. Mosley, &lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;A balanced approach to the multi-class imbalance problem&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">L. Mosley，&lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;一种解决多类别不平衡问题的平衡方法&lt;/a&gt;，IJCV 2010。</target>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C.Moore,John DeNero.</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">L1惩罚和逻辑回归中的稀疏性。</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">L1规范。\(R(w):=\sum_{i=1}^{n})|w_i|\),从而导致稀疏的解。</target>
        </trans-unit>
        <trans-unit id="b5bea6bb158694d1df1789af92353788b4accacc" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse solutions.</source>
          <target state="translated">L1规范。\(R(w):=\sum_{j=1}^{m})|w_j|\),从而导致稀疏的解。</target>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">L2规范。\(R(w):=\frac{1}{2})\sum_{i=1}^{n}w_i^2/)。)</target>
        </trans-unit>
        <trans-unit id="2f4d6f15c348b07a6c4412e8ecac45c72eb1770b" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),</source>
          <target state="translated">L2规范。\(R(w):=\frac{1}{2})\sum_{j=1}^{m}w_j^2=||w||_2^2/)。)</target>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">L2惩罚(正则化项)参数。</target>
        </trans-unit>
        <trans-unit id="512ddf6d4bbcf9517a6def433a9acfdaefb1e3cd" translate="yes" xml:space="preserve">
          <source>LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This reduces the log posterior to:</source>
          <target state="translated">LDA是QDA的一种特殊情况,其中假设每个类的高斯数共享相同的协方差矩阵。\对所有(k)类的高斯数(Sigma_k=Sigma/)。这就将对数后验降低为:</target>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR &amp;ndash;大型线性分类的库</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR:大型线性分类库</target>
        </trans-unit>
        <trans-unit id="919f2c891fd7b6ae4005ef3cab68511f9b26c031" translate="yes" xml:space="preserve">
          <source>LIBSVM: A Library for Support Vector Machines</source>
          <target state="translated">LIBSVM:支持向量机库</target>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">LSA也被称为潜在语义索引,LSI,不过严格来说,这是指它在持久性索引中用于信息检索的目的。</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">LSH森林是一种近似的方法,一些来自索引数据集的真实邻居可能会从结果中丢失。</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">LSH森林。Locality Sensitive Hashing forest[1]是香草近似最近邻搜索方法的一种替代方法。LSH森林的数据结构已经使用排序数组和二进制搜索以及32位固定长度哈希来实现。随机投影被用作近似余弦距离的哈希族。</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">LSTAT:人口中地位较低的百分比</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">标签传播</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">标签传播数字主动学习</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">标签传播位数。演示性能</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">标签传播学习复杂结构</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">通过转导分配给每个项目的标签。</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">标签被认为是积极的,其他的被认为是消极的。</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">根据LOF评分和污染参数,标签为1表示不入流,-1表示离群。</target>
        </trans-unit>
        <trans-unit id="0facd2ec455a2234134eaa8ce0e172bf077ca396" translate="yes" xml:space="preserve">
          <source>Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.</source>
          <target state="translated">正向类的标签,除非y_true全为0或全为-1,否则默认为大标签。默认为大标签,除非y_true全为0或全为-1,这种情况下pos_label默认为1。</target>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">正类的标签。如果无,则以最大标签作为正类。</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">标签传播表示半监督图推理算法的一些变化。</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">标签传播模型有两种内置内核方法。内核的选择会影响算法的可扩展性和性能。有以下几种方法:</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">标签排名平均精度（LRAP）对样本取平均值，得出以下问题的答案：对于每个地面真相标签，排名靠前的标签中有多少部分是真实标签？如果您能够对与每个样本相关的标签进行更好的排名，则此性能指标将更高。获得的分数始终严格大于0，最佳值为1。如果每个样本中只有一个相关标签，则标签排名的平均精度等于&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;平均倒数排名&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">标签排名平均精度(LRAP)是指分配给每个样本的每个地面真值标签的平均值,即真值与总标签的比值,分数较低。</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">标签,它是为离群值样本(在给定半径上没有邻居的样本)提供的。如果设置为None,当检测到离群值时,会引发ValueError。</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">半监督学习的LabelSpreading模型</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">将所有类成员分配到同一簇的标签是完整的不一定是纯粹的,因此受到惩罚。</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">将所有类成员都分配到同一簇的标签是完整的不是同质的,因此受到惩罚。</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">有纯簇的标签,其成员来自相同的类,是同质的,但不必要的分裂会损害完整性,因此也会惩罚V-measure。</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">标记的数据。</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">通过寻找给定样本的最近中心点来标记一个新样本。</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">由第一个注释者指定的标签。</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">由第二个注释器分配的标签。卡伯统计信息是对称的，因此交换 &lt;code&gt;y1&lt;/code&gt; 和 &lt;code&gt;y2&lt;/code&gt; 不会更改该值。</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">在对子群进行全局聚类后,分配给子群中心点的标签。</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">与每个人脸图像相关的标签。这些标签的范围在0-39之间,对应于主体ID。</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">与每个人脸图像相关的标签。这些标签的范围在0-5748之间,与人名ID相对应。</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">与每对图像相关联的标签。两个标签值是不同的人或同一个人。</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">标签为X.</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">各点的标签</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">每个点的标签(如果compute_labels设置为True)。</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">每个点的标签。</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">标签以限制组内的置换，即，在具有相同组标识符的样本之间置换 &lt;code&gt;y&lt;/code&gt; 值。如果未指定，则在所有样本中排列 &lt;code&gt;y&lt;/code&gt; 值。</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">大的 &lt;code&gt;n_clusters&lt;/code&gt; 和 &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">大的 &lt;code&gt;n_samples&lt;/code&gt; 和 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">大数据集,去除异常值,减少数据。</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">大的离群值</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Lars只对路径中的每个结点计算一个路径解。因此,当只有少数几个结点时,它的效率非常高,如果特征或样本较少,就会出现这种情况。同时,它能够在不设置任何元参数的情况下计算出完整的路径。相反,坐标下降法是在一个预先指定的网格上计算路径点(这里我们使用默认的)。因此,如果网格点的数量小于路径中的结点数量,则效率更高。如果特征的数量真的很大,并且有足够的样本来选择大量的特征,这样的策略就会很有趣。在数值误差方面,对于重度相关的变量,Lars会积累更多的误差,而坐标下降算法只会对网格上的路径进行采样。</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">拉索和弹性网</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">稀疏信号的Lasso和弹性网络</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">Lasso和弹性网(L1和L2惩罚)采用坐标下降法实现。</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">用最小角度回归法计算的套索。</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">沿正则化路径迭代拟合的Lasso线性模型。</target>
        </trans-unit>
        <trans-unit id="47657b9ded4cc2a2b0764459c9d492e6cc3f4eb7" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path.</source>
          <target state="translated">Lasso线性模型,沿正则化路径进行迭代拟合。</target>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">Lasso模型与Lars拟合,使用BIC或AIC进行模型选择。</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">Lasso模型拟合与最小角度回归,又称。</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">Lasso模型拟合与最小角度回归,也就是Lars。</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">Lasso模型选择。交叉验证/AIC/BIC。</target>
        </trans-unit>
        <trans-unit id="5632592b831e91b94b8f3294c0115a52d64872b7" translate="yes" xml:space="preserve">
          <source>Lasso models (see the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; User Guide section) estimates sparse coefficients. LassoCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for the model estimation.</source>
          <target state="translated">套索模型（请参阅《&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;套索&lt;/a&gt;用户指南》部分）估算稀疏系数。LassoCV应用交叉验证，以确定正则化参数（ &lt;code&gt;alpha&lt;/code&gt; ）的哪个值最适合模型估计。</target>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">密集和稀疏数据上的Lasso。</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">使用LARS的套索路径</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">套索使用坐标下降（&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">Latent Dirichlet Allocation是一种用于文本体等离散数据集的生成性概率模型。它也是一种主题模型,用于从文档集合中发现抽象主题。</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">用在线变量贝叶斯算法进行潜在的Dirichlet分配</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">数据的潜在表现形式。</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">稍后您可以用以下方法将腌制好的模型加载回来 (可能在另一个 Python 进程中)。</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">稍后,您可以用以下方法重新加载腌制的模型 (可能在另一个 Python 进程中)。</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">纬度住宅小区纬度</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">叶大小传递给&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;。这会影响构造和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="90341c46ba90925b69433ce5faecb3e1b8c85d8c" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;code&gt;BallTree&lt;/code&gt; or &lt;code&gt;KDTree&lt;/code&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">叶子大小传递给 &lt;code&gt;BallTree&lt;/code&gt; 或 &lt;code&gt;KDTree&lt;/code&gt; 。这会影响构造和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">传递给BallTree或KDTree的叶子大小。这可能会影响构造和查询的速度,以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">传递给 BallTree 或 cKDTree 的叶子大小。这可能会影响构造和查询的速度,以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">学习数据X的NMF模型,并返回变换后的数据。</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">学习数据X的NMF模型。</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">了解功能名称列表-&amp;gt;索引映射并转换X。</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">了解功能名称列表-&amp;gt;索引映射。</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">学习原始文件中所有令牌的词汇词典。</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">在训练数据上学习和应用减维。</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">从X中学习经验方差。</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">用方差贝叶斯方法学习数据X的模型。</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">学习idf向量(全局项权重)</target>
        </trans-unit>
        <trans-unit id="fb7b507c119ba0834ef410110951b80a51da9b63" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights).</source>
          <target state="translated">学习idf向量(全局项权重)。</target>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">学习非计算核的反变换。(即学会寻找点的前像)</target>
        </trans-unit>
        <trans-unit id="351ef97b73f57653764681dfe2a94603d5d1cbab" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return document-term matrix.</source>
          <target state="translated">学习词汇词典并返回文档-术语矩阵。</target>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">学习词汇词典,并返回术语-文档矩阵。</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">从训练集学习词汇和idf。</target>
        </trans-unit>
        <trans-unit id="51c9b9cb5d5b207d3afa5b6e39e7500c1ff633ed" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return document-term matrix.</source>
          <target state="translated">学习词汇和idf,返回文档-术语矩阵。</target>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">学习词汇和idf,返回术语-文档矩阵。</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">学习图结构</target>
        </trans-unit>
        <trans-unit id="4ecad9f15b8037b0486e20e0cb489ddd61caeca5" translate="yes" xml:space="preserve">
          <source>Learning an embedding</source>
          <target state="translated">学习嵌入</target>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">学习和预测</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">学习曲线。</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">学习问题可分为几类。</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">权重更新的学习率时间表。</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学习率通过 &lt;code&gt;learning_rate&lt;/code&gt; 缩小每个分类器的贡献。在 &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;n_estimators&lt;/code&gt; 之间需要权衡。</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学习率通过 &lt;code&gt;learning_rate&lt;/code&gt; 缩小每个回归变量的贡献。在 &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;n_estimators&lt;/code&gt; 之间需要权衡。</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">学习预测函数的参数并在相同的数据上对其进行测试是一个方法上的错误：一个仅重复其刚刚看到的样本标签的模型将获得完美的评分，但无法预测任何有用的信息，看不见的数据。这种情况称为&lt;strong&gt;过拟合&lt;/strong&gt;。为了避免这种情况，执行（监督）机器学习实验时通常的做法是将部分可用数据保留为&lt;strong&gt;测试集&lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 。请注意，&amp;ldquo;实验&amp;rdquo;一词并非仅表示学术用途，因为即使在商业环境中，机器学习通常也是从实验开始的。</target>
        </trans-unit>
        <trans-unit id="7ed56c1456833faed4202c795166989f5307ee69" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; techniques.</source>
          <target state="translated">学习预测函数的参数并在相同数据上对其进行测试是一个方法上的错误：一个仅重复其刚刚看到的样本标签的模型将获得完美的评分，但无法预测任何有用的信息，看不见的数据。这种情况称为&lt;strong&gt;过拟合&lt;/strong&gt;。为了避免这种情况，在执行（监督）机器学习实验时，通常的做法是将部分可用数据保留为&lt;strong&gt;测试集&lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 。请注意，&amp;ldquo;实验&amp;rdquo;一词并非仅表示学术用途，因为即使在商业环境中，机器学习通常也是从实验开始的。这是模型训练中典型的交叉验证工作流程的流程图。最佳参数可以通过以下方式确定&lt;a href=&quot;grid_search#grid-search&quot;&gt;网格搜索&lt;/a&gt;技术。</target>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">独立学习每个输出,而不是连锁。</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">最小角度回归模型又称。</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">最小角度回归模型又称LAR模型</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">将数据的最小二乘投影到稀疏分量上。</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">最小绝对偏差（ &lt;code&gt;'lad'&lt;/code&gt; ）：用于回归的鲁棒损失函数。初始模型由目标值的中位数给出。</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">最小二乘（ &lt;code&gt;'ls'&lt;/code&gt; ）：回归的自然选择，因为其具有优越的计算性能。初始模型由目标值的平均值给出。</target>
        </trans-unit>
        <trans-unit id="972ad47a68ab0dd02c8d4f9c32f25ef79120c408" translate="yes" xml:space="preserve">
          <source>Least-Squares: Linear regression (Ridge or Lasso depending on \(R\)). \(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).</source>
          <target state="translated">最小二乘法。线性回归(Ridge或Lasso,取决于(R\))。\(L(y_i,f(x_i))=\frac{1}{2}(y_i-f(x_i))^2/)。</target>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">最小二乘法。山脊回归:</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">最小角度回归（&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="8427893bdf84ecb2b84085547aa1cfdd8fd0e807" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.</source>
          <target state="translated">最小角度回归(LARS)是一种针对高维数据的回归算法,由Bradley Efron、Trevor Hastie、Iain Johnstone和Robert Tibshirani开发。LARS类似于正向逐步回归。在每一步,它都会找到与目标相关度最高的特征。当有多个特征具有相同的相关性时,它不是沿着同一特征继续前进,而是沿着特征之间的等比方向前进。</target>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">最小角度回归(LARS)是一种针对高维数据的回归算法,由Bradley Efron、Trevor Hastie、Iain Johnstone和Robert Tibshirani开发。LARS类似于正向逐步回归。在每一步中,它都会找到与反应最相关的预测因子。当有多个预测因子具有相同的相关性时,它不是沿着同一个预测因子继续前进,而是沿着预测因子之间的等比方向前进。</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">撇开一组交叉验证器</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">离开P组,退出交叉验证器。</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">不提P组。</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">不提P观察。</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">漏掉一个意见。</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">留一漏一交叉验证器</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">留空交叉验证器</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O,Wolf M.Honey,I Shrunk the Sample Covariance Matrix.The Journal of Portfolio Management 30(4),110-119,2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolf是一种特殊的收缩形式，其中收缩系数是使用O.Ledoit和M.Wolf的公式计算的，如《 Ledit和Wolf，多变量分析杂志》&amp;ldquo;大范围协方差矩阵的适当条件估计器&amp;rdquo;中所述。 ，第88卷，第2期，2004年2月，第365-411页。</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Ledoit-Wolf与OAS估计数</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">LedoitWolf估算器</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">返回的内核k(X,Y)的左参数。</target>
        </trans-unit>
        <trans-unit id="db46139863e59ff060f10e61ce009637ab35e3fe" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y).</source>
          <target state="translated">返回的内核k(X,Y)的左参数。</target>
        </trans-unit>
        <trans-unit id="9bf558c9b2f1ab98bdd863d46c825f77b8bcb622" translate="yes" xml:space="preserve">
          <source>Left to right.</source>
          <target state="translated">从左到右</target>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">路径的长度。 &lt;code&gt;eps=1e-3&lt;/code&gt; 表示 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">路径的长度。 &lt;code&gt;eps=1e-3&lt;/code&gt; 表示 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="817bbec7d68f2acac91e1c4383d071e50b618e53" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters</source>
          <target state="translated">对参数数量的敏感性较低</target>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">对参数数量的敏感度较低。</target>
        </trans-unit>
        <trans-unit id="5f1b602bd58c70400c6cc3123702692e92eee6c0" translate="yes" xml:space="preserve">
          <source>Lessons learned</source>
          <target state="translated">经验教训</target>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">让K(x,z)是一个由phi(x)^T phi(z)定义的内核,其中phi是一个将x映射到希尔伯特空间的函数。KernelCenterer将数据居中(即归一化为零均值),而不需要明确计算phi(x)。它相当于用sklearn.preprocessing.StandardScaler(with_std=False)对phi(x)进行居中处理。</target>
        </trans-unit>
        <trans-unit id="821c4d001d9578c562e1b0af64f8be6da39e632f" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(\sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">假设\（S \）为相似度矩阵，而\（X \）为\（n \）输入点的坐标。差异\（\ hat {d} _ {ij} \）是对以某些最佳方式选择的相似性的转换。然后通过\（\ sum_ {i &amp;lt;j} d_ {ij}（X）-\ hat {d} _ {ij}（X）\）定义称为压力的目标。</target>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">令\（S \）为相似矩阵，\（X \）为\（n \）输入点的坐标。差异\（\ hat {d} _ {ij} \）是以某些最佳方式选择的相似性的转换。然后通过\（sum_ {i &amp;lt;j} d_ {ij}（X）-\ hat {d} _ {ij}（X）\）定义称为压力的目标。</target>
        </trans-unit>
        <trans-unit id="5ab8a1b7961470064849295cd500b6579b4fa397" translate="yes" xml:space="preserve">
          <source>Let \(X_S\) be the set of target features (i.e. the &lt;code&gt;features&lt;/code&gt; parameter) and let \(X_C\) be its complement.</source>
          <target state="translated">令\（X_S \）为目标要素的集合（即 &lt;code&gt;features&lt;/code&gt; 参数），令\（X_C \）为目标要素的补数。</target>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">让节点/(m/)的数据用/(Q/)表示。对于每一个候选拆分/(theta=(j,t_m)/)由特征/(j/)和阈值/(t_m/)组成,将数据分割成/(Q_{left}(\theta)/)和/(Q_{right}(\theta)/)子集。</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">现在让我们尝试通过对重叠区域的平均来重建原始图像的补丁。</target>
        </trans-unit>
        <trans-unit id="b5fecaca6e2e29d4dbee3904d66ce06eade247b5" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s compute the performance of this constant prediction baseline with 3 different regression metrics:</source>
          <target state="translated">让我们使用3种不同的回归指标来计算此恒定预测基准的性能：</target>
        </trans-unit>
        <trans-unit id="253db59ca1134838191e01d302a48262e9ba6374" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s consider the following trained regression model:</source>
          <target state="translated">让我们考虑以下训练有素的回归模型：</target>
        </trans-unit>
        <trans-unit id="b44b9f209d8c4175d9e9b8f1619b5304447365c9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s fit a MLPRegressor and compute single-variable partial dependence plots</source>
          <target state="translated">让我们拟合一个MLPRegressor并计算单变量偏相关图</target>
        </trans-unit>
        <trans-unit id="c338f438f84e4277f756f203f24830c86a10546d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load data from the newsgroups dataset which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).</source>
          <target state="translated">让我们从新闻组数据集中加载数据，该数据组包含关于20个主题的18000个新闻组帖子，这些帖子分为两个子集：一个用于培训（或开发），另一个用于测试（或性能评估）。</target>
        </trans-unit>
        <trans-unit id="b852fb92508c2f8d11c716ed1054a5db1d558210" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load the motor claim dataset from OpenML: &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https://www.openml.org/d/41214&lt;/a&gt;</source>
          <target state="translated">让我们从OpenML加载电机声明数据集：&lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https&lt;/a&gt; ://www.openml.org/d/41214</target>
        </trans-unit>
        <trans-unit id="ed3c266c74a01e549482a05ad99384adcdbf59a9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s make the same partial dependence plot for the 2 features interaction, this time in 3 dimensions.</source>
          <target state="translated">让我们为2个要素交互绘制相同的部分依赖图，这次是在3维中。</target>
        </trans-unit>
        <trans-unit id="2f393e9b32e94a400250ae55bd302c219802fcb3" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now compute the partial dependence plots for this neural network using the model-agnostic (brute-force) method:</source>
          <target state="translated">现在，使用与模型无关的方法（蛮力）来计算该神经网络的部分依赖图：</target>
        </trans-unit>
        <trans-unit id="c401ba9464abf901fd51a8e8666504a0d45adbfb" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now fit a GradientBoostingRegressor and compute the partial dependence plots either or one or two variables at a time.</source>
          <target state="translated">现在，让我们拟合GradientBoostingRegressor并一次计算偏倚图，或者一次或一个或两个变量。</target>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">让我们打印第一个已加载文件的第一行：</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">假设您对样本10、25和50感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">假设您对样本10、50和85感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">假设您对样本10、80和140感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="e8b31884f71e52e12b72fcb054664d8ab80d318b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation object:</source>
          <target state="translated">让我们看看它如何查找&lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;交叉验证对象：</target>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">让我们看看它如何查找 &lt;code&gt;KFold&lt;/code&gt; 交叉验证对象：</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">让我们看一下最有用的功能是：</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">让我们以以下几点为例。第一项出现在100％的时间中，因此不是很有趣。其他两个功能仅在不到50％的时间内出现，因此可能更能代表文档的内容：</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">让我们使用默认设置再试一次：</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">让我们用它来标记和计算文本文档的极简语料库中的单词出现次数：</target>
        </trans-unit>
        <trans-unit id="292ee05108cd151612c92edea6da34acc9a56662" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.</source>
          <target state="translated">让我们使用熊猫来加载泰坦尼克号数据集的副本。下面显示了如何对数字和分类特征应用单独的预处理。</target>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">让我们直观地比较许多scikit-learn交叉验证对象的交叉验证行为。下面，我们将遍历几个常见的交叉验证对象，以可视化方式查看每个对象的行为。</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">啰嗦程度。</target>
        </trans-unit>
        <trans-unit id="339e81226696b46d7377960244af7f4dcb540176" translate="yes" xml:space="preserve">
          <source>Lewis, D. D., Yang, Y., Rose, T. G., &amp;amp; Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.</source>
          <target state="translated">Lewis DD，Yang，Y.，Rose TG，＆Li，F.（2004）。RCV1：用于文本分类研究的新基准集合。机器学习研究杂志，5，361-397。</target>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="87b3d037e844d0b272ce5bfc0fb0a06e31f13827" translate="yes" xml:space="preserve">
          <source>License: BSD 3 clause</source>
          <target state="translated">授权许可:BSD 3条款</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">采用3-clause BSD授权。</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman，M.（2013年）。UCI机器学习存储库[ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]。加州尔湾市：加利福尼亚大学信息与计算机科学学院。</target>
        </trans-unit>
        <trans-unit id="e76bfa901cdd22255b64ebacf70ec6f76b7f04ab" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman，M.（2013年）。UCI机器学习存储库[ &lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt; ]。加州尔湾市：加利福尼亚大学信息与计算机科学学院。</target>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">像&lt;a href=&quot;tree#tree&quot;&gt;决策树&lt;/a&gt;一样，树林也扩展到&lt;a href=&quot;tree#tree-multioutput&quot;&gt;多输出问题&lt;/a&gt;（如果Y是大小 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 的数组）。</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">与 &lt;code&gt;Pipeline&lt;/code&gt; 一样，可以使用 &lt;code&gt;set_params&lt;/code&gt; 替换各个步骤，并通过将其设置为 &lt;code&gt;'drop'&lt;/code&gt; 来忽略它们：</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">和MultinomialNB一样,这个分类器也适用于离散数据。不同的是,MultinomialNB适用于发生次数,而BernoulliNB则是针对二进制/boolean特征设计的。</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">类似于fit(X)之后的transform(X),但不需要在内存中实现X的实体化。</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">像在Pipeline和FeatureUnion中一样，这允许使用 &lt;code&gt;set_params&lt;/code&gt; 设置变压器及其参数，并在网格搜索中进行搜索。</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">像管道一样，要素联合具有一个名为&lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt;的简写构造函数，该构造函数不需要显式命名组件。</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">与缩放器一样，&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;会将所有功能置于相同的已知范围或分布中。但是，通过执行秩变换，它可以平滑异常分布，并且与缩放方法相比，不受异常值的影响。但是，它确实扭曲了要素内部和要素之间的相关性和距离。</target>
        </trans-unit>
        <trans-unit id="73666b411bce492e14d6ba5736c0ca5eebf6eb1b" translate="yes" xml:space="preserve">
          <source>Like the Poisson GLM above, the gradient boosted trees model minimizes the Poisson deviance. However, because of a higher predictive power, it reaches lower values of Poisson deviance.</source>
          <target state="translated">和上面的泊松GLM一样,梯度提升树模型也能使泊松偏差最小化。然而,由于预测能力较高,它达到的泊松偏差值较低。</target>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">缓存大小的字节数限制。</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">限制返回的邻居的距离。(默认是传递给构造函数的值)。</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">线性判别分析</target>
        </trans-unit>
        <trans-unit id="bb7eb231c96859c6082b4ce0d3b796b4329f6e8f" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">线性判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）和二次判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）是两个经典的分类器，顾名思义，它们分别是线性决策面和二次决策面。</target>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">线性判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）和二次判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）是两个经典分类器，顾名思义，它们分别是线性决策面和二次决策面。</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">线性判别分析（LDA）试图识别出&lt;em&gt;类别之间&lt;/em&gt;差异最大的属性。尤其是，与PCA相比，LDA是使用已知类别标签的受监督方法。</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">线性判别分析和二次判别分析。</target>
        </trans-unit>
        <trans-unit id="37e8c1f7f3b5f52be8ccec8de8de9cd593660b24" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt;&lt;/a&gt; module, and Neighborhood Components Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module, are supervised dimensionality reduction method, i.e. they make use of the provided labels, contrary to other methods.</source>
          <target state="translated">监督了&lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;维方法，即来自&lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt; &lt;/a&gt;模块的线性判别分析和来自sklearn.neighbors模块的邻域分量分析，即与其他方法相反，它们使用提供的标签。</target>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">以L1先验作为正则器(又称Lasso)训练的线性模型。</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">线性回归实例</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">线性SVC在默认情况下不是概率分类器，但在此示例中启用了内置校准选项（ &lt;code&gt;probability=True&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">线性SVMs</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">线性和-一个n维向量,包含所有样本的总和。</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">线性支持向量分类（&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;）与RandomForestClassifier相比，显示出更大的S型曲线，这是最大利润率方法（比较Niculescu-Mizil和Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;）的典型方法，该方法侧重于接近决策边界的硬样本（支持向量）。</target>
        </trans-unit>
        <trans-unit id="aa48807728eba9fefe9fd82435afe3ea7ac48643" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">线性支持向量分类（&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;）与RandomForestClassifier相比，显示出甚至更多的S型曲线，这是最大利润率方法（比较Niculescu-Mizil和Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt;）的典型方法，该方法侧重于接近决策边界的硬样本（支持向量）。</target>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">线性支持向量分类。</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">线性支持向量回归。</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">带有协方差椭圆体的线性和四元判别分析方法</target>
        </trans-unit>
        <trans-unit id="f5530856b3075f323ba02b6ac9992d5aae8eee6e" translate="yes" xml:space="preserve">
          <source>Linear classifiers</source>
          <target state="translated">线性分类器</target>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">线性分类器(SVM、逻辑回归、a.o.)与SGD训练。</target>
        </trans-unit>
        <trans-unit id="9a198808a208105876aff5b3459a1743d02b9e6c" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, etc.) with SGD training.</source>
          <target state="translated">线性分类器(SVM、逻辑回归等)与SGD训练。</target>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">利用中心数据的奇异值分解进行线性降维,只保留最重要的奇异向量,将数据投射到低维空间。</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">利用数据的奇异值分解将其投射到一个较低维度的空间,进行线性降维。</target>
        </trans-unit>
        <trans-unit id="6451e9b1aa60579e3e911ed4aef57d459ed7cbf1" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">利用数据的奇异值分解将其投射到一个较低维度的空间,进行线性降维。在应用SVD之前,输入的数据是居中的,但不对每个特征进行缩放。</target>
        </trans-unit>
        <trans-unit id="c7a4096bc58f13af1ea98cce08fd73c62fb91596" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">利用数据的奇异值分解进行线性降维,只保留最重要的奇异向量,将数据投射到低维空间。在应用SVD之前,输入数据被居中,但不对每个特征进行缩放。</target>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">线性核（ &lt;code&gt;kernel = 'linear'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">l2正则化的线性最小二乘法。</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">通过用SGD最小化正则化经验损失来拟合线性模型。</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">线性模型,用于检验众多回归者中每个回归者的个体效应。这是一个在特征选择程序中使用的评分函数,而不是一个自由的特征选择程序。</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">线性模型:从回归到稀疏性。</target>
        </trans-unit>
        <trans-unit id="0b1d2caa3dbccbb7fc7a7a3c7fac23bacc286b81" translate="yes" xml:space="preserve">
          <source>Linear models with regularization</source>
          <target state="translated">正规化的线性模型</target>
        </trans-unit>
        <trans-unit id="46d0fcb8f937066fa349aa94a3c710921424dd9b" translate="yes" xml:space="preserve">
          <source>Linear models with sparse coefficients</source>
          <target state="translated">稀疏系数的线性模型</target>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">线性模型。\(y=Xbeta+epsilon)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">线性回归</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">线性回归模型,对离群值是稳健的。</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">以L1和L2组合前导为正则化的线性回归。</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">线性山脊回归。</target>
        </trans-unit>
        <trans-unit id="1dc397a187bb8fae755995aa069f79a9d565d581" translate="yes" xml:space="preserve">
          <source>Linear support vector classification.</source>
          <target state="translated">线性支持向量分类。</target>
        </trans-unit>
        <trans-unit id="5959458e20a73276c12d61f1d64604d66daab52d" translate="yes" xml:space="preserve">
          <source>LinearRegression</source>
          <target state="translated">LinearRegression</target>
        </trans-unit>
        <trans-unit id="6ff9599a07718d87fb124205a5a88e8262436582" translate="yes" xml:space="preserve">
          <source>LinearRegression fits a linear model with coefficients w = (w1, &amp;hellip;, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.</source>
          <target state="translated">LinearRegression用系数w =（w1，&amp;hellip;，wp）拟合线性模型，以最小化数据集中观察到的目标与通过线性近似预测的目标之间的平方余数。</target>
        </trans-unit>
        <trans-unit id="daf34c391f43051e2982c2bbeba34bf1a7727132" translate="yes" xml:space="preserve">
          <source>List containing the artists for the annotation boxes making up the tree.</source>
          <target state="translated">包含组成树的注释框的艺术家的列表。</target>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">包含输入的训练-测试分割的列表。</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">（名称，类别）的列表，其中 &lt;code&gt;name&lt;/code&gt; 是作为字符串的类别名称，而 &lt;code&gt;class&lt;/code&gt; 是类别的实际类型。</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">连锁的(名称,变换)元组(实现拟合/变换)列表,按照它们连锁的顺序,最后一个对象是估计器。</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">指定要应用于数据子集的变换对象的(名称、变换器、列)元组列表。</target>
        </trans-unit>
        <trans-unit id="d6a17bcffaea2ea2f18b9842614672499d0a26c6" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">(name,transformer,columns)元组的列表,指定要应用于数据子集的变换对象。</target>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">&lt;code&gt;n_features&lt;/code&gt; 维数据点列表。每行对应一个数据点。</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">y向量中可能出现的所有类的列表。</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">用于计算模型的Alpha列表。如果 &lt;code&gt;None&lt;/code&gt; 自动设置Alpha</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">计算模型的字母列表。如果None,则自动设置Alphas。</target>
        </trans-unit>
        <trans-unit id="a5422f4e0e412f7e68186f61afde0578ebd8abd7" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically.</source>
          <target state="translated">计算模型的字母列表。如果无,则自动设置Alphas。</target>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">计算模型的字母列表。如果没有提供,则自动设置。</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">术语数组列表。</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">内置内核的列表。</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">Logistic回归模型的系数列表。如果fit_intercept设置为True，则第二维将为n_features +1，其中最后一项表示截距。对于 &lt;code&gt;multiclass='multinomial'&lt;/code&gt; ，形状为（n_classes，n_cs，n_features）或（n_classes，n_cs，n_features +1）。</target>
        </trans-unit>
        <trans-unit id="d090495d2c127f32b67f3946c4bdcba721a89fcd" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If &lt;code&gt;None&lt;/code&gt; is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">索引矩阵的标签列表。这可用于重新排序或选择标签的子集。如果 &lt;code&gt;None&lt;/code&gt; 指定， &lt;code&gt;y_true&lt;/code&gt; 或 &lt;code&gt;y_pred&lt;/code&gt; 至少出现一次的那些将按排序顺序使用。</target>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">索引矩阵的标签列表。这可用于重新排序或选择标签的子集。如果没有给出， &lt;code&gt;y_true&lt;/code&gt; 或 &lt;code&gt;y_pred&lt;/code&gt; 至少出现一次的那些将按排序顺序使用。</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">索引矩阵的标签列表。这可以用于选择标签的子集。如果为None，则使用在 &lt;code&gt;y1&lt;/code&gt; 或 &lt;code&gt;y2&lt;/code&gt; 中至少出现一次的所有标签。</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">n_features-dimensional数据点的列表。每一行对应一个数据点。</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">n_features-dimensional数据点的列表。每一行对应一个查询。</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">确保可分片的对象清单。</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">附在数据X上的样本权重列表。</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">样本清单;</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">要应用于数据的变换器对象的列表。每个元组的前半部分是变换器的名称。</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">正则化参数的值列表或指定应使用的正则化参数数量的整数。在这种情况下,参数将在1e-4和1e-4之间按对数比例选择。</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">计权类型列表以计算分数。没有表示没有加权；&amp;ldquo;线性&amp;rdquo;是指线性加权；&amp;ldquo;二次方&amp;rdquo;是指二次加权。</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">刘飞飞，丁婷，启明和周志华。&amp;ldquo;隔离林。&amp;rdquo; 数据挖掘，2008年。ICDM'08。第八届IEEE国际会议。</target>
        </trans-unit>
        <trans-unit id="ef6455b6e1e2fee9a705bb5d5a6878e2089d032f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;rsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">刘飞飞，丁婷，启明和周志华。&amp;ldquo;隔离林。&amp;rdquo; 数据挖掘，2008年。ICDM'08。第八届IEEE国际会议。</target>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">刘飞飞，丁婷，启明和周志华。&amp;ldquo;基于隔离的异常检测。&amp;rdquo; ACM数据知识发现交易（TKDD）6.1（2012）：3。</target>
        </trans-unit>
        <trans-unit id="d7cbf66ae3940637cf4feeef412f2113fd5144b3" translate="yes" xml:space="preserve">
          <source>Load Data and Train a SVC</source>
          <target state="translated">加载数据并训练SVC</target>
        </trans-unit>
        <trans-unit id="5a977facf077b843c15be4adf2b27656870bbc8b" translate="yes" xml:space="preserve">
          <source>Load Data and train model</source>
          <target state="translated">加载数据并训练模型</target>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">加载从&lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;下载的数据集</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">加载并返回波士顿房价数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">加载并返回乳腺癌威斯康星州数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">加载并返回糖尿病数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">加载并返回数字数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">加载并返回虹膜数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">加载并返回linnerud数据集(多变量回归)。</target>
        </trans-unit>
        <trans-unit id="24c66578782cdc888ab71f9a25a06214508e7234" translate="yes" xml:space="preserve">
          <source>Load and return the physical excercise linnerud dataset.</source>
          <target state="translated">加载并返回物理锻炼lininnerud数据集。</target>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">加载并返回葡萄酒数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="34956b05e013a2f3d8d5817783733a9d69ea9a29" translate="yes" xml:space="preserve">
          <source>Load data from the training set</source>
          <target state="translated">从训练集加载数据</target>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">从多个文件中加载SVMlight格式的数据集。</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">将svmlight/libsvm格式的数据集加载到稀疏的CSR矩阵中。</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">加载样本图像进行图像处理。</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">装入以类别为子文件夹名的文本文件。</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">加载20个新闻组数据集,并将其矢量化为标记计数(分类)。</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">加载加州住房数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">加载标注的野生面孔(LFW)对数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">加载标注在野外的面孔(LFW)人数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">从AT＆T（分类）中加载Olivetti人脸数据集。</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">加载RCV1多标签数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">加载覆盖类型数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="ad3fd711e27424bfdcf7677e93b2ded911f0bbc0" translate="yes" xml:space="preserve">
          <source>Load the data</source>
          <target state="translated">装入数据</target>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">从20个新闻组数据集中加载文件名和数据(分类)。</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">加载kddcup99数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">加载单个样本图像的numpy数组。</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">菲利普斯等人的物种分布数据集加载器。</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">菲利普斯等人的物种分布数据集加载器(2006年)。(2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">加载一个示例数据集</target>
        </trans-unit>
        <trans-unit id="caf6cb93de1911a37a3c4f9c173bcddd3283525a" translate="yes" xml:space="preserve">
          <source>Loading datasets, basic feature extraction and target definitions</source>
          <target state="translated">加载数据集,基本特征提取和目标定义。</target>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">从外部数据集加载</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">加载20个新闻组数据集。</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">加载 &lt;code&gt;china&lt;/code&gt; 和 &lt;code&gt;flower&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">本地线性嵌入</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">局部线性嵌入(LLE)寻求数据的低维投影,以保持局部邻域内的距离。它可以被认为是一系列的局部主成分分析,这些分析在全球范围内进行比较,以找到最佳的非线性嵌入。</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;函数或其面向对象的对等&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;执行局部线性嵌入。</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">给定X的高斯混合物的对数似然。</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">p=0或p=1时,对数损失是未定义的,所以概率被削为max(eps,min(1-eps,p))。</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">对数损失,又称逻辑损失或交叉熵损失。</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">对数损失（也称为逻辑回归损失或交叉熵损失）在概率估计中定义。它通常用于（多项式）Logistic回归和神经网络，以及期望最大化的某些变体中，并且可以用于评估分类器的概率输出（ &lt;code&gt;predict_proba&lt;/code&gt; ），而不是其离散预测。</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">概率估计的对数。</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">X中每个数据点的对数概率。</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">各类的对数概率(平滑化)。</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">当前模型下每个样本的对数似然率</target>
        </trans-unit>
        <trans-unit id="f4a66282780599e98e06ef51f0d5990ef29ec975" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model.</source>
          <target state="translated">在当前模型下,每个样本的对数似然。</target>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">对数似然率得分在左出数据上跨折。</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">训练数据Theta的对数边际似然率。</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">Log:Logistic Regression.</target>
        </trans-unit>
        <trans-unit id="710d6654ab013b68e75b864460870bdb608ee833" translate="yes" xml:space="preserve">
          <source>Log: equivalent to Logistic Regression. \(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).</source>
          <target state="translated">Log:相当于Logistic回归。\(L(y_i,f(x_i))=\log(1+\exp (-y_i f(x_i)))\)。</target>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Logistic Regression(又名logit,MaxEnt)分类器。</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">Logistic回归3类分类器</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Logistic Regression CV(又名logit,MaxEnt)分类器。</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">Logistic函数</target>
        </trans-unit>
        <trans-unit id="90723c3f54f2127002d5e8087f3fd75704debc54" translate="yes" xml:space="preserve">
          <source>Logistic regression is implemented in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net regularization.</source>
          <target state="translated">Logistic回归是在&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; 中&lt;/a&gt;实现的。此实现可以通过可选的\（\ ell_1 \），\（\ ell_2 \）或Elastic-Net正则化来拟合二进制，One-vs-Rest或多项式Lo​​gistic回归。</target>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">提出了对原始像素值的Logistic回归进行比较。该例子表明,BernoulliRBM提取的特征有助于提高分类精度。</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">内置交叉验证的Logistic回归</target>
        </trans-unit>
        <trans-unit id="96304f46c8b5deeee00fad5a320967f13f13e69f" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation.</source>
          <target state="translated">Logistic回归,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">逻辑回归尽管有其名称，但它是用于分类而非回归的线性模型。逻辑回归在文献中也称为对数回归，最大熵分类（MaxEnt）或对数线性分类器。在此模型中，使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic函数&lt;/a&gt;对描述单个试验可能结果的概率进行建模。</target>
        </trans-unit>
        <trans-unit id="8e225015f4826ba9beac040479565ae8cd20d1cd" translate="yes" xml:space="preserve">
          <source>Logistic regression.</source>
          <target state="translated">Logistic回归。</target>
        </trans-unit>
        <trans-unit id="9cb16d85d5ce6d29fbfeaf0784f0b2e2c61ca591" translate="yes" xml:space="preserve">
          <source>LogisticRegression</source>
          <target state="translated">LogisticRegression</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegression由于直接优化了对数损失,所以会返回校准好的预测。相比之下,其他方法返回的概率是有偏差的,每个方法的偏差不同。</target>
        </trans-unit>
        <trans-unit id="25965de326ab0450ae299a566621b897c09262e7" translate="yes" xml:space="preserve">
          <source>Long-awaited Generalized Linear Models with non-normal loss functions are now available. In particular, three new regressors were implemented: &lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt;&lt;code&gt;PoissonRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt;&lt;code&gt;GammaRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt;&lt;code&gt;TweedieRegressor&lt;/code&gt;&lt;/a&gt;. The Poisson regressor can be used to model positive integer counts, or relative frequencies. Read more in the &lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;User Guide&lt;/a&gt;. Additionally, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; supports a new &amp;lsquo;poisson&amp;rsquo; loss as well.</source>
          <target state="translated">具有非正态损失函数的期待已久的广义线性模型现已上市。特别是，实现了三个新的回归器：&lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt; &lt;code&gt;PoissonRegressor&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt; &lt;code&gt;GammaRegressor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt; &lt;code&gt;TweedieRegressor&lt;/code&gt; &lt;/a&gt;。泊松回归器可用于模拟正整数计数或相对频率。在《&lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;用户指南》中&lt;/a&gt;阅读更多内容。此外，&lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; 也&lt;/a&gt;支持新的&amp;ldquo;泊松&amp;rdquo;损失。</target>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">经度房屋幢经度</target>
        </trans-unit>
        <trans-unit id="900f1aa8e30fd6bc29b8e8d4cae98f421ef462cd" translate="yes" xml:space="preserve">
          <source>Looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others, like AGE, varies a lot more, several decades.</source>
          <target state="translated">看系数图来衡量特征的重要性可能会产生误导,因为其中一些特征的变化幅度很小,而另一些特征,如AGE,变化幅度更大,有几十年之久。</target>
        </trans-unit>
        <trans-unit id="65fb0d7c39bbac75b47a97a31a7e2974bcfa2753" translate="yes" xml:space="preserve">
          <source>Looking closely at the WAGE distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models such as ridge or lasso work best for a normal distribution of error).</source>
          <target state="translated">仔细观察WAGE分布,会发现它有一个长尾。为此,我们应该取其对数将其近似地变成正态分布(线性模型如脊或套索对误差的正态分布效果最好)。</target>
        </trans-unit>
        <trans-unit id="e9aa6c9b011905252d25083008c9809acc3f4160" translate="yes" xml:space="preserve">
          <source>Loss function used by the algorithm.</source>
          <target state="translated">算法使用的损失函数。</target>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">低级方法</target>
        </trans-unit>
        <trans-unit id="43b8e239b3dbfa96de18c459c0e716f889fbf1ff" translate="yes" xml:space="preserve">
          <source>Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.</source>
          <target state="translated">最低预测值的下限(最小值可能仍然更高)。如果没有设置,默认为-inf。</target>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">推理的最佳拟合度的似然性(训练数据相对于模型的似然性)的下限值。</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">EM最佳拟合度的对数似然值(训练数据相对于模型)的下限值。</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;X_train_&lt;/code&gt; 中内核的下三角Cholesky分解</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M. Bawa，T。Condie和P. Ganesan，&amp;ldquo; LSH森林：相似性搜索的自调整索引&amp;rdquo;，第14届万维网国际会议的WWW '05会议记录，651-660，2005年。</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumont等人，&lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;带有随机子窗口和多个输出随机树的快速多类图像注释&lt;/a&gt;，国际计算机视觉理论与应用会议2009</target>
        </trans-unit>
        <trans-unit id="4f0f168494bf38e0f99da8c5a97b71596101a871" translate="yes" xml:space="preserve">
          <source>M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, Journal of Machine Learning Research, Vol. 1, 2001.</source>
          <target state="translated">M.E.Tipping,Sparse Bayesian Learning and the Relevance Vector Machine,Journal of Machine Learning Research,Vol.1,2001.</target>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M.Everingham，L.Van Gool，CKI Williams，J.Winn，A.Zisserman，&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;《 Pascal视觉对象类（VOC）挑战》&lt;/a&gt;，IJCV 2010。</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">MAE输出为非负浮点数。最佳值为0.0。</target>
        </trans-unit>
        <trans-unit id="dcac13e5386ab7554d99f42d2774ea6dec7dc033" translate="yes" xml:space="preserve">
          <source>MARR</source>
          <target state="translated">MARR</target>
        </trans-unit>
        <trans-unit id="203b5e4f3efe3b38e1b9f876ab3923dffadb1fa1" translate="yes" xml:space="preserve">
          <source>MARR_Unmarried</source>
          <target state="translated">MARR_Unmarried</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV自有住房的中位数价值（以1000美元计）</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLP可以将非线性模型拟合到训练数据。 &lt;code&gt;clf.coefs_&lt;/code&gt; 包含构成模型参数的权重矩阵：</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">MLP对特征缩放很敏感。</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">MLP需要调整一些超参数,如隐藏神经元数量、层数和迭代次数。</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">MLP在两个数组上进行训练:大小为(n_samples,n_features)的数组X,存放以浮点特征向量表示的训练样本;大小为(n_samples,)的数组y,存放训练样本的目标值(类标签)。</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">MLP使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;随机梯度下降&lt;/a&gt;，&lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;或&lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS进行&lt;/a&gt;训练。随机梯度下降（SGD）使用损失函数相对于需要调整的参数的梯度来更新参数，即</target>
        </trans-unit>
        <trans-unit id="4fc94508d8948819a05a769d08877751f90a9958" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">MLP使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;随机梯度下降&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;或&lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS进行&lt;/a&gt;训练。随机梯度下降（SGD）使用损失函数相对于需要调整的参数的梯度来更新参数，即</target>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">MLP使用Backpropagation进行训练。更准确地说,它使用某种形式的梯度下降进行训练,梯度的计算使用Backpropagation。对于分类,它将交叉熵损失函数最小化,给出每个样本的概率估计矢量/(P(y|x)\)。</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">MLP根据问题类型的不同,使用不同的损失函数。分类的损失函数是Cross-Antropy,在二进制情况下,它被赋予:</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">带有隐藏层的MLP具有非凸损失函数,其中存在一个以上的局部最小值。因此不同的随机权重初始化会导致不同的验证精度。</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPClassifier 迭代训练,因为在每个时间步骤中,损失函数与模型参数的部分导数被计算出来,以更新参数。</target>
        </trans-unit>
        <trans-unit id="a1925f1c916c80accddbe48b0d0e8da75d102083" translate="yes" xml:space="preserve">
          <source>MLPRegressor</source>
          <target state="translated">MLPRegressor</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPRegressor 迭代训练,因为在每个时间步长计算损失函数相对于模型参数的部分导数以更新参数。</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">MNIST类化,采用多项逻辑+L1的方法</target>
        </trans-unit>
        <trans-unit id="fbb70ceca40f03de1a52a87cdebacfd0c2426668" translate="yes" xml:space="preserve">
          <source>MNIST classification using multinomial logistic + L1</source>
          <target state="translated">MNIST分类,采用多元逻辑+L1的方法</target>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">机器学习算法需要数据。转到每个 &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; 子文件夹，然后从那里运行 &lt;code&gt;fetch_data.py&lt;/code&gt; 脚本（首先阅读它们之后）。</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">机器学习是关于学习数据集的某些属性，然后针对另一个数据集测试这些属性。机器学习中的一种常见做法是通过将数据集分成两部分来评估算法。我们称其中一组为&lt;strong&gt;训练集&lt;/strong&gt;，在该&lt;strong&gt;训练集&lt;/strong&gt;上我们学习一些属性；我们将另一个集称为&lt;strong&gt;测试集&lt;/strong&gt;，在其上测试学习的属性。</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">机器学习:问题设置</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="ca3ae45b6eafdd0a6dfea23df60842bdb389e9fc" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">训练集（称为&lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;度）观测值的马氏距离。</target>
        </trans-unit>
        <trans-unit id="55e9152468183c8a16be469ac6ea3dbb0e42dd62" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">训练集（称为&lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;度）观测值的马氏距离。</target>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">训练集（称为 &lt;code&gt;fit&lt;/code&gt; 度）观测值的马氏距离。</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">Mahalanobis到中心的距离</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="7a412cc8631eaef465ac98b25829da66ced3b43d" translate="yes" xml:space="preserve">
          <source>Main takeaways</source>
          <target state="translated">主要收获</target>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">对输入数据进行复制。</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">用2d做一个大圆包含一个小圆。</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">从性能指标或损失函数中制作一个评分器。</target>
        </trans-unit>
        <trans-unit id="723bb825ac5b7b14f789cce44f7b667d39fe6543" translate="yes" xml:space="preserve">
          <source>Make and use a deep copy of X and Y (if Y exists)</source>
          <target state="translated">制作并使用X和Y的深层副本(如果Y存在)。</target>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">使数组可以进行交叉验证的索引。</target>
        </trans-unit>
        <trans-unit id="5f9f781fbc2f44502a26f1a8945369508ac6ebd5" translate="yes" xml:space="preserve">
          <source>Make pipeline to preprocess the data</source>
          <target state="translated">制作流水线来预处理数据</target>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">确保X在其第一轴上有一个最小的样本数(2D阵列的行)。</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">确保阵列是二维的、正方形的、对称的。</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">确保2D阵列具有最少数量的要素（列）。默认值1拒绝空数据集。仅当X有效具有2维或最初为1D并 &lt;code&gt;ensure_2d&lt;/code&gt; 为True 时，才强制执行此检查。设置为0将禁用此检查。</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">确保2D阵列具有最少数量的要素（列）。默认值1拒绝空数据集。仅当输入数据有效地具有2维或原始为1D且 &lt;code&gt;ensure_2d&lt;/code&gt; 为True 时，才强制执行此检查。设置为0将禁用此检查。</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">确保阵列在其第一轴上有一个最小的样本数(2D阵列的行)。设置为0将禁用这项检查。</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">确保在所有功能上使用相同的比例。由于流形学习方法是基于最近邻居搜索的，因此该算法的执行效果可能不佳。有关缩放异构数据的便捷方法，请参见&lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bd58d70c4390b61fe411e7823adf47cfb5052e9e" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; (see &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt;).</source>
          <target state="translated">确保在拟合模型之前对训练数据进行置换（混洗），或在每次迭代后使用 &lt;code&gt;shuffle=True&lt;/code&gt; 进行混洗（默认情况下使用）。同样，理想情况下，应使用 &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; 功能进行标准化（请参见&lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">确保在拟合模型之前对训练数据进行置换（混洗），或在每次迭代后使用 &lt;code&gt;shuffle=True&lt;/code&gt; 进行混洗。</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">做两个交错的半圆</target>
        </trans-unit>
        <trans-unit id="69015b0f2ce90a52d27e40a08b160550cfb23a90" translate="yes" xml:space="preserve">
          <source>Making predictions</source>
          <target state="translated">进行预测</target>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">苹果酸。</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">苹果酸</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">Manifold Learning可以被认为是对PCA等线性框架进行泛化的尝试,使其对数据中的非线性结构敏感。虽然存在监督变体,但典型的歧路学习问题是无监督的:它从数据本身学习数据的高维结构,而不使用预定的分类。</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">割裂球体上的Manifold学习方法。</target>
        </trans-unit>
        <trans-unit id="5f7bca3c10846eb5854c536c3448fedf998ffbcf" translate="yes" xml:space="preserve">
          <source>Manifold learning</source>
          <target state="translated">歧管学习</target>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">Manifold学习是一种非线性降维的方法。这个任务的算法是基于许多数据集的维度只是人为的高的想法。</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">手写数字上的流形学习：局部线性嵌入，Isomap&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="ad97dd09eb7e528a0b4debe1b9c745e650b7307a" translate="yes" xml:space="preserve">
          <source>Manually setting one of the environment variables (&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, or &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt;) will take precedence over what joblib tries to do. The total number of threads will be &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Note that setting this limit will also impact your computations in the main process, which will only use &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below).</source>
          <target state="translated">手动设置环境变量之一（ &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; ， &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt; ， &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt; 或 &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; ）将优先于joblib尝试执行的操作。线程总数将为 &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; 。请注意，设置此限制还将影响您在主过程中的计算，该过程仅使用 &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; 。 Joblib公开了一个上下文管理器，以更好地控制其工作线程中的线程数（请参阅下面链接的joblib文档）。</target>
        </trans-unit>
        <trans-unit id="0471386edfe0bf057e3ed471b66689189fdc892c" translate="yes" xml:space="preserve">
          <source>Manufacturing</source>
          <target state="translated">Manufacturing</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">许多应用需要能够决定一个新的观察是否属于相同的分布与现有观测（它是一个&lt;em&gt;内点&lt;/em&gt;），或（这是一个应该被视为不同的&lt;em&gt;异常值&lt;/em&gt;）。通常，此功能用于清除实际数据集。必须做出两个重要的区分：</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">许多集群,可能是连接性限制</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">许多集群,可能是连通性的限制,非欧氏距离</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">群落多,群落大小不均,几何形状不平坦。</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">许多数据集包含不同类型的要素，例如文本，浮点数和日期，其中每种类型的要素都需要单独的预处理或要素提取步骤。通常，在应用scikit-learn方法之前，例如使用&lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;，对数据进行预处理最容易。由于以下原因之一，在将数据传递给scikit-learn之前进行处理可能会出现问题：</target>
        </trans-unit>
        <trans-unit id="d589144c9193e129ab4721a317a3dc3eaa909106" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">许多数据集包含不同类型的要素，例如文本，浮点数和日期，其中每种类型的要素都需要单独的预处理或要素提取步骤。通常，在应用scikit-learn方法之前，例如使用&lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;，对数据进行预处理最容易。由于以下原因之一，在将数据传递给scikit-learn之前对其进行处理可能会出现问题：</target>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">许多度量未指定名称用作 &lt;code&gt;scoring&lt;/code&gt; 值，有时是因为它们需要附加参数，例如&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;。在这种情况下，您需要生成一个适当的计分对象。生成用于评分的可调用对象的最简单方法是使用&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;。该函数将指标转换为可用于模型评估的可调用对象。</target>
        </trans-unit>
        <trans-unit id="c6eff6cfb2b81378a219da9247d45ca528d18b55" translate="yes" xml:space="preserve">
          <source>Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt;&lt;code&gt;KNeighborsRegressor&lt;/code&gt;&lt;/a&gt;, but also some clustering methods such as &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;, and some manifold embeddings such as &lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt;&lt;code&gt;TSNE&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;Isomap&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">许多scikit-learn估计器都依赖于最近的邻居：一些分类器和回归器（例如&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt; &lt;code&gt;KNeighborsRegressor&lt;/code&gt; )&lt;/a&gt;，还有一些聚类方法（例如&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;SpectralClustering&lt;/code&gt; &lt;/a&gt;）以及一些流形嵌入（例如&lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt; &lt;code&gt;TSNE&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;Isomap&lt;/code&gt; )&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3a89fb6cdb80688c2fbf1190407f9abec89dd4f3" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">许多统计问题都需要估计总体的协方差矩阵，这可以看作是对数据集散点图形状的估计。在大多数情况下，必须对其属性（大小，结构，同质性）对估计质量有很大影响的样本进行这种估计。所述&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;包提供用于在各种设置精确地估计群体的协方差矩阵的工具。</target>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">许多统计问题都需要估计总体的协方差矩阵，这可以看作是对数据集散点图形状的估计。在大多数情况下，必须对其属性（大小，结构，同质性）对估计质量有很大影响的样本进行这种估计。所述 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包提供用于在各种设置精确地估计群体的协方差矩阵的工具。</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">很多很多&amp;hellip;&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">将数据映射到正态分布</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">使用功率转换将数据映射到正态分布。</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">使用参数 &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; 将数据映射到标准正态分布。</target>
        </trans-unit>
        <trans-unit id="12f61571505f27a87deaf94928434363c3add704" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution='normal'&lt;/code&gt;.</source>
          <target state="translated">使用参数 &lt;code&gt;output_distribution='normal'&lt;/code&gt; 将数据映射到标准正态分布。</target>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">将每个分类特征名称映射到一个值列表中,使编码为i的值是列表中的第i个。</target>
        </trans-unit>
        <trans-unit id="67b185421bf5d928c6a7d5bf74ee20d677f50228" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list. If &lt;code&gt;as_frame&lt;/code&gt; is True, this is None.</source>
          <target state="translated">将每个分类要素名称映射到值列表，以使编码为i的值在列表中成为第i个。如果 &lt;code&gt;as_frame&lt;/code&gt; 为True，则为None。</target>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">转换后数据的边际分布。选择为&amp;ldquo;统一&amp;rdquo;（默认）或&amp;ldquo;正常&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">马克&amp;middot;施密特（Mark Schmidt），尼古拉斯&amp;middot;勒&amp;middot;鲁（Nicolas Le Roux）和弗朗西斯&amp;middot;巴赫（Francis Bach）：&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;通过随机平均梯度最小化有限和。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c75a2b42a0ab364e58b54559c45fcda50d1973f4" translate="yes" xml:space="preserve">
          <source>Married</source>
          <target state="translated">Married</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">掩码要用在X上。</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">产核。</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">数学上,它由一个线性模型组成,该模型以正则器为前导训练。要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型由混合的先验和先验作为正则器进行训练。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型以混合的前值作为正则器进行训练。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="85bbf115a5892290f61e157be8a21f18c7185291" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm and \(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型由一个混合的正则化的(1)正则和(2)正则组成。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="a22bf31bc3080adb481367d05d21298b6681e5ee" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型采用混合的正则化准则进行训练。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="47b46d2e2bec7b475ce40ef0c30d61c9f27a62f1" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个添加了正则化项的线性模型组成。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">从数学上讲,这种收缩包括减少经验协方差矩阵的最小和最大特征值之间的比值。它可以通过简单地根据给定的偏移量移动每一个特征值来实现,这相当于找到协方差矩阵的l2-penalized Maximum Likelihood Estimator。在实践中,收缩归结为一个简单的一个凸变换。\(\Sigma_{\rm shrunk}=(1-\alpha)hat{Sigma}+\alphafrac{{\rm Tr}hat{Sigma}}{p}rm Id\)。</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">数学上,截断SVD应用于训练样本(X/)产生一个低阶近似(X/)。</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">矩阵\（C \）使得\（C_ {i，j} \）是真实类\（i \）和预测类\（j \）中的样本数。如果 &lt;code&gt;eps is None&lt;/code&gt; ，则此数组的dtype将为整数。如果给出了 &lt;code&gt;eps&lt;/code&gt; ，则dtype将为float。如果 &lt;code&gt;sparse=True&lt;/code&gt; ,将是 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">点间相似性矩阵</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">矩阵要缩放。</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">矩阵分解</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">使用特征的方差进行标准化的矩阵。</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">矩阵的两列要交换。</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">两行要交换的矩阵。</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">电子步骤中更新文档主题分布的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">最大化ELBO相当于最小化/(q(z,/theta,/beta)/)和真实后验/(p(z,/theta,/beta |w,/alpha,/eta)/)之间的Kullback-Leibler(KL)分歧。</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">减去目标均值后，最大化对数边际似然率将产生以下内核，其LML为-83.214：</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">由Johnson-Lindenstrauss定理定义的最大失真率。如果给定一个数组,它将以数组的方式计算一个安全的分量数。</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">最大似然协方差估计器</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">残差的最大规范。如果不是None,则覆盖n_nonzero_coefs。</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">每个节点中CF子簇的最大数量。如果有新的样本进入,使子簇的数量超过了 branching_factor,那么该节点就会被分割成两个节点,每个节点中的子簇都会被重新分配。该节点的父子簇被移除,并增加两个新的子簇作为2个分裂节点的父子簇。</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">时代的最大数目不能满足 &lt;code&gt;tol&lt;/code&gt; 改善。仅在Solver ='sgd'或'adam'时有效</target>
        </trans-unit>
        <trans-unit id="7d6dde474581154e61a4c14f0f2c50aef90aa524" translate="yes" xml:space="preserve">
          <source>Maximum number of imputation rounds to perform before returning the imputations computed during the final round. A round is a single imputation of each feature with missing values. The stopping criterion is met once &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt; tol, where &lt;code&gt;X_t&lt;/code&gt; is &lt;code&gt;X&lt;/code&gt; at iteration &lt;code&gt;t. Note that early stopping is only
applied if ``sample_posterior=False`&lt;/code&gt;.</source>
          <target state="translated">返回最后一轮计算出的插补之前要执行的插补回合的最大数量。回合是缺少值的每个要素的单一估算。一旦 &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt;tol满足停止条件，其中 &lt;code&gt;X_t&lt;/code&gt; 在迭代 &lt;code&gt;t. Note that early stopping is only applied if ``sample_posterior=False`&lt;/code&gt; 处为 &lt;code&gt;X&lt;/code&gt; 。请注意，仅在``sample_posterior = False''时才应用提前停止。</target>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">最大迭代次数</target>
        </trans-unit>
        <trans-unit id="ec0c7c7cfd1a4dd46776f2839b3e2ee30db0ceb0" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations allowed.</source>
          <target state="translated">允许的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">计时结束前的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">拟合期间的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">arpack的最大迭代次数。如果为 &quot;无&quot;,则由arpack选择最佳值。</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。对于&amp;ldquo; sparse_cg&amp;rdquo;和&amp;ldquo; lsqr&amp;rdquo;求解器，默认值由scipy.sparse.linalg确定。对于&amp;ldquo;下垂&amp;rdquo;求解器，默认值为1000。</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。对于&amp;ldquo; sparse_cg&amp;rdquo;和&amp;ldquo; lsqr&amp;rdquo;求解器，默认值由scipy.sparse.linalg确定。对于&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; saga求解器&amp;rdquo;，默认值为1000。</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。默认值由scipy.sparse.linalg决定。</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">随机样本选择的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">arpack求解器的最大迭代次数。如果eigen_solver =='dense'，则不使用。</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">计算空间中位数的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">优化的最大迭代次数。应至少为250次。</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">求解器的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="23888143df53d4407b7d5db42e819ad8fc40bfb6" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations in the optimization.</source>
          <target state="translated">优化过程中的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">SMACOF算法单次运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">k-means算法单次运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">k-means算法运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">优化算法的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">在停止之前,独立于任何早期停止标准的启发式方法,对完整数据集的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="a1da02f682252ffa124e7544bc23e0152d7ce8c8" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations performed on each seed.</source>
          <target state="translated">对每个种子执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="ff75c0f1937b00a0d18d3456b10469179a13bfd5" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations run across all classes.</source>
          <target state="translated">所有类的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="5d873ede9710528d1162698e1b4121133119149c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">求解器收敛的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="0f2d5ee22794f6faeed2ed56167e69832301d9fb" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that &lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; should run for.</source>
          <target state="translated">应当运行 &lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; 最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">迭代，可以是最大数量跳过由于发现零个内围层或由下式定义的无效数据 &lt;code&gt;is_data_valid&lt;/code&gt; 由下式定义的或无效的模型 &lt;code&gt;is_model_valid&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">scipy.optimization.fmin_l_bfgs_b应该运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="f8b9c126c90c88245cf150185cad9a95bb9ec437" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; or &lt;code&gt;lasso_lars&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; 或 &lt;code&gt;lasso_lars&lt;/code&gt; ,则要执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">在Lars算法中执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="67f387c33c051b181969f68bc7a00e313e621f7a" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform when solving the lasso problem.</source>
          <target state="translated">求解套索问题时的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">执行的最大迭代次数,无限制时设置为无穷大。</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">执行的最大迭代次数。可用于提前停止。</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">在我们中止优化之前没有进展的最大迭代次数,在250次初始迭代后使用,早期夸大。注意,每50次迭代才检查一次进度,所以这个值四舍五入到50的下一个倍数。</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">如果尚未收敛,则在聚类操作终止之前,每个种子点的最大迭代次数(对于该种子点)。</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">最大迭代次数。默认为300次</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">最大迭代次数。默认为300次。</target>
        </trans-unit>
        <trans-unit id="78adce28aba1dcec634d8308251161f963f008c1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Should be greater than or equal to 1.</source>
          <target state="translated">最大迭代次数。应大于或等于1。</target>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">最大迭代次数。求解器迭代直到收敛（由&amp;ldquo; tol&amp;rdquo;确定）或此迭代次数。对于随机求解器（&amp;ldquo; sgd&amp;rdquo;，&amp;ldquo; adam&amp;rdquo;），请注意，这确定了时期数（每个数据点将使用多少次），而不是梯度步数。</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">为了计算效率,用于估计量子数的最大样本数。请注意,对于数值相同的稀疏矩阵和密集矩阵,子抽样程序可能有所不同。</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">要执行的最大迭代次数，因此要包含的最大功能。 &lt;code&gt;n_features&lt;/code&gt; 的 10％，但至少5个（如果有）。</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 是 &lt;code&gt;max_iter&lt;/code&gt; ， &lt;code&gt;n_features&lt;/code&gt; 或 &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; 的路径中的节点数，以较小者为准。</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 可以是 &lt;code&gt;max_iter&lt;/code&gt; ， &lt;code&gt;n_features&lt;/code&gt; 或相关性大于 &lt;code&gt;alpha&lt;/code&gt; 的路径中的节点数（以较小者为准）。</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 是 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 或 &lt;code&gt;n_features&lt;/code&gt; 中的较小者。</target>
        </trans-unit>
        <trans-unit id="2519f5f4a0d8bcad0dcee25fb3c2cfe0d8efda04" translate="yes" xml:space="preserve">
          <source>Maximum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one max value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to np.inf.</source>
          <target state="translated">可能的最大估算值。广播以整形（n_features）为标量。如果为阵列状，则期望形状（n_features，），每个要素一个最大值。 &lt;code&gt;None&lt;/code&gt; （默认）将转换为np.inf。</target>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">数据样本的最大残差被归类为一个正常值。默认情况下，将阈值选择为目标值 &lt;code&gt;y&lt;/code&gt; 的MAD（中值绝对偏差）。</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">单个训练集的最大尺寸。</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">X对样本的最大平方和。仅在SAG求解器中使用。如果无,将通过所有样本进行计算。该值应该预先计算,以加快交叉验证。</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">最大步长(正则化)。默认值为1.0。</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">双集群的最大值。</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">右边界的输入数组 &lt;code&gt;X_&lt;/code&gt; 的最大值。</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">可以是使用&amp;ldquo; Jaccard系数&amp;rdquo;的字符串&amp;ldquo; jaccard&amp;rdquo;，也可以是任何带有四个参数的函数，每个参数都是一个一维指示符向量：（a_rows，a_columns，b_rows，b_columns）。</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">可以包含（&amp;ldquo;页眉&amp;rdquo;，&amp;ldquo;页脚&amp;rdquo;，&amp;ldquo;引号&amp;rdquo;）的任何子集。这些文本中的每一种都是将被检测到并从新闻组帖子中删除的文本类型，从而防止分类器过度适合元数据。</target>
        </trans-unit>
        <trans-unit id="5b986034f147ad60f742da340ca1bd20b5f08ce7" translate="yes" xml:space="preserve">
          <source>McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.</source>
          <target state="translated">McCullagh,Peter;Nelder,John(1989年)。通用线性模型,第二版。Boca Raton。Chapman and Hall/CRC。ISBN 0-412-31760-5。</target>
        </trans-unit>
        <trans-unit id="72639b42074abb6c8ea20684337c75e653e13eb2" translate="yes" xml:space="preserve">
          <source>McSherry, F., &amp;amp; Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.</source>
          <target state="translated">McSherry，F.和Najork，M.（2008年3月）。计算信息检索性能可有效地衡量存在得分的情况。在欧洲信息检索会议上（pp。414-421）。施普林格，柏林，海德堡。</target>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">平均绝对误差。</target>
        </trans-unit>
        <trans-unit id="90a417c7a65441a9ebd4508d554460a1437266a0" translate="yes" xml:space="preserve">
          <source>Mean Gamma deviance regression loss.</source>
          <target state="translated">平均Gamma偏差回归损失。</target>
        </trans-unit>
        <trans-unit id="b5e71e9559d855f0bb974ab566c48b140de9b95a" translate="yes" xml:space="preserve">
          <source>Mean Poisson deviance regression loss.</source>
          <target state="translated">平均泊松偏差回归损失。</target>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">所有样品的平均轮廓系数。</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">平均平方误差。</target>
        </trans-unit>
        <trans-unit id="04bada6d1e51be3ec0e69f413da51c5f62d2f9fc" translate="yes" xml:space="preserve">
          <source>Mean Tweedie deviance regression loss.</source>
          <target state="translated">平均特威迪偏差回归损失。</target>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">平均绝对误差回归损失</target>
        </trans-unit>
        <trans-unit id="640f16564a014e166473dc43ba616e7e2ca59c7f" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) w.r.t. y.</source>
          <target state="translated">self.predict(X)w.r.t.y的平均精度。</target>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict(X)wrt.y的平均精度。</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">最佳估计器的平均交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">Best_estimator的平均交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="905638cd4671af1b6b218d93d25fb805a7548993" translate="yes" xml:space="preserve">
          <source>Mean of feature importance over &lt;code&gt;n_repeats&lt;/code&gt;.</source>
          <target state="translated">特征重要性超过 &lt;code&gt;n_repeats&lt;/code&gt; 的平均值。</target>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">预测分布的平均值a查询点</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">查询点预测分布的平均值。</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">训练目标的平均值或中位数或分位数或用户给出的定值。</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">均值漂移聚类旨在发现平滑密度的样本中的&amp;ldquo;斑点&amp;rdquo;。这是基于质心的算法，它通过将质心的候选更新为给定区域内点的均值来工作。然后在后处理阶段对这些候选对象进行过滤，以消除几乎重复的部分，从而形成最终的形心集。</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">使用平核进行平均移位聚类。</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">每个折线上测试集的平均平方误差,改变l1_ratio和alpha。</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">平均平方误差回归损失</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">平均平方对数误差回归损失</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="896bb25ed00af769d9d9cdb21af7655bd0a22654" translate="yes" xml:space="preserve">
          <source>Measure and plot the results</source>
          <target state="translated">测量和绘制结果</target>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">测量一组点的两个聚类的相似度。</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">X中的测量误差</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">测量误差(y)</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">MedInc 整體收入中位數</target>
        </trans-unit>
        <trans-unit id="ca6bd4b635d61f1c13fd0394e55000bb30738881" translate="yes" xml:space="preserve">
          <source>Median absolute error output is non-negative floating point. The best value is 0.0. Read more in the &lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">中位数绝对错误输出是非负浮点数。最佳值为0.0。在《&lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;用户指南》中&lt;/a&gt;阅读更多内容。</target>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">绝对误差中位数回归损失</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">中等 &lt;code&gt;n_samples&lt;/code&gt; ，小的 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">成员 &lt;code&gt;coef_&lt;/code&gt; 持有权重\（w \）</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">成员 &lt;code&gt;intercept_&lt;/code&gt; 持有\（b \）</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">成员 &lt;code&gt;intercept_&lt;/code&gt; 持有截距（又名偏移量或偏差）：</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">传递给worker的numpy数组的映射模式。有关更多详细信息，请参见'max_nbytes'参数文档。</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">大样本量的内存消耗</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">元估计器对变换后的目标进行回归。</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">用于建立带有变压器的复合模型的元估计器。</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">基于重要性权重选择特征的元变换器。</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="bebb8fb7cc6768e9928f39fcd0184089c7f19d03" translate="yes" xml:space="preserve">
          <source>Method for initialization</source>
          <target state="translated">初始化方法</target>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">一种k均值算法的初始化方法；默认为'k-means ++'。</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初始化方法，默认为'k-means ++'：</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初始化方法，默认为'k-means ++'：</target>
        </trans-unit>
        <trans-unit id="0e3feb124243dbfe777425d1e7de652a8a95432b" translate="yes" xml:space="preserve">
          <source>Method for initialization:</source>
          <target state="translated">用于初始化的方法。</target>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">方法名称</target>
        </trans-unit>
        <trans-unit id="f2ddbb16b4269f001b2886168e6ee73674985a74" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;.</source>
          <target state="translated">将奇异向量归一化并将其转换为双簇的方法。可以是&amp;ldquo;比例&amp;rdquo;，&amp;ldquo;双随机&amp;rdquo;或&amp;ldquo;对数&amp;rdquo;之一。作者建议使用&amp;ldquo;日志&amp;rdquo;。但是，如果数据稀疏，则日志规范化将不起作用，这就是为什么默认值为&amp;ldquo;双随机&amp;rdquo;的原因。</target>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">将奇异向量归一化并将其转换为双簇的方法。可以是&amp;ldquo;比例&amp;rdquo;，&amp;ldquo;双随机&amp;rdquo;或&amp;ldquo;对数&amp;rdquo;之一。作者建议使用&amp;ldquo;日志&amp;rdquo;。但是，如果数据稀疏，则日志规范化将不起作用，这就是默认值为&amp;ldquo;双随机&amp;rdquo;的原因。注意：如果 &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; ，则数据不能稀疏。</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">用来寻找最短路径的方法。</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">用于对转换结果进行编码的方法。</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">用于初始化过程的方法。默认值：如果n_components &amp;lt;n_features，则为'nndsvd'，否则为随机数。有效选项：</target>
        </trans-unit>
        <trans-unit id="2babbe784e2e75bf6bc5c5da588a447b4ed201c9" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None.</source>
          <target state="translated">用于初始化过程的方法。默认值。缺省:无。</target>
        </trans-unit>
        <trans-unit id="7506fd4c85f1f80b13ff77585a3eea447916c5af" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None. Valid options:</source>
          <target state="translated">用于初始化过程的方法。默认值。无。有效选项:</target>
        </trans-unit>
        <trans-unit id="6e9cbcdd1d5058381751f008677bca0c5dd1dd9b" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">用于更新 &lt;code&gt;_component&lt;/code&gt; 的方法。仅用于&lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt;方法。通常，如果数据量很大，则联机更新将比批处理更新快得多。</target>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">用于更新 &lt;code&gt;_component&lt;/code&gt; 的方法。仅用于 &lt;code&gt;fit&lt;/code&gt; 方法。通常，如果数据量很大，则联机更新将比批处理更新快得多。</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="8de2b023f4bb12cf6f4720283ae55f1dda2214ee" translate="yes" xml:space="preserve">
          <source>Methods called for each base estimator. It can be:</source>
          <target state="translated">为每个基础估计器调用的方法。它可以是:</target>
        </trans-unit>
        <trans-unit id="66a03315f429c2912a3083f4347509dcbe2b4de3" translate="yes" xml:space="preserve">
          <source>Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.</source>
          <target state="translated">用于距离计算的度量。可以使用 scikit-learn 或 scipy.spatial.distance 中的任何指标。</target>
        </trans-unit>
        <trans-unit id="a01ea489bdc9c9d6a182edc027242ff94374f848" translate="yes" xml:space="preserve">
          <source>Metric used to compute distances to neighbors.</source>
          <target state="translated">用于计算与邻居的距离的度量。</target>
        </trans-unit>
        <trans-unit id="223dc067df313c03c1e792b17b98d4c951add488" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;ldquo;precomputed&amp;rdquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted. If &amp;ldquo;precomputed&amp;rdquo;, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.</source>
          <target state="translated">用于计算链接的度量。可以是&amp;ldquo;欧几里得&amp;rdquo;，&amp;ldquo; l1&amp;rdquo;，&amp;ldquo; l2&amp;rdquo;，&amp;ldquo;曼哈顿&amp;rdquo;，&amp;ldquo;余弦&amp;rdquo;或&amp;ldquo;预先计算&amp;rdquo;。如果链接为&amp;ldquo;病房&amp;rdquo;，则仅接受&amp;ldquo;欧几里得&amp;rdquo;。如果&amp;ldquo;预先计算&amp;rdquo;，则需要距离矩阵（而不是相似度矩阵）作为拟合方法的输入。</target>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">用于计算链接的度量。可以是&amp;ldquo;欧几里得&amp;rdquo;，&amp;ldquo; l1&amp;rdquo;，&amp;ldquo; l2&amp;rdquo;，&amp;ldquo;曼哈顿&amp;rdquo;，&amp;ldquo;余弦&amp;rdquo;或&amp;ldquo;预计算&amp;rdquo;。如果链接为&amp;ldquo;病房&amp;rdquo;，则仅接受&amp;ldquo;欧几里得&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">各种机器学习任务的可用指标在下面的章节中详细介绍。</target>
        </trans-unit>
        <trans-unit id="6333551e93ef2383df0508df89bd1ca423b74bc9" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping, &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;, 2001.</source>
          <target state="translated">Michael E. Tipping，&lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;稀疏贝叶斯学习和相关向量机&lt;/a&gt;，2001年。</target>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">Michael E. Tipping：&lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;稀疏贝叶斯学习和相关向量机&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">迷你批量K-Means聚类法</target>
        </trans-unit>
        <trans-unit id="f81af70401b9fbaba1cc8f3d806f31408afbc851" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering.</source>
          <target state="translated">迷你批量K-Means聚类。</target>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">小批量稀疏主成分分析</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">小批量字典学习</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">小批量稀疏PCA（&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt;）是的一个变种&lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt;是快，但不准确。对于给定的迭代次数，通过迭代功能部件的小块可以达到提高的速度。</target>
        </trans-unit>
        <trans-unit id="acc629f9bc13af6fe4ccc30d47949b9a29f4708a" translate="yes" xml:space="preserve">
          <source>Minimal cost complexity pruning recursively finds the node with the &amp;ldquo;weakest link&amp;rdquo;. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. To get an idea of what values of &lt;code&gt;ccp_alpha&lt;/code&gt; could be appropriate, scikit-learn provides &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt;&lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt;&lt;/a&gt; that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.</source>
          <target state="translated">最小的成本复杂度修剪可递归地找到具有&amp;ldquo;最弱链接&amp;rdquo;的节点。最弱的链接以有效Alpha为特征，其中有效Alpha最小的节点首先被修剪。为了了解 &lt;code&gt;ccp_alpha&lt;/code&gt; 的合适值，scikit-learn提供了&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt; &lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt; &lt;/a&gt;，它在修剪过程的每个步骤都返回有效的alpha值和相应的总叶杂质。随着alpha的增加，会修剪更多的树，这会增加其叶子的总杂质。</target>
        </trans-unit>
        <trans-unit id="8f8dae4007afe0299bfaa99f8bc74594de701fdf" translate="yes" xml:space="preserve">
          <source>Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of &lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]&lt;/a&gt;. This algorithm is parameterized by \(\alpha\ge0\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of a given tree \(T\):</source>
          <target state="translated">最小成本复杂性修剪是用于修剪树以避免过度拟合的算法，在&lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]的&lt;/a&gt;第3章中进行了描述。该算法由称为复杂性参数的\（\ alpha \ ge0 \）参数化。复杂度参数用于定义给定树\（T \）的成本复杂度度量\（R_ \ alpha（T）\）：</target>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">使目标函数最小化。</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">用随机平均梯度最小化有限求和&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">最小协方差导数(MCD):协方差的鲁棒估计器。</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">最小协方差决定因素估计器</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">沿着路径的最小相关度。它对应于Lasso中的正则化参数alpha参数。</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">假设有足够的项目满足 &lt;code&gt;min_hash_match&lt;/code&gt; 约束，则每个估算器评估的最小候选数。</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">从原始数据中随机选择的最小样本数。当作用于样本的绝对数量 &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; ，作为相对数处理 &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; ），用于 &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; 。通常将其选择为估计给定 &lt;code&gt;base_estimator&lt;/code&gt; 所需的最少样本数。默认情况下，假定 &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; 估计器，并将 &lt;code&gt;min_samples&lt;/code&gt; 选择为 &lt;code&gt;X.shape[1] + 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7055b8e6fba3c22d096f09a773f8fa2a0f2c2a45" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead.</source>
          <target state="translated">OPTICS群集中的最小样本数，表示为绝对数或样本数的分数（四舍五入至少为2）。如果为 &lt;code&gt;None&lt;/code&gt; ，则使用 &lt;code&gt;min_samples&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="143330e48296c9730a85d5042f3f4108bc450e1a" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead. Used only when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">OPTICS群集中的最小样本数，表示为绝对数或样本数的分数（四舍五入至少为2）。如果为 &lt;code&gt;None&lt;/code&gt; ，则使用 &lt;code&gt;min_samples&lt;/code&gt; 的值。仅在 &lt;code&gt;cluster_method='xi'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1c28e0e254bdc137eed30061d47cd3e6720cfe75" translate="yes" xml:space="preserve">
          <source>Minimum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one min value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to -np.inf.</source>
          <target state="translated">最小可能的估算值。广播以整形（n_features）为标量。如果为数组状，则期望形状（n_features，），每个要素的最小值为1。 &lt;code&gt;None&lt;/code&gt; （默认）将转换为-np.inf。</target>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">双集群的最小值。</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">左边界的输入数组 &lt;code&gt;X_&lt;/code&gt; 的最小值。</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="8984ca78ae6f645a8da0469517e3e68a7c22986d" translate="yes" xml:space="preserve">
          <source>Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between &lt;code&gt;1e0&lt;/code&gt; and &lt;code&gt;1e3&lt;/code&gt;:</source>
          <target state="translated">与上面的示例类似，我们可以指定一个连续的随机变量，该变量在 &lt;code&gt;1e0&lt;/code&gt; 和 &lt;code&gt;1e3&lt;/code&gt; 之间以对数形式均匀分布：</target>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">镜像 &lt;code&gt;class_log_prior_&lt;/code&gt; ，以将MultinomialNB解释为线性模型。</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">镜像 &lt;code&gt;feature_log_prob_&lt;/code&gt; 以便将MultinomialNB解释为线性模型。</target>
        </trans-unit>
        <trans-unit id="5f2cbd107037ed23248e5058a7a64cd6bae05468" translate="yes" xml:space="preserve">
          <source>Miscellaneous</source>
          <target state="translated">Miscellaneous</target>
        </trans-unit>
        <trans-unit id="0d2ecb69e7b12979a3e40a5ab8b5183911f1a3c3" translate="yes" xml:space="preserve">
          <source>Miscellaneous and introductory examples for scikit-learn.</source>
          <target state="translated">scikit-learn的杂项和介绍性例子。</target>
        </trans-unit>
        <trans-unit id="26655e342820eb1000893c259228858eef67a34d" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values</source>
          <target state="translated">丢失的属性值</target>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">缺少属性值。</target>
        </trans-unit>
        <trans-unit id="905705cdb93f7b29194485a892d91da6c1cdc874" translate="yes" xml:space="preserve">
          <source>Missing Value Imputation</source>
          <target state="translated">缺失值推算</target>
        </trans-unit>
        <trans-unit id="67cc34b1cd58b9ef03f7ff376e8541732aac180b" translate="yes" xml:space="preserve">
          <source>Missing information</source>
          <target state="translated">缺少资料</target>
        </trans-unit>
        <trans-unit id="0e00e76132a4d6b917901e5526c250a336a29108" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用基本&lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt;将缺失值替换为平均值，中位数或最频繁的值。</target>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">可以使用基本&lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt;将缺失值替换为平均值，中位数或最频繁的值。中位数是具有高强度变量的数据的更可靠的估计器，这些变量可能会主导结果（也称为&amp;ldquo;长尾巴&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">&amp;ldquo;数据&amp;rdquo;中的缺失值表示为NaN。&amp;ldquo;目标&amp;rdquo;中的缺失值表示为NaN（数字目标）或&amp;ldquo;无&amp;rdquo;（类别目标）</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">scikit-learn中所有双聚类估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">scikit-learn中所有分类器的mixin类。</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有集群估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有密度估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有回归估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">在scikit-learn中,所有变压器的mixin类。</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">模型复杂度影响</target>
        </trans-unit>
        <trans-unit id="9c567347b8af7d91b331f07af5f77ec0a361f505" translate="yes" xml:space="preserve">
          <source>Model Selection</source>
          <target state="translated">模型选择</target>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">模型选择界面</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">模型混合。当一个有监督的估计器的预测被用于训练另一个估计器时,在集合方法中。</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">模型复杂性</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">scikit-learn中的模型压缩目前只涉及线性模型。在这种情况下,这意味着我们要控制模型的稀疏性(即模型向量中非零坐标的数量)。一般来说,将模型稀疏性与稀疏的输入数据表示结合起来是个好主意。</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">模型持久性</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">模型重塑包括仅选择一部分可用特征以适合模型。换句话说，如果模型在学习阶段放弃了特征，那么我们可以从输入中去除那些特征。这有几个好处。首先，它减少了模型本身的内存（从而减少了时间）开销。一旦我们知道上次运行要保留哪些功能，它还允许丢弃管道中的显式功能选择组件。最后，它可以通过不收集和构建模型丢弃的特征来帮助减少数据访问和特征提取层中上游的处理时间和I / O使用率。例如，如果原始数据来自数据库，则可以通过使查询返回更轻的记录来编写更简单，更快速的查询或减少I / O使用量。在这一刻，重塑需要在scikit-learn中手动执行。在稀疏输入的情况下（尤其是在 &lt;code&gt;CSR&lt;/code&gt; 格式），通常只要不生成相关功能就足够了，而不必将其列留空。</target>
        </trans-unit>
        <trans-unit id="28eeecfcba5c4e3a6c993b8bf2c6736730dfbd15" translate="yes" xml:space="preserve">
          <source>Model selection</source>
          <target state="translated">模型选择</target>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">使用诸如&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; 之&lt;/a&gt;类的工具进行模型选择和评估，会采用一个 &lt;code&gt;scoring&lt;/code&gt; 参数，该参数控制它们对所评估的估算器应用何种度量。</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">通过评估各种参数设置进行的模型选择可以看作是使用标记数据&amp;ldquo;训练&amp;rdquo;网格参数的一种方法。</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">利用概率PCA和因子分析(FA)进行模型选择。</target>
        </trans-unit>
        <trans-unit id="96389a0f02a3826ae6017961d0301435c315a116" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">没有嵌套CV的模型选择使用相同的数据来调整模型参数并评估模型性能。因此，信息可能会&amp;ldquo;渗入&amp;rdquo;模型并过度拟合数据。这种影响的大小主要取决于数据集的大小和模型的稳定性。有关这些问题的分析，请参阅Cawley和Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">没有嵌套CV的模型选择使用相同的数据来调整模型参数并评估模型性能。因此，信息可能会&amp;ldquo;渗入&amp;rdquo;模型并过度拟合数据。这种影响的大小主要取决于数据集的大小和模型的稳定性。有关这些问题的分析，请参见Cawley和Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">模型选择:选择估计器及其参数</target>
        </trans-unit>
        <trans-unit id="ed1ba8eabae7e8d7de3d25705f2020f1428a1a11" translate="yes" xml:space="preserve">
          <source>Model the number of claims with a Poisson distribution, and the average claim amount per claim, also known as severity, as a Gamma distribution and multiply the predictions of both in order to get the total claim amount.</source>
          <target state="translated">用泊松分布模拟索赔数量,用Gamma分布模拟每次索赔的平均金额,也就是严重程度,并将两者的预测值相乘,从而得到总的索赔金额。</target>
        </trans-unit>
        <trans-unit id="7c9e82e3e8aa374bb01cb1f0583b83a3f30a27a0" translate="yes" xml:space="preserve">
          <source>Model the total claim amount per exposure directly, typically with a Tweedie distribution of Tweedie power \(p \in (1, 2)\).</source>
          <target state="translated">直接模拟每次暴露的总索赔额,通常采用特威迪分布的特威迪功率(p/in (1,2)\)。</target>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">模型验证</target>
        </trans-unit>
        <trans-unit id="44e8839819f969bae0c352bffb18b8d9aae37a64" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">对物种的地理分布进行建模是保护生物学中的一个重要问题。在此示例中，我们根据过去的观察结果和14个环境变量对两个南美哺乳动物的地理分布进行了建模。由于我们只有积极的例子（没有不成功的观察结果），因此我们将此问题转换为密度估计问题，并使用&lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;作为我们的建模工具。该数据集由Phillips等提供。al。（2006）。如果可用，该示例将使用&lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;底图&lt;/a&gt;绘制南美的海岸线和国家边界。</target>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">对物种的地理分布进行建模是保护生物学中的一个重要问题。在此示例中，我们根据过去的观察结果和14个环境变量对两个南美哺乳动物的地理分布进行了建模。由于我们只有正面的例子（没有不成功的意见），我们投这个问题是一个密度估计问题，并使用 &lt;code&gt;OneClassSVM&lt;/code&gt; 由包提供 &lt;code&gt;sklearn.svm&lt;/code&gt; 作为我们的建模工具。数据集由Phillips等提供。等 （2006）。如果可用，该示例将使用&lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;底图&lt;/a&gt;来绘制南美的海岸线和国家边界。</target>
        </trans-unit>
        <trans-unit id="3432d8d9b44052d02847746ae08d1ac381072a89" translate="yes" xml:space="preserve">
          <source>Modified Huber: \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) &amp;gt; 1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.</source>
          <target state="translated">修改后的Huber：\（L（y_i，f（x_i））= \ max（0，1-y_i f（x_i））^ 2 \）如果\（y_i f（x_i）&amp;gt; 1 \）和\（L（ y_i，f（x_i））= -4 y_i f（x_i）\）否则。</target>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">模块&lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt;实现内核岭回归。</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">梯度下降更新的动量。应该在0到1之间。仅在solver ='sgd'时使用。</target>
        </trans-unit>
        <trans-unit id="08837633f9d15f78a0ca0401b5e29aae44a3cb25" translate="yes" xml:space="preserve">
          <source>Monotonic Constraints</source>
          <target state="translated">单调性约束</target>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann，F.和Triggs，B.和Jurie，F.&amp;ldquo;使用随机聚类森林的快速判别式视觉密码本&amp;rdquo; NIPS 2007</target>
        </trans-unit>
        <trans-unit id="3f683b2b5fe59dc7e9963e0b844a2be959abe1bd" translate="yes" xml:space="preserve">
          <source>More details about the losses formulas can be found in the &lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">有关损耗公式的更多详细信息，请参见《&lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">可以在MacKay的David JC的&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;贝叶斯插值&lt;/a&gt;文章中找到更多详细信息。</target>
        </trans-unit>
        <trans-unit id="3dd8319d03052df7074a6e0643293f5dc781d510" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">可以在&lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;的文档中找到更多详细信息</target>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">可以在&lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;的文档中找到更多详细信息</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">有关可用于模型选择的工具的更多详细信息，请参见&amp;ldquo; &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;交叉验证：评估估计器性能&lt;/a&gt;和&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;调整&lt;/a&gt;估计器的超参数&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">更正式地说,样本(k)作为样本(i)的典范的责任是:</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">更正式地说，我们将核心样本定义为数据集中的样本，以便在 &lt;code&gt;eps&lt;/code&gt; 距离内存在 &lt;code&gt;min_samples&lt;/code&gt; 其他样本，这些样本被定义为核心样本的&lt;em&gt;邻居&lt;/em&gt;。这告诉我们核心样本位于向量空间的密集区域。群集是一组核心样本，可以通过递归取岩芯样品，发现其所有的邻居都岩芯样品，发现所有的构建&lt;em&gt;自己的&lt;/em&gt;邻居是岩心样品，等等。集群还具有一组非核心样本，这些样本是集群中核心样本的邻居，但本身不是核心样本。直观地讲，这些样本位于群集的边缘。&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">更一般而言，当分类器的准确性太接近随机性时，这可能意味着出了点问题：功能无济于事，超参数未正确调整，分类器正遭受类不平衡之苦，等等&amp;hellip;&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">更多信息可以在找到&lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;SciPy的安装页面&lt;/a&gt;，并在此&lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;博客文章&lt;/a&gt;从丹尼尔&amp;middot;努里其中有一些很好的一步一步安装的Debian / Ubuntu的指令。</target>
        </trans-unit>
        <trans-unit id="c3cc4720813506dbf91cef9b68d3a09728559160" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">更多信息可以在找到&lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;SciPy的安装页面&lt;/a&gt;，并在此&lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;博客文章&lt;/a&gt;从丹尼尔&amp;middot;努里其中有一些很好的一步一步安装的Debian / Ubuntu的指令。</target>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">更多来自OpenML的元数据</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">更准确地说，是在考虑了初始模型后对目标响应的期望；部分依赖图不包括 &lt;code&gt;init&lt;/code&gt; 模型。</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">更易读的代码,特别是它避免了构造参数列表。</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">更具体地说,对于线性和二次判别分析,/(P(X|y)/)被建模为具有密度的多变量高斯分布。</target>
        </trans-unit>
        <trans-unit id="2b3cb69cb34819cd8834522c11758ddc50e1f474" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(x|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">更具体地说,对于线性和二次判别分析,/(P(x|y)/)被建模为具有密度的多变量高斯分布。</target>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">最常见的是,将差距设定为/(hat{d}_{ij}=b S_{ij})。</target>
        </trans-unit>
        <trans-unit id="242544fc56d0b5c7cafd51d576a4b175219e1770" translate="yes" xml:space="preserve">
          <source>Most estimators based on nearest neighbors graphs now accept precomputed sparse graphs as input, to reuse the same graph for multiple estimator fits. To use this feature in a pipeline, one can use the &lt;code&gt;memory&lt;/code&gt; parameter, along with one of the two new transformers, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The precomputation can also be performed by custom estimators to use alternative implementations, such as approximate nearest neighbors methods. See more details in the &lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">现在，大多数基于最近邻图的估计量都接受预先计算的稀疏图作为输入，以将同一图重用于多个估计量拟合。要在管道中使用此功能，可以使用 &lt;code&gt;memory&lt;/code&gt; 参数，以及两个新的转换程序&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt; 之一&lt;/a&gt;。预计算也可以由自定义估算器执行，以使用替代实现，例如近似最近邻方法。请参阅《&lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;用户指南》&lt;/a&gt;中的更多详细信息。</target>
        </trans-unit>
        <trans-unit id="44814fa11b1099a09b484290756712e9a8679a34" translate="yes" xml:space="preserve">
          <source>Most of the parameters are unchanged from &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. One exception is the &lt;code&gt;max_iter&lt;/code&gt; parameter that replaces &lt;code&gt;n_estimators&lt;/code&gt;, and controls the number of iterations of the boosting process:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; 中的&lt;/a&gt;大多数参数均未更改。一个例外是 &lt;code&gt;max_iter&lt;/code&gt; 参数，它替换 &lt;code&gt;n_estimators&lt;/code&gt; ，并控制提升过程的迭代次数：</target>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">大部分方差可以用宽度为effective_rank的钟形曲线来解释:奇异值曲线的低秩部分是。</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">大多数scikit-learn模型通常是相当快的,因为它们是通过编译的Cython扩展或优化的计算库实现的。另一方面,在许多实际应用中,特征提取过程(即把数据库行或网络数据包等原始数据转化为numpy数组)控制着整个预测时间。例如在路透社的文本分类任务中,整个准备工作(读取和解析SGML文件,将文本标记化,并将其散列到一个通用的向量空间中)所花费的时间是实际预测代码的100到500倍,这取决于所选择的模型。</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">在自然语言处理（NLP）和信息检索（IR）文献中，大多数LSA处理都交换矩阵\（X \）的轴，因此其形状为 &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; 。我们以另一种方式更好地匹配scikit-learn API来提供LSA，但是找到的奇异值是相同的。</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">多目标分类</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">多目标回归</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">多类AdaBoosted决策树。</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">多类目标。</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">多类目标。指标矩阵开启多标签分类。</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">多维缩放</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">多层Perceptron分类器。</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">多层Perceptron对特征缩放很敏感，因此强烈建议对数据进行缩放。例如，将输入向量X上的每个属性缩放为[0，1]或[-1，+1]，或将其标准化为均值0和方差1。请注意，您必须对测试集应用&lt;em&gt;相同的&lt;/em&gt;缩放比例有意义的结果。您可以使用 &lt;code&gt;StandardScaler&lt;/code&gt; 进行标准化。</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">多层感知器回归器。</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">多输出决策树回归</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">跨多个预测器预测的多产出目标。注:为每个预测器生成单独的模型。</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">多产出目标。</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">多输出目标。指标矩阵开启多标签估计。</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">以L1/L2混合正则化训练的多任务ElasticNet模型。</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">多任务L1/L2 ElasticNet,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">多任务L1/L2 Lasso,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">多任务L1/L2 Lasso,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">以L1/L2混合正则化训练的多任务Lasso模型。</target>
        </trans-unit>
        <trans-unit id="162889b9c309e59105e244387d111bbb75ed4cc7" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</source>
          <target state="translated">以L1/L2混合正态为正则器训练的多任务Lasso模型。</target>
        </trans-unit>
        <trans-unit id="0119eef45392f9d57273a8cd6ca2fcc5f0003969" translate="yes" xml:space="preserve">
          <source>Multi-task linear regressors with variable selection</source>
          <target state="translated">多任务线性回归器与变量选择</target>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">多类和多标签分类策略</target>
        </trans-unit>
        <trans-unit id="8ddfaa46f2a114c89c7cad99fd0ddf4dc7314399" translate="yes" xml:space="preserve">
          <source>Multiclass case:</source>
          <target state="translated">多类案件。</target>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">多级分类</target>
        </trans-unit>
        <trans-unit id="868117baea7dbed0e92972aac1d890c07c8ae48f" translate="yes" xml:space="preserve">
          <source>Multiclass data will be treated as if binarized under a one-vs-rest transformation. Returned confusion matrices will be in the order of sorted unique labels in the union of (y_true, y_pred).</source>
          <target state="translated">多类数据将被视为二值化的一vs休止符转换。返回的混淆矩阵将按照(y_true,y_pred)联合中唯一标签的排序顺序进行。</target>
        </trans-unit>
        <trans-unit id="2d8780a18f5ba3e6cb5bb5a579ea67a8f2550bb3" translate="yes" xml:space="preserve">
          <source>Multiclass only. Determines the type of configuration to use. The default value raises an error, so either &lt;code&gt;'ovr'&lt;/code&gt; or &lt;code&gt;'ovo'&lt;/code&gt; must be passed explicitly.</source>
          <target state="translated">仅多类。确定要使用的配置类型。默认值会引发错误，因此必须显式传递 &lt;code&gt;'ovr'&lt;/code&gt; 或 &lt;code&gt;'ovo'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f6acefc7f4185f0f4df5b1178aaed55afe1147ed" translate="yes" xml:space="preserve">
          <source>Multiclass only. List of labels that index the classes in &lt;code&gt;y_score&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, the numerical or lexicographical order of the labels in &lt;code&gt;y_true&lt;/code&gt; is used.</source>
          <target state="translated">仅多类。索引 &lt;code&gt;y_score&lt;/code&gt; 中的类的标签列表。如果为 &lt;code&gt;None&lt;/code&gt; ，则使用 &lt;code&gt;y_true&lt;/code&gt; 中标签的数字或字典顺序。</target>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">如Zadrozny和Elkan所推荐的那样,多类概率估计是通过简单的归一化从二元(一与休息)估计中得出的。</target>
        </trans-unit>
        <trans-unit id="39d0ca41499d6b7f83a17678aedf5fae33705c06" translate="yes" xml:space="preserve">
          <source>Multiclass problems are binarized and treated like the corresponding multilabel problem:</source>
          <target state="translated">多类问题被二值化,并与相应的多标签问题一样处理。</target>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">多类设置</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">对新群体的多类稀疏对数回归20</target>
        </trans-unit>
        <trans-unit id="abed0a03e9d2180975b0a68aad7dbbccddc0d2d0" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logistic regression on 20newgroups</source>
          <target state="translated">20newgroups上的多类稀疏逻辑回归。</target>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">多类光谱聚类，2003年Stella X.Yu，Shijianbo &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4386a05880a13b860ce6d4571568b773373f22e3" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">多类光谱聚类，2003年Stella X.Yu，Shijianbo &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">多类与多标签拟合</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">多维缩放</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">多标签分类</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">多标签排名指标</target>
        </trans-unit>
        <trans-unit id="ce79d912af81c5a374445c22e6e3cfe9ecb9a93c" translate="yes" xml:space="preserve">
          <source>Multilabel-indicator case:</source>
          <target state="translated">多标签指示器案例。</target>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">可以将多度量计分指定为预定义分数名称的字符串列表，也可以将dict映射为计分者名称与计分器功能和/或预定义计分者名称。有关更多详细信息，请参见&lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;使用多重度量评估&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">多项式+L1罚款</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">多项式+二级惩罚</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">多项式偏差（ &lt;code&gt;'deviance'&lt;/code&gt; ）：具有 &lt;code&gt;n_classes&lt;/code&gt; 互斥类的多类分类的负多项式对数似然损失函数。它提供了概率估计。初始模型由每个类别的先验概率给出。在每次迭代中，都必须构造 &lt;code&gt;n_classes&lt;/code&gt; 回归树，这使得GBRT对于具有大量类的数据集而言效率很低。</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">可以使用MultiOutputClassifier将多输出分类支持添加到任何分类 &lt;code&gt;MultiOutputClassifier&lt;/code&gt; 。该策略包括为每个目标配备一个分类器。这允许多个目标变量分类。此类的目的是扩展估计器，以便能够估计在单个X预测矩阵上训练的一系列目标函数（f1，f2，f3 ...，fn），以预测一系列响应（y1，y2，y3 &amp;hellip;，yn）。</target>
        </trans-unit>
        <trans-unit id="8ec2d1e390ee85463a8b9edc1df8f6a33597454a" translate="yes" xml:space="preserve">
          <source>Multioutput methods</source>
          <target state="translated">多输出方法</target>
        </trans-unit>
        <trans-unit id="8e7bfb83db794fa15648cd7ab3b23c509cc8018c" translate="yes" xml:space="preserve">
          <source>Multioutput regression</source>
          <target state="translated">多产出回归</target>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">多输出回归支持可以通过MultiOutputRegressor添加到任何回归 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 。该策略包括为每个目标安装一个回归器。由于每个目标仅由一个回归变量表示，因此可以通过检查其相应的回归变量来获取有关目标的知识。由于 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 每个目标可容纳一个回归器，因此无法利用目标之间的相关性。</target>
        </trans-unit>
        <trans-unit id="96d87119823da5637cea208be6276fad5c922737" translate="yes" xml:space="preserve">
          <source>Multioutput- multiclass classification</source>
          <target state="translated">多输出-多类分类</target>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">使用 &lt;code&gt;cross_validate&lt;/code&gt; 进行多指标评估（更多信息，请参考 &lt;code&gt;scoring&lt;/code&gt; 参数doc）</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">可以通过将 &lt;code&gt;scoring&lt;/code&gt; 参数设置为度量记分器名称列表或将记分器名称映射到记分器可调用项的字典来完成多个度量参数的搜索。</target>
        </trans-unit>
        <trans-unit id="24f6a3478d65f4dead755fd18d3792f81fa6260d" translate="yes" xml:space="preserve">
          <source>Multiple stacking layers can be achieved by assigning &lt;code&gt;final_estimator&lt;/code&gt; to a &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">通过将 &lt;code&gt;final_estimator&lt;/code&gt; 分配给&lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt; &lt;code&gt;StackingRegressor&lt;/code&gt; &lt;/a&gt;可以实现多个堆栈层：</target>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">每个变压器特征的乘法权重。键是变压器名称,值是权重。</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">每台变压器的特征乘法权重。变压器的输出乘以这些权重。键是变压器名称,值是权重。</target>
        </trans-unit>
        <trans-unit id="afa1ae58a55a69631c4c27e76dfc260c619f73c7" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C for each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">每个类的参数C的乘数。根据 &lt;code&gt;class_weight&lt;/code&gt; 参数进行计算。</target>
        </trans-unit>
        <trans-unit id="b0e900a51b93880c89d198a9d719bd1f20cdb329" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C of each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">每个类的参数C的乘数。根据 &lt;code&gt;class_weight&lt;/code&gt; 参数进行计算。</target>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">将特征乘以指定的值。如果没有,那么特征将被一个在[1,100]中抽取的随机值所缩放。注意,缩放发生在移位之后。</target>
        </trans-unit>
        <trans-unit id="d54881ba1eca5e77240b1b917c4b4af86c4398ba" translate="yes" xml:space="preserve">
          <source>Multiplying the coefficients by the standard deviation of the related feature would reduce all the coefficients to the same unit of measure. As we will see &lt;a href=&quot;#scaling-num&quot;&gt;after&lt;/a&gt; this is equivalent to normalize numerical variables to their standard deviation, as \(y = \sum{coef_i \times X_i} = \sum{(coef_i \times std_i) \times (X_i / std_i)}\).</source>
          <target state="translated">将系数乘以相关特征的标准偏差会将所有系数减少到相同的度量单位。正如我们将看到的&lt;a href=&quot;#scaling-num&quot;&gt;，&lt;/a&gt;这等效于将数值变量归一化为其标准偏差，如\（y = \ sum {coef_i \ times X_i} = \ sum {（coef_i \ times std_i）\ times（X_i / std_i）} \） 。</target>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">多线程BLAS库有时会与Python的 &lt;code&gt;multiprocessing&lt;/code&gt; 模块发生冲突，例如 &lt;code&gt;GridSearchCV&lt;/code&gt; 和采用 &lt;code&gt;n_jobs&lt;/code&gt; 参数的大多数其他估计器都使用该模块（除了 &lt;code&gt;SGDClassifier&lt;/code&gt; ， &lt;code&gt;SGDRegressor&lt;/code&gt; ， &lt;code&gt;Perceptron&lt;/code&gt; ， &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; 和基于树的方法（例如随机森林））。当使用OpenMP支持构建时，Apple的Accelerate和OpenBLAS就是如此。</target>
        </trans-unit>
        <trans-unit id="3483a919f49e511ca829411c285a74281e005ef3" translate="yes" xml:space="preserve">
          <source>Multivariate imputation of missing values.</source>
          <target state="translated">缺失值的多变量推算。</target>
        </trans-unit>
        <trans-unit id="3e386b49678343bab915e96a29e16b2a1aa09993" translate="yes" xml:space="preserve">
          <source>Multivariate imputer that estimates each feature from all the others.</source>
          <target state="translated">多变量推算器,从所有其他特征中估计出每个特征。</target>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">必须在第一次调用 partial_fit 时提供,可以在后续调用中省略。</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">必须满足基础估计器的输入假设。</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">相互信息(不按机会调整)</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">两个聚类之间的相互信息。</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">两个随机变量之间的互信息（MI）&lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;是一个非负值，用于衡量变量之间的相关性。当且仅当两个随机变量是独立的，并且等于较高的值意味着较高的依赖性时，它等于零。</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">两个随机变量之间的互信息（MI）&lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;是一个非负值，用于衡量变量之间的相关性。当且仅当两个随机变量是独立的，并且等于较高的值意味着较高的依赖性时，它等于零。</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">特征与目标之间的相互信息。</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">连续目标的相互信息。</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">互通信息,以利于连续目标。</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">离散目标的相互信息。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
