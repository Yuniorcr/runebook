<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="6fd372cb1c5e72e14169cfb741de07d656f9f533" translate="yes" xml:space="preserve">
          <source>A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</source>
          <target state="translated">A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</target>
        </trans-unit>
        <trans-unit id="25e7a287394a0eef59197bb60e52ac1f6a84a5df" translate="yes" xml:space="preserve">
          <source>A store implementation that uses a file to store the underlying key-value pairs.</source>
          <target state="translated">A store implementation that uses a file to store the underlying key-value pairs.</target>
        </trans-unit>
        <trans-unit id="6eb9ae0897420968aee10a2b095c52bec596e05c" translate="yes" xml:space="preserve">
          <source>A string</source>
          <target state="translated">ÎÅà</target>
        </trans-unit>
        <trans-unit id="62ebd3a50dba8122d0758d5d23ce37a93f24ab7e" translate="yes" xml:space="preserve">
          <source>A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through &lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt;&lt;code&gt;get_worker_info()&lt;/code&gt;&lt;/a&gt; and the result can be passed in to functions such as &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; to avoid copying a string on every invocation.</source>
          <target state="translated">A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through &lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt; &lt;code&gt;get_worker_info()&lt;/code&gt; &lt;/a&gt; and the result can be passed in to functions such as &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; &lt;/a&gt; to avoid copying a string on every invocation.</target>
        </trans-unit>
        <trans-unit id="eb68c1bf6abfe77fe220468c23dfc4e2ccab3d1b" translate="yes" xml:space="preserve">
          <source>A tensor can be constructed from a Python &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;&lt;code&gt;list&lt;/code&gt;&lt;/a&gt; or sequence using the &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; constructor:</source>
          <target state="translated">A tensor can be constructed from a Python &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt; &lt;code&gt;list&lt;/code&gt; &lt;/a&gt; or sequence using the &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; constructor:</target>
        </trans-unit>
        <trans-unit id="78690c5a267b02f430ea07580ff8566f6ba33315" translate="yes" xml:space="preserve">
          <source>A tensor can be created with &lt;code&gt;requires_grad=True&lt;/code&gt; so that &lt;a href=&quot;autograd#module-torch.autograd&quot;&gt;&lt;code&gt;torch.autograd&lt;/code&gt;&lt;/a&gt; records operations on them for automatic differentiation.</source>
          <target state="translated">A tensor can be created with &lt;code&gt;requires_grad=True&lt;/code&gt; so that &lt;a href=&quot;autograd#module-torch.autograd&quot;&gt; &lt;code&gt;torch.autograd&lt;/code&gt; &lt;/a&gt; records operations on them for automatic differentiation.</target>
        </trans-unit>
        <trans-unit id="28d24ca39991219e32d7e5d9431b5638f6178577" translate="yes" xml:space="preserve">
          <source>A tensor containing the STFT result with shape described above</source>
          <target state="translated">A tensor containing the STFT result with shape described above</target>
        </trans-unit>
        <trans-unit id="4fa2d7d56cae6fbb5c5952f1e0d2ce41103f7c68" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-complex Fourier transform result</target>
        </trans-unit>
        <trans-unit id="f7a71af7c596a286496bba8f33f4ec3e6cafe018" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex inverse Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-complex inverse Fourier transform result</target>
        </trans-unit>
        <trans-unit id="247163b1a472b2aa12f5bee5985796f6dfb72690" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-real inverse Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-real inverse Fourier transform result</target>
        </trans-unit>
        <trans-unit id="795a9e2ee149db6f297c73b4efe85421d1b43891" translate="yes" xml:space="preserve">
          <source>A tensor containing the real-to-complex Fourier transform result</source>
          <target state="translated">A tensor containing the real-to-complex Fourier transform result</target>
        </trans-unit>
        <trans-unit id="808c26c158b1b41512b2f49967eb9955a23d3ece" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists,</source>
          <target state="translated">A tensor equivalent to converting all the input tensors into lists,</target>
        </trans-unit>
        <trans-unit id="9f00a5099d29a70d5489d75e795a620e76bd2892" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists, do &lt;code&gt;itertools.combinations&lt;/code&gt; or &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; on these lists, and finally convert the resulting list into tensor.</source>
          <target state="translated">A tensor equivalent to converting all the input tensors into lists, do &lt;code&gt;itertools.combinations&lt;/code&gt; or &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; on these lists, and finally convert the resulting list into tensor.</target>
        </trans-unit>
        <trans-unit id="ffa7cd4e81e99e52c54f7880515bfb697dd56d45" translate="yes" xml:space="preserve">
          <source>A tensor of shape equal to the broadcasted shape of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">A tensor of shape equal to the broadcasted shape of &lt;code&gt;condition&lt;/code&gt; , &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;y&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9f636fee4017e68defd8eca1418692d893e6ca2e" translate="yes" xml:space="preserve">
          <source>A tensor of specific data type can be constructed by passing a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and/or a &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; to a constructor or tensor creation op:</source>
          <target state="translated">A tensor of specific data type can be constructed by passing a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; and/or a &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; to a constructor or tensor creation op:</target>
        </trans-unit>
        <trans-unit id="56a6db01c3a1cc54c7399f9ada8907490b637b87" translate="yes" xml:space="preserve">
          <source>A tensor or a tuple of tensors containing</source>
          <target state="translated">A tensor or a tuple of tensors containing</target>
        </trans-unit>
        <trans-unit id="7015c602ad96676ce56c48693c2eb3931eef821e" translate="yes" xml:space="preserve">
          <source>A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</source>
          <target state="translated">A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</target>
        </trans-unit>
        <trans-unit id="fc0920364dd37ceba72cafbd19ccdfa555706749" translate="yes" xml:space="preserve">
          <source>A transformer model.</source>
          <target state="translated">A transformer model.</target>
        </trans-unit>
        <trans-unit id="6179bf6b8f65bc90ae5ee40cc1d9226f83588987" translate="yes" xml:space="preserve">
          <source>A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;) model with corresponding parameters.</source>
          <target state="translated">A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;) model with corresponding parameters.</target>
        </trans-unit>
        <trans-unit id="8d66b15a6d20ea906de4e959af867fbbfb815b34" translate="yes" xml:space="preserve">
          <source>A tuple containing subtypes &lt;code&gt;T0&lt;/code&gt;, &lt;code&gt;T1&lt;/code&gt;, etc. (e.g. &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt;)</source>
          <target state="translated">A tuple containing subtypes &lt;code&gt;T0&lt;/code&gt; , &lt;code&gt;T1&lt;/code&gt; , etc. (e.g. &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="d1879bf0c8405911e4b464be498b79cafb324a20" translate="yes" xml:space="preserve">
          <source>A tuple of tensors containing</source>
          <target state="translated">A tuple of tensors containing</target>
        </trans-unit>
        <trans-unit id="a2414a0a13983fc4726a965cde7c58a73e7199ce" translate="yes" xml:space="preserve">
          <source>A user &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; instance to the result value. Use the blocking API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt;&lt;/a&gt; to retrieve the result value locally.</source>
          <target state="translated">A user &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt; instance to the result value. Use the blocking API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt; &lt;/a&gt; to retrieve the result value locally.</target>
        </trans-unit>
        <trans-unit id="327c954237e47bd5486826604abda3addea9acfd" translate="yes" xml:space="preserve">
          <source>A value which is either None or type &lt;code&gt;T&lt;/code&gt;</source>
          <target state="translated">A value which is either None or type &lt;code&gt;T&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="50a643ab21488e75a8cc3c466f11fce2f4ad673a" translate="yes" xml:space="preserve">
          <source>A wrapper around Python&amp;rsquo;s assert which is symbolically traceable.</source>
          <target state="translated">A wrapper around Python&amp;rsquo;s assert which is symbolically traceable.</target>
        </trans-unit>
        <trans-unit id="6c4e7377e95c980888bb30d4e414aed707349a68" translate="yes" xml:space="preserve">
          <source>A wrapper around any of the 3 key-value stores (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;) that adds a prefix to each key inserted to the store.</source>
          <target state="translated">A wrapper around any of the 3 key-value stores (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt;) that adds a prefix to each key inserted to the store.</target>
        </trans-unit>
        <trans-unit id="893fa7187a5a433ff31c6a646ba041165efd5e8a" translate="yes" xml:space="preserve">
          <source>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: &lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/a&gt;</source>
          <target state="translated">A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: &lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1eb4168992b5101108db3eeb3c96de61bbb45012" translate="yes" xml:space="preserve">
          <source>APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.</source>
          <target state="translated">APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.</target>
        </trans-unit>
        <trans-unit id="bccb0000d0e87e05464132ae5d1a9fb1c8982b8a" translate="yes" xml:space="preserve">
          <source>ATen operators</source>
          <target state="translated">ATen operators</target>
        </trans-unit>
        <trans-unit id="4bfc2c1b851857261545e0a7eb83c1db10ae6260" translate="yes" xml:space="preserve">
          <source>AX = B</source>
          <target state="translated">AX = B</target>
        </trans-unit>
        <trans-unit id="afc108ce2cbe35fd87218f8b08145cd05b2342e4" translate="yes" xml:space="preserve">
          <source>AX = b</source>
          <target state="translated">AX = b</target>
        </trans-unit>
        <trans-unit id="0d45206d15c0adf19b111143610d728deb04b811" translate="yes" xml:space="preserve">
          <source>A^T A / (m - 1)</source>
          <target state="translated">A^T A / (m - 1)</target>
        </trans-unit>
        <trans-unit id="086c7a89401b2c5d32553704f5cbdf2377564e0a" translate="yes" xml:space="preserve">
          <source>Abstract base class for creation of new pruning techniques.</source>
          <target state="translated">Abstract base class for creation of new pruning techniques.</target>
        </trans-unit>
        <trans-unit id="ce6150c1e37157c2b0ede38ac1f069f652c4bd27" translate="yes" xml:space="preserve">
          <source>Accepts as argument an instance of a BasePruningMethod or an iterable of them.</source>
          <target state="translated">Accepts as argument an instance of a BasePruningMethod or an iterable of them.</target>
        </trans-unit>
        <trans-unit id="f884a5a94ee360d4b70504f91c5e58e4219b6c1e" translate="yes" xml:space="preserve">
          <source>Accessing Module Parameters</source>
          <target state="translated">Accessing Module Parameters</target>
        </trans-unit>
        <trans-unit id="f2929dbc213f512c19707fd1ca48c5d74f64d113" translate="yes" xml:space="preserve">
          <source>Accumulate the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by adding to the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is added to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Accumulate the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by adding to the indices in the order given in &lt;code&gt;index&lt;/code&gt; . For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt; , then the &lt;code&gt;i&lt;/code&gt; th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; is added to the &lt;code&gt;j&lt;/code&gt; th row of &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="af92bf9a55b4e8ebc9cf19933cf41beac97277f2" translate="yes" xml:space="preserve">
          <source>Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&amp;rsquo;s law&lt;/a&gt;.</source>
          <target state="translated">Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&amp;rsquo;s law&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="6753d63f427d91531462240c7159228dba9dbe41" translate="yes" xml:space="preserve">
          <source>Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</source>
          <target state="translated">Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</target>
        </trans-unit>
        <trans-unit id="b7d62c155e1ec348bbbea519cad55c85951b3b6e" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool1d</source>
          <target state="translated">AdaptiveAvgPool1d</target>
        </trans-unit>
        <trans-unit id="0068238690eed5246c61ee781eb3611fe6751168" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool2d</source>
          <target state="translated">AdaptiveAvgPool2d</target>
        </trans-unit>
        <trans-unit id="65992cfd09d95132c3908bd45a21c959a32be4e1" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool3d</source>
          <target state="translated">AdaptiveAvgPool3d</target>
        </trans-unit>
        <trans-unit id="a60f272cfc14a259a9716f4818eb79c22d95c973" translate="yes" xml:space="preserve">
          <source>AdaptiveLogSoftmaxWithLoss</source>
          <target state="translated">AdaptiveLogSoftmaxWithLoss</target>
        </trans-unit>
        <trans-unit id="84f10363ff1424545148abb336751c9a84b6497d" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool1d</source>
          <target state="translated">AdaptiveMaxPool1d</target>
        </trans-unit>
        <trans-unit id="b001386de556a7b95313a763136a21be6a3d2d16" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool2d</source>
          <target state="translated">AdaptiveMaxPool2d</target>
        </trans-unit>
        <trans-unit id="9315f841b98a9cfd3c84766eadb0e1b9b7bcbe32" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool3d</source>
          <target state="translated">AdaptiveMaxPool3d</target>
        </trans-unit>
        <trans-unit id="53b75d7b44ea18bab3550e79b5f19b4d01eb4d3d" translate="yes" xml:space="preserve">
          <source>Add a scalar or tensor to &lt;code&gt;self&lt;/code&gt; tensor. If both &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; are specified, each element of &lt;code&gt;other&lt;/code&gt; is scaled by &lt;code&gt;alpha&lt;/code&gt; before being used.</source>
          <target state="translated">Add a scalar or tensor to &lt;code&gt;self&lt;/code&gt; tensor. If both &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; are specified, each element of &lt;code&gt;other&lt;/code&gt; is scaled by &lt;code&gt;alpha&lt;/code&gt; before being used.</target>
        </trans-unit>
        <trans-unit id="363f75ca10e654de02076c9e812636b00b9240b1" translate="yes" xml:space="preserve">
          <source>Add a set of hyperparameters to be compared in TensorBoard.</source>
          <target state="translated">Add a set of hyperparameters to be compared in TensorBoard.</target>
        </trans-unit>
        <trans-unit id="2981d9b125bbce3a8357f497272caa27d116b65e" translate="yes" xml:space="preserve">
          <source>Add audio data to summary.</source>
          <target state="translated">Add audio data to summary.</target>
        </trans-unit>
        <trans-unit id="378dceab0437ac530e9f4b067ecb1268bcc2db27" translate="yes" xml:space="preserve">
          <source>Add batched image data to summary.</source>
          <target state="translated">Add batched image data to summary.</target>
        </trans-unit>
        <trans-unit id="b6c70cf24a9dc9ee0a583b95ed8a5b22e79a979d" translate="yes" xml:space="preserve">
          <source>Add embedding projector data to summary.</source>
          <target state="translated">Add embedding projector data to summary.</target>
        </trans-unit>
        <trans-unit id="745fa53d610381d82d054b114f7c40ae6aaa4f61" translate="yes" xml:space="preserve">
          <source>Add graph data to summary.</source>
          <target state="translated">Add graph data to summary.</target>
        </trans-unit>
        <trans-unit id="360796ce4b88f8a72813e93a376d34c0228713ee" translate="yes" xml:space="preserve">
          <source>Add histogram to summary.</source>
          <target state="translated">Add histogram to summary.</target>
        </trans-unit>
        <trans-unit id="1b6b7d6099858bcb255707e61a495522a0414994" translate="yes" xml:space="preserve">
          <source>Add image data to summary.</source>
          <target state="translated">Add image data to summary.</target>
        </trans-unit>
        <trans-unit id="a392c90962df4acfb05e6d0ec46fd5a47980ff9d" translate="yes" xml:space="preserve">
          <source>Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</source>
          <target state="translated">Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</target>
        </trans-unit>
        <trans-unit id="701a19440eea643652da7e69f517b8c3d995eacc" translate="yes" xml:space="preserve">
          <source>Add scalar data to summary.</source>
          <target state="translated">Add scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="7a54b0b6b6b02f66bd6beaecffa1d99a5f7eb192" translate="yes" xml:space="preserve">
          <source>Add text data to summary.</source>
          <target state="translated">Add text data to summary.</target>
        </trans-unit>
        <trans-unit id="c77f8dadb528de130d984e34a450fb3582dda9aa" translate="yes" xml:space="preserve">
          <source>Add video data to summary.</source>
          <target state="translated">Add video data to summary.</target>
        </trans-unit>
        <trans-unit id="8afe6f3185ed76e023f80393638df4f157d48f52" translate="yes" xml:space="preserve">
          <source>Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</source>
          <target state="translated">Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</target>
        </trans-unit>
        <trans-unit id="4c9c6ec239352be639e5bda7f0d0b398caeb9237" translate="yes" xml:space="preserve">
          <source>Adding support for operators</source>
          <target state="translated">Adding support for operators</target>
        </trans-unit>
        <trans-unit id="4e032cc35826dbb2a4a560c47d8b0573e59463ab" translate="yes" xml:space="preserve">
          <source>Additional args:</source>
          <target state="translated">Additional args:</target>
        </trans-unit>
        <trans-unit id="b02695e3612016d107e21b0824bdec94963550bb" translate="yes" xml:space="preserve">
          <source>Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt; . For each value in &lt;code&gt;src&lt;/code&gt; , the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d9baa44eaf376e383c39686ddc11f7bd15a1ea88" translate="yes" xml:space="preserve">
          <source>Adds a buffer to the module.</source>
          <target state="translated">Adds a buffer to the module.</target>
        </trans-unit>
        <trans-unit id="677c1d2632a65843e9a01046b892b48b9f5176ac" translate="yes" xml:space="preserve">
          <source>Adds a child module to the current module.</source>
          <target state="translated">Adds a child module to the current module.</target>
        </trans-unit>
        <trans-unit id="96b77db46874289e2dd12b6bc9489ce00f46440a" translate="yes" xml:space="preserve">
          <source>Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</source>
          <target state="translated">Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</target>
        </trans-unit>
        <trans-unit id="f9bccaca3fe9322a4122d15d5e4fdb25202c0320" translate="yes" xml:space="preserve">
          <source>Adds a parameter to the module.</source>
          <target state="translated">Adds a parameter to the module.</target>
        </trans-unit>
        <trans-unit id="7fc7b19e96d1e48b76f041d88e6e8c8791a06fa6" translate="yes" xml:space="preserve">
          <source>Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;scatter_()&lt;/code&gt;&lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;scatter_()&lt;/code&gt; &lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt; , it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8562c008b76c53309d3a0973a409f19d43babb11" translate="yes" xml:space="preserve">
          <source>Adds many scalar data to summary.</source>
          <target state="translated">Adds many scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="8058a9f23e00e9bb14b6abf55b56e8a8543f9c29" translate="yes" xml:space="preserve">
          <source>Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</source>
          <target state="translated">Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</target>
        </trans-unit>
        <trans-unit id="fd54948ee53264250bb1b351262b4ccb8986d94e" translate="yes" xml:space="preserve">
          <source>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</source>
          <target state="translated">Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</target>
        </trans-unit>
        <trans-unit id="ab685d43fc5e49c46e526f76f07f3d59e811972e" translate="yes" xml:space="preserve">
          <source>Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</target>
        </trans-unit>
        <trans-unit id="b2b204082818c243f404c5bfdc3dc16bbc32640c" translate="yes" xml:space="preserve">
          <source>After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</source>
          <target state="translated">After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</target>
        </trans-unit>
        <trans-unit id="cd74631d206c5c267345dcc56490ab6ea1c83f20" translate="yes" xml:space="preserve">
          <source>After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, or &lt;code&gt;str&lt;/code&gt;. All values must be of the same type; heterogenous types for enum values are not supported.</source>
          <target state="translated">After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt; , &lt;code&gt;float&lt;/code&gt; , or &lt;code&gt;str&lt;/code&gt; . All values must be of the same type; heterogenous types for enum values are not supported.</target>
        </trans-unit>
        <trans-unit id="c52a739b9d98878e44d92e91256235a6d294431c" translate="yes" xml:space="preserve">
          <source>After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="f3dd860a8580060d82d526ad04c5badbd57df94f" translate="yes" xml:space="preserve">
          <source>After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</source>
          <target state="translated">After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</target>
        </trans-unit>
        <trans-unit id="e03f01ec1cb89a34bde4816ebeb0fe756b9b7364" translate="yes" xml:space="preserve">
          <source>After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="9ae05a9ce240652b1911853b65ddd62f9764383c" translate="yes" xml:space="preserve">
          <source>AlexNet</source>
          <target state="translated">AlexNet</target>
        </trans-unit>
        <trans-unit id="ca5a1956913984160d31d9bf92ec542323ba8065" translate="yes" xml:space="preserve">
          <source>AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="translated">AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</target>
        </trans-unit>
        <trans-unit id="52fc4196dd42d00268d82ad7f69953e231b88729" translate="yes" xml:space="preserve">
          <source>Alexnet</source>
          <target state="translated">Alexnet</target>
        </trans-unit>
        <trans-unit id="c4836f5ef10696c1523f7a542472a979fa86940f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="251f8be41a3289fe0ff13ca86e83184aaa6d1685" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt;&lt;code&gt;clamp_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt; &lt;code&gt;clamp_()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8b1674d7e68b7ce19ae1eac32e5f2d61ba39a3fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b806d3eb44bb5995d21b017162c333c30909bfd1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt;&lt;code&gt;numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt; &lt;code&gt;numel()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8003ce6f6a4824ac7e86312aee5a809819964394" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae0811ab2275dcf323f302291b8578862167855b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dff47f32b9bc388203bf7e97dfa096239e9aa4fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="27f71b5c7ea505c6db6e7c3f268907cac51983c8" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8915eae6d7b9ee1bf0b6f73f5bcf701208d83a87" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="12a1bcfe361d7554bc235392cdf072ae77235de7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21de09801d295b9e55c5ca95a97e6b707bdd91c4" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42b48abe60c1061b77c3d37b15698be8fee9c90" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b913a66219f4d1a88ba91e403c72efb7f3ef3abb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4ba0bdfda926bfb5d618f4ee65334d6fb8eb1eee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="346a30cebac1d8f7c67c1c3a7f4f3bc3613f3f76" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b313b53bc520608d37513795d2fb79da1dd596f1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fd6c12495884b27e735c199934fdc7b613fa577d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="982d40969f8de3053b6a7ba2029aed4446d9284b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="2d9a228af380b06ff04ec79aacc440723157779f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="93282bf10a2917c15fe551a51f5161e90b61877f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5c53bd5a9189ae9c36a9718c39849a71cf4795d9" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="84a5bea4d6c5059fe64ceca353dc77a1a8d6f0c7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="07f60a052509a04e4581fb6abfc90937c2c71d1e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠</target>
        </trans-unit>
        <trans-unit id="d26757f4ad01130675ba8ea2672c12a2db5b8aab" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠</target>
        </trans-unit>
        <trans-unit id="f1445190c40314f7662b67191fe5caef59adeec0" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="b7d58a750debce958ceee5c25ad01617693d3ea2" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt; ÎåÄÌïú Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="3aec840f94f18b783a10543e6249d4a95dd33cf1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="606d0a5ac071a495928d8903629e0d3c2c53055d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="dee63cafa3f7f5b85ddbc8fc58438f4ee2396691" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="8c0014cc8ab298138389abb57c1b96765b473b42" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="23c4daa625a3c02d837a48a43fcee80a5cf020be" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt; ÎåÄÌïú Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="2cc182b7bdf6fcacb622e0e829ca673368ffde7e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="caa996e0ac1f8de7d67f7455447758713ba8feee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt; ÎåÄÌïú Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="392a801bbfecc8b02cebb4cb713410918e491a2f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt; ÎåÄÌïú Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="9b13ac85fa2f8f4adaad80131c925608d0b684eb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="6b6841b18d2b1421ff7068e9ab0cf0387e0beeda" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt; ÎåÄÌïú Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="4bb2a8621505158fea7a3e7e83a0736217506d1f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="3eaccfa1051feb57b0d0c06ee1f7f0ca18ed2f48" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="80bfac967c271e4287120446d7e735504a25deee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠</target>
        </trans-unit>
        <trans-unit id="63adb787982c4f010005f5042a7176ac57a5c3dd" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="004a1c4ca4c726826c71b8d3f3c668a30c19eca5" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">ÌïÑÎìú Î≤àÌò∏ 0Ïùò ‚Äã‚ÄãÎ≥ÑÎ™Ö</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">ÌïÑÎìú Î≤àÌò∏ 1Ïùò Î≥ÑÎ™Ö</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">ÌïÑÎìú Î≤àÌò∏ 2Ïùò Î≥ÑÎ™Ö</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">ÌïÑÎìú Î≤àÌò∏ 3Ïùò Î≥ÑÎ™Ö</target>
        </trans-unit>
        <trans-unit id="c8d021883b0a18d7d212863e386fe9e4590e884d" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="50e05f8a73a607a4d8a0fae45b49062e73a2e308" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="47e1617de19cb0d4a15c8e0fcabf777055dd3bb7" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; Î≥ÑÏπ≠ .</target>
        </trans-unit>
        <trans-unit id="7d50263b9330ad372189eff67c5c8da79c613de2" translate="yes" xml:space="preserve">
          <source>All RNN modules accept packed sequences as inputs.</source>
          <target state="translated">Î™®Îì† RNN Î™®ÎìàÏùÄ Ìå®ÌÇπ Îêú ÏãúÌÄÄÏä§Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏïÑÎì§ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bcd80e614fd3639e154329146ec9d72471b9fa7b" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">Ïù¥ Î™®Îì† ÌÖêÏÑú &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; ÏûÖÎãàÎã§ &lt;code&gt;False&lt;/code&gt; Í¥ÄÎ°Ä Ïûé ÌÖêÏÑúÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fae256c49cab7960ddc648f6bae5910cd2d3503e" translate="yes" xml:space="preserve">
          <source>All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt;.</source>
          <target state="translated">ÏñëÏûêÌôî Îêú Î≤ÑÏ†ÑÏùÑ Ï†úÏô∏Ìïú Î™®Îì† TorchVision Î™®Îç∏ÏùÄ ONNXÎ°ú ÎÇ¥Î≥¥ÎÇº Ïàò ÏûàÏäµÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt; ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="5dce89529d95dc69f4c1f1ca05bfc2111d81383b" translate="yes" xml:space="preserve">
          <source>All arguments are forwarded to the &lt;code&gt;setuptools.Extension&lt;/code&gt; constructor.</source>
          <target state="translated">Î™®Îì† Ïù∏ÏàòÎäî &lt;code&gt;setuptools.Extension&lt;/code&gt; ÏÉùÏÑ±Ïûê Î°ú Ï†ÑÎã¨Îê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="7c7fdd1a2c6981ce3b8e5aa9cb3e2ddd838235f0" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; may contain additional names that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">Î™®Îì† Ï∞®Ïõê Ïù¥Î¶Ñ &lt;code&gt;self&lt;/code&gt; Ïóê ÏûàÏñ¥ÏïºÌï©ÎãàÎã§ &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; ÏóêÎäî &lt;code&gt;self.names&lt;/code&gt; Ïóê ÏóÜÎäî Ï∂îÍ∞Ä Ïù¥Î¶Ñ Ïù¥ Ìè¨Ìï®Îê† Ïàò ÏûàÏäµÎãàÎã§ . Ï∂úÎ†• ÌÖêÏÑúÎäî Í∞ÅÍ∞ÅÏùò ÏÉà Ïù¥Î¶ÑÏóê ÎåÄÌï¥ ÌÅ¨Í∏∞Í∞Ä 1 Ï∞®ÏõêÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1576ac96be05f9aa6b0251d2e542e4b0f488bd8b" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;code&gt;other.names&lt;/code&gt;. &lt;code&gt;other&lt;/code&gt; may contain named dimensions that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; Ïùò Î™®Îì† Ï∞®Ïõê Ïù¥Î¶ÑÏùÄ &lt;code&gt;other.names&lt;/code&gt; Ïóê ÏûàÏñ¥ÏïºÌï©ÎãàÎã§ . &lt;code&gt;other&lt;/code&gt; Îäî &lt;code&gt;self.names&lt;/code&gt; Ïóê ÏóÜÎäî Î™ÖÎ™Ö Îêú Ï∞®ÏõêÏùÑ Ìè¨Ìï® Ìï† Ïàò ÏûàÏäµÎãàÎã§ . Ï∂úÎ†• ÌÖêÏÑúÎäî Í∞ÅÍ∞ÅÏùò ÏÉà Ïù¥Î¶ÑÏóê ÎåÄÌï¥ ÌÅ¨Í∏∞Í∞Ä 1 Ï∞®ÏõêÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="91188b536d6c07689225e8b91d26aa659eef6d49" translate="yes" xml:space="preserve">
          <source>All elements must be greater than</source>
          <target state="translated">Î™®Îì† ÏöîÏÜåÎäî Îã§ÏùåÎ≥¥Îã§ Ïª§ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fa3a1109ed903de03a2630b0856e9124664ffc41" translate="yes" xml:space="preserve">
          <source>All functions must be valid TorchScript functions (including &lt;code&gt;__init__()&lt;/code&gt;).</source>
          <target state="translated">Î™®Îì† Ìï®ÏàòÎäî Ïú†Ìö®Ìïú TorchScript Ìï®Ïàò Ïó¨ÏïºÌï©ÎãàÎã§ ( &lt;code&gt;__init__()&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="8102d7ae18041c98a7f876840a19500755dde277" translate="yes" xml:space="preserve">
          <source>All modules, no matter their device, are always loaded onto the CPU during loading. This is different from &lt;a href=&quot;torch.load#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s semantics and may change in the future.</source>
          <target state="translated">Ïû•ÏπòÏóê Í¥ÄÍ≥ÑÏóÜÏù¥ Î™®Îì† Î™®ÎìàÏùÄÎ°úÎìúÌïòÎäî ÎèôÏïà Ìï≠ÏÉÅ CPUÏóêÎ°úÎìúÎê©ÎãàÎã§. Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;torch.load#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; Ïùò ÏùòÎØ∏ÏôÄ Îã§Î•¥Î©∞ Ìñ•ÌõÑ Î≥ÄÍ≤Ω Îê† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="391810af4c5adb65a3bbe96665345a54a154a4ff" translate="yes" xml:space="preserve">
          <source>All of &lt;code&gt;dims&lt;/code&gt; must be consecutive in order in the &lt;code&gt;self&lt;/code&gt; tensor, but not necessary contiguous in memory.</source>
          <target state="translated">Î™®Îì† &lt;code&gt;dims&lt;/code&gt; ÏùÄ &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑú ÏóêÏÑú ÏàúÏÑúÎåÄÎ°ú Ïó∞ÏÜçÎêòÏñ¥Ïïº ÌïòÏßÄÎßå Î©îÎ™®Î¶¨ÏóêÏÑú Ïó∞ÏÜçÏ†Å Ïùº ÌïÑÏöîÎäî ÏóÜÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6c24f14499bb0bd88c18f05a4fe7875666f28f6d" translate="yes" xml:space="preserve">
          <source>All of the dims of &lt;code&gt;self&lt;/code&gt; must be named in order to use this method. The resulting tensor is a view on the original tensor.</source>
          <target state="translated">Ïù¥ Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌïòÎ†§Î©¥ &lt;code&gt;self&lt;/code&gt; Ïùò Î™®Îì† Ìù¨ÎØ∏Ìïú Ïù¥Î¶ÑÏùÑ ÏßÄÏ†ïÌï¥ÏïºÌï©ÎãàÎã§. Í≤∞Í≥º ÌÖêÏÑúÎäî ÏõêÎûò ÌÖêÏÑúÏùò Î∑∞ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8d9325f5f71bde68b50e39d98dd2b2fc4b9b3f89" translate="yes" xml:space="preserve">
          <source>All operations that support named tensors propagate names.</source>
          <target state="translated">Î™ÖÎ™Ö Îêú ÌÖêÏÑúÎ•º ÏßÄÏõêÌïòÎäî Î™®Îì† ÏûëÏóÖÏùÄ Ïù¥Î¶ÑÏùÑ Ï†ÑÌååÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c773fd48da7cc8c1f4621a11671c67f1f6303a0c" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. You can use the following transform to normalize:</source>
          <target state="translated">Î™®Îì† ÏÇ¨Ï†Ñ ÌõàÎ†® Îêú Î™®Îç∏ÏùÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ï†ïÍ∑úÌôî Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎ•º Í∏∞ÎåÄÌï©ÎãàÎã§. [0, 1] Î≤îÏúÑÎ°úÎ°úÎìú Îêú Îã§Ïùå &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; Î∞è &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôîÎê©ÎãàÎã§ . Îã§Ïùå Î≥ÄÌôòÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôî Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ec2e3e9eabe115169e1deab63808ca48a84df052" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; and &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt;.</source>
          <target state="translated">ÏÇ¨Ï†Ñ ÌõàÎ†® Îêú Î™®Îì† Î™®Îç∏ÏùÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ï†ïÍ∑úÌôî Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎ•º Í∏∞ÎåÄÌï©ÎãàÎã§. Ï¶â, HÏôÄ WÎäî 112Í∞Ä Îê† Í≤ÉÏúºÎ°ú ÏòàÏÉÅÎêòÎäî ÌòïÌÉú (3 x T x H x W)Ïùò 3 Ï±ÑÎÑê RGB ÎπÑÎîîÏò§ ÎØ∏Îãà Î∞∞Ïπò, TÎäî ÌÅ¥Î¶ΩÏùò ÎπÑÎîîÏò§ ÌîÑÎ†àÏûÑ Ïàò. Ïù¥ÎØ∏ÏßÄÎäî [0, 1] Î≤îÏúÑÎ°úÎ°úÎìú Ìïú Îã§Ïùå &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; Î∞è &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt; ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôîÌï¥Ïïº Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7463404aa8619413edebf6ff4257cb99add42cba" translate="yes" xml:space="preserve">
          <source>All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn&amp;rsquo;t have certain devices), an exception is raised.</source>
          <target state="translated">Ïù¥Ï†ÑÏóê Ï†ÄÏû• Ìïú Î™®Îì† Î™®ÎìàÏùÄ Ïû•ÏπòÏóê Í¥ÄÍ≥ÑÏóÜÏù¥ Î®ºÏ†Ä CPUÏóêÎ°úÎìú Îêú Îã§Ïùå Ï†ÄÏû•Îêú Ïû•ÏπòÎ°ú Ïù¥ÎèôÎê©ÎãàÎã§. Ïù¥Í≤ÉÏù¥ Ïã§Ìå®ÌïòÎ©¥ (Ïòà : Îü∞ÌÉÄÏûÑ ÏãúÏä§ÌÖúÏóê ÌäπÏ†ï Ïû•ÏπòÍ∞Ä ÏóÜÍ∏∞ ÎïåÎ¨∏Ïóê) ÏòàÏô∏Í∞Ä Î∞úÏÉùÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f4fb70a59932061a3d80fc551911560fa69384f6" translate="yes" xml:space="preserve">
          <source>All summed &lt;code&gt;dim&lt;/code&gt; are squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting an output tensor having &lt;code&gt;dim&lt;/code&gt; fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Î™®Îì† Ìï©ÏÇ∞ Îêú &lt;code&gt;dim&lt;/code&gt; Ïù¥ ÏïïÏ∂ïÎêòÏñ¥ ( &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt; Ï∞∏Ï°∞ ) Ï∂úÎ†• ÌÖêÏÑú Í∞Ä &lt;code&gt;input&lt;/code&gt; Î≥¥Îã§ &lt;code&gt;dim&lt;/code&gt; Îçî Ï†ÅÏùÄ Ï∞®ÏõêÏùÑ Í∞ñÍ≤å Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6bd30243e5faf7879695a6c25ee7d928afafccc6" translate="yes" xml:space="preserve">
          <source>All tensors need to be of the same size.</source>
          <target state="translated">Î™®Îì† ÌÖêÏÑúÎäî ÌÅ¨Í∏∞Í∞Ä Í∞ôÏïÑÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="647c455337c3b30ca33d7810c4e9c4f5a9641226" translate="yes" xml:space="preserve">
          <source>All the weights and biases are initialized from</source>
          <target state="translated">Î™®Îì† Í∞ÄÏ§ëÏπòÏôÄ Ìé∏Ìñ•ÏùÄ Îã§ÏùåÏóêÏÑú Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="970c0ae90b9eb34e0cb6a356cf47964705868146" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces.</source>
          <target state="translated">Î™®Îç∏Ïù¥ Îã§Î•∏ ÌëúÌòÑ Î∂ÄÎ∂Ñ Í≥µÍ∞ÑÏùò Ï†ïÎ≥¥Ïóê Í≥µÎèôÏúºÎ°ú Ï∞∏Ïó¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5702cc92d2bcf7e780dcdf99f53eee4b5e88684e" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need</source>
          <target state="translated">Î™®Îç∏Ïù¥ Îã§Î•∏ ÌëúÌòÑ Î∂ÄÎ∂Ñ Í≥µÍ∞ÑÏùò Ï†ïÎ≥¥Ïóê Í≥µÎèôÏúºÎ°ú Ï∞∏Ïó¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ï∞∏Ï°∞ Ï∞∏Ï°∞ :Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌïú Î™®Îì† Í≤É</target>
        </trans-unit>
        <trans-unit id="bcb917dff27ba02ff781ce514dcf9c4e2df73ba5" translate="yes" xml:space="preserve">
          <source>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</source>
          <target state="translated">Alpha DropoutÏùÄ ÏûêÏ≤¥ Ï†ïÍ∑úÌôî ÏÜçÏÑ±ÏùÑ Ïú†ÏßÄÌïòÎäî Dropout Ïú†ÌòïÏûÖÎãàÎã§. ÌèâÍ∑†Ïù¥ 0Ïù¥Í≥† Îã®ÏúÑ ÌëúÏ§Ä Ìé∏Ï∞®Í∞ÄÏûàÎäî ÏûÖÎ†•Ïùò Í≤ΩÏö∞ ÏïåÌåå ÎìúÎ°≠ ÏïÑÏõÉÏùò Ï∂úÎ†•ÏùÄ ÏûÖÎ†•Ïùò ÏõêÎûò ÌèâÍ∑†Í≥º ÌëúÏ§Ä Ìé∏Ï∞®Î•º Ïú†ÏßÄÌï©ÎãàÎã§. Alpha DropoutÏùÄ SELU ÌôúÏÑ±Ìôî Í∏∞Îä•Í≥º Ìï®Íªò ÏÇ¨Ïö©ÎêòÏñ¥ Ï∂úÎ†•Ïùò ÌèâÍ∑† Î∞è Îã®ÏúÑ ÌëúÏ§Ä Ìé∏Ï∞®Í∞Ä 0Ïù¥ÎêòÎèÑÎ°ùÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="468dc142cbb278c344ed57a0c2341d7dabf6044d" translate="yes" xml:space="preserve">
          <source>AlphaDropout</source>
          <target state="translated">AlphaDropout</target>
        </trans-unit>
        <trans-unit id="697ae46b13c17cf9f965bf8eeb7388f2f2599380" translate="yes" xml:space="preserve">
          <source>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">ÎòêÌïú Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌõàÎ†® Ï§ëÏóêÏù¥ Í≥ÑÏ∏µÏùÄ Í≥ÑÏÇ∞ Îêú ÌèâÍ∑† Î∞è Î∂ÑÏÇ∞Ïùò Ï∂îÏ†ïÏπòÎ•º Í≥ÑÏÜç Ïã§ÌñâÌïòÎ©∞, ÌèâÍ∞Ä Ï§ëÏóê Ï†ïÍ∑úÌôîÏóê ÏÇ¨Ïö©Îê©ÎãàÎã§. Ïã§Ìñâ Ï∂îÏ†ïÏπòÎäî Í∏∞Î≥∏ &lt;code&gt;momentum&lt;/code&gt; 0.1 Î°ú Ïú†ÏßÄÎê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="8bf20b1915a4ba9029b2263ed8aac08cdcf55753" translate="yes" xml:space="preserve">
          <source>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</source>
          <target state="translated">Ïû•Ïãù Ïûê Ïó≠Ìï†ÎèÑÌï©ÎãàÎã§. (Í¥ÑÌò∏Î°ú Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌï¥ÏïºÌï©ÎãàÎã§.)</target>
        </trans-unit>
        <trans-unit id="51924beeca0c98eba05c19c63804e09424b9e355" translate="yes" xml:space="preserve">
          <source>Also known as Glorot initialization.</source>
          <target state="translated">Glorot Ï¥àÍ∏∞ÌôîÎùºÍ≥†ÎèÑÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="99d08174dc1487c12f409bf77bbf4a6a16dc84ad" translate="yes" xml:space="preserve">
          <source>Also known as He initialization.</source>
          <target state="translated">He Ï¥àÍ∏∞ÌôîÎùºÍ≥†ÎèÑÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9319f57915da9aa68da62d35edfd64135450d0d5" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(input_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;input_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">ÎòêÌïú Ïú†Ïùò &lt;code&gt;len(input_tensor_lists)&lt;/code&gt; ÏôÄ Í∞Å ÏöîÏÜåÏùò ÌÅ¨Í∏∞ &lt;code&gt;input_tensor_lists&lt;/code&gt; (Í∞Å ÏöîÏÜå Î™©Î°ùÏù¥Î©∞, Îî∞ÎùºÏÑú &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt; )Ïù¥ Ìï®ÏàòÎ•º Ìò∏Ï∂ú Î™®Îì† Î∂ÑÏÇ∞ ÌîÑÎ°úÏÑ∏Ïä§Ïóê ÎåÄÌï¥ ÎèôÏùºÌï¥ÏïºÌïúÎã§.</target>
        </trans-unit>
        <trans-unit id="3a0c3f7b9176b92a14d4b6b5cc0f2c401a31947c" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(output_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;output_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">ÎòêÌïú Ïú†Ïùò &lt;code&gt;len(output_tensor_lists)&lt;/code&gt; ÏôÄ Í∞Å ÏöîÏÜåÏùò ÌÅ¨Í∏∞ &lt;code&gt;output_tensor_lists&lt;/code&gt; (Í∞Å ÏöîÏÜå Î™©Î°ùÏù¥Î©∞, Îî∞ÎùºÏÑú &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt; )Ïù¥ Ìï®ÏàòÎ•º Ìò∏Ï∂ú Î™®Îì† Î∂ÑÏÇ∞ ÌîÑÎ°úÏÑ∏Ïä§Ïóê ÎåÄÌï¥ ÎèôÏùºÌï¥ÏïºÌïúÎã§.</target>
        </trans-unit>
        <trans-unit id="043751522414e7c9aaee8aec5948d07968c78350" translate="yes" xml:space="preserve">
          <source>Although the recipe for forward pass needs to be defined within this function, one should call the &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</source>
          <target state="translated">Ìè¨ÏõåÎìú Ìå®Ïä§Î•ºÏúÑÌïú Î†àÏãúÌîºÎäîÏù¥ Ìï®Ïàò ÎÇ¥ÏóêÏÑú Ï†ïÏùòÎêòÏñ¥Ïïº ÌïòÏßÄÎßå, Ï†ÑÏûêÎäî Îì±Î°ù Îêú ÌõÑÌÅ¨Î•º Ïã§ÌñâÌïòÍ≥† ÌõÑÏûêÎäî ÏûêÎèôÏúºÎ°ú Î¨¥ÏãúÌïòÎØÄÎ°ú ÎÇòÏ§ëÏóê &lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; Ïù∏Ïä§ÌÑ¥Ïä§Î•º Ìò∏Ï∂úÌï¥Ïïº Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a16ed36f1048b1b7e767b2e1655943a9aa7dfbb6" translate="yes" xml:space="preserve">
          <source>An &lt;code&gt;RRef&lt;/code&gt; (Remote REFerence) is a reference to a value of some type &lt;code&gt;T&lt;/code&gt; (e.g. &lt;code&gt;Tensor&lt;/code&gt;) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules&lt;/a&gt; that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;Remote Reference Protocol&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;RRef&lt;/code&gt; (ÏõêÍ≤© Ï∞∏Ï°∞)Îäî Ïñ¥Îñ§ Ïú†ÌòïÏùò Í∞íÏóê ÎåÄÌïú Ï∞∏Ï°∞ Ïù∏ &lt;code&gt;T&lt;/code&gt; (Ïòà &lt;code&gt;Tensor&lt;/code&gt; ÏõêÍ≤© ÏûëÏóÖÏûê). Ïù¥ Ìï∏Îì§ÏùÄ Ï∞∏Ï°∞ Îêú ÏõêÍ≤© Í∞íÏùÑ ÏÜåÏú†ÏûêÏóêÍ≤å Ïú†ÏßÄÌïòÏßÄÎßå ÎÇòÏ§ëÏóê Í∞íÏù¥ Î°úÏª¨ ÏûëÏóÖÏûêÏóêÍ≤å Ï†ÑÏÜ°ÎêúÎã§Îäî ÏùòÎØ∏Îäî ÏóÜÏäµÎãàÎã§. RRefÎäî Îã§Î•∏ ÏõåÏª§Ïóê Ï°¥Ïû¨ ÌïòÎäî &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.ModulesÏóê&lt;/a&gt; ÎåÄÌïú Ï∞∏Ï°∞Î•º Î≥¥Ïú† ÌïòÍ≥† Ï†ÅÏ†àÌïú Ìï®ÏàòÎ•º Ìò∏Ï∂úÌïòÏó¨ ÌõàÎ†® Ï§ëÏóê Îß§Í∞ú Î≥ÄÏàòÎ•º Í≤ÄÏÉâÌïòÍ±∞ÎÇò ÏàòÏ†ï Ìï®ÏúºÎ°úÏç® Îã§Ï§ë Î®∏Ïã† ÌõàÎ†®Ïóê ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§ . ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;ÏõêÍ≤© Ï∞∏Ï°∞ ÌîÑÎ°úÌÜ†ÏΩú&lt;/a&gt; ÏùÑ Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="366e57ddf274ce33b825366c8f324bc7720a8851" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity.</source>
          <target state="translated">tanh ÎòêÎäî ReLU ÎπÑÏÑ†Ìòï ÏÑ±Ïù¥ÏûàÎäî Elman RNN ÏÖÄ.</target>
        </trans-unit>
        <trans-unit id="cda80fd86e845c48c7faf0e81a98aca3de57ab49" translate="yes" xml:space="preserve">
          <source>An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; in order to initialize RPC with specific configurations, such as the RPC timeout and &lt;code&gt;init_method&lt;/code&gt; to be used.</source>
          <target state="translated">RPC Î∞±ÏóîÎìúÎ°ú Ï†ÑÎã¨ Îêú ÏòµÏÖòÏùÑ Ï∫°ÏäêÌôîÌïòÎäî Ï∂îÏÉÅ Íµ¨Ï°∞ÏûÖÎãàÎã§. Ïù¥ ÌÅ¥ÎûòÏä§Ïùò Ïù∏Ïä§ÌÑ¥Ïä§Îäî ÏÇ¨Ïö©Ìï† RPC ÏãúÍ∞Ñ Ï†úÌïú Î∞è &lt;code&gt;init_method&lt;/code&gt; ÏôÄ Í∞ôÏùÄ ÌäπÏ†ï Íµ¨ÏÑ±ÏúºÎ°ú RPCÎ•º Ï¥àÍ∏∞ÌôîÌïòÍ∏∞ ÏúÑÌï¥ &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; Ïóê Ï†ÑÎã¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="34f08a969a81be9fad81ba44cfa1144a898067e4" translate="yes" xml:space="preserve">
          <source>An additional dimension of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; is appended in the returned tensor.</source>
          <target state="translated">ÌÅ¨Í∏∞ &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; Ïùò Ï∂îÍ∞Ä Ï∞®ÏõêÏù¥ Î∞òÌôò Îêú ÌÖêÏÑúÏóê Ï∂îÍ∞ÄÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="619ad847534358910127729afc029b7953abcecf" translate="yes" xml:space="preserve">
          <source>An empty dict is assumed have type &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. The types of other dict literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">Îπà dictÎäî &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; Ïú†ÌòïÏùÑ Í∞ñÎäîÎã§ Í≥† Í∞ÄÏ†ï Ìï©ÎãàÎã§. Îã§Î•∏ dict Î¶¨ÌÑ∞Îü¥Ïùò Ïú†ÌòïÏùÄ Î©§Î≤Ñ Ïú†ÌòïÏóêÏÑú ÌååÏÉùÎê©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;#default-types&quot;&gt;Í∏∞Î≥∏ Ïú†Ìòï&lt;/a&gt; ÏùÑ Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="c1a0eecf8a5a6cdc72f4e0da994b510481da5a4d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed have type &lt;code&gt;List[Tensor]&lt;/code&gt;. The types of other list literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">Îπà Î™©Î°ùÏùÄ &lt;code&gt;List[Tensor]&lt;/code&gt; Ïú†ÌòïÏù¥ ÏûàÎã§Í≥† Í∞ÄÏ†ï Ìï©ÎãàÎã§. Îã§Î•∏ Î™©Î°ù Î¶¨ÌÑ∞Îü¥Ïùò Ïú†ÌòïÏùÄ Î©§Î≤Ñ Ïú†ÌòïÏóêÏÑú ÌååÏÉùÎê©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;#default-types&quot;&gt;Í∏∞Î≥∏ Ïú†Ìòï&lt;/a&gt; ÏùÑ Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="d84c564cad7cf9fb4e3caa453c3a49e8beeae54d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed to be &lt;code&gt;List[Tensor]&lt;/code&gt; and empty dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. To instantiate an empty list or dict of other types, use &lt;code&gt;Python 3 type hints&lt;/code&gt;.</source>
          <target state="translated">Îπà Î™©Î°ùÏùÄ &lt;code&gt;List[Tensor]&lt;/code&gt; Ïù¥Í≥† Îπà dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; ÎùºÍ≥† Í∞ÄÏ†ïÌï©ÎãàÎã§ . Îπà Î™©Î°ù ÎòêÎäî Îã§Î•∏ Ïú†ÌòïÏùò ÏÇ¨Ï†ÑÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÎ†§Î©¥ &lt;code&gt;Python 3 type hints&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc24c0a7e765c257ec2c1deb32cf9b9b1bc99fd5" translate="yes" xml:space="preserve">
          <source>An empty sparse tensor can be constructed by specifying its size:</source>
          <target state="translated">Îπà Ìù¨ÏÜå ÌÖêÏÑúÎäî ÌÅ¨Í∏∞Î•º ÏßÄÏ†ïÌïòÏó¨ ÏÉùÏÑ± Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="17f91bb7684b57d410e71c667849b986b1e265c6" translate="yes" xml:space="preserve">
          <source>An enum class of available backends.</source>
          <target state="translated">ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∞±ÏóîÎìúÏùò Ïó¥Í±∞ Ìòï ÌÅ¥ÎûòÏä§ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a60c11218c3c0643831cc9fba424f6f68723b504" translate="yes" xml:space="preserve">
          <source>An enum-like class for available reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt;.</source>
          <target state="translated">ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ï∂ïÏÜå ÏûëÏóÖÏóê ÎåÄÌïú Ïó¥Í±∞ Ìòï ÌÅ¥ÎûòÏä§ : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; , &lt;code&gt;MAX&lt;/code&gt; , &lt;code&gt;BAND&lt;/code&gt; , &lt;code&gt;BOR&lt;/code&gt; Î∞è &lt;code&gt;BXOR&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c5d6691fbcb10d6ab33410657c5bcc031006a7f4" translate="yes" xml:space="preserve">
          <source>An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.</source>
          <target state="translated">ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î∞±ÏóîÎìúÏùò Ïó¥Í±∞ Ìòï ÌÅ¥ÎûòÏä§ : GLOO, NCCL, MPI Î∞è Í∏∞ÌÉÄ Îì±Î°ù Îêú Î∞±ÏóîÎìú.</target>
        </trans-unit>
        <trans-unit id="519ac03d4f41ca6837697ea6c98bf8c921545893" translate="yes" xml:space="preserve">
          <source>An example of such normalization can be found in the imagenet example &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;here&lt;/a&gt;</source>
          <target state="translated">Ïù¥Îü¨Ìïú Ï†ïÍ∑úÌôîÏùò ÏòàÎäî &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;Ïó¨Í∏∞&lt;/a&gt; Ïù¥ÎØ∏ÏßÄ ÎÑ∑ Ïòà ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="44802d0d5256af16bcf81baadaa0893f0644e7bc" translate="yes" xml:space="preserve">
          <source>An integral output tensor cannot accept a floating point tensor.</source>
          <target state="translated">Ï†ÅÎ∂Ñ Ï∂úÎ†• ÌÖêÏÑúÎäî Î∂ÄÎèô ÏÜåÏàòÏ†ê ÌÖêÏÑúÎ•º Î∞õÏïÑ Îì§Ïùº Ïàò ÏóÜÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ada1c75faa68087ab2e8ec52c71f9a5f98fa8946" translate="yes" xml:space="preserve">
          <source>An torch.Generator object.</source>
          <target state="translated">torch.Generator Í∞úÏ≤¥ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ef94106a43404d867533b831248eaa8963699ea0" translate="yes" xml:space="preserve">
          <source>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired &lt;code&gt;world_size&lt;/code&gt;. The URL should start with &lt;code&gt;file://&lt;/code&gt; and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn&amp;rsquo;t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; call on the same file path/name.</source>
          <target state="translated">Îòê Îã§Î•∏ Ï¥àÍ∏∞Ìôî Î∞©Î≤ïÏùÄ ÏõêÌïòÎäî &lt;code&gt;world_size&lt;/code&gt; ÏôÄ Ìï®Íªò Í∑∏Î£πÏùò Î™®Îì† ÏãúÏä§ÌÖúÏóêÏÑú Í≥µÏú†ÎêòÍ≥† Î≥º ÏàòÏûàÎäî ÌååÏùº ÏãúÏä§ÌÖúÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§ . URLÏùÄ &lt;code&gt;file://&lt;/code&gt; Î°ú ÏãúÏûëÌï¥Ïïº ÌïòÎ©∞ Í≥µÏú† ÌååÏùº ÏãúÏä§ÌÖúÏóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî ÌååÏùº (Í∏∞Ï°¥ ÎîîÎ†âÌÜ†Î¶¨Ïóê ÏûàÏùå)Ïóê ÎåÄÌïú Í≤ΩÎ°úÎ•º Ìè¨Ìï® Ìï¥Ïïº Ìï©ÎãàÎã§. ÌååÏùº ÏãúÏä§ÌÖú Ï¥àÍ∏∞ÌôîÎäî ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞ ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±ÎêòÏßÄÎßå ÌååÏùºÏùÑ ÏÇ≠Ï†úÌïòÏßÄÎäî ÏïäÏäµÎãàÎã§. Îî∞ÎùºÏÑú ÎèôÏùºÌïú ÌååÏùº Í≤ΩÎ°ú / Ïù¥Î¶ÑÏóê ÎåÄÌïú Îã§Ïùå &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; Ìò∏Ï∂ú Ï†ÑÏóê ÌååÏùºÏù¥ Ï†ïÎ¶¨ÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÎäî Í≤ÉÏùÄ ÏÇ¨Ïö©ÏûêÏùò Ï±ÖÏûÑ ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1eec009a2337c1a488dd1b74c1aff7cc1d139bf1" translate="yes" xml:space="preserve">
          <source>Any other functionality from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module not explitily listed in this documentation is unsupported.</source>
          <target state="translated">Ïù¥ Î¨∏ÏÑúÏóê Î™ÖÏãú Ï†ÅÏúºÎ°ú ÎÇòÏó¥ÎêòÏßÄ ÏïäÏùÄ &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; Î™®ÎìàÏùò Îã§Î•∏ Í∏∞Îä• ÏùÄ ÏßÄÏõêÎêòÏßÄ ÏïäÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9267fbcc5ac03c112f491ee9bc7bbb21bb5ae1c5" translate="yes" xml:space="preserve">
          <source>Append the given callback function to this &lt;code&gt;Future&lt;/code&gt;, which will be run when the &lt;code&gt;Future&lt;/code&gt; is completed. Multiple callbacks can be added to the same &lt;code&gt;Future&lt;/code&gt;, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this &lt;code&gt;Future&lt;/code&gt;. The callback function can use the &lt;code&gt;Future.wait()&lt;/code&gt; API to get the value.</source>
          <target state="translated">&lt;code&gt;Future&lt;/code&gt; Í∞Ä ÏôÑÎ£åÎêòÎ©¥ Ïã§ÌñâÎê† Ïù¥ &lt;code&gt;Future&lt;/code&gt; Ïóê Ï£ºÏñ¥ÏßÑ ÏΩúÎ∞± Ìï®ÏàòÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§ . ÎèôÏùºÌïú &lt;code&gt;Future&lt;/code&gt; Ïóê Ïó¨Îü¨ ÏΩúÎ∞±ÏùÑ Ï∂îÍ∞Ä Ìï† Ïàò ÏûàÏúºÎ©∞ Ï∂îÍ∞Ä Îêú Í≤ÉÍ≥º ÎèôÏùºÌïú ÏàúÏÑúÎ°ú Ìò∏Ï∂úÎê©ÎãàÎã§. ÏΩúÎ∞±ÏùÄÏù¥ &lt;code&gt;Future&lt;/code&gt; Ïóê ÎåÄÌïú Ï∞∏Ï°∞ Ïù∏ ÌïòÎÇòÏùò Ïù∏ÏàòÎ•º Í∞ÄÏ†∏ÏïºÌï©ÎãàÎã§ . ÏΩúÎ∞± Ìï®ÏàòÎäî &lt;code&gt;Future.wait()&lt;/code&gt; APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í∞íÏùÑ Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f3f60044b06335eb6c4230e3c39f5e0218445878" translate="yes" xml:space="preserve">
          <source>Appendix</source>
          <target state="translated">Appendix</target>
        </trans-unit>
        <trans-unit id="c87f37c9f3693b628e726dc6f101341b952d29db" translate="yes" xml:space="preserve">
          <source>Appends a given module to the end of the list.</source>
          <target state="translated">Î™©Î°ù ÎÅùÏóê Ï£ºÏñ¥ÏßÑ Î™®ÎìàÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="379011982516fbdbffaea6138d56d674a94f5e18" translate="yes" xml:space="preserve">
          <source>Appends a given parameter at the end of the list.</source>
          <target state="translated">Î™©Î°ù ÎÅùÏóê Ï£ºÏñ¥ÏßÑ Îß§Í∞ú Î≥ÄÏàòÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9b249ec6f74bf6c17017438ecc23770483a688cc" translate="yes" xml:space="preserve">
          <source>Appends modules from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterableÏùò Î™®ÎìàÏùÑ Î™©Î°ù ÎÅùÏóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0e68b1f2c86ef40318501ff9b078b446cee7c813" translate="yes" xml:space="preserve">
          <source>Appends parameters from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterableÏùò Îß§Í∞ú Î≥ÄÏàòÎ•º Î™©Î°ù ÎÅùÏóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="378aa8a7eda72c52086938707b31878d8a635ee4" translate="yes" xml:space="preserve">
          <source>Applied element-wise, as:</source>
          <target state="translated">Îã§ÏùåÍ≥º Í∞ôÏù¥ ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="47ea67db7d24ea56bb6efc03577734231d9fa58e" translate="yes" xml:space="preserve">
          <source>Applies 2D average-pooling operation in</source>
          <target state="translated">2D ÌèâÍ∑† ÌíÄÎßÅ ÏûëÏóÖÏùÑ</target>
        </trans-unit>
        <trans-unit id="26a3eb8c50568931de48e1e7fa354baa87e37c44" translate="yes" xml:space="preserve">
          <source>Applies 3D average-pooling operation in</source>
          <target state="translated">3D ÌèâÍ∑† ÌíÄÎßÅ ÏûëÏóÖÏùÑ</target>
        </trans-unit>
        <trans-unit id="897491bacd3642b7742fd85de111604a2b1e2139" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;callable&lt;/code&gt; for each element in &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; and stores the results in &lt;code&gt;self&lt;/code&gt; tensor. &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; ÌÖêÏÑú Î∞è Ï£ºÏñ¥ÏßÑ &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; Í∞Å ÏöîÏÜåÏóê ÎåÄÌï¥ &lt;code&gt;callable&lt;/code&gt; ÏùÑ Ï†ÅÏö© ÌïòÍ≥† Í≤∞Í≥ºÎ•º &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑú Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§ . &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑú Î∞è Ï£ºÏñ¥ÏßÑ &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; Îäî &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Î∏åÎ°úÎìú Ï∫êÏä§ÌåÖ Í∞ÄÎä•&lt;/a&gt; Ìï¥ÏïºÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="854151a01bba98f938fa936327548dc77fb47b3f" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;fn&lt;/code&gt; recursively to every submodule (as returned by &lt;code&gt;.children()&lt;/code&gt;) as well as self. Typical use includes initializing the parameters of a model (see also &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt;).</source>
          <target state="translated">selfÎøêÎßå ÏïÑÎãàÎùº Î™®Îì† ÌïòÏúÑ Î™®Îìà ( &lt;code&gt;.children()&lt;/code&gt; ÏùòÌï¥ Î∞òÌôò Îê® )Ïóê &lt;code&gt;fn&lt;/code&gt; ÏùÑ Ïû¨Í∑Ä Ï†ÅÏúºÎ°ú Ï†ÅÏö© Ìï©ÎãàÎã§. ÏùºÎ∞òÏ†ÅÏù∏ ÏÇ¨Ïö©ÏóêÎäî Î™®Îç∏Ïùò Îß§Í∞ú Î≥ÄÏàò Ï¥àÍ∏∞ÌôîÍ∞Ä Ìè¨Ìï®Îê©ÎãàÎã§ ( &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt; Ï∞∏Ï°∞ ).</target>
        </trans-unit>
        <trans-unit id="f3ce1d7a64849414112d779b273ff91d45596516" translate="yes" xml:space="preserve">
          <source>Applies Alpha Dropout over the input.</source>
          <target state="translated">ÏûÖÎ†•Ïóê ÏïåÌåå ÎìúÎ°≠ ÏïÑÏõÉÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="13e7aada5fd7c3d78fcf6b9c0d54d77028725d8c" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization for each channel across a batch of data.</source>
          <target state="translated">Îç∞Ïù¥ÌÑ∞ Î∞∞ÏπòÏóêÏÑú Í∞Å Ï±ÑÎÑêÏóê ÎåÄÌï¥ Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="29f9114f73a8742a5df31f7a4732b3ef2d2fa5ed" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; ÎÖºÎ¨∏Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 2D ÎòêÎäî 3D ÏûÖÎ†• (ÏÑ†ÌÉù ÏÇ¨Ìï≠ Ïù∏ Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 1D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="77df134b3c1dec023ae7933dc984fdf0b9a37d0b" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; ÎÖºÎ¨∏Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 4D ÏûÖÎ†• (Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 2D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="d5008d35280faa701906a5c7bb40531dff590ddb" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; ÎÖºÎ¨∏Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 5D ÏûÖÎ†• (Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 3D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="ebba8e6ee8c679b795f34839517e1330fe0f6cd0" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">Î¨∏ÏÑú Batch Normalization &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; Î¨∏ÏÑúÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú N Ï∞®Ïõê ÏûÖÎ†• (Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî [N-2] D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="4c0b063a3d01c57a39497f55e2c1c93c68f9edde" translate="yes" xml:space="preserve">
          <source>Applies Group Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;Group Normalization&lt;/a&gt;</source>
          <target state="translated">ÎÖºÎ¨∏ &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;Í∑∏Î£π Ï†ïÍ∑úÌôîÏóê&lt;/a&gt; ÏÑ§Î™Ö ÎêúÎåÄÎ°ú ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞ÏπòÏóê ÎåÄÌï¥ Í∑∏Î£π Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6ddcab60a314dad6ac4e8205f6d0bbe42d633f04" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization for each channel in each data sample in a batch.</source>
          <target state="translated">ÏùºÍ¥Ñ Ï†ÅÏúºÎ°ú Í∞Å Îç∞Ïù¥ÌÑ∞ ÏÉòÌîåÏùò Í∞Å Ï±ÑÎÑêÏóê ÎåÄÌï¥ Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8eba97c1edce80ebe5a8f2d8c4ea438567d2707c" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; Î¨∏ÏÑúÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 3D ÏûÖÎ†• (ÏÑ†ÌÉù ÏÇ¨Ìï≠ Ïù∏ Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 1D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="1c1378db79ff8ef5bf285d891bdde624c5ed8210" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; Î¨∏ÏÑúÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 4D ÏûÖÎ†• (Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 2D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="331da363e42c71d65e839afe96ad5dce770a5331" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; Î¨∏ÏÑúÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú 5D ÏûÖÎ†• (Ï∂îÍ∞Ä Ï±ÑÎÑê Ï∞®ÏõêÏù¥ÏûàÎäî 3D ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞Ïπò)Ïóê Ïù∏Ïä§ÌÑ¥Ïä§ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b97189b49bbea1acf1c1f0194b9ab27dea9973b9" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization for last certain number of dimensions.</source>
          <target state="translated">ÎßàÏßÄÎßâ ÌäπÏ†ï ÏàòÏùò Ï∞®ÏõêÏóê ÎåÄÌï¥ Î†àÏù¥Ïñ¥ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="835e00e544806b9f13082cf66ead874e6019f00b" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt;</source>
          <target state="translated">Î¨∏ÏÑú Î†àÏù¥Ïñ¥ Ï†ïÍ∑úÌôîÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú ÏûÖÎ†•Ïùò ÎØ∏Îãà Î∞∞ÏπòÏóê ÎåÄÌï¥ &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Î†àÏù¥Ïñ¥ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aeb29e205e308a59093be0d4988fa5cd74bac086" translate="yes" xml:space="preserve">
          <source>Applies SoftMax over features to each spatial location.</source>
          <target state="translated">Í∏∞Îä•Ïóê SoftMaxÎ•º Í∞Å Í≥µÍ∞Ñ ÏúÑÏπòÏóê Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="68727ddc338094e31b998534ceb0a7cd1e1f8321" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 1D Ï†ÅÏùë Ìòï ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0d6edcf464a5fa2406d025e84b2ed153de9c75ee" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 1D Ï†ÅÏùë Ìòï ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b8f3289d5daff81c1311cb6f2ca1629e42cafa4f" translate="yes" xml:space="preserve">
          <source>Applies a 1D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 1D ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3c27388a69cba70f821eaba1e90c8524f7a152d3" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized 1D input composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú 1D ÏûÖÎ†•Ïóê 1D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="322ebd47033ac7c9c030a4056a89e60da48e7a7e" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏñëÏûêÌôî Îêú ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 1D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="dcfb7d54383b0ff98c06cfb1f40c5d85baf1b8d7" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 1D ÌöåÏÑ†ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ba598dc972597d5f42702090e4175518b59bd582" translate="yes" xml:space="preserve">
          <source>Applies a 1D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 1D ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6e7aac7c6517399757e4cbb358ae6fceeed56898" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 1D Ï†ÑÎ†• ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8bfc842196843fc0db2f2d55709b870df2c8219f" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 1D Ï†ÑÎ†• ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§. &lt;code&gt;p&lt;/code&gt; Ïùò Í±∞Îì≠ Ï†úÍ≥±Ïóê ÎåÄÌïú Î™®Îì† ÏûÖÎ†•Ïùò Ìï© Ïù¥ 0Ïù¥Î©¥ Í∏∞Ïö∏Í∏∞ÎèÑ 0ÏúºÎ°ú ÏÑ§Ï†ïÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="96ca07ea2e411b1ba9e96092e82956074c9632be" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 1D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4b460f2239dd693e787bc347edd22db345631a88" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">&quot;ÎîîÏª® Î≥º Î£®ÏÖò&quot;Ïù¥ÎùºÍ≥†ÎèÑÌïòÎäî Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 1D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="aa89da022829347e9eb20aeec1fec7371b510238" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏñëÏûêÌôî Îêú ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ï†ÅÏùë Ìòï ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2990176eebf3f395560769ef6ba618123ba3cbdc" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ï†ÅÏùë Ìòï ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8f9f8906e50a1a40da69b6542725773d5f97b476" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ï†ÅÏùë Ìòï ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d576b7f4a24ec8fd4a5244faa85b001a714e4b2b" translate="yes" xml:space="preserve">
          <source>Applies a 2D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4eef15eebd1e3c58ec128ae4ac04428c46e00ff5" translate="yes" xml:space="preserve">
          <source>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• Ï±ÑÎÑêÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ïù¥Ï§ë ÏÑ†Ìòï ÏóÖ ÏÉòÌîåÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="47468ad50007cdb83f448361561cc024fef32584" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized 2D input composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú 2D ÏûÖÎ†•Ïóê 2D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3f8605d5715138e74b1fe2f63f6110cb337b8e9e" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏñëÏûêÌôî Îêú ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="01f53bdcdfe76a89f9da101eca78388d0e3db80b" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input image composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 2D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4d7022a0fdb1e618b639247856e6903aab3855e6" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d5648bdd82e29b0ec766dab1d7ea5bd15fb57316" translate="yes" xml:space="preserve">
          <source>Applies a 2D fractional max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 2D Î∂ÄÎ∂Ñ ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4dcf07079bf5dd6c28cbfd49dde4f39b4bf6b35b" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏñëÏûêÌôî Îêú ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê ÎåÄÌï¥ 2D ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a476032b7da8421b8000aa56e627562cba0b8e78" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="16be7a5fb7dca84029d9fea2edb8b4f2f0ee57f8" translate="yes" xml:space="preserve">
          <source>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• Ï±ÑÎÑêÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D ÏµúÍ∑º Ï†ë Ïù¥ÏõÉ ÏóÖ ÏÉòÌîåÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="41e06a057945b6ec3fbb4344ac26327e886f585c" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ï†ÑÎ†• ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d69ec22c35e226c6aa2b1af68d48e3fa60401801" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 2D Ï†ÑÎ†• ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§. &lt;code&gt;p&lt;/code&gt; Ïùò Í±∞Îì≠ Ï†úÍ≥±Ïóê ÎåÄÌïú Î™®Îì† ÏûÖÎ†•Ïùò Ìï© Ïù¥ 0Ïù¥Î©¥ Í∏∞Ïö∏Í∏∞ÎèÑ 0ÏúºÎ°ú ÏÑ§Ï†ïÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="aa4b7ef410c54190539113762f6eeb2a66bf418c" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">ÎïåÎïåÎ°ú &quot;ÎîîÏª® Î≥º Î£®ÏÖò&quot;Ïù¥ÎùºÍ≥†ÎèÑÌïòÎäî Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 2D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b43e206b10ee89d84a5a812651aa838b543004e7" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 2D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e5d643fce9e39e07cff6793421bc79eff19c89aa" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D Ï†ÅÏùë Ìòï ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ae56276da0a49f342cfd4b640808d55b08117aa7" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D Ï†ÅÏùë Ìòï ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="37570017fc80764b6133f223b871fd527ef1696c" translate="yes" xml:space="preserve">
          <source>Applies a 3D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D ÌèâÍ∑† ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="af8bbac2d382374f13d6575cf2d20a9ea3e1524d" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized 3D input composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú 3D ÏûÖÎ†•Ïóê 3D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c2ec5baf08b4d035b1c172c872a14f1141d860e8" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏñëÏûêÌôî Îêú ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏñëÏûêÌôî Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f6f649a7867c44dcfc76acf042b1d45c5f149502" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input image composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 3D ÌöåÏÑ†ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="51acb800dbe635481a7193d943f2c4fdf6f9b633" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D Ïª®Î≥º Î£®ÏÖòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a5c65698b496aec798c667f51e238522d8ca0853" translate="yes" xml:space="preserve">
          <source>Applies a 3D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê 3D ÏµúÎåÄ ÌíÄÎßÅÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="330c7c56dc877c7532afe521797eaa31d71f0cd0" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;</source>
          <target state="translated">&quot;ÎîîÏª® Î≥º Î£®ÏÖò&quot;Ïù¥ÎùºÍ≥†ÎèÑÌïòÎäî Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 3D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8ceaf63b96a331eabb115beb8c3e00540f6c5ffc" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 3D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="65ce4bde72c1f78502a6398b7865adc72e50ac02" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.</source>
          <target state="translated">Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê 3D Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§. Ï†ÑÏπò Ïª®Î≥º Î£®ÏÖò Ïó∞ÏÇ∞ÏûêÎäî Í∞Å ÏûÖÎ†• Í∞íÏóê ÌïôÏäµ Í∞ÄÎä•Ìïú Ïª§ÎÑêÏùÑ ÏöîÏÜåÎ≥ÑÎ°ú Í≥±ÌïòÍ≥† Î™®Îì† ÏûÖÎ†• ÌäπÏÑ± ÌèâÎ©¥Ïùò Ï∂úÎ†•ÏùÑ Ìï©ÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="66c150b86ae084c85a0e9289376927637ec64e05" translate="yes" xml:space="preserve">
          <source>Applies a bilinear transformation to the incoming data:</source>
          <target state="translated">ÏàòÏã† Îç∞Ïù¥ÌÑ∞Ïóê Ïåç ÏÑ†Ìòï Î≥ÄÌôòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6ff756a7c7cfe8cec72e517aee9af4136e1371be" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming data:</source>
          <target state="translated">ÏàòÏã† Îç∞Ïù¥ÌÑ∞Ïóê ÏÑ†Ìòï Î≥ÄÌôòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8ff337dc0db982c6291a92a0d7e5b13fd2dbe747" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming quantized data:</source>
          <target state="translated">Îì§Ïñ¥Ïò§Îäî ÏñëÏûêÌôî Îêú Îç∞Ïù¥ÌÑ∞Ïóê ÏÑ†Ìòï Î≥ÄÌôòÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a2436507aea812ee14f1556396ee7b6b4cbdbe1d" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer Elman RNN with</source>
          <target state="translated">Îã§Ï∏µ Elman RNNÏùÑ Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bc4223d58e32410e3d29e09ef9a1db009d298cd1" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</source>
          <target state="translated">ÏûÖÎ†• ÏãúÌÄÄÏä§Ïóê Îã§ Í≥ÑÏ∏µ Í≤åÏù¥Ìä∏ Î∞òÎ≥µ Ïû•Ïπò (GRU) RNNÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="02da21c27e34ce09b75309eefe898e142b0a5cb5" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</source>
          <target state="translated">ÏûÖÎ†• ÏãúÌÄÄÏä§Ïóê Îã§Ï∏µ Ïû•Îã®Í∏∞ Í∏∞Ïñµ (LSTM) RNNÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7660fbeb32f57e21a2dce8c33a454664d3adf995" translate="yes" xml:space="preserve">
          <source>Applies a softmax followed by a logarithm.</source>
          <target state="translated">ÏÜåÌîÑÌä∏ Îß•Ïä§ Îí§Ïóê Î°úÍ∑∏Î•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="16d6a0fbab19784fd6044224005fc862384c1a6c" translate="yes" xml:space="preserve">
          <source>Applies a softmax function.</source>
          <target state="translated">ÏÜåÌîÑÌä∏ Îß•Ïä§ Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3511c68e1b0c335caf44167160bac43a1d7fe51a" translate="yes" xml:space="preserve">
          <source>Applies a softmin function.</source>
          <target state="translated">softmin Í∏∞Îä•ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="cfc3d07eb38a8451b3b1a0481d05e3ce6d35b93e" translate="yes" xml:space="preserve">
          <source>Applies alpha dropout to the input.</source>
          <target state="translated">ÏûÖÎ†•Ïóê ÏïåÌåå ÎìúÎ°≠ ÏïÑÏõÉÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d7a232c41e3f6477e97dc6f5c3a6a2571c32c1b2" translate="yes" xml:space="preserve">
          <source>Applies element-wise</source>
          <target state="translated">ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©</target>
        </trans-unit>
        <trans-unit id="113342e492fe7e12d769f5fd51f90c782c13aba3" translate="yes" xml:space="preserve">
          <source>Applies element-wise the function</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ Ìï®Ïàò Ï†ÅÏö©</target>
        </trans-unit>
        <trans-unit id="f558b6f91b1db6ba6cd7e44d550b3b8856e30ab7" translate="yes" xml:space="preserve">
          <source>Applies element-wise,</source>
          <target state="translated">ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bdcea2507c8f3d67607511bd4053b83de34dceec" translate="yes" xml:space="preserve">
          <source>Applies element-wise, the function</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ, Ìï®Ïàò Ï†ÅÏö©</target>
        </trans-unit>
        <trans-unit id="e6f0771cea3151fdc337ea7709543dfa37058eea" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</source>
          <target state="translated">Ï±ÑÎÑêÏù¥ Îëê Î≤àÏß∏ Ï∞®ÏõêÏùÑ Ï∞®ÏßÄÌïòÎäî Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê Î°úÏª¨ ÏùëÎãµ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5dfd4a4a091440af428452b6ccfc1c2f2eb1887c" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</source>
          <target state="translated">Ï±ÑÎÑêÏù¥ Îëê Î≤àÏß∏ Ï∞®ÏõêÏùÑ Ï∞®ÏßÄÌïòÎäî Ïó¨Îü¨ ÏûÖÎ†• ÌèâÎ©¥ÏúºÎ°ú Íµ¨ÏÑ±Îêú ÏûÖÎ†• Ïã†Ìò∏Ïóê Î°úÏª¨ ÏùëÎãµ Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§. Ï±ÑÎÑê Ï†ÑÏ≤¥Ïóê Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4ed90484b4c8624d14fcf12a0e8edf257e35f7b9" translate="yes" xml:space="preserve">
          <source>Applies pruning reparametrization to the tensor corresponding to the parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; without actually pruning any units.</source>
          <target state="translated">Îß§Í∞ú Î≥ÄÏàò Ìò∏Ï∂úÏóê Ìï¥ÎãπÌïòÎäî ÌÖêÏÑúÏóê Í∞ÄÏßÄ ÏπòÍ∏∞ reparametrizationÏùÑ Ï†ÅÏö© &lt;code&gt;name&lt;/code&gt; Ïùò &lt;code&gt;module&lt;/code&gt; Ïã§Ï†úÎ°ú Îã®ÏúÑÎ•º ÏπòÍ∏∞ÏóÜÏù¥.</target>
        </trans-unit>
        <trans-unit id="459126de8057e720ba86989e3d80f97bc3223f0e" translate="yes" xml:space="preserve">
          <source>Applies quantized rectified linear unit function element-wise:</source>
          <target state="translated">ÏñëÏûêÌôî Îêú Ï†ïÎ•ò ÏÑ†Ìòï Îã®ÏúÑ Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fdc304cf2ec7d610cd24075ed2e43e1a79d41759" translate="yes" xml:space="preserve">
          <source>Applies spectral normalization to a parameter in the given module.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ Î™®ÎìàÏùò Îß§Í∞ú Î≥ÄÏàòÏóê Ïä§ÌéôÌä∏Îüº Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2134b29fbca7ca9ac873ada1558b714c901876b0" translate="yes" xml:space="preserve">
          <source>Applies the</source>
          <target state="translated">Ï†ÅÏö©</target>
        </trans-unit>
        <trans-unit id="a78621f5f34b400e9542d53a0ce88e233a0027bb" translate="yes" xml:space="preserve">
          <source>Applies the Gaussian Error Linear Units function:</source>
          <target state="translated">Gaussian Error Linear Units Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="26cda85cebacaa2f55fa0de1b622a487073b568a" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise</source>
          <target state="translated">HardTanh Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="12f634982a2632889413c5e4a92650f28c9bdade" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise. See &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt;&lt;code&gt;Hardtanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">HardTanh Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt; &lt;code&gt;Hardtanh&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="95b3d284066a600a911ef55c57442d7f4b86507f" translate="yes" xml:space="preserve">
          <source>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</source>
          <target state="translated">Softmax Ìï®ÏàòÎ•º n Ï∞®Ïõê ÏûÖÎ†• TensorÏóê Ï†ÅÏö©ÌïòÏó¨ n Ï∞®Ïõê Ï∂úÎ†• TensorÏùò ÏöîÏÜåÍ∞Ä [0,1] Î≤îÏúÑÏóê ÏûàÍ≥† Ìï©Í≥ÑÍ∞Ä 1Ïù¥ÎêòÎèÑÎ°ù Ï°∞Ï†ïÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="01834f69d8e2aaf053d85470251564527cc332f1" translate="yes" xml:space="preserve">
          <source>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">Softmin Ìï®ÏàòÎ•º n Ï∞®Ïõê ÏûÖÎ†• TensorÏóê Ï†ÅÏö©ÌïòÏó¨ n Ï∞®Ïõê Ï∂úÎ†• TensorÏùò ÏöîÏÜåÍ∞Ä &lt;code&gt;[0, 1]&lt;/code&gt; Î≤îÏúÑÏóê ÏûàÍ≥† Ìï©Í≥ÑÍ∞Ä 1Ïù¥ÎêòÎèÑÎ°ù Ï°∞Ï†ïÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e581807715bdab40e3c6fa2d722c3f49a2fef542" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c91f96c06647ec0aa46ecdb47707667cb0cb30c3" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function:</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="20e17ca8da87d2e89b6ddbe8ec999295cc6de103" translate="yes" xml:space="preserve">
          <source>Applies the function &lt;code&gt;callable&lt;/code&gt; to each element in the tensor, replacing each element with the value returned by &lt;code&gt;callable&lt;/code&gt;.</source>
          <target state="translated">ÌÖêÏÑúÏùò Í∞Å ÏöîÏÜåÏóê &lt;code&gt;callable&lt;/code&gt; Ìï®ÏàòÎ•º Ï†ÅÏö© ÌïòÏó¨ Í∞Å ÏöîÏÜåÎ•º &lt;code&gt;callable&lt;/code&gt; Ïù¥ Î∞òÌôò Ìïú Í∞íÏúºÎ°ú Î∞îÍøâÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="4f92865eb288d8379f17f76a886fec1e420e13a3" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise</source>
          <target state="translated">ÌïòÎìú ÏàòÏ∂ï Í∏∞Îä•ÏùÑ ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="56fce9ee97b2fb62286ad99c1ca046ac1d4d2b2c" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise:</source>
          <target state="translated">ÌïòÎìú ÏàòÏ∂ï Í∏∞Îä•ÏùÑ ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ec160fe4b0b150b142f63859285de9f24ee55b53" translate="yes" xml:space="preserve">
          <source>Applies the hardswish function, element-wise, as described in the paper:</source>
          <target state="translated">Î¨∏ÏÑúÏóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú ÏöîÏÜåÎ≥ÑÎ°ú hardswish Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="936f4f75c2b9f6e3826efbb88a4e51ae2ab84f70" translate="yes" xml:space="preserve">
          <source>Applies the latest &lt;code&gt;method&lt;/code&gt; by computing the new partial masks and returning its combination with the &lt;code&gt;default_mask&lt;/code&gt;. The new partial mask should be computed on the entries or channels that were not zeroed out by the &lt;code&gt;default_mask&lt;/code&gt;. Which portions of the tensor &lt;code&gt;t&lt;/code&gt; the new mask will be calculated from depends on the &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (handled by the type handler):</source>
          <target state="translated">ÏÉàÎ°úÏö¥ Î∂ÄÎ∂Ñ ÎßàÏä§ÌÅ¨Î•º Í≥ÑÏÇ∞ÌïòÍ≥† &lt;code&gt;default_mask&lt;/code&gt; ÏôÄÏùò Ï°∞Ìï©ÏùÑ Î∞òÌôò ÌïòÏó¨ ÏµúÏã† &lt;code&gt;method&lt;/code&gt; Î•º Ï†ÅÏö©Ìï©ÎãàÎã§ . ÏÉà Î∂ÄÎ∂Ñ ÎßàÏä§ÌÅ¨Îäî &lt;code&gt;default_mask&lt;/code&gt; Ïóê ÏùòÌï¥ Ï†úÎ°úÌôîÎêòÏßÄ ÏïäÏùÄ Ìï≠Î™© ÎòêÎäî Ï±ÑÎÑêÏóêÏÑú Í≥ÑÏÇ∞ÎêòÏñ¥ÏïºÌï©ÎãàÎã§ . ÌÖêÏÑúÏùò Ïñ¥Îäê Î∂ÄÎ∂Ñ &lt;code&gt;t&lt;/code&gt; Ïò® ÏùòÏ°¥ÏóêÏÑú ÏÉàÎ°úÏö¥ ÎßàÏä§ÌÅ¨Í∞Ä Í≥ÑÏÇ∞ Îê† &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (ÌÉÄÏûÖ Ìï∏Îì§Îü¨Ïóê ÏùòÌï¥ Ï≤òÎ¶¨)</target>
        </trans-unit>
        <trans-unit id="7d91ea3d6a32f01c8496b5becb21a5b21016d757" translate="yes" xml:space="preserve">
          <source>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</source>
          <target state="translated">ÎÖºÎ¨∏Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú, ÏöîÏÜåÎ≥ÑÎ°ú Î¨¥ÏûëÏúÑ ÎàÑÏ∂ú Ï†ïÎ•ò ÎùºÏù¥ÎÑà Ïú†Îãõ Ìï®ÏàòÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="82d1c6b2a1ee5d5cde8c89bdcc2746329c59171c" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">Ï†ïÎ•ò Îêú ÏÑ†Ìòï Îã®ÏúÑ Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="bbd19df946d0f4bb7fc4682300e73774ba71feab" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">Ï†ïÎ•ò Îêú ÏÑ†Ìòï Îã®ÏúÑ Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="78f0af302507bc331ec744dca018a734a0627b7b" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise:</source>
          <target state="translated">Ï†ïÎ•ò Îêú ÏÑ†Ìòï Îã®ÏúÑ Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="43edbfe54e1d38b1a754c8ddb026e3655a93da7b" translate="yes" xml:space="preserve">
          <source>Applies the silu function, element-wise.</source>
          <target state="translated">silu Ìï®ÏàòÎ•º ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f0e7dde2fc1c92ff1943988e21d9ce05bb8c8f04" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise</source>
          <target state="translated">ÏÜåÌîÑÌä∏ ÏàòÏ∂ï Í∏∞Îä•ÏùÑ ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7e5fad00eee592f1c4a5342fdc927299a5993172" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise:</source>
          <target state="translated">ÏÜåÌîÑÌä∏ ÏàòÏ∂ï Í∏∞Îä•ÏùÑ ÏöîÏÜåÎ≥ÑÎ°ú Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4f3fa5f7feb89ef4634370ab877a48de7ffe1ed7" translate="yes" xml:space="preserve">
          <source>Applies weight normalization to a parameter in the given module.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ Î™®ÎìàÏùò Îß§Í∞ú Î≥ÄÏàòÏóê Í∞ÄÏ§ëÏπò Ï†ïÍ∑úÌôîÎ•º Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="adb3943666d494e8bd63152770abcc75d5223bd5" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">ÎèôÏùºÌïú Ïù∏ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨Ïù¥ Ìï®ÏàòÏùò Ï∂úÎ†•Ïóê &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ÅÏö© ÌïòÎ©¥ ÏûÖÎ†•Ïùò ÎåÄÍ∞ÅÏÑ† Ìï≠Î™©Ïù¥ÏûàÎäî ÎåÄÍ∞Å ÌñâÎ†¨Ïù¥ ÏÉùÏÑ±Îê©ÎãàÎã§. Í∑∏Îü¨ÎÇò &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; Îäî Í∏∞Î≥∏ ÌÅ¨Í∏∞Í∞Ä Îã§Î•¥ÎØÄÎ°ú Î™ÖÏãú Ï†ÅÏúºÎ°ú ÏßÄÏ†ïÌï¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="09ebc50e43a0da169dba1646717349a905de80e9" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a matrix identical to input. However, &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">ÎèôÏùºÌïú Ïù∏ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨Ïù¥ Ìï®ÏàòÏùò Ï∂úÎ†•Ïóê &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; ÏùÑ Ï†ÅÏö©ÌïòÎ©¥ ÏûÖÎ†•Í≥º ÎèôÏùºÌïú ÌñâÎ†¨Ïù¥ ÏÉùÏÑ±Îê©ÎãàÎã§. Í∑∏Îü¨ÎÇò &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; ÏùÄ Í∏∞Î≥∏ ÌÅ¨Í∏∞Í∞Ä Îã§Î•¥ÎØÄÎ°ú Î™ÖÏãú Ï†ÅÏúºÎ°ú ÏßÄÏ†ïÌï¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d22103e4c9027e5c172cbf43f3dd7371a41d32a7" translate="yes" xml:space="preserve">
          <source>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be &lt;strong&gt;scattered&lt;/strong&gt; on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">ÏûÑÏùòÏùò ÏúÑÏπò Î∞è ÌÇ§ÏõåÎìú ÏûÖÎ†•Ïù¥ DataParallelÎ°ú Ï†ÑÎã¨ Îê† Ïàò ÏûàÏßÄÎßå ÏùºÎ∂Ä Ïú†ÌòïÏùÄ ÌäπÎ≥ÑÌûà Ï≤òÎ¶¨Îê©ÎãàÎã§. ÌÖêÏÑúÎäî ÏßÄÏ†ïÎêú Ìù¨ÎØ∏Ìïú &lt;strong&gt;Í≥≥Ïóê Ìù©Ïñ¥Ï†∏&lt;/strong&gt; ÏûàÏäµÎãàÎã§ (Í∏∞Î≥∏Í∞í 0). ÌäúÌîå, Î™©Î°ù Î∞è dict Ïú†ÌòïÏùÄ ÏñïÏùÄ Î≥µÏÇ¨Îê©ÎãàÎã§. Îã§Î•∏ Ïú†ÌòïÏùÄ Îã§Î•∏ Ïä§Î†àÎìúÍ∞ÑÏóê Í≥µÏú†ÎêòÎ©∞ Î™®Îç∏Ïùò ÏàúÎ∞©Ìñ• Ìå®Ïä§Ïóê Í∏∞Î°ùÎêòÎ©¥ ÏÜêÏÉÅ Îê† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b6075aed00d09ffa3057c99d13847e6be9606a66" translate="yes" xml:space="preserve">
          <source>Args:</source>
          <target state="translated">Args:</target>
        </trans-unit>
        <trans-unit id="511f2c74f69da56453fcefe6e26731eae720fb15" translate="yes" xml:space="preserve">
          <source>Args: &lt;code&gt;mod&lt;/code&gt; a float module, either produced by torch.quantization utilities or directly from user</source>
          <target state="translated">Args : torch.quantization Ïú†Ìã∏Î¶¨Ìã∞ÏóêÏÑú ÏÉùÏÑ±ÌïòÍ±∞ÎÇò ÏÇ¨Ïö©ÏûêÎ°úÎ∂ÄÌÑ∞ ÏßÅÏ†ë ÏÉùÏÑ± Ìïú float Î™®ÎìàÏùÑ &lt;code&gt;mod&lt;/code&gt; Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a0478ca5f4c068ca3ac12f9474f6a15865ce9053" translate="yes" xml:space="preserve">
          <source>Arguments:</source>
          <target state="translated">Arguments:</target>
        </trans-unit>
        <trans-unit id="3839352701cd3d321f8924d7921c49d951abf8f7" translate="yes" xml:space="preserve">
          <source>Arguments::</source>
          <target state="translated">Arguments::</target>
        </trans-unit>
        <trans-unit id="6104f39ed22a2cd32e98536a3447a01c4b9f4781" translate="yes" xml:space="preserve">
          <source>Arithmetic Operators</source>
          <target state="translated">ÏÇ∞Ïà† Ïó∞ÏÇ∞Ïûê</target>
        </trans-unit>
        <trans-unit id="c0e74bbb76aa26e443aa08680a9aad05da89267b" translate="yes" xml:space="preserve">
          <source>Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998.</source>
          <target state="translated">Art B. Owen. Sobol Î∞è Niederreiter-Xing Ìè¨Ïù∏Ìä∏Î•º Ïä§ÌÅ¨Îû®Î∏îÌï©ÎãàÎã§. Journal of Complexity, 14 (4) : 466-489, 1998 ÎÖÑ 12 Ïõî.</target>
        </trans-unit>
        <trans-unit id="44e38ee54f654b08e43c3403586961f50c567585" translate="yes" xml:space="preserve">
          <source>As a result of these changes, the following items are considered deprecated and should not appear in new code:</source>
          <target state="translated">Ïù¥Îü¨Ìïú Î≥ÄÍ≤ΩÏúºÎ°ú Ïù∏Ìï¥ Îã§Ïùå Ìï≠Î™©ÏùÄ Îçî Ïù¥ÏÉÅ ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Í≤ÉÏúºÎ°ú Í∞ÑÏ£ºÎêòÎ©∞ ÏÉà ÏΩîÎìúÏóê ÎÇòÌÉÄÎÇòÏßÄ ÏïäÏïÑÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9f1d874b6f85d0af8d36d263ad9bfb71343abc57" translate="yes" xml:space="preserve">
          <source>As a special case, when &lt;code&gt;input&lt;/code&gt; has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.</source>
          <target state="translated">ÌäπÎ≥ÑÌïú Í≤ΩÏö∞Î°ú, &lt;code&gt;input&lt;/code&gt; Ïóê Ï∞®ÏõêÏù¥ 0Ïù¥Í≥† Ïä§ÏπºÎùº Í∞íÏù¥ 0Ïù¥ ÏïÑÎãå Í≤ΩÏö∞ ÏöîÏÜåÍ∞Ä 1 Í∞úÏù∏ 1 Ï∞®Ïõê ÌÖêÏÑúÎ°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="50ba4ee86165b9263342121969588460e8e3d7d0" translate="yes" xml:space="preserve">
          <source>As a subset of Python, any valid TorchScript function is also a valid Python function. This makes it possible to &lt;code&gt;disable TorchScript&lt;/code&gt; and debug the function using standard Python tools like &lt;code&gt;pdb&lt;/code&gt;. The reverse is not true: there are many valid Python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.</source>
          <target state="translated">PythonÏùò ÌïòÏúÑ ÏßëÌï© Ïù∏ Ïú†Ìö®Ìïú TorchScript Ìï®ÏàòÎèÑ Ïú†Ìö®Ìïú Python Ìï®ÏàòÏûÖÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ &lt;code&gt;disable TorchScript&lt;/code&gt; Î•º ÎπÑÌôúÏÑ±Ìôî ÌïòÍ≥† &lt;code&gt;pdb&lt;/code&gt; ÏôÄ Í∞ôÏùÄ ÌëúÏ§Ä Python ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ìï®ÏàòÎ•º ÎîîÎ≤ÑÍ∑∏ Ìï† Ïàò ÏûàÏäµÎãàÎã§ . Í∑∏ Î∞òÎåÄÎäî ÏÇ¨Ïã§Ïù¥ ÏïÑÎãôÎãàÎã§. Ïú†Ìö®Ìïú TorchScript ÌîÑÎ°úÍ∑∏Îû®Ïù¥ ÏïÑÎãå Ïú†Ìö®Ìïú Python ÌîÑÎ°úÍ∑∏Îû®Ïù¥ ÎßéÏù¥ ÏûàÏäµÎãàÎã§. ÎåÄÏã† TorchScriptÎäî PyTorchÏóêÏÑú Ïã†Í≤ΩÎßù Î™®Îç∏ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Îç∞ ÌïÑÏöîÌïú PythonÏùò Í∏∞Îä•Ïóê ÌäπÌûà Ï§ëÏ†êÏùÑ Îë°ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b6cd63f503882dea2536baff15dc121bd94a50a6" translate="yes" xml:space="preserve">
          <source>As above, but the sample points are spaced uniformly at a distance of &lt;code&gt;dx&lt;/code&gt;.</source>
          <target state="translated">ÏúÑÏôÄ Í∞ôÏßÄÎßå ÏÉòÌîå Ìè¨Ïù∏Ìä∏Îäî &lt;code&gt;dx&lt;/code&gt; Í±∞Î¶¨ÏóêÏÑú Í∑†Ïùº Ìïú Í∞ÑÍ≤©ÏúºÎ°ú Î∞∞Ïπò Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4514250ef4bd2637a52c6977efbe9cd977631d0f" translate="yes" xml:space="preserve">
          <source>As described in the paper &lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Efficient Object Localization Using Convolutional Networks&lt;/a&gt; , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Convolutional NetworksÎ•º ÏÇ¨Ïö©Ìïú Efficient Object Localization&lt;/a&gt; ÎÖºÎ¨∏Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú ÌîºÏ≤ò Îßµ ÎÇ¥Ïùò Ïù∏Ï†ë ÌîΩÏÖÄÏù¥ Í∞ïÌïú ÏÉÅÍ¥Ä Í¥ÄÍ≥ÑÍ∞ÄÏûàÎäî Í≤ΩÏö∞ (ÏùºÎ∞òÏ†ÅÏúºÎ°ú Ï¥àÍ∏∞ Ïª®Î≥º Î£®ÏÖò Î†àÏù¥Ïñ¥Ïùò Í≤ΩÏö∞) iid ÎìúÎ°≠ ÏïÑÏõÉÏù¥ ÌôúÏÑ±ÌôîÎ•º Ï†ïÍ∑úÌôîÌïòÏßÄ ÏïäÍ≥† Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Ìö®Í≥ºÏ†ÅÏù∏ ÌïôÏäµÎ•†ÏùÑ Ï¥àÎûòÌï©ÎãàÎã§. Í∞êÏÜå.</target>
        </trans-unit>
        <trans-unit id="99099b94ea4fc98fc5c41ec54e22d892cee1ab67" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4Î∂ÄÌÑ∞Ïù¥ Ìï®ÏàòÎäî &lt;code&gt;out&lt;/code&gt; ÌÇ§ÏõåÎìúÎ•º ÏßÄÏõêÌïòÏßÄ ÏïäÏäµÎãàÎã§ . ÎåÄÏïàÏúºÎ°ú, Ïù¥Ï†Ñ &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; ÏùÄ &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt; Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3fb950c139664617530977c05541f892b77d98f6" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4Î∂ÄÌÑ∞Ïù¥ Ìï®ÏàòÎäî &lt;code&gt;out&lt;/code&gt; ÌÇ§ÏõåÎìúÎ•º ÏßÄÏõêÌïòÏßÄ ÏïäÏäµÎãàÎã§ . ÎåÄÏïàÏúºÎ°ú, Ïù¥Ï†Ñ &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; ÏùÄ &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt; Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5d301eba7fb8fca354fa028e6aad9db923bee3a2" translate="yes" xml:space="preserve">
          <source>As of PyTorch v1.7, Windows support for the distributed package only covers collective communications with Gloo backend, &lt;code&gt;FileStore&lt;/code&gt;, and &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Therefore, the &lt;code&gt;init_method&lt;/code&gt; argument in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; must point to a file. This works for both local and shared file systems:</source>
          <target state="translated">PyTorch v1.7Î∂ÄÌÑ∞ Î∂ÑÏÇ∞ Ìå®ÌÇ§ÏßÄÏóê ÎåÄÌïú Windows ÏßÄÏõêÏùÄ Gloo Î∞±ÏóîÎìú, &lt;code&gt;FileStore&lt;/code&gt; Î∞è &lt;code&gt;DistributedDataParallel&lt;/code&gt; Í≥ºÏùò ÏßëÌï© Ï†Å ÌÜµÏã† Îßå Ìè¨Ìï® Ìï©ÎãàÎã§. Îî∞ÎùºÏÑú &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; Ïùò &lt;code&gt;init_method&lt;/code&gt; Ïù∏Ïàò Îäî ÌååÏùºÏùÑ Í∞ÄÎ¶¨Ïºú ÏïºÌï©ÎãàÎã§. Ïù¥Í≤ÉÏùÄ Î°úÏª¨ Î∞è Í≥µÏú† ÌååÏùº ÏãúÏä§ÌÖú Î™®ÎëêÏóêÏÑú ÏûëÎèôÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="80184e7d134febd3ee09d8016449ea572c67e780" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;input&lt;/code&gt; given is expected to contain &lt;em&gt;log-probabilities&lt;/em&gt; and is not restricted to a 2D Tensor. The targets are interpreted as &lt;em&gt;probabilities&lt;/em&gt; by default, but could be considered as &lt;em&gt;log-probabilities&lt;/em&gt; with &lt;code&gt;log_target&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">ÏôÄ ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;NLLLoss&lt;/code&gt; &lt;/a&gt; Ïùò &lt;code&gt;input&lt;/code&gt; Ï£ºÏñ¥ÏßÑÏùÑ Ìè¨Ìï® Ìï† Í≤ÉÏúºÎ°ú ÏòàÏÉÅÎêúÎã§ &lt;em&gt;Î°úÍ∑∏ ÌôïÎ•†&lt;/em&gt; Î∞è 2 Ï∞®Ïõê ÌÖêÏÑúÏóê Ï†úÌïúÎêòÏßÄ ÏïäÎäîÎã§. ÎåÄÏÉÅÏùÄÎ°ú Ìï¥ÏÑùÎê©ÎãàÎã§ &lt;em&gt;ÌôïÎ•†&lt;/em&gt; Í∏∞Î≥∏Ï†ÅÏúºÎ°ú, Í∑∏Îü¨ÎÇòÎ°ú Í∞ÑÏ£º Îê† Ïàò &lt;em&gt;Î°úÍ∑∏ ÌôïÎ•†&lt;/em&gt; Î°ú &lt;code&gt;log_target&lt;/code&gt; Ïùò Î°ú ÏÑ§Ï†ï &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="99266bbca47ea9ff5e29020debec6a9680d5bf7a" translate="yes" xml:space="preserve">
          <source>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a range of &lt;code&gt;[0, 1]&lt;/code&gt; and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. They have been trained on images resized such that their minimum size is 520.</source>
          <target state="translated">Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò Î™®Îç∏Í≥º ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú Î™®Îì† ÏÇ¨Ï†Ñ ÌïôÏäµ Îêú Î™®Îç∏ÏùÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú Ï†ïÍ∑úÌôî Îêú ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎ•º Í∏∞ÎåÄÌï©ÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎäî &lt;code&gt;[0, 1]&lt;/code&gt; Î≤îÏúÑÎ°úÎ°úÎìú Ìïú Îã§Ïùå &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; Î∞è &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôîÌï¥Ïïº Ìï©ÎãàÎã§. ÏµúÏÜå ÌÅ¨Í∏∞Í∞Ä 520Ïù¥ÎêòÎèÑÎ°ù ÌÅ¨Í∏∞Í∞Ä Ï°∞Ï†ï Îêú Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÍµêÏú°ÏùÑ Î∞õÏïòÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8b593995c88f61a609044a01e37e8e4ccc22e065" translate="yes" xml:space="preserve">
          <source>Assumptions</source>
          <target state="translated">Assumptions</target>
        </trans-unit>
        <trans-unit id="855f8e40c74ae8a51a99a06c8a2a032f398b5ed4" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</source>
          <target state="translated">async_opÏù¥ TrueÎ°ú ÏÑ§Ï†ïÎêú Í≤ΩÏö∞ ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Ìï∏Îì§ÏûÖÎãàÎã§. ÏóÜÏùå (async_opÏù¥ ÏïÑÎãàÍ±∞ÎÇò Í∑∏Î£πÏùò ÏùºÎ∂ÄÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞)</target>
        </trans-unit>
        <trans-unit id="a78338cab2cb5c76c6ab7df23e81a35449f6df4f" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</source>
          <target state="translated">async_opÏù¥ TrueÎ°ú ÏÑ§Ï†ïÎêú Í≤ΩÏö∞ ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Ìï∏Îì§ÏûÖÎãàÎã§. async_opÏù¥ ÏïÑÎãàÍ±∞ÎÇò Í∑∏Î£πÏùò ÏùºÎ∂ÄÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞ ÏóÜÏùå.</target>
        </trans-unit>
        <trans-unit id="4042af2d84df423621c1b7387da3334c7f85d192" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, otherwise</source>
          <target state="translated">async_opÏù¥ TrueÎ°ú ÏÑ§Ï†ïÎêú Í≤ΩÏö∞ ÎπÑÎèôÍ∏∞ ÏûëÏóÖ Ìï∏Îì§ÏûÖÎãàÎã§. ÏóÜÏùå, Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥</target>
        </trans-unit>
        <trans-unit id="1889aec46759ffe70c5c9ddb3fee3b5ee5713f7b" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters (of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; ÏóêÏÑú Í∞Å ÏûÖÎ†• Ï±ÑÎÑêÏùÄ ÏûêÏ≤¥ ÌïÑÌÑ∞ ÏÑ∏Ìä∏ (ÌÅ¨Í∏∞</target>
        </trans-unit>
        <trans-unit id="6510deaa2781bdfd17b677974666275b9af08554" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; ÏóêÏÑú Í∞Å ÏûÖÎ†• Ï±ÑÎÑêÏùÄ ÌÅ¨Í∏∞Ïùò ÏûêÏ≤¥ ÌïÑÌÑ∞ ÏÑ∏Ìä∏ÏôÄ Ïª®Î≥º Î£®ÏÖòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9d3f19004934696d7b2d6e8d06f0e3a963b23772" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size:</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; ÏóêÏÑú Í∞Å ÏûÖÎ†• Ï±ÑÎÑêÏùÄ Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌÅ¨Í∏∞Ïùò ÏûêÏ≤¥ ÌïÑÌÑ∞ ÏÑ∏Ìä∏ÏôÄ Ïó∞Í≤∞Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bcb1bde7e11ed9170bfe340f3dbc5867346695da" translate="yes" xml:space="preserve">
          <source>At groups=1, all inputs are convolved to all outputs.</source>
          <target state="translated">Í∑∏Î£π = 1ÏóêÏÑú Î™®Îì† ÏûÖÎ†•ÏùÄ Î™®Îì† Ï∂úÎ†•ÏúºÎ°ú Ïª®Î≥º Î£®ÏÖòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f9d6240328fee5db3b7a1921b6420e8a8acf15ff" translate="yes" xml:space="preserve">
          <source>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</source>
          <target state="translated">groups = 2ÏóêÏÑú ÏûëÏóÖÏùÄ Îëê Í∞úÏùò conv Î†àÏù¥Ïñ¥Î•º ÎÇòÎûÄÌûàÎëêÍ≥† Í∞ÅÍ∞Å ÏûÖÎ†• Ï±ÑÎÑêÏùò Ï†àÎ∞òÏùÑÎ≥¥Í≥† Ï∂úÎ†• Ï±ÑÎÑêÏùò Ï†àÎ∞òÏùÑ ÏÉùÏÑ± Ìïú Îã§Ïùå Îëò Îã§ Ïó∞Í≤∞ÌïòÎäî Í≤ÉÍ≥º Í∞ôÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="da760faa855b591ad5bebacae023465206f25e52" translate="yes" xml:space="preserve">
          <source>At p =</source>
          <target state="translated">p =ÏóêÏÑú</target>
        </trans-unit>
        <trans-unit id="ac85559223d37e290d1c1660fd99f2a4cf5904d9" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</source>
          <target state="translated">p = 1ÏóêÏÑú Ìï©Í≥Ñ ÌíÄÎßÅ (ÌèâÍ∑† ÌíÄÎßÅÏóê ÎπÑÎ°Ä)ÏùÑ ÏñªÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9dbb94515aba090ad21fcfa70bd96411862efa24" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</source>
          <target state="translated">p = 1ÏóêÏÑú Sum Pooling (ÌèâÍ∑† ÌíÄÎßÅÏóê ÎπÑÎ°Ä)ÏùÑ ÏñªÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="74e0b9c80dca267a78a89ad68a2f6c73241b5973" translate="yes" xml:space="preserve">
          <source>Attention</source>
          <target state="translated">Attention</target>
        </trans-unit>
        <trans-unit id="9b0aef7ef75761256026f8850aa6308137966a97" translate="yes" xml:space="preserve">
          <source>Attribute Lookup On Python Modules</source>
          <target state="translated">Python Î™®ÎìàÏùò ÏÜçÏÑ± Ï°∞Ìöå</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="cde0319b4a4a6d08df1fd51fb49e58f04a817a7e" translate="yes" xml:space="preserve">
          <source>Attributes of a ScriptModule can be marked constant by annotating them with &lt;code&gt;Final[T]&lt;/code&gt;</source>
          <target state="translated">ScriptModuleÏùò ÏÜçÏÑ±ÏùÄ &lt;code&gt;Final[T]&lt;/code&gt; Î°ú Ï£ºÏÑùÏùÑ Îã¨ÏïÑ ÏÉÅÏàòÎ°ú ÌëúÏãú Ìï† Ïàò ÏûàÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="effc3797a92fafe90a0f719052ebdd67ab00baf2" translate="yes" xml:space="preserve">
          <source>Attributes: Same as torch.nn.quantized.Conv3d</source>
          <target state="translated">ÏÜçÏÑ± : torch.nn.quantized.Conv3dÏôÄ ÎèôÏùº</target>
        </trans-unit>
        <trans-unit id="78965d92bd603a457514f5c8bd35fc06957dafd3" translate="yes" xml:space="preserve">
          <source>Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.</source>
          <target state="translated">AutogradÎäî ÌòÑÏû¨ Ï†úÌïúÎêú Î∞©ÏãùÏúºÎ°ú Î™ÖÎ™Ö Îêú ÌÖêÏÑúÎ•º ÏßÄÏõêÌï©ÎãàÎã§. autogradÎäî Î™®Îì† ÌÖêÏÑúÏùò Ïù¥Î¶ÑÏùÑ Î¨¥ÏãúÌï©ÎãàÎã§. Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ÏùÄ Ïó¨Ï†ÑÌûà ‚Äã‚ÄãÏò≥ÏßÄ Îßå Ïù¥Î¶ÑÏù¥Ï£ºÎäî ÏïàÏ†ÑÏÑ±ÏùÑ ÏûÉÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1cead4310cfb62879040be96cb0226da15307414" translate="yes" xml:space="preserve">
          <source>Autograd is supported, see &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd support&lt;/a&gt;. Because gradients are currently unnamed, optimizers may work but are untested.</source>
          <target state="translated">AutogradÍ∞Ä ÏßÄÏõêÎê©ÎãàÎã§ . &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd ÏßÄÏõêÏùÑ&lt;/a&gt; Ï∞∏Ï°∞ ÌïòÏÑ∏Ïöî . Í∑∏ÎûòÎîîÏñ∏Ìä∏Îäî ÌòÑÏû¨ Ïù¥Î¶ÑÏù¥ ÏßÄÏ†ïÎêòÏßÄ ÏïäÏïòÍ∏∞ ÎïåÎ¨∏Ïóê ÏµúÏ†ÅÌôî ÌîÑÎ°úÍ∑∏Îû®ÏùÄ ÏûëÎèô Ìï† Ïàò ÏûàÏßÄÎßå ÌÖåÏä§Ìä∏ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1fc85b24b64820e5c1a91ec86a7fdeb96b65e6ed" translate="yes" xml:space="preserve">
          <source>Autograd mechanics</source>
          <target state="translated">Autograd Ïó≠Ìïô</target>
        </trans-unit>
        <trans-unit id="a6a465a4bfe05284d5084187d5372a6d8da9b829" translate="yes" xml:space="preserve">
          <source>Autograd recording during the forward pass</source>
          <target state="translated">Ï†ïÎ∞©Ìñ• Ìå®Ïä§ Ï§ë Autograd Í∏∞Î°ù</target>
        </trans-unit>
        <trans-unit id="7807d41b89e768682ae049c6166592980a550e48" translate="yes" xml:space="preserve">
          <source>Autograd support</source>
          <target state="translated">Autograd ÏßÄÏõê</target>
        </trans-unit>
        <trans-unit id="9b8005aa270f8147e0440468e241bd00f96800bc" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision examples</source>
          <target state="translated">ÏûêÎèô ÌòºÌï© Ï†ïÎ∞ÄÎèÑ Ïòà</target>
        </trans-unit>
        <trans-unit id="0f3dc77bfd956e56fcd5b8e898b3fabd32b08034" translate="yes" xml:space="preserve">
          <source>Automatic Trace Checking</source>
          <target state="translated">ÏûêÎèô Ï∂îÏ†Å Í≤ÄÏÇ¨</target>
        </trans-unit>
        <trans-unit id="1def506ac3e846cb4c938843152cd0b9bba71135" translate="yes" xml:space="preserve">
          <source>AvgPool1d</source>
          <target state="translated">AvgPool1d</target>
        </trans-unit>
        <trans-unit id="c673cdd08db86374f9ff97d13da9945ad47b1180" translate="yes" xml:space="preserve">
          <source>AvgPool2d</source>
          <target state="translated">AvgPool2d</target>
        </trans-unit>
        <trans-unit id="fc7fc296c7e1f04112232fe8a9e0e82d5c7f190f" translate="yes" xml:space="preserve">
          <source>AvgPool3d</source>
          <target state="translated">AvgPool3d</target>
        </trans-unit>
        <trans-unit id="6da0341102c44a220c312de3110fd1209e71eaf5" translate="yes" xml:space="preserve">
          <source>Ax = b</source>
          <target state="translated">ÎèÑÎÅº = b</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="a9d087c32e7ca877fdba43990aa26038f88f00de" translate="yes" xml:space="preserve">
          <source>B \times P \times M</source>
          <target state="translated">B \ x P \ x M</target>
        </trans-unit>
        <trans-unit id="db6d439058d0e22e68b29c790c70d29e6828be28" translate="yes" xml:space="preserve">
          <source>B \times P \times R</source>
          <target state="translated">B \ x P \ x R</target>
        </trans-unit>
        <trans-unit id="2dfbff36e0eaece5eff122157b4277846a87f5f4" translate="yes" xml:space="preserve">
          <source>B \times R \times M</source>
          <target state="translated">B \ times R \ times M</target>
        </trans-unit>
        <trans-unit id="45bdfdeb5092f65ada4a9a1c95c08cee2ffa142d" translate="yes" xml:space="preserve">
          <source>BAND</source>
          <target state="translated">BAND</target>
        </trans-unit>
        <trans-unit id="0ef786803f54f9a9e02024a654989e60c7f87b66" translate="yes" xml:space="preserve">
          <source>BCELoss</source>
          <target state="translated">BCELoss</target>
        </trans-unit>
        <trans-unit id="c174768efb833ed7a3edebffb9950a4f61c4c940" translate="yes" xml:space="preserve">
          <source>BCEWithLogitsLoss</source>
          <target state="translated">BCEWithLogitsLoss</target>
        </trans-unit>
        <trans-unit id="2c89bbb2577ddfe977123efaba3737f659629fff" translate="yes" xml:space="preserve">
          <source>BLAS and LAPACK Operations</source>
          <target state="translated">BLAS Î∞è LAPACK ÏûëÏóÖ</target>
        </trans-unit>
        <trans-unit id="4aefcf5c188e5bba658f180d7b47a0379fa5e70c" translate="yes" xml:space="preserve">
          <source>BOR</source>
          <target state="translated">BOR</target>
        </trans-unit>
        <trans-unit id="bd606f52a5ad5bc6c6cc894b3accba4125210f66" translate="yes" xml:space="preserve">
          <source>BXOR</source>
          <target state="translated">BXOR</target>
        </trans-unit>
        <trans-unit id="e758ca64563fdd62965a2b97b76d555b1a70d938" translate="yes" xml:space="preserve">
          <source>Backend</source>
          <target state="translated">Backend</target>
        </trans-unit>
        <trans-unit id="b3776d63ad7b7a84bfe20d9c6d4a53a2b25d0e43" translate="yes" xml:space="preserve">
          <source>Backends</source>
          <target state="translated">Backends</target>
        </trans-unit>
        <trans-unit id="82921ea0c75d2b9e5a6aaafaf19c2a51a8ee3c26" translate="yes" xml:space="preserve">
          <source>Backends that come with PyTorch</source>
          <target state="translated">PyTorchÏôÄ Ìï®Íªò Ï†úÍ≥µÎêòÎäî Î∞±ÏóîÎìú</target>
        </trans-unit>
        <trans-unit id="64dd60fe1a049fe6db3eb1369dec2e42bf428e21" translate="yes" xml:space="preserve">
          <source>Background</source>
          <target state="translated">Background</target>
        </trans-unit>
        <trans-unit id="4c7660dd341e52619271ac35d19b4aa963dcdc89" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú Îí§Î°úÎäî &lt;code&gt;input&lt;/code&gt; Ïù¥ Î∞òÏ†ÑÎêòÏßÄ ÏïäÏùÑ Îïå ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú SVD Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§ . Ïù¥ Í≤ΩÏö∞ &lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú Ïù¥Ï§ë Ïó≠Î∞©Ìñ• ÏùÄ &lt;code&gt;input&lt;/code&gt; Í≥†Ïú† Ìïú ÌäπÏù¥ Í∞íÏù¥ ÏóÜÏùÑ Îïå Î∂àÏïàÏ†ï Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="c28a70ba3b665ff380843011cb9bf4004fe6ce27" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú Ïó≠Î∞©Ìñ• ÏùÄ &lt;code&gt;input&lt;/code&gt; Ïù¥ Î∞òÏ†ÑÎêòÏßÄ ÏïäÏùÑ Îïå ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú SVD Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§ . Ïù¥ Í≤ΩÏö∞, &lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú double back ÏùÄ &lt;code&gt;input&lt;/code&gt; Í≥†Ïú† Ìïú ÌäπÏù¥ Í∞íÏù¥ ÏóÜÏùÑ Îïå Î∂àÏïàÏ†ï Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="28943ec2b2ba443d9dbd186c1f96eafcfe632579" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú Ïó≠Î∞©Ìñ• ÏùÄ &lt;code&gt;input&lt;/code&gt; Î∞òÏ†Ñ Ìï† Ïàò ÏóÜÎäî Í≤ΩÏö∞ ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú SVD Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§ . Ïù¥ Í≤ΩÏö∞ &lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌïú Ïù¥Ï§ë Ïó≠Î∞©Ìñ• ÏùÄ &lt;code&gt;input&lt;/code&gt; Í≥†Ïú† Ìïú ÌäπÏù¥ Í∞íÏù¥ ÏóÜÏùÑ Îïå Î∂àÏïàÏ†ï Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="bb44593f48eb0328822a985f5aa285528658fb23" translate="yes" xml:space="preserve">
          <source>Bartlett window function.</source>
          <target state="translated">Bartlett Ï∞Ω Í∏∞Îä•.</target>
        </trans-unit>
        <trans-unit id="8f6ce4f7d8508e7be7e4cda0e650d850c9905e8a" translate="yes" xml:space="preserve">
          <source>Base class for all neural network modules.</source>
          <target state="translated">Î™®Îì† Ïã†Í≤ΩÎßù Î™®ÎìàÏùò Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3756aaad75fa5f15b1e3570c38539a1b4fa27093" translate="yes" xml:space="preserve">
          <source>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">Î∞∞Ìè¨ Îêú PyTorchÏóêÏÑú Ï†úÍ≥µÌïòÎäî 3 Í∞ú ( &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt; Î∞è &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt; ) ÏôÄ Í∞ôÏùÄ Î™®Îì† Ï†ÄÏû•ÏÜå Íµ¨ÌòÑÏùò Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§ÏûÖÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="fb7f5bac791ae3c9dcca099683932046b2428b2e" translate="yes" xml:space="preserve">
          <source>BasePruningMethod</source>
          <target state="translated">BasePruningMethod</target>
        </trans-unit>
        <trans-unit id="7070665795a3b04470b95d5347795e00aee639f7" translate="yes" xml:space="preserve">
          <source>Basic name inference rules</source>
          <target state="translated">Í∏∞Î≥∏ Ïù¥Î¶Ñ Ï∂îÎ°† Í∑úÏπô</target>
        </trans-unit>
        <trans-unit id="5fcebeefad3cdbbf8733aa928160dec7dc90c1a1" translate="yes" xml:space="preserve">
          <source>Basics</source>
          <target state="translated">Basics</target>
        </trans-unit>
        <trans-unit id="16f46715e1716fa628885ccd95ddf80183d01012" translate="yes" xml:space="preserve">
          <source>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;. For instance, given data &lt;code&gt;abc&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; the &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; would contain data &lt;code&gt;axbc&lt;/code&gt; with &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt;.</source>
          <target state="translated">Î∞∞Ïπò ÌÅ¨Í∏∞Îäî &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt; Ï†ÑÎã¨ Îêú Îã§ÏñëÌïú ÏãúÌÄÄÏä§ Í∏∏Ïù¥Í∞Ä ÏïÑÎãàÎùº Î∞∞ÏπòÏùò Í∞Å ÏãúÌÄÄÏä§ Îã®Í≥ÑÏóêÏÑú ÏöîÏÜå ÏàòÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§ . ÏòàÎ•º Îì§Ïñ¥, Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ &lt;code&gt;abc&lt;/code&gt; Î∞è &lt;code&gt;x&lt;/code&gt; &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; Îäî&lt;/a&gt; Îç∞Ïù¥ÌÑ∞ Ìè¨Ìï®Îê† Í≤ÉÏù¥Îã§ &lt;code&gt;axbc&lt;/code&gt; ÏôÄ &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt; ÏùÑ .</target>
        </trans-unit>
        <trans-unit id="640ed9b96cb507644eb24c180e159849e7025eaa" translate="yes" xml:space="preserve">
          <source>BatchNorm</source>
          <target state="translated">BatchNorm</target>
        </trans-unit>
        <trans-unit id="c7c537e5e7d31a4a94ee4f8e5ebc01c365e0771a" translate="yes" xml:space="preserve">
          <source>BatchNorm1d</source>
          <target state="translated">BatchNorm1d</target>
        </trans-unit>
        <trans-unit id="539a37ce419a6050de06321e52ac8941c6a520dd" translate="yes" xml:space="preserve">
          <source>BatchNorm2d</source>
          <target state="translated">BatchNorm2d</target>
        </trans-unit>
        <trans-unit id="4036bb17e086d06a41c9d03825932c8325ec759f" translate="yes" xml:space="preserve">
          <source>BatchNorm3d</source>
          <target state="translated">BatchNorm3d</target>
        </trans-unit>
        <trans-unit id="58a700d3fc7db1c024d5e6e31878faaaa556887d" translate="yes" xml:space="preserve">
          <source>Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.</source>
          <target state="translated">Î™ÖÎ™Ö Îêú ÌÖêÏÑúÎäî Î™ÖÎ™ÖÎêòÏßÄ ÏïäÏùÄ ÌÖêÏÑúÏôÄ Í≥µÏ°¥ Ìï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥Î¶ÑÏùÑ Ï†ïÏ†úÌïòÎ©¥ Î™ÖÎ™Ö Îêú ÌÖêÏÑúÏôÄ Î™ÖÎ™ÖÎêòÏßÄ ÏïäÏùÄ ÌÖêÏÑú Î™®ÎëêÏóêÏÑú ÏûëÎèôÌïòÎäî Î™ÖÎ™Ö Îêú ÌÖêÏÑú Ïù∏Ïãù ÏΩîÎìúÎ•º ÏûëÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="85ad6aa0a04066bce002e177e5b44f0e8ada2c9c" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done for each channel in the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, +)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">Î∞∞Ïπò Ï†ïÍ∑úÌôîÎäî &lt;code&gt;C&lt;/code&gt; Ï∞®ÏõêÏùò Í∞Å Ï±ÑÎÑêÏóê ÎåÄÌï¥ ÏàòÌñâ ÎêòÏñ¥ &lt;code&gt;(N, +)&lt;/code&gt; Ïä¨ÎùºÏù¥Ïä§ Ïóê ÎåÄÌïú ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞ ÌïòÎØÄÎ°úÏù¥ Î≥ºÎ•® Î∞∞Ïπò Ï†ïÍ∑úÌôî ÎòêÎäî ÏãúÍ≥µÍ∞Ñ Î∞∞Ïπò Ï†ïÍ∑úÌôîÎùºÍ≥† Î∂ÄÎ•¥Îäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏù∏ Ïö©Ïñ¥ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fd602c285d639d22638df01e41486fc6be835af0" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, D, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">Î∞∞Ïπò Ï†ïÍ∑úÌôîÎäî &lt;code&gt;(N, D, H, W)&lt;/code&gt; Ïä¨ÎùºÏù¥Ïä§ Ïóê ÎåÄÌïú ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞ ÌïòÎäî &lt;code&gt;C&lt;/code&gt; Ï∞®ÏõêÏóêÏÑú ÏàòÌñâÎêòÍ∏∞ ÎïåÎ¨∏ÏóêÏù¥ Î≥ºÎ•® Î∞∞Ïπò Ï†ïÍ∑úÌôî ÎòêÎäî ÏãúÍ≥µÍ∞Ñ Î∞∞Ïπò Ï†ïÍ∑úÌôîÎùºÍ≥† Î∂ÄÎ•¥Îäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏù∏ Ïö©Ïñ¥ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e3a6337703c27cf2e6ddd618d4baf0da4204696b" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Spatial Batch Normalization.</source>
          <target state="translated">Î∞∞Ïπò Ï†ïÍ∑úÌôîÎäî &lt;code&gt;(N, H, W)&lt;/code&gt; Ïä¨ÎùºÏù¥Ïä§ Ïóê ÎåÄÌïú ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞ ÌïòÎäî &lt;code&gt;C&lt;/code&gt; Ï∞®ÏõêÏóêÏÑú ÏàòÌñâÎêòÍ∏∞ ÎïåÎ¨∏ÏóêÏù¥ Í≥µÍ∞Ñ Î∞∞Ïπò Ï†ïÍ∑úÌôîÎùºÍ≥† Î∂ÄÎ•¥Îäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏù∏ Ïö©Ïñ¥ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="288d84de2a904a705b5e7ec68955d71169bacf13" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, L)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Temporal Batch Normalization.</source>
          <target state="translated">Î∞∞Ïπò Ï†ïÍ∑úÌôîÎäî &lt;code&gt;(N, L)&lt;/code&gt; Ïä¨ÎùºÏù¥Ïä§ Ïóê ÎåÄÌïú ÌÜµÍ≥ÑÎ•º Í≥ÑÏÇ∞ ÌïòÎäî &lt;code&gt;C&lt;/code&gt; Ï∞®ÏõêÏóêÏÑú ÏàòÌñâÎêòÍ∏∞ ÎïåÎ¨∏ÏóêÏù¥ ÏûÑÏãú Î∞∞Ïπò Ï†ïÍ∑úÌôîÎ•º Î∂ÄÎ•¥Îäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏù∏ Ïö©Ïñ¥ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0da3b2b883ef3b23c37366196cd69afd8c187cdd" translate="yes" xml:space="preserve">
          <source>Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in &lt;code&gt;input[0]&lt;/code&gt; would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.</source>
          <target state="translated">Ïã†Ìò∏Í∞Ä ÏãúÍ∞Ñ ÏòÅÏó≠ÏóêÏÑú HermitianÏù¥Í∏∞ ÎïåÎ¨∏Ïóê Í≤∞Í≥ºÎäî Ï£ºÌååÏàò ÏòÅÏó≠ÏóêÏÑú Ïã§Ï†úÏûÖÎãàÎã§. ÏùºÎ∂Ä ÏûÖÎ†• Ï£ºÌååÏàòÎäî Hermitian ÏÜçÏÑ±ÏùÑ Ï∂©Ï°±ÌïòÍ∏∞ ÏúÑÌï¥ Ïã§Ïàò Í∞íÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§. Ïù¥Îü¨Ìïú Í≤ΩÏö∞ Í∞ÄÏÉÅ Íµ¨ÏÑ± ÏöîÏÜåÎäî Î¨¥ÏãúÎê©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥, &lt;code&gt;input[0]&lt;/code&gt; ÌóàÏàò ÏÑ±Î∂ÑÏùÄ Ïã§Ï†ú Ï∂úÎ†•ÏúºÎ°ú ÌëúÌòÑÌï† ÏàòÏóÜÎäî ÌïòÎÇò Ïù¥ÏÉÅÏùò Î≥µÏû°Ìïú Ï£ºÌååÏàò Ìï≠ÏùÑ ÏÉùÏÑ±ÌïòÎØÄÎ°ú Ìï≠ÏÉÅ Î¨¥ÏãúÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2e29f922f6839735f3a19bbe923a21ab693afacc" translate="yes" xml:space="preserve">
          <source>Because your script will be profiled, please ensure that it exits in a finite amount of time.</source>
          <target state="translated">Ïä§ÌÅ¨Î¶ΩÌä∏Í∞Ä ÌîÑÎ°úÌååÏùº ÎßÅÎêòÎØÄÎ°ú Ï†úÌïúÎêú ÏãúÍ∞Ñ ÎÇ¥Ïóê Ï¢ÖÎ£åÎêòÎäîÏßÄ ÌôïÏù∏ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="956a144b3d9996462b7b18cbedf9997fc2011650" translate="yes" xml:space="preserve">
          <source>Before going further, more details on TensorBoard can be found at &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt;</source>
          <target state="translated">Í≥ÑÏÜç ÏßÑÌñâÌïòÍ∏∞ Ï†ÑÏóê TensorBoardÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt; ÏóêÏÑú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8b23ce245f346d21fc21404045956dd5e7ec60de" translate="yes" xml:space="preserve">
          <source>Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; which would initialize the RPC framework, RRef framework and distributed autograd.</source>
          <target state="translated">RPC Î∞è Î∂ÑÏÇ∞ autograd ÌîÑÎ¶¨ÎØ∏Ìã∞Î∏åÎ•º ÏÇ¨Ïö©ÌïòÍ∏∞ Ï†ÑÏóê Ï¥àÍ∏∞ÌôîÍ∞Ä ÏàòÌñâÎêòÏñ¥ÏïºÌï©ÎãàÎã§. RPC ÌîÑÎ†àÏûÑ ÏõåÌÅ¨Î•º Ï¥àÍ∏∞ÌôîÌïòÎ†§Î©¥ RPC ÌîÑÎ†àÏûÑ ÏõåÌÅ¨, RRef ÌîÑÎ†àÏûÑ ÏõåÌÅ¨ Î∞è Î∂ÑÏÇ∞ autogradÎ•º Ï¥àÍ∏∞ÌôîÌïòÎäî &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; Î•º ÏÇ¨Ïö©Ìï¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3e921d24df994f442d57e133126d10fb2ac6e163" translate="yes" xml:space="preserve">
          <source>Below is an example of running a TorchScript function using RPC.</source>
          <target state="translated">Îã§ÏùåÏùÄ RPCÎ•º ÏÇ¨Ïö©ÌïòÏó¨ TorchScript Ìï®ÏàòÎ•º Ïã§ÌñâÌïòÎäî ÏòàÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="dc8bffef30a767db701efa64dcad8d3b22a3cf6c" translate="yes" xml:space="preserve">
          <source>Bernoulli</source>
          <target state="translated">Bernoulli</target>
        </trans-unit>
        <trans-unit id="01f8c251cc6790b547148a802d152b484b1bf377" translate="yes" xml:space="preserve">
          <source>Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials - Custom C++ and CUDA Extensions&lt;/a&gt; and &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt;. The capability of third-party backends are decided by their own implementations.</source>
          <target state="translated">GLOO / MPI / NCCL Î∞±ÏóîÎìú Ïô∏ÏóêÎèÑ PyTorch Î∂ÑÏÇ∞ÏùÄ Îü∞ÌÉÄÏûÑ Î†àÏßÄÏä§ÌÑ∞ Î©îÏª§ÎãàÏ¶òÏùÑ ÌÜµÌï¥ ÌÉÄÏÇ¨ Î∞±ÏóîÎìúÎ•º ÏßÄÏõêÌï©ÎãàÎã§. C ++ ExtensionÏùÑ ÌÜµÌï¥ ÌÉÄÏÇ¨ Î∞±ÏóîÎìúÎ•º Í∞úÎ∞úÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ï∞∏Ï°∞Îäî &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials-Custom C ++ Î∞è CUDA Extensions&lt;/a&gt; Î∞è &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ . ÌÉÄÏÇ¨ Î∞±ÏóîÎìúÏùò Í∏∞Îä•ÏùÄ ÏûêÏ≤¥ Íµ¨ÌòÑÏóê ÏùòÌï¥ Í≤∞Ï†ïÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="a8b51aa01c82ba019a69245f557ba6ce284edd3e" translate="yes" xml:space="preserve">
          <source>Bilinear</source>
          <target state="translated">Bilinear</target>
        </trans-unit>
        <trans-unit id="054debc367aa35026cc5f23e63b04983a105cd4a" translate="yes" xml:space="preserve">
          <source>Binary arithmetic ops: &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;Unifies names from inputs&lt;/a&gt;</source>
          <target state="translated">Ïù¥ÏßÑ ÏÇ∞Ïà† Ïó∞ÏÇ∞ : &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;ÏûÖÎ†•Ïùò Ïù¥Î¶Ñ ÌÜµÌï©&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5200f1acde5b24e6b432770b7da7700cd60a0c9b" translate="yes" xml:space="preserve">
          <source>Blackman window function.</source>
          <target state="translated">Î∏îÎûôÎß® Ï∞Ω Í∏∞Îä•.</target>
        </trans-unit>
        <trans-unit id="0f52a562e394a39a60e84e9b3ac335fc3bd698d8" translate="yes" xml:space="preserve">
          <source>Block until the value of this &lt;code&gt;Future&lt;/code&gt; is ready.</source>
          <target state="translated">Ïù¥ &lt;code&gt;Future&lt;/code&gt; Ïùò Í∞ÄÏπò Í∞Ä Ï§ÄÎπÑ Îê† ÎïåÍπåÏßÄ Ï∞®Îã®ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="c2959d258f8603010c5e64b30d6b389ff4f7e543" translate="yes" xml:space="preserve">
          <source>Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.</source>
          <target state="translated">ÏÜåÏú†ÏûêÏóêÏÑú Î°úÏª¨ ÎÖ∏ÎìúÎ°ú RRefÏùò Í∞íÏùÑ Î≥µÏÇ¨ÌïòÏó¨ Î∞òÌôòÌïòÎäî Ï∞®Îã® Ìò∏Ï∂ú. ÌòÑÏû¨ ÎÖ∏ÎìúÍ∞Ä ÏÜåÏú†Ïûê Ïù∏ Í≤ΩÏö∞ Î°úÏª¨ Í∞íÏóê ÎåÄÌïú Ï∞∏Ï°∞Î•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b76ff4906f33c2dd97ddd929b9662ba8cac6174c" translate="yes" xml:space="preserve">
          <source>Boolean</source>
          <target state="translated">Boolean</target>
        </trans-unit>
        <trans-unit id="8bdc4a42a8aebafd81e1c9ebefc552e2af4c86e0" translate="yes" xml:space="preserve">
          <source>Both &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must have integer types.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Î™®Îëê Ï†ïÏàò Ïú†ÌòïÏù¥ ÏûàÏñ¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1f1c331482420b1dcc09832ffc768fcc272a654b" translate="yes" xml:space="preserve">
          <source>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</source>
          <target state="translated">Îß§Í∞ú Î≥ÄÏàòÏôÄ ÏòÅÍµ¨ Î≤ÑÌçº (Ïòà : ÌèâÍ∑† Ïã§Ìñâ)Í∞Ä Î™®Îëê Ìè¨Ìï®Îê©ÎãàÎã§. ÌÇ§Îäî Ìï¥Îãπ Îß§Í∞ú Î≥ÄÏàò Î∞è Î≤ÑÌçº Ïù¥Î¶ÑÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fbb16819919dac69a8f12c76f83bc60666b08a4a" translate="yes" xml:space="preserve">
          <source>Break and Continue</source>
          <target state="translated">Ï§ëÎã®ÌïòÍ≥† Í≥ÑÏÜç</target>
        </trans-unit>
        <trans-unit id="7263f9de457f4107fd587921961d9e2a1123f9c7" translate="yes" xml:space="preserve">
          <source>Broadcasting semantics</source>
          <target state="translated">Î∞©ÏÜ° ÏùòÎØ∏Î°†</target>
        </trans-unit>
        <trans-unit id="c0e07d9c87aab6412703456e09b86450b239d9d4" translate="yes" xml:space="preserve">
          <source>Broadcasts the given tensors according to &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Broadcasting semantics&lt;/a&gt;.</source>
          <target state="translated">Î∏åÎ°úÎìú &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Ï∫êÏä§ÌåÖ ÏùòÎØ∏Î°†&lt;/a&gt; Ïóê Îî∞Îùº Ï£ºÏñ¥ÏßÑ ÌÖêÏÑúÎ•º Î∏åÎ°úÎìú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="3ef2cb9c843900d814506b00e103fcfa6e5a1790" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group with multiple GPU tensors per node.</source>
          <target state="translated">ÎÖ∏Îìú Îãπ Ïó¨Îü¨ GPU ÌÖêÏÑúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ÑÏ≤¥ Í∑∏Î£πÏóê ÌÖêÏÑúÎ•º Î∏åÎ°úÎìú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8cd98b60116d9de3db535a744915ab8ea6c752e2" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group.</source>
          <target state="translated">ÌÖêÏÑúÎ•º Ï†ÑÏ≤¥ Í∑∏Î£πÏóê Î∏åÎ°úÎìú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9d0d383693b784a550085e06cd3563d333282d5c" translate="yes" xml:space="preserve">
          <source>Buffers can be accessed as attributes using given names.</source>
          <target state="translated">Î≤ÑÌçºÎäî Ï£ºÏñ¥ÏßÑ Ïù¥Î¶ÑÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÜçÏÑ±ÏúºÎ°ú Ïï°ÏÑ∏Ïä§ Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="072f59c8f878e682443386c6e7570c90195dbc31" translate="yes" xml:space="preserve">
          <source>Built-in Functions and Modules</source>
          <target state="translated">ÎÇ¥Ïû• Ìï®Ïàò Î∞è Î™®Îìà</target>
        </trans-unit>
        <trans-unit id="2faa9e18e8bd0f32f4bea2db29afed6e0ae12775" translate="yes" xml:space="preserve">
          <source>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt;&lt;code&gt;new_group()&lt;/code&gt;&lt;/a&gt; function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a &lt;code&gt;group&lt;/code&gt; argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏßëÌï©Ï≤¥Îäî Í∏∞Î≥∏ Í∑∏Î£π (ÏõîÎìúÎùºÍ≥†ÎèÑ Ìï®)ÏóêÏÑú ÏûëÎèôÌïòÎ©∞ Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§Í∞Ä Î∂ÑÏÇ∞ Ìï®Ïàò Ìò∏Ï∂úÏùÑ ÏûÖÎ†•Ìï¥ÏïºÌï©ÎãàÎã§. Í∑∏Îü¨ÎÇò ÏùºÎ∂Ä ÏõåÌÅ¨Î°úÎìúÎäîÎ≥¥Îã§ ÏÑ∏Î∂ÑÌôî Îêú ÌÜµÏã†Ïùò Ïù¥Ï†êÏùÑ ÎàÑÎ¶¥ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Í≤ÉÏùÄ Î∂ÑÏÇ∞ Í∑∏Î£πÏù¥ ÏûëÎèôÌïòÎäî Í≥≥ÏûÖÎãàÎã§. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt; &lt;code&gt;new_group()&lt;/code&gt; &lt;/a&gt; Ìï®ÏàòÎäî Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§Ïùò ÏûÑÏùòÏùò ÌïòÏúÑ ÏßëÌï©ÏúºÎ°ú ÏÉà Í∑∏Î£πÏùÑ ÎßåÎìúÎäî Îç∞ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. Î™®Îì† ÏßëÌï©Ï≤¥Ïóê ÎåÄÌïú &lt;code&gt;group&lt;/code&gt; Ïù∏ÏàòÎ°ú Ï†úÍ≥µ Îê† ÏàòÏûàÎäî Î∂àÌà¨Î™Ö Ìïú Í∑∏Î£π Ìï∏Îì§ÏùÑ Î∞òÌôòÌï©ÎãàÎã§ (ÏßëÌï©ÏùÄ Ïûò ÏïåÎ†§ÏßÑ ÌäπÏ†ï ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ìå®ÌÑ¥ÏóêÏÑú Ï†ïÎ≥¥Î•º ÍµêÌôòÌïòÎäî Î∂ÑÏÇ∞ Ìï®ÏàòÏûÖÎãàÎã§).</target>
        </trans-unit>
        <trans-unit id="aa6f8066d9ec2654575e0a2dde8cde52f9a324b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;dim&lt;/code&gt; is the last dimension of the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú &lt;code&gt;dim&lt;/code&gt; ÏùÄ &lt;code&gt;input&lt;/code&gt; ÌÖêÏÑú Ïùò ÎßàÏßÄÎßâ Ï∞®ÏõêÏûÖÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="fbd6c7669b0b5fa4b44c256ffe1ad2c51d96bac8" translate="yes" xml:space="preserve">
          <source>By default, all parameters to a TorchScript function are assumed to be Tensor. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú TorchScript Ìï®ÏàòÏóê ÎåÄÌïú Î™®Îì† Îß§Í∞ú Î≥ÄÏàòÎäî TensorÎ°ú Í∞ÑÏ£ºÎê©ÎãàÎã§. TorchScript Ìï®ÏàòÏóê ÎåÄÌïú Ïù∏ÏàòÍ∞Ä Îã§Î•∏ Ïú†ÌòïÏûÑÏùÑ ÏßÄÏ†ïÌïòÎ†§Î©¥ ÏúÑÏóê ÎÇòÏó¥Îêú Ïú†ÌòïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ MyPy Ïä§ÌÉÄÏùº Ïú†Ìòï Ï£ºÏÑùÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3269c80ab5ee7ca74422e9e598672c0d5563eb7c" translate="yes" xml:space="preserve">
          <source>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú NCCL Î∞è Gloo Î∞±ÏóîÎìúÎäî ÏÇ¨Ïö©Ìï† Ïò¨Î∞îÎ•∏ ÎÑ§Ìä∏ÏõåÌÅ¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î•º Ï∞æÏúºÎ†§Í≥†Ìï©ÎãàÎã§. ÏûêÎèôÏúºÎ°ú Í∞êÏßÄ Îêú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Í∞Ä Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Îã§Ïùå ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïû¨Ï†ïÏùò Ìï† Ïàò ÏûàÏäµÎãàÎã§ (Í∞Å Î∞±ÏóîÎìúÏóê Ï†ÅÏö© Í∞ÄÎä•).</target>
        </trans-unit>
        <trans-unit id="a47868a7f38c8661f0bf65ea44d4c34b8a4bf540" translate="yes" xml:space="preserve">
          <source>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the &lt;code&gt;MAX_JOBS&lt;/code&gt; environment variable to a non-negative number.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ninja Î∞±ÏóîÎìúÎäî #CPUS + 2 Î™ÖÏùò ÏûëÏóÖÏûêÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌôïÏû• ÌîÑÎ°úÍ∑∏Îû®ÏùÑ ÎπåÎìúÌï©ÎãàÎã§. Ïù¥Í≤ÉÏùÄ ÏùºÎ∂Ä ÏãúÏä§ÌÖúÏóêÏÑú ÎÑàÎ¨¥ ÎßéÏùÄ ÏûêÏõêÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. &lt;code&gt;MAX_JOBS&lt;/code&gt; ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏùåÏàòÍ∞Ä ÏïÑÎãå Ïà´Ïûê Î°ú ÏÑ§Ï†ïÌïòÏó¨ ÏûëÏóÖÏûê ÏàòÎ•º Ï†úÏñ¥ Ìï† Ïàò ÏûàÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0d1c0495670db55861d9901a2985f3df3c09c53d" translate="yes" xml:space="preserve">
          <source>By default, the directory to which the build file is emitted and the resulting library compiled to is &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; is the temporary folder on the current platform and &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; the name of the extension. This location can be overridden in two ways. First, if the &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; environment variable is set, it replaces &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; and all extensions will be compiled into subfolders of this directory. Second, if the &lt;code&gt;build_directory&lt;/code&gt; argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÎπåÎìú ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÍ≥† Í≤∞Í≥º ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä Ïª¥ÌååÏùºÎêòÎäî &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt; Îäî &amp;lt;tmp&amp;gt; / torch_extensions / &amp;lt;name&amp;gt;ÏûÖÎãàÎã§ . Ïó¨Í∏∞ÏÑú &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; Îäî ÌòÑÏû¨ ÌîåÎû´ÌèºÏùò ÏûÑÏãú Ìè¥ÎçîÏù¥Í≥† &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; ÏùÄ ÌôïÏû•Ïùò Ïù¥Î¶ÑÏûÖÎãàÎã§. . Ïù¥ ÏúÑÏπòÎäî Îëê Í∞ÄÏßÄ Î∞©Î≤ïÏúºÎ°ú Ïû¨Ï†ïÏùò Ìï† Ïàò ÏûàÏäµÎãàÎã§. Î®ºÏ†Ä &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; ÌôòÍ≤Ω Î≥ÄÏàòÍ∞Ä ÏÑ§Ï†ïÎêòÎ©¥ &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; Î•º ÎåÄÏ≤¥ ÌïòÍ≥† Î™®Îì† ÌôïÏû•Ïù¥Ïù¥ ÎîîÎ†âÌÜ†Î¶¨Ïùò ÌïòÏúÑ Ìè¥ÎçîÎ°ú Ïª¥ÌååÏùºÎê©ÎãàÎã§. ÎëòÏß∏, Ïù¥ Ìï®ÏàòÏóê ÎåÄÌïú &lt;code&gt;build_directory&lt;/code&gt; Ïù∏ÏàòÍ∞Ä Ï†úÍ≥µÎêòÎ©¥ Ï†ÑÏ≤¥ Í≤ΩÎ°úÎ•º ÎçÆÏñ¥ ÏîÅÎãàÎã§. Ï¶â, ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä Ìï¥Îãπ Ìè¥ÎçîÎ°ú ÏßÅÏ†ë Ïª¥ÌååÏùºÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="836a1bf0b7e01bddc495fceffa973ccc50f2cdd2" translate="yes" xml:space="preserve">
          <source>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°úÏù¥ Í≥ÑÏ∏µÏùÄ ÌõàÎ†® Î∞è ÌèâÍ∞Ä Î™®Îìú Î™®ÎëêÏóêÏÑú ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≥ÑÏÇ∞ Îêú Ïù∏Ïä§ÌÑ¥Ïä§ ÌÜµÍ≥ÑÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="260db48b6c19b12ad8aaa2c223fbe53f044c91eb" translate="yes" xml:space="preserve">
          <source>By default, we decode byte strings as &lt;code&gt;utf-8&lt;/code&gt;. This is to avoid a common error case &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra &lt;code&gt;encoding&lt;/code&gt; keyword argument to specify how these objects should be loaded, e.g., &lt;code&gt;encoding='latin1'&lt;/code&gt; decodes them to strings using &lt;code&gt;latin1&lt;/code&gt; encoding, and &lt;code&gt;encoding='bytes'&lt;/code&gt; keeps them as byte arrays which can be decoded later with &lt;code&gt;byte_array.decode(...)&lt;/code&gt;.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Î∞îÏù¥Ìä∏ Î¨∏ÏûêÏó¥ÏùÑ &lt;code&gt;utf-8&lt;/code&gt; Î°ú ÎîîÏΩîÎî© Ìï©ÎãàÎã§. Ïù¥Í≤ÉÏùÄ ÏùºÎ∞òÏ†ÅÏù∏ Ïò§Î•ò ÏÇ¨Î°ÄÎ•º Î∞©ÏßÄÌïòÍ∏∞ÏúÑÌïú Í≤ÉÏûÖÎãàÎã§. &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; Python 2ÏóêÏÑú Python 3ÏóêÏÑú Ï†ÄÏû• Ìïú ÌååÏùºÏùÑÎ°úÎìú Ìï† Îïå ... Ïù¥ Í∏∞Î≥∏Í∞íÏù¥ Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏúºÎ©¥ Ï∂îÍ∞Ä &lt;code&gt;encoding&lt;/code&gt; ÌÇ§ÏõåÎìú Ïù∏ÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏßÄÏ†ïÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Í∞ùÏ≤¥Î•ºÎ°úÎìúÌïòÎäî Î∞©Î≤ï, ÏòàÎ•º Îì§Ïñ¥ &lt;code&gt;encoding='latin1'&lt;/code&gt; ÏùÄ &lt;code&gt;latin1&lt;/code&gt; Ïù∏ÏΩîÎî©ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î¨∏ÏûêÏó¥Î°ú ÎîîÏΩîÎî© ÌïòÍ≥† &lt;code&gt;encoding='bytes'&lt;/code&gt; Îäî ÎÇòÏ§ëÏóê &lt;code&gt;byte_array.decode(...)&lt;/code&gt; Î°ú ÎîîÏΩîÎî© Ìï† ÏàòÏûàÎäî Î∞îÏù¥Ìä∏ Î∞∞Ïó¥Î°ú Ïú†ÏßÄÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="5b3e53bd55890d668be711af0b27d6b4e4085bb4" translate="yes" xml:space="preserve">
          <source>By default, we don&amp;rsquo;t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌååÏùºÏùÑÎ°úÎìú Ìïú ÌõÑÏóêÎäî Ï†ïÎ¶¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. HubÎäî &lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt; ÏùòÌï¥ Î∞òÌôò Îêú ÎîîÎ†âÌÜ†Î¶¨Ïóê Ïù¥ÎØ∏ÏûàÎäî Í≤ΩÏö∞ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ï∫êÏãúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="4d31b2b62d5a7017f58599071410bf4ed9eb8d68" translate="yes" xml:space="preserve">
          <source>By default, with &lt;code&gt;dim=0&lt;/code&gt;, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use &lt;code&gt;dim=None&lt;/code&gt;.</source>
          <target state="translated">Í∏∞Î≥∏Ï†ÅÏúºÎ°ú &lt;code&gt;dim=0&lt;/code&gt; ÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ ÌëúÏ§ÄÏù¥ Ï∂úÎ†• Ï±ÑÎÑê / ÌèâÎ©¥Î≥ÑÎ°ú ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú Í≥ÑÏÇ∞Îê©ÎãàÎã§. Ï†ÑÏ≤¥ Í∞ÄÏ§ëÏπò ÌÖêÏÑúÏóê ÎåÄÌïú ÎÖ∏Î¶ÑÏùÑ Í≥ÑÏÇ∞ÌïòÎ†§Î©¥ &lt;code&gt;dim=None&lt;/code&gt; ÏùÑ ÏÇ¨Ïö©ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="32096c2e0eff33d844ee6d675407ace18289357d" translate="yes" xml:space="preserve">
          <source>C</source>
          <target state="translated">C</target>
        </trans-unit>
        <trans-unit id="eaef64b3192bf8d5502eee309124b228be2379ec" translate="yes" xml:space="preserve">
          <source>C = \log(\pi) \times \frac{p (p - 1)}{4}</source>
          <target state="translated">C = \ log (\ pi) \ times \ frac {p (p-1)} {4}</target>
        </trans-unit>
        <trans-unit id="4ee9f8327170df1bedff8fda035491204285f3c3" translate="yes" xml:space="preserve">
          <source>C = \text{number of classes (including blank)}</source>
          <target state="translated">C = \ text {ÌÅ¥ÎûòÏä§ Ïàò (Í≥µÎ∞± Ìè¨Ìï®)}</target>
        </trans-unit>
        <trans-unit id="f46c4b42af8b2196cc344f07319e812268c3c0dd" translate="yes" xml:space="preserve">
          <source>C \times \prod(\text{kernel\_size})</source>
          <target state="translated">C \ times \ prod (\ text {Ïª§ÎÑê \ _size})</target>
        </trans-unit>
        <trans-unit id="fc2b4216164cfb01ac45112054b3fedda8b56c86" translate="yes" xml:space="preserve">
          <source>C++</source>
          <target state="translated">C++</target>
        </trans-unit>
        <trans-unit id="a3d883aa22c9b3cbe1562cc3d62c063468b9a196" translate="yes" xml:space="preserve">
          <source>C=\text{num\_channels}</source>
          <target state="translated">C=\text{num\_channels}</target>
        </trans-unit>
        <trans-unit id="798a57343d5fc6a0cf122924f6ca62852b64f4a6" translate="yes" xml:space="preserve">
          <source>CELU</source>
          <target state="translated">CELU</target>
        </trans-unit>
        <trans-unit id="ff221d4752ce05f5a91bbf1d28b78a7bf7e2ddaa" translate="yes" xml:space="preserve">
          <source>CPU</source>
          <target state="translated">CPU</target>
        </trans-unit>
        <trans-unit id="d2b3baf18b41b52a701d4019f7ffaa284e02f53a" translate="yes" xml:space="preserve">
          <source>CPU hosts with Ethernet interconnect</source>
          <target state="translated">Ïù¥ÎçîÎÑ∑ ÏÉÅÌò∏ Ïó∞Í≤∞Ïù¥ÏûàÎäî CPU Ìò∏Ïä§Ìä∏</target>
        </trans-unit>
        <trans-unit id="bfea39d4ef79843c8c035774adc370d11a93b4e8" translate="yes" xml:space="preserve">
          <source>CPU hosts with InfiniBand interconnect</source>
          <target state="translated">InfiniBand ÏÉÅÌò∏ Ïó∞Í≤∞Ïù¥ÏûàÎäî CPU Ìò∏Ïä§Ìä∏</target>
        </trans-unit>
        <trans-unit id="aca0030d1b8e86f8e968a622d4b61c5e238ad1fa" translate="yes" xml:space="preserve">
          <source>CPU tensor</source>
          <target state="translated">CPU ÌÖêÏÑú</target>
        </trans-unit>
        <trans-unit id="1adcc1c5aae9ba0bc68cde14e7017456258e8921" translate="yes" xml:space="preserve">
          <source>CPU threading and TorchScript inference</source>
          <target state="translated">CPU Ïä§Î†àÎî© Î∞è TorchScript Ï∂îÎ°†</target>
        </trans-unit>
        <trans-unit id="5c435b722f2bc9152d11b1225cc3099efa162504" translate="yes" xml:space="preserve">
          <source>CTCLoss</source>
          <target state="translated">CTCLoss</target>
        </trans-unit>
        <trans-unit id="1ab6d957380a70ab72c7926a9d4fae8cf48ecf35" translate="yes" xml:space="preserve">
          <source>CUDA semantics</source>
          <target state="translated">CUDA ÏùòÎØ∏Î°†</target>
        </trans-unit>
        <trans-unit id="3f5da429aac783fd0a3a0da898da1913e428656f" translate="yes" xml:space="preserve">
          <source>CUDA support with mixed compilation is provided. Simply pass CUDA source files (&lt;code&gt;.cu&lt;/code&gt; or &lt;code&gt;.cuh&lt;/code&gt;) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking &lt;code&gt;cudart&lt;/code&gt;. You can pass additional flags to nvcc via &lt;code&gt;extra_cuda_cflags&lt;/code&gt;, just like with &lt;code&gt;extra_cflags&lt;/code&gt; for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the &lt;code&gt;CUDA_HOME&lt;/code&gt; environment variable is the safest option.</source>
          <target state="translated">ÌòºÌï© Ïª¥ÌååÏùºÏùÑ ÌÜµÌïú CUDA ÏßÄÏõêÏù¥ Ï†úÍ≥µÎê©ÎãàÎã§. CUDA ÏÜåÏä§ ÌååÏùº ( &lt;code&gt;.cu&lt;/code&gt; ÎòêÎäî &lt;code&gt;.cuh&lt;/code&gt; )ÏùÑ Îã§Î•∏ ÏÜåÏä§ÏôÄ Ìï®Íªò Ï†ÑÎã¨ÌïòÍ∏∞ ÎßåÌïòÎ©¥ Îê©ÎãàÎã§. Ïù¥Îü¨Ìïú ÌååÏùºÏùÄ C ++ Ïª¥ÌååÏùºÎü¨Í∞Ä ÏïÑÎãå nvccÎ°ú Í∞êÏßÄÎêòÍ≥† Ïª¥ÌååÏùºÎê©ÎãàÎã§. Ïó¨Í∏∞ÏóêÎäî CUDA lib64 ÎîîÎ†âÌÜ†Î¶¨Î•º ÎùºÏù¥Î∏åÎü¨Î¶¨ ÎîîÎ†âÌÜ†Î¶¨Î°ú Ï†ÑÎã¨ÌïòÍ≥† &lt;code&gt;cudart&lt;/code&gt; Ïó∞Í≤∞Ïù¥ Ìè¨Ìï® Îê©ÎãàÎã§ . ÎãπÏã†ÏùÄÏùÑ ÌÜµÌï¥ NVCCÏóê Ï∂îÍ∞Ä ÌîåÎûòÍ∑∏Î•º Ï†ÑÎã¨Ìï† Ïàò ÏûàÏäµÎãàÎã§ &lt;code&gt;extra_cuda_cflags&lt;/code&gt; Îã®ÏßÄÏôÄ ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú, &lt;code&gt;extra_cflags&lt;/code&gt; C ++Ìï©ÎãàÎã§. CUDA ÏÑ§Ïπò ÎîîÎ†âÌÜ†Î¶¨Î•º Ï∞æÍ∏∞ÏúÑÌïú Îã§ÏñëÌïú Ìú¥Î¶¨Ïä§Ìã±Ïù¥ ÏÇ¨Ïö©ÎêòÎ©∞ ÏùºÎ∞òÏ†ÅÏúºÎ°ú Ïûò ÏûëÎèôÌï©ÎãàÎã§. Í∑∏Î†áÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ &lt;code&gt;CUDA_HOME&lt;/code&gt; ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏù¥ Í∞ÄÏû• ÏïàÏ†ÑÌïú ÏòµÏÖòÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5abf962ed164e31df7bbd04bfe694578c153b51d" translate="yes" xml:space="preserve">
          <source>Caching logic</source>
          <target state="translated">Ï∫êÏã± Î°úÏßÅ</target>
        </trans-unit>
        <trans-unit id="e8a73f7e1b281ff72f0cd1f11baf202568648a12" translate="yes" xml:space="preserve">
          <source>Calculates determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨Ïùò ÌñâÎ†¨Ïãù ÎòêÎäî Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨Ïùò Î∞∞ÏπòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b469ab85777119e331c0d2b40b68cd19ac71da49" translate="yes" xml:space="preserve">
          <source>Calculates log determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨ ÎòêÎäî Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨Ïùò Î∞∞ÏπòÏùò Î°úÍ∑∏ ÌñâÎ†¨ÏãùÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="dbc9fd03c3e295de099741d45013bec1bd5703f7" translate="yes" xml:space="preserve">
          <source>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be &amp;ldquo;many-to-one&amp;rdquo;, which limits the length of the target sequence such that it must be</source>
          <target state="translated">Ïó∞ÏÜç (Î∂ÑÌï†ÎêòÏßÄ ÏïäÏùÄ) ÏãúÍ≥ÑÏó¥Í≥º ÎåÄÏÉÅ ÏãúÌÄÄÏä§ Í∞ÑÏùò ÏÜêÏã§ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§. CTCLossÎäî ÎåÄÏÉÅÏóê ÎåÄÌïú ÏûÖÎ†• Ï†ïÎ†¨ Í∞ÄÎä•ÏÑ±ÏùÑ Ìï©ÏÇ∞ÌïòÏó¨ Í∞Å ÏûÖÎ†• ÎÖ∏ÎìúÏóê ÎåÄÌï¥ ÎØ∏Î∂Ñ Ìï† ÏàòÏûàÎäî ÏÜêÏã§ Í∞íÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§. ÌÉÄÍ≤üÏóê ÎåÄÌïú ÏûÖÎ†•Ïùò Ï†ïÎ†¨ÏùÄ&amp;ldquo;Îã§ ÎåÄÏùº&amp;rdquo;Î°ú Í∞ÄÏ†ïÎêòÎ©∞, Ïù¥Îäî ÌÉÄÍ≤ü ÏãúÌÄÄÏä§Ïùò Í∏∏Ïù¥Î•º Ï†úÌïúÌïòÏó¨</target>
        </trans-unit>
        <trans-unit id="8b95dc9e720785341df91a709dd18edb57e422e8" translate="yes" xml:space="preserve">
          <source>Calculates pointwise</source>
          <target state="translated">Ìè¨Ïù∏Ìä∏ Îã®ÏúÑÎ°ú Í≥ÑÏÇ∞</target>
        </trans-unit>
        <trans-unit id="3f73e5660ccb505df5953e1fbed92e6c8a064be1" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</source>
          <target state="translated">2D ÌÖêÏÑúÏùò ÏùòÏÇ¨ Ïó≠ (Î¨¥Ïñ¥-ÌéúÎ°úÏ¶à Ïó≠Ïù¥ÎùºÍ≥†ÎèÑ Ìï®)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="afbe6768e28de20bd0669bfb42db4b0e5335e016" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; for more details</source>
          <target state="translated">2D ÌÖêÏÑúÏùò ÏùòÏÇ¨ Ïó≠ (Î¨¥Ïñ¥-ÌéúÎ°úÏ¶à Ïó≠Ïù¥ÎùºÍ≥†ÎèÑ Ìï®)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö© ÏùÄ &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="bf3fa1ed9287f3a31249968ae80488bbbc90709d" translate="yes" xml:space="preserve">
          <source>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</source>
          <target state="translated">Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨ ÎòêÎäî Ï†ïÏÇ¨Í∞ÅÌòï ÌñâÎ†¨ Î∞∞ÏπòÏùò ÌñâÎ†¨ÏãùÏùò Î∂ÄÌò∏ Î∞è Î°úÍ∑∏ Ï†àÎåÄ Í∞íÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bc5cae9078b78a08c10f30abb3ba9b5b22cfef54" translate="yes" xml:space="preserve">
          <source>Callables prefixed with underscore are considered as helper functions which won&amp;rsquo;t show up in &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Î∞ëÏ§ÑÎ°ú ÏãúÏûëÌïòÎäî &lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt; ÏùÄ torch.hub.list () Ïóê ÌëúÏãúÎêòÏßÄ ÏïäÎäî ÎèÑÏö∞ÎØ∏ Ìï®ÏàòÎ°ú Í∞ÑÏ£ºÎê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="9c42beef7db382c9ae158419c3ebc5c8310023f1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt; Ìò∏Ï∂ú</target>
        </trans-unit>
        <trans-unit id="265655dd73dcd39520c5cd9a954e45b04d35e8a1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; is equivalent to calling &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt;. The &lt;code&gt;periodic&lt;/code&gt; argument is intended as a helpful shorthand to produce a periodic window as input to functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Ìò∏Ï∂ú &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; Ìò∏Ï∂úÌïòÎäî Í≤ÉÍ≥º &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt; . &lt;code&gt;periodic&lt;/code&gt; Ïù∏Ïàò Í∞ôÏùÄ Ìï®ÏàòÏóê ÎåÄÌïú ÏûÖÎ†•ÏúºÎ°ú Ï£ºÍ∏∞Ï†Å ÏúàÎèÑÏö∞ Ï†úÏ°∞ÌïòÎäî Ïú†Ïö©Ìïú ÏÜçÍ∏∞ ÎßàÎ†®ÎêúÎã§ &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0d622de7b9cec6af9b50c591c0762f95f6cf23f1" translate="yes" xml:space="preserve">
          <source>Calling a submodule directly (e.g. &lt;code&gt;self.resnet(input)&lt;/code&gt;) is equivalent to calling its &lt;code&gt;forward&lt;/code&gt; method (e.g. &lt;code&gt;self.resnet.forward(input)&lt;/code&gt;).</source>
          <target state="translated">ÏÑúÎ∏å Î™®ÎìàÏùÑ ÏßÅÏ†ë Ìò∏Ï∂úÌïòÎäî Í≤ÉÏùÄ (Ïòà : &lt;code&gt;self.resnet(input)&lt;/code&gt; ) &lt;code&gt;forward&lt;/code&gt; Î©îÏÜåÎìú Î•º Ìò∏Ï∂úÌïòÎäî Í≤ÉÍ≥º Í∞ôÏäµÎãàÎã§ (Ïòà : &lt;code&gt;self.resnet.forward(input)&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1b7d84e5b4532eb537ef53414226540774bd94f3" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú Ïó≠Î∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="57076e248ecf298e1219a0fa6ecf299084bfedd8" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú Ïó≠Î∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="6550668f3134865d4fd7ecb672c8b13821a60616" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú Ïó≠Î∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎìúÎäî Îç∞ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="393935e7b8f54e6c891a458a9a0a5b90620581df" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú ÏàúÎ∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="3c4696c3c77b6fbb0767ee074a0b08401ad5fd2b" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú ÏàúÎ∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎì§Í∏∞ ÏúÑÌï¥ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a0f807127c82b5c379fa18d9be9a714272f46e17" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">ÎèôÏùºÌïú Ï†ïÍ∑úÌôî Î™®ÎìúÎ°ú ÏàúÎ∞©Ìñ• Î≥ÄÌôò ( &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; )ÏùÑ Ìò∏Ï∂úÌïòÎ©¥ Îëê Î≥ÄÌôòÍ∞ÑÏóê &lt;code&gt;1/n&lt;/code&gt; Ïùò Ï†ÑÏ≤¥ Ï†ïÍ∑úÌôîÍ∞Ä Ï†ÅÏö©Îê©ÎãàÎã§ . Ïù¥Í≤ÉÏùÄ &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; Î•º Ï†ïÌôïÌïú Ïó≠ ÏúºÎ°ú ÎßåÎìúÎäî Îç∞ ÌïÑÏöîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="76582af9585743776e20d4bdf66734ecbe7e7ff9" translate="yes" xml:space="preserve">
          <source>Calls to &lt;code&gt;builtin functions&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;builtin functions&lt;/code&gt; Ìò∏Ï∂ú</target>
        </trans-unit>
        <trans-unit id="eb07741ad617617e9abda7e3d52fee63a305a121" translate="yes" xml:space="preserve">
          <source>Calls to methods of builtin types like tensor: &lt;code&gt;x.mm(y)&lt;/code&gt;</source>
          <target state="translated">ÌÖêÏÑúÏôÄ Í∞ôÏùÄ ÎÇ¥Ïû• Ïú†ÌòïÏùò Î©îÏÑúÎìú Ìò∏Ï∂ú : &lt;code&gt;x.mm(y)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fc7e70542d9259610f72ca817bd3eb6584edd2c8" translate="yes" xml:space="preserve">
          <source>Calls to other script functions:</source>
          <target state="translated">Îã§Î•∏ Ïä§ÌÅ¨Î¶ΩÌä∏ Ìï®Ïàò Ìò∏Ï∂ú :</target>
        </trans-unit>
        <trans-unit id="b6867b70db2065294481ad42aed53ba49e6355b8" translate="yes" xml:space="preserve">
          <source>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size</source>
          <target state="translated">ÌÅ¨Í∏∞ ÏûÖÎ†•ÏùÑ Ï†úÍ≥µÌïòÏó¨ 2D Ïù¥ÎØ∏ÏßÄÏôÄ Í∞ôÏùÄ Í≥†Ï∞®Ïõê ÏûÖÎ†•ÏóêÎèÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="221c9205ab30df3bd1004805355ca477c455a15b" translate="yes" xml:space="preserve">
          <source>Can only be called once and before any inter-op parallel work is started (e.g. JIT execution).</source>
          <target state="translated">inter-op Î≥ëÎ†¨ ÏûëÏóÖÏù¥ ÏãúÏûëÎêòÍ∏∞ Ï†ÑÏóê Ìïú Î≤àÎßå Ìò∏Ï∂ú Ìï† Ïàò ÏûàÏäµÎãàÎã§ (Ïòà : JIT Ïã§Ìñâ).</target>
        </trans-unit>
        <trans-unit id="3805fd56c2af8071d51bc53ae0a92fdcb58807db" translate="yes" xml:space="preserve">
          <source>Casting Examples:</source>
          <target state="translated">Ï∫êÏä§ÌåÖ Ïòà :</target>
        </trans-unit>
        <trans-unit id="e7500c883cdd17fa4172ea83911ebf91a32625de" translate="yes" xml:space="preserve">
          <source>Casts</source>
          <target state="translated">Casts</target>
        </trans-unit>
        <trans-unit id="26538798d0973ae83ca7715b676794ed3b172f33" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;bfloat16&lt;/code&gt; datatype.</source>
          <target state="translated">Î™®Îì† Î∂ÄÎèô ÏÜåÏàòÏ†ê Îß§Í∞ú Î≥ÄÏàòÏôÄ Î≤ÑÌçºÎ•º &lt;code&gt;bfloat16&lt;/code&gt; Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b6c764881902912eb6ba271fb55ce5179af34673" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;double&lt;/code&gt; datatype.</source>
          <target state="translated">Î™®Îì† Î∂ÄÎèô ÏÜåÏàòÏ†ê Îß§Í∞ú Î≥ÄÏàòÏôÄ Î≤ÑÌçºÎ•º &lt;code&gt;double&lt;/code&gt; Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="78461ae0a43c2d54b7c9efa684628bba9ad1c1ed" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;half&lt;/code&gt; datatype.</source>
          <target state="translated">Î™®Îì† Î∂ÄÎèô ÏÜåÏàòÏ†ê Îß§Í∞ú Î≥ÄÏàòÏôÄ Î≤ÑÌçºÎ•º &lt;code&gt;half&lt;/code&gt; Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="661d47897b8e53cbd52a45424e86044636652304" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to float datatype.</source>
          <target state="translated">Î™®Îì† Î∂ÄÎèô ÏÜåÏàòÏ†ê Îß§Í∞ú Î≥ÄÏàò Î∞è Î≤ÑÌçºÎ•º Î∂ÄÎèô Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4724f395a64ded08f676d2e9e0393dcb0f712246" translate="yes" xml:space="preserve">
          <source>Casts all parameters and buffers to &lt;code&gt;dst_type&lt;/code&gt;.</source>
          <target state="translated">Î™®Îì† Îß§Í∞ú Î≥ÄÏàòÏôÄ Î≤ÑÌçºÎ•º &lt;code&gt;dst_type&lt;/code&gt; ÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="79f02a31265abcb1bb988f26d0c498d397fdd789" translate="yes" xml:space="preserve">
          <source>Casts this storage to bfloat16 type</source>
          <target state="translated">Ïù¥ Ïä§ÌÜ†Î¶¨ÏßÄÎ•º bfloat16 Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bd487dabbd2c7187cd079ec504def8aaf3a7b571" translate="yes" xml:space="preserve">
          <source>Casts this storage to bool type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º bool Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6694cf255316b894b3ad2697cc3b38ec9452ca42" translate="yes" xml:space="preserve">
          <source>Casts this storage to byte type</source>
          <target state="translated">Ïù¥ Ïä§ÌÜ†Î¶¨ÏßÄÎ•º Î∞îÏù¥Ìä∏ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏</target>
        </trans-unit>
        <trans-unit id="151ff3570c6883e5b0af6409a5ae071d0e154dbc" translate="yes" xml:space="preserve">
          <source>Casts this storage to char type</source>
          <target state="translated">Ïù¥ Ïä§ÌÜ†Î¶¨ÏßÄÎ•º char Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§Ìä∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d3e9e6a46615473165fa7e0ec3e61fb30d1e8866" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex double type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Î≥µÏû°Ìïú Ïù¥Ï§ë Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="510b5fa8a83cea216c27cd3b246f5ff67c971a64" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex float type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Î≥µÏû°Ìïú ÌîåÎ°úÌä∏ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2e339cd7fb3f62430cb2f45b27e9e325c9fd432f" translate="yes" xml:space="preserve">
          <source>Casts this storage to double type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º ÎçîÎ∏î Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="46bf8db49512235eab20518ddd006bc28b925a2c" translate="yes" xml:space="preserve">
          <source>Casts this storage to float type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º float Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="63e25d02d152bfc3b6809c0d00d52a9786a45699" translate="yes" xml:space="preserve">
          <source>Casts this storage to half type</source>
          <target state="translated">Ïù¥ Ïä§ÌÜ†Î¶¨ÏßÄÎ•º ÌïòÌîÑ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8dd4507b920a808cec77926ae75ebeb4c2c63230" translate="yes" xml:space="preserve">
          <source>Casts this storage to int type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º int Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="afab2e0bade340b8cecdf6e13b386611e22e2730" translate="yes" xml:space="preserve">
          <source>Casts this storage to long type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Í∏¥ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d4ce7dd1afcf534c203c4afd8fc54fe512dfe75d" translate="yes" xml:space="preserve">
          <source>Casts this storage to short type</source>
          <target state="translated">Ïù¥ Ï†ÄÏû•ÏÜåÎ•º ÏßßÏùÄ Ïú†ÌòïÏúºÎ°ú Ï∫êÏä§ÌåÖÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="795a0c324ff1f130803359d377842d67e3339ceb" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on parameters in this module.</source>
          <target state="translated">autogradÏóêÏÑúÏù¥ Î™®ÎìàÏùò Îß§Í∞ú Î≥ÄÏàòÏóê ÎåÄÌïú ÏûëÏóÖÏùÑ Í∏∞Î°ùÌï¥ÏïºÌïòÎäîÏßÄ Î≥ÄÍ≤ΩÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="771633aa9e4ffd6ed00dfc406fa17e09d36a9380" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on this tensor: sets this tensor&amp;rsquo;s &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; attribute in-place. Returns this tensor.</source>
          <target state="translated">autogradÍ∞ÄÏù¥ ÌÖêÏÑúÏóêÏÑú ÏûëÏóÖÏùÑ Í∏∞Î°ùÌï¥ÏïºÌïòÎäîÏßÄ Î≥ÄÍ≤Ω :Ïù¥ ÌÖêÏÑúÏùò &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; ÏÜçÏÑ±ÏùÑ Ï†úÏûêÎ¶¨Ïóê ÏÑ§Ï†ïÌï©ÎãàÎã§. Ïù¥ ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="df974d2905d5cd175e8ce062cca16506fc69a1da" translate="yes" xml:space="preserve">
          <source>Channel dim is the 2nd dim of input. When input has dims &amp;lt; 2, then there is no channel dim and the number of channels = 1.</source>
          <target state="translated">Channel dimÏùÄ ÏûÖÎ†•Ïùò Îëê Î≤àÏß∏ dimÏûÖÎãàÎã§. ÏûÖÎ†•Ïù¥ Ìù¨ÎØ∏Ìïú 2 ÎØ∏ÎßåÏù¥Î©¥ Ï±ÑÎÑêÏù¥ Ìù¨ÎØ∏ Ìï¥ÏßÄÍ≥† Ï±ÑÎÑê ÏàòÍ∞Ä 1ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="56ff021269456a017773120e4ed3d4af95c8ae59" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt; ÏóêÏÑú ÏÉÅÏÜç Îêú Î™®ÎìàÏóêÏÑú &lt;code&gt;forward_pre_hooks&lt;/code&gt; Î•º Ï∞æÏïÑ &lt;code&gt;module&lt;/code&gt; Ïù¥ Ï†ïÎ¶¨ ÎêòÏóàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="e4c8690ed4ddc9de7d901e88e48d1d54b0026ac9" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;code&gt;BasePruningMethod&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;BasePruningMethod&lt;/code&gt; ÏóêÏÑú ÏÉÅÏÜç Îêú Î™®ÎìàÏóêÏÑú &lt;code&gt;forward_pre_hooks&lt;/code&gt; Î•º Ï∞æÏïÑ &lt;code&gt;module&lt;/code&gt; Ïù¥ Ï†ïÎ¶¨ ÎêòÏóàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b3a48ae8d9eebbbcfe68f114073dd271c96bf314" translate="yes" xml:space="preserve">
          <source>Check whether it&amp;rsquo;s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread.</source>
          <target state="translated">ONNX ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ï§ëÍ∞ÑÏóê ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÏã≠ÏãúÏò§. Ïù¥ Ìï®ÏàòÎäî torch.onnx.export () Ï§ëÍ∞ÑÏóê TrueÎ•º Î∞òÌôòÌï©ÎãàÎã§. torch.onnx.exportÎäî Îã®Ïùº Ïä§Î†àÎìúÎ°ú Ïã§ÌñâÌï¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e151686a765ce214247b375cc4d7aa1c97e4e14f" translate="yes" xml:space="preserve">
          <source>Checking if the default process group has been initialized</source>
          <target state="translated">Í∏∞Î≥∏ ÌîÑÎ°úÏÑ∏Ïä§ Í∑∏Î£πÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏóàÎäîÏßÄ ÌôïÏù∏</target>
        </trans-unit>
        <trans-unit id="b37b3cf9158406adf7b90792ec9193ec0e0c1df6" translate="yes" xml:space="preserve">
          <source>Checkpoint a model or part of the model</source>
          <target state="translated">Î™®Îç∏ ÎòêÎäî Î™®Îç∏Ïùò ÏùºÎ∂ÄÎ•º Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏</target>
        </trans-unit>
        <trans-unit id="95fac7ec70656fe8c623fe8fbf3c5c04c9b16115" translate="yes" xml:space="preserve">
          <source>Checkpointing doesn&amp;rsquo;t work with &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt;, but only with &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Í≤ÄÏÇ¨ Ï†êÏùÄ ÏûëÎèôÌïòÏßÄ ÏïäÏäµÎãàÎã§ &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt; , ÎßåÏóê &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="f51f71651c6e78eb334ceb82bf5f372ed3b9e12e" translate="yes" xml:space="preserve">
          <source>Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply &lt;code&gt;preserve_rng_state=False&lt;/code&gt; to &lt;code&gt;checkpoint&lt;/code&gt; or &lt;code&gt;checkpoint_sequential&lt;/code&gt; to omit stashing and restoring the RNG state during each checkpoint.</source>
          <target state="translated">Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏Îäî Ïó≠Î∞©Ìñ• ÎèôÏïà Í∞Å Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ ÏÑ∏Í∑∏Î®ºÌä∏Ïóê ÎåÄÌï¥ ÏàúÎ∞©Ìñ• ÌÜµÍ≥º ÏÑ∏Í∑∏Î®ºÌä∏Î•º Îã§Ïãú Ïã§ÌñâÌïòÏó¨ Íµ¨ÌòÑÎê©ÎãàÎã§. Ïù¥Î°ú Ïù∏Ìï¥ RNG ÏÉÅÌÉúÏôÄ Í∞ôÏùÄ ÏßÄÏÜçÏ†ÅÏù∏ ÏÉÅÌÉúÍ∞Ä Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ÏóÜÏù¥ ÏßÑÌñâÎê† Ïàò ÏûàÏäµÎãàÎã§. Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ï≤¥ÌÅ¨ Ìè¨Ïù∏ÌåÖÏóêÎäî RNGÎ•º ÏÇ¨Ïö©ÌïòÎäî Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ Ìå®Ïä§ (Ïòà : ÎìúÎ°≠ ÏïÑÏõÉÏùÑ ÌÜµÌï¥)Í∞Ä ÎπÑ Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ Ìå®Ïä§ÏôÄ ÎπÑÍµêÌïòÏó¨ Í≤∞Ï†ïÏ†Å Ï∂úÎ†•ÏùÑ Í∞ñÎèÑÎ°ù RNG ÏÉÅÌÉúÎ•º Ï†ÄÍ∏ÄÎßÅÌïòÎäî Î°úÏßÅÏù¥ Ìè¨Ìï®Îê©ÎãàÎã§. RNG ÏÉÅÌÉúÎ•º Ïà®Í∏∞Í≥† Î≥µÏõêÌïòÎäî ÎÖºÎ¶¨Îäî Í≤ÄÏÇ¨ Ï†ê ÏûëÏóÖÏùò Îü∞ÌÉÄÏûÑÏóê Îî∞Îùº Ï§ëÍ∞Ñ ÏÑ±Îä• Ï†ÄÌïòÎ•º Ï¥àÎûòÌï† Ïàò ÏûàÏäµÎãàÎã§. Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ÎêòÏßÄ ÏïäÏùÄ Ìå®Ïä§ÏôÄ ÎπÑÍµê Ìïú Í≤∞Ï†ïÏ†Å Ï∂úÎ†•Ïù¥ ÌïÑÏöîÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ &lt;code&gt;preserve_rng_state=False&lt;/code&gt; Î•º &lt;code&gt;checkpoint&lt;/code&gt; ÎòêÎäî &lt;code&gt;checkpoint_sequential&lt;/code&gt; Ïóê Ï†úÍ≥µÌïòÏã≠ÏãúÏò§. Í∞Å Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ ÎèôÏïà ÏùÄÎãâ Î∞è RNG ÏÉÅÌÉú Î≥µÏõêÏùÑ ÏÉùÎûµÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="332b6971e8e3a29969ff874b4073e4272d2a843f" translate="yes" xml:space="preserve">
          <source>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does &lt;strong&gt;not&lt;/strong&gt; save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.</source>
          <target state="translated">Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏Îäî Ïª¥Ìì®ÌåÖÏùÑ Î©îÎ™®Î¶¨ÏôÄ Í±∞ÎûòÌïòÏó¨ ÏûëÎèôÌï©ÎãàÎã§. Ïó≠Î∞©Ìñ• Í≥ÑÏÇ∞ÏùÑ ÏúÑÌï¥ Ï†ÑÏ≤¥ Í≥ÑÏÇ∞ Í∑∏ÎûòÌîÑÏùò Î™®Îì† Ï§ëÍ∞Ñ ÌôúÏÑ±ÌôîÎ•º Ï†ÄÏû•ÌïòÎäî ÎåÄÏã† Ï≤¥ÌÅ¨ Ìè¨Ïù∏Ìä∏ Î∂ÄÎ∂ÑÏùÄ Ï§ëÍ∞Ñ ÌôúÏÑ±ÌôîÎ•º Ï†ÄÏû• ÌïòÏßÄ &lt;strong&gt;ÏïäÍ≥†&lt;/strong&gt; ÎåÄÏã† Ïó≠Î∞©Ìñ• Ìå®Ïä§ÏóêÏÑú Îã§Ïãú Í≥ÑÏÇ∞Ìï©ÎãàÎã§. Î™®Îç∏Ïùò Î™®Îì† Î∂ÄÎ∂ÑÏóê Ï†ÅÏö© Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="39f6e80dd8c8ac31e71117491def641d00a288d1" translate="yes" xml:space="preserve">
          <source>Checks if tensor is in shared memory.</source>
          <target state="translated">ÌÖêÏÑúÍ∞Ä Í≥µÏú† Î©îÎ™®Î¶¨Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="19004c9ecdb08358c5474afc574d3f10f8cf3c71" translate="yes" xml:space="preserve">
          <source>Checks if the MPI backend is available.</source>
          <target state="translated">MPI Î∞±ÏóîÎìúÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f2d9da2e3f6bd1dd91d87846cd5b419112d24146" translate="yes" xml:space="preserve">
          <source>Checks if the NCCL backend is available.</source>
          <target state="translated">NCCL Î∞±ÏóîÎìúÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="56268a6e36c10e47601c6c33d1ee9618154a6c67" translate="yes" xml:space="preserve">
          <source>Choosing the network interface to use</source>
          <target state="translated">ÏÇ¨Ïö©Ìï† ÎÑ§Ìä∏ÏõåÌÅ¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ†ÌÉù</target>
        </trans-unit>
        <trans-unit id="d9eaceff91c313f7cab180625e8b4dd09baebd23" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Î™®Îì† ÏöîÏÜåÎ•º &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; Î≤îÏúÑ Î°ú Í≥†Ï†ïÌïòÍ≥† Í≤∞Í≥º ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="378e42a306697a4dd63aa3dd31b3c55c600db5d2" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Î™®Îì† ÏöîÏÜåÎ•º &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; Î≤îÏúÑ Î°ú Í≥†Ï†ïÌïòÍ≥† Í≤∞Í≥º ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="cfe592b543afcbd470b6fcfa84bfb4061fbeb434" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be larger or equal &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Î™®Îì† ÏöîÏÜåÎ•º &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; Î≥¥Îã§ ÌÅ¨Í±∞ÎÇò Í∞ôÎèÑÎ°ù ÌÅ¥Îû®ÌîÑÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="e7bab3a95a3c2fe6a5a776d67024bda46c058f02" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be smaller or equal &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Î™®Îì† ÏöîÏÜåÎ•º &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; Î≥¥Îã§ ÏûëÍ±∞ÎÇò Í∞ôÍ≤å Í≥†Ï†ï Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a8943ee6c6160fec8979faa9096fac2e19ce7bfd" translate="yes" xml:space="preserve">
          <source>Classes must be new-style classes, as we use &lt;code&gt;__new__()&lt;/code&gt; to construct them with pybind11.</source>
          <target state="translated">&lt;code&gt;__new__()&lt;/code&gt; Î•º ÏÇ¨Ïö©ÌïòÏó¨ pybind11 Î°ú Íµ¨ÏÑ±ÌïòÎØÄÎ°ú ÌÅ¥ÎûòÏä§Îäî ÏÉàÎ°úÏö¥ Ïä§ÌÉÄÏùºÏùò ÌÅ¥ÎûòÏä§ Ïó¨ÏïºÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="9b6b05a782b6d150aaf1e98e0fdbbcbc865ca9be" translate="yes" xml:space="preserve">
          <source>Classes that inherit from &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.ScriptModule&lt;/code&gt; ÏóêÏÑú ÏÉÅÏÜçÎêòÎäî ÌÅ¥ÎûòÏä§</target>
        </trans-unit>
        <trans-unit id="94c2a3189e7f7885455350c4c7a8df2d0d6ad1d1" translate="yes" xml:space="preserve">
          <source>Classification</source>
          <target state="translated">Classification</target>
        </trans-unit>
        <trans-unit id="140c2943a67faf9ddd529fa9ef19fca05ebb9209" translate="yes" xml:space="preserve">
          <source>Clears the cuFFT plan cache.</source>
          <target state="translated">cuFFT Í≥ÑÌöç Ï∫êÏãúÎ•º ÏßÄ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="026a5307ef4b381234e9524521b9341950a6f9cf" translate="yes" xml:space="preserve">
          <source>Clip acc@1</source>
          <target state="translated">ÌÅ¥Î¶Ω acc @ 1</target>
        </trans-unit>
        <trans-unit id="a7ba9ed0f0f5ac74d1bfd4abe70aaeb95921f947" translate="yes" xml:space="preserve">
          <source>Clip acc@5</source>
          <target state="translated">ÌÅ¥Î¶Ω acc @ 5</target>
        </trans-unit>
        <trans-unit id="75ccb1e878d6b2fcd6f1473f8a7c353d6b2e4806" translate="yes" xml:space="preserve">
          <source>Clips gradient norm of an iterable of parameters.</source>
          <target state="translated">Î∞òÎ≥µ Í∞ÄÎä•Ìïú Îß§Í∞ú Î≥ÄÏàòÏùò Í∑∏ÎùºÎîîÏñ∏Ìä∏ ÌëúÏ§ÄÏùÑ ÏûêÎ¶ÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="990a2e75895b3d0ccfe4b20a82d20496b3e8ee0f" translate="yes" xml:space="preserve">
          <source>Clips gradient of an iterable of parameters at specified value.</source>
          <target state="translated">ÏßÄÏ†ïÎêú Í∞íÏóêÏÑú Î∞òÎ≥µ Í∞ÄÎä•Ìïú Îß§Í∞ú Î≥ÄÏàòÏùò Í∑∏ÎùºÎîîÏñ∏Ìä∏Î•º ÏûêÎ¶ÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2b8f8ed2e1db909d591f68c446a99d83144eb890" translate="yes" xml:space="preserve">
          <source>Code running on Node 0</source>
          <target state="translated">ÎÖ∏Îìú 0ÏóêÏÑú Ïã§ÌñâÎêòÎäî ÏΩîÎìú</target>
        </trans-unit>
        <trans-unit id="4a4d07ff7beaaa363e77cbea9319ab7a99ad01d0" translate="yes" xml:space="preserve">
          <source>Code running on Node 1</source>
          <target state="translated">ÎÖ∏Îìú 1ÏóêÏÑú Ïã§ÌñâÎêòÎäî ÏΩîÎìú</target>
        </trans-unit>
        <trans-unit id="bfa7c14251111856333ef843853b74b24bdf052b" translate="yes" xml:space="preserve">
          <source>Collective functions</source>
          <target state="translated">ÏßëÎã® Í∏∞Îä•</target>
        </trans-unit>
        <trans-unit id="43092611315cdcde3137662c72b15d7a3a889e75" translate="yes" xml:space="preserve">
          <source>Collects the provided &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects into a single combined &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that is completed when all of the sub-futures are completed.</source>
          <target state="translated">Ï†úÍ≥µÎêú &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; Í∞ùÏ≤¥Î•º Î™®Îì† ÌïòÏúÑ &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; Í∞Ä ÏôÑÎ£å Îê† Îïå ÏôÑÎ£å ÎêòÎäî Îã®Ïùº Í≤∞Ìï© Future Î°ú ÏàòÏßëÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="ce079595058d15f2351c808e0ae1b284bea0c578" translate="yes" xml:space="preserve">
          <source>Combines an array of sliding local blocks into a large containing tensor.</source>
          <target state="translated">Ïä¨ÎùºÏù¥Îî© Î°úÏª¨ Î∏îÎ°ù Î∞∞Ïó¥ÏùÑ ÌÅ∞ Ìè¨Ìï® ÌÖêÏÑúÎ°ú Í≤∞Ìï©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="46488d0a373ab0b6babd0f93cccfb474321ae981" translate="yes" xml:space="preserve">
          <source>Combining Distributed DataParallel with Distributed RPC Framework</source>
          <target state="translated">Distributed DataParallelÍ≥º Distributed RPC ÌîÑÎ†àÏûÑ ÏõåÌÅ¨ Í≤∞Ìï©</target>
        </trans-unit>
        <trans-unit id="c170217f39333026f4f3aa0323fa905552560ec9" translate="yes" xml:space="preserve">
          <source>Common environment variables</source>
          <target state="translated">Í≥µÌÜµ ÌôòÍ≤Ω Î≥ÄÏàò</target>
        </trans-unit>
        <trans-unit id="086cb07d3ecde2b840e1f458bfa9fda481174a5f" translate="yes" xml:space="preserve">
          <source>Common linear algebra operations.</source>
          <target state="translated">ÏùºÎ∞òÏ†ÅÏù∏ ÏÑ†Ìòï ÎåÄÏàò Ïó∞ÏÇ∞.</target>
        </trans-unit>
        <trans-unit id="b4f955c58ceb08791953e5d03b53e986ad69907d" translate="yes" xml:space="preserve">
          <source>Commonly used ones include the following for debugging purposes:</source>
          <target state="translated">ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏÇ¨Ïö©ÎêòÎäî Í≤ÉÏùÄ ÎîîÎ≤ÑÍπÖ Î™©Ï†ÅÏúºÎ°ú Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bfd58ee3a270f3a931009900e1008d549bbd7453" translate="yes" xml:space="preserve">
          <source>Community</source>
          <target state="translated">Community</target>
        </trans-unit>
        <trans-unit id="e0c1faa1db1f49518cfef07ea955debfa4db607f" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; Ïùò Ï†ÑÏ≤¥ Ï∂úÎ†•Í≥º ÎπÑÍµêÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="0ccc8d4186f8b576455c6bf986c275fa4e1ca508" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; Ïùò Ï†ÑÏ≤¥ Ï∂úÎ†•Í≥º ÎπÑÍµêÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="d9194e4e84d8475d85c26bc6d3d0aabdb9a3cfba" translate="yes" xml:space="preserve">
          <source>Compared against the full output from &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;, we have all elements up to the Nyquist frequency.</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; Ïùò Ï†ÑÏ≤¥ Ï∂úÎ†•Í≥º ÎπÑÍµê ÌïòÎ©¥ Nyquist Ï£ºÌååÏàòÍπåÏßÄ Î™®Îì† ÏöîÏÜåÍ∞Ä ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="25acda77b0bc6b058bd349dd6f4a271e2a967bfe" translate="yes" xml:space="preserve">
          <source>Comparison Operators</source>
          <target state="translated">ÎπÑÍµê Ïó∞ÏÇ∞Ïûê</target>
        </trans-unit>
        <trans-unit id="64e945410bd38eb8bc541fa4fe0d1a5601a0a7ba" translate="yes" xml:space="preserve">
          <source>Comparison Ops</source>
          <target state="translated">ÎπÑÍµê Ïö¥ÏòÅ</target>
        </trans-unit>
        <trans-unit id="c73def212afdc811169afd7e77aebfbeecb5facc" translate="yes" xml:space="preserve">
          <source>Complex Numbers</source>
          <target state="translated">Î≥µÏÜåÏàò</target>
        </trans-unit>
        <trans-unit id="258ac8f61c6506a35c0dfc4029a4818ba9555f2a" translate="yes" xml:space="preserve">
          <source>Complex values are infinite when their real or imaginary part is infinite.</source>
          <target state="translated">Ïã§Ïàò ÎòêÎäî ÌóàÏàò Î∂ÄÎ∂ÑÏù¥ Î¨¥Ìïú Ìï† Îïå Î≥µÏû°Ìïú Í∞íÏùÄ Î¨¥ÌïúÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="20602cb78c6ade6e08a8e69fafec22e07a7a725f" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Discrete Fourier Transform.</source>
          <target state="translated">Î≥µÌï©ÏóêÏÑú Î≥µÌï© Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôò.</target>
        </trans-unit>
        <trans-unit id="a7075fa71ba5aa25f75795b49f1a3a1c5779af40" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Inverse Discrete Fourier Transform.</source>
          <target state="translated">Î≥µÏÜåÏàòÏóêÏÑú Î≥µÏÜåÏàò Ïó≠ Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôò.</target>
        </trans-unit>
        <trans-unit id="cfb4fe1390cd2aa35fb926daaca343f331617fd7" translate="yes" xml:space="preserve">
          <source>Complex-to-real Inverse Discrete Fourier Transform.</source>
          <target state="translated">Î≥µÏÜåÏàòÏóêÏÑú Ïã§Ï†úÎ°úÏùò Ïó≠ Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôò.</target>
        </trans-unit>
        <trans-unit id="49a3dcb8dae28aaaa8933d10f9d1fc99995fdb01" translate="yes" xml:space="preserve">
          <source>Compute combinations of length</source>
          <target state="translated">Í∏∏Ïù¥ Ï°∞Ìï© Í≥ÑÏÇ∞</target>
        </trans-unit>
        <trans-unit id="ad88b5c709893b3c20a622a8af743326fc15e581" translate="yes" xml:space="preserve">
          <source>Computes</source>
          <target state="translated">Computes</target>
        </trans-unit>
        <trans-unit id="b29ddfa9b93c0c21e2cd9673bf929a7688cb085a" translate="yes" xml:space="preserve">
          <source>Computes &lt;code&gt;input&lt;/code&gt; divided by &lt;code&gt;other&lt;/code&gt;, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í∞íÏùÑ &lt;code&gt;other&lt;/code&gt; Î°ú ÎÇòÎàà Í∞íÏùÑ ÏöîÏÜå Î≥ÑÎ°ú Í≥ÑÏÇ∞ ÌïòÍ≥† Í∞Å Î™´ÏùÑ 0ÏúºÎ°ú Î∞òÏò¨Î¶ºÌï©ÎãàÎã§. ÎèôÎì±ÌïòÍ≤å, Î™´ÏùÑ ÏûêÎ¶ÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d3181f8cc8e8fa272eef8d4d3ff7058798e00f63" translate="yes" xml:space="preserve">
          <source>Computes a QR decomposition of &lt;code&gt;input&lt;/code&gt;, but without constructing</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò QR Î∂ÑÌï¥Î•º Í≥ÑÏÇ∞ ÌïòÏßÄÎßå Íµ¨ÏÑ±ÌïòÏßÄ ÏïäÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f69a85669ba4f546c3f6c75d8d8a225191c83e6a" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; &lt;/a&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="bf04c1ac66a51bee90d0426e3c8106a0e4185416" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; &lt;/a&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="f360cd6902e1a0579d80b170c5497449c1afe016" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; &lt;/a&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="26e6d25fb90ed6ad09263d6546b96e18a94d0c3e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool1d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool1d&lt;/code&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="18c31c026887e211a21e3dc7927a9d793872976e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool2d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool2d&lt;/code&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="85ab6bb517a0364e7f136e092fa61b3c016286ff" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool3d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool3d&lt;/code&gt; Ïùò Î∂ÄÎ∂Ñ Ïó≠ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b7cc19759037b27ca2d7fb79591e62353c0937a7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by zeroing out the channels along the specified dim with the lowest Ln-norm.</source>
          <target state="translated">ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïóê ÎåÄÌïú ÎßàÏä§ÌÅ¨Î•º Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ . Í∏∞Î≥∏ &lt;code&gt;default_mask&lt;/code&gt; (ÌÖêÏÑúÍ∞Ä ÏïÑÏßÅ Ï†ïÎ¶¨ÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ 1Ïùò ÎßàÏä§ÌÅ¨ Ïó¨Ïïº Ìï®) ÏóêÏÑú ÏãúÏûë ÌïòÏó¨ &lt;code&gt;default_mask&lt;/code&gt; ÎÖ∏Î¶ÑÏù¥ Í∞ÄÏû• ÎÇÆÏùÄ ÏßÄÏ†ïÎêú dimÏùÑ Îî∞Îùº Ï±ÑÎÑêÏùÑ 0ÏúºÎ°ú Ï†úÍ±∞ÌïòÏó¨ default_mask ÏúÑÏóê Ï†ÅÏö© Ìï† ÎßàÏä§ÌÅ¨Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="40d80ee71e511c62ded894cfa5f39bc5b20875f0" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; according to the specific pruning method recipe.</source>
          <target state="translated">ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïóê ÎåÄÌïú ÎßàÏä§ÌÅ¨Î•º Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ . Í∏∞Î≥∏ &lt;code&gt;default_mask&lt;/code&gt; (ÌÖêÏÑúÍ∞Ä ÏïÑÏßÅ ÌîÑ Î£®ÎãùÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ ÎßàÏä§ÌÅ¨ Ïó¨Ïïº Ìï®)ÏóêÏÑú ÏãúÏûë ÌïòÏó¨ ÌäπÏ†ï ÌîÑ Î£®Îãù Î∞©Î≤ï Î†àÏãúÌîºÏóê Îî∞Îùº &lt;code&gt;default_mask&lt;/code&gt; ÏúÑÏóê Ï†ÅÏö© Ìï† ÏûÑÏùò ÎßàÏä§ÌÅ¨Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="65434598b41da121b29ca65831d67382801ae0c7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by randomly zeroing out channels along the specified dim of the tensor.</source>
          <target state="translated">ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïóê ÎåÄÌïú ÎßàÏä§ÌÅ¨Î•º Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ . Í∏∞Î≥∏ &lt;code&gt;default_mask&lt;/code&gt; (ÌÖêÏÑúÍ∞Ä ÏïÑÏßÅ Ï†ïÎ¶¨ÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ 1Ïùò ÎßàÏä§ÌÅ¨ Ïó¨Ïïº Ìï®) ÏóêÏÑú ÏãúÏûë ÌïòÏó¨ ÌÖêÏÑú Ïùò ÏßÄÏ†ïÎêú dimÏùÑ Îî∞Îùº Ï±ÑÎÑêÏùÑ Î¨¥ÏûëÏúÑÎ°ú Ï†úÎ°úÌôî ÌïòÏó¨ &lt;code&gt;default_mask&lt;/code&gt; ÏúÑÏóê Ï†ÅÏö© Ìï† ÏûÑÏùò ÎßàÏä§ÌÅ¨Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="180bc7b0d8de7293424fe248fc89cd2c995f979c" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; ÏßÄÏ†ïÎêú Ï†ïÎ¶¨ Í∑úÏπôÏóê Îî∞Îùº ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïùò Ï†ïÎ¶¨ Îêú Î≤ÑÏ†ÑÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="7a3fb50e3449e8908990f0c00a969ef20d83c411" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; ÏßÄÏ†ïÎêú Ï†ïÎ¶¨ Í∑úÏπôÏóê Îî∞Îùº ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïùò Ï†ïÎ¶¨ Îêú Î≤ÑÏ†ÑÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="9efff4b9b21781ca7329ebc9ab81c9ec882d85a1" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; ÏßÄÏ†ïÎêú Ï†ïÎ¶¨ Í∑úÏπôÏóê Îî∞Îùº ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïùò Ï†ïÎ¶¨ Îêú Î≤ÑÏ†ÑÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="f99800562bd522721e9ea1fa3102c89a24cca75f" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; ÏßÄÏ†ïÎêú Ï†ïÎ¶¨ Í∑úÏπôÏóê Îî∞Îùº ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïùò Ï†ïÎ¶¨ Îêú Î≤ÑÏ†ÑÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="28c95a2d472ee12c714c4dae6bc27475d57d2a6e" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;code&gt;compute_mask()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;compute_mask()&lt;/code&gt; ÏßÄÏ†ïÎêú Ï†ïÎ¶¨ Í∑úÏπôÏóê Îî∞Îùº ÏûÖÎ†• ÌÖêÏÑú &lt;code&gt;t&lt;/code&gt; Ïùò Ï†ïÎ¶¨ Îêú Î≤ÑÏ†ÑÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† Î∞òÌôòÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="2ad7888f31836fca25fd59e8bf8deaf0775bd8bb" translate="yes" xml:space="preserve">
          <source>Computes batched the p-norm distance between each pair of the two collections of row vectors.</source>
          <target state="translated">Îëê Ìñâ Î≤°ÌÑ∞ Î™®ÏùåÏùò Í∞Å Ïåç ÏÇ¨Ïù¥Ïóê Î∞∞Ïπò Îêú p- ÎÖ∏Î¶Ñ Í±∞Î¶¨Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f2e6dd93c4230ac985c6eeef22bb5e42799d351c" translate="yes" xml:space="preserve">
          <source>Computes element-wise equality</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ ÎèôÎì±ÏÑ±ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="23c55c6419438b7d4003d50aab0fa866dc6b754c" translate="yes" xml:space="preserve">
          <source>Computes log probabilities for all</source>
          <target state="translated">Î™®ÎëêÏóê ÎåÄÌïú Î°úÍ∑∏ ÌôïÎ•†ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bffd347218e0b14edb14dc921db35164acedc246" translate="yes" xml:space="preserve">
          <source>Computes sums or means of &amp;lsquo;bags&amp;rsquo; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">Ï§ëÍ∞Ñ ÏûÑÎ≤†Îî©ÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÏßÄ ÏïäÍ≥† ÏûÑÎ≤†Îî© 'Î∞±'Ïùò Ìï©Í≥Ñ ÎòêÎäî ÏàòÎã®ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a7d9c1eb8347c9d1cc0af2b2b967716bd8c4ae77" translate="yes" xml:space="preserve">
          <source>Computes sums, means or maxes of &lt;code&gt;bags&lt;/code&gt; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">Ï§ëÍ∞Ñ ÏûÑÎ≤†Îî©ÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÏßÄ ÏïäÍ≥† ÏûÑÎ≤†Îî© &lt;code&gt;bags&lt;/code&gt; Ìï©Í≥Ñ, ÌèâÍ∑† ÎòêÎäî ÏµúÎåÄ Í∞íÏùÑ Í≥ÑÏÇ∞ Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b71f4915a8b78a90bf71f4feb80ae5bf3e75096c" translate="yes" xml:space="preserve">
          <source>Computes the</source>
          <target state="translated">Í≥ÑÏÇ∞</target>
        </trans-unit>
        <trans-unit id="ff498bc657a9c74488608c0ce73fd38063feeafe" translate="yes" xml:space="preserve">
          <source>Computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;multivariate log-gamma function&lt;/a&gt;) with dimension</source>
          <target state="translated">Ï∞®ÏõêÏùÑ ÏÇ¨Ïö© ÌïòÏó¨ &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;Îã§Î≥ÄÎüâ Î°úÍ∑∏ Í∞êÎßà Ìï®Ïàò&lt;/a&gt; )Î•º Í≥ÑÏÇ∞ Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="76557cba93a25c6c86c33cc0d82d4b6080c0ca6f" translate="yes" xml:space="preserve">
          <source>Computes the Cholesky decomposition of a symmetric positive-definite matrix</source>
          <target state="translated">ÏñëÏùò Ï†ïÏùò ÎåÄÏπ≠ ÌñâÎ†¨Ïùò Ï¥êÎ†à Ïä§ÌÇ§ Î∂ÑÌï¥Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="81b53a3d90c63815ad24683a8c1c69a90d47d01e" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê ÎåÄÌïú Ìó§ÎπÑ ÏÇ¨Ïù¥Îìú Ïä§ÌÖù Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="5d07d50a47d9ca8fe0d585f7aa291a49089343e4" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;. The Heaviside step function is defined as:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê ÎåÄÌïú Ìó§ÎπÑ ÏÇ¨Ïù¥Îìú Ïä§ÌÖù Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . Ìó§ÎπÑ ÏÇ¨Ïù¥Îìú Ïä§ÌÖù Ìï®ÏàòÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="275e74bec95b0ab85103e19b6d6479c703706e12" translate="yes" xml:space="preserve">
          <source>Computes the Kaiser window with window length &lt;code&gt;window_length&lt;/code&gt; and shape parameter &lt;code&gt;beta&lt;/code&gt;.</source>
          <target state="translated">Ï∞Ω Í∏∏Ïù¥ &lt;code&gt;window_length&lt;/code&gt; Î∞è Î™®Ïñë Îß§Í∞ú Î≥ÄÏàò &lt;code&gt;beta&lt;/code&gt; Î°ú Ïπ¥Ïù¥Ï†Ä Ï∞ΩÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="622fda98680f4721949b3e2bf9b78a14c34deb65" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;.</source>
          <target state="translated">ÌñâÎ†¨Ïùò LU Î∂ÑÌï¥ ÎòêÎäî ÌñâÎ†¨ &lt;code&gt;A&lt;/code&gt; Ïùò Î∞∞ÏπòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="8ec07270d301f5370c046b7066eb8663a8f402a1" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;. Returns a tuple containing the LU factorization and pivots of &lt;code&gt;A&lt;/code&gt;. Pivoting is done if &lt;code&gt;pivot&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">ÌñâÎ†¨Ïùò LU Î∂ÑÌï¥ ÎòêÎäî ÌñâÎ†¨ &lt;code&gt;A&lt;/code&gt; Ïùò Î∞∞ÏπòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . &lt;code&gt;A&lt;/code&gt; Ïùò LU Î∂ÑÌï¥ Î∞è ÌîºÎ≤óÏùÑ Ìè¨Ìï®ÌïòÎäî ÌäúÌîåÏùÑ Î∞òÌôòÌï©ÎãàÎã§ . ÌîºÎ≤óÏù¥ &lt;code&gt;True&lt;/code&gt; Î°ú ÏÑ§Ï†ï ÎêòÎ©¥ &lt;code&gt;pivot&lt;/code&gt; Ïù¥ ÏàòÌñâÎê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0c3fba3f6161937ff4ae86bdb0e983d0c0fdda0b" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò N Ï∞®Ïõê Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôòÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="5852120d66c32f7c318de497ae26eeb74f1de763" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò N Ï∞®Ïõê Ïó≠ Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôòÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="efd57d1e36575205366c8a3b4753ee576bd1a231" translate="yes" xml:space="preserve">
          <source>Computes the N-dimensional discrete Fourier transform of real &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Ïã§Ïàò &lt;code&gt;input&lt;/code&gt; Ïùò N Ï∞®Ïõê Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôòÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a21392e07d8e832fdaaf1fc616e555a310a70752" translate="yes" xml:space="preserve">
          <source>Computes the QR decomposition of a matrix or a batch of matrices &lt;code&gt;input&lt;/code&gt;, and returns a namedtuple (Q, R) of tensors such that</source>
          <target state="translated">ÌñâÎ†¨Ïùò QR Î∂ÑÌï¥ ÎòêÎäî ÌñâÎ†¨ &lt;code&gt;input&lt;/code&gt; Ïùò Î∞∞ÏπòÎ•º Í≥ÑÏÇ∞ÌïòÍ≥† Îã§Ïùå Í≥º Í∞ôÏùÄ ÌÖêÏÑúÏùò Î™ÖÎ™Ö Îêú ÌäúÌîå (Q, R)ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d476f44296e977553528b2efe494b3789b4d6cc8" translate="yes" xml:space="preserve">
          <source>Computes the absolute value of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; ÏóêÏûàÎäî Í∞Å ÏöîÏÜåÏùò Ï†àÎåÄ Í∞íÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="da0e0333ca7d69a3bf51d86eddafcf5537f32ee1" translate="yes" xml:space="preserve">
          <source>Computes the base two exponential function of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò Î∞ëÏù¥ 2 Ïù∏ ÏßÄÏàò Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="833d30840f6f79f8ac884e63624d72a29fe2aedb" translate="yes" xml:space="preserve">
          <source>Computes the batchwise pairwise distance between vectors</source>
          <target state="translated">Î≤°ÌÑ∞ ÏÇ¨Ïù¥Ïùò Î∞∞Ïπò Î≥Ñ ÏåçÎ≥Ñ Í±∞Î¶¨Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b81b9d06afde312e3c1de6fa3003c1826f20a144" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ ANDÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="724c0661558717da97da9f13301714db62123db8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ ANDÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . ÏûÖÎ†• ÌÖêÏÑúÎäî Ï†ïÏàò ÎòêÎäî Î∂ÄÏö∏ Ïú†ÌòïÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§. Î∂ÄÏö∏ ÌÖêÏÑúÏùò Í≤ΩÏö∞ ÎÖºÎ¶¨ ANDÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4c385fff8361fd76237dd3d0d310601e4ded675d" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÌÖêÏÑúÏùò ÎπÑÌä∏ Îã®ÏúÑ NOTÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d34002186774eda6ff1544d9bb235cb5b5e464f1" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÌÖêÏÑúÏùò ÎπÑÌä∏ Îã®ÏúÑ NOTÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§. ÏûÖÎ†• ÌÖêÏÑúÎäî Ï†ïÏàò ÎòêÎäî Î∂ÄÏö∏ Ïú†ÌòïÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§. Î∂ÄÏö∏ ÌÖêÏÑúÏùò Í≤ΩÏö∞ ÎÖºÎ¶¨ NOTÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1676d4cae5deb51721032154a09c9a5c26471fe8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ Îã®ÏúÑ ORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="db7eb5bd0ae610915cc8f5aaa985b610903472f9" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ Îã®ÏúÑ ORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . ÏûÖÎ†• ÌÖêÏÑúÎäî Ï†ïÏàò ÎòêÎäî Î∂ÄÏö∏ Ïú†ÌòïÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§. Î∂ÄÏö∏ ÌÖêÏÑúÏùò Í≤ΩÏö∞ ÎÖºÎ¶¨ ORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2a7aa82e188d57e1be91dd1e59e101918adf2efe" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ Îã®ÏúÑ XORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="88579c44bdc54edf5679b9534b4247fa5a02a043" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÎπÑÌä∏ Îã®ÏúÑ XORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . ÏûÖÎ†• ÌÖêÏÑúÎäî Ï†ïÏàò ÎòêÎäî Î∂ÄÏö∏ Ïú†ÌòïÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§. Î∂ÄÏö∏ ÌÖêÏÑúÏùò Í≤ΩÏö∞ ÎÖºÎ¶¨ XORÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8a3e45b925774a34d85da6c0c7b80c7c01e6ba8d" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê ÎåÄÌïú Î≥¥ÏôÑ Ïò§Ï∞® Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="159a12e467808284be4f45386c2f19ec971818e9" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;. The complementary error function is defined as follows:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê ÎåÄÌïú Î≥¥ÏôÑ Ïò§Ï∞® Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . Î≥¥ÏôÑ Ïò§Î•ò Ìï®ÏàòÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ïÏùòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1983c92beb407d56255dcc00b5fe7b66872fc6d6" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors.</source>
          <target state="translated">Îëê ÌÖêÏÑúÏùò ÎÇ¥Ï†Å (ÎÇ¥Ï†Å)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9a5b212f2d4dd5a8260512297de68c3095e68cab" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.</source>
          <target state="translated">Îëê ÌÖêÏÑúÏùò ÎÇ¥Ï†Å (ÎÇ¥Ï†Å)ÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§. vdot (a, b) Ìï®ÏàòÎäî dot (a, b)ÏôÄÎäî Îã§Î•¥Í≤å Î≥µÏÜåÏàòÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§. Ï≤´ Î≤àÏß∏ Ïù∏ÏàòÍ∞Ä Î≥µÏÜåÏàòÏù¥Î©¥ Ï≤´ Î≤àÏß∏ Ïù∏ÏàòÏùò Ïº§Î†à Î≥µÏÜåÏàòÍ∞Ä ÎÇ¥Ï†Å Í≥ÑÏÇ∞Ïóê ÏÇ¨Ïö©Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e16561fbcf4a6c2f2dd9afb54001a93a98731fdc" translate="yes" xml:space="preserve">
          <source>Computes the eigenvalues and eigenvectors of a real square matrix.</source>
          <target state="translated">Ïã§Ïàò Ï†úÍ≥± ÌñâÎ†¨Ïùò Í≥†Ïú† Í∞íÍ≥º Í≥†Ïú† Î≤°ÌÑ∞Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="52f9bd141b84eda92242b06f7f6d290a253966b4" translate="yes" xml:space="preserve">
          <source>Computes the element-wise angle (in radians) of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ &lt;code&gt;input&lt;/code&gt; ÌÖêÏÑú Ïùò ÏöîÏÜå Î≥Ñ Í∞ÅÎèÑ (ÎùºÎîîÏïà Îã®ÏúÑ)Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="47dc21f5f4986e27abb977788d16b590a07a71d6" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ &lt;code&gt;input&lt;/code&gt; ÌÖêÏÑú Ïùò ÏöîÏÜå Î≥Ñ Ïº§Î†àÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="9a964097a08f63a42878b22585751fd92a8682df" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor. If :attr`input` has a non-complex dtype, this function just returns &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ &lt;code&gt;input&lt;/code&gt; ÌÖêÏÑú Ïùò ÏöîÏÜå Î≥Ñ Ïº§Î†àÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ . : attr`input`Ïóê Î≥µÏû°ÌïòÏßÄ ÏïäÏùÄ dtypeÏù¥ ÏûàÏúºÎ©¥Ïù¥ Ìï®ÏàòÎäî &lt;code&gt;input&lt;/code&gt; ÏùÑ Î∞òÌôò Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="8e315d5a6fd3a46bc729f0eda3f3fb92b601c50b" translate="yes" xml:space="preserve">
          <source>Computes the element-wise greatest common divisor (GCD) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÏöîÏÜå Î≥Ñ ÏµúÎåÄ Í≥µÏïΩÏàò (GCD)Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="91ee032a4ff000ec84d35b04317442f4984e6937" translate="yes" xml:space="preserve">
          <source>Computes the element-wise least common multiple (LCM) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Í≥º &lt;code&gt;other&lt;/code&gt; Ïùò ÏöîÏÜå Î≥Ñ ÏµúÏÜå Í≥µÎ∞∞Ïàò (LCM)Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="8e7b021553644e46362536880a5431b72a0587b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="68b2bf2409a8b5d2be8906244e0a49294d2e9679" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a94b7ab9f10124804e19d264d7207840dc6728d2" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor.</target>
        </trans-unit>
        <trans-unit id="117610e848f679548feba89108c832e6e9a20124" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9572b2dd91b0d03ca02c1394c350df1174a646b0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="256665a803b61ee6a430a86e174d4bca0ee69abe" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c172a20dd69ad22bb7b0a4de9500f73f3c3f98b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="c8837d2e60640bc8199344497981535b4ce62699" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b73dd1c77581f85619be2c0a672fa472582b7ef0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c821156c5be7a7c6ec010c82496a8862539fc670" translate="yes" xml:space="preserve">
          <source>Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6b3693910cf57f242c4267e044d56aef8263e01c" translate="yes" xml:space="preserve">
          <source>Computes the element-wise remainder of division.</source>
          <target state="translated">Computes the element-wise remainder of division.</target>
        </trans-unit>
        <trans-unit id="9003dd6b7c6711ac836cacfed24f4e0435a56aec" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element.</source>
          <target state="translated">Computes the error function of each element.</target>
        </trans-unit>
        <trans-unit id="29774d569920339acd042c100fd75031d60d53ac" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element. The error function is defined as follows:</source>
          <target state="translated">Computes the error function of each element. The error function is defined as follows:</target>
        </trans-unit>
        <trans-unit id="2434be214f6e5985b87d4c052539ff329a7f0314" translate="yes" xml:space="preserve">
          <source>Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cb83638df4a6cb7d822fc5fb54ba573a3cd29240" translate="yes" xml:space="preserve">
          <source>Computes the gradient of current tensor w.r.t. graph leaves.</source>
          <target state="translated">Computes the gradient of current tensor w.r.t. graph leaves.</target>
        </trans-unit>
        <trans-unit id="ce4470aa5bf338d4ad1d1c606fbb7a81df3b9429" translate="yes" xml:space="preserve">
          <source>Computes the histogram of a tensor.</source>
          <target state="translated">Computes the histogram of a tensor.</target>
        </trans-unit>
        <trans-unit id="28063e348dc010ab497d4b723f1ef5afa57d8d1a" translate="yes" xml:space="preserve">
          <source>Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a8f61c7d805f4a069d9c46e57aa8a4eda42a714f" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ac7da6502603da8eebb2a12646d0b0cbe100d072" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;. The inverse error function is defined in the range</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; . The inverse error function is defined in the range</target>
        </trans-unit>
        <trans-unit id="e0fc6a3f3385283ab40e97e09d370d8da0903b9c" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42895543a1b54cd19094b1bf3f3033fd72a6293" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8157bbb847ce3db3aa43a43004aeed9105153b04" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="751c20273a8826ac2930fa4c66f27dae7e42581e" translate="yes" xml:space="preserve">
          <source>Computes the inverse of a symmetric positive-definite matrix</source>
          <target state="translated">Computes the inverse of a symmetric positive-definite matrix</target>
        </trans-unit>
        <trans-unit id="5ad2395a52ab42ac3844b6eab85a745737496c2b" translate="yes" xml:space="preserve">
          <source>Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0ab463de8fae2192880ba9572b3595b33254233d" translate="yes" xml:space="preserve">
          <source>Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="53d62470ba220fb7afc0e820b9e1d137675f35a7" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="df866b02f2fffb4ec82fb366d91cc209748d5c59" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e2bd9739508dd66799ff1e51b1381c47e1c1143b" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</target>
        </trans-unit>
        <trans-unit id="1c827fe5b069aea41e126efde791986643cae21d" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b12e2ad844db4fa1d8dc61bc76b5cc7fede08ebb" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="9a50e0c97fa82cb476bece9bdcb5b778a06bbe5b" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c4aeca69a4a2b0bea72290f1b36d08fba25dfffc" translate="yes" xml:space="preserve">
          <source>Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt;. This function will be faster if the rows are contiguous.</source>
          <target state="translated">Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt; . This function will be faster if the rows are contiguous.</target>
        </trans-unit>
        <trans-unit id="52fb62cb631b335790a2488186d42d7739c05544" translate="yes" xml:space="preserve">
          <source>Computes the solution to the least squares and least norm problems for a full rank matrix</source>
          <target state="translated">Computes the solution to the least squares and least norm problems for a full rank matrix</target>
        </trans-unit>
        <trans-unit id="cf99caa72d6917de8396ddac94469751042f2b03" translate="yes" xml:space="preserve">
          <source>Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cbcbb3162b2e52eeda82ef5d679b6c19cdfaf1b5" translate="yes" xml:space="preserve">
          <source>Computing dependencies</source>
          <target state="translated">Computing dependencies</target>
        </trans-unit>
        <trans-unit id="6ad79ab6353b1eee8ebbc085e10d17c4fcfb024f" translate="yes" xml:space="preserve">
          <source>Concat</source>
          <target state="translated">Concat</target>
        </trans-unit>
        <trans-unit id="4ddddf59160aed751a5f07007a03c044d9753ba8" translate="yes" xml:space="preserve">
          <source>Concatenates a sequence of tensors along a new dimension.</source>
          <target state="translated">Concatenates a sequence of tensors along a new dimension.</target>
        </trans-unit>
        <trans-unit id="1b32473fe5755da0d831ffcfc10b7cdd982c9092" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</target>
        </trans-unit>
        <trans-unit id="a9ec200f382eb7e2b6f899132d934e4d406c3bcb" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</target>
        </trans-unit>
        <trans-unit id="1f8d364de32d6d9b96e9b370a225d6dab6d594c7" translate="yes" xml:space="preserve">
          <source>Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</source>
          <target state="translated">Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; &lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</target>
        </trans-unit>
        <trans-unit id="30e59e381077a379cb9607bde3a4a42eb3f45ab9" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</target>
        </trans-unit>
        <trans-unit id="7f6c99c148a673c2e06016f795e6fb113b6d25b3" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</target>
        </trans-unit>
        <trans-unit id="a3d8f578f82ef706b4638b32cc8ac9d832c7f7c7" translate="yes" xml:space="preserve">
          <source>ConstantPad1d</source>
          <target state="translated">ConstantPad1d</target>
        </trans-unit>
        <trans-unit id="998c295145e82549d2f17c1a6ba6c23bef09837b" translate="yes" xml:space="preserve">
          <source>ConstantPad2d</source>
          <target state="translated">ConstantPad2d</target>
        </trans-unit>
        <trans-unit id="4a15d68828e68d36dcfe86ffe64b4e4f826c7f8a" translate="yes" xml:space="preserve">
          <source>ConstantPad3d</source>
          <target state="translated">ConstantPad3d</target>
        </trans-unit>
        <trans-unit id="0a41b38808acdf43af00c5eb932dd87575946224" translate="yes" xml:space="preserve">
          <source>ConstantPadNd</source>
          <target state="translated">ConstantPadNd</target>
        </trans-unit>
        <trans-unit id="0f386d7e7881b32fa39cb7b62bdb15c0f3a4c0e1" translate="yes" xml:space="preserve">
          <source>Constants</source>
          <target state="translated">Constants</target>
        </trans-unit>
        <trans-unit id="cba7185d08d214544898ab239c21fb5414c4fc69" translate="yes" xml:space="preserve">
          <source>Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt;.</source>
          <target state="translated">Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3e93ee8b7e4549e7e136ce0f0147ddef90f905dc" translate="yes" xml:space="preserve">
          <source>Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="14faa516e57e5e69781808c4221cf6c62dc83c10" translate="yes" xml:space="preserve">
          <source>Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="243abd8ffcbf23837c06014609b82a14c78192f1" translate="yes" xml:space="preserve">
          <source>Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="25ef738c9d86e2fc8c4436c90a8dc11de37e7492" translate="yes" xml:space="preserve">
          <source>Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</source>
          <target state="translated">Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</target>
        </trans-unit>
        <trans-unit id="e3591588e6daf80e91783781c9de4873426a6c50" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="0b9d4f7f98afc347ec7829b73a659d20e47f4656" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="a8efeef3cc601befcf295dbb51e707b10b91e402" translate="yes" xml:space="preserve">
          <source>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="63a7411b909e98e9e117a9221e16319404fd9e0b" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="40a8ca8c7bf720c8a0f63021a990b502e7b406b0" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="c44ff1a6ea4f78ec6e55eb5a4dd0b0381511fd66" translate="yes" xml:space="preserve">
          <source>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="0bac8f59a0445e0c89aaa35b2ecaf7957f6cbc76" translate="yes" xml:space="preserve">
          <source>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="ed68461c0a66e54bb8577296b733b09ee8628e1f" translate="yes" xml:space="preserve">
          <source>Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fa58d20a53a578ca582a51e6082babe2633522d9" translate="yes" xml:space="preserve">
          <source>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a RetinaNet model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="c060b450d2f139169d01201742e2de6bd6813c33" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c1b1411f7ad7f2a9785e3f797d1ed1dca55c0522" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="946788267a37fd704ae9ab9be9bb919471421254" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c3079515cd84378d124e9d68610779cdd6cd48ea" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="17f894712d4813d2bb2bfed6f1251b3bc7ad7f28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21791c63e7cf531ba91fa49b8e8790ebd26c8e28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="3f22df20de166bd700f665def840237abc20c0b1" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="7cbc2c2e6f7677b79f4e49073177abc0eaf5c360" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f93f080d3b4b5a3e3dc0939299e73f9681c6cfa7" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="be72a7a131ec1a761dd654077e838e6f0e7ea737" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;. A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt;, in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; . A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt; , in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="0e69f365d19e40e598e945db7b7eaf81602d0597" translate="yes" xml:space="preserve">
          <source>Constructs a tensor with &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">Constructs a tensor with &lt;code&gt;data&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="26d1a380017a1ee5db1ae9f2be051aa31298d8a4" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning.</target>
        </trans-unit>
        <trans-unit id="ecedca4e6711cc4546829104ac643201429f438a" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</target>
        </trans-unit>
        <trans-unit id="e040a458f46532a90ec69fa0b4bfc33ba151c98b" translate="yes" xml:space="preserve">
          <source>Containers</source>
          <target state="translated">Containers</target>
        </trans-unit>
        <trans-unit id="a5f7ef3dcfb494670b1f12c053712004a30a5030" translate="yes" xml:space="preserve">
          <source>Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</source>
          <target state="translated">Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</target>
        </trans-unit>
        <trans-unit id="69cef769a134b0d4a56c19cba61f8a8ed316d60c" translate="yes" xml:space="preserve">
          <source>Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt;, which is required to correctly execute a distributed autograd pass.</source>
          <target state="translated">Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt; , which is required to correctly execute a distributed autograd pass.</target>
        </trans-unit>
        <trans-unit id="05fa9ad5bb0946657dbbab246393aea9c7883c54" translate="yes" xml:space="preserve">
          <source>Context-manager that disabled gradient calculation.</source>
          <target state="translated">Context-manager that disabled gradient calculation.</target>
        </trans-unit>
        <trans-unit id="98ceadd98235bb51cb8307cf83dd7c6d79e59720" translate="yes" xml:space="preserve">
          <source>Context-manager that enables gradient calculation.</source>
          <target state="translated">Context-manager that enables gradient calculation.</target>
        </trans-unit>
        <trans-unit id="8479c988b868dfa3b4c887d081e83a92acf0bb43" translate="yes" xml:space="preserve">
          <source>Context-manager that sets gradient calculation to on or off.</source>
          <target state="translated">Context-manager that sets gradient calculation to on or off.</target>
        </trans-unit>
        <trans-unit id="4fbe3836d5db9ff249ce993c595d3f0f979273c3" translate="yes" xml:space="preserve">
          <source>Conv</source>
          <target state="translated">Conv</target>
        </trans-unit>
        <trans-unit id="0f579280c2328a913d6d725180603d671f003e1c" translate="yes" xml:space="preserve">
          <source>Conv1d</source>
          <target state="translated">Conv1d</target>
        </trans-unit>
        <trans-unit id="40b3c3b8c860add6637a732716bb59fe0a472372" translate="yes" xml:space="preserve">
          <source>Conv2d</source>
          <target state="translated">Conv2d</target>
        </trans-unit>
        <trans-unit id="11775e3bcd7c158060f3c37b634846211f59b0a8" translate="yes" xml:space="preserve">
          <source>Conv3d</source>
          <target state="translated">Conv3d</target>
        </trans-unit>
        <trans-unit id="7bd21924a004fee82a9b59ee3da28ba6cf10e0cd" translate="yes" xml:space="preserve">
          <source>ConvBn1d</source>
          <target state="translated">ConvBn1d</target>
        </trans-unit>
        <trans-unit id="b49eb764a64698e64c0e874c6a40f0bd980f5854" translate="yes" xml:space="preserve">
          <source>ConvBn2d</source>
          <target state="translated">ConvBn2d</target>
        </trans-unit>
        <trans-unit id="f2a4cf1ba0285a54525c0f0bced1d81a716af040" translate="yes" xml:space="preserve">
          <source>ConvBnReLU1d</source>
          <target state="translated">ConvBnReLU1d</target>
        </trans-unit>
        <trans-unit id="9a62f54f222bf04d2c6e425b49c602b198a2f54e" translate="yes" xml:space="preserve">
          <source>ConvBnReLU2d</source>
          <target state="translated">ConvBnReLU2d</target>
        </trans-unit>
        <trans-unit id="02be55e7a33c6df1aae2e7f342fb637e9ac77c6b" translate="yes" xml:space="preserve">
          <source>ConvReLU1d</source>
          <target state="translated">ConvReLU1d</target>
        </trans-unit>
        <trans-unit id="ee43a211652ce7b140d6ac846ff3539f57bfe4ce" translate="yes" xml:space="preserve">
          <source>ConvReLU2d</source>
          <target state="translated">ConvReLU2d</target>
        </trans-unit>
        <trans-unit id="9c9f7992fc4e6c43d3ef3ba68725dbe4ae51d5e7" translate="yes" xml:space="preserve">
          <source>ConvReLU3d</source>
          <target state="translated">ConvReLU3d</target>
        </trans-unit>
        <trans-unit id="afca9fffa388b0f4407644f89fc3d2572519c33f" translate="yes" xml:space="preserve">
          <source>ConvTranspose1d</source>
          <target state="translated">ConvTranspose1d</target>
        </trans-unit>
        <trans-unit id="fc42d54df990ea8e2fd96576309fbe506686556e" translate="yes" xml:space="preserve">
          <source>ConvTranspose2d</source>
          <target state="translated">ConvTranspose2d</target>
        </trans-unit>
        <trans-unit id="aa70f25fef0f0928f03585abe233d18e1b7a43bf" translate="yes" xml:space="preserve">
          <source>ConvTranspose3d</source>
          <target state="translated">ConvTranspose3d</target>
        </trans-unit>
        <trans-unit id="bf6b6d744534af31ad6085d41b063c70edf02466" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</target>
        </trans-unit>
        <trans-unit id="6e35177ef47547c230ba4dcc90eda6d927bb9e5a" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</target>
        </trans-unit>
        <trans-unit id="5a0770a286742ba2629162d473ed7de8b93bc405" translate="yes" xml:space="preserve">
          <source>Convert one vector to the parameters</source>
          <target state="translated">Convert one vector to the parameters</target>
        </trans-unit>
        <trans-unit id="be41e2cca6d10c1b2760ba9429806df5479848bc" translate="yes" xml:space="preserve">
          <source>Convert parameters to one vector</source>
          <target state="translated">Convert parameters to one vector</target>
        </trans-unit>
        <trans-unit id="01733ec2af89cb05f3440f813cca55cd11cb2b18" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6906a54aac4ab7e793a9985a2b6c90ca5f574d9a" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;. If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt;, no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt;. Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; . If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt; , no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt; . Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</target>
        </trans-unit>
        <trans-unit id="5490b3bea11fc41ac534c3f667531da495228f6a" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</source>
          <target state="translated">Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</target>
        </trans-unit>
        <trans-unit id="a8473a264e544b1bee1239b9dbf9ad1a93b701b5" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a quantized tensor with given scale and zero point.</source>
          <target state="translated">Converts a float tensor to a quantized tensor with given scale and zero point.</target>
        </trans-unit>
        <trans-unit id="7757dbf24e068e638ad997ae9c4d2e07adceb8c8" translate="yes" xml:space="preserve">
          <source>Convolution Layers</source>
          <target state="translated">Convolution Layers</target>
        </trans-unit>
        <trans-unit id="be084db8daa3740ea18a2c2e59749d4c0010cf94" translate="yes" xml:space="preserve">
          <source>Convolution functions</source>
          <target state="translated">Convolution functions</target>
        </trans-unit>
        <trans-unit id="989f24495d4e5092a0b9ea78596dc97ffbe13f9f" translate="yes" xml:space="preserve">
          <source>Conv{1,2,3}D</source>
          <target state="translated">Conv{1,2,3}D</target>
        </trans-unit>
        <trans-unit id="1737f5913c8956ac817ace6ab2658a389afc44d8" translate="yes" xml:space="preserve">
          <source>Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</source>
          <target state="translated">Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ffea1905ca53aaa0c2e1dd04c44cfd1d1d610e9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="b20573e90f9dd7fefb1d1ba5fccbebf327c4350e" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="2aa7f8afed131bd721ebda279fdf8b30aaa83be9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="3fbe05d7c19e34a099d5b27aecd7e50cc995055f" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="c58511363c704a11c0530ab36e516cead0a26399" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</source>
          <target state="translated">Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</target>
        </trans-unit>
        <trans-unit id="31514897cedc5e7c028b472351d6c9c6b226eacf" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b302a58928c86985fd5405fb9b12e6f3b385a56d" translate="yes" xml:space="preserve">
          <source>Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt; . For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt; , then the &lt;code&gt;i&lt;/code&gt; th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt; th row of &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b91c805903c6cb0b3510a18d72ca193bc741d244" translate="yes" xml:space="preserve">
          <source>Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="278e4ebde33e7e777180bcca75800abdc23470b8" translate="yes" xml:space="preserve">
          <source>Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="bbf794aba20b276c8548169896a15590fe109e14" translate="yes" xml:space="preserve">
          <source>CosineEmbeddingLoss</source>
          <target state="translated">CosineEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="38519b5b3e654f4e1b794c35fd7670bb326f9738" translate="yes" xml:space="preserve">
          <source>CosineSimilarity</source>
          <target state="translated">CosineSimilarity</target>
        </trans-unit>
        <trans-unit id="09bf9d0f08c3cee1a60852d335fb4cf2f77d1281" translate="yes" xml:space="preserve">
          <source>Count the frequency of each value in an array of non-negative ints.</source>
          <target state="translated">Count the frequency of each value in an array of non-negative ints.</target>
        </trans-unit>
        <trans-unit id="fa674450dd813dec8a084d834fc97c369ca9a763" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0da99e01ab08d505f8e3aacba31f722f60ab5af" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;. If no dim is specified then all non-zeros in the tensor are counted.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; . If no dim is specified then all non-zeros in the tensor are counted.</target>
        </trans-unit>
        <trans-unit id="8bd108ea71b4b5faec418d13c1eaaf112af7d5fc" translate="yes" xml:space="preserve">
          <source>Create a block diagonal matrix from provided tensors.</source>
          <target state="translated">Create a block diagonal matrix from provided tensors.</target>
        </trans-unit>
        <trans-unit id="661a797fdd966423deb47c56a841ce6d565626a3" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="b873bdb21fd500fbd503a0fa2ae31f161f0a0486" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8b2c6a8315871636ff748acb6e7ca0ce33cac24a" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8c6bb577e26b7162d7d6c68e8046a3fec50e1ed2" translate="yes" xml:space="preserve">
          <source>Create a qat module from a float module or qparams_dict</source>
          <target state="translated">float Î™®Îìà ÎòêÎäî qparams_dictÏóêÏÑú qat Î™®Îìà ÎßåÎì§Í∏∞</target>
        </trans-unit>
        <trans-unit id="608ebd71f80f5c1a22f515709f259847e91c3798" translate="yes" xml:space="preserve">
          <source>Create a quantized module from a float module or qparams_dict</source>
          <target state="translated">float Î™®Îìà ÎòêÎäî qparams_dictÏóêÏÑú ÏñëÏûêÌôî Îêú Î™®Îìà ÎßåÎì§Í∏∞</target>
        </trans-unit>
        <trans-unit id="25224bf405304e1ab57eac3f4ba2017fad81ac0a" translate="yes" xml:space="preserve">
          <source>Create a symbolic function named &lt;code&gt;symbolic&lt;/code&gt; in the corresponding Function class.</source>
          <target state="translated">Ìï¥Îãπ Function ÌÅ¥ÎûòÏä§Ïóê &lt;code&gt;symbolic&lt;/code&gt; Ïù¥ÎùºÎäî Í∏∞Ìò∏ Ìï®ÏàòÎ•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="05e63a314a968dccb93df86ac820277d35c3b513" translate="yes" xml:space="preserve">
          <source>Create a view of an existing &lt;code&gt;torch.Tensor&lt;/code&gt;&lt;code&gt;input&lt;/code&gt; with specified &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;storage_offset&lt;/code&gt;.</source>
          <target state="translated">ÏßÄÏ†ïÎêú &lt;code&gt;size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; Î∞è &lt;code&gt;storage_offset&lt;/code&gt; Î°ú Í∏∞Ï°¥ &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;code&gt;input&lt;/code&gt; ÏùòÎ≥¥Í∏∞Î•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b77a1bd9ce30851f58a1b21fe4ac8853dde6d2a8" translate="yes" xml:space="preserve">
          <source>Create special chart by collecting charts tags in &amp;lsquo;scalars&amp;rsquo;. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.</source>
          <target state="translated">'Ïä§ÏπºÎùº'ÏóêÏÑú Ï∞®Ìä∏ ÌÉúÍ∑∏Î•º ÏàòÏßëÌïòÏó¨ ÌäπÎ≥ÑÌïú Ï∞®Ìä∏Î•º ÎßåÎì≠ÎãàÎã§. Ïù¥ Ìï®ÏàòÎäî Í∞Å SummaryWriter () Í∞úÏ≤¥Ïóê ÎåÄÌï¥ Ìïú Î≤àÎßå Ìò∏Ï∂ú Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÌÖêÏÑú Î≥¥ÎìúÏóê Î©îÌÉÄ Îç∞Ïù¥ÌÑ∞ Îßå Ï†úÍ≥µÌïòÎØÄÎ°ú ÌõàÎ†® Î£®ÌîÑ Ï†ÑÌõÑÏóê Ìï®ÏàòÎ•º Ìò∏Ï∂ú Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a4594b741f646db69f78e37c46423c0ff117e55e" translate="yes" xml:space="preserve">
          <source>Creates Embedding instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">ÏßÄÏ†ïÎêú 2 Ï∞®Ïõê FloatTensorÏóêÏÑú Embedding Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a6e25ee28f09c0ec7810655de85d0843eaddac29" translate="yes" xml:space="preserve">
          <source>Creates EmbeddingBag instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">ÏßÄÏ†ïÎêú 2 Ï∞®Ïõê FloatTensorÏóêÏÑú EmbeddingBag Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7ab70bdf8d978808ae2341155ab591efacf83cbe" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; ÏóêÏÑú &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; Î•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="380a5b530b50dcf602f5ae20b20833a1876b83b8" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; ÏóêÏÑú &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; Î•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="8ff7109193ed0423f307bf24dbf6b88c15e440c1" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;SummaryWriter&lt;/code&gt; that will write out events and summaries to the event file.</source>
          <target state="translated">Ïù¥Î≤§Ìä∏ Î∞è ÏöîÏïΩÏùÑ Ïù¥Î≤§Ìä∏ ÌååÏùºÏóê Í∏∞Î°ù ÌïòÎäî &lt;code&gt;SummaryWriter&lt;/code&gt; Î•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="ba6567ea492c95adff11560e78e65d6f2fa6dfe4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for C++.</source>
          <target state="translated">C ++ Ïö© &lt;code&gt;setuptools.Extension&lt;/code&gt; ÏùÑ ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b766aaed745dd6ce4f2529c6188fca66de086dd4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for CUDA/C++.</source>
          <target state="translated">CUDA / C ++ Ïö© &lt;code&gt;setuptools.Extension&lt;/code&gt; ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0ccb6b54dc9399c2c85ab56d56ee522676eba886" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</source>
          <target state="translated">ÎåÄÏÉÅÍ≥º Ï∂úÎ†• ÏÇ¨Ïù¥Ïùò Ïù¥ÏßÑ ÍµêÏ∞® ÏóîÌä∏Î°úÌîºÎ•º Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3d34756a1f2c85c7f54f427b38b59a7f138b4091" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given input tensors</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÌÖêÏÑúÏùò ÏÜêÏã§ÏùÑ Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b30adb8efde8fc483debf3f678fe0a50c0f3a700" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given inputs</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÏÜêÏã§ÏùÑ Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="db68393122ce4917cd40e5341fb476ef56ce47be" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean absolute error (MAE) between each element in the input</source>
          <target state="translated">ÏûÖÎ†•Ïùò Í∞Å ÏöîÏÜå Í∞ÑÏùò ÌèâÍ∑† Ï†àÎåÄ Ïò§Ï∞® (MAE)Î•º Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="dc0bceb74569c8f166528464397135db651de1e9" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input</source>
          <target state="translated">ÏûÖÎ†•Ïùò Í∞Å ÏöîÏÜå ÏÇ¨Ïù¥Ïùò ÌèâÍ∑† Ï†úÍ≥± Ïò§Ï∞® (Ï†úÍ≥± L2 ÌëúÏ§Ä)Î•º Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c567813437a56bca1c0ee05ac4bbbc7a9a0f0b86" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given an input tensors</source>
          <target state="translated">ÏûÖÎ†• ÌÖêÏÑúÍ∞Ä Ï£ºÏñ¥ÏßÄÎ©¥ ÏÇºÏ§ë ÏÑ† ÏÜêÏã§ÏùÑ Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="54c0c219984f4a50b8cfc1212d879c396ddab80a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given input tensors</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÌÖêÏÑúÏùò ÏÇºÏ§ë ÏÜêÏã§ÏùÑ Ï∏°Ï†ïÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3ab7e1fd5db78a2e9eadeb7c1fe0ccb243aebcd4" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input</source>
          <target state="translated">ÏûÖÎ†• Í∞Ñ Îã§Ï§ë ÌÅ¥ÎûòÏä§ Î∂ÑÎ•ò ÌûåÏßÄ ÏÜêÏã§ (ÎßàÏßÑ Í∏∞Î∞ò ÏÜêÏã§)ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0f6e83829c9a9db048daa12ae2ef6905382cf541" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input</source>
          <target state="translated">ÏûÖÎ†• Í∞Ñ Îã§Ï§ë ÌÅ¥ÎûòÏä§ Îã§Ï§ë Î∂ÑÎ•ò ÌûåÏßÄ ÏÜêÏã§ (ÎßàÏßÑ Í∏∞Î∞ò ÏÜêÏã§)ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="74a30cc1dfda2e2ca6b72fcb42e123c4afa11917" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input</source>
          <target state="translated">ÏûÖÎ†• Í∞Ñ ÏµúÎåÄ ÏóîÌä∏Î°úÌîºÎ•º Í∏∞Î∞òÏúºÎ°ú Îã§Ï§ë Î†àÏù¥Î∏î ÏùºÎåÄÏùº ÏÜêÏã§ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="38862788d574ffd77ba587c5570ecdb676e04e5f" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a two-class classification logistic loss between input tensor</source>
          <target state="translated">ÏûÖÎ†• ÌÖêÏÑú Í∞ÑÏùò 2 ÌÅ¥ÎûòÏä§ Î∂ÑÎ•ò Î°úÏßÄÏä§Ìã± ÏÜêÏã§ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Í∏∞Ï§ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d4196579876a20368dab571bd9df29f488cfb987" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ Ï†àÎåÄ Ïò§Ï∞®Í∞Ä Î≤†ÌÉÄ Ïù¥ÌïòÎ°ú Îñ®Ïñ¥ÏßÄÎ©¥ Ï†úÍ≥±Ìï≠ÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ L1 Ìï≠ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í∏∞Ï§ÄÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="416d9c5bdd3061ae8e8917cecda26e9fe1f6412a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the &lt;code&gt;MSELoss&lt;/code&gt; and in some cases prevents exploding gradients (e.g. see &lt;code&gt;Fast R-CNN&lt;/code&gt; paper by Ross Girshick). Also known as the Huber loss:</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ Ï†àÎåÄ Ïò§Ï∞®Í∞Ä Î≤†ÌÉÄ Ïù¥ÌïòÎ°ú Îñ®Ïñ¥ÏßÄÎ©¥ Ï†úÍ≥±Ìï≠ÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ L1 Ìï≠ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í∏∞Ï§ÄÏùÑ ÎßåÎì≠ÎãàÎã§. &lt;code&gt;MSELoss&lt;/code&gt; Î≥¥Îã§ ÌäπÏù¥ ÏπòÏóê Îçú ÎØºÍ∞êÌïòÎ©∞ Í≤ΩÏö∞Ïóê Îî∞Îùº Í∏âÍ≤©Ìïú Í∏∞Ïö∏Í∏∞Î•º Î∞©ÏßÄÌï©ÎãàÎã§ (Ïòà : Ross GirshickÏùò &lt;code&gt;Fast R-CNN&lt;/code&gt; ÎÖºÎ¨∏ Ï∞∏Ï°∞ ). Huber ÏÜêÏã§Ïù¥ÎùºÍ≥†ÎèÑÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1ebf98c1eaeed448f01f072825db935178bbb990" translate="yes" xml:space="preserve">
          <source>Creates a new distributed group.</source>
          <target state="translated">ÏÉà Î∂ÑÏÇ∞ Í∑∏Î£πÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="073fb44c832f84ccfa42ff94accd75b4dd72e7da" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from</source>
          <target state="translated">Í∞íÏùò Í∞ÑÍ≤©Ïù¥ Í∑†Îì± Ìïú 1 Ï∞®Ïõê ÌÖêÏÑú ÌÅ¨Í∏∞ &lt;code&gt;steps&lt;/code&gt; Î•º ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="350152525d5dacc5decf599c92018f7fa5823d28" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive.</source>
          <target state="translated">Í∞íÏù¥ &lt;code&gt;start&lt;/code&gt; Î∂ÄÌÑ∞ &lt;code&gt;end&lt;/code&gt; Í∑†Îì± Ìïú Í∞ÑÍ≤© ( Ìè¨Ìï®) Ïù∏ 1 Ï∞®Ïõê ÌÖêÏÑú ÌÅ¨Í∏∞ &lt;code&gt;steps&lt;/code&gt; Î•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="9f41c69b98f8c4bce082aab055bcac8455947a3e" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive. That is, the value are:</source>
          <target state="translated">Í∞íÏù¥ &lt;code&gt;start&lt;/code&gt; Î∂ÄÌÑ∞ &lt;code&gt;end&lt;/code&gt; Í∑†Îì± Ìïú Í∞ÑÍ≤© ( Ìè¨Ìï®) Ïù∏ 1 Ï∞®Ïõê ÌÖêÏÑú ÌÅ¨Í∏∞ &lt;code&gt;steps&lt;/code&gt; Î•º ÎßåÎì≠ÎãàÎã§ . Ï¶â, Í∞íÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9962e59b2ab133d8c3b2267ae38553f28f8a79bd" translate="yes" xml:space="preserve">
          <source>Creates a quantized module from a float module or qparams_dict.</source>
          <target state="translated">float Î™®Îìà ÎòêÎäî qparams_dictÏóêÏÑú ÏñëÏûêÌôî Îêú Î™®ÎìàÏùÑ ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d2147eb4da2cad834986bf746aeef9a006067d0f" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; Î°ú Ï±ÑÏõåÏßÑ ÌÅ¨Í∏∞ &lt;code&gt;size&lt;/code&gt; Ïùò ÌÖêÏÑúÎ•º ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="ed2b3c6c1dcd8ed54d69945a6e4acc25f855c267" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. The tensor&amp;rsquo;s dtype is inferred from &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; Î°ú Ï±ÑÏõåÏßÑ ÌÅ¨Í∏∞ &lt;code&gt;size&lt;/code&gt; Ïùò ÌÖêÏÑúÎ•º ÎßåÎì≠ÎãàÎã§ . ÌÖêÏÑúÏùò dtypeÏùÄ &lt;code&gt;fill_value&lt;/code&gt; ÏóêÏÑú Ïú†Ï∂îÎê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="12f064b3e975b58c920e1056343b0cd452f5ecab" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">ÎàÑÍµ¨ (ÏùòÌï¥ ÏßÄÏ†ïÎêú ÌäπÏ†ï 2D ÌèâÎ©¥Ïùò ÎåÄÍ∞ÅÏÑ† ÌÖêÏÑú ÏûëÏÑ± &lt;code&gt;dim1&lt;/code&gt; Î∞è &lt;code&gt;dim2&lt;/code&gt; )Î°ú Ï±ÑÏõåÏßÑÎã§ &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a4e06d7745c58e373370f1d2833461757426df92" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.</source>
          <target state="translated">ÎàÑÍµ¨ (ÏùòÌï¥ ÏßÄÏ†ïÎêú ÌäπÏ†ï 2D ÌèâÎ©¥Ïùò ÎåÄÍ∞ÅÏÑ† ÌÖêÏÑú ÏûëÏÑ± &lt;code&gt;dim1&lt;/code&gt; Î∞è &lt;code&gt;dim2&lt;/code&gt; )Î°ú Ï±ÑÏõåÏßÑÎã§ &lt;code&gt;input&lt;/code&gt; . Î∞∞Ïπò Îêú ÎåÄÍ∞Å ÌñâÎ†¨ÏùÑ ÏâΩÍ≤å ÏÉùÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ Î∞òÌôò Îêú ÌÖêÏÑúÏùò ÎßàÏßÄÎßâ 2 Ï∞®ÏõêÏúºÎ°ú ÌòïÏÑ±Îêú 2D ÌèâÎ©¥Ïù¥ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏÑ†ÌÉùÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="54d06cd8eb770a4665e06b480e41c18b35882c33" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution.</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; Î•º Ïã§ÌñâÌïòÍ≥†Ïù¥ Ïã§Ìñâ Í≤∞Í≥º Í∞íÏóê ÎåÄÌïú Ï∞∏Ï°∞Î•º Ïã§ÌñâÌïòÎäî ÎπÑÎèôÍ∏∞ ÏûëÏóÖÏùÑ ÎßåÎì≠ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a528098d6266957ce08cc5436471c4e687e167c2" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution. &lt;code&gt;fork&lt;/code&gt; will return immediately, so the return value of &lt;code&gt;func&lt;/code&gt; may not have been computed yet. To force completion of the task and access the return value invoke &lt;code&gt;torch.jit.wait&lt;/code&gt; on the Future. &lt;code&gt;fork&lt;/code&gt; invoked with a &lt;code&gt;func&lt;/code&gt; which returns &lt;code&gt;T&lt;/code&gt; is typed as &lt;code&gt;torch.jit.Future[T]&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, &lt;code&gt;fork&lt;/code&gt; will not execute in parallel. &lt;code&gt;fork&lt;/code&gt; will also not execute in parallel when invoked while tracing, however the &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;wait&lt;/code&gt; calls will be captured in the exported IR Graph. .. warning:</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; Î•º Ïã§ÌñâÌïòÍ≥†Ïù¥ Ïã§Ìñâ Í≤∞Í≥º Í∞íÏóê ÎåÄÌïú Ï∞∏Ï°∞Î•º Ïã§ÌñâÌïòÎäî ÎπÑÎèôÍ∏∞ ÏûëÏóÖÏùÑ ÎßåÎì≠ÎãàÎã§ . &lt;code&gt;fork&lt;/code&gt; Îäî Ï¶âÏãú Î∞òÌôòÎêòÎØÄÎ°ú &lt;code&gt;func&lt;/code&gt; Ïùò Î∞òÌôò Í∞íÏùÄ ÏïÑÏßÅ Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÏïòÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÏûëÏóÖÏùÑ Í∞ïÏ†úÎ°ú ÏôÑÎ£åÌïòÍ≥† Î∞òÌôò Í∞íÏóê Ïï°ÏÑ∏Ïä§ ÌïòÎ†§Î©¥ FutureÏóêÏÑú &lt;code&gt;torch.jit.wait&lt;/code&gt; Î•º Ìò∏Ï∂ú Ìï©ÎãàÎã§. &lt;code&gt;T&lt;/code&gt; Î•º Î∞òÌôò ÌïòÎäî &lt;code&gt;func&lt;/code&gt; Î°ú Ìò∏Ï∂ú Îêú &lt;code&gt;fork&lt;/code&gt; Îäî &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; ÌòïÏãùÏúºÎ°ú ÏßÄÏ†ïÎê©ÎãàÎã§ . &lt;code&gt;fork&lt;/code&gt; Ìò∏Ï∂úÏùÄ ÏûÑÏùòÎ°ú Ï§ëÏ≤© Îê† Ïàò ÏûàÏúºÎ©∞ ÏúÑÏπò Î∞è ÌÇ§ÏõåÎìú Ïù∏ÏàòÎ°ú Ìò∏Ï∂ú Îê† Ïàò ÏûàÏäµÎãàÎã§. ÎπÑÎèôÍ∏∞ Ïã§ÌñâÏùÄ TorchScriptÏóêÏÑú Ïã§ÌñâÎê† ÎïåÎßå Î∞úÏÉùÌï©ÎãàÎã§. ÏàúÏàò ÌååÏù¥Ïç¨ÏóêÏÑú Ïã§ÌñâÌïòÎ©¥ &lt;code&gt;fork&lt;/code&gt; Í∞Ä Î≥ëÎ†¨Î°ú Ïã§ÌñâÎêòÏßÄ ÏïäÏäµÎãàÎã§. &lt;code&gt;fork&lt;/code&gt; Îäî Ï∂îÏ†ÅÌïòÎäî ÎèôÏïà Ìò∏Ï∂ú Îê† Îïå Î≥ëÎ†¨Î°ú Ïã§ÌñâÎêòÏßÄ ÏïäÏßÄÎßå &lt;code&gt;fork&lt;/code&gt; Î∞è &lt;code&gt;wait&lt;/code&gt; Ìò∏Ï∂úÏùÄ ÎÇ¥ Î≥¥ÎÇ∏ IR Í∑∏ÎûòÌîÑÏóê Ï∫°Ï≤òÎê©ÎãàÎã§. .. Í≤ΩÍ≥† :</target>
        </trans-unit>
        <trans-unit id="5c73fcf2c3b7a88652c30df05d4a49be7a9a3a82" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</source>
          <target state="translated">ÏùòÏÇ¨ ÎÇúÏàòÎ•º ÏÉùÏÑ±ÌïòÎäî ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏÉÅÌÉúÎ•º Í¥ÄÎ¶¨ÌïòÎäî ÏÉùÏÑ±Í∏∞ Í∞ùÏ≤¥Î•º ÎßåÎì§Í≥† Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7bf69af350942e6f19285c4071e0d9bc1d0390fb" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; functions.</source>
          <target state="translated">ÏùòÏÇ¨ ÎÇúÏàòÎ•º ÏÉùÏÑ±ÌïòÎäî ÏïåÍ≥†Î¶¨Ï¶òÏùò ÏÉÅÌÉúÎ•º Í¥ÄÎ¶¨ÌïòÎäî ÏÉùÏÑ±Í∏∞ Í∞ùÏ≤¥Î•º ÎßåÎì§Í≥† Î∞òÌôòÌï©ÎãàÎã§. ÎßéÏùÄ &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;ÎÇ¥Î∂Ä ÏûÑÏùò ÏÉòÌîåÎßÅ&lt;/a&gt; Ìï®Ïàò ÏóêÏÑú ÌÇ§ÏõåÎìú Ïù∏ÏàòÎ°ú ÏÇ¨Ïö©Îê©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="6db27514dc4a87b77455deb1f68ac2812f51084a" translate="yes" xml:space="preserve">
          <source>Creating TorchScript Code</source>
          <target state="translated">TorchScript ÏΩîÎìú ÏÉùÏÑ±</target>
        </trans-unit>
        <trans-unit id="14dc9f6dca53e0df6aaf4e5eb53a7efa6593aab3" translate="yes" xml:space="preserve">
          <source>Creating named tensors</source>
          <target state="translated">Î™ÖÎ™Ö Îêú ÌÖêÏÑú ÎßåÎì§Í∏∞</target>
        </trans-unit>
        <trans-unit id="733cbb78a0d286a0935c6c571466400a11828907" translate="yes" xml:space="preserve">
          <source>Creation Ops</source>
          <target state="translated">Ï∞ΩÏ°∞ ÏûëÏ†Ñ</target>
        </trans-unit>
        <trans-unit id="d66fef2dd7f330ad8c7b2de8229080af12ec8719" translate="yes" xml:space="preserve">
          <source>Creation of this class requires that &lt;code&gt;torch.distributed&lt;/code&gt; to be already initialized, by calling &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Ïù¥ ÌÅ¥ÎûòÏä§Ïùò Ï∞ΩÏ°∞Ìï¥Ïïº &lt;code&gt;torch.distributed&lt;/code&gt; Ìò∏Ï∂úÌïòÏó¨ Ïù¥ÎØ∏ Ï¥àÍ∏∞Ìôî Ìï† &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711f625ceb8a65f44028c4e7b7e4e818fc7445a7" translate="yes" xml:space="preserve">
          <source>CrossEntropyLoss</source>
          <target state="translated">CrossEntropyLoss</target>
        </trans-unit>
        <trans-unit id="5eec0b5e11526f784758a0afbe48baade8e71e9e" translate="yes" xml:space="preserve">
          <source>Current implementation of &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</source>
          <target state="translated">ÌòÑÏû¨ &lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; Íµ¨ÌòÑÏùÄ Î©îÎ™®Î¶¨ Ïò§Î≤Ñ Ìó§ÎìúÎ•º ÎèÑÏûÖÌïòÎØÄÎ°ú ÏûëÏùÄ ÌÖêÏÑúÍ∞Ä ÎßéÏùÄ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú ÏòàÍ∏∞Ïπò ÏïäÍ≤å ÎÜíÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ Î∞úÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ Í≤ΩÏö∞ ÌïòÎÇòÏùò ÌÅ∞ Íµ¨Ï°∞Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="96c1a2df528a935c7c808490ba7a2921d055775f" translate="yes" xml:space="preserve">
          <source>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ÌòÑÏû¨ Íµ¨ÌòÑÏùÄ Î™®Îì† Ìò∏Ï∂úÏóê Í∞ÄÏ§ëÏπòÎ•º Î∂ÄÏó¨ÌïòÎØÄÎ°ú ÏÑ±Îä•Ïù¥ Ï†ÄÌïòÎê©ÎãàÎã§. Ïò§Î≤Ñ Ìó§ÎìúÎ•º ÌîºÌïòÎ†§Î©¥ &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;Linear&lt;/code&gt; Î•º&lt;/a&gt; ÏÇ¨Ïö©ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="392192a4ae05351b1f7b21e538ff3e8ca534e67d" translate="yes" xml:space="preserve">
          <source>Currently &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; only supports &lt;code&gt;DistributedDataParallel&lt;/code&gt; (DDP) with single GPU per process. Use &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt;&lt;/a&gt; to convert &lt;code&gt;BatchNorm*D&lt;/code&gt; layer to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; before wrapping Network with DDP.</source>
          <target state="translated">ÌòÑÏû¨ &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; ÏùÄ ÌîÑÎ°úÏÑ∏Ïä§ Îãπ Îã®Ïùº GPUÍ∞ÄÏûàÎäî DDP ( &lt;code&gt;DistributedDataParallel&lt;/code&gt; ) Îßå ÏßÄÏõêÌï©ÎãàÎã§ . ÎÑ§Ìä∏ÏõåÌÅ¨Î•º DDPÎ°ú ÎûòÌïëÌïòÍ∏∞ Ï†ÑÏóê &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt; &lt;/a&gt; ÏùÑ ÏÇ¨Ïö© ÌïòÏó¨ &lt;code&gt;BatchNorm*D&lt;/code&gt; Î†àÏù¥Ïñ¥Î•º &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; ÏúºÎ°ú Î≥ÄÌôò Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a78ac4f002658abaaa21040baac2a600bf544edc" translate="yes" xml:space="preserve">
          <source>Currently in the CUDA implementation and the CPU implementation when dim is specified, &lt;code&gt;torch.unique&lt;/code&gt; always sort the tensor at the beginning regardless of the &lt;code&gt;sort&lt;/code&gt; argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; which avoids the sorting.</source>
          <target state="translated">ÌòÑÏû¨ CUDA Íµ¨ÌòÑ Î∞è dimÏù¥ ÏßÄÏ†ïÎêú CPU Íµ¨ÌòÑÏóêÏÑú &lt;code&gt;torch.unique&lt;/code&gt; Îäî &lt;code&gt;sort&lt;/code&gt; Ïù∏ÏàòÏóê Í¥ÄÍ≥ÑÏóÜÏù¥ Ìï≠ÏÉÅ Ï≤òÏùåÏóê ÌÖêÏÑúÎ•º Ï†ïÎ†¨Ìï©ÎãàÎã§ . Ï†ïÎ†¨Ïù¥ ÎäêÎ¶¥ Ïàò ÏûàÏúºÎØÄÎ°ú ÏûÖÎ†• ÌÖêÏÑúÍ∞Ä Ïù¥ÎØ∏ Ï†ïÎ†¨ Îêú Í≤ΩÏö∞ Ï†ïÎ†¨ÏùÑ ÌîºÌïòÎäî &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt; Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="c8a61a0d816f8b21bf7f4b46f888c832b03676e2" translate="yes" xml:space="preserve">
          <source>Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).</source>
          <target state="translated">ÌòÑÏû¨ Í≥µÍ∞Ñ Î∞è Ï≤¥Ï†Å ÏóÖ ÏÉòÌîåÎßÅÏù¥ ÏßÄÏõêÎê©ÎãàÎã§ (Ïòà : ÏòàÏÉÅ ÏûÖÎ†•ÏùÄ 4 Ï∞®Ïõê ÎòêÎäî 5 Ï∞®Ïõê).</target>
        </trans-unit>
        <trans-unit id="8a9d7a3e06793c263711ce33ba0ffdc49d409027" translate="yes" xml:space="preserve">
          <source>Currently supported operations and subsystems</source>
          <target state="translated">ÌòÑÏû¨ ÏßÄÏõêÎêòÎäî ÏûëÏóÖ Î∞è ÌïòÏúÑ ÏãúÏä§ÌÖú</target>
        </trans-unit>
        <trans-unit id="3b05bf74a2646cf7882607d1b6e8f3921b3f2124" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">ÌòÑÏû¨ ÏãúÍ∞ÑÏ†Å, Í≥µÍ∞ÑÏ†Å Î∞è Ï≤¥Ï†Å ÏÉòÌîåÎßÅÏù¥ ÏßÄÏõêÎê©ÎãàÎã§. Ï¶â, ÏòàÏÉÅ ÏûÖÎ†•ÏùÄ Î™®ÏñëÏù¥ 3D, 4D ÎòêÎäî 5DÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="af454dd72bdbaa9839806374d040bd77db038329" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">ÌòÑÏû¨ ÏãúÍ∞ÑÏ†Å, Í≥µÍ∞ÑÏ†Å Î∞è Ï≤¥Ï†Å ÏóÖ ÏÉòÌîåÎßÅÏù¥ ÏßÄÏõêÎê©ÎãàÎã§. Ï¶â, ÏòàÏÉÅ ÏûÖÎ†•ÏùÄ Î™®ÏñëÏù¥ 3D, 4D ÎòêÎäî 5DÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d49c5e3f020c5093dc8a1df7be3b5947455c7de6" translate="yes" xml:space="preserve">
          <source>Currently three initialization methods are supported:</source>
          <target state="translated">ÌòÑÏû¨ ÏÑ∏ Í∞ÄÏßÄ Ï¥àÍ∏∞Ìôî Î∞©Î≤ïÏù¥ ÏßÄÏõêÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ecab6b4c3bf958fdadbd30836155137da6ef98e2" translate="yes" xml:space="preserve">
          <source>Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128</source>
          <target state="translated">ÌòÑÏû¨ Ïú†Ìö®Ìïú Ïä§ÏπºÎùº Î∞è ÌÖêÏÑú Ï°∞Ìï©ÏùÄ 1. Î∂ÄÎèô dtype Î∞è torch.doubleÏùò Ïä§ÏπºÎùº 2. Ï†ïÏàò dtype Î∞è torch.longÏùò Ïä§ÏπºÎùº 3. Î≥µÌï© dtype Î∞è torch.complex128Ïùò Ïä§ÏπºÎùº</target>
        </trans-unit>
        <trans-unit id="0cf55afa31790d6a153e9124662c9ad715c264be" translate="yes" xml:space="preserve">
          <source>Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.</source>
          <target state="translated">ÌòÑÏû¨ 3D Ï∂úÎ†• ÌÖêÏÑú (ÌéºÏ≥êÏßÑ Î∞∞Ïπò Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ ÌÖêÏÑú) Îßå ÏßÄÏõêÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="cf7422cb9205f4ada47421ece107faeeaf3ef476" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D input tensors (batched image-like tensors) are supported.</source>
          <target state="translated">ÌòÑÏû¨Îäî 4 Ï∞®Ïõê ÏûÖÎ†• ÌÖêÏÑú (ÏùºÍ¥Ñ Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ ÌÖêÏÑú) Îßå ÏßÄÏõêÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="804337292e10b528ce9da39728466ea21dd2d93d" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D output tensors (batched image-like tensors) are supported.</source>
          <target state="translated">ÌòÑÏû¨Îäî 4D Ï∂úÎ†• ÌÖêÏÑú (ÏùºÍ¥Ñ Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ ÌÖêÏÑú) Îßå ÏßÄÏõêÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="55e4dc48ed51c13b29c9e279e5505c0a895f1cb1" translate="yes" xml:space="preserve">
          <source>Currently, only spatial (4-D) and volumetric (5-D) &lt;code&gt;input&lt;/code&gt; are supported.</source>
          <target state="translated">ÌòÑÏû¨ Í≥µÍ∞Ñ (4-D) Î∞è Ï≤¥Ï†Å (5-D) &lt;code&gt;input&lt;/code&gt; Îßå ÏßÄÏõêÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b674ceb20a7250912a1c130e2165e5889d62f4ab" translate="yes" xml:space="preserve">
          <source>Custom operators</source>
          <target state="translated">ÎßûÏ∂§ Ïó∞ÏÇ∞Ïûê</target>
        </trans-unit>
        <trans-unit id="3461dd173d30fce828765c425b88ef7516427e68" translate="yes" xml:space="preserve">
          <source>CustomFromMask</source>
          <target state="translated">CustomFromMask</target>
        </trans-unit>
        <trans-unit id="50c9e8d5fc98727b4bbc93cf5d64a68db647f04f" translate="yes" xml:space="preserve">
          <source>D</source>
          <target state="translated">D</target>
        </trans-unit>
        <trans-unit id="eb3635e47f534f46cf856ff7cd245f64d37e6d18" translate="yes" xml:space="preserve">
          <source>DCGAN</source>
          <target state="translated">DCGAN</target>
        </trans-unit>
        <trans-unit id="17ba820b5bcd55a78cfd53d227f66dc3adcb9245" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride [0]}-2 \ times \ text {padding [0]} + \ text {kernel \ _size [0]}</target>
        </trans-unit>
        <trans-unit id="11e96ca0ef0a7d1c0968f6c0aa661473e65963b1" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride} [0]-2 \ times \ text {padding} [0] + \ text {dilation} [0] \ times (\ text { Ïª§ÎÑê \ _size} [0]-1) + \ text {output \ _padding} [0] + 1</target>
        </trans-unit>
        <trans-unit id="c8db0974bd119b8e244f7c9a0fbb5cd60340ca9b" translate="yes" xml:space="preserve">
          <source>D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}</source>
          <target state="translated">D_ {out} = D_ {in} + \ text {padding \ _front} + \ text {padding \ _back}</target>
        </trans-unit>
        <trans-unit id="343f5280293229b193ab7d6cc2f6fecb4a2a501b" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor D_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="5daaf33316a5a1e996b64f9e865fa2c06a216412" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {dilation} [0] \ times (\ text {kernel \ _size} [0] -1)-1} {\ text {stride} [0]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="a58eeddbe8ce176e982a4bd5546958165921baf4" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {kernel \ _size} [0]} {\ text {stride} [0]} + 1 \ Ïò§Î•∏Ï™Ω \ r Î∞îÎã•</target>
        </trans-unit>
        <trans-unit id="58d17582a05d81dbc970a2fb5dde4c86c385cf6c" translate="yes" xml:space="preserve">
          <source>Danger</source>
          <target state="translated">Danger</target>
        </trans-unit>
        <trans-unit id="ee503fe5765b8d9456bf21def7f9e06d2b12ebb6" translate="yes" xml:space="preserve">
          <source>Data type</source>
          <target state="translated">Îç∞Ïù¥ÌÑ∞ ÌòïÏãù</target>
        </trans-unit>
        <trans-unit id="5e3a2e3c18839bb47fb3084db81d051ea2d9e572" translate="yes" xml:space="preserve">
          <source>DataParallel</source>
          <target state="translated">DataParallel</target>
        </trans-unit>
        <trans-unit id="f41afa14a2956bd6dee2529d81450b9fde84c7bb" translate="yes" xml:space="preserve">
          <source>DataParallel Layers (multi-GPU, distributed)</source>
          <target state="translated">DataParallel Î†àÏù¥Ïñ¥ (Îã§Ï§ë GPU, Î∂ÑÏÇ∞)</target>
        </trans-unit>
        <trans-unit id="2c2866ebc153b9ba5ee12510871be4ea8e2d66ad" translate="yes" xml:space="preserve">
          <source>DataParallel functions (multi-GPU, distributed)</source>
          <target state="translated">DataParallel Ìï®Ïàò (Îã§Ï§ë GPU, Î∂ÑÏÇ∞)</target>
        </trans-unit>
        <trans-unit id="75c05e642a4a82474f0ded1d73a7d77a7cab17de" translate="yes" xml:space="preserve">
          <source>DeQuantize</source>
          <target state="translated">DeQuantize</target>
        </trans-unit>
        <trans-unit id="895b27c88016513d278a0ce3dc0663fae3829d58" translate="yes" xml:space="preserve">
          <source>Debugging</source>
          <target state="translated">Debugging</target>
        </trans-unit>
        <trans-unit id="2d9ba523f6bd6f1366ef31447c6896de739c5ebc" translate="yes" xml:space="preserve">
          <source>Debugging this script with &lt;code&gt;pdb&lt;/code&gt; works except for when we invoke the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function. We can globally disable JIT, so that we can call the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function and not compile it. If the above script is called &lt;code&gt;disable_jit_example.py&lt;/code&gt;, we can invoke it like so:</source>
          <target state="translated">&lt;code&gt;pdb&lt;/code&gt; Î°úÏù¥ Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÎîîÎ≤ÑÍπÖ ÌïòÎäî Í≤ÉÏùÄ &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; Ìï®ÏàòÎ•º Ìò∏Ï∂úÌïòÎäî Í≤ΩÏö∞Î•º Ï†úÏô∏ÌïòÍ≥† ÏûëÎèô Ìï©ÎãàÎã§. JITÎ•º Ï†ÑÏó≠ Ï†ÅÏúºÎ°ú ÎπÑÌôúÏÑ±ÌôîÌïòÏó¨ &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; Ìï®ÏàòÎ•º ÏùºÎ∞òÏ†ÅÏù∏ Python Ìï®ÏàòÎ°ú Ìò∏Ï∂úÌïòÍ≥† Ïª¥ÌååÏùºÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÏúÑÏùò Ïä§ÌÅ¨Î¶ΩÌä∏Í∞Ä &lt;code&gt;disable_jit_example.py&lt;/code&gt; Ïù∏ Í≤ΩÏö∞ Îã§ÏùåÍ≥º Í∞ôÏù¥ Ìò∏Ï∂ú Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9f7b680b643589b9ed8668bf524f99287df6399c" translate="yes" xml:space="preserve">
          <source>Decodes a DLPack to a tensor.</source>
          <target state="translated">DLPackÏùÑ ÌÖêÏÑúÎ°ú ÎîîÏΩîÎî©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bbe3208131f8562d117c292d268d2f8db716693e" translate="yes" xml:space="preserve">
          <source>DeepLabV3</source>
          <target state="translated">DeepLabV3</target>
        </trans-unit>
        <trans-unit id="67529023df524563878c2ac2647a317ad8500070" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet101</source>
          <target state="translated">DeepLabV3 ResNet101</target>
        </trans-unit>
        <trans-unit id="617f04b5a03ab100dd0d3e0e5532b5eb50903269" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50</source>
          <target state="translated">DeepLabV3 ResNet50</target>
        </trans-unit>
        <trans-unit id="d2b8f3a731cc6350b5d6b3c7afddb1acf6006314" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50, ResNet101</source>
          <target state="translated">DeepLabV3 ResNet50, ResNet101</target>
        </trans-unit>
        <trans-unit id="8c8189f5b90e5b1272f55bc887fde7008a4bafdf" translate="yes" xml:space="preserve">
          <source>Default Types</source>
          <target state="translated">Í∏∞Î≥∏ Ïú†Ìòï</target>
        </trans-unit>
        <trans-unit id="e51881f10b051af0af29bc3e379261ed831a6098" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (no normalization).</source>
          <target state="translated">Í∏∞Î≥∏Í∞íÏùÄ &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (Ï†ïÍ∑úÌôî ÏóÜÏùå)ÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0e0fda91e6a2713611da6dc3cb2473efcd25b651" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (normalize by &lt;code&gt;1/n&lt;/code&gt;).</source>
          <target state="translated">Í∏∞Î≥∏Í∞íÏùÄ &lt;code&gt;&quot;backward&quot;&lt;/code&gt; ( &lt;code&gt;1/n&lt;/code&gt; ÏúºÎ°ú Ï†ïÍ∑úÌôî ).</target>
        </trans-unit>
        <trans-unit id="8e4f64360906e7f1452e75f1085dc507b2bff801" translate="yes" xml:space="preserve">
          <source>Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">Í∏∞Î≥∏Í∞í : &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be8bdbcf95f1b2c543f6b5f2c72d836f7c5b5704" translate="yes" xml:space="preserve">
          <source>Defaults to zero if not provided. where</source>
          <target state="translated">Ï†úÍ≥µÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Í∏∞Î≥∏Í∞íÏùÄ 0ÏûÖÎãàÎã§. Ïñ¥Îîî</target>
        </trans-unit>
        <trans-unit id="0c57e9446082424cd876b20c90b07486b438b9b9" translate="yes" xml:space="preserve">
          <source>Define the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt;, for example &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch/onnx/symbolic_opset9.py&lt;/a&gt;. Make sure the function has the same name as the ATen operator/function defined in &lt;code&gt;VariableType.h&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; Í∏∞Ìò∏ Ìï®ÏàòÎ•º Ï†ïÏùòÌï©ÎãàÎã§ ( Ïòà : &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch / onnx / symbolic_opset9.py)&lt;/a&gt; . Ìï®ÏàòÏùò Ïù¥Î¶ÑÏù¥ &lt;code&gt;VariableType.h&lt;/code&gt; Ïóê Ï†ïÏùò Îêú ATen Ïó∞ÏÇ∞Ïûê / Ìï®ÏàòÏôÄ ÎèôÏùºÌïú ÏßÄ ÌôïÏù∏ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="7773324d0fbb19d2b097125eeec23bba37fd9fae" translate="yes" xml:space="preserve">
          <source>Defines the computation performed at every call.</source>
          <target state="translated">Î™®Îì† Ìò∏Ï∂úÏóêÏÑú ÏàòÌñâÎêòÎäî Í≥ÑÏÇ∞ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9fdfda877e82e2aeb2ae17e08e28fcbd6d18a121" translate="yes" xml:space="preserve">
          <source>Deletes the key-value pair associated with &lt;code&gt;key&lt;/code&gt; from the store. Returns &lt;code&gt;true&lt;/code&gt; if the key was successfully deleted, and &lt;code&gt;false&lt;/code&gt; if it was not.</source>
          <target state="translated">Ï†ÄÏû•ÏÜåÏóêÏÑú &lt;code&gt;key&lt;/code&gt; ÏôÄ Ïó∞Í≤∞Îêú ÌÇ§-Í∞í ÏåçÏùÑ ÏÇ≠Ï†úÌï©ÎãàÎã§ . ÌÇ§Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÇ≠Ï†ú Îêú Í≤ΩÏö∞ &lt;code&gt;true&lt;/code&gt; Î•º Î∞òÌôòÌïòÍ≥† Í∑∏Î†áÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ &lt;code&gt;false&lt;/code&gt; Î•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="fa7b4a50c0e0f2cffa01451426972ab2f487a9dd" translate="yes" xml:space="preserve">
          <source>DenseNet</source>
          <target state="translated">DenseNet</target>
        </trans-unit>
        <trans-unit id="8b2ff8d2942b70f38d615ed1e52ba52e2285edab" translate="yes" xml:space="preserve">
          <source>Densenet-121</source>
          <target state="translated">Densenet-121</target>
        </trans-unit>
        <trans-unit id="d26e787bd56b9c7aab1c534a7f7ead9ab4b12d73" translate="yes" xml:space="preserve">
          <source>Densenet-121 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;Î∞ÄÏßë &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Ïó∞Í≤∞Îêú Ïª®Î≥º Î£®ÏÖò ÎÑ§Ìä∏ÏõåÌÅ¨&quot;Ïùò&lt;/a&gt; Densenet-121 Î™®Îç∏</target>
        </trans-unit>
        <trans-unit id="dc79947b23872b66c941b9c7a01bd0904908a8d9" translate="yes" xml:space="preserve">
          <source>Densenet-161</source>
          <target state="translated">Densenet-161</target>
        </trans-unit>
        <trans-unit id="8079b2939275d34b5b7b870ad8e0a1f7fe8edd0d" translate="yes" xml:space="preserve">
          <source>Densenet-161 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;Î∞ÄÏßë &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Ïó∞Í≤∞Îêú Ïª®Î≥º Î£®ÏÖò ÎÑ§Ìä∏ÏõåÌÅ¨&quot;Ïùò&lt;/a&gt; Densenet-161 Î™®Îç∏</target>
        </trans-unit>
        <trans-unit id="f8e6fd5af55a790890421b436f68cbc8406a0b19" translate="yes" xml:space="preserve">
          <source>Densenet-169</source>
          <target state="translated">Densenet-169</target>
        </trans-unit>
        <trans-unit id="7480a230cf176343f7e3077f6904ede5855a19d8" translate="yes" xml:space="preserve">
          <source>Densenet-169 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;Î∞ÄÏßë &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Ïó∞Í≤∞Îêú Ïª®Î≥º Î£®ÏÖò ÎÑ§Ìä∏ÏõåÌÅ¨&quot;Ïùò&lt;/a&gt; Densenet-169 Î™®Îç∏</target>
        </trans-unit>
        <trans-unit id="fc80ee1a1bf823e3bf3be2c3a3194f577392f377" translate="yes" xml:space="preserve">
          <source>Densenet-201</source>
          <target state="translated">Densenet-201</target>
        </trans-unit>
        <trans-unit id="7124ecd6d83a80564dc1ba72e45a576f118ac991" translate="yes" xml:space="preserve">
          <source>Densenet-201 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;Î∞ÄÏßë &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;Ïó∞Í≤∞Îêú Ïª®Î≥º Î£®ÏÖò ÎÑ§Ìä∏ÏõåÌÅ¨&quot;Ïùò&lt;/a&gt; Densenet-201 Î™®Îç∏</target>
        </trans-unit>
        <trans-unit id="7a89d9b0077197cdfbba3a66cddb06012c599011" translate="yes" xml:space="preserve">
          <source>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;, and not a full &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;. It is up to the user to add proper padding.</source>
          <target state="translated">Ïª§ÎÑêÏùò ÌÅ¨Í∏∞Ïóê Îî∞Îùº ÏûÖÎ†•Ïùò Ïó¨Îü¨ Ïó¥ (ÎßàÏßÄÎßâ)Ïù¥ ÏÜêÏã§ Îê† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Îäî Ï†ÑÏ≤¥ &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;ÏÉÅÌò∏ &lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;ÏÉÅÍ¥Ä&lt;/a&gt; Ïù¥ ÏïÑÎãàÎùº Ïú†Ìö®Ìïú ÏÉÅÌò∏ ÏÉÅÍ¥Ä Ïù¥Í∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§ . Ï†ÅÏ†àÌïú Ìå®Îî©ÏùÑ Ï∂îÍ∞ÄÌïòÎäî Í≤ÉÏùÄ ÏÇ¨Ïö©ÏûêÏóêÍ≤å Îã¨Î†§ ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7a0685deb3b8e4521a47904d7970aabb4610a92a" translate="yes" xml:space="preserve">
          <source>Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the &lt;code&gt;custom_opsets&lt;/code&gt; dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 custom ops&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX Runtime custom ops&lt;/a&gt;.</source>
          <target state="translated">Ïª§Ïä§ÌÖÄ Ïó∞ÏÇ∞ÏûêÏóê Îî∞Îùº ÌïòÎÇò ÎòêÎäî Í∏∞Ï°¥ ONNX ÏûëÏóÖÏùò Ï°∞Ìï©ÏúºÎ°ú ÎÇ¥Î≥¥ÎÇº Ïàò ÏûàÏäµÎãàÎã§. ONNXÏóêÏÑúÎèÑ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏûëÏóÖÏúºÎ°ú ÎÇ¥Î≥¥ÎÇº ÏàòÎèÑ ÏûàÏäµÎãàÎã§. Ïù¥ Í≤ΩÏö∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞Ïãú &lt;code&gt;custom_opsets&lt;/code&gt; ÏÇ¨Ï†ÑÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÎèÑÎ©îÏù∏ Î∞è Î≤ÑÏ†Ñ (ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï opset)ÏùÑ ÏßÄÏ†ïÌï† Ïàò ÏûàÏäµÎãàÎã§ . Î™ÖÏãú Ï†ÅÏúºÎ°ú ÏßÄÏ†ïÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï opset Î≤ÑÏ†ÑÏùÄ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú 1Î°ú ÏÑ§Ï†ïÎê©ÎãàÎã§. ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ONNX ÏûëÏóÖÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ ÏùºÏπòÌïòÎäî ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏûëÏóÖ Íµ¨ÌòÑ (Ïòà : &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏûëÏóÖ&lt;/a&gt; , &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX Îü∞ÌÉÄÏûÑ ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï ÏûëÏóÖ)ÏúºÎ°ú&lt;/a&gt; ÏÑ†ÌÉùÌïú Î∞±ÏóîÎìúÎ•º ÌôïÏû•Ìï¥ÏïºÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0ab688461ca113091fd169e55e7c9e66deb886f0" translate="yes" xml:space="preserve">
          <source>Deprecated enum-like class for reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, and &lt;code&gt;MAX&lt;/code&gt;.</source>
          <target state="translated">Ï∂ïÏÜå ÏûëÏóÖÏóê ÎåÄÌï¥ ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Ïó¥Í±∞ Ìòï ÌÅ¥ÎûòÏä§ : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; Î∞è &lt;code&gt;MAX&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="44fd5a225ad67410d4630aa610f55ced5a7a7c71" translate="yes" xml:space="preserve">
          <source>Dequantizes an incoming tensor</source>
          <target state="translated">Îì§Ïñ¥Ïò§Îäî ÌÖêÏÑúÎ•º Ïó≠ ÏñëÏûêÌôîÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="55f8ebc805e65b5b71ddafdae390e3be2bcd69af" translate="yes" xml:space="preserve">
          <source>Description</source>
          <target state="translated">Description</target>
        </trans-unit>
        <trans-unit id="5966dbc5c4d197355b50f3dc66783c2fa78a5226" translate="yes" xml:space="preserve">
          <source>Design Notes</source>
          <target state="translated">ÎîîÏûêÏù∏ ÎÖ∏Ìä∏</target>
        </trans-unit>
        <trans-unit id="fbe406308e1df41bfda74c701d4a66a5af72c8b6" translate="yes" xml:space="preserve">
          <source>Design Reasoning</source>
          <target state="translated">ÎîîÏûêÏù∏ Ï∂îÎ°†</target>
        </trans-unit>
        <trans-unit id="91441a05e70cda570458aad59617766bb3ed7236" translate="yes" xml:space="preserve">
          <source>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</source>
          <target state="translated">TensorÎ•º ÏÉùÏÑ± Ìïú Í∑∏ÎûòÌîÑÏóêÏÑú Î∂ÑÎ¶¨ÌïòÏó¨ Î¶¨ÌîÑÎ°ú ÎßåÎì≠ÎãàÎã§. Î∑∞Îäî Ï†úÏûêÎ¶¨ÏóêÏÑú Î∂ÑÎ¶¨ Ìï† Ïàò ‚Äã‚ÄãÏóÜÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0e01337242e0297823f889a52bc9dad47f00c9ad" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">Ïú†Ìòï ÏäπÍ≤© &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;Î¨∏ÏÑúÏóê&lt;/a&gt; ÏÑ§Î™Ö Îêú PyTorch Ï∫êÏä§ÌåÖ Í∑úÏπôÏóêÏÑú Ïú†Ìòï Î≥ÄÌôòÏù¥ ÌóàÏö©ÎêòÎäîÏßÄ Ïó¨Î∂ÄÎ•º Í≤∞Ï†ïÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="d2244fb2618c9d05384c145a55a7a5a37bf99078" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">Ïú†Ìòï ÏäπÍ≤© &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;Î¨∏ÏÑúÏóê&lt;/a&gt; ÏÑ§Î™Ö Îêú PyTorch Ï∫êÏä§ÌåÖ Í∑úÏπôÏóêÏÑú Ïú†Ìòï Î≥ÄÌôòÏù¥ ÌóàÏö©ÎêòÎäîÏßÄ Ïó¨Î∂ÄÎ•º Í≤∞Ï†ïÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a5a74a6df09278b88cb6ea23b7d7f2570c33babf" translate="yes" xml:space="preserve">
          <source>Device</source>
          <target state="translated">Device</target>
        </trans-unit>
        <trans-unit id="1c1f67e2f072a5af5bf17679821b62f151e69437" translate="yes" xml:space="preserve">
          <source>Dict Construction</source>
          <target state="translated">Dict Í±¥ÏÑ§</target>
        </trans-unit>
        <trans-unit id="4461566599c5b88c3897f351e4ae4b0ddc655116" translate="yes" xml:space="preserve">
          <source>Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:</source>
          <target state="translated">ÌëúÏ§Ä SVDÏôÄ Îã¨Î¶¨ Î∞òÌôò Îêú ÌñâÎ†¨Ïùò ÌÅ¨Í∏∞Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ ÏßÄÏ†ïÎêú ÏàúÏúÑ Î∞è q Í∞íÏóê Îî∞Îùº Îã§Î¶ÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="abe2291b9a77a8ae7585fd7e4b1df0a06016b998" translate="yes" xml:space="preserve">
          <source>Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).</source>
          <target state="translated">Ï∞®Ïõê Ïù¥Î¶ÑÏóêÎäî Î¨∏Ïûê ÎòêÎäî Î∞ëÏ§ÑÏù¥ Ìè¨Ìï®Îê† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú Ï∞®Ïõê Ïù¥Î¶ÑÏùÄ Ïú†Ìö®Ìïú Python Î≥ÄÏàò Ïù¥Î¶ÑÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§ (Ï¶â, Î∞ëÏ§ÑÎ°ú ÏãúÏûëÌïòÏßÄ ÏïäÏùå).</target>
        </trans-unit>
        <trans-unit id="e1cf09d41f0116a87479fbe7f5ef39aeac24297b" translate="yes" xml:space="preserve">
          <source>Disable JIT for Debugging</source>
          <target state="translated">ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ JIT ÎπÑÌôúÏÑ±Ìôî</target>
        </trans-unit>
        <trans-unit id="dae5511126cad0d55eede647161bef3c8ecf5358" translate="yes" xml:space="preserve">
          <source>Disables denormal floating numbers on CPU.</source>
          <target state="translated">CPUÏóêÏÑú ÎπÑÏ†ïÍ∑ú Î∂ÄÎèô Ïà´ÏûêÎ•º ÎπÑÌôúÏÑ±ÌôîÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c513ea041a153d26658bb6cf95f8c77cf39ac2f0" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;Tensor.backward()&lt;/code&gt;&lt;/a&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">Í∑∏ÎùºÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ÏùÑ ÎπÑÌôúÏÑ±ÌôîÌïòÎ©¥ &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;Tensor.backward()&lt;/code&gt; &lt;/a&gt; Ìò∏Ï∂úÌïòÏßÄ ÏïäÏùÑ Í≤ÉÏù¥ ÌôïÏã§ Ìï† Îïå Ï∂îÎ°†Ïóê Ïú†Ïö©Ìï©ÎãàÎã§ . Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ &lt;code&gt;requires_grad=True&lt;/code&gt; Í∞ÄÏûàÎäî Í≥ÑÏÇ∞Ïóê ÎåÄÌïú Î©îÎ™®Î¶¨ ÏÜåÎπÑÎ•º Ï§ÑÏùº Ïàò ÏûàÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="f9c83b5259f966caf231a71b3ad8e69e880665ed" translate="yes" xml:space="preserve">
          <source>Discrete Fourier transforms and related functions.</source>
          <target state="translated">Ïù¥ÏÇ∞ Ìë∏Î¶¨Ïóê Î≥ÄÌôò Î∞è Í¥ÄÎ†® Ìï®Ïàò.</target>
        </trans-unit>
        <trans-unit id="938004e21f3f3e82099132266fc1891a7f13de20" translate="yes" xml:space="preserve">
          <source>Distance Functions</source>
          <target state="translated">Í±∞Î¶¨ Ìï®Ïàò</target>
        </trans-unit>
        <trans-unit id="9cde45ececcca1e1adb41659d3d28a0420087495" translate="yes" xml:space="preserve">
          <source>Distance functions</source>
          <target state="translated">Í±∞Î¶¨ Í∏∞Îä•</target>
        </trans-unit>
        <trans-unit id="303a9c010db654973c68f36cc8c79b5ce73fe308" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Context</source>
          <target state="translated">Î∂ÑÏÇ∞ Îêú Autograd Ïª®ÌÖçÏä§Ìä∏</target>
        </trans-unit>
        <trans-unit id="310713a581eec3788cf900c68063e38885f001bd" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Design</source>
          <target state="translated">Î∂ÑÏÇ∞ Ìòï Autograd ÎîîÏûêÏù∏</target>
        </trans-unit>
        <trans-unit id="fa35316634dcb2108118798dd9f61f3d3f7fbb66" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Framework</source>
          <target state="translated">Î∂ÑÏÇ∞ Autograd ÌîÑÎ†àÏûÑ ÏõåÌÅ¨</target>
        </trans-unit>
        <trans-unit id="ec9dd95759950a78e8ade825f089a889c2eb7fab" translate="yes" xml:space="preserve">
          <source>Distributed Backward Pass</source>
          <target state="translated">Î∂ÑÏÇ∞ Îêú Ïó≠Î∞©Ìñ• Ìå®Ïä§</target>
        </trans-unit>
        <trans-unit id="aba915801825e438dbb9e75721a94e96b234ba7d" translate="yes" xml:space="preserve">
          <source>Distributed Data Parallel</source>
          <target state="translated">Î∂ÑÏÇ∞ Îç∞Ïù¥ÌÑ∞ Î≥ëÎ†¨</target>
        </trans-unit>
        <trans-unit id="ef10fda1ae05d95a37d1371a4a918aad1148a2da" translate="yes" xml:space="preserve">
          <source>Distributed Key-Value Store</source>
          <target state="translated">Î∂ÑÏÇ∞ ÌÇ§-Í∞í Ï†ÄÏû•ÏÜå</target>
        </trans-unit>
        <trans-unit id="34eb6d25d260c57a8faa37ee402539b363513315" translate="yes" xml:space="preserve">
          <source>Distributed Optimizer</source>
          <target state="translated">Î∂ÑÏÇ∞ ÏµúÏ†ÅÌôî ÎèÑÍµ¨</target>
        </trans-unit>
        <trans-unit id="75f572c75699c1008b8a3d33982f73a57479f298" translate="yes" xml:space="preserve">
          <source>Distributed Pipeline Parallel</source>
          <target state="translated">Î∂ÑÏÇ∞ ÌååÏù¥ÌîÑ ÎùºÏù∏ Î≥ëÎ†¨</target>
        </trans-unit>
        <trans-unit id="5dfffd4675ffa868d517cf4ad2d7b981131e1dbd" translate="yes" xml:space="preserve">
          <source>Distributed RPC Framework</source>
          <target state="translated">Î∂ÑÏÇ∞ RPC ÌîÑÎ†àÏûÑ ÏõåÌÅ¨</target>
        </trans-unit>
        <trans-unit id="e3de08dea5da9f0cffd62432ca7da9726c39e218" translate="yes" xml:space="preserve">
          <source>Distributed communication package - torch.distributed</source>
          <target state="translated">Î∂ÑÏÇ∞ ÌÜµÏã† Ìå®ÌÇ§ÏßÄ-torch.distributed</target>
        </trans-unit>
        <trans-unit id="67b250f513d5c0c4bc692b9447f357fc7ef369ea" translate="yes" xml:space="preserve">
          <source>DistributedDataParallel</source>
          <target state="translated">DistributedDataParallel</target>
        </trans-unit>
        <trans-unit id="ec463333beac30f92377ae176f21cc4eb4b46520" translate="yes" xml:space="preserve">
          <source>DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.</source>
          <target state="translated">DistributedOptimizerÎäî ÏûëÏóÖÏûêÏóê Ìù©Ïñ¥Ï†∏ÏûàÎäî Îß§Í∞ú Î≥ÄÏàòÏóê ÎåÄÌïú ÏõêÍ≤© Ï∞∏Ï°∞Î•º Í∞ÄÏ†∏ÏôÄ Í∞Å Îß§Í∞ú Î≥ÄÏàòÏóê ÎåÄÌï¥ Î°úÏª¨Î°ú ÏßÄÏ†ïÎêú ÏµúÏ†ÅÌôî ÌîÑÎ°úÍ∑∏Îû®ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e4468b3425489b998cab9185522d9f737628cd1d" translate="yes" xml:space="preserve">
          <source>Divides each element of the input &lt;code&gt;input&lt;/code&gt; by the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">ÏûÖÎ†• &lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜå Î•º &lt;code&gt;other&lt;/code&gt; Ïùò Ìï¥Îãπ ÏöîÏÜåÎ°ú ÎÇòÎàïÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="33968e4e64500a95f8035e27727c823cadd48d34" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÌÖêÏÑú ÏãúÌÄÄÏä§Ïùò Îç∞Ïπ¥Î•¥Ìä∏ Í≥±ÏùÑ ÏàòÌñâÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="721f0d8eda69e29b574fd64fe0f6c7674a1a6f89" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors. The behavior is similar to python&amp;rsquo;s &lt;code&gt;itertools.product&lt;/code&gt;.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÌÖêÏÑú ÏãúÌÄÄÏä§Ïùò Îç∞Ïπ¥Î•¥Ìä∏ Í≥±ÏùÑ ÏàòÌñâÌïòÏã≠ÏãúÏò§. ÎèôÏûëÏùÄ pythonÏùò &lt;code&gt;itertools.product&lt;/code&gt; ÏôÄ Ïú†ÏÇ¨Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="fcb94671187d8e10ddb359c9decbb94311e20300" translate="yes" xml:space="preserve">
          <source>Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It&amp;rsquo;s highly recommended to add a few examples here.</source>
          <target state="translated">Ìï®ÏàòÏùò ÎèÖ Ïä§Ìä∏ÎßÅÏùÄ ÎèÑÏõÄÎßê Î©îÏãúÏßÄÎ°ú ÏûëÎèôÌï©ÎãàÎã§. Î™®Îç∏Ïù¥ ÏàòÌñâÌïòÎäî ÏûëÏóÖÍ≥º ÌóàÏö©ÎêòÎäî ÏúÑÏπò / ÌÇ§ÏõåÎìú Ïù∏ÏàòÍ∞Ä Î¨¥ÏóáÏù∏ÏßÄ ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïó¨Í∏∞Ïóê Î™á Í∞ÄÏßÄ ÏòàÎ•º Ï∂îÍ∞ÄÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="96f57a29074dff5e315a68cd23e9cc22e3995e9f" translate="yes" xml:space="preserve">
          <source>Does a linear interpolation of two tensors &lt;code&gt;start&lt;/code&gt; (given by &lt;code&gt;input&lt;/code&gt;) and &lt;code&gt;end&lt;/code&gt; based on a scalar or tensor &lt;code&gt;weight&lt;/code&gt; and returns the resulting &lt;code&gt;out&lt;/code&gt; tensor.</source>
          <target state="translated">Îëê ÌÖêÏÑúÏùò ÏÑ†Ìòï Î≥¥Í∞Ñ ÏïäÏùå &lt;code&gt;start&lt;/code&gt; (Ï£ºÏñ¥ÏßÑ &lt;code&gt;input&lt;/code&gt; ) Î∞è &lt;code&gt;end&lt;/code&gt; Ïä§ÏπºÎùº ÎòêÎäî ÌÖêÏÑúÏóê Í∏∞Ï¥à &lt;code&gt;weight&lt;/code&gt; Î≥µÍ∑Ä Í≤∞Í≥º &lt;code&gt;out&lt;/code&gt; ÌÖêÏÑú.</target>
        </trans-unit>
        <trans-unit id="e06ffaf662b8ea46a067dea9fb380262a87db6a2" translate="yes" xml:space="preserve">
          <source>Down/up samples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">ÏûÖÎ†•ÏùÑ Ï£ºÏñ¥ÏßÑ &lt;code&gt;size&lt;/code&gt; ÎòêÎäî Ï£ºÏñ¥ÏßÑ &lt;code&gt;scale_factor&lt;/code&gt; Î°ú Îã§Ïö¥ / ÏóÖ ÏÉòÌîåÎßÅÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e61af53453cf013b5be43bae97307de95bad250e" translate="yes" xml:space="preserve">
          <source>Download object at the given URL to a local path.</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ URLÏùò Í∞úÏ≤¥Î•º Î°úÏª¨ Í≤ΩÎ°úÎ°ú Îã§Ïö¥Î°úÎìúÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="361a0dc66d29b16c754d93724204f2ac9044c37d" translate="yes" xml:space="preserve">
          <source>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</source>
          <target state="translated">Bernoulli Î∂ÑÌè¨ÏóêÏÑú Ïù¥ÏßÑ ÎÇúÏàò (0 ÎòêÎäî 1)Î•º Í∑∏Î¶ΩÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="99a0d2b3ff3d729e26380413abca3d7df775cdde" translate="yes" xml:space="preserve">
          <source>Dropout</source>
          <target state="translated">Dropout</target>
        </trans-unit>
        <trans-unit id="132aa5f9595506ef0ca296d6be570b1e4aa7acb2" translate="yes" xml:space="preserve">
          <source>Dropout Layers</source>
          <target state="translated">ÎìúÎ°≠ ÏïÑÏõÉ Î†àÏù¥Ïñ¥</target>
        </trans-unit>
        <trans-unit id="15a66e933b5615cb5433ce9102df020458b34bcb" translate="yes" xml:space="preserve">
          <source>Dropout functions</source>
          <target state="translated">ÎìúÎ°≠ ÏïÑÏõÉ Í∏∞Îä•</target>
        </trans-unit>
        <trans-unit id="cfe20a4021b31e2ea913c19efd2987a230ac723c" translate="yes" xml:space="preserve">
          <source>Dropout2d</source>
          <target state="translated">Dropout2d</target>
        </trans-unit>
        <trans-unit id="830971c9b2da3b705ee96a71f9d0d4e5128e9bb7" translate="yes" xml:space="preserve">
          <source>Dropout3d</source>
          <target state="translated">Dropout3d</target>
        </trans-unit>
        <trans-unit id="a7c8e330062301237ad2aac63813f57bd9bb1e14" translate="yes" xml:space="preserve">
          <source>Due to limited dynamic range of half datatype, performing this operation in half precision may cause the first element of result to overflow for certain inputs.</source>
          <target state="translated">half Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏùò Ï†úÌïúÎêú ÎèôÏ†Å Î≤îÏúÑÎ°ú Ïù∏Ìï¥Ïù¥ ÏûëÏóÖÏùÑ Î∞ò Ï†ïÎ∞ÄÎèÑÎ°ú ÏàòÌñâÌïòÎ©¥ ÌäπÏ†ï ÏûÖÎ†•Ïóê ÎåÄÌï¥ Í≤∞Í≥ºÏùò Ï≤´ Î≤àÏß∏ ÏöîÏÜåÍ∞Ä Ïò§Î≤ÑÌîåÎ°ú Îê† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="36b5672884c133b8c278456fa603d4322ce8f54a" translate="yes" xml:space="preserve">
          <source>Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.</source>
          <target state="translated">CUDA Ïª§ÎÑêÏùò ÎπÑÎèôÍ∏∞ ÌäπÏÑ±ÏúºÎ°ú Ïù∏Ìï¥ CUDA ÏΩîÎìúÏóê ÎåÄÌï¥ Ïã§ÌñâÌï† Îïå cProfile Ï∂úÎ†• Î∞è CPU Î™®Îìú autograd ÌîÑÎ°úÌååÏùº Îü¨Í∞Ä Ïò¨Î∞îÎ•∏ ÌÉÄÏù¥Î∞çÏùÑ ÌëúÏãúÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§.Î≥¥Í≥† Îêú CPU ÏãúÍ∞ÑÏùÄ Ïª§ÎÑêÏùÑ ÏãúÏûëÌïòÎäî Îç∞ ÏÇ¨Ïö© Îêú ÏãúÍ∞ÑÏùÑÎ≥¥Í≥†ÌïòÏßÄÎßå ÏãúÍ∞ÑÏùÄ Ìè¨Ìï®ÌïòÏßÄ ÏïäÏäµÎãàÎã§. ÏûëÏóÖÏù¥ ÎèôÍ∏∞ÌôîÎ•º ÏàòÌñâÌïòÏßÄ ÏïäÎäî Ìïú Ïª§ÎÑêÏùÄ GPUÏóêÏÑú Ïã§ÌñâÌïòÎäî Îç∞ ÏÜåÎπÑÌñàÏäµÎãàÎã§. ÎèôÍ∏∞ÌôîÎ•º ÏàòÌñâÌïòÎäî ÏûëÏóÖÏùÄ ÏùºÎ∞ò CPU Î™®Îìú ÌîÑÎ°úÌååÏùº Îü¨ÏóêÏÑú Îß§Ïö∞ ÎßéÏùÄ ÎπÑÏö©Ïù¥ ÎìúÎäî Í≤ÉÏúºÎ°ú Î≥¥ÏûÖÎãàÎã§. ÌÉÄÏù¥Î∞çÏù¥ Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ CUDA Î™®Îìú autograd ÌîÑÎ°úÌååÏùº Îü¨Í∞Ä ÎèÑÏõÄÏù¥ Îê† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="aa6e674ff3a01dbf141329517674a02a51f4e055" translate="yes" xml:space="preserve">
          <source>Due to the conjugate symmetry, &lt;code&gt;input&lt;/code&gt; do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when &lt;code&gt;input&lt;/code&gt; is given by &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt;. In such case, set the &lt;code&gt;onesided&lt;/code&gt; argument of this method to &lt;code&gt;True&lt;/code&gt;. Moreover, the original signal shape information can sometimes be lost, optionally set &lt;code&gt;signal_sizes&lt;/code&gt; to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.</source>
          <target state="translated">Ïº§Î†à ÎåÄÏπ≠ÏúºÎ°ú Ïù∏Ìï¥ &lt;code&gt;input&lt;/code&gt; ÏùÄ Ï†ÑÏ≤¥ Î≥µÏÜå Ï£ºÌååÏàò Í∞íÏùÑ Ìè¨Ìï® Ìï† ÌïÑÏöîÍ∞Ä ÏóÜÏäµÎãàÎã§. ÏãúÏùò Í≤ΩÏö∞ÏôÄ Í±∞Ïùò Ï†àÎ∞òÏùò Í∞íÏùÄ Ï∂©Î∂ÑÌïòÎã§ &lt;code&gt;input&lt;/code&gt; Ï£ºÏñ¥ÏßÑÎã§ &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; ÏôÄ &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt; . Ïù¥ Í≤ΩÏö∞Ïù¥ Î©îÏÑúÎìú Ïùò &lt;code&gt;onesided&lt;/code&gt; Ïù∏ÏàòÎ•º &lt;code&gt;True&lt;/code&gt; Î°ú ÏÑ§Ï†ï Ìï©ÎãàÎã§. ÎòêÌïú ÏõêÎûò Ïã†Ìò∏ Î™®Ïñë Ï†ïÎ≥¥Í∞Ä ÏÜêÏã§ Îê† Ïàò ÏûàÏäµÎãàÎã§. ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú &lt;code&gt;signal_sizes&lt;/code&gt; Î•º ÏõêÎûò Ïã†Ìò∏Ïùò ÌÅ¨Í∏∞Î°ú ÏÑ§Ï†ïÌïòÏó¨ (Î∞∞Ïπò Î™®Îìú Ïù∏ Í≤ΩÏö∞ Î∞∞Ïπò Ï∞®ÏõêÏóÜÏù¥) Ïò¨Î∞îÎ•∏ Î™®ÏñëÏúºÎ°ú Î≥µÍµ¨Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3f628f956c51a607f601ca3a9c65aebac60f6d34" translate="yes" xml:space="preserve">
          <source>Duplicate modules are returned only once. In the following example, &lt;code&gt;l&lt;/code&gt; will be returned only once.</source>
          <target state="translated">Ï§ëÎ≥µ Î™®ÎìàÏùÄ Ìïú Î≤àÎßå Î∞òÌôòÎê©ÎãàÎã§. Îã§Ïùå ÏòàÏóêÏÑú &lt;code&gt;l&lt;/code&gt; ÏùÄ Ìïú Î≤àÎßå Î∞òÌôòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="50fa35c3d84d578bb945e995330eef7c63dcf91c" translate="yes" xml:space="preserve">
          <source>During backward, only gradients at &lt;code&gt;nnz&lt;/code&gt; locations of &lt;code&gt;input&lt;/code&gt; will propagate back. Note that the gradients of &lt;code&gt;input&lt;/code&gt; is coalesced.</source>
          <target state="translated">Ïó≠Î∞©Ìñ• ÎèôÏïàÏóêÎäî &lt;code&gt;input&lt;/code&gt; &lt;code&gt;nnz&lt;/code&gt; ÏúÑÏπòÏóêÏûàÎäî Í∑∏ÎûòÎîîÏñ∏Ìä∏ Îßå Îã§Ïãú Ï†ÑÌååÎê©ÎãàÎã§. &lt;code&gt;input&lt;/code&gt; Ïùò Í∏∞Ïö∏Í∏∞ Í∞Ä Ìï©Ï≥êÏßëÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8e08ad58135bf66f71d67cdf9e3eb42be9cdd416" translate="yes" xml:space="preserve">
          <source>During evaluation the module simply computes an identity function.</source>
          <target state="translated">ÌèâÍ∞ÄÌïòÎäî ÎèôÏïà Î™®ÎìàÏùÄ Îã®ÏàúÌûà ÏãùÎ≥Ñ Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c42cc58336b9bff21da008fbb38ac485f6938f2c" translate="yes" xml:space="preserve">
          <source>During inference, the model requires only the input tensors, and returns the post-processed predictions as a &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt;, one for each input image. The fields of the &lt;code&gt;Dict&lt;/code&gt; are as follows:</source>
          <target state="translated">Ï∂îÎ°†ÌïòÎäî ÎèôÏïà Î™®Îç∏ÏùÄ ÏûÖÎ†• ÌÖêÏÑú Îßå ÌïÑÏöîÌïòÍ≥† ÌõÑ Ï≤òÎ¶¨ Îêú ÏòàÏ∏° ÏùÑ Í∞Å ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÌïòÎÇòÏî© &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt; Î°ú Î∞òÌôòÌï©ÎãàÎã§ . &lt;code&gt;Dict&lt;/code&gt; Ïùò ÌïÑÎìúÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="899a3b765d6ee8d823d5852dbea555348e22fa78" translate="yes" xml:space="preserve">
          <source>During training, it randomly masks some of the elements of the input tensor with probability &lt;em&gt;p&lt;/em&gt; using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</source>
          <target state="translated">ÌõàÎ†® Ï§ëÏóê Î≤†Î•¥ÎàÑÏù¥ Î∂ÑÌè¨Ïùò ÏÉòÌîåÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏûÖÎ†• ÌÖêÏÑúÏùò ÏùºÎ∂Ä ÏöîÏÜåÎ•º ÌôïÎ•† &lt;em&gt;p&lt;/em&gt; Î°ú Î¨¥ÏûëÏúÑÎ°ú ÎßàÏä§ÌÇπ Ìï©ÎãàÎã§. ÎßàÏä§ÌÇπ Ìï† ÏöîÏÜåÎäî Î™®Îì† Ìè¨ÏõåÎìú Ìò∏Ï∂úÏóêÏÑú Î¨¥ÏûëÏúÑ ÌôîÎêòÍ≥† ÌèâÍ∑† 0Í≥º Îã®ÏúÑ ÌëúÏ§Ä Ìé∏Ï∞®Î•º Ïú†ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ ÌÅ¨Í∏∞Í∞Ä Ï°∞Ï†ïÎêòÍ≥† Ïù¥ÎèôÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1ddb72e8fb2bca4bbdd9cee1bd0f4124ba674fe1" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution.</source>
          <target state="translated">ÌõàÎ†® Ï§ëÏóê Bernoulli Î∂ÑÌè¨Ïùò ÌëúÎ≥∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌôïÎ•† &lt;code&gt;p&lt;/code&gt; Î°ú ÏûÖÎ†• ÌÖêÏÑúÏùò ÏùºÎ∂Ä ÏöîÏÜåÎ•º Î¨¥ÏûëÏúÑÎ°ú 0ÏúºÎ°ú ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="7e07f3de383a6b909b32c2c8420ab783da80d74b" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</source>
          <target state="translated">ÌõàÎ†® Ï§ëÏóê Bernoulli Î∂ÑÌè¨Ïùò ÌëúÎ≥∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌôïÎ•† &lt;code&gt;p&lt;/code&gt; Î°ú ÏûÖÎ†• ÌÖêÏÑúÏùò ÏùºÎ∂Ä ÏöîÏÜåÎ•º Î¨¥ÏûëÏúÑÎ°ú 0ÏúºÎ°ú ÎßåÎì≠ÎãàÎã§. Í∞Å Ï±ÑÎÑêÏùÄ Î™®Îì† Ï∞©Ïã† Ï†ÑÌôòÏóêÏÑú ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú 0Ïù¥Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e72931d7ae530922c7186be0d18873ff29d42369" translate="yes" xml:space="preserve">
          <source>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</source>
          <target state="translated">ÌïôÏäµ Ï§ëÏóê Î™®Îç∏ÏùÄ ÏûÖÎ†• ÌÖêÏÑúÏôÄ Îã§ÏùåÏùÑ Ìè¨Ìï®ÌïòÎäî ÎåÄÏÉÅ (ÏÇ¨Ï†Ñ Î™©Î°ù)ÏùÑ Î™®Îëê ÏòàÏÉÅÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e0184adedf913b076626646d3f52c3b49c39ad6d" translate="yes" xml:space="preserve">
          <source>E</source>
          <target state="translated">E</target>
        </trans-unit>
        <trans-unit id="a466f9bd6da8a16be528d1b97b22f8b80c50fc61" translate="yes" xml:space="preserve">
          <source>E (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;)</source>
          <target state="translated">E ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;ÌÖêÏÑú&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="db547439274fed58f2b9ee57841dc8931b457c07" translate="yes" xml:space="preserve">
          <source>ELU</source>
          <target state="translated">ELU</target>
        </trans-unit>
        <trans-unit id="c4cf619f3157e4a678ac53e564134cffe6210613" translate="yes" xml:space="preserve">
          <source>Each &lt;code&gt;torch.Tensor&lt;/code&gt; has a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Í∞Å &lt;code&gt;torch.Tensor&lt;/code&gt; ÏóêÎäî &lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; Î∞è &lt;a href=&quot;#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; Ïù¥&lt;/a&gt; ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="1c0427e3aafaddc6d9583e9cefbad6140b6d07cf" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;input&lt;/code&gt; is multiplied by the corresponding element of the Tensor &lt;code&gt;other&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">ÌÖêÏÑú &lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê Tensor &lt;code&gt;other&lt;/code&gt; Ïùò Ìï¥Îãπ ÏöîÏÜåÍ∞Ä Í≥±Ìï¥ÏßëÎãàÎã§ . Í≤∞Í≥º ÌÖêÏÑúÍ∞Ä Î∞òÌôòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="728738a25fa4324927c05f66f2cfc30782cc84df" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;other&lt;/code&gt; is multiplied by the scalar &lt;code&gt;alpha&lt;/code&gt; and added to each element of the tensor &lt;code&gt;input&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; ÌÖêÏÑúÏùò Í∞Å ÏöîÏÜåÏóê Ïä§ÏπºÎùº &lt;code&gt;alpha&lt;/code&gt; Í≥±ÌïòÍ≥† ÌÖêÏÑú &lt;code&gt;input&lt;/code&gt; Ïùò Í∞Å ÏöîÏÜåÏóê Îçî Ìï©ÎãàÎã§ . Í≤∞Í≥º ÌÖêÏÑúÍ∞Ä Î∞òÌôòÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6ea1bfa1407efaeb9ea4478b91f07efe596058d3" translate="yes" xml:space="preserve">
          <source>Each element will be masked independently on every forward call with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.</source>
          <target state="translated">Í∞Å ÏöîÏÜåÎäî Bernoulli Î∂ÑÌè¨Ïùò ÏÉòÌîåÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌôïÎ•† &lt;code&gt;p&lt;/code&gt; Î°ú Î™®Îì† Ï∞©Ïã† Ï†ÑÌôòÏóêÏÑú ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú ÎßàÏä§ÌÇπÎê©ÎãàÎã§ . ÎßàÏä§ÌÇπ Ìï† ÏöîÏÜåÎäî Î™®Îì† Ìè¨ÏõåÎìú Ìò∏Ï∂úÏóêÏÑú Î¨¥ÏûëÏúÑ ÌôîÎêòÍ≥† ÌèâÍ∑† Î∞è Îã®ÏúÑ Î∂ÑÏÇ∞ÏùÑ 0ÏúºÎ°ú Ïú†ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ ÌÅ¨Í∏∞Í∞Ä Ï°∞Ï†ïÎêòÍ≥† Ïù¥ÎèôÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="04229e567ec03d81b5712793e639928468412520" translate="yes" xml:space="preserve">
          <source>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and &amp;ldquo;GIL-thrashing&amp;rdquo; that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</source>
          <target state="translated">Í∞Å ÌîÑÎ°úÏÑ∏Ïä§ÏóêÎäî ÎèÖÎ¶ΩÏ†Å Ïù∏ Python Ïù∏ÌÑ∞ÌîÑÎ¶¨ÌÑ∞Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ÏûàÏñ¥ Îã®Ïùº Python ÌîÑÎ°úÏÑ∏Ïä§ÏóêÏÑú Ïó¨Îü¨ Ïã§Ìñâ Ïä§Î†àÎìú, Î™®Îç∏ Î≥µÏ†úÎ≥∏ ÎòêÎäî GPUÎ•º Íµ¨Îèô Ìï† Îïå Î∞úÏÉùÌïòÎäî Ï∂îÍ∞Ä Ïù∏ÌÑ∞ÌîÑÎ¶¨ÌÑ∞ Ïò§Î≤Ñ Ìó§ÎìúÏôÄ &quot;GIL Ïä§ ÎûòÏã±&quot;ÏùÑ Ï†úÍ±∞Ìï©ÎãàÎã§. Ïù¥Îäî Î∞òÎ≥µ Î†àÏù¥Ïñ¥ ÎòêÎäî ÎßéÏùÄ ÏûëÏùÄ Íµ¨ÏÑ± ÏöîÏÜåÍ∞ÄÏûàÎäî Î™®Îç∏ÏùÑ Ìè¨Ìï®ÌïòÏó¨ Python Îü∞ÌÉÄÏûÑÏùÑ ÎßéÏù¥ ÏÇ¨Ïö©ÌïòÎäî Î™®Îç∏Ïóê ÌäπÌûà Ï§ëÏöîÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3c11c2aae826827df9743912100e743d6a724614" translate="yes" xml:space="preserve">
          <source>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</source>
          <target state="translated">Í∞Å ÌîÑÎ°úÏÑ∏Ïä§Îäî ÏûêÏ≤¥ ÏµúÏ†ÅÌôî ÌîÑÎ°úÍ∑∏Îû®ÏùÑ Ïú†ÏßÄÌïòÍ≥† Í∞Å Î∞òÎ≥µÎßàÎã§ ÏôÑÏ†ÑÌïú ÏµúÏ†ÅÌôî Îã®Í≥ÑÎ•º ÏàòÌñâÌï©ÎãàÎã§. Ï§ëÎ≥µ Îêú Í≤ÉÏ≤òÎüº Î≥¥Ïùº Ïàò ÏûàÏßÄÎßå Í∑∏ÎûòÎîîÏñ∏Ìä∏Í∞Ä Ïù¥ÎØ∏ Ìï®Íªò ÏàòÏßëÎêòÍ≥† ÌîÑÎ°úÏÑ∏Ïä§Í∞ÑÏóê ÌèâÍ∑†ÌôîÎêòÏñ¥ Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§Ïóê ÎåÄÌï¥ ÎèôÏùºÌïòÎØÄÎ°ú Îß§Í∞ú Î≥ÄÏàò Î∏åÎ°úÎìú Ï∫êÏä§Ìä∏ Îã®Í≥ÑÍ∞Ä ÌïÑÏöîÌïòÏßÄ ÏïäÏúºÎØÄÎ°ú ÎÖ∏ÎìúÍ∞ÑÏóê ÌÖêÏÑúÎ•º Ï†ÑÏÜ°ÌïòÎäî Îç∞ ÏÜåÏöîÎêòÎäî ÏãúÍ∞ÑÏù¥ Ï§ÑÏñ¥ Îì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="25ddc73f3c685d39e655eadbdd24861cccbd855c" translate="yes" xml:space="preserve">
          <source>Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.</source>
          <target state="translated">Í∞Å ÌîÑÎ°úÏÑ∏Ïä§Îäî ÏûÖÎ†• ÌÖêÏÑú Î™©Î°ùÏùÑ Í∑∏Î£πÏùò Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§Ïóê Î∂ÑÏÇ∞ÏãúÌÇ§Í≥† ÏàòÏßë Îêú ÌÖêÏÑú Î™©Î°ùÏùÑ Ï∂úÎ†• Î™©Î°ùÏóê Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e01a4afade34e5c383597dd828cf5ce98ca5f574" translate="yes" xml:space="preserve">
          <source>Each process will receive exactly one tensor and store its data in the &lt;code&gt;tensor&lt;/code&gt; argument.</source>
          <target state="translated">Í∞Å ÌîÑÎ°úÏÑ∏Ïä§Îäî Ï†ïÌôïÌûà ÌïòÎÇòÏùò ÌÖêÏÑúÎ•º ÏàòÏã†ÌïòÍ≥† Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞Î•º &lt;code&gt;tensor&lt;/code&gt; Ïù∏Ïàò Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a930fc3f8ffe54e36efcacdd772fc5e7328c868c" translate="yes" xml:space="preserve">
          <source>Each tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. The tensor class also provides multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage and defines numeric operations on it.</source>
          <target state="translated">Í∞Å ÌÖêÏÑúÏóêÎäî Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Ïú† ÌïòÎäî Ïó∞Í≤∞Îêú &lt;code&gt;torch.Storage&lt;/code&gt; Í∞Ä ÏûàÏäµÎãàÎã§. ÌÖêÏÑú ÌÅ¥ÎûòÏä§Îäî ÎòêÌïú Ïä§ÌÜ†Î¶¨ÏßÄÏóê ÎåÄÌïú Îã§Ï∞®ÏõêÏùò &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;Ïä§Ìä∏ÎùºÏù¥Îìú&lt;/a&gt; Î∑∞Î•º Ï†úÍ≥µÌïòÍ≥† Ïù¥Ïóê ÎåÄÌïú Ïà´Ïûê Ïó∞ÏÇ∞ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0df7dd6db96e14d83140e8da4324d11663fe359e" translate="yes" xml:space="preserve">
          <source>Each tensor in &lt;code&gt;output_tensor_list&lt;/code&gt; should reside on a separate GPU, as should each list of tensors in &lt;code&gt;input_tensor_lists&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_tensor_list&lt;/code&gt; Ïùò Í∞Å ÌÖêÏÑú Îäî &lt;code&gt;input_tensor_lists&lt;/code&gt; Ïùò Í∞Å ÌÖêÏÑú Î™©Î°ùÍ≥º ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú Î≥ÑÎèÑÏùò GPUÏóê ÏûàÏñ¥ÏïºÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="96aad5e1712347cedb5784e57172eb768752a304" translate="yes" xml:space="preserve">
          <source>Efficient softmax approximation as described in &lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier, and Herv&amp;eacute; J&amp;eacute;gou&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier Î∞è Herv&amp;eacute; J&amp;eacute;gouÏùò GPUÏóê ÎåÄÌïú Ìö®Ïú®Ï†ÅÏù∏ ÏÜåÌîÑÌä∏ Îß•Ïä§&lt;/a&gt; Í∑ºÏÇ¨Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú Ìö®Ïú®Ï†ÅÏù∏ ÏÜåÌîÑÌä∏ Îß•Ïä§ Í∑ºÏÇ¨ .</target>
        </trans-unit>
        <trans-unit id="cba1a5641f37d31826ff43a90ac27303cd2de000" translate="yes" xml:space="preserve">
          <source>Element-wise arctangent of</source>
          <target state="translated">ÏöîÏÜå Î≥Ñ ÏïÑÌÅ¨ ÌÉÑÏ††Ìä∏</target>
        </trans-unit>
        <trans-unit id="c0ce0d75b4a69f8d83fed56dda584bc104824e33" translate="yes" xml:space="preserve">
          <source>Elements lower than min and higher than max are ignored.</source>
          <target state="translated">ÏµúÏÜåÎ≥¥Îã§ ÎÇÆÍ≥† ÏµúÎåÄÎ≥¥Îã§ ÎÜíÏùÄ ÏöîÏÜåÎäî Î¨¥ÏãúÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="279868256307295ed763af9335611205d2a9e0e9" translate="yes" xml:space="preserve">
          <source>Eliminates all but the first element from every consecutive group of equivalent elements.</source>
          <target state="translated">ÎèôÏùºÌïú ÏöîÏÜåÏùò Î™®Îì† Ïó∞ÏÜç Í∑∏Î£πÏóêÏÑú Ï≤´ Î≤àÏß∏ ÏöîÏÜåÎ•º Ï†úÏô∏Ìïú Î™®Îì† ÏöîÏÜåÎ•º ‚Äã‚ÄãÏ†úÍ±∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="65e872502ba89d481aa8b75804d63958258d47d9" translate="yes" xml:space="preserve">
          <source>Embedding</source>
          <target state="translated">Embedding</target>
        </trans-unit>
        <trans-unit id="fb52acf7809d84b49f20c1e0c2aeb4d9421e883d" translate="yes" xml:space="preserve">
          <source>Embedding (no optional arguments supported)</source>
          <target state="translated">ÏûÑÎ≤†Îî© (ÏßÄÏõêÎêòÎäî ÏÑ†ÌÉùÏ†Å Ïù∏Ïàò ÏóÜÏùå)</target>
        </trans-unit>
        <trans-unit id="ad4c45718740fb7fd09a983448014fcde632287c" translate="yes" xml:space="preserve">
          <source>EmbeddingBag</source>
          <target state="translated">EmbeddingBag</target>
        </trans-unit>
        <trans-unit id="f25f65444d58a4000e445a3586674f2f0a1c519b" translate="yes" xml:space="preserve">
          <source>EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by &lt;code&gt;mode&lt;/code&gt;. If &lt;code&gt;per_sample_weights`&lt;/code&gt; is passed, the only supported &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, which computes a weighted sum according to &lt;code&gt;per_sample_weights&lt;/code&gt;.</source>
          <target state="translated">EmbeddingBagÎäî ÎòêÌïú ÏàúÎ∞©Ìñ• Ìå®Ïä§Ïóê ÎåÄÌïú Ïù∏ÏàòÎ°ú ÏÉòÌîå Îãπ Í∞ÄÏ§ëÏπòÎ•º ÏßÄÏõêÌï©ÎãàÎã§. Ïù¥Í≤ÉÏùÄ &lt;code&gt;mode&lt;/code&gt; Ïóê ÏùòÌï¥ ÏßÄÏ†ïÎêú Í∞ÄÏ§ëÏπò Í∞êÏÜåÎ•º ÏàòÌñâÌïòÍ∏∞ Ï†ÑÏóê EmbeddingÏùò Ï∂úÎ†•ÏùÑ Ï°∞Ï†ï Ìï©ÎãàÎã§ . Í≤ΩÏö∞ &lt;code&gt;per_sample_weights`&lt;/code&gt; Í∞Ä Ï†ÑÎã¨ÎêòÎ©¥, ÏßÄÏõêÎêòÎäî &lt;code&gt;mode&lt;/code&gt; Ïù¥Îã§ &lt;code&gt;&quot;sum&quot;&lt;/code&gt; Ïóê Îî∞Î•∏ Í∞ÄÏ§ë Îêú Ìï©ÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥†, &lt;code&gt;per_sample_weights&lt;/code&gt; Ïù¥ .</target>
        </trans-unit>
        <trans-unit id="7a184f78683775965d53e63c45e77c29a82cb431" translate="yes" xml:space="preserve">
          <source>Enables .grad attribute for non-leaf Tensors.</source>
          <target state="translated">Î¶¨ÌîÑÍ∞Ä ÏïÑÎãå TensorÏóê .grad ÏÜçÏÑ±ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4ba6db1c18e6b89f8ae8769b517165aebdeeb5e7" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt; ÎòêÎäî &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt; Î•º ÌÜµÌï¥ ÎπÑÌôúÏÑ±Ìôî Îêú Í≤ΩÏö∞ Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ÏùÑ ÌôúÏÑ±ÌôîÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="129b2e35235ca3917459fbda73dccc7c7ba4bb9f" translate="yes" xml:space="preserve">
          <source>Ensures that the tensor memory is not reused for another tensor until all current work queued on &lt;code&gt;stream&lt;/code&gt; are complete.</source>
          <target state="translated">&lt;code&gt;stream&lt;/code&gt; Ïóê ÎåÄÍ∏∞Ï§ëÏù∏ Î™®Îì† ÌòÑÏû¨ ÏûëÏóÖ Ïù¥ ÏôÑÎ£å Îê† ÎïåÍπåÏßÄ ÌÖêÏÑú Î©îÎ™®Î¶¨Í∞Ä Îã§Î•∏ ÌÖêÏÑúÏóê Ïû¨ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎèÑÎ°ù Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="02f525cd5d7591c6c78960a007006cf272db0f53" translate="yes" xml:space="preserve">
          <source>Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.</source>
          <target state="translated">ÏßÑÏûÖ Ï†ê Ìï®ÏàòÎäî Î™®Îç∏ (nn.module) ÎòêÎäî ÏÇ¨Ïö©Ïûê ÏõåÌÅ¨ ÌîåÎ°úÎ•º Îçî ÏõêÌôúÌïòÍ≤å ÎßåÎìúÎäî Î≥¥Ï°∞ ÎèÑÍµ¨ (Ïòà : ÌÜ†ÌÅ¨ ÎÇòÏù¥Ï†Ä)Î•º Î∞òÌôò Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b0dd14c2fc526f82985ca8c8a3a6e1c3b85b6488" translate="yes" xml:space="preserve">
          <source>Environment variable initialization</source>
          <target state="translated">ÌôòÍ≤Ω Î≥ÄÏàò Ï¥àÍ∏∞Ìôî</target>
        </trans-unit>
        <trans-unit id="089481a55a3bf2c4a369d848036bccebf8abe33b" translate="yes" xml:space="preserve">
          <source>Equivalent to input[:,::-1]. Requires the array to be at least 2-D.</source>
          <target state="translated">input [:, ::-1]Í≥º Í∞ôÏäµÎãàÎã§. Î∞∞Ïó¥Ïù¥ 2 Ï∞®Ïõê Ïù¥ÏÉÅÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3b4933d2950181258ba9867fa0e789a6fac6c0e2" translate="yes" xml:space="preserve">
          <source>Equivalent to input[::-1,&amp;hellip;]. Requires the array to be at least 1-D.</source>
          <target state="translated">input [::-1,&amp;hellip;]Í≥º Í∞ôÏäµÎãàÎã§. Î∞∞Ïó¥Ïù¥ 1 Ï∞®Ïõê Ïù¥ÏÉÅÏù¥Ïñ¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="9c1eedde4e72567b3c48e7393929af1241088b7e" translate="yes" xml:space="preserve">
          <source>Errors such as timeouts for the &lt;code&gt;remote&lt;/code&gt; API are handled on a best-effort basis. This means that when remote calls initiated by &lt;code&gt;remote&lt;/code&gt; fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as &lt;code&gt;to_here&lt;/code&gt; or fork call), then future uses of the &lt;code&gt;RRef&lt;/code&gt; will appropriately raise errors. However, it is possible that the user application will use the &lt;code&gt;RRef&lt;/code&gt; before the errors are handled. In this case, errors may not be raised as they have not yet been handled.</source>
          <target state="translated">&lt;code&gt;remote&lt;/code&gt; APIÏóê ÎåÄÌïú ÏãúÍ∞Ñ Ï¥àÍ≥ºÏôÄ Í∞ôÏùÄ Ïò§Î•ò Îäî ÏµúÏÑ†ÏùÑ Îã§Ìï¥ Ï≤òÎ¶¨Îê©ÎãàÎã§. Ï¶â , ÏãúÍ∞Ñ Ï¥àÍ≥º Ïò§Î•òÏôÄ Í∞ôÏù¥ &lt;code&gt;remote&lt;/code&gt; ÏãúÏûëÎêú ÏõêÍ≤© Ìò∏Ï∂úÏù¥ Ïã§Ìå® Ìï† Îïå Ïò§Î•ò Ï≤òÎ¶¨Ïóê ÏµúÏÑ†Ïùò Ï†ëÍ∑º Î∞©ÏãùÏùÑ Ï∑®Ìï©ÎãàÎã§. Ïù¥Îäî Ïò§Î•òÍ∞Ä ÎπÑÎèôÍ∏∞ÏãùÏúºÎ°ú Í≤∞Í≥º RRefÏóêÏÑú Ï≤òÎ¶¨ÎêòÍ≥† ÏÑ§Ï†ïÎê®ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. Ïù¥ Ï≤òÎ¶¨ Ï†ÑÏóê ÏùëÏö© ÌîÑÎ°úÍ∑∏Îû®ÏóêÏÑú RRefÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ (Ïòà : &lt;code&gt;to_here&lt;/code&gt; ÎòêÎäî fork Ìò∏Ï∂ú) ÎÇòÏ§ëÏóê &lt;code&gt;RRef&lt;/code&gt; Î•º ÏÇ¨Ïö© ÌïòÎ©¥ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§. Í∑∏Îü¨ÎÇò Ïò§Î•òÍ∞Ä Ï≤òÎ¶¨ÎêòÍ∏∞ Ï†ÑÏóê ÏÇ¨Ïö©Ïûê ÏùëÏö© ÌîÑÎ°úÍ∑∏Îû®Ïù¥ &lt;code&gt;RRef&lt;/code&gt; Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§ . Ïù¥ Í≤ΩÏö∞ ÏïÑÏßÅ Ï≤òÎ¶¨ÎêòÏßÄ ÏïäÏïòÏúºÎØÄÎ°ú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2352230ae4369b2400cd9a8b59625450e193a06a" translate="yes" xml:space="preserve">
          <source>Estimate</source>
          <target state="translated">Estimate</target>
        </trans-unit>
        <trans-unit id="9f5d3b9ce4cb19a213d647b445b4900659d33a6d" translate="yes" xml:space="preserve">
          <source>Evaluates module(input) in parallel across the GPUs given in device_ids.</source>
          <target state="translated">device_idsÏóê ÏßÄÏ†ïÎêú GPUÏóêÏÑú Î≥ëÎ†¨Î°ú Î™®Îìà (ÏûÖÎ†•)ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5b74f76cd70090d77d5d968780f896243c537f02" translate="yes" xml:space="preserve">
          <source>Every &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; has a corresponding storage of the same data type.</source>
          <target state="translated">Î™®Îì† &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; ÏóêÎäî ÎèôÏùºÌïú Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏùò Ìï¥Îãπ Ïä§ÌÜ†Î¶¨ÏßÄÍ∞Ä ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="88cdbf2ac191cecaae2230a964d90524ead7b613" translate="yes" xml:space="preserve">
          <source>Every collective operation function supports the following two kinds of operations:</source>
          <target state="translated">Î™®Îì† ÏßëÌï© Ïó∞ÏÇ∞ Í∏∞Îä•ÏùÄ Îã§Ïùå Îëê Ï¢ÖÎ•òÏùò Ïó∞ÏÇ∞ÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="cac312ad37989d865266909c5b6681a3176edc55" translate="yes" xml:space="preserve">
          <source>Everything in a user defined &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript Class&lt;/a&gt; is exported by default, functions can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">ÏÇ¨Ïö©Ïûê Ï†ïÏùò &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript ÌÅ¥ÎûòÏä§Ïùò&lt;/a&gt; Î™®Îì† Í≤ÉÏùÄ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÎÇ¥Î≥¥ÎÇ¥ÏßÄÎ©∞ ÌïÑÏöîÌïú Í≤ΩÏö∞ &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; Ìï®ÏàòÎ•º Íæ∏Î∞Ä Ïàò ÏûàÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0f01ed56a1e32a05e5ef96e4d779f34784af9a96" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="translated">Example</target>
        </trans-unit>
        <trans-unit id="b3c7ccbaed13d7cf475ac0a8540d9af04a1abfba" translate="yes" xml:space="preserve">
          <source>Example (a type mismatch)</source>
          <target state="translated">Ïòà (Ïú†Ìòï Î∂àÏùºÏπò)</target>
        </trans-unit>
        <trans-unit id="fb18992fbb684cb9203c712439d9448a7d50b7c9" translate="yes" xml:space="preserve">
          <source>Example (an exported and ignored method in a module):</source>
          <target state="translated">Ïòà (Î™®ÎìàÏóêÏÑú ÎÇ¥Î≥¥ÎÇ¥Í≥† Î¨¥Ïãú Îêú Î©îÏÑúÎìú) :</target>
        </trans-unit>
        <trans-unit id="3fb80256910d817ce9141fdd4451ee10894ec35c" translate="yes" xml:space="preserve">
          <source>Example (calling a script function in a traced function):</source>
          <target state="translated">Ïòà (Ï∂îÏ†Å Îêú Ìï®ÏàòÏóêÏÑú Ïä§ÌÅ¨Î¶ΩÌä∏ Ìï®Ïàò Ìò∏Ï∂ú) :</target>
        </trans-unit>
        <trans-unit id="b5bcda1a3bfb1af96df6dd020d0588805d88feba" translate="yes" xml:space="preserve">
          <source>Example (calling a traced function in script):</source>
          <target state="translated">Ïòà (Ïä§ÌÅ¨Î¶ΩÌä∏ÏóêÏÑú Ï∂îÏ†Å Îêú Ìï®Ïàò Ìò∏Ï∂ú) :</target>
        </trans-unit>
        <trans-unit id="9488ed97f58e814938180bd3de0fc03a37f8d8c1" translate="yes" xml:space="preserve">
          <source>Example (fork a free function):</source>
          <target state="translated">Ïòà (ÌîÑÎ¶¨ Ìï®Ïàò Ìè¨ÌÅ¨) :</target>
        </trans-unit>
        <trans-unit id="8d40b7fb2ebedc8ff715bb44861bcfbdc43fa817" translate="yes" xml:space="preserve">
          <source>Example (fork a module method):</source>
          <target state="translated">Ïòà (Î™®Îìà Î©îÏÑúÎìú Î∂ÑÍ∏∞) :</target>
        </trans-unit>
        <trans-unit id="c61fd1aa74a0dc25c5318d75c242307960c7041c" translate="yes" xml:space="preserve">
          <source>Example (refining types on parameters and locals):</source>
          <target state="translated">Ïòà (Îß§Í∞ú Î≥ÄÏàò Î∞è ÏßÄÏó≠Ïóê ÎåÄÌïú Ïú†Ìòï Íµ¨Ï≤¥Ìôî) :</target>
        </trans-unit>
        <trans-unit id="9f3b97fb80872ca69d304fba5bcdeedcdbea8e85" translate="yes" xml:space="preserve">
          <source>Example (scripting a function):</source>
          <target state="translated">Ïòà (Ìï®Ïàò Ïä§ÌÅ¨Î¶ΩÌåÖ) :</target>
        </trans-unit>
        <trans-unit id="de4e7f540085eb6296ec8260ef53372b15064ca1" translate="yes" xml:space="preserve">
          <source>Example (scripting a module with traced submodules):</source>
          <target state="translated">Ïòà (Ï∂îÏ†Å Îêú ÌïòÏúÑ Î™®ÎìàÎ°ú Î™®Îìà Ïä§ÌÅ¨Î¶ΩÌåÖ) :</target>
        </trans-unit>
        <trans-unit id="c5b0acb6ce867883478ae6d5579cfda80cbb9404" translate="yes" xml:space="preserve">
          <source>Example (scripting a simple module with a Parameter):</source>
          <target state="translated">Ïòà (Îß§Í∞ú Î≥ÄÏàòÎ°ú Í∞ÑÎã®Ìïú Î™®Îìà Ïä§ÌÅ¨Î¶ΩÌåÖ) :</target>
        </trans-unit>
        <trans-unit id="98fb0963dfe59db2efb10e03278c046397333fb9" translate="yes" xml:space="preserve">
          <source>Example (tracing a function):</source>
          <target state="translated">Ïòà (Ìï®Ïàò Ï∂îÏ†Å) :</target>
        </trans-unit>
        <trans-unit id="0dad6a76da0fc25a19704212a257d6379a12034b" translate="yes" xml:space="preserve">
          <source>Example (tracing a module with multiple methods):</source>
          <target state="translated">Ïòà (Ïó¨Îü¨ Î©îÏÑúÎìúÎ°ú Î™®Îìà Ï∂îÏ†Å) :</target>
        </trans-unit>
        <trans-unit id="07f0e6f11243b3ae74d74b1d28cef8c785cb16fa" translate="yes" xml:space="preserve">
          <source>Example (tracing an existing module):</source>
          <target state="translated">Ïòà (Í∏∞Ï°¥ Î™®Îìà Ï∂îÏ†Å) :</target>
        </trans-unit>
        <trans-unit id="68ede4837476671e511c48828e0de48f7b06d426" translate="yes" xml:space="preserve">
          <source>Example (type annotations for Python 3):</source>
          <target state="translated">Ïòà (Python 3Ïùò Ïú†Ìòï Ï£ºÏÑù) :</target>
        </trans-unit>
        <trans-unit id="87a104ad2463311906e75683079bbad94a4fd5c2" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.export&lt;/code&gt; on a method):</source>
          <target state="translated">Ïòà ( Î©îÏÑúÎìúÏóê &lt;code&gt;@torch.jit.export&lt;/code&gt; ÏÇ¨Ïö© ) :</target>
        </trans-unit>
        <trans-unit id="d1c7a00dbf049f876c9759d8441a029ed8e2f4ac" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; on a method):</source>
          <target state="translated">ÏòàÏ†ú ( Î©îÏÑúÎìúÏóê &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; ÏÇ¨Ïö©) :</target>
        </trans-unit>
        <trans-unit id="f45fe2e88f095f6547360c930d8119bf8c200149" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore&lt;/code&gt; on a method):</source>
          <target state="translated">Ïòà ( Î©îÏÑúÎìúÏóê &lt;code&gt;@torch.jit.ignore&lt;/code&gt; ÏÇ¨Ïö© ) :</target>
        </trans-unit>
        <trans-unit id="ee3db4aefed96eb801a5613278d0970a19c709cc" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.unused&lt;/code&gt; on a method):</source>
          <target state="translated">Ïòà ( Î©îÏÑúÎìúÏóê &lt;code&gt;@torch.jit.unused&lt;/code&gt; ÏÇ¨Ïö© ) :</target>
        </trans-unit>
        <trans-unit id="15234a3935f9814be4e208815ad355afc291f504" translate="yes" xml:space="preserve">
          <source>Example (using a traced module):</source>
          <target state="translated">Ïòà (Ï∂îÏ†Å Îêú Î™®Îìà ÏÇ¨Ïö©) :</target>
        </trans-unit>
        <trans-unit id="49895a4623d5b4a9344031509b184da5a2c67818" translate="yes" xml:space="preserve">
          <source>Example. if we have the following shape for inputs and outputs:</source>
          <target state="translated">Ïòà. ÏûÖÎ†• Î∞è Ï∂úÎ†•Ïóê ÎåÄÌï¥ Îã§ÏùåÍ≥º Í∞ôÏùÄ Î™®ÏñëÏù¥ÏûàÎäî Í≤ΩÏö∞ :</target>
        </trans-unit>
        <trans-unit id="c63737abd7347a7ae582cb9fbdf37d6c0e5b251e" translate="yes" xml:space="preserve">
          <source>Example:</source>
          <target state="translated">Example:</target>
        </trans-unit>
        <trans-unit id="8ec20a84a991e14aeedf10c8e4e9247bf2c9315c" translate="yes" xml:space="preserve">
          <source>Example: End-to-end AlexNet from PyTorch to ONNX</source>
          <target state="translated">Ïòà : PyTorchÏóêÏÑú ONNX Î°úÏùò Ï¢ÖÎã® Í∞Ñ AlexNet</target>
        </trans-unit>
        <trans-unit id="038d2f8486ff39be2d765514d254dcc770c02da8" translate="yes" xml:space="preserve">
          <source>Example: Suppose the last window is: &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; vs &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</source>
          <target state="translated">Ïòà : ÎßàÏßÄÎßâ Ï∞ΩÏù¥ Îã§ÏùåÍ≥º Í∞ôÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§. &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; ÎåÄ &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6104a08ed7eb335488d3ea3f93c6210a1cc4746c" translate="yes" xml:space="preserve">
          <source>Example::</source>
          <target state="translated">Example::</target>
        </trans-unit>
        <trans-unit id="eb01bf04c9a0e8a71c45816513df424f1c7ffedb" translate="yes" xml:space="preserve">
          <source>Examples</source>
          <target state="translated">Examples</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="de3e24010b1050a8265462ab7e8f3a95c00717ea" translate="yes" xml:space="preserve">
          <source>Examples::</source>
          <target state="translated">Examples::</target>
        </trans-unit>
        <trans-unit id="8cc1b49841e2eac10bc692d1dee6f79c0d6dc0f7" translate="yes" xml:space="preserve">
          <source>Expand this tensor to the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.expand_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.expand(other.size())&lt;/code&gt;.</source>
          <target state="translated">Ïù¥ ÌÖêÏÑúÎ•º &lt;code&gt;other&lt;/code&gt; ÏôÄ Í∞ôÏùÄ ÌÅ¨Í∏∞Î°ú ÌôïÏû•Ìï©ÎãàÎã§ . &lt;code&gt;self.expand_as(other)&lt;/code&gt; Îäî &lt;code&gt;self.expand(other.size())&lt;/code&gt; ÏôÄ ÎèôÏùºÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="7381344dd49d389b0f60204fb4dbd8a7ee76371e" translate="yes" xml:space="preserve">
          <source>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the &lt;code&gt;stride&lt;/code&gt; to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</source>
          <target state="translated">ÌÖêÏÑúÎ•º ÌôïÏû•ÌïòÎ©¥ ÏÉà Î©îÎ™®Î¶¨Í∞Ä Ìï†ÎãπÎêòÏßÄ ÏïäÍ≥†, &lt;code&gt;stride&lt;/code&gt; Î•º 0 ÏúºÎ°ú ÏÑ§Ï†ïÌïòÏó¨ ÌÅ¨Í∏∞ 1Ïùò Ï∞®ÏõêÏù¥ Îçî ÌÅ∞ ÌÅ¨Í∏∞Î°ú ÌôïÏû•ÎêòÎäî Í∏∞Ï°¥ ÌÖêÏÑúÏóê ÏÉà Î∑∞Îßå ÏÉùÏÑ± Îê©ÎãàÎã§. ÌÅ¨Í∏∞ 1Ïùò Î™®Îì† Ï∞®ÏõêÏùÄ ÏûÑÏùòÏùò Í∞íÏúºÎ°ú ÌôïÏû• Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÏÉàÎ°úÏö¥ Î©îÎ™®Î¶¨Î•º Ìï†ÎãπÌïòÏßÄ ÏïäÍ≥†.</target>
        </trans-unit>
        <trans-unit id="76b27001feb75250d904bb68f5aaa46711a6a0f1" translate="yes" xml:space="preserve">
          <source>Expands the dimension &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; of the &lt;code&gt;self&lt;/code&gt; tensor over multiple dimensions of sizes given by &lt;code&gt;sizes&lt;/code&gt;.</source>
          <target state="translated">Ï∞®Ïõê ÌôïÏû• &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt; Ïùò &lt;code&gt;self&lt;/code&gt; Ïóê ÏùòÌï¥ Ï£ºÏñ¥ÏßÑ ÌÅ¨Í∏∞Ïùò Îã§Ï∞®Ïõê ÏúÑÏóê ÌÖêÏÑú &lt;code&gt;sizes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6e40e001bf1e0b420d657a639d6d67387b6d4923" translate="yes" xml:space="preserve">
          <source>Expected inputs are spatial (4 dimensional). Use &lt;code&gt;upsample_trilinear&lt;/code&gt; fo volumetric (5 dimensional) inputs.</source>
          <target state="translated">ÏòàÏÉÅÎêòÎäî ÏûÖÎ†•ÏùÄ Í≥µÍ∞Ñ (4 Ï∞®Ïõê)ÏûÖÎãàÎã§. Ï≤¥Ï†Å (5 Ï∞®Ïõê) ÏûÖÎ†•Ïóê ÎåÄÌï¥ &lt;code&gt;upsample_trilinear&lt;/code&gt; Î•º ÏÇ¨Ïö© Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="551ccd41342efa881333f0f1e558164218f9345a" translate="yes" xml:space="preserve">
          <source>Expected result:</source>
          <target state="translated">ÏòàÏÉÅ Í≤∞Í≥º:</target>
        </trans-unit>
        <trans-unit id="8439d6715059ee70b74368357e1335f786d20b96" translate="yes" xml:space="preserve">
          <source>Expects &lt;code&gt;input&lt;/code&gt; to be &amp;lt;= 2-D tensor and transposes dimensions 0 and 1.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; Ïù¥ 2 Ï∞®Ïõê ÌÖêÏÑúÎ≥¥Îã§ ÏûëÏùÑ Í≤ÉÏúºÎ°ú ÏòàÏÉÅ ÌïòÍ≥† Ï∞®Ïõê 0Í≥º 1ÏùÑ Ï†ÑÏπòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8133389ca86b79f9ac63f2057897dfbe1cda5cc8" translate="yes" xml:space="preserve">
          <source>Explicit alignment by names</source>
          <target state="translated">Ïù¥Î¶ÑÏóê ÏùòÌïú Î™ÖÏãú Ï†Å Ï†ïÎ†¨</target>
        </trans-unit>
        <trans-unit id="58b807aacff8abe3f97a111c7a1e5f71d192fe91" translate="yes" xml:space="preserve">
          <source>Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)</source>
          <target state="translated">Î™®Îç∏ÏùÑ ONNX ÌòïÏãùÏúºÎ°ú ÎÇ¥ Î≥¥ÎÉÖÎãàÎã§. Ïù¥ ÏùµÏä§Ìè¨ÌÑ∞Îäî ÏùµÏä§Ìè¨Ìä∏ Îê† Ïã§Ìñâ Ï∂îÏ†ÅÏùÑ ÏñªÍ∏∞ ÏúÑÌï¥ Î™®Îç∏ÏùÑ Ìïú Î≤à Ïã§ÌñâÌï©ÎãàÎã§. ÌòÑÏû¨Î°úÏÑúÎäî Ï†úÌïúÎêú ÎèôÏ†Å Î™®Îç∏ (Ïòà : RNN) ÏÑ∏Ìä∏Î•º ÏßÄÏõêÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="69c85a023a54b1f8defe44bdc84c4b1308b399c8" translate="yes" xml:space="preserve">
          <source>Exporting models with unsupported ONNX operators can be achieved using the &lt;code&gt;operator_export_type&lt;/code&gt; flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX.</source>
          <target state="translated">ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ONNX Ïó∞ÏÇ∞ÏûêÍ∞ÄÏûàÎäî Î™®Îç∏ ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Îäî ÎÇ¥Î≥¥ÎÇ¥Í∏∞ API Ïùò &lt;code&gt;operator_export_type&lt;/code&gt; ÌîåÎûòÍ∑∏Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏàòÌñâ Ìï† Ïàò ÏûàÏäµÎãàÎã§ . Ïù¥ ÌîåÎûòÍ∑∏Îäî ÏÇ¨Ïö©ÏûêÍ∞Ä ONNXÏóêÏÑú Îì±Î°ù Î∞è ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ATen Î∞è ÎπÑ ATen Ïó∞ÏÇ∞ÏûêÎ•º ÎÇ¥Î≥¥ÎÇ¥Î†§Í≥† Ìï† Îïå Ïú†Ïö©Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ae5fccd8dcd8fc317f8edfc8259af86cd2967a29" translate="yes" xml:space="preserve">
          <source>Expressions</source>
          <target state="translated">Expressions</target>
        </trans-unit>
        <trans-unit id="44bdb40abdeed26ffb35b097c6be620648eb56bc" translate="yes" xml:space="preserve">
          <source>Extending PyTorch</source>
          <target state="translated">PyTorch ÌôïÏû•</target>
        </trans-unit>
        <trans-unit id="a505bba828df658cd19ca21d48f6b057b88e1432" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; outputs. Such operation is really only stable when &lt;code&gt;input&lt;/code&gt; is full rank with all distinct singular values. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; even if the original backward is only on &lt;code&gt;S&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;U&lt;/code&gt; Î∞è &lt;code&gt;V&lt;/code&gt; Ï∂úÎ†•ÏùÑ ÌÜµÌï¥ ÌõÑÏßÑ Ìï† Îïå Í∞ÅÎ≥ÑÌïúÏ£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§ . Ïù¥Îü¨Ìïú ÏûëÏóÖÏùÄ &lt;code&gt;input&lt;/code&gt; Ïù¥ Î™®Îì† Í≥†Ïú† Ìïú ÌäπÏù¥ Í∞íÏúºÎ°ú Ï†ÑÏ≤¥ ÏàúÏúÑ Ïùº ÎïåÎßå ÏïàÏ†ïÏ†Å ÏûÖÎãàÎã§. Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Í∑∏ÎùºÎîîÏñ∏Ìä∏Í∞Ä Ï†úÎåÄÎ°ú Ï†ïÏùòÎêòÏßÄ ÏïäÏïÑ &lt;code&gt;NaN&lt;/code&gt; Ïù¥ ÎÇòÌÉÄÎÇ† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÌïú Ïù¥Ï§ë Îí§Î°úÎäî ÏõêÎûò Îí§Î°úÍ∞Ä &lt;code&gt;S&lt;/code&gt; ÏóêÎßåÏûàÎäî Í≤ΩÏö∞ÏóêÎèÑ ÏùºÎ∞òÏ†ÅÏúºÎ°ú &lt;code&gt;U&lt;/code&gt; Î∞è &lt;code&gt;V&lt;/code&gt; Î•º ÌÜµÌï¥ Ï∂îÍ∞Ä Îí§Î°ú ÏûëÏóÖÏùÑ ÏàòÌñâÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="7869f96c8dcb40e05a18db44c02551a0604c8dd8" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined.</source>
          <target state="translated">Ï∂úÎ†•ÏùÑ Îí§Î°ú Ìï† Îïå Ï∂îÍ∞ÄÏ£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. Ïù¥Îü¨Ìïú Ïó∞ÏÇ∞ÏùÄ Î™®Îì† Í≥†Ïú† Í∞íÏù¥ Íµ¨Î≥Ñ Îê† ÎïåÎßå Ïã§Ï†úÎ°ú ÏïàÏ†ïÏ†ÅÏûÖÎãàÎã§. Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Í∑∏ÎùºÎîîÏñ∏Ìä∏Í∞Ä Ï†úÎåÄÎ°ú Ï†ïÏùòÎêòÏßÄ ÏïäÏïÑ &lt;code&gt;NaN&lt;/code&gt; Ïù¥ ÎÇòÌÉÄÎÇ† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="35e4d02caad3850761be7f0a939f52dc8e6d05c6" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from a batched input tensor.</source>
          <target state="translated">ÏùºÍ¥Ñ ÏûÖÎ†• ÌÖêÏÑúÏóêÏÑú Ïä¨ÎùºÏù¥Îî© Î°úÏª¨ Î∏îÎ°ùÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="545d81b806ede14bef050d6bbe02b72b7b5279d1" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from an batched input tensor.</source>
          <target state="translated">ÏùºÍ¥Ñ ÏûÖÎ†• ÌÖêÏÑúÏóêÏÑú Ïä¨ÎùºÏù¥Îî© Î°úÏª¨ Î∏îÎ°ùÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d1636ed5d55d52fc51dab6a56c1fdb216178f4c5" translate="yes" xml:space="preserve">
          <source>FAST mode algorithm</source>
          <target state="translated">FAST Î™®Îìú ÏïåÍ≥†Î¶¨Ï¶ò</target>
        </trans-unit>
        <trans-unit id="5d8a2052196e0929b1ddeff4f9fa793509ac8f6c" translate="yes" xml:space="preserve">
          <source>FCN ResNet101</source>
          <target state="translated">FCN ResNet101</target>
        </trans-unit>
        <trans-unit id="d876bcc45100c189667e02c674ab45db60de2d8b" translate="yes" xml:space="preserve">
          <source>FCN ResNet50</source>
          <target state="translated">FCN ResNet50</target>
        </trans-unit>
        <trans-unit id="70deee53be1d417368b869145a93da9f61814dda" translate="yes" xml:space="preserve">
          <source>FCN ResNet50, ResNet101</source>
          <target state="translated">FCN ResNet50, ResNet101</target>
        </trans-unit>
        <trans-unit id="58296524e883e134fa6cc4840027902c76d7e637" translate="yes" xml:space="preserve">
          <source>Factory functions now take a new &lt;code&gt;names&lt;/code&gt; argument that associates a name with each dimension.</source>
          <target state="translated">Ïù¥Ï†ú Ìå©ÌÜ†Î¶¨ Ìï®Ïàò Îäî Ïù¥Î¶ÑÏùÑ Í∞Å Ï∞®ÏõêÍ≥º Ïó∞Í≤∞ÌïòÎäî ÏÉà &lt;code&gt;names&lt;/code&gt; Ïù∏ÏàòÎ•ºÎ∞õÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="97cdbdc7feff827efb082a6b6dd2727237cd49fd" translate="yes" xml:space="preserve">
          <source>False</source>
          <target state="translated">False</target>
        </trans-unit>
        <trans-unit id="6bca42e6cb3531d60196488b0aafe02849dfad8a" translate="yes" xml:space="preserve">
          <source>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</source>
          <target state="translated">Ïª¥ÌååÏùºÎü¨Í∞Ä PyTorchÏôÄ ABIÏôÄ Ìò∏ÌôòÎêòÏßÄ ÏïäÏùÑ Í∞ÄÎä•ÏÑ±Ïù¥ÏûàÎäî Í≤ΩÏö∞ False, Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ TrueÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f66134050db6d6f1871f0a392769d7d300bffc78" translate="yes" xml:space="preserve">
          <source>Faster R-CNN</source>
          <target state="translated">Îçî Îπ†Î•∏ R-CNN</target>
        </trans-unit>
        <trans-unit id="19fa372bf4511d5897a6678506a36d53384e83c1" translate="yes" xml:space="preserve">
          <source>Faster R-CNN ResNet-50 FPN</source>
          <target state="translated">Îçî Îπ†Î•∏ R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="ceaa939a1707b8201f9f233e5c8d2c8a11872247" translate="yes" xml:space="preserve">
          <source>Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">Îçî Îπ†Î•∏ R-CNNÏùÄ Í≥†Ï†ï Îêú ÌÅ¨Í∏∞Ïùò ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Í≥†Ï†ï Îêú Î∞∞Ïπò ÌÅ¨Í∏∞Î°ú ONNXÎ°ú ÎÇ¥Î≥¥ÎÇº Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b9a92e1a2a80529a37624caec86105f3856f6680" translate="yes" xml:space="preserve">
          <source>FeatureDropout (training mode not supported)</source>
          <target state="translated">FeatureDropout (Ìä∏Î†àÏù¥Îãù Î™®ÎìúÎäî ÏßÄÏõêÎêòÏßÄ ÏïäÏùå)</target>
        </trans-unit>
        <trans-unit id="023ddfe2580672ca0eeb5f867eac27e114575b07" translate="yes" xml:space="preserve">
          <source>Features described in this documentation are classified by release status:</source>
          <target state="translated">Ïù¥ Î¨∏ÏÑúÏóê ÏÑ§Î™Ö Îêú Í∏∞Îä•ÏùÄ Î¶¥Î¶¨Ïä§ ÏÉÅÌÉúÎ≥ÑÎ°ú Î∂ÑÎ•òÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="4ed223cb662e6eadab1b5774d593969ec64456ed" translate="yes" xml:space="preserve">
          <source>Features for large-scale deployments</source>
          <target state="translated">ÎåÄÍ∑úÎ™® Î∞∞Ìè¨Î•ºÏúÑÌïú Í∏∞Îä•</target>
        </trans-unit>
        <trans-unit id="29fd83b7db12e4d9231f579c15fdaeb5d71086e4" translate="yes" xml:space="preserve">
          <source>Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&amp;gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.</source>
          <target state="translated">ÏµúÏÜå 2 Ï∞®ÏõêÏù¥ÏûàÎäî ÌÖêÏÑúÏùò Ï£º ÎåÄÍ∞ÅÏÑ†ÏùÑ Ï±Ñ ÏõÅÎãàÎã§. dims&amp;gt; 2 Ïù∏ Í≤ΩÏö∞ ÏûÖÎ†•Ïùò Î™®Îì† ÏπòÏàòÎäî Í∏∏Ïù¥Í∞Ä Í∞ôÏïÑÏïºÌï©ÎãàÎã§. Ïù¥ Ìï®ÏàòÎäî ÏûÖÎ†• ÌÖêÏÑúÎ•º Ï†úÏûêÎ¶¨ÏóêÏÑú ÏàòÏ†ïÌïòÍ≥† ÏûÖÎ†• ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="46412e89386beda08d3ce994cbdd41f7b9cb6e05" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the exponential distribution:</source>
          <target state="translated">ÏßÄÏàò Î∂ÑÌè¨ÏóêÏÑú Í∞ÄÏ†∏Ïò® ÏöîÏÜåÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="1aea402a861ce53378696f47d930c7de8244acde" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the geometric distribution:</source>
          <target state="translated">Í∏∞ÌïòÌïôÏ†Å Î∂ÑÌè¨ÏóêÏÑú Í∞ÄÏ†∏Ïò® ÏöîÏÜåÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="1abd5eabced3b954f0b9c8f459ed264742cdc1be" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements samples from the normal distribution parameterized by &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt; Î∞è &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; Î°ú Î™®ÏàòÌôî&lt;/a&gt; Îêú Ï†ïÍ∑ú Î∂ÑÌè¨Ïùò ÏöîÏÜå ÏÉòÌîåÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="be7637b77a168dc9781dec5a6963103f27f1e666" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the continuous uniform distribution:</source>
          <target state="translated">Ïó∞ÏÜç Í∑†Ïùº Î∂ÑÌè¨ÏóêÏÑú ÏÉòÌîåÎßÅ Îêú Ïà´ÏûêÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="963887f4ec9debd27ff138179ec127b4ceb1a324" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the discrete uniform distribution over &lt;code&gt;[from, to - 1]&lt;/code&gt;. If not specified, the values are usually only bounded by &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s data type. However, for floating point types, if unspecified, range will be &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; to ensure that every value is representable. For example, &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; will be uniform in &lt;code&gt;[0, 2^53]&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º &lt;code&gt;[from, to - 1]&lt;/code&gt; Í±∏Ï≥ê Ïù¥ÏÇ∞ Í∑†Îì± Î∂ÑÌè¨ÏóêÏÑú ÏÉòÌîåÎßÅ Ìïú Ïà´ÏûêÎ°ú Ï±Ñ ÏõÅÎãàÎã§ . ÏßÄÏ†ïÌïòÏßÄ ÏïäÏúºÎ©¥ Í∞íÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÏùò Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏóê ÏùòÌï¥ÏÑúÎßå Ï†úÌïúÎê©ÎãàÎã§ . Í∑∏Îü¨ÎÇò Î∂ÄÎèô ÏÜåÏàòÏ†ê Ïú†ÌòïÏùò Í≤ΩÏö∞ ÏßÄÏ†ïÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Î≤îÏúÑÎäî Î™®Îì† Í∞íÏùÑ ÌëúÌòÑÌï† Ïàò ÏûàÎèÑÎ°ù &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; Îê©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥ &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; ÏùÄ &lt;code&gt;[0, 2^53]&lt;/code&gt; ÏóêÏÑú Í∑†Ïùº Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b7f10ef5f693feaeaa010e2aa027c2e16d3faf27" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers samples from the log-normal distribution parameterized by the given mean</source>
          <target state="translated">Ï£ºÏñ¥ÏßÑ ÌèâÍ∑†ÏúºÎ°ú Î™®ÏàòÌôî Îêú Î°úÍ∑∏ Ï†ïÍ∑ú Î∂ÑÌè¨Ïùò Ïà´Ïûê ÏÉòÌîåÎ°ú &lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a537a70caec95c45887848e05dbf76327e57f2bf" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with the specified value.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º ÏßÄÏ†ïÎêú Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="84e59f1fa6c91fd2336c166329083335c07742c5" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with zeros.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; ÌÖêÏÑúÎ•º 0ÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="c357bdf1d8cc36baaa85a26ceaf45fb123516806" translate="yes" xml:space="preserve">
          <source>Fills each location of &lt;code&gt;self&lt;/code&gt; with an independent sample from</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; Í∞Å ÏúÑÏπò Î•º Îã§ÏùåÏùò ÎèÖÎ¶ΩÏ†Å Ïù∏ ÏÉòÌîåÎ°ú Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="75c253cce2f7953023782f50f2d324ceb99f06ec" translate="yes" xml:space="preserve">
          <source>Fills elements of &lt;code&gt;self&lt;/code&gt; tensor with &lt;code&gt;value&lt;/code&gt; where &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor.</source>
          <target state="translated">Ïùò ÏöîÏÜåÎ•º Ï±Ñ ÏõÅÎãàÎã§ &lt;code&gt;self&lt;/code&gt; ÏôÄ ÌÖêÏÑúÎ•º &lt;code&gt;value&lt;/code&gt; &lt;code&gt;mask&lt;/code&gt; TrueÏûÖÎãàÎã§. Ïùò Î™®Ïñë &lt;code&gt;mask&lt;/code&gt; Ìï¥Ïïº &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Ï∫êÏä§Ìä∏ Í∞ÄÎä•Ìïú&lt;/a&gt; Í∏∞Î≥∏ ÌÖêÏÑúÏùò Î™®Ïñë.</target>
        </trans-unit>
        <trans-unit id="7f2c547e676c650b0294e152d6742725981b5e7b" translate="yes" xml:space="preserve">
          <source>Fills the 2-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the identity matrix. Preserves the identity of the inputs in &lt;code&gt;Linear&lt;/code&gt; layers, where as many inputs are preserved as possible.</source>
          <target state="translated">2 Ï∞®Ïõê ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Î•º Îã®ÏúÑ ÌñâÎ†¨Î°ú Ï±Ñ ÏõÅÎãàÎã§ . Í∞ÄÎä•Ìïú Ìïú ÎßéÏùÄ ÏûÖÎ†•Ïù¥ Î≥¥Ï°¥ÎêòÎäî &lt;code&gt;Linear&lt;/code&gt; Î†àÏù¥Ïñ¥ Ïùò ÏûÖÎ†• IDÎ•º Ïú†ÏßÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5e10a9506b95f83d62c2104cb07cd55facde3766" translate="yes" xml:space="preserve">
          <source>Fills the 2D input &lt;code&gt;Tensor&lt;/code&gt; as a sparse matrix, where the non-zero elements will be drawn from the normal distribution</source>
          <target state="translated">2D ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Î•º Ìù¨ÏÜå ÌñâÎ†¨Î°ú Ï±Ñ ÏõÅÎãàÎã§. Ïó¨Í∏∞ÏÑú 0Ïù¥ ÏïÑÎãå ÏöîÏÜåÎäî Ï†ïÍ∑ú Î∂ÑÌè¨ÏóêÏÑú Í∑∏Î†§ÏßëÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="5e66c6e2e841f8a8523fead84aa32e52990c1b4b" translate="yes" xml:space="preserve">
          <source>Fills the elements of the &lt;code&gt;self&lt;/code&gt; tensor with value &lt;code&gt;val&lt;/code&gt; by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">Ïùò ÏöîÏÜå Ï±ÑÏö¥Îã§ &lt;code&gt;self&lt;/code&gt; Í∞íÍ≥º ÌÖêÏÑú &lt;code&gt;val&lt;/code&gt; Ï£ºÏñ¥ÏßÑ ÏàúÏÑúÎåÄÎ°ú Ïù∏Îç±Ïä§Î•º ÏÑ†ÌÉùÌïòÏó¨ &lt;code&gt;index&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3a21a74e5af8bd75efccfeb2dc749fe2483f779d" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with a (semi) orthogonal matrix, as described in &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.</source>
          <target state="translated">ÏûÖÎ†• Ï±ÑÏö¥Îã§ &lt;code&gt;Tensor&lt;/code&gt; Ïóê ÏÑ§Î™Ö ÎêúÎåÄÎ°ú (ÏÑ∏ÎØ∏) ÏßÅÍµê ÌñâÎ†¨ &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; ÏÉâÏä§, A. Ïô∏ Ïïå -. (2013). ÏûÖÎ†• ÌÖêÏÑúÎäî 2 Ï∞®Ïõê Ïù¥ÏÉÅÏùÑ Í∞ÄÏ†∏ÏïºÌïòÎ©∞ 2 Ï∞®ÏõêÏùÑ Ï¥àÍ≥ºÌïòÎäî ÌÖêÏÑúÏùò Í≤ΩÏö∞ ÌõÑÌñâ Ï∞®ÏõêÏù¥ ÌèâÎ©¥ÌôîÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="06242c8039ea176e183491ddfbaaf7c2d66c68e8" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">&lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; -He, K. et al.Ïóê ÏÑ§Î™Ö Îêú Î∞©Î≤ïÏóê Îî∞Îùº ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Î•º Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§ . (2015), Ï†ïÍ∑ú Î∂ÑÌè¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í≤∞Í≥º ÌÖêÏÑúÎäî Îã§ÏùåÏóêÏÑú ÏÉòÌîåÎßÅ Îêú Í∞íÏùÑ Í∞ñÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="342727420e0d4e4b77efe66611c6eb6db5be0acf" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">&lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; -He, K. et al.Ïóê ÏÑ§Î™Ö Îêú Î∞©Î≤ïÏóê Îî∞Îùº ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Î•º Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§ . (2015), Í∑†Îì± Î∂ÑÌè¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í≤∞Í≥º ÌÖêÏÑúÎäî Îã§ÏùåÏóêÏÑú ÏÉòÌîåÎßÅ Îêú Í∞íÏùÑ Í∞ñÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="71e3100123b6866d1997e6df9daeefd940ad80de" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">Ï†ïÍ∑ú Î∂ÑÌè¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot, X. &amp;amp; Bengio, Y. (2010)Ïóê ÏÑ§Î™Ö Îêú Î∞©Î≤ïÏóê Îî∞Îùº ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Ïóê Í∞íÏùÑ Ï±Ñ ÏõÅÎãàÎã§ . Í≤∞Í≥º ÌÖêÏÑúÎäî Îã§ÏùåÏóêÏÑú ÏÉòÌîåÎßÅ Îêú Í∞íÏùÑ Í∞ñÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="2313477ebbbc49bb7f4bf7d5c39c5ea0206267ac" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">Í∑†Ïùº Î∂ÑÌè¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot, X. &amp;amp; Bengio, Y. (2010)Ïóê ÏÑ§Î™Ö Îêú Î∞©Î≤ïÏóê Îî∞Îùº ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Ïóê Í∞íÏùÑ Ï±Ñ ÏõÅÎãàÎã§ . Í≤∞Í≥º ÌÖêÏÑúÎäî Îã§ÏùåÏóêÏÑú ÏÉòÌîåÎßÅ Îêú Í∞íÏùÑ Í∞ñÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="afacae29047abc8bbea1f5a18cb9f7afddce8004" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">ÏûÖÎ†• TensorÎ•º Ïä§ÏπºÎùº Í∞í &lt;code&gt;0&lt;/code&gt; ÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="0749fb4668d1e4b76f3a1ba9aa423e792bc38d58" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">ÏûÖÎ†• TensorÎ•º Ïä§ÏπºÎùº Í∞í &lt;code&gt;1&lt;/code&gt; Î°ú Ï±Ñ ÏõÅÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="498455641766cf82c112342c938821f2f05dedf1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the value</source>
          <target state="translated">ÏûÖÎ†• TensorÎ•º Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="92513a6a0cb417782fe4aae045c4a1b66c923b2f" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the normal distribution</source>
          <target state="translated">ÏûÖÎ†• TensorÎ•º Ï†ïÍ∑ú Î∂ÑÌè¨ÏóêÏÑú Í∞ÄÏ†∏Ïò® Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="6b3d75035cf3e14fc34e56b36e9d5379c6dd95d1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the uniform distribution</source>
          <target state="translated">ÏûÖÎ†• TensorÎ•º Í∑†Ïùº Î∂ÑÌè¨ÏóêÏÑú Í∞ÄÏ†∏Ïò® Í∞íÏúºÎ°ú Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="53430dbd9b3ec1b5ed7bd9a57a201a7f2d1a60f2" translate="yes" xml:space="preserve">
          <source>Fills the tensor with numbers drawn from the Cauchy distribution:</source>
          <target state="translated">ÏΩîÏãú Î∂ÑÌè¨ÏóêÏÑú Ï∂îÏ∂úÌïú Ïà´ÏûêÎ°ú ÌÖêÏÑúÎ•º Ï±Ñ ÏõÅÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c6632aebf9bc7adafe6c2b8a271e6b35dc1107ed" translate="yes" xml:space="preserve">
          <source>Fills the {3, 4, 5}-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the Dirac delta function. Preserves the identity of the inputs in &lt;code&gt;Convolutional&lt;/code&gt; layers, where as many input channels are preserved as possible. In case of groups&amp;gt;1, each group of channels preserves identity</source>
          <target state="translated">{3, 4, 5} Ï∞®Ïõê ÏûÖÎ†• &lt;code&gt;Tensor&lt;/code&gt; Î•º Dirac Îç∏ÌÉÄ Ìï®ÏàòÎ°ú Ï±Ñ ÏõÅÎãàÎã§ . Í∞ÄÎä•Ìïú Ìïú ÎßéÏùÄ ÏûÖÎ†• Ï±ÑÎÑêÏù¥ Î≥¥Ï°¥ÎêòÎäî &lt;code&gt;Convolutional&lt;/code&gt; Î†àÏù¥Ïñ¥ Ïùò ÏûÖÎ†• IDÎ•º Ïú†ÏßÄÌï©ÎãàÎã§. Í∑∏Î£π&amp;gt; 1Ïùò Í≤ΩÏö∞ Í∞Å Ï±ÑÎÑê Í∑∏Î£πÏùÄ IDÎ•º Ïú†ÏßÄÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="695c574fe765006dd0a54d1a5cdae48d767984bd" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved.</source>
          <target state="translated">Î°úÎ∂ÄÌÑ∞ Ïù∏Îç±Ïä§ Ï∞æÍ∏∞ &lt;em&gt;Ïµú&lt;/em&gt; Ïùò ÏπòÏàò &lt;code&gt;sorted_sequence&lt;/code&gt; ÏóêÏÑú Ìï¥Îãπ Í∞í Í≤ΩÏö∞ Í∑∏Ïóê Îî∞Îùº, &lt;code&gt;values&lt;/code&gt; Ïù∏Îç±Ïä§ ÏïûÏóê ÏÇΩÏûÖÌïòÍ≥†, Ìï¥Îãπ ÏàúÏÑú &lt;em&gt;Ïµú&lt;/em&gt; ÎÇ¥Ïùò ÏπòÏàò &lt;code&gt;sorted_sequence&lt;/code&gt; Ïù¥ Î≥¥Ï°¥ Îê†ÏûàÎã§.</target>
        </trans-unit>
        <trans-unit id="bc20e9bbc443d3b0fd0b14b6ff920327eb09f1d2" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved. Return a new tensor with the same size as &lt;code&gt;values&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary of &lt;code&gt;sorted_sequence&lt;/code&gt; is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="translated">Î°úÎ∂ÄÌÑ∞ Ïù∏Îç±Ïä§ Ï∞æÍ∏∞ &lt;em&gt;Ïµú&lt;/em&gt; Ïùò ÏπòÏàò &lt;code&gt;sorted_sequence&lt;/code&gt; ÏóêÏÑú Ìï¥Îãπ Í∞í Í≤ΩÏö∞ Í∑∏Ïóê Îî∞Îùº, &lt;code&gt;values&lt;/code&gt; Ïù∏Îç±Ïä§ ÏïûÏóê ÏÇΩÏûÖÌïòÍ≥†, Ìï¥Îãπ ÏàúÏÑú &lt;em&gt;Ïµú&lt;/em&gt; ÎÇ¥Ïùò ÏπòÏàò &lt;code&gt;sorted_sequence&lt;/code&gt; Ïù¥ Î≥¥Ï°¥ Îê†ÏûàÎã§. &lt;code&gt;values&lt;/code&gt; ÏôÄ Í∞ôÏùÄ ÌÅ¨Í∏∞Ïùò ÏÉà ÌÖêÏÑúÎ•º Î∞òÌôò Ìï©ÎãàÎã§ . Í≤ΩÏö∞ &lt;code&gt;right&lt;/code&gt; (Í∏∞Î≥∏Í∞í) FalseÏûÖÎãàÎã§, Îã§ÏùåÏùò ÏôºÏ™Ω Í≤ΩÍ≥Ñ &lt;code&gt;sorted_sequence&lt;/code&gt; Ïù¥ Îã´ÌûôÎãàÎã§. Î≥¥Îã§ Í≥µÏãùÏ†ÅÏúºÎ°ú Î∞òÌôò Îêú Ïù∏Îç±Ïä§Îäî Îã§Ïùå Í∑úÏπôÏùÑ Ï∂©Ï°±Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="03215b19f0bffce926c83716ff9399ab447bdf55" translate="yes" xml:space="preserve">
          <source>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</source>
          <target state="translated">ÌñâÎ†¨Ïù¥ÏóÜÎäî LOBPCG Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏñëÏùò ÎåÄÏπ≠ÏúºÎ°ú Ï†ïÏùò Îêú ÏùºÎ∞ò Í≥†Ïú† Í∞í Î¨∏Ï†úÏùò k Í∞úÏùò Í∞ÄÏû• ÌÅ∞ (ÎòêÎäî Í∞ÄÏû• ÏûëÏùÄ) Í≥†Ïú† Í∞íÍ≥º Ìï¥ÎãπÌïòÎäî Í≥†Ïú† Î≤°ÌÑ∞Î•º Ï∞æÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d6ca52cc281a8bbbaa3d9b53b49079e29eb878fa" translate="yes" xml:space="preserve">
          <source>First convert your model from GPU to CPU and then save it, like so:</source>
          <target state="translated">Î®ºÏ†Ä Î™®Îç∏ÏùÑ GPUÏóêÏÑú CPUÎ°ú Î≥ÄÌôò Ìïú Îã§Ïùå Îã§ÏùåÍ≥º Í∞ôÏù¥ Ï†ÄÏû•Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="56b7d451e08bf1ccd9aa8267b497203df559b032" translate="yes" xml:space="preserve">
          <source>First, if you repeatedly perform an operation that can produce duplicate entries (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt;&lt;/a&gt;), you should occasionally coalesce your sparse tensors to prevent them from growing too large.</source>
          <target state="translated">Ï≤´Ïß∏, Ï§ëÎ≥µ Ìï≠Î™©ÏùÑ ÏÉùÏÑ± Ìï† ÏàòÏûàÎäî ÏûëÏóÖ (Ïòà : &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt; &lt;/a&gt; ) ÏùÑ Î∞òÎ≥µÏ†ÅÏúºÎ°ú ÏàòÌñâÌïòÎäî Í≤ΩÏö∞ Í∞ÄÎÅîÏî© Ìù¨ÏÜå ÌÖêÏÑúÎ•º Î≥ëÌï©ÌïòÏó¨ ÎÑàÎ¨¥ Ïª§ÏßÄÏßÄ ÏïäÎèÑÎ°ùÌï¥ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0af14ddb20aabbe1bd98f28f2c2f744284ccedb3" translate="yes" xml:space="preserve">
          <source>Flatten</source>
          <target state="translated">Flatten</target>
        </trans-unit>
        <trans-unit id="557473442912b0bbbc1f4c6573c0fe9837b23ef2" translate="yes" xml:space="preserve">
          <source>Flattens &lt;code&gt;dims&lt;/code&gt; into a single dimension with name &lt;code&gt;out_dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;out_dim&lt;/code&gt; Ïù¥ÎùºÎäî Ïù¥Î¶ÑÏúºÎ°ú Îã®Ïùº Ï∞®ÏõêÏúºÎ°ú ÌèâÌèâ &lt;code&gt;dims&lt;/code&gt; Ìï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="80a931c216b5d2a192745812d7ca953d11d59b70" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims in a tensor.</source>
          <target state="translated">ÌÖêÏÑúÏóêÏÑú Ïó∞ÏÜç Îêú Î≤îÏúÑÏùò Ìù¨ÎØ∏ Ìï®ÏùÑ ÌèâÌèâÌïòÍ≤åÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e2ed5b97e25777e5a578840b676a3b9e44b41cab" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor.</source>
          <target state="translated">Ïó∞ÏÜç Îêú Î≤îÏúÑÏùò DimÏùÑ ÌÖêÏÑúÎ°ú ÌèâÌèâÌïòÍ≤å ÎßåÎì≠ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="bcfce6cdf44c8905f6faca75da235eedd5635aac" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">Ïó∞ÏÜç Îêú Î≤îÏúÑÏùò DimÏùÑ ÌÖêÏÑúÎ°ú ÌèâÌèâÌïòÍ≤å ÎßåÎì≠ÎãàÎã§. &lt;code&gt;Sequential&lt;/code&gt; Í≥º Ìï®Íªò ÏÇ¨Ïö© Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="db0a60d5a36fb0f5467808a539ac8c0b01a8dadd" translate="yes" xml:space="preserve">
          <source>Flip array in the left/right direction, returning a new tensor.</source>
          <target state="translated">ÏÉà ÌÖêÏÑúÎ•º Î∞òÌôòÌïòÎäî ÏôºÏ™Ω / Ïò§Î•∏Ï™Ω Î∞©Ìñ•ÏúºÎ°ú Î∞∞Ïó¥ÏùÑ Îí§ÏßëÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e9abeaf079a0d69d0b2ee4704e055dbacab017f3" translate="yes" xml:space="preserve">
          <source>Flip array in the up/down direction, returning a new tensor.</source>
          <target state="translated">Î∞∞Ïó¥ÏùÑ ÏúÑ / ÏïÑÎûò Î∞©Ìñ•ÏúºÎ°ú Îí§ÏßëÏñ¥ ÏÉàÎ°úÏö¥ ÌÖêÏÑúÎ•º Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d6b601389a5cfb7f115ef7332083ae3431e4e4e3" translate="yes" xml:space="preserve">
          <source>Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.</source>
          <target state="translated">Í∞Å Ïó¥Ïùò Ìï≠Î™©ÏùÑ ÏúÑ / ÏïÑÎûò Î∞©Ìñ•ÏúºÎ°ú Îí§ÏßëÏäµÎãàÎã§. ÌñâÏùÄ Ïú†ÏßÄÎêòÏßÄÎßå Ïù¥Ï†ÑÍ≥º Îã§Î•∏ ÏàúÏÑúÎ°ú ÎÇòÌÉÄÎÇ©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f14bb6de935d5a8cea7f1dc6e4323a38f4e11462" translate="yes" xml:space="preserve">
          <source>Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.</source>
          <target state="translated">Í∞Å ÌñâÏùò Ìï≠Î™©ÏùÑ ÏôºÏ™Ω / Ïò§Î•∏Ï™Ω Î∞©Ìñ•ÏúºÎ°ú Îí§ÏßëÏäµÎãàÎã§. Ïó¥ÏùÄ Ïú†ÏßÄÎêòÏßÄÎßå Ïù¥Ï†ÑÍ≥º Îã§Î•∏ ÏàúÏÑúÎ°ú ÎÇòÌÉÄÎÇ©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="23c9f78a2bf71d761dc5a430092962a70ec045f2" translate="yes" xml:space="preserve">
          <source>FloatFunctional</source>
          <target state="translated">FloatFunctional</target>
        </trans-unit>
        <trans-unit id="0352ecfbeaceff6ef8214a3af42e602c1d2f0c6a" translate="yes" xml:space="preserve">
          <source>Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.</source>
          <target state="translated">Ïù¥Î≤§Ìä∏ ÌååÏùºÏùÑ ÎîîÏä§ÌÅ¨Î°ú ÌîåÎü¨ÏãúÌï©ÎãàÎã§. Ïù¥ Î©îÏÑúÎìúÎ•º Ìò∏Ï∂úÌïòÏó¨ Î≥¥Î•òÏ§ëÏù∏ Î™®Îì† Ïù¥Î≤§Ìä∏Í∞Ä ÎîîÏä§ÌÅ¨Ïóê Í∏∞Î°ùÎêòÏóàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b6ba0db1f814179114ec82fbf188b2eb5be2596e" translate="yes" xml:space="preserve">
          <source>Fold</source>
          <target state="translated">Fold</target>
        </trans-unit>
        <trans-unit id="ab2123970899470af7c3bcdbf9832cd5ea8344b1" translate="yes" xml:space="preserve">
          <source>Following this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;, you can create and register your own custom ops implementation in PyTorch. Here&amp;rsquo;s how to export such model to ONNX.:</source>
          <target state="translated">Ïù¥ ÌäúÌÜ†Î¶¨ÏñºÏóê Îî∞Îùº &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;ÏÇ¨Ïö©Ïûê ÏßÄÏ†ï C ++ Ïó∞ÏÇ∞ÏûêÎ•º ÏÇ¨Ïö©ÌïòÏó¨ TorchScript ÌôïÏû•, PyTorchÏóêÏÑú ÏÇ¨Ïö©Ïûê&lt;/a&gt; ÏßÄÏ†ï ÏûëÏóÖ Íµ¨ÌòÑÏùÑ ÎßåÎì§Í≥† Îì±Î°ù Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Î™®Îç∏ÏùÑ ONNXÎ°ú ÎÇ¥Î≥¥ÎÇ¥Îäî Î∞©Î≤ïÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§. :</target>
        </trans-unit>
        <trans-unit id="c762389bca1bb2b05603a57f1353b6b58529cac2" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects returned by &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;future.wait()&lt;/code&gt; should not be called after &lt;code&gt;shutdown()&lt;/code&gt;.</source>
          <target state="translated">Îì§Ïñ¥ &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; Ïóê ÏùòÌï¥ Î∞òÌôò Îêú Í∞ùÏ≤¥ &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt; , &lt;code&gt;future.wait()&lt;/code&gt; Ïù¥ÌõÑÏóê Ìò∏Ï∂ú Ìï† Ïàò ÏóÜÏäµÎãàÎã§ &lt;code&gt;shutdown()&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="068880daf2a8a0b7a7463ef0178819889346c85b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;N&lt;/code&gt;-dimensional padding, use &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Îì§Î©¥ &lt;code&gt;N&lt;/code&gt; Ï∞®Ïõê Ìå®Îî© ÏÇ¨Ïö© &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt; &lt;code&gt;torch.nn.functional.pad()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0224a530bfe0946c7afd8a4140361f1cd0dcd86b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;torch.nn.functional&lt;/code&gt; operators, we support the following:</source>
          <target state="translated">Îì§Ïñ¥ &lt;code&gt;torch.nn.functional&lt;/code&gt; ÏÇ¨ÏóÖÏûê, Ïö∞Î¶¨Îäî Îã§ÏùåÏùÑ ÏßÄÏõêÌï©ÎãàÎã§ :</target>
        </trans-unit>
        <trans-unit id="e1e6b2fc17a48e201a5b7dbea3ac88cd7be57af5" translate="yes" xml:space="preserve">
          <source>For CPU tensors, this method is currently only available with MKL. Use &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt;&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;/a&gt; to check if MKL is installed.</source>
          <target state="translated">CPU ÌÖêÏÑúÏùò Í≤ΩÏö∞Ïù¥ Î∞©Î≤ïÏùÄ ÌòÑÏû¨ MKLÏóêÏÑúÎßå ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt; &lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt; &lt;/a&gt; ÏùÑ ÏÇ¨Ïö© ÌïòÏó¨ MKLÏù¥ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="5a9dc733d94431c6a4d80dbab50dc02413b91172" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;cuFFT plan cache&lt;/a&gt; for more details on how to monitor and control the cache.</source>
          <target state="translated">CUDA ÌÖêÏÑúÏùò Í≤ΩÏö∞, ÎèôÏùºÌïú Íµ¨ÏÑ±ÏùÑ Í∞ÄÏßÑ ÎèôÏùºÌïú ÏßÄÏò§Î©îÌä∏Î¶¨Ïùò ÌÖêÏÑúÏóêÏÑú FFT Î©îÏÑúÎìúÎ•º Î∞òÎ≥µÏ†ÅÏúºÎ°ú Ïã§ÌñâÌïòÎäî ÏÜçÎèÑÎ•º ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ cuFFT Í≥ÑÌöçÏóê LRU Ï∫êÏãúÍ∞Ä ÏÇ¨Ïö©Îê©ÎãàÎã§. &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;Ï∫êÏãú&lt;/a&gt; Î•º Î™®ÎãàÌÑ∞ÎßÅÌïòÍ≥† Ï†úÏñ¥ÌïòÎäî ‚Äã‚ÄãÎ∞©Î≤ïÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö© ÏùÄ cuFFT Í≥ÑÌöç Ï∫êÏãú Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="c3f7242e16cf3e0469ec63da054b257bd101a205" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.</source>
          <target state="translated">CUDA ÌÖêÏÑúÏùò Í≤ΩÏö∞Ïù¥ Ìï®ÏàòÎäî ÌÖêÏÑúÍ∞ÄÏûàÎäî GPUÏùò Ïû•Ïπò ÏÑúÏàòÎ•º Î∞òÌôòÌï©ÎãàÎã§. CPU ÌÖêÏÑúÏùò Í≤ΩÏö∞ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ce3ce048e3708a40dbc3057fc7dbd0788b47316f" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="translated">Ïù¥ ÌÖêÏÑúÎ•º Îì§Ïñ¥ &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; Ïù∏ &lt;code&gt;True&lt;/code&gt; Í∑∏Îì§Ïù¥ ÏÇ¨Ïö©ÏûêÏóê ÏùòÌï¥ ÏÉùÏÑ± Îêú Í≤ΩÏö∞, Í∑∏Îì§ÏùÄ Ïûé ÌÖêÏÑú Îê† Í≤ÉÏûÖÎãàÎã§. Ï¶â, ÏûëÏóÖÏùò Í≤∞Í≥ºÍ∞Ä ÏïÑÎãàÎØÄÎ°ú &lt;code&gt;grad_fn&lt;/code&gt; ÏùÄ NoneÏûÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="8d2d3af0bd6ef80615c67c0f72bb823e693063c0" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor the output is specified by:</source>
          <target state="translated">3 Ï∞®Ïõê ÌÖêÏÑúÏùò Í≤ΩÏö∞ Ï∂úÎ†•ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏù¥ ÏßÄÏ†ïÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="3f570fa1da407a7894b9ad9cdc01e54242454c59" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="translated">3 Ï∞®Ïõê ÌÖêÏÑúÏùò Í≤ΩÏö∞ &lt;code&gt;self&lt;/code&gt; Îäî Îã§ÏùåÍ≥º Í∞ôÏù¥ ÏóÖÎç∞Ïù¥Ìä∏Îê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="dd4d8e8f7d7355e68f887ffe9f112d1d44ab2731" translate="yes" xml:space="preserve">
          <source>For a comprehensive list of name inference rules, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;. Here are two common operations that may be useful to go over:</source>
          <target state="translated">Ïù¥Î¶Ñ Ïú†Ï∂î Í∑úÏπôÏùò Ï†ÑÏ≤¥ Î™©Î°ùÏùÄ &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Î™ÖÎ™Ö Îêú ÌÖêÏÑú Ïó∞ÏÇ∞Ïûê Ï†ÅÏö© Î≤îÏúÑÎ•º&lt;/a&gt; Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ . ÏÇ¥Ìé¥Î≥¥Í∏∞Ïóê Ïú†Ïö©Ìïú Îëê Í∞ÄÏßÄ ÏùºÎ∞òÏ†ÅÏù∏ ÏûëÏóÖÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="f91f8919582617014f59317b7352c3f1154712ab" translate="yes" xml:space="preserve">
          <source>For a full listing of supported Python features, see &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python Language Reference Coverage&lt;/a&gt;.</source>
          <target state="translated">ÏßÄÏõêÎêòÎäî Python Í∏∞Îä•Ïùò Ï†ÑÏ≤¥ Î™©Î°ùÏùÄ &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python Ïñ∏Ïñ¥ Ï∞∏Ï°∞ Î≤îÏúÑÎ•º Ï∞∏Ï°∞&lt;/a&gt; ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="4cd19a97e1a392f82662e959d703403f2351a98b" translate="yes" xml:space="preserve">
          <source>For a gentle introduction to TorchScript, see the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; tutorial.</source>
          <target state="translated">TorchScriptÏóê ÎåÄÌïú &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Í∞ÑÎã®Ìïú ÏÜåÍ∞úÎäî TorchScript ÏÜåÍ∞ú&lt;/a&gt; ÏûêÏäµÏÑú Î•º Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="8e0e8c59f7042dfe9b36d63277e5e7440cb658b4" translate="yes" xml:space="preserve">
          <source>For a tensor &lt;code&gt;input&lt;/code&gt; of sizes</source>
          <target state="translated">ÌÅ¨Í∏∞ Ïùò ÌÖêÏÑú &lt;code&gt;input&lt;/code&gt; Ïùò Í≤ΩÏö∞</target>
        </trans-unit>
        <trans-unit id="7afccb9a65464232e6685319e5c664295be01466" translate="yes" xml:space="preserve">
          <source>For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a PyTorch Model in C++&lt;/a&gt; tutorial.</source>
          <target state="translated">PyTorch Î™®Îç∏ÏùÑ TorchScriptÎ°ú Î≥ÄÌôòÌïòÍ≥† C ++ÏóêÏÑú Ïã§ÌñâÌïòÎäî Ï¢ÖÎã® Í∞Ñ ÏòàÏ†úÎäî C ++ÏóêÏÑú &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;PyTorch Î™®Îç∏Î°úÎìú&lt;/a&gt; ÏûêÏäµÏÑúÎ•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.</target>
        </trans-unit>
        <trans-unit id="c08a3397ebd78b324e2b03c935d901274f6914be" translate="yes" xml:space="preserve">
          <source>For bags of constant length and no &lt;code&gt;per_sample_weights&lt;/code&gt;, this class</source>
          <target state="translated">Í∏∏Ïù¥Í∞Ä ÏùºÏ†ïÌïòÍ≥† &lt;code&gt;per_sample_weights&lt;/code&gt; Í∞Ä ÏóÜÎäî bag Ïùò Í≤ΩÏö∞Ïù¥ ÌÅ¥ÎûòÏä§</target>
        </trans-unit>
        <trans-unit id="ce5d81031fb4d7e97b33ce443ec1d33497cec1cd" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ÏûÖÎ†• Ïù∏Ïàò, Îß§Í∞ú Î≥ÄÏàò Î∞è Íµ¨ÌòÑÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; Î•º&lt;/a&gt; Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="36b1210fa3359dec217e0f8625012720c3f10bc6" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ÏûÖÎ†• Ïù∏Ïàò, Îß§Í∞ú Î≥ÄÏàò Î∞è Íµ¨ÌòÑÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; Î•º&lt;/a&gt; Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="17fc1bf0dec370ea37946985facbba42ffd22e0f" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">ÏûÖÎ†• Ïù∏Ïàò, Îß§Í∞ú Î≥ÄÏàò Î∞è Íµ¨ÌòÑÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; Î•º&lt;/a&gt; Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§ .</target>
        </trans-unit>
        <trans-unit id="88fa6dcc55dbef1b20fcb850f35a6375f7b3938c" translate="yes" xml:space="preserve">
          <source>For each element in the input sequence, each layer computes the following function:</source>
          <target state="translated">ÏûÖÎ†• ÏãúÌÄÄÏä§Ïùò Í∞Å ÏöîÏÜåÏóê ÎåÄÌï¥ Í∞Å Í≥ÑÏ∏µÏùÄ Îã§Ïùå Ìï®ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="d139363266e05a6b552e5defd18e04652bf83d44" translate="yes" xml:space="preserve">
          <source>For each mini-batch sample, the loss in terms of the 1D input</source>
          <target state="translated">Í∞Å ÎØ∏Îãà Î∞∞Ïπò ÏÉòÌîåÏóê ÎåÄÌï¥ 1D ÏûÖÎ†• Ï∏°Î©¥ÏóêÏÑú ÏÜêÏã§</target>
        </trans-unit>
        <trans-unit id="431645996fbd3fae9490db55c077050efdd9469b" translate="yes" xml:space="preserve">
          <source>For each output location &lt;code&gt;output[n, :, h, w]&lt;/code&gt;, the size-2 vector &lt;code&gt;grid[n, h, w]&lt;/code&gt; specifies &lt;code&gt;input&lt;/code&gt; pixel locations &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, which are used to interpolate the output value &lt;code&gt;output[n, :, h, w]&lt;/code&gt;. In the case of 5D inputs, &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; specifies the &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; pixel locations for interpolating &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt;. &lt;code&gt;mode&lt;/code&gt; argument specifies &lt;code&gt;nearest&lt;/code&gt; or &lt;code&gt;bilinear&lt;/code&gt; interpolation method to sample the input pixels.</source>
          <target state="translated">Í∞Å Ï∂úÎ†• ÏúÑÏπò &lt;code&gt;output[n, :, h, w]&lt;/code&gt; Ïóê ÎåÄÌï¥ size-2 Î≤°ÌÑ∞ &lt;code&gt;grid[n, h, w]&lt;/code&gt; Îäî Ï∂úÎ†• Í∞í &lt;code&gt;output[n, :, h, w]&lt;/code&gt; ÏùÑ Î≥¥Í∞ÑÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÎäî &lt;code&gt;input&lt;/code&gt; ÌîΩÏÖÄ ÏúÑÏπò &lt;code&gt;x&lt;/code&gt; Î∞è &lt;code&gt;y&lt;/code&gt; Î•º ÏßÄÏ†ïÌï©ÎãàÎã§ . h, w] . 5D ÏûÖÎ†•Ïùò Í≤ΩÏö∞ &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; Îäî &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt; Î≥¥Í∞ÑÏùÑÏúÑÌïú &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;y&lt;/code&gt; , &lt;code&gt;z&lt;/code&gt; ÌîΩÏÖÄ ÏúÑÏπòÎ•º ÏßÄÏ†ïÌï©ÎãàÎã§ . &lt;code&gt;mode&lt;/code&gt; Ïù∏Ïàò Îäî ÏûÖÎ†• ÌîΩÏÖÄÏùÑ ÏÉòÌîåÎßÅÌïòÍ∏∞ ÏúÑÌï¥ &lt;code&gt;nearest&lt;/code&gt; ÎòêÎäî &lt;code&gt;bilinear&lt;/code&gt; Î≥¥Í∞Ñ Î∞©Î≤ïÏùÑ ÏßÄÏ†ïÌï©ÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="a707b704ee783d908df45582d74fb5eed5208cc2" translate="yes" xml:space="preserve">
          <source>For example, assigning to &lt;code&gt;self&lt;/code&gt; outside of the &lt;code&gt;__init__()&lt;/code&gt; method:</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥, &lt;code&gt;__init__()&lt;/code&gt; Î©îÏÑúÎìú Ïô∏Î∂Ä ÏóêÏÑú &lt;code&gt;self&lt;/code&gt; Ïóê Ìï†Îãπ :</target>
        </trans-unit>
        <trans-unit id="598cd3a9f173b17b51c024dfc8ec80175fdbbd3c" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is a vector of size N, the result will also be a vector of size N, with elements.</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ &lt;code&gt;input&lt;/code&gt; Ïù¥ ÌÅ¨Í∏∞Í∞Ä N Ïù∏ Î≤°ÌÑ∞ Ïù∏ Í≤ΩÏö∞ Í≤∞Í≥ºÎäî ÏöîÏÜåÍ∞ÄÏûàÎäî ÌÅ¨Í∏∞Í∞Ä N Ïù∏ Î≤°ÌÑ∞ÎèÑÎê©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c5eb0f2bf9e3391a2f0f43ac5e99b77a4de61cf1" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ &lt;code&gt;input&lt;/code&gt; Ïù¥ ÌòïÌÉú Ïù∏ Í≤ΩÏö∞ :</target>
        </trans-unit>
        <trans-unit id="cb7354d4a324678f0e4b20e916940e161856f119" translate="yes" xml:space="preserve">
          <source>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then &lt;code&gt;pos_weight&lt;/code&gt; for the class should be equal to</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê Îã®Ïùº ÌÅ¥ÎûòÏä§Ïùò 100 Í∞úÏùò Í∏çÏ†ï Î∞è 300 Í∞úÏùò Î∂ÄÏ†ïÏ†ÅÏù∏ ÏòàÏ†úÍ∞Ä Ìè¨Ìï® Îêú Í≤ΩÏö∞ ÌÅ¥ÎûòÏä§Ïùò &lt;code&gt;pos_weight&lt;/code&gt; Îäî Îã§Ïùå Í≥º Í∞ôÏïÑÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="ec89e688ce49781ad9518cc67374c388cc79a9c9" translate="yes" xml:space="preserve">
          <source>For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ Î∂ÑÏÇ∞ ÌïôÏäµÏóê ÏÇ¨Ïö©ÌïòÎäî ÏãúÏä§ÌÖúÏóê 2 Í∞úÏùò ÎÖ∏ÎìúÍ∞Ä ÏûàÍ≥† Í∞Å ÎÖ∏ÎìúÏóêÎäî 8 Í∞úÏùò GPUÍ∞Ä ÏûàÏäµÎãàÎã§. 16 Í∞úÏùò GPU Í∞ÅÍ∞ÅÏóêÎäî Î™®Îëê Ï∂ïÏÜåÌïòÎ†§Îäî ÌÖêÏÑúÍ∞Ä ÏûàÏäµÎãàÎã§. Îã§Ïùå ÏΩîÎìúÎäî Ï∞∏Ï°∞Î°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="b076a6f19f5c3f95d3197664aa2092f90ac3e4fd" translate="yes" xml:space="preserve">
          <source>For example, suppose that we wanted to implement an operator by operating directly on &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt;. Multiplication by a scalar can be implemented in the obvious way, as multiplication distributes over addition; however, square root cannot be implemented directly, since &lt;code&gt;sqrt(a + b) != sqrt(a) +
sqrt(b)&lt;/code&gt; (which is what would be computed if you were given an uncoalesced tensor.)</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt; &lt;/a&gt; ÏóêÏÑú ÏßÅÏ†ë ÏûëÎèôÌïòÏó¨ Ïó∞ÏÇ∞ÏûêÎ•º Íµ¨ÌòÑÌïòÍ≥† Ïã∂Îã§Í≥† Í∞ÄÏ†ï Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§ . Ïä§ÏπºÎùºÏóê ÏùòÌïú Í≥±ÏÖàÏùÄ Í≥±ÏÖàÏù¥ ÎçßÏÖàÏóê Î∂ÑÏÇ∞ÎêòÎØÄÎ°ú Î™ÖÎ∞±Ìïú Î∞©ÏãùÏúºÎ°ú Íµ¨ÌòÑ Îê† Ïàò ÏûàÏäµÎãàÎã§. Í∑∏Îü¨ÎÇò &lt;code&gt;sqrt(a + b) != sqrt(a) + sqrt(b)&lt;/code&gt; (ÌÜµÌï©ÎêòÏßÄ ÏïäÏùÄ ÌÖêÏÑúÍ∞Ä Ï£ºÏñ¥ÏßÄÎ©¥ Í≥ÑÏÇ∞ÎêòÎäî Í≤ÉÏûÖÎãàÎã§ ) ÎïåÎ¨∏Ïóê Ï†úÍ≥±Í∑ºÏùÄ ÏßÅÏ†ë Íµ¨ÌòÑÌï† Ïàò ÏóÜÏäµÎãàÎã§ .</target>
        </trans-unit>
        <trans-unit id="b3dee60b13728b6faa87ab6d08e758ac130cfdfa" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; must be a real number, otherwise an integer.</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; ÎòêÎäî &lt;code&gt;DoubleTensor&lt;/code&gt; Ïú†ÌòïÏùò ÏûÖÎ†•Ïùò Í≤ΩÏö∞ &lt;code&gt;value&lt;/code&gt; ÏùÄ Ïã§Ïàò Ïó¨ÏïºÌïòÎ©∞ Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Ï†ïÏàò Ïó¨ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="a79d59fd72d10daf74904a50c1c53f2a33467eeb" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; ÎòêÎäî &lt;code&gt;DoubleTensor&lt;/code&gt; Ïú†ÌòïÏùò ÏûÖÎ†•Ïùò Í≤ΩÏö∞ &lt;code&gt;beta&lt;/code&gt; Î∞è &lt;code&gt;alpha&lt;/code&gt; Ïù∏Ïàò Îäî Ïã§Ïàò Ïó¨ÏïºÌïòÎ©∞ Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Ï†ïÏàò Ïó¨ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="c25cdbf09c7b02888e46dd28ab2af0cd145a33a9" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; ÎòêÎäî &lt;code&gt;DoubleTensor&lt;/code&gt; Ïú†ÌòïÏùò ÏûÖÎ†•Ïùò Í≤ΩÏö∞ &lt;code&gt;beta&lt;/code&gt; Î∞è &lt;code&gt;alpha&lt;/code&gt; Ïù∏Ïàò Îäî Ïã§Ïàò Ïó¨ÏïºÌïòÎ©∞ Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥ Ï†ïÏàò Ïó¨ÏïºÌï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">ÏòàÎ•º Îì§Ïñ¥ :</target>
        </trans-unit>
        <trans-unit id="22c3fe0b8bd316af1ba2f91851ec2f8f7995ee83" translate="yes" xml:space="preserve">
          <source>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt;&lt;code&gt;Tensor.get_device()&lt;/code&gt;&lt;/a&gt;, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</source>
          <target state="translated">Î†àÍ±∞Ïãú Ïù¥Ïú†Î°ú Ïû•ÏπòÎäî cuda Ïû•ÏπòÎ°ú Ï∑®Í∏âÎêòÎäî Îã®Ïùº Ïû•Ïπò ÏÑúÏàòÎ•º ÌÜµÌï¥ Íµ¨ÏÑ± Îê† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Í≤ÉÏùÄ cuda ÌÖêÏÑúÏóê ÎåÄÌïú ÏÑúÏàòÎ•º Î∞òÌôòÌïòÍ≥† cpu ÌÖêÏÑúÏóê ÎåÄÌï¥ ÏßÄÏõêÎêòÏßÄ ÏïäÎäî &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt; &lt;code&gt;Tensor.get_device()&lt;/code&gt; &lt;/a&gt; ÏôÄ ÏùºÏπò Ìï©ÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="0aa3a8773fa895e1e6152468962406770d9b0977" translate="yes" xml:space="preserve">
          <source>For loops over constant nn.ModuleList</source>
          <target state="translated">ÏÉÅÏàò nn.ModuleListÏóê ÎåÄÌïú For Î£®ÌîÑ</target>
        </trans-unit>
        <trans-unit id="d0ca847043fc995de69359f3596ce96734b521c7" translate="yes" xml:space="preserve">
          <source>For loops over tuples</source>
          <target state="translated">ÌäúÌîåÏóê ÎåÄÌïú For Î£®ÌîÑ</target>
        </trans-unit>
        <trans-unit id="5aa0148bf5ef53de46ea15471ea81ef66c0693f4" translate="yes" xml:space="preserve">
          <source>For loops with range</source>
          <target state="translated">Î≤îÏúÑÍ∞ÄÏûàÎäî Î£®ÌîÑ</target>
        </trans-unit>
        <trans-unit id="0d65cf1ed6eabecd00c26213c93e0d17f7fcdf45" translate="yes" xml:space="preserve">
          <source>For more complicated uses of the profilers (like in a multi-GPU case), please see &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; or &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt;&lt;/a&gt; for more information.</source>
          <target state="translated">ÌîÑÎ°úÌååÏùº Îü¨Ïùò Îçî Î≥µÏû°Ìïú ÏÇ¨Ïö© (Ïòà : Îã§Ï§ë GPUÏùò Í≤ΩÏö∞)Ïóê ÎåÄÌï¥ÏÑúÎäî &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; ÎòêÎäî &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt; &lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt; &lt;/a&gt; ÏóêÏÑú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÑ Ï∞∏Ï°∞ÌïòÏã≠ÏãúÏò§.</target>
        </trans-unit>
        <trans-unit id="7b8781b8ea8a1a7fd16e315cb8caa2f0d8fd81ce" translate="yes" xml:space="preserve">
          <source>For more information on &lt;code&gt;torch.sparse_coo&lt;/code&gt; tensors, see &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">Ïóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;code&gt;torch.sparse_coo&lt;/code&gt; Ïùò ÌÖêÏÑú, Ï∞∏Ï°∞ &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5a0ad772d1091f121cc7efb6b451798990b8911a" translate="yes" xml:space="preserve">
          <source>For more information on tensor views, see &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;Tensor Views&lt;/a&gt;.</source>
          <target state="translated">ÌÖêÏÑú Î∑∞Ïóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;ÌÖêÏÑúÎ≥¥Í∏∞&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1f3909dab7a0a74dfec923801d50d6afccbbd76f" translate="yes" xml:space="preserve">
          <source>For more information on the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt; attributes of a &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;, see &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;Tensor Attributes&lt;/a&gt;.</source>
          <target state="translated">Ïò® ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; Î∞è &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; &lt;/a&gt; (A)Ïùò ÏÜçÏÑ± &lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; Ï∞∏Ï°∞ &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;ÌÖêÏÑú ÏÜçÏÑ±&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e6568b8e5f7166a4f6114617787be1538e34997e" translate="yes" xml:space="preserve">
          <source>For now, normalization code can be found in &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt;, see the &lt;code&gt;Normalize&lt;/code&gt; function there. Note that it differs from standard normalization for images because it assumes the video is 4d.</source>
          <target state="translated">ÌòÑÏû¨ Ï†ïÍ∑úÌôî ÏΩîÎìúÎäî &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt; ÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÏäµÎãàÎã§ . Ïó¨Í∏∞ÏÑú &lt;code&gt;Normalize&lt;/code&gt; Ìï®ÏàòÎ•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî . ÎπÑÎîîÏò§Í∞Ä 4dÎùºÍ≥† Í∞ÄÏ†ïÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÌëúÏ§Ä Ï†ïÍ∑úÌôîÏôÄ Îã§Î¶ÖÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="43d57f5d34139266a152ef339407d7c7bfa3e16a" translate="yes" xml:space="preserve">
          <source>For numerical stability the implementation reverts to the linear function when</source>
          <target state="translated">ÏàòÏπò ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌï¥ Íµ¨ÌòÑÏùÄ Îã§ÏùåÍ≥º Í∞ôÏùÄ Í≤ΩÏö∞ ÏÑ†Ìòï Ìï®ÏàòÎ°ú ÎêòÎèåÏïÑÍ∞ëÎãàÎã§.</target>
        </trans-unit>
        <trans-unit id="e2c95cbc7cf538a8a02cb4f1bc38de8b5a3ed2b9" translate="yes" xml:space="preserve">
          <source>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</source>
          <target state="translated">Í∞ùÏ≤¥ Í∞êÏßÄ Î∞è Ïù∏Ïä§ÌÑ¥Ïä§ Î∂ÑÌï†Ïùò Í≤ΩÏö∞ ÏÇ¨Ï†Ñ ÌïôÏäµ Îêú Î™®Îç∏ÏùÄ Îã§Ïùå ÌÅ¥ÎûòÏä§Ïùò ÏòàÏ∏°ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
