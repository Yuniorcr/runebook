<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="374b7dab14fec694b4f5430f4c9e29dd3262cee0" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt;&lt;code&gt;torch.atleast_2d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt; &lt;code&gt;torch.atleast_2d()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b6ef108509ff368b6991089b8608a37aca821f0d" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</source>
          <target state="translated">This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</target>
        </trans-unit>
        <trans-unit id="c5b9e7c481799905ba918331b66fdb7b66a8da08" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt;&lt;code&gt;torch.atleast_3d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt; &lt;code&gt;torch.atleast_3d()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c60beaf37836d9a93c155f39fdfc6ecdf807a30e" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="d6810c532f9495d317b56fc8164ab42f154df6d1" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="14aba227b9dce4ab34682734cfe12d5fc9a3b9e5" translate="yes" xml:space="preserve">
          <source>This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form</source>
          <target state="translated">이것은 Sutskever et. al. 및 양식 업데이트를 사용하는 기타 프레임 워크</target>
        </trans-unit>
        <trans-unit id="60444d425f826d3c68c2b033a78fb2f503cd684c" translate="yes" xml:space="preserve">
          <source>This is likely less than the amount shown in &lt;code&gt;nvidia-smi&lt;/code&gt; since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;Memory management&lt;/a&gt; for more details about GPU memory management.</source>
          <target state="translated">사용되지 않은 메모리 중 일부는 캐싱 할당자가 보유 할 수 있고 일부 컨텍스트는 GPU에서 생성해야하기 때문에 이는 &lt;code&gt;nvidia-smi&lt;/code&gt; 에 표시된 양보다 적을 수 있습니다. 참조 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-management&quot;&gt;메모리 관리를&lt;/a&gt; GPU 메모리 관리에 대한 자세한 내용은.</target>
        </trans-unit>
        <trans-unit id="c37867c00b5ddfbf25d4de90fb71189dbd82b435" translate="yes" xml:space="preserve">
          <source>This is mainly useful for changing the shape of the result of &lt;a href=&quot;#torch.distributions.independent.Independent.log_prob&quot;&gt;&lt;code&gt;log_prob()&lt;/code&gt;&lt;/a&gt;. For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:</source>
          <target state="translated">이것은 주로 &lt;a href=&quot;#torch.distributions.independent.Independent.log_prob&quot;&gt; &lt;code&gt;log_prob()&lt;/code&gt; &lt;/a&gt; 결과의 모양을 변경하는 데 유용합니다 . 예를 들어 다변량 정규 분포와 모양이 같은 대각선 정규 분포를 만들려면 (교환 가능) 다음을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="89d9637a7ca542b85a68e4470c388a54ccbbc2a4" translate="yes" xml:space="preserve">
          <source>This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.</source>
          <target state="translated">이것은 bijective가 아니며 HMC에 사용할 수 없습니다. 그러나 이것은 대부분 좌표 단위로 작동하므로 (최종 정규화 제외) 좌표 단위 최적화 알고리즘에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="18f227b9fbcb58581a48e62cc2b1e4b0a1736273" translate="yes" xml:space="preserve">
          <source>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</source>
          <target state="translated">This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</target>
        </trans-unit>
        <trans-unit id="61139b0235e460eab7709588bf0db5fef3c1ed33" translate="yes" xml:space="preserve">
          <source>This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</source>
          <target state="translated">This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</target>
        </trans-unit>
        <trans-unit id="5d93a3f447e3870288983d617bf3d5ca0babcaca" translate="yes" xml:space="preserve">
          <source>This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="b773fd2df4f9d314a4e612fca433c54f4b6db8e8" translate="yes" xml:space="preserve">
          <source>This is the default strategy (except for macOS and OS X where it&amp;rsquo;s not supported).</source>
          <target state="translated">이것이 기본 전략입니다 (지원되지 않는 macOS 및 OS X 제외).</target>
        </trans-unit>
        <trans-unit id="b58bf6260a34528b9c54af5a74ed650426bd462d" translate="yes" xml:space="preserve">
          <source>This is the functional version of the DataParallel module.</source>
          <target state="translated">This is the functional version of the DataParallel module.</target>
        </trans-unit>
        <trans-unit id="28e50c155d4263693358c9685af859f0b288737a" translate="yes" xml:space="preserve">
          <source>This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).</source>
          <target state="translated">이것은 가장 일반적인 경우이며 데이터의 미니 배치를 가져 와서 일괄 처리 된 샘플로 조합하는 것에 해당합니다. 즉, 하나의 차원이 배치 차원 (일반적으로 첫 번째) 인 Tensor를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="9f2aa90c9ddd953d8134c3b2013f0dd5a5e04793" translate="yes" xml:space="preserve">
          <source>This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt; &lt;code&gt;ELU&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="051cad7dbbb9111a95c9efe4f04cfeb331282a9e" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="ea4fe276ddec4ebc483841a1170deaf100cb0956" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt; &lt;code&gt;BatchNorm3d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e5ce0ce0859be0cbdcd66ae4d96e1834e80fb440" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt;&lt;code&gt;GroupNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt; &lt;code&gt;GroupNorm&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4fd9da5ac2b1219bc045ac6f1d4efc7962f29b18" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt; &lt;code&gt;Hardswish&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="81ee4b5a559a55d776be5e5ad40fea15e08c77c3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt; &lt;code&gt;InstanceNorm1d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="306614a03d85542a417f30827501dbc097b0e4e2" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt; &lt;code&gt;InstanceNorm2d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5c54c49dfa0b1be230f77c6acfc8832f75ff793b" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt; &lt;code&gt;InstanceNorm3d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="1ab22dac0481c7e9d72041caf66a8d6e6bce24a3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt; &lt;code&gt;LayerNorm&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="1c1757bb112d3facda0c6b8d39d0c0a08d49265a" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt;&lt;code&gt;hardswish()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt; &lt;code&gt;hardswish()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c6493b5bad0951f0b43e4846b14cd1a429b62cc1" translate="yes" xml:space="preserve">
          <source>This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5295faab37629f799f6e6fbb1e9825a33f998b68" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;torch.max()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="19c006dca5dfcf6b24613e9eb4ec7d4564e3ffde" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;torch.min()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="ea4ca9f34dc42157c431ad05a09921e13d264890" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt; &lt;code&gt;torch.sort()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="f4162fd55661c8ae3715301b7e45fb9d304f83a8" translate="yes" xml:space="preserve">
          <source>This is typically passed to an optimizer.</source>
          <target state="translated">This is typically passed to an optimizer.</target>
        </trans-unit>
        <trans-unit id="8881d58b1f068203ff9ef6d05a7b8193f5aad690" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="bc05416a2fcfa2651cf566b1804c2fef547cfa8c" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="cdf0d415e18e8e03494fb14369b3744608b2dde4" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="387b12018432b2012dd14d16de801f3f2b4ba6be" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e9103d955a0e478fd4fc8f34beda0ed1c416c5a7" translate="yes" xml:space="preserve">
          <source>This is used by the &lt;code&gt;quantization&lt;/code&gt; utility functions to add the quant and dequant modules, before &lt;code&gt;convert&lt;/code&gt; function &lt;code&gt;QuantStub&lt;/code&gt; will just be observer, it observes the input tensor, after &lt;code&gt;convert&lt;/code&gt;, &lt;code&gt;QuantStub&lt;/code&gt; will be swapped to &lt;code&gt;nnq.Quantize&lt;/code&gt; which does actual quantization. Similarly for &lt;code&gt;DeQuantStub&lt;/code&gt;.</source>
          <target state="translated">이것은에 의해 사용되는 &lt;code&gt;quantization&lt;/code&gt; 전에 퀀트 및 dequant 모듈을 추가하는 유틸리티 함수 &lt;code&gt;convert&lt;/code&gt; 기능 &lt;code&gt;QuantStub&lt;/code&gt; 가 단지 관찰자 것, 그것은 입력 텐서가, 후에 관찰 &lt;code&gt;convert&lt;/code&gt; , &lt;code&gt;QuantStub&lt;/code&gt; 가 로 스왑됩니다 &lt;code&gt;nnq.Quantize&lt;/code&gt; 실제 양자화를 수행합니다. &lt;code&gt;DeQuantStub&lt;/code&gt; 과 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="7feb7536faa55e8682dba21d0a2c61b1c664ec04" translate="yes" xml:space="preserve">
          <source>This is used e.g. for indices returned from a max &lt;code&gt;Function&lt;/code&gt;.</source>
          <target state="translated">이것은 예를 들어 max &lt;code&gt;Function&lt;/code&gt; 에서 반환 된 인덱스에 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="18ae6606a0b9fa5a1d8375a29d5adf1260f68aa6" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</source>
          <target state="translated">This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</target>
        </trans-unit>
        <trans-unit id="44bcfdd8636d4fef63e74c75c8c5e2e8049a6723" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</source>
          <target state="translated">This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</target>
        </trans-unit>
        <trans-unit id="82ff1110b5f2f18a81804af99bb7a1033487ea4f" translate="yes" xml:space="preserve">
          <source>This is useful for implementing efficient sub-pixel convolution with a stride of</source>
          <target state="translated">This is useful for implementing efficient sub-pixel convolution with a stride of</target>
        </trans-unit>
        <trans-unit id="055744440e1ad514287a476301fa832544763a8f" translate="yes" xml:space="preserve">
          <source>This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.</source>
          <target state="translated">이는 촐레 스키 분해 측면에서 양의 정부 호 행렬을 매개 변수화하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="0b3e0c96ed0785872533b5ba8bff1ae227ce271e" translate="yes" xml:space="preserve">
          <source>This layer uses statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">This layer uses statistics computed from input data in both training and evaluation modes.</target>
        </trans-unit>
        <trans-unit id="e61f8043e790e86849febfe55d39af2c5aac3218" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</source>
          <target state="translated">This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</target>
        </trans-unit>
        <trans-unit id="940b923ef28b8c58cfd52fc019408d07c1f8a33f" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</source>
          <target state="translated">This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</target>
        </trans-unit>
        <trans-unit id="b85bf08f0c04a7b3a3773bc9f4c6784cf3720ebd" translate="yes" xml:space="preserve">
          <source>This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</source>
          <target state="translated">This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</target>
        </trans-unit>
        <trans-unit id="26f1b287bc5579a8caeed012280a7b3f3a9f07cb" translate="yes" xml:space="preserve">
          <source>This means that &lt;code&gt;model.base&lt;/code&gt;&amp;rsquo;s parameters will use the default learning rate of &lt;code&gt;1e-2&lt;/code&gt;, &lt;code&gt;model.classifier&lt;/code&gt;&amp;rsquo;s parameters will use a learning rate of &lt;code&gt;1e-3&lt;/code&gt;, and a momentum of &lt;code&gt;0.9&lt;/code&gt; will be used for all parameters.</source>
          <target state="translated">즉, &lt;code&gt;model.base&lt;/code&gt; 의 매개 변수는 기본 학습률 &lt;code&gt;1e-2&lt;/code&gt; 를 사용 하고 &lt;code&gt;model.classifier&lt;/code&gt; 의 매개 변수는 학습률 &lt;code&gt;1e-3&lt;/code&gt; 을 사용하며 모멘텀 &lt;code&gt;0.9&lt;/code&gt; 는 모든 매개 변수에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0b5f23050e1225367270ed2f80d8587f70aa2118" translate="yes" xml:space="preserve">
          <source>This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt;. Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt;, and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</source>
          <target state="translated">This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt; . Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt; , and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</target>
        </trans-unit>
        <trans-unit id="6b6071e9753ecc5843a463c244f78990925791d4" translate="yes" xml:space="preserve">
          <source>This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</source>
          <target state="translated">This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</target>
        </trans-unit>
        <trans-unit id="56fec11901471926f8e8051da013976d59b168bf" translate="yes" xml:space="preserve">
          <source>This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</source>
          <target state="translated">This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</target>
        </trans-unit>
        <trans-unit id="ab188eb8949a2274bed5b2cb6c64d4adfb8be156" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</target>
        </trans-unit>
        <trans-unit id="9b6b6de7677cc3f29ad4b4bb71322ca8119f722c" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</target>
        </trans-unit>
        <trans-unit id="07d469ff88f699ae509e6d44562a14e49142eb33" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; with differences only in formats of the input and output.</target>
        </trans-unit>
        <trans-unit id="f0df83d3e836abc896e3d3ef88cddff7fe41858c" translate="yes" xml:space="preserve">
          <source>This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; with differences only in formats of the input and output.</target>
        </trans-unit>
        <trans-unit id="cdf0174c78d429d9d5f3575a0eab21bb4e7cb40a" translate="yes" xml:space="preserve">
          <source>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</source>
          <target state="translated">This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</target>
        </trans-unit>
        <trans-unit id="ca8069362a7e2b0181c1b2a91e97eec6d51986a6" translate="yes" xml:space="preserve">
          <source>This method is implemented using the Singular Value Decomposition.</source>
          <target state="translated">This method is implemented using the Singular Value Decomposition.</target>
        </trans-unit>
        <trans-unit id="a51d9e13859426129477705a920dac6fd0c48f90" translate="yes" xml:space="preserve">
          <source>This method modifies the module in-place.</source>
          <target state="translated">This method modifies the module in-place.</target>
        </trans-unit>
        <trans-unit id="44d70c89b433cfca3df93bd2dca81609164685a8" translate="yes" xml:space="preserve">
          <source>This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</source>
          <target state="translated">This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</target>
        </trans-unit>
        <trans-unit id="ba184c175718a9033afc302007ca57b3b2566fe2" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; , this normalizes the result by dividing it with</target>
        </trans-unit>
        <trans-unit id="4d0394dd315af559db4d9ad47ffb52f54e5c2212" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; , this normalizes the result by dividing it with</target>
        </trans-unit>
        <trans-unit id="75583911adf2f93140c081540537180003c155f0" translate="yes" xml:space="preserve">
          <source>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; is called.</target>
        </trans-unit>
        <trans-unit id="36d7188a05dab83ac9defa225d250cd47190ba2f" translate="yes" xml:space="preserve">
          <source>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</source>
          <target state="translated">This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</target>
        </trans-unit>
        <trans-unit id="57da4e0220bd8e9709ab47673f34882d1449aae9" translate="yes" xml:space="preserve">
          <source>This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</source>
          <target state="translated">This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</target>
        </trans-unit>
        <trans-unit id="da76922486c33377aa2791af6603164e435bb6ab" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</source>
          <target state="translated">This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</target>
        </trans-unit>
        <trans-unit id="d9d848a6942846852bedaf5aea8e5e5bd3995b73" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</source>
          <target state="translated">This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</target>
        </trans-unit>
        <trans-unit id="9cf96f197fff58f45fdefadd1ad233845146b8c4" translate="yes" xml:space="preserve">
          <source>This mode should be enabled only for debugging as the different tests will slow down your program execution.</source>
          <target state="translated">이 모드는 다른 테스트로 인해 프로그램 실행 속도가 느려지므로 디버깅 용으로 만 활성화해야합니다.</target>
        </trans-unit>
        <trans-unit id="15b7320d744a4575a9e6699705096be13d718c1b" translate="yes" xml:space="preserve">
          <source>This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt;. However, corresponding parameters in different processes must have the same strides.</source>
          <target state="translated">This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt; . However, corresponding parameters in different processes must have the same strides.</target>
        </trans-unit>
        <trans-unit id="db9962266fa6ecf5ecb4337d45f21434962d46f1" translate="yes" xml:space="preserve">
          <source>This module also contains any parameters that the original module had as well.</source>
          <target state="translated">This module also contains any parameters that the original module had as well.</target>
        </trans-unit>
        <trans-unit id="5790600450f4acb47de56be8f01d6569e2e765da" translate="yes" xml:space="preserve">
          <source>This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt;, the gradient reduction on these mixed types of parameters will just work fine.</source>
          <target state="translated">This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt; , the gradient reduction on these mixed types of parameters will just work fine.</target>
        </trans-unit>
        <trans-unit id="aaad9c62985785aa5e132959be1bf299d5f15d45" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</source>
          <target state="translated">This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</target>
        </trans-unit>
        <trans-unit id="56c78b422ae4f13f13cfea0f3331ef6b5469addb" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</source>
          <target state="translated">This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</target>
        </trans-unit>
        <trans-unit id="a2fc61472d65b92ddda57c12782f0c49ee43ce07" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="51bd62541eadcc8d3cc881058d8a71b8bed71b7f" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="03c2d1d66e7773d2c26345cff96e3404e927c5ec" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="8a37a41f99ccf18be289e494dc942de371c38958" translate="yes" xml:space="preserve">
          <source>This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</target>
        </trans-unit>
        <trans-unit id="e4e13a3367c83e826e07b89f6231cff61b5213ab" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</target>
        </trans-unit>
        <trans-unit id="b724d7ccdd2d4a2fad1d30f042a783b90f3e1b70" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</source>
          <target state="translated">This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</target>
        </trans-unit>
        <trans-unit id="b31e8020fcc76734bdb0706e9d94b6fab41f09de" translate="yes" xml:space="preserve">
          <source>This module implements the combined (fused) modules conv + relu which can be then quantized.</source>
          <target state="translated">This module implements the combined (fused) modules conv + relu which can be then quantized.</target>
        </trans-unit>
        <trans-unit id="abb55caf7c23cacd70b75be7b8534a2288b0ddd0" translate="yes" xml:space="preserve">
          <source>This module implements the functions you call directly to convert your model from FP32 to quantized form. For example the &lt;a href=&quot;#torch.quantization.prepare&quot;&gt;&lt;code&gt;prepare()&lt;/code&gt;&lt;/a&gt; is used in post training quantization to prepares your model for the calibration step and &lt;a href=&quot;#torch.quantization.convert&quot;&gt;&lt;code&gt;convert()&lt;/code&gt;&lt;/a&gt; actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu.</source>
          <target state="translated">이 모듈은 모델을 FP32에서 양자화 된 형식으로 변환하기 위해 직접 호출하는 함수를 구현합니다. 예를 들어 &lt;a href=&quot;#torch.quantization.prepare&quot;&gt; &lt;code&gt;prepare()&lt;/code&gt; &lt;/a&gt; 는 보정 단계를 위해 모델을 준비하기 위해 사후 훈련 양자화에 사용되며 &lt;a href=&quot;#torch.quantization.convert&quot;&gt; &lt;code&gt;convert()&lt;/code&gt; &lt;/a&gt; 실제로 가중치를 int8로 변환하고 작업을 양자화 된 대응 항목으로 바꿉니다. 모델에 대한 입력을 양자화하고 conv + relu와 같은 중요한 융합을 수행하는 것과 같은 작업을위한 다른 도우미 함수가 있습니다.</target>
        </trans-unit>
        <trans-unit id="a5f06261e4042915626c1433e6142f8f0b72218a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized implementations of fused operations like conv + relu.</source>
          <target state="translated">This module implements the quantized implementations of fused operations like conv + relu.</target>
        </trans-unit>
        <trans-unit id="fe84855e7e341d93fbb671f10f878fe740d68b6a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt;.</source>
          <target state="translated">This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1cc16ec4495bf63da75c9357c640897793fcbd84" translate="yes" xml:space="preserve">
          <source>This module implements the versions of those fused operations needed for quantization aware training.</source>
          <target state="translated">This module implements the versions of those fused operations needed for quantization aware training.</target>
        </trans-unit>
        <trans-unit id="cd564c5f343a0b5d3e2281d9491b97ab89a390d6" translate="yes" xml:space="preserve">
          <source>This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</source>
          <target state="translated">This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</target>
        </trans-unit>
        <trans-unit id="2d28957208d801ad4a8275f10517753872c57b97" translate="yes" xml:space="preserve">
          <source>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</source>
          <target state="translated">This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</target>
        </trans-unit>
        <trans-unit id="a56a23f8f5587064fad32f9351698184c9934864" translate="yes" xml:space="preserve">
          <source>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</source>
          <target state="translated">이 모듈은 종종 단어 임베딩을 저장하고 색인을 사용하여 검색하는 데 사용됩니다. 모듈에 대한 입력은 인덱스 목록이고 출력은 해당 단어 임베딩입니다.</target>
        </trans-unit>
        <trans-unit id="8c517186ab821b45b2c9090ee2862c5050c7fd67" translate="yes" xml:space="preserve">
          <source>This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design&lt;/a&gt;.</source>
          <target state="translated">이 모듈은 모델 병렬 학습과 같은 애플리케이션에 사용할 수있는 RPC 기반 분산 autograd 프레임 워크를 제공합니다. 간단히 말해, 애플리케이션은 RPC를 통해 그래디언트 기록 텐서를 보내고받을 수 있습니다. 정방향 패스에서는 기울기 기록 텐서가 RPC를 통해 전송되는시기를 기록하고 역방향 패스 중에이 정보를 사용하여 RPC를 사용하여 분산 된 역방향 패스를 수행합니다. 자세한 내용은 &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="87af39490036eebaa3381d1990a65c31f6408fae" translate="yes" xml:space="preserve">
          <source>This module returns a &lt;code&gt;NamedTuple&lt;/code&gt; with &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;loss&lt;/code&gt; fields. See further documentation for details.</source>
          <target state="translated">이 모듈은 &lt;code&gt;output&lt;/code&gt; 및 &lt;code&gt;loss&lt;/code&gt; 필드 가있는 &lt;code&gt;NamedTuple&lt;/code&gt; 을 반환 합니다. 자세한 내용은 추가 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="59e9905faed8adfff4eeb0ecdbe57c1cac83f3a4" translate="yes" xml:space="preserve">
          <source>This module supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32를&lt;/a&gt; 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="c9c9e1a196eef3e1f967008237f4372d9edc34d7" translate="yes" xml:space="preserve">
          <source>This module works only with the multi-process, single-device usage of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, which means that a single process works on a single GPU.</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt; 의 다중 프로세스 단일 장치 사용에서만 작동합니다. 즉, 단일 프로세스가 단일 GPU에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="0f30baa501fa38b25a8509424e2193e61d72c032" translate="yes" xml:space="preserve">
          <source>This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">이 관찰자는 들어오는 텐서의 최소 및 최대 이동 평균을 기반으로 양자화 매개 변수를 계산합니다. 모듈은 들어오는 텐서의 평균 최소 및 최대를 기록하고이 통계를 사용하여 양자화 매개 변수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="29203fffadc612cba8aabcec105c43163d833d44" translate="yes" xml:space="preserve">
          <source>This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">이 관찰자는 텐서 최소 / 최대 통계를 사용하여 채널 별 양자화 매개 변수를 계산합니다. 모듈은 들어오는 텐서의 실행 최소 및 최대를 기록하고이 통계를 사용하여 양자화 매개 변수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b2adf96c5dd76cc8a637924002f78629974574c7" translate="yes" xml:space="preserve">
          <source>This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</source>
          <target state="translated">이 관찰자는 텐서 최소 / 최대 통계를 사용하여 양자화 매개 변수를 계산합니다. 모듈은 들어오는 텐서의 실행 최소 및 최대를 기록하고이 통계를 사용하여 양자화 매개 변수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="31368b5a919564a12c569127df416d6acb1973be" translate="yes" xml:space="preserve">
          <source>This op should be disambiguated with &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt; which performs a reduction on a single tensor.</source>
          <target state="translated">이 연산은 단일 텐서에서 감소를 수행하는 &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt; &lt;code&gt;torch.logsumexp()&lt;/code&gt; &lt;/a&gt; 로 명확해야합니다 .</target>
        </trans-unit>
        <trans-unit id="3256e874629b9e3bf7609aa4774907058a898427" translate="yes" xml:space="preserve">
          <source>This operation is not differentiable.</source>
          <target state="translated">이 작업은 구별 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="f253d72c955897e96cbe7e655ade3b1e375b9508" translate="yes" xml:space="preserve">
          <source>This operation is useful for explicit broadcasting by names (see examples).</source>
          <target state="translated">이 작업은 이름 별 명시 적 브로드 캐스팅에 유용합니다 (예제 참조).</target>
        </trans-unit>
        <trans-unit id="be6c030e3433204ea07b32a8e6ce714fff330e3a" translate="yes" xml:space="preserve">
          <source>This operator supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">이 연산자는 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32를&lt;/a&gt; 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="f6cbc4c392cdef9bca907c34e1833c252683a82b" translate="yes" xml:space="preserve">
          <source>This optimizer doesn&amp;rsquo;t support per-parameter options and parameter groups (there can be only one).</source>
          <target state="translated">이 옵티마이 저는 매개 변수 별 옵션 및 매개 변수 그룹을 지원하지 않습니다 (하나만있을 수 있음).</target>
        </trans-unit>
        <trans-unit id="aa1075ffa7fba27cc0d195af9a6f5444b94b97ce" translate="yes" xml:space="preserve">
          <source>This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</source>
          <target state="translated">이 패키지는 CPU 텐서와 동일한 기능을 구현하지만 계산을 위해 GPU를 사용하는 CUDA 텐서 유형에 대한 지원을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="836145aa5caebff4eaffcf9dc94e2f473ff1df15" translate="yes" xml:space="preserve">
          <source>This package provides a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects. Currently, the &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type is primarily used by the &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;.</source>
          <target state="translated">이 패키지는 비동기 실행을 캡슐화하는 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 유형과 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 개체 에 대한 작업을 단순화하는 유틸리티 함수 집합을 제공 합니다. 현재 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 유형은 주로 &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;분산 RPC 프레임 워크&lt;/a&gt; 에서 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="510d5659f81fbb2c92add3848324ac3e769287d8" translate="yes" xml:space="preserve">
          <source>This requires &lt;code&gt;scipy&lt;/code&gt; to be installed</source>
          <target state="translated">&lt;code&gt;scipy&lt;/code&gt; 를 설치 해야합니다.</target>
        </trans-unit>
        <trans-unit id="70b5dd9eee5d2e3ba91077407f335c67991b22b8" translate="yes" xml:space="preserve">
          <source>This scheduler is not chainable.</source>
          <target state="translated">이 스케줄러는 연결할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="a42db487ec40ca84c9d3c3617737b87da6c0e14d" translate="yes" xml:space="preserve">
          <source>This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.</source>
          <target state="translated">이 섹션에는 위의 기본 API를 기반으로 빌드되고 jacobians, hessians 등을 계산할 수있는 autograd 용 상위 수준 API가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="7d1f42ea5265cdd17159cef860e2867af1a5fed2" translate="yes" xml:space="preserve">
          <source>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</source>
          <target state="translated">이 섹션에서는 PyTorch 1.2의 TorchScript 변경 사항에 대해 자세히 설명합니다. TorchScript를 처음 사용하는 경우이 섹션을 건너 뛸 수 있습니다. PyTorch 1.2를 사용하는 TorchScript API에는 두 가지 주요 변경 사항이 있습니다.</target>
        </trans-unit>
        <trans-unit id="65513dd85adae4947833e9b405731c9c92f7268c" translate="yes" xml:space="preserve">
          <source>This section provides a brief overview into how different sharing strategies work. Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that&amp;rsquo;s the only way they can be shared.</source>
          <target state="translated">이 섹션에서는 다양한 공유 전략이 작동하는 방식에 대한 간략한 개요를 제공합니다. CPU 텐서에만 적용됩니다. CUDA 텐서는 공유 할 수있는 유일한 방법이므로 항상 CUDA API를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="26517401ee04a7507f48917c7c6d007aa588bc0d" translate="yes" xml:space="preserve">
          <source>This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:</source>
          <target state="translated">이 별도의 직렬화는 다중 프로세스 데이터로드를 사용하는 동안 Windows와 호환되는지 확인하기 위해 두 단계를 수행해야 함을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="4a73b78954115d8ec831d20a068a6d8ccc312011" translate="yes" xml:space="preserve">
          <source>This strategy will use file descriptors as shared memory handles. Whenever a storage is moved to shared memory, a file descriptor obtained from &lt;code&gt;shm_open&lt;/code&gt; is cached with the object, and when it&amp;rsquo;s going to be sent to other processes, the file descriptor will be transferred (e.g. via UNIX sockets) to it. The receiver will also cache the file descriptor and &lt;code&gt;mmap&lt;/code&gt; it, to obtain a shared view onto the storage data.</source>
          <target state="translated">이 전략은 파일 설명자를 공유 메모리 핸들로 사용합니다. 저장소가 공유 메모리로 이동 될 때마다 &lt;code&gt;shm_open&lt;/code&gt; 에서 얻은 파일 설명자가 객체와 함께 캐시되고 다른 프로세스로 전송 될 때 파일 설명자가 여기로 전송됩니다 (예 : UNIX 소켓을 통해). 수신자는 또한 파일 설명자를 캐시하고 &lt;code&gt;mmap&lt;/code&gt; 하여 스토리지 데이터에 대한 공유 뷰를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="18d6cf184b4d322d9e59ed200a7c995c7651fa42" translate="yes" xml:space="preserve">
          <source>This strategy will use file names given to &lt;code&gt;shm_open&lt;/code&gt; to identify the shared memory regions. This has a benefit of not requiring the implementation to cache the file descriptors obtained from it, but at the same time is prone to shared memory leaks. The file can&amp;rsquo;t be deleted right after its creation, because other processes need to access it to open their views. If the processes fatally crash, or are killed, and don&amp;rsquo;t call the storage destructors, the files will remain in the system. This is very serious, because they keep using up the memory until the system is restarted, or they&amp;rsquo;re freed manually.</source>
          <target state="translated">이 전략은 공유 메모리 영역을 식별하기 위해 &lt;code&gt;shm_open&lt;/code&gt; 에 제공된 파일 이름을 사용합니다 . 이는 구현에서 얻은 파일 설명자를 캐시 할 필요가 없다는 이점이 있지만 동시에 공유 메모리 누수가 발생하기 쉽습니다. 다른 프로세스가보기를 열려면 파일에 액세스해야하므로 파일을 만든 직후에는 삭제할 수 없습니다. 프로세스가 치명적으로 충돌하거나 종료되고 스토리지 소멸자를 호출하지 않으면 파일이 시스템에 남아 있습니다. 시스템이 다시 시작되거나 수동으로 해제 될 때까지 메모리를 계속 사용하기 때문에 이것은 매우 심각한 문제입니다.</target>
        </trans-unit>
        <trans-unit id="792e85a6083fd7bff7900d359e6ad50e4d63f9a7" translate="yes" xml:space="preserve">
          <source>This subset is restricted:</source>
          <target state="translated">이 하위 집합은 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="fabd02d487546b940bc53aa8b39e79347651d21b" translate="yes" xml:space="preserve">
          <source>This transform arises as an iterated sigmoid transform in a stick-breaking construction of the &lt;code&gt;Dirichlet&lt;/code&gt; distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.</source>
          <target state="translated">이 변환은 &lt;code&gt;Dirichlet&lt;/code&gt; 분포 의 스틱 파괴 구조에서 반복 된 시그 모이 드 변환으로 발생 합니다. 첫 번째로 짓은 시그 모이 드를 통해 첫 번째 확률과 다른 모든 확률로 변환 된 다음 프로세스가 반복됩니다.</target>
        </trans-unit>
        <trans-unit id="f610c707a33d251ea07e96805fc742b6f94fb773" translate="yes" xml:space="preserve">
          <source>This will call &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt;&lt;/a&gt; on each worker containing parameters to be optimized, and will block until all workers return. The provided &lt;code&gt;context_id&lt;/code&gt; will be used to retrieve the corresponding &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;context&lt;/code&gt;&lt;/a&gt; that contains the gradients that should be applied to the parameters.</source>
          <target state="translated">이렇게하면 최적화 할 매개 변수가 포함 된 각 워커에서 &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt; &lt;/a&gt; 이 호출 되고 모든 워커가 반환 될 때까지 차단됩니다. 제공된 &lt;code&gt;context_id&lt;/code&gt; 는 매개 변수에 적용해야하는 그라디언트를 포함 하는 해당 &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;context&lt;/code&gt; &lt;/a&gt; 를 검색하는 데 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="71b21a25c294ca711bdf2a3235c7d1f311e30f3a" translate="yes" xml:space="preserve">
          <source>This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in &lt;code&gt;backward()&lt;/code&gt;, but it&amp;rsquo;s always going to be a zero tensor with the same shape as the shape of a corresponding output.</source>
          <target state="translated">이렇게하면 출력에 기울기가 필요하지 않은 것으로 표시되어 역방향 계산의 효율성이 높아집니다. &lt;code&gt;backward()&lt;/code&gt; 각 출력에 대해 그래디언트를 허용해야 하지만 항상 해당 출력의 모양과 동일한 모양을 가진 0 텐서가됩니다.</target>
        </trans-unit>
        <trans-unit id="c51f7b72279e26fd7cf66d9d61e7f7204bf3b26a" translate="yes" xml:space="preserve">
          <source>Threshold</source>
          <target state="translated">Threshold</target>
        </trans-unit>
        <trans-unit id="4c8543e85c70d9042806658de48a8eae05e8c630" translate="yes" xml:space="preserve">
          <source>Threshold is defined as:</source>
          <target state="translated">임계 값은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a2ed3cfe5a92d0e9667b7a33a4666b01b859c7a8" translate="yes" xml:space="preserve">
          <source>Thresholds each element of the input Tensor.</source>
          <target state="translated">입력 Tensor의 각 요소를 임계 값으로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="880f2f41d4783f36e126ec98db6135c252954795" translate="yes" xml:space="preserve">
          <source>To achieve this, developers need to touch the source code of PyTorch. Please follow the &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;instructions&lt;/a&gt; for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;.</source>
          <target state="translated">이를 위해 개발자는 PyTorch의 소스 코드를 만져야합니다. 소스에서 PyTorch를 설치 하려면 &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;지침&lt;/a&gt; 을 따르십시오 . 원하는 연산자가 ONNX에서 표준화 된 경우 이러한 연산자를 내보내기위한 지원을 쉽게 추가 할 수 있습니다 (연산자에 대한 기호 기능 추가). 운영자가 표준화되었는지 여부를 확인하려면 &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX 운영자 목록을&lt;/a&gt; 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="b5aa3a35df66cb08f05248ae2964bf5546e98afe" translate="yes" xml:space="preserve">
          <source>To align a tensor to a specific order, use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">텐서를 특정 순서로 정렬하려면 &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="42a1cb8b698578c9fe48bbb02d735034e28323f6" translate="yes" xml:space="preserve">
          <source>To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of &lt;code&gt;torch.Tensor.item()&lt;/code&gt;. Torch supports implicit cast of single-element tensors to numbers. E.g.:</source>
          <target state="translated">ONNX 모델의 일부로 고정 값 상수로 가변 스칼라 텐서를 내 보내지 않으려면 &lt;code&gt;torch.Tensor.item()&lt;/code&gt; 사용을 피하십시오 . Torch는 단일 요소 텐서의 암시 적 캐스트를 숫자로 지원합니다. 예 :</target>
        </trans-unit>
        <trans-unit id="e33cac23c601d0b5ef35b2617bd1e0b84d5ac6c7" translate="yes" xml:space="preserve">
          <source>To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as well.</source>
          <target state="translated">모듈을 저장할 수 있으려면 네이티브 Python 함수를 호출하지 않아야합니다. 즉, 모든 하위 모듈도 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 의&lt;/a&gt; 하위 클래스 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="517547cd8d7cdf6793257301cea557fd580f4509" translate="yes" xml:space="preserve">
          <source>To change an existing tensor&amp;rsquo;s &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; and/or &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, consider using &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt; method on the tensor.</source>
          <target state="translated">기존 텐서의 변경 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 및 / 또는 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 을&lt;/a&gt; 사용하여 고려 &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 텐서에 방법.</target>
        </trans-unit>
        <trans-unit id="cccb194237ee7a6be8a8fca8580f1ea27dde7026" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; (and recursively compile anything it calls), add the &lt;a href=&quot;../jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator to the method. To opt out of compilation use &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;forward&lt;/code&gt; 이외의 메서드를 컴파일하려면 (그리고 호출하는 모든 것을 재귀 적으로 컴파일하려면) &lt;a href=&quot;../jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt; 데코레이터를 메서드에 추가합니다 . 컴파일을 거부하려면 &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="13132861c718e28cce4e074280b9a030df11d21a" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; that is not called from &lt;code&gt;forward&lt;/code&gt;, add &lt;code&gt;@torch.jit.export&lt;/code&gt;.</source>
          <target state="translated">이외의 방법 컴파일하려면 &lt;code&gt;forward&lt;/code&gt; 에서 호출되지 않습니다 &lt;code&gt;forward&lt;/code&gt; 추가 &lt;code&gt;@torch.jit.export&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f8e9d3b1e59225987eeef2681c83b1bcdde57d0" translate="yes" xml:space="preserve">
          <source>To compile the sources, the default system compiler (&lt;code&gt;c++&lt;/code&gt;) is used, which can be overridden by setting the &lt;code&gt;CXX&lt;/code&gt; environment variable. To pass additional arguments to the compilation process, &lt;code&gt;extra_cflags&lt;/code&gt; or &lt;code&gt;extra_ldflags&lt;/code&gt; can be provided. For example, to compile your extension with optimizations, pass &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt;. You can also use &lt;code&gt;extra_cflags&lt;/code&gt; to pass further include directories.</source>
          <target state="translated">소스를 컴파일하려면 &lt;code&gt;CXX&lt;/code&gt; 환경 변수를 설정하여 재정의 할 수있는 기본 시스템 컴파일러 ( &lt;code&gt;c++&lt;/code&gt; )가 사용됩니다 . 컴파일 프로세스에 추가 인수를 전달하기 위해 &lt;code&gt;extra_cflags&lt;/code&gt; 또는 &lt;code&gt;extra_ldflags&lt;/code&gt; 를 제공 할 수 있습니다. 예를 들어 최적화를 사용하여 확장을 컴파일하려면 &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt; 를 전달하십시오 . &lt;code&gt;extra_cflags&lt;/code&gt; 를 사용 하여 추가 포함 디렉토리를 전달할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e66cd2fec20ece4ce742fe906879dd31b4bc3e7a" translate="yes" xml:space="preserve">
          <source>To compute log-probabilities for all classes, the &lt;code&gt;log_prob&lt;/code&gt; method can be used.</source>
          <target state="translated">모든 클래스에 대한 로그 확률을 계산하려면 &lt;code&gt;log_prob&lt;/code&gt; 메서드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="77b8f89ffdd6b5d01dd6f264735e5d9852b97a53" translate="yes" xml:space="preserve">
          <source>To construct an &lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer&lt;/code&gt;&lt;/a&gt; you have to give it an iterable containing the parameters (all should be &lt;code&gt;Variable&lt;/code&gt; s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.</source>
          <target state="translated">&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer&lt;/code&gt; &lt;/a&gt; 를 구성하려면 최적화 할 매개 변수 (모두 &lt;code&gt;Variable&lt;/code&gt; 이어야 함 )를 포함하는 iterable을 제공해야합니다 . 그런 다음 학습률, 가중치 감소 등과 같은 최적화 프로그램 별 옵션을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d8b9882386102437caf895d70f81d6894ddf1097" translate="yes" xml:space="preserve">
          <source>To counter the problem of shared memory file leaks, &lt;a href=&quot;#module-torch.multiprocessing&quot;&gt;&lt;code&gt;torch.multiprocessing&lt;/code&gt;&lt;/a&gt; will spawn a daemon named &lt;code&gt;torch_shm_manager&lt;/code&gt; that will isolate itself from the current process group, and will keep track of all shared memory allocations. Once all processes connected to it exit, it will wait a moment to ensure there will be no new connections, and will iterate over all shared memory files allocated by the group. If it finds that any of them still exist, they will be deallocated. We&amp;rsquo;ve tested this method and it proved to be robust to various failures. Still, if your system has high enough limits, and &lt;code&gt;file_descriptor&lt;/code&gt; is a supported strategy, we do not recommend switching to this one.</source>
          <target state="translated">공유 메모리 파일 누출 문제를 해결하기 위해 &lt;a href=&quot;#module-torch.multiprocessing&quot;&gt; &lt;code&gt;torch.multiprocessing&lt;/code&gt; &lt;/a&gt; 은 현재 프로세스 그룹에서 자신을 분리하고 모든 공유 메모리 할당을 추적하는 &lt;code&gt;torch_shm_manager&lt;/code&gt; 라는 데몬을 생성합니다 . 연결된 모든 프로세스가 종료되면 새 연결이 없는지 확인하기 위해 잠시 기다렸다가 그룹에서 할당 한 모든 공유 메모리 파일을 반복합니다. 아직 존재하는 것이 발견되면 할당이 해제됩니다. 이 방법을 테스트 한 결과 다양한 오류에 대해 견고 함이 입증되었습니다. 그래도 시스템의 제한이 충분히 높고 &lt;code&gt;file_descriptor&lt;/code&gt; 가 지원되는 전략 인 경우이 전략으로 전환하지 않는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="bcee88acc8b939fa9c28961779b7b64da43214ae" translate="yes" xml:space="preserve">
          <source>To create a tensor with pre-existing data, use &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">기존 데이터로 텐서를 만들려면 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; 사용 하세요 .</target>
        </trans-unit>
        <trans-unit id="427cce7803261872f64a384400396e0d1af0bda9" translate="yes" xml:space="preserve">
          <source>To create a tensor with similar type but different size as another tensor, use &lt;code&gt;tensor.new_*&lt;/code&gt; creation ops.</source>
          <target state="translated">유형은 비슷하지만 다른 텐서와 크기가 다른 텐서를 만들려면 &lt;code&gt;tensor.new_*&lt;/code&gt; 생성 작업을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="ed5a59600ac4cfd9c95f09892fb740465f6d0959" translate="yes" xml:space="preserve">
          <source>To create a tensor with specific size, use &lt;code&gt;torch.*&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">특정 크기, 용도와 텐서 만들 &lt;code&gt;torch.*&lt;/code&gt; 텐서 생성 OPS (참조 &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;작성 작전을&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="a84b4146d0aefbfce9733ab7943da8a8972fc9bd" translate="yes" xml:space="preserve">
          <source>To create a tensor with the same size (and similar types) as another tensor, use &lt;code&gt;torch.*_like&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">다른 텐서와 동일한 크기 (및 유사한 유형)의 텐서를 생성 하려면 &lt;code&gt;torch.*_like&lt;/code&gt; 텐서 생성 작업을 사용합니다 ( &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="e323c9fb04b24bbe21e136c6130c8b1036dfb2e1" translate="yes" xml:space="preserve">
          <source>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (&amp;ldquo;CPU total time is much greater than CUDA total time&amp;rdquo;). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.</source>
          <target state="translated">살펴볼 (CPU 전용 모드 또는 CUDA 모드) 자동 등급 프로파일 러 출력을 결정하려면 먼저 스크립트가 CPU 바운드인지 ( &quot;CPU 총 시간이 CUDA 총 시간보다 훨씬 큽니다&quot;) 확인해야합니다. CPU 바운드 인 경우 CPU 모드 autograd 프로파일 러의 결과를 보면 도움이됩니다. 반면에 스크립트가 GPU에서 실행하는 데 대부분의 시간을 소비하는 경우 CUDA 모드 autograd 프로파일 러의 출력에서 ​​책임있는 CUDA 연산자를 찾기 시작하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1d6487001340d96dfbd24e0dfe67d91d448bd378" translate="yes" xml:space="preserve">
          <source>To enable &lt;code&gt;backend == Backend.MPI&lt;/code&gt;, PyTorch needs to be built from source on a system that supports MPI.</source>
          <target state="translated">&lt;code&gt;backend == Backend.MPI&lt;/code&gt; 를 활성화하려면 MPI를 지원하는 시스템의 소스에서 PyTorch를 빌드해야합니다.</target>
        </trans-unit>
        <trans-unit id="928a8a1dba70cf38d7b64d41e9f2de79497ca1b2" translate="yes" xml:space="preserve">
          <source>To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a &lt;code&gt;Future&lt;/code&gt; object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with &lt;code&gt;@staticmethod&lt;/code&gt; or &lt;code&gt;@classmethod&lt;/code&gt;, &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt;.</source>
          <target state="translated">비동기 실행을 사용하려면 애플리케이션이이 데코레이터가 반환 한 함수 객체를 RPC API에 전달해야합니다. RPC가이 데코레이터에 의해 설치된 속성을 감지하면이 함수가 &lt;code&gt;Future&lt;/code&gt; 객체를 반환하고 그에 따라 처리 할 것임을 알고 있습니다. 그러나 이것이 함수를 정의 할 때이 데코레이터가 가장 바깥쪽에 있어야한다는 것을 의미하지는 않습니다. 예를 들면, 함께 결합 될 때 &lt;code&gt;@staticmethod&lt;/code&gt; 또는 &lt;code&gt;@classmethod&lt;/code&gt; , &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; 필요한 목표 함수는 정적 또는 클래스 함수로서 인식 될 수 있도록 내부 장식한다. 이 대상 함수는 액세스 할 때 정적 또는 클래스 메서드가 &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; 의해 설치된 속성을 유지하기 때문에 여전히 비동기 적으로 실행할 수 있습니다..</target>
        </trans-unit>
        <trans-unit id="16234789078f98ba40aad60005ffa95f2e9e8ac6" translate="yes" xml:space="preserve">
          <source>To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.</source>
          <target state="translated">올바른 수의 스레드가 사용되는지 확인하려면 eager, JIT 또는 autograd 코드를 실행하기 전에 set_num_threads를 호출해야합니다.</target>
        </trans-unit>
        <trans-unit id="2cd4366390e742467d09cd376578959df8585e90" translate="yes" xml:space="preserve">
          <source>To export a raw ir.</source>
          <target state="translated">원시 IR을 내보내려면.</target>
        </trans-unit>
        <trans-unit id="13a45cf8b5a671e88848c78052e09b8b9f6c109f" translate="yes" xml:space="preserve">
          <source>To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.</source>
          <target state="translated">ONNX에서 지원되지 않는 ATen 연산자를 대체합니다. 지원되는 연산자는 정기적으로 ONNX로 내보내집니다. 다음 예에서 aten :: triu는 ONNX에서 지원되지 않습니다. 내보내기는이 연산자를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="64ba89c98e2ef72ba75fa028267bbaa3251bdd0e" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a complex data type, the property &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt;&lt;code&gt;is_complex&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a complex data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 이 복합 데이터 유형 인지 확인하기 위해 is_complex 속성을 &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt; &lt;code&gt;is_complex&lt;/code&gt; &lt;/a&gt; 수 있습니다. 이 속성 은 데이터 유형이 복합 데이터 유형 인 경우 &lt;code&gt;True&lt;/code&gt; 를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="4bde254844ec10d73414a4f9097cbf8e8d5d1c09" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a floating point data type, the property &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt;&lt;code&gt;is_floating_point&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a floating point data type.</source>
          <target state="translated">있는지 확인하려면 &lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 은&lt;/a&gt; 부동 소수점 데이터 유형, 속성 &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt; &lt;code&gt;is_floating_point&lt;/code&gt; 은&lt;/a&gt; 반환, 사용할 수있는 &lt;code&gt;True&lt;/code&gt; 데이터 유형이 부동 소수점 데이터 유형 인 경우.</target>
        </trans-unit>
        <trans-unit id="48e44e1c5bae8e339aa29cc00850c62cff2a0ea7" translate="yes" xml:space="preserve">
          <source>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It&amp;rsquo;s also helpful to include a minimal working example.</source>
          <target state="translated">사용자가 문서를 앞뒤로 참조하지 않고 탐색 할 수 있도록 repo 소유자가 기능 도움말 메시지를 명확하고 간결하게 만드는 것이 좋습니다. 최소한의 작업 예제를 포함하는 것도 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="e334bd6959cf3a682b73059d2817327b2e249b8f" translate="yes" xml:space="preserve">
          <source>To iterate over the full Cartesian product use &lt;code&gt;itertools.product(m.enumerate_support())&lt;/code&gt;.</source>
          <target state="translated">전체 데카르트 곱을 반복하려면 &lt;code&gt;itertools.product(m.enumerate_support())&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6e370e4d333aa52e5158136385c8ce683abfc7e1" translate="yes" xml:space="preserve">
          <source>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</source>
          <target state="translated">확장을로드하기 위해 주어진 소스를 동적 라이브러리로 컴파일하는 데 사용되는 Ninja 빌드 파일이 생성됩니다. 이 라이브러리는 이후에 현재 Python 프로세스에 모듈로로드되고 사용할 준비가 된이 함수에서 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="0f1500bbb514e4eeb3d360549b7085361d0903f1" translate="yes" xml:space="preserve">
          <source>To look up what optional arguments this module offers:</source>
          <target state="translated">이 모듈이 제공하는 선택적 인수를 찾으려면 다음을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="efe5a1ff1c27c51217ed9e88d6d14c167e44ae98" translate="yes" xml:space="preserve">
          <source>To make it easier to understand, here is a small example:</source>
          <target state="translated">이해하기 쉽도록 다음은 작은 예입니다.</target>
        </trans-unit>
        <trans-unit id="4861c71c5547ba6da8095cee191825540e015436" translate="yes" xml:space="preserve">
          <source>To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to &lt;code&gt;torch&lt;/code&gt;, the TorchScript compiler is actually resolving it to the &lt;code&gt;torch&lt;/code&gt; Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.</source>
          <target state="translated">TorchScript 작성을보다 편리하게하기 위해 스크립트 코드가 주변 범위의 Python 값을 참조하도록 허용합니다. 예를 들어, &lt;code&gt;torch&lt;/code&gt; 에 대한 참조가있을 때마다 TorchScript 컴파일러는 함수가 선언 될 때 실제로이를 &lt;code&gt;torch&lt;/code&gt; Python 모듈로 해석합니다 . 이러한 Python 값은 TorchScript의 첫 번째 클래스 부분이 아닙니다. 대신 컴파일 타임에 TorchScript가 지원하는 기본 유형으로 de-sugared됩니다. 이것은 컴파일이 발생할 때 참조되는 Python 값의 동적 유형에 따라 다릅니다. 이 섹션에서는 TorchScript에서 Python 값에 액세스 할 때 사용되는 규칙에 대해 설명합니다.</target>
        </trans-unit>
        <trans-unit id="56b4b291b8193f650ab440b4d0108041657026e3" translate="yes" xml:space="preserve">
          <source>To obtain repeatable results, reset the seed for the pseudorandom number generator</source>
          <target state="translated">반복 가능한 결과를 얻으려면 의사 난수 생성기의 시드를 재설정하십시오.</target>
        </trans-unit>
        <trans-unit id="693cb0335aff5d84a8ad039de675ef1c71894064" translate="yes" xml:space="preserve">
          <source>To prevent underflow, &amp;ldquo;gradient scaling&amp;rdquo; multiplies the network&amp;rsquo;s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don&amp;rsquo;t flush to zero.</source>
          <target state="translated">언더 플로를 방지하기 위해 &quot;그라디언트 스케일링&quot;은 네트워크의 손실에 스케일 팩터를 곱하고 스케일링 된 손실에 대해 역방향 패스를 호출합니다. 그런 다음 네트워크를 통해 역방향으로 흐르는 그라디언트는 동일한 요소로 확장됩니다. 즉, 그래디언트 값은 크기가 더 크므로 0으로 플러시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="cdaaaf6ab8e173c9d7ceb41b8dea6cb46e09c870" translate="yes" xml:space="preserve">
          <source>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</source>
          <target state="translated">사용자 정의 된 추가 정보를 인쇄하려면 자체 모듈에서이 방법을 다시 구현해야합니다. 한 줄 및 여러 줄 문자열이 모두 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="ff43c0b33f5574b95fded09d54e409788745cf85" translate="yes" xml:space="preserve">
          <source>To run the exported script with &lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;, you will need to install &lt;code&gt;caffe2&lt;/code&gt;: If you don&amp;rsquo;t have one already, Please &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;follow the install instructions&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2로&lt;/a&gt; 내 보낸 스크립트를 실행 하려면 &lt;code&gt;caffe2&lt;/code&gt; 를 설치해야합니다. 아직 설치 하지 않은 경우 &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;설치 지침을 따르십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c62b3cc0b531750df7ee34e20beaf808665f78f8" translate="yes" xml:space="preserve">
          <source>To specify the scale, it takes either the &lt;code&gt;size&lt;/code&gt; or the &lt;code&gt;scale_factor&lt;/code&gt; as it&amp;rsquo;s constructor argument.</source>
          <target state="translated">스케일을 지정하려면 생성자 인수로 &lt;code&gt;size&lt;/code&gt; 또는 &lt;code&gt;scale_factor&lt;/code&gt; 를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="8bc8fb9e3000235214e485d36fb9186012476b04" translate="yes" xml:space="preserve">
          <source>To stop the compiler from compiling a method, add &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;@ignore&lt;/code&gt; leaves the</source>
          <target state="translated">컴파일러가 메소드 컴파일을 중지하려면 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt; 추가하십시오 . &lt;code&gt;@ignore&lt;/code&gt; 잎은</target>
        </trans-unit>
        <trans-unit id="83e075942db53d26dad4b0cea801b6df02d892d5" translate="yes" xml:space="preserve">
          <source>To take a batch diagonal, pass in dim1=-2, dim2=-1.</source>
          <target state="translated">배치 대각선을 취하려면 dim1 = -2, dim2 = -1을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="06d7e723aad8726de16a32b689408fd5f6a39bff" translate="yes" xml:space="preserve">
          <source>To trace a specific method on a module, see &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace_module&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">모듈에서 특정 메소드를 추적하려면 &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt; &lt;code&gt;torch.jit.trace_module&lt;/code&gt; 을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="513b8c269a49217eb8a34177b8e8cfe4d36acacc" translate="yes" xml:space="preserve">
          <source>To use &lt;a href=&quot;#module-torch.optim&quot;&gt;&lt;code&gt;torch.optim&lt;/code&gt;&lt;/a&gt; you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</source>
          <target state="translated">&lt;a href=&quot;#module-torch.optim&quot;&gt; &lt;code&gt;torch.optim&lt;/code&gt; &lt;/a&gt; 을 사용하려면 현재 상태를 유지하고 계산 된 그래디언트를 기반으로 매개 변수를 업데이트하는 옵티 마이저 객체를 생성해야합니다.</target>
        </trans-unit>
        <trans-unit id="7cbbf69a127d5627b468752b233e45c932bef649" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;DistributedDataParallel&lt;/code&gt; on a host with N GPUs, you should spawn up &lt;code&gt;N&lt;/code&gt; processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; for every process or by calling:</source>
          <target state="translated">N 개의 GPU가있는 호스트에서 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 사용하려면 &lt;code&gt;N&lt;/code&gt; 개의 프로세스를 생성하여 각 프로세스가 0에서 N-1까지 단일 GPU에서 독점적으로 작동하도록해야합니다. 이는 모든 프로세스에 대해 &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; 를 설정 하거나 다음을 호출 하여 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="55e1a9559362bb4b36c706d26afd5c7744bfa4b4" translate="yes" xml:space="preserve">
          <source>To use a &lt;code&gt;nn.ModuleList&lt;/code&gt; inside a compiled method, it must be marked constant by adding the name of the attribute to the &lt;code&gt;__constants__&lt;/code&gt; list for the type. For loops over a &lt;code&gt;nn.ModuleList&lt;/code&gt; will unroll the body of the loop at compile time, with each member of the constant module list.</source>
          <target state="translated">컴파일 된 메서드 내 에서 &lt;code&gt;nn.ModuleList&lt;/code&gt; 를 사용하려면 해당 유형 의 &lt;code&gt;__constants__&lt;/code&gt; 목록에 속성 이름을 추가하여 상수로 표시해야합니다 . &lt;code&gt;nn.ModuleList&lt;/code&gt; 에 대한 For 루프 는 컴파일 타임에 상수 모듈 목록의 각 멤버와 함께 루프 본문을 펼 칩니다 .</target>
        </trans-unit>
        <trans-unit id="527375a7ef1d0e262542dedd5c0253dc05250f0d" translate="yes" xml:space="preserve">
          <source>To use these functions the torch.fft module must be imported since its name conflicts with the &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">이러한 함수를 사용하려면 이름이 &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt; 함수 와 충돌하므로 torch.fft 모듈을 가져와야 합니다.</target>
        </trans-unit>
        <trans-unit id="43427ba5c0a00d9ec53c4149e3f5de362381fec9" translate="yes" xml:space="preserve">
          <source>To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.</source>
          <target state="translated">이를 사용하여 프로세스 전반에 걸쳐 입력이 고르지 않은 학습을 가능하게하려면이 컨텍스트 관리자를 학습 루프에 감싸면됩니다. 모델이나 데이터로드에 대한 추가 수정이 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6239832323d9bb66e30c43f099010c4cf674fbce" translate="yes" xml:space="preserve">
          <source>To utilize &lt;em&gt;script-based&lt;/em&gt; exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:</source>
          <target state="translated">동적 루프를 캡처 하기 위해 &lt;em&gt;스크립트 기반&lt;/em&gt; 내보내기 를 사용하려면 &lt;em&gt;스크립트&lt;/em&gt; 에 루프를 작성하고 일반 nn.Module에서 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="111ccaae56ca75ec4b6793420aa1331efeccd7da" translate="yes" xml:space="preserve">
          <source>Top-1 error</source>
          <target state="translated">Top-1 오류</target>
        </trans-unit>
        <trans-unit id="f1240994c38def01302ee353c1f3e6ab783c6922" translate="yes" xml:space="preserve">
          <source>Top-5 error</source>
          <target state="translated">Top-5 오류</target>
        </trans-unit>
        <trans-unit id="ce0f1f9bf9ba00bef8a66791462d63e5a977f58e" translate="yes" xml:space="preserve">
          <source>Top-level quantization APIs</source>
          <target state="translated">최상위 양자화 API</target>
        </trans-unit>
        <trans-unit id="ecebe7e89f3537ec9d48a23fefb140aef1ac7e73" translate="yes" xml:space="preserve">
          <source>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</source>
          <target state="translated">Torch는 다음과 같은 CPU 및 GPU 변형으로 10 개의 텐서 유형을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="2270c240add89c3f53429a527826adf7f8c68848" translate="yes" xml:space="preserve">
          <source>Torch hub works by importing the package as if it was installed. There&amp;rsquo;re some side effects introduced by importing in Python. For example, you can see new items in Python caches &lt;code&gt;sys.modules&lt;/code&gt; and &lt;code&gt;sys.path_importer_cache&lt;/code&gt; which is normal Python behavior.</source>
          <target state="translated">토치 허브는 설치된 것처럼 패키지를 가져 와서 작동합니다. Python으로 가져 오면 몇 가지 부작용이 발생합니다. 예를 들어, Python에서 정상적인 Python 동작 인 &lt;code&gt;sys.modules&lt;/code&gt; 및 &lt;code&gt;sys.path_importer_cache&lt;/code&gt; 를 캐시하는 새 항목을 볼 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e678d1ba0a8c71917974dd6b809d475b07d566ed" translate="yes" xml:space="preserve">
          <source>Torch mobile supports &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blacklisting optimization set and a preserved method list</source>
          <target state="translated">Torch 모바일은 평가 모드에서 모듈로 최적화 패스 목록을 실행하기 위해 &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; 유틸리티를 지원 합니다. 이 메서드는 torch.jit.ScriptModule 객체, 블랙리스트 최적화 세트 및 보존 된 메서드 목록과 같은 매개 변수를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="03340697e5da4f35bf9335b934c01e6bb269d07d" translate="yes" xml:space="preserve">
          <source>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</source>
          <target state="translated">Torch는 COO (rdinate) 형식의 희소 텐서를 지원하므로 대부분의 요소가 0 인 텐서를 효율적으로 저장하고 처리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="efa8e2bf58145beaa70cc1139b800b0cc73441ae" translate="yes" xml:space="preserve">
          <source>TorchElastic</source>
          <target state="translated">TorchElastic</target>
        </trans-unit>
        <trans-unit id="3b90040dd3c7ea550e3ae7ebb31cbbf38c50d775" translate="yes" xml:space="preserve">
          <source>TorchScript</source>
          <target state="translated">TorchScript</target>
        </trans-unit>
        <trans-unit id="604c8e5563e16356b9e1a864727a3495aaa4b8f6" translate="yes" xml:space="preserve">
          <source>TorchScript Classes</source>
          <target state="translated">TorchScript 클래스</target>
        </trans-unit>
        <trans-unit id="5d73d728706f6b5bfb8bc6488f8b896532317b47" translate="yes" xml:space="preserve">
          <source>TorchScript Enums</source>
          <target state="translated">TorchScript 열거 형</target>
        </trans-unit>
        <trans-unit id="2d3d498afcd6dbfb2767a5e73bfdf3c641826801" translate="yes" xml:space="preserve">
          <source>TorchScript Language</source>
          <target state="translated">TorchScript 언어</target>
        </trans-unit>
        <trans-unit id="655dca0ad6a50c42b950785617112aa26c2b7ea8" translate="yes" xml:space="preserve">
          <source>TorchScript Language Reference</source>
          <target state="translated">TorchScript 언어 참조</target>
        </trans-unit>
        <trans-unit id="4ca55dbce27b090f25e1beef5f42bc0b74a3e196" translate="yes" xml:space="preserve">
          <source>TorchScript Unsupported Pytorch Constructs</source>
          <target state="translated">TorchScript 지원되지 않는 Pytorch 구성</target>
        </trans-unit>
        <trans-unit id="f37751da08a275fc84f6719af858bb6fca4efcaa" translate="yes" xml:space="preserve">
          <source>TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</source>
          <target state="translated">TorchScript는 또한 IR 그래프의 형태로 코드 프리티 프린터보다 낮은 수준의 표현을 가지고 있습니다.</target>
        </trans-unit>
        <trans-unit id="1f6a6dd855f78002eaa186408d37b5a14d38fb6e" translate="yes" xml:space="preserve">
          <source>TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.</source>
          <target state="translated">TorchScript는 또한 Python에 정의 된 상수를 사용하는 방법을 제공합니다. 이는 하이퍼 매개 변수를 함수에 하드 코딩하거나 범용 상수를 정의하는 데 사용할 수 있습니다. Python 값을 상수로 처리하도록 지정하는 방법에는 두 가지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="252b708f15ebb50ba55ac0b5f321607abb84f702" translate="yes" xml:space="preserve">
          <source>TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.</source>
          <target state="translated">TorchScript는 Python 함수를 호출 할 수 있습니다. 이 기능은 모델을 TorchScript로 점진적으로 변환 할 때 매우 유용합니다. 모델은 기능별로 TorchScript로 이동하여 Python 함수에 대한 호출을 그대로 둘 수 있습니다. 이렇게하면 진행하면서 모델의 정확성을 점진적으로 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e634a60c8e32dbe73734d8f74e5fc62a8bbd892" translate="yes" xml:space="preserve">
          <source>TorchScript can lookup attributes on modules. &lt;code&gt;Builtin functions&lt;/code&gt; like &lt;code&gt;torch.add&lt;/code&gt; are accessed this way. This allows TorchScript to call functions defined in other modules.</source>
          <target state="translated">TorchScript는 모듈의 속성을 조회 할 수 있습니다. &lt;code&gt;torch.add&lt;/code&gt; 와 같은 &lt;code&gt;Builtin functions&lt;/code&gt; 는 이런 방식으로 액세스됩니다. 이를 통해 TorchScript는 다른 모듈에 정의 된 함수를 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="05a14206c4e373e5e12c57e024877cf642631ecf" translate="yes" xml:space="preserve">
          <source>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a &lt;code&gt;NamedTuple&lt;/code&gt; with methods attached).</source>
          <target state="translated">TorchScript 클래스 지원은 실험적입니다. 현재는 간단한 레코드와 유사한 유형에 가장 적합합니다 ( 메서드가 첨부 된 &lt;code&gt;NamedTuple&lt;/code&gt; 을 생각해 보십시오 ).</target>
        </trans-unit>
        <trans-unit id="40d12a53eedc122f7c1c26cc26562fe87abc6002" translate="yes" xml:space="preserve">
          <source>TorchScript classes are statically typed. Members can only be declared by assigning to self in the &lt;code&gt;__init__()&lt;/code&gt; method.</source>
          <target state="translated">TorchScript 클래스는 정적으로 형식화됩니다. 멤버는 &lt;code&gt;__init__()&lt;/code&gt; 메서드 에서 self에 할당해야만 선언 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6fd4fa394dbd8f52548eb83a0ab865caf3d46d44" translate="yes" xml:space="preserve">
          <source>TorchScript does not support &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt; so this type is not used</source>
          <target state="translated">TorchScript는 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt; &lt;code&gt;bytes&lt;/code&gt; &lt;/a&gt; 지원하지 않으므로이 유형은 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="95d83676fd42ab283c45a040ec05adf9e17c4b2a" translate="yes" xml:space="preserve">
          <source>TorchScript does not support all features and types of the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority.</source>
          <target state="translated">TorchScript는 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; 모듈 의 모든 기능과 유형을 지원하지 않습니다 . 이들 중 일부는 미래에 추가 될 가능성이 낮은보다 근본적인 것들이며, 우선 순위로 할 충분한 사용자 요구가있는 경우 다른 것들이 추가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="155bc1507dbbed5e44c922d48bd933f969683779" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python that can either be written directly (using the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code.</source>
          <target state="translated">TorchScript는 직접 작성 ( &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 데코레이터 사용)하거나 추적을 통해 Python 코드에서 자동으로 생성 할 수있는 정적으로 형식화 된 Python의 하위 집합입니다 . 추적을 사용할 때 코드는 텐서에 실제 연산자 만 기록하고 단순히 주변의 다른 Python 코드를 실행하고 삭제하여 Python의이 하위 집합으로 자동 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="6e3be4ae1ba105244947e696b16b923455648d05" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt; for details.</source>
          <target state="translated">TorchScript는 정적으로 형식화 된 Python의 하위 집합이므로 많은 Python 기능이 TorchScript에 직접 적용됩니다. 자세한 내용은 전체 &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript 언어 참조&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b80f73ada7844bf2ed31f70d01150d3e0f521094" translate="yes" xml:space="preserve">
          <source>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</source>
          <target state="translated">TorchScript는 PyTorch 코드에서 직렬화 및 최적화 가능한 모델을 생성하는 방법입니다. 모든 TorchScript 프로그램은 Python 프로세스에서 저장하고 Python 종속성이없는 프로세스에서로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f3730c4db0b9ee76825d3365cc7e6ee5d81c09f3" translate="yes" xml:space="preserve">
          <source>TorchScript provides a code pretty-printer for all &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; instances. This pretty-printer gives an interpretation of the script method&amp;rsquo;s code as valid Python syntax. For example:</source>
          <target state="translated">TorchScript는 모든 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 인스턴스에 대한 코드 프리티 프린터를 제공 합니다. 이 예쁜 프린터는 스크립트 메서드의 코드를 유효한 Python 구문으로 해석합니다. 예를 들면 :</target>
        </trans-unit>
        <trans-unit id="6f800d70869c5af16f03f352efad0babdc4a4cee" translate="yes" xml:space="preserve">
          <source>TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, &lt;code&gt;torch.distributed.rpc&lt;/code&gt; supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.</source>
          <target state="translated">RPC의 TorchScript 지원은 프로토 타입 기능이며 변경 될 수 있습니다. v1.5.0부터 &lt;code&gt;torch.distributed.rpc&lt;/code&gt; 는 TorchScript 함수를 RPC 대상 함수로 호출하는 것을 지원하며 TorchScript 함수를 실행하는 데 GIL이 필요하지 않으므로 호출 수신자 측에서 병렬 처리를 개선하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="7d483bee6192edebb2157f5cffc7ebe0cf7bf9b5" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of Python&amp;rsquo;s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement.</source>
          <target state="translated">TorchScript는 Python의 변수 확인 (예 : 범위 지정) 규칙의 하위 집합을 지원합니다. 지역 변수는 변수가 함수를 통한 모든 경로에서 동일한 유형을 가져야한다는 제한을 제외하고 Python에서와 동일하게 작동합니다. 변수가 if 문의 다른 분기에서 다른 유형을 갖는 경우 if 문의 종료 후 변수를 사용하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="439e6a570c3e92811a193d2bb2eb0c532aac85c2" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the &lt;code&gt;torch&lt;/code&gt; namespace, all functions in &lt;code&gt;torch.nn.functional&lt;/code&gt; and most modules from &lt;code&gt;torch.nn&lt;/code&gt; are supported in TorchScript.</source>
          <target state="translated">TorchScript는 PyTorch가 제공하는 텐서 및 신경망 기능의 하위 집합을 지원합니다. Tensor의 대부분의 메서드와 &lt;code&gt;torch&lt;/code&gt; 네임 스페이스의 함수, &lt;code&gt;torch.nn.functional&lt;/code&gt; 의 모든 함수 및 torch.nn의 대부분의 모듈이 &lt;code&gt;torch.nn&lt;/code&gt; 에서 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="85815975862ed9b632ab9ab2cb6516181e624194" translate="yes" xml:space="preserve">
          <source>TorchScript supports the following types of statements:</source>
          <target state="translated">TorchScript는 다음 유형의 문을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="237a371f4a7260a3005de6488e3d8470546b5ec9" translate="yes" xml:space="preserve">
          <source>TorchScript supports the use of most PyTorch functions and many Python built-ins. See &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; for a full reference of supported functions.</source>
          <target state="translated">TorchScript는 대부분의 PyTorch 함수와 많은 Python 내장 기능의 사용을 지원합니다. 지원되는 함수에 대한 전체 참조는 &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="f626090730ab6e08eee60582294fe0a9f90efe3d" translate="yes" xml:space="preserve">
          <source>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</source>
          <target state="translated">TorchScript는 계산을 나타 내기 위해 정적 단일 할당 (SSA) 중간 표현 (IR)을 사용합니다. 이 형식의 명령어는 ATen (PyTorch의 C ++ 백엔드) 연산자와 루프 및 조건에 대한 제어 흐름 연산자를 포함한 기타 기본 연산자로 구성됩니다. 예로서:</target>
        </trans-unit>
        <trans-unit id="ee07cfab05a06e606f9327817d635689c5cd9b5d" translate="yes" xml:space="preserve">
          <source>TorchScript will refine the type of a variable of type &lt;code&gt;Optional[T]&lt;/code&gt; when a comparison to &lt;code&gt;None&lt;/code&gt; is made inside the conditional of an if-statement or checked in an &lt;code&gt;assert&lt;/code&gt;. The compiler can reason about multiple &lt;code&gt;None&lt;/code&gt; checks that are combined with &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, and &lt;code&gt;not&lt;/code&gt;. Refinement will also occur for else blocks of if-statements that are not explicitly written.</source>
          <target state="translated">TorchScript는 if- 문의 조건부 내에서 &lt;code&gt;None&lt;/code&gt; 에 대한 비교 가 이루어 지거나 &lt;code&gt;assert&lt;/code&gt; 에서 체크인 될 때 &lt;code&gt;Optional[T]&lt;/code&gt; 유형의 변수 유형을 구체화합니다 . 컴파일러는 &lt;code&gt;and&lt;/code&gt; , &lt;code&gt;or&lt;/code&gt; , &lt;code&gt;not&lt;/code&gt; 과 결합 된 여러 &lt;code&gt;None&lt;/code&gt; 검사 에 대해 추론 할 수 있습니다 . 명시 적으로 작성되지 않은 if 문의 else 블록에 대해서도 구체화가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="db3578de30ac281698832393e03259941ecc59a4" translate="yes" xml:space="preserve">
          <source>TorchServe</source>
          <target state="translated">TorchServe</target>
        </trans-unit>
        <trans-unit id="174786ece7d01b50c5a292bbe7c461a354fd217b" translate="yes" xml:space="preserve">
          <source>TorchVision support</source>
          <target state="translated">TorchVision 지원</target>
        </trans-unit>
        <trans-unit id="eb01e02a87150ef514202bea3337f1bbd7e6a429" translate="yes" xml:space="preserve">
          <source>Total norm of the parameters (viewed as a single vector).</source>
          <target state="translated">매개 변수의 총 노름 (단일 벡터로 표시됨).</target>
        </trans-unit>
        <trans-unit id="117b4c950645374477e1f2b2a96517111745fe19" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">함수를 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 파일 또는 &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="4cae3f60263770c81b066cebd1a77d6ae0c2a925" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on &lt;code&gt;Tensor&lt;/code&gt;s and lists, dictionaries, and tuples of &lt;code&gt;Tensor&lt;/code&gt;s.</source>
          <target state="translated">함수를 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 파일 또는 &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 을 반환합니다 . 추적은 &lt;code&gt;Tensor&lt;/code&gt; 및 목록, 사전 및 &lt;code&gt;Tensor&lt;/code&gt; 튜플 에서만 작동하는 코드에 이상적입니다 .</target>
        </trans-unit>
        <trans-unit id="20f26cd87e3024b30b8eeb8b74edbe9cdd81f4b6" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">모듈을 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 가능한 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="3bad49a27d3b91711b9d48661ccd1bb2133d17d3" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. When a module is passed to &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced. With &lt;code&gt;trace_module&lt;/code&gt;, you can specify a dictionary of method names to example inputs to trace (see the &lt;code&gt;inputs&lt;/code&gt;) argument below.</source>
          <target state="translated">모듈을 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 가능한 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 을 반환합니다 . 모듈이 &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; 에&lt;/a&gt; 전달 되면 &lt;code&gt;forward&lt;/code&gt; 메소드 만 실행되고 추적됩니다. &lt;code&gt;trace_module&lt;/code&gt; 을 사용하면 아래의 인수 를 추적 할 ( &lt;code&gt;inputs&lt;/code&gt; 참조) 예제 입력에 메서드 이름 사전을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d93ef3f27cd83786d560cf0e7159be09c5658df2" translate="yes" xml:space="preserve">
          <source>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</source>
          <target state="translated">추적 된 함수는 스크립트 함수를 호출 할 수 있습니다. 이는 대부분의 모델이 피드 포워드 네트워크 일지라도 모델의 작은 부분에 약간의 제어 흐름이 필요할 때 유용합니다. 추적 된 함수에 의해 호출 된 스크립트 함수 내부의 제어 흐름이 올바르게 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="0fce73bbb5357ac93c61680e5f58be84db277c7b" translate="yes" xml:space="preserve">
          <source>Tracer</source>
          <target state="translated">Tracer</target>
        </trans-unit>
        <trans-unit id="d09bb53da249750b0b3b27f27eabe9dbddadc0ed" translate="yes" xml:space="preserve">
          <source>Tracer Warnings</source>
          <target state="translated">추적자 경고</target>
        </trans-unit>
        <trans-unit id="bf933c3083c5a6cb4cedac1b1e0c22d6553dab1d" translate="yes" xml:space="preserve">
          <source>Tracing Edge Cases</source>
          <target state="translated">가장자리 케이스 추적</target>
        </trans-unit>
        <trans-unit id="c5153b07a6f011eb8b3585b8b94ddfea62a7a0ce" translate="yes" xml:space="preserve">
          <source>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</source>
          <target state="translated">입력에 의존하는 제어 흐름 추적 (예 : 텐서 모양)</target>
        </trans-unit>
        <trans-unit id="656eeb39a9810745d15b70b8f4cca3cf416cf8d6" translate="yes" xml:space="preserve">
          <source>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</source>
          <target state="translated">텐서 뷰의 내부 작업 추적 (예 : 할당의 왼쪽에 인덱싱)</target>
        </trans-unit>
        <trans-unit id="e793ff03e25bf5dd4f85e642caecdeee487a3a73" translate="yes" xml:space="preserve">
          <source>Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned &lt;code&gt;ScriptModule&lt;/code&gt; will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,</source>
          <target state="translated">추적은 데이터에 종속되지 않고 (예 : 텐서의 데이터에 대한 조건이 없음) 추적되지 않은 외부 종속성 (예 : 입력 / 출력 수행 또는 전역 변수 액세스)이없는 함수 및 모듈 만 올바르게 기록합니다. 추적은 주어진 텐서에서 주어진 함수가 실행될 때 수행 된 작업 만 기록합니다. 따라서 반환 된 &lt;code&gt;ScriptModule&lt;/code&gt; 은 모든 입력에서 항상 동일한 추적 그래프를 실행합니다. 이는 모듈이 입력 및 / 또는 모듈 상태에 따라 다른 작업 집합을 실행할 것으로 예상되는 경우 몇 가지 중요한 의미를 갖습니다. 예를 들면</target>
        </trans-unit>
        <trans-unit id="1289e51c253ed91b441a90b66b33a5a7cc2af8cf" translate="yes" xml:space="preserve">
          <source>Tracing vs Scripting</source>
          <target state="translated">추적 vs 스크립팅</target>
        </trans-unit>
        <trans-unit id="4de8ec680853c82acd4d7ddcd35ef7c092a558c3" translate="yes" xml:space="preserve">
          <source>Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.</source>
          <target state="translated">추적은 if 문이나 루프와 같은 제어 흐름을 기록하지 않습니다. 이 제어 흐름이 모듈 전체에서 일정하면 문제가 없으며 제어 흐름 결정을 인라인하는 경우가 많습니다. 그러나 때때로 제어 흐름은 실제로 모델 자체의 일부입니다. 예를 들어, 순환 네트워크는 입력 시퀀스의 (동적 일 가능성이있는) 길이에 대한 루프입니다.</target>
        </trans-unit>
        <trans-unit id="b6fe7f5e79177b05f6d251ecb9c162d45455045d" translate="yes" xml:space="preserve">
          <source>Training</source>
          <target state="translated">Training</target>
        </trans-unit>
        <trans-unit id="9799ca804afd728925d42f0fad8bc4ebafb48be1" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.</source>
          <target state="translated">제약이없는 행렬에서 음이 아닌 대각선 항목이있는 하위 삼각형 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="2028c39f00230fbfeeb2cd068176cf529f49e2ed" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.</source>
          <target state="translated">구속되지 않은 공간에서 스틱 브레이킹 프로세스를 통해 하나의 추가 차원의 심플 렉스로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="69c694e67b533e546676316ef1f032d5873aac31" translate="yes" xml:space="preserve">
          <source>Transform from unconstrained space to the simplex via</source>
          <target state="translated">제한되지 않은 공간에서 심플 렉스로 변환</target>
        </trans-unit>
        <trans-unit id="8e9519c4f24f7581be45723ce541214099b89154" translate="yes" xml:space="preserve">
          <source>Transform functor that applies a sequence of transforms &lt;code&gt;tseq&lt;/code&gt; component-wise to each submatrix at &lt;code&gt;dim&lt;/code&gt; in a way compatible with &lt;a href=&quot;generated/torch.stack#torch.stack&quot;&gt;&lt;code&gt;torch.stack()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.stack#torch.stack&quot;&gt; &lt;code&gt;torch.stack()&lt;/code&gt; &lt;/a&gt; 과 호환되는 방식으로 &lt;code&gt;dim&lt;/code&gt; 에서 각 부분 행렬에 구성 요소 &lt;code&gt;tseq&lt;/code&gt; 일련의 tseq 변환을 적용하는 변환 펑터 .</target>
        </trans-unit>
        <trans-unit id="7f966e79d9dc111e8433e53a94b6d1fe7a4bd63c" translate="yes" xml:space="preserve">
          <source>Transform functor that applies a sequence of transforms &lt;code&gt;tseq&lt;/code&gt; component-wise to each submatrix at &lt;code&gt;dim&lt;/code&gt;, of length &lt;code&gt;lengths[dim]&lt;/code&gt;, in a way compatible with &lt;a href=&quot;generated/torch.cat#torch.cat&quot;&gt;&lt;code&gt;torch.cat()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">변환 시퀀스가 적용되는 펑 변환 &lt;code&gt;tseq&lt;/code&gt; 각 행렬 요소로 와이즈 &lt;code&gt;dim&lt;/code&gt; 길이의, &lt;code&gt;lengths[dim]&lt;/code&gt; 와 호환하는 방식으로, &lt;a href=&quot;generated/torch.cat#torch.cat&quot;&gt; &lt;code&gt;torch.cat()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bd1a4c11c8705b74a20ba32754ab9f1d32f26cf3" translate="yes" xml:space="preserve">
          <source>Transform via the mapping</source>
          <target state="translated">매핑을 통해 변환</target>
        </trans-unit>
        <trans-unit id="186888187095080b876d7340f9103c9cc0aaf2ee" translate="yes" xml:space="preserve">
          <source>Transform via the pointwise affine mapping</source>
          <target state="translated">점별 아핀 매핑을 통해 변환</target>
        </trans-unit>
        <trans-unit id="b14aa86e0c434d274e86855b383dde3e3e75a076" translate="yes" xml:space="preserve">
          <source>TransformedDistribution</source>
          <target state="translated">TransformedDistribution</target>
        </trans-unit>
        <trans-unit id="68c170c0011cf476eed353d994b12887940cfc96" translate="yes" xml:space="preserve">
          <source>Transformer</source>
          <target state="translated">Transformer</target>
        </trans-unit>
        <trans-unit id="6e42fdb55f8e197d60cafca548c1e82579acbea4" translate="yes" xml:space="preserve">
          <source>Transformer Layers</source>
          <target state="translated">트랜스포머 레이어</target>
        </trans-unit>
        <trans-unit id="ef303bb941bf717e2006e7273f0810b07b78b045" translate="yes" xml:space="preserve">
          <source>TransformerDecoder</source>
          <target state="translated">TransformerDecoder</target>
        </trans-unit>
        <trans-unit id="39490c0e215073daec405a4b72d513bc1f4fb82e" translate="yes" xml:space="preserve">
          <source>TransformerDecoder is a stack of N decoder layers</source>
          <target state="translated">TransformerDecoder는 N 개의 디코더 레이어 스택입니다.</target>
        </trans-unit>
        <trans-unit id="92f20fa7687824fdbb370a6b475f6eeb6f79194b" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer</source>
          <target state="translated">TransformerDecoderLayer</target>
        </trans-unit>
        <trans-unit id="98ae4e9bf414c9fe8b37e6dbc99426a1c7335c2f" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</source>
          <target state="translated">TransformerDecoderLayer는 자체 참석, 다중 헤드 참석 및 피드 포워드 네트워크로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="04a4a3c2fb795f7db70756477e2d518c4a892786" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerDecoderLayer는 자체 참석, 다중 헤드 참석 및 피드 포워드 네트워크로 구성됩니다. 이 표준 디코더 레이어는 &quot;Attention Is All You Need&quot;라는 논문을 기반으로합니다. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심 만 있으면됩니다. Advances in Neural Information Processing Systems, 페이지 6000-6010. 사용자는 응용 프로그램 중에 다른 방식으로 수정하거나 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="32d7343edd3b94812b03b1cdf834b1b16cc2a3fc" translate="yes" xml:space="preserve">
          <source>TransformerEncoder</source>
          <target state="translated">TransformerEncoder</target>
        </trans-unit>
        <trans-unit id="933db464a961834a0fb72e3b98e0d537c401c215" translate="yes" xml:space="preserve">
          <source>TransformerEncoder is a stack of N encoder layers</source>
          <target state="translated">TransformerEncoder는 N 인코더 계층의 스택입니다.</target>
        </trans-unit>
        <trans-unit id="2a732f60462f98f99de0f8f387f8c71e6c32aba7" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer</source>
          <target state="translated">TransformerEncoderLayer</target>
        </trans-unit>
        <trans-unit id="7b3ae5e9124dac923ef7ce7bbf1287aa019083f2" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network.</source>
          <target state="translated">TransformerEncoderLayer는 자체 참석 및 피드 포워드 네트워크로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="0c8dccb3d09a0f9d0d9b17ea9825251d663f04d4" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerEncoderLayer는 자체 참석 및 피드 포워드 네트워크로 구성됩니다. 이 표준 인코더 계층은 &quot;주의가 필요한 모든 것&quot;이라는 논문을 기반으로합니다. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심 만 있으면됩니다. Advances in Neural Information Processing Systems, 페이지 6000-6010. 사용자는 응용 프로그램 중에 다른 방식으로 수정하거나 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="06b62690eb29918ca38e1018a11381720478cf29" translate="yes" xml:space="preserve">
          <source>Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers</source>
          <target state="translated">모듈을 탐색하고 모든 관찰자를 dict에 저장하십시오. 이것은 주로 양자화 정확도 디버그에 사용됩니다. debug : param mod : 모든 옵저버를 저장하려는 최상위 모듈 : param prefix : 현재 모듈의 접두사 : param target_dict : 모든 옵저버를 저장하는 데 사용되는 사전</target>
        </trans-unit>
        <trans-unit id="4cec3758c8a59b886f1bbcffd16de3c1f5af9091" translate="yes" xml:space="preserve">
          <source>Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.</source>
          <target state="translated">이 생성 컨텍스트에서 하나 이상의 프로세스를 결합하려고합니다. 둘 중 하나가 0이 아닌 종료 상태로 종료 된 경우이 함수는 나머지 프로세스를 종료하고 첫 번째 프로세스 종료 원인과 함께 예외를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="a9cb51a530db6f55c97c4e0720722c1b583dafb7" translate="yes" xml:space="preserve">
          <source>TripletMarginLoss</source>
          <target state="translated">TripletMarginLoss</target>
        </trans-unit>
        <trans-unit id="858db382b48e5cc5988f431f28f1130330c1eaf1" translate="yes" xml:space="preserve">
          <source>TripletMarginWithDistanceLoss</source>
          <target state="translated">TripletMarginWithDistanceLoss</target>
        </trans-unit>
        <trans-unit id="88b33e4e12f75ac8bf792aebde41f1a090f3a612" translate="yes" xml:space="preserve">
          <source>True</source>
          <target state="translated">True</target>
        </trans-unit>
        <trans-unit id="56d8a88a6680e193e3cef0877ab543f4d2d36e12" translate="yes" xml:space="preserve">
          <source>True if all differences satisfy allclose condition</source>
          <target state="translated">모든 차이가 allclose 조건을 충족하는 경우 참</target>
        </trans-unit>
        <trans-unit id="6692b9e66830b65bbf15ff9de3c8b1af8bfbc5eb" translate="yes" xml:space="preserve">
          <source>Tuple Construction</source>
          <target state="translated">튜플 구성</target>
        </trans-unit>
        <trans-unit id="2c809de45543797accbb7ba7433a5877747ac289" translate="yes" xml:space="preserve">
          <source>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to &lt;code&gt;pack_padded_sequence&lt;/code&gt; or &lt;code&gt;pack_sequence&lt;/code&gt;.</source>
          <target state="translated">패딩 된 시퀀스를 포함하는 Tensor의 튜플과 배치에있는 각 시퀀스의 길이 목록을 포함하는 Tensor. 배치 요소는 배치가 &lt;code&gt;pack_padded_sequence&lt;/code&gt; 또는 &lt;code&gt;pack_sequence&lt;/code&gt; 에 전달 될 때 원래 순서대로 다시 정렬됩니다 .</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="19fd25590b1f3de52164e96d6a585a4ae3f02132" translate="yes" xml:space="preserve">
          <source>Two names &lt;em&gt;match&lt;/em&gt; if they are equal (string equality) or if at least one is &lt;code&gt;None&lt;/code&gt;. Nones are essentially a special &amp;ldquo;wildcard&amp;rdquo; name.</source>
          <target state="translated">두 이름 이 같거나 (문자열 같음) 적어도 하나가 &lt;code&gt;None&lt;/code&gt; 인 경우 &lt;em&gt;일치&lt;/em&gt; 합니다 . None은 본질적으로 특별한 &quot;와일드 카드&quot;이름입니다.</target>
        </trans-unit>
        <trans-unit id="3deb7456519697ecf4eefc455516c969a3681bae" translate="yes" xml:space="preserve">
          <source>Type</source>
          <target state="translated">Type</target>
        </trans-unit>
        <trans-unit id="74a99ad458c9adf9d415d4bdb125da11688a37f7" translate="yes" xml:space="preserve">
          <source>Type Info</source>
          <target state="translated">유형 정보</target>
        </trans-unit>
        <trans-unit id="58cf557846d7f65d34e8a92739eef2665164fad4" translate="yes" xml:space="preserve">
          <source>Type aliases</source>
          <target state="translated">타입 별칭</target>
        </trans-unit>
        <trans-unit id="c96d0a66889e478977efae5dbe9bb76dea1e1106" translate="yes" xml:space="preserve">
          <source>Type mismatch errors &lt;em&gt;in&lt;/em&gt; an autocast-enabled region are a bug; if this is what you observe, please file an issue.</source>
          <target state="translated">불일치 오류 입력 &lt;em&gt;에&lt;/em&gt; 버그가 있습니다 자동 시전 가능 영역; 이것이 당신이 관찰하는 것이라면 문제를 제기하십시오.</target>
        </trans-unit>
        <trans-unit id="93b9e289e2842469d001eccf7ad5d79f3c302dc9" translate="yes" xml:space="preserve">
          <source>Types</source>
          <target state="translated">Types</target>
        </trans-unit>
        <trans-unit id="c0d285234eb927c53c2fc1dda528e04061e0d90d" translate="yes" xml:space="preserve">
          <source>Types produced by &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; can be used in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt; &lt;code&gt;collections.namedtuple&lt;/code&gt; 에&lt;/a&gt; 의해 생성 된 유형 은 TorchScript에서 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e041e0fe9045d1072a9765e6e123108f2940daba" translate="yes" xml:space="preserve">
          <source>Typically, in SWA the learning rate is set to a high constant value. &lt;code&gt;SWALR&lt;/code&gt; is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:</source>
          <target state="translated">일반적으로 SWA에서 학습률은 높은 상수 값으로 설정됩니다. &lt;code&gt;SWALR&lt;/code&gt; 은 학습률을 고정 된 값으로 어닐링 한 다음 일정하게 유지하는 학습률 스케줄러입니다. 예를 들어, 다음 코드는 각 파라미터 그룹 내에서 5 epochs에서 학습률을 초기 값에서 0.05로 선형 적으로 어닐링하는 스케줄러를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="b2c7c0caa10a0cca5ea7d69e54018ae0c0389dd6" translate="yes" xml:space="preserve">
          <source>U</source>
          <target state="translated">U</target>
        </trans-unit>
        <trans-unit id="4e5f249d2049283bfab86474c53e19434fabe07e" translate="yes" xml:space="preserve">
          <source>URL specifying how to initialize the process group. Default is &lt;code&gt;env://&lt;/code&gt;</source>
          <target state="translated">프로세스 그룹을 초기화하는 방법을 지정하는 URL입니다. 기본값은 &lt;code&gt;env://&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="9016840b6ab501762ae42c3bc2d77284127ecd9d" translate="yes" xml:space="preserve">
          <source>Unflatten</source>
          <target state="translated">Unflatten</target>
        </trans-unit>
        <trans-unit id="22753787538a4df3368517dc3a28771d6a6bd1c2" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape.</source>
          <target state="translated">원하는 모양으로 확장하는 텐서 희미한 평면을 해제합니다.</target>
        </trans-unit>
        <trans-unit id="ffd6987181a5e9a56284c36d90107b63ec0ad279" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">원하는 모양으로 확장하는 텐서 희미한 평면을 해제합니다. &lt;code&gt;Sequential&lt;/code&gt; 과 함께 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="02d85ea4efaa0d1ca099f19843e9be07e28b954d" translate="yes" xml:space="preserve">
          <source>Unfold</source>
          <target state="translated">Unfold</target>
        </trans-unit>
        <trans-unit id="27c8f884a26740cbb923c350e9fa436fb8314b34" translate="yes" xml:space="preserve">
          <source>Unfortunately, the concrete &lt;code&gt;subset&lt;/code&gt; that was used is lost. For more information see &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;this discussion&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;these experiments&lt;/a&gt;.</source>
          <target state="translated">불행히도 사용 된 구체적인 &lt;code&gt;subset&lt;/code&gt; 이 손실됩니다. 자세한 내용은 &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;이 토론&lt;/a&gt; 또는 &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;이 실험을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7a4ec9e5c0816d2c18ff0c058bf903c94688d657" translate="yes" xml:space="preserve">
          <source>Unfortunately, there&amp;rsquo;s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or &lt;a href=&quot;#torch.autograd.profiler.load_nvprof&quot;&gt;&lt;code&gt;torch.autograd.profiler.load_nvprof()&lt;/code&gt;&lt;/a&gt; can load the results for inspection e.g. in Python REPL.</source>
          <target state="translated">안타깝게도 nvprof가 수집 한 데이터를 디스크에 플러시하도록 강제 할 수있는 방법이 없으므로 CUDA 프로파일 링을 위해이 컨텍스트 관리자를 사용하여 nvprof 추적에 주석을 달고 프로세스가 종료 될 때까지 기다려야합니다. 그런 다음 NVIDIA Visual Profiler (nvvp)를 사용하여 타임 라인을 시각화하거나 &lt;a href=&quot;#torch.autograd.profiler.load_nvprof&quot;&gt; &lt;code&gt;torch.autograd.profiler.load_nvprof()&lt;/code&gt; &lt;/a&gt; 가 검사 결과를로드 할 수 있습니다 ( 예 : Python REPL).</target>
        </trans-unit>
        <trans-unit id="e96b0da8b763233f21fa51e37813da0eba866185" translate="yes" xml:space="preserve">
          <source>Uniform</source>
          <target state="translated">Uniform</target>
        </trans-unit>
        <trans-unit id="2786d7c847a0723edc813cd593fabc9e06192fcd" translate="yes" xml:space="preserve">
          <source>Unless otherwise specified, this function should not modify the &lt;code&gt;.grad&lt;/code&gt; field of the parameters.</source>
          <target state="translated">달리 지정하지 않는 한이 함수는 매개 변수 의 &lt;code&gt;.grad&lt;/code&gt; 필드를 수정해서는 안됩니다 .</target>
        </trans-unit>
        <trans-unit id="b69bc99e876e93b97ea0728c1acca97b14bbffef" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt;, this function copies the tensor&amp;rsquo;s data.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt; 와 달리이 함수는 텐서의 데이터를 복사합니다.</target>
        </trans-unit>
        <trans-unit id="ef5392b685d4622304e5dfc150347ec19f0ee0af" translate="yes" xml:space="preserve">
          <source>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the &lt;code&gt;affine&lt;/code&gt; option, Layer Normalization applies per-element scale and bias with &lt;code&gt;elementwise_affine&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; 옵션을 사용하여 각 전체 채널 / 평면에 스칼라 스케일 및 바이어스를 적용하는 Batch Normalization 및 Instance Normalization과 달리 Layer Normalization은 &lt;code&gt;elementwise_affine&lt;/code&gt; 을 사용 하여 요소 별 스케일 및 바이어스를 적용 합니다.</target>
        </trans-unit>
        <trans-unit id="8607cd4c09ad49a23e91f63072337c8e4b9af158" translate="yes" xml:space="preserve">
          <source>Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.</source>
          <target state="translated">CPU 텐서와 달리 전송 프로세스는 수신 프로세스가 텐서의 사본을 보유하는 한 원래 텐서를 유지하는 데 필요합니다. refcounting은 내부적으로 구현되지만 사용자는 다음 모범 사례를 따라야합니다.</target>
        </trans-unit>
        <trans-unit id="2498c971c849991894b17969e87a3329f48f1d2d" translate="yes" xml:space="preserve">
          <source>Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions.</source>
          <target state="translated">Python과 달리 TorchScript 함수의 각 변수에는 단일 정적 유형이 있어야합니다. 이렇게하면 TorchScript 기능을보다 쉽게 ​​최적화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3d7bdd35d7e8961eff68645ed326f054b5482c52" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented</source>
          <target state="translated">구현 가능성이 낮음</target>
        </trans-unit>
        <trans-unit id="a5b6f222f736f9cef541160a005848fe4ef77888" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented (however &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt;&lt;code&gt;typing.Optional&lt;/code&gt;&lt;/a&gt; is supported)</source>
          <target state="translated">구현 가능성이 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt; &lt;code&gt;typing.Optional&lt;/code&gt; &lt;/a&gt; (단, 입력 옵션 은 지원됨)</target>
        </trans-unit>
        <trans-unit id="01773bdc2e10d2279820d7115b2522610a09e4f3" translate="yes" xml:space="preserve">
          <source>Unpacks the data and pivots from a LU factorization of a tensor.</source>
          <target state="translated">텐서의 LU 분해에서 데이터 및 피벗을 압축 해제합니다.</target>
        </trans-unit>
        <trans-unit id="a7afe79ac1105fe133bbed5120636913c111a929" translate="yes" xml:space="preserve">
          <source>Unsupported Typing Constructs</source>
          <target state="translated">지원되지 않는 타이핑 구문</target>
        </trans-unit>
        <trans-unit id="b2d1fe571301d72d9191816bf953a47985e426ec" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">매핑 또는 반복 가능한 키-값 쌍으로 &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt; 를 업데이트하여 기존 키를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="4eca5c6565cbf1cfd8391f002e24b24767bdf27d" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">매핑 또는 반복 가능한 키-값 쌍으로 &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt; 를 업데이트하여 기존 키를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="b985b6244019737115cb9e96cf752b208328d084" translate="yes" xml:space="preserve">
          <source>Updates the scale factor.</source>
          <target state="translated">축척 비율을 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="e9f4cec65260954e7a90830aa6f73b90e5c8817f" translate="yes" xml:space="preserve">
          <source>Upsample</source>
          <target state="translated">Upsample</target>
        </trans-unit>
        <trans-unit id="0b472c3013e4cd26ac1f5f84d9b57c5b6b4ca455" translate="yes" xml:space="preserve">
          <source>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</source>
          <target state="translated">주어진 다중 채널 1D (시간), 2D (공간) 또는 3D (체적) 데이터를 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="55dfe2d567c845d264928977c08cd8d377265025" translate="yes" xml:space="preserve">
          <source>Upsamples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">입력을 주어진 &lt;code&gt;size&lt;/code&gt; 또는 주어진 &lt;code&gt;scale_factor&lt;/code&gt; 로 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="c9b84912595a004dd92417c63046087231bc7d36" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using bilinear upsampling.</source>
          <target state="translated">쌍 선형 업 샘플링을 사용하여 입력을 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="a31dc0017d79484c9069d06f3ac78a206e41dbfa" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using nearest neighbours&amp;rsquo; pixel values.</source>
          <target state="translated">가장 가까운 이웃의 픽셀 값을 사용하여 입력을 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="fef8a1cac9f9d013e57f5acb6da8a9f36cede8d1" translate="yes" xml:space="preserve">
          <source>UpsamplingBilinear2d</source>
          <target state="translated">UpsamplingBilinear2d</target>
        </trans-unit>
        <trans-unit id="2a7960d23688a71e6707ef6d16f626ed000e779c" translate="yes" xml:space="preserve">
          <source>UpsamplingNearest2d</source>
          <target state="translated">UpsamplingNearest2d</target>
        </trans-unit>
        <trans-unit id="96d06615a1d6c33a776a3eb6549cbfd22dbce234" translate="yes" xml:space="preserve">
          <source>Usage of this function is discouraged in favor of &lt;a href=&quot;#torch.cuda.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt;. In most cases it&amp;rsquo;s better to use &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environmental variable.</source>
          <target state="translated">이 기능의 사용은 &lt;a href=&quot;#torch.cuda.device&quot;&gt; &lt;code&gt;device&lt;/code&gt; &lt;/a&gt; 를 위해 권장되지 않습니다 . 대부분의 경우 &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; 환경 변수 를 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="680c6c9f314b68f86ff46d8fdf9a289c7fb0c343" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt;&lt;code&gt;align_as()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to align tensor dimensions by name to a specified ordering. This is useful for performing &amp;ldquo;broadcasting by names&amp;rdquo;.</source>
          <target state="translated">사용 &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt; &lt;code&gt;align_as()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 지정된 순서에 이름으로 정렬 텐서 치수. 이것은 &quot;이름으로 방송&quot;을 수행 할 때 유용합니다.</target>
        </trans-unit>
        <trans-unit id="66804d0e4e83078e14bfdbaa37e395675103792c" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to permute large amounts of dimensions without mentioning all of them as in required by &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 를 사용 하여 &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt; &lt;code&gt;permute()&lt;/code&gt; &lt;/a&gt; 에서 요구하는대로 모든 차원을 언급하지 않고 다량의 차원을 permute 합니다.</target>
        </trans-unit>
        <trans-unit id="ede9bff88acacf3e23cc669406c39e0d8aba75b8" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;torch.Tensor.item()&lt;/code&gt;&lt;/a&gt; to get a Python number from a tensor containing a single value:</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;torch.Tensor.item()&lt;/code&gt; &lt;/a&gt; 을 사용 하여 단일 값을 포함하는 텐서에서 Python 번호를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="e75b6839717cab84ecfa02664778f32a2c198780" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; to access the dimension names of a tensor and &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt; to rename named dimensions.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 을 사용 하여 텐서의 차원 이름에 액세스하고 &lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt; 을 사용하여 명명 된 차원의 이름을 변경하십시오.</target>
        </trans-unit>
        <trans-unit id="b5d22900f5f0642196ebec69d6c5572874ee7c1b" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt;&lt;code&gt;flatten()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt;&lt;code&gt;unflatten()&lt;/code&gt;&lt;/a&gt; to flatten and unflatten dimensions, respectively. These methods are more verbose than &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, but have more semantic meaning to someone reading the code.</source>
          <target state="translated">사용 &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt; &lt;code&gt;flatten()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt; &lt;code&gt;unflatten()&lt;/code&gt; &lt;/a&gt; 각각 평평하고 패턴 화 해제 치수합니다. 이러한 메서드는 &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 보다 더 장황 하지만 코드를 읽는 사람에게 더 의미가 있습니다.</target>
        </trans-unit>
        <trans-unit id="b67226dee210381a9009b14565e54ae65d2034d5" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; if no stream is specified.</source>
          <target state="translated">스트림이 지정되지 않은 경우 &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="641cc18d21edd9671570b1c6c598d26a3e5396f4" translate="yes" xml:space="preserve">
          <source>Use Gloo, unless you have specific reasons to use MPI.</source>
          <target state="translated">MPI를 사용해야하는 특별한 이유가 없다면 Gloo를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="4b4d173cba748d683c0210fec722f5787173d83a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</source>
          <target state="translated">특히 다중 프로세스 단일 노드 또는 다중 노드 분산 학습에 대해 현재 최상의 분산 GPU 학습 성능을 제공하므로 NCCL을 사용하십시오. NCCL에 문제가 발생하면 Gloo를 대체 옵션으로 사용하십시오. (현재 Gloo는 GPU의 경우 NCCL보다 느리게 실행됩니다.)</target>
        </trans-unit>
        <trans-unit id="9b1bf082bc9bf3ac7914e6187c00d474a91fb73a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it&amp;rsquo;s the only backend that currently supports InfiniBand and GPUDirect.</source>
          <target state="translated">NCCL은 현재 InfiniBand 및 GPUDirect를 지원하는 유일한 백엔드이므로 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5e1700010ee503b77904278f8d396bfe42da63c7" translate="yes" xml:space="preserve">
          <source>Use external data format</source>
          <target state="translated">외부 데이터 형식 사용</target>
        </trans-unit>
        <trans-unit id="9d0232b01e54b3a2e55b622100adbb68b4edb340" translate="yes" xml:space="preserve">
          <source>Use of Python Values</source>
          <target state="translated">Python 값 사용</target>
        </trans-unit>
        <trans-unit id="73922dca1fc1eb81dc09b31de16429c2bad893e0" translate="yes" xml:space="preserve">
          <source>Use the Gloo backend for distributed &lt;strong&gt;CPU&lt;/strong&gt; training.</source>
          <target state="translated">분산 &lt;strong&gt;CPU&lt;/strong&gt; 훈련을 위해 Gloo 백엔드를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="fc3648b93e2309e15d382b23a53006e5ccb31b40" translate="yes" xml:space="preserve">
          <source>Use the NCCL backend for distributed &lt;strong&gt;GPU&lt;/strong&gt; training</source>
          <target state="translated">분산 &lt;strong&gt;GPU&lt;/strong&gt; 학습에 NCCL 백엔드 사용</target>
        </trans-unit>
        <trans-unit id="9ca66ca0ae6872cefb7cdaba9a1b1d7c4fb946f1" translate="yes" xml:space="preserve">
          <source>Used to infer dtype for python complex numbers. The default complex dtype is set to &lt;code&gt;torch.complex128&lt;/code&gt; if default floating point dtype is &lt;code&gt;torch.float64&lt;/code&gt;, otherwise it&amp;rsquo;s set to &lt;code&gt;torch.complex64&lt;/code&gt;</source>
          <target state="translated">파이썬 복소수에 대한 dtype을 추론하는 데 사용됩니다. 기본 복잡한 DTYPE로 설정되어 &lt;code&gt;torch.complex128&lt;/code&gt; 기본 부동 소수점 DTYPE 경우 &lt;code&gt;torch.float64&lt;/code&gt; 에 그렇지 않은 경우의 설정 &lt;code&gt;torch.complex64&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fac6c061fc6d273aebc0e9ab0c99f04ae81e1097" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Argument</source>
          <target state="translated">소유자가 인수 인 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="a20f912ad88b795f826c479ed261f676c30da2f7" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Return Value</source>
          <target state="translated">반환 값으로 소유자와 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="a7e43a6c0dede8cc47d9e15900c78d9da28530c0" translate="yes" xml:space="preserve">
          <source>User Share RRef with User</source>
          <target state="translated">사용자와 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="78d0e495aa5431e6e0a8139c0666804c17a54940" translate="yes" xml:space="preserve">
          <source>User extensions can register their own location tags and tagging and deserialization methods using &lt;code&gt;torch.serialization.register_package()&lt;/code&gt;.</source>
          <target state="translated">사용자 확장은 &lt;code&gt;torch.serialization.register_package()&lt;/code&gt; 사용하여 자체 위치 태그와 태그 지정 및 역 직렬화 방법을 등록 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a2628232b96e27d1cfe74641b5e5eef1e4a68d67" translate="yes" xml:space="preserve">
          <source>Users can force a reload by calling &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt;. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</source>
          <target state="translated">사용자는 &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt; 를 호출하여 강제로 다시로드 할 수 있습니다 . 이렇게하면 기존 github 폴더와 다운로드 된 가중치가 삭제되고 새로운 다운로드가 다시 초기화됩니다. 이것은 업데이트가 동일한 브랜치에 게시 될 때 유용하며 사용자는 최신 릴리스를 따라갈 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0dfaf311b141161fd5f1c71366b90c3c330831f" translate="yes" xml:space="preserve">
          <source>Users may use customized &lt;code&gt;collate_fn&lt;/code&gt; to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.</source>
          <target state="translated">사용자는 사용자 지정 &lt;code&gt;collate_fn&lt;/code&gt; 을 사용하여 사용자 지정 일괄 처리를 수행 할 수 있습니다 . 예를 들어 첫 번째 차원이 아닌 차원을 따라 정렬하거나 다양한 길이의 시퀀스를 채우거나 사용자 지정 데이터 유형에 대한 지원을 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36a0e861f968553beec79e7c334a566865bdb40a" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; if no stream is specified. The stream&amp;rsquo;s device must match the event&amp;rsquo;s device.</source>
          <target state="translated">스트림이 지정되지 않은 경우 &lt;code&gt;torch.cuda.current_stream()&lt;/code&gt; 사용 합니다. 스트림의 장치는 이벤트의 장치와 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="daa3e4bf690e79b2d584e60908e08290c5eeab22" translate="yes" xml:space="preserve">
          <source>Using &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt; will be more efficient: all computations internally are based on &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt;&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt;&lt;code&gt;precision_matrix&lt;/code&gt;&lt;/a&gt; is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; &lt;/a&gt; 을 사용하는 것이 더 효율적입니다. 내부적으로 모든 계산은 &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; 을&lt;/a&gt; 기반으로 합니다 . 경우 &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt; &lt;code&gt;covariance_matrix&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt; &lt;code&gt;precision_matrix&lt;/code&gt; 이&lt;/a&gt; 대신 전달됩니다, 단지 콜레 분해를 사용하여 해당 하위 삼각 행렬을 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="df3d36e2101db7508ee9c65bba6c79f4162eb560" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;DistributedDataParallel&lt;/code&gt; in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; is experimental and subject to change.</source>
          <target state="translated">&lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; 와 함께 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 사용하는 것은 실험적이며 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6f8c8c4de2dead884cf1cd1b1efe492df15bdbf3" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;torch.jit.trace&lt;/code&gt; and &lt;code&gt;torch.jit.trace_module&lt;/code&gt;, you can turn an existing module or Python function into a TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.</source>
          <target state="translated">&lt;code&gt;torch.jit.trace&lt;/code&gt; 및 &lt;code&gt;torch.jit.trace_module&lt;/code&gt; 을 사용 하면 기존 모듈 또는 Python 함수를 TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 로 바꿀 수 있습니다 . 예제 입력을 제공하고 함수를 실행하여 모든 텐서에서 수행 된 작업을 기록해야합니다.</target>
        </trans-unit>
        <trans-unit id="016141762b3eda50d55b6434765c656b21e2c21e" translate="yes" xml:space="preserve">
          <source>Using GPU tensors as arguments or return values of &lt;code&gt;func&lt;/code&gt; is not supported since we don&amp;rsquo;t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="translated">GPU 텐서를 인수로 사용하거나 &lt;code&gt;func&lt;/code&gt; 의 반환 값을 사용 하는 것은 유선을 통한 GPU 텐서 전송을 지원하지 않기 때문에 지원되지 않습니다. GPU 텐서를 인수로 사용하거나 &lt;code&gt;func&lt;/code&gt; 의 값을 반환하기 전에 CPU 텐서를 명시 적으로 복사해야합니다 .</target>
        </trans-unit>
        <trans-unit id="40f8083d709ef1dfeba3266d73cd40bc2120c4c0" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute matrix norms:</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 인수를 사용하여 행렬 규범 계산 :</target>
        </trans-unit>
        <trans-unit id="34eeb9d75aa34a25cf342ae4a398eead839c1594" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute vector norms:</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 인수를 사용하여 벡터 노름 계산 :</target>
        </trans-unit>
        <trans-unit id="46e68d726011984151416846f828c085d1a8cabd" translate="yes" xml:space="preserve">
          <source>Using this method with &lt;code&gt;create_graph=True&lt;/code&gt; will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using &lt;code&gt;autograd.grad&lt;/code&gt; when creating the graph to avoid this. If you have to use this function, make sure to reset the &lt;code&gt;.grad&lt;/code&gt; fields of your parameters to &lt;code&gt;None&lt;/code&gt; after use to break the cycle and avoid the leak.</source>
          <target state="translated">&lt;code&gt;create_graph=True&lt;/code&gt; 와 함께이 메서드를 사용 하면 매개 변수와 해당 그라디언트 사이에 참조주기가 생성되어 메모리 누수가 발생할 수 있습니다. 이를 방지하려면 그래프를 만들 때 &lt;code&gt;autograd.grad&lt;/code&gt; 를 사용하는 것이 좋습니다 . 이 기능을 사용해야하는 경우 사용 후 매개 변수 의 &lt;code&gt;.grad&lt;/code&gt; 필드를 &lt;code&gt;None&lt;/code&gt; 으로 재설정 하여주기를 중단하고 누출을 방지하십시오.</target>
        </trans-unit>
        <trans-unit id="892e242741e1c766ef8440ce7e813adfc429718b" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv2d&lt;/code&gt; modules.</source>
          <target state="translated">일반적으로 입력은 &lt;code&gt;nn.Conv2d&lt;/code&gt; 모듈 에서 제공됩니다 .</target>
        </trans-unit>
        <trans-unit id="62c50ebd5d624de117b146ade6f9d5575d770fc3" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv3d&lt;/code&gt; modules.</source>
          <target state="translated">일반적으로 입력은 &lt;code&gt;nn.Conv3d&lt;/code&gt; 모듈 에서 제공됩니다 .</target>
        </trans-unit>
        <trans-unit id="18fdc5ee8b1f8fba8dabaa933373c0483ab7fad7" translate="yes" xml:space="preserve">
          <source>Utilities</source>
          <target state="translated">Utilities</target>
        </trans-unit>
        <trans-unit id="f16cdccb3faaa5e67faa6fdb65caa5cf29d51cbb" translate="yes" xml:space="preserve">
          <source>Utility functions</source>
          <target state="translated">유틸리티 기능</target>
        </trans-unit>
        <trans-unit id="ad96fcc3041d4053b9449ab4a648c6341e2217d7" translate="yes" xml:space="preserve">
          <source>Utility functions in other modules</source>
          <target state="translated">다른 모듈의 유틸리티 기능</target>
        </trans-unit>
        <trans-unit id="1c41457151932f1c1abddf366a843f91dd5ea098" translate="yes" xml:space="preserve">
          <source>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</source>
          <target state="translated">단위를 가지 치기하지 않고 하나의 마스크로 가지 치기 매개 변수를 생성하는 유틸리티 가지 치기 방법입니다.</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="1eafbd543ea2d8baa3df59414ec62e236836b78c" translate="yes" xml:space="preserve">
          <source>V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</source>
          <target state="translated">V. Balntas, et al .: 삼중 항 손실이있는 얕은 컨볼 루션 기능 설명자 학습 : &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2badedb167ecf8f97a1ffcdd2817401074fa728e" translate="yes" xml:space="preserve">
          <source>VGG</source>
          <target state="translated">VGG</target>
        </trans-unit>
        <trans-unit id="76dc2de0cc7c95272fc67f826f2dc5fa09c8ded2" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) from &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot; &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&lt;/a&gt; &quot;의 VGG 11 계층 모델 (구성 &quot;A&quot;)</target>
        </trans-unit>
        <trans-unit id="efac66ad38738b85f5ef46a914f1b2bede87a54e" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&lt;/a&gt; &quot;를 사용하는 VGG 11 계층 모델 (구성 &quot;A&quot;)</target>
        </trans-unit>
        <trans-unit id="8ab10add11dcdf5fb182aad6d9f35aa59134c466" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 13 계층 모델 (구성&amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c68abe1f69acd7242c26b04b450c20879de247dc" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;를 사용하는&lt;/a&gt; VGG 13- 레이어 모델 (구성 &quot;B&quot;)</target>
        </trans-unit>
        <trans-unit id="7df2c09a84b7ee83b6d8493aa3dccf2e12cc714a" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 16 계층 모델 (구성&amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4979b51da2376b295925fa358ed83201a60846eb" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;를 사용하는&lt;/a&gt; VGG 16 레이어 모델 (구성 &quot;D&quot;)</target>
        </trans-unit>
        <trans-unit id="734385d469c0b7b16d39310aab3a223999973e9e" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 19 레이어 모델 (구성&amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b468de2d615302723a63a19cf8ef07ba2f87d240" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;lsquo;E&amp;rsquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화를 사용하는 VGG 19 레이어 모델 (구성 'E') &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22dca5ed30facb9e021b2a416131b71d9addd5df" translate="yes" xml:space="preserve">
          <source>VGG-11</source>
          <target state="translated">VGG-11</target>
        </trans-unit>
        <trans-unit id="59c5eb8db8735266b63e11d68abfe0513f53e937" translate="yes" xml:space="preserve">
          <source>VGG-11 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-11</target>
        </trans-unit>
        <trans-unit id="275a0abbe0e323332b36f91fd1e0bbabaa98823b" translate="yes" xml:space="preserve">
          <source>VGG-13</source>
          <target state="translated">VGG-13</target>
        </trans-unit>
        <trans-unit id="00e8568346d410021e9a56f3b1b1f1983ac75021" translate="yes" xml:space="preserve">
          <source>VGG-13 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-13</target>
        </trans-unit>
        <trans-unit id="3fa5851c47708aca43af1f031237ec76652d5fb8" translate="yes" xml:space="preserve">
          <source>VGG-16</source>
          <target state="translated">VGG-16</target>
        </trans-unit>
        <trans-unit id="6794221fe94aa99e89cd4a3fc469cd560c5b1798" translate="yes" xml:space="preserve">
          <source>VGG-16 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-16</target>
        </trans-unit>
        <trans-unit id="e33f8119a3d18ad4bc21aa5913ffff22adbc62fa" translate="yes" xml:space="preserve">
          <source>VGG-19</source>
          <target state="translated">VGG-19</target>
        </trans-unit>
        <trans-unit id="6a06101d542844efc9851734dd33c0a3fcfb9071" translate="yes" xml:space="preserve">
          <source>VGG-19 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-19</target>
        </trans-unit>
        <trans-unit id="08ab4ecc000363002865da057bb07b708354e689" translate="yes" xml:space="preserve">
          <source>Valid operation names:</source>
          <target state="translated">유효한 작업 이름 :</target>
        </trans-unit>
        <trans-unit id="8ab2f6ea14647497320511c9699ad1fa98390d6e" translate="yes" xml:space="preserve">
          <source>Value associated with &lt;code&gt;key&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; is in the store.</source>
          <target state="translated">가치와 관련된 &lt;code&gt;key&lt;/code&gt; 경우 &lt;code&gt;key&lt;/code&gt; 가게에 있습니다.</target>
        </trans-unit>
        <trans-unit id="8f65e3050b4aa23d2b9ffeafc8ef420283f1bc31" translate="yes" xml:space="preserve">
          <source>Values looked up as attributes of a module are assumed to be constant:</source>
          <target state="translated">모듈의 속성으로 조회 된 값은 상수로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="766a55330fbdeceea3990ed650f19ff9d7ebc2b2" translate="yes" xml:space="preserve">
          <source>Vandermonde matrix. If increasing is False, the first column is</source>
          <target state="translated">Vandermonde 행렬. 증가가 False이면 첫 번째 열은</target>
        </trans-unit>
        <trans-unit id="2363c9d67b7ae2a0e8fa492fb3e5e19109b5b856" translate="yes" xml:space="preserve">
          <source>Variable (deprecated)</source>
          <target state="translated">변수 (지원 중단됨)</target>
        </trans-unit>
        <trans-unit id="8993fc586517fabcc3d0fce5c92e800dc4e3de15" translate="yes" xml:space="preserve">
          <source>Variable Resolution</source>
          <target state="translated">가변 해상도</target>
        </trans-unit>
        <trans-unit id="ac018db1f7b00972061adff843d37497d8ee153c" translate="yes" xml:space="preserve">
          <source>Variables</source>
          <target state="translated">Variables</target>
        </trans-unit>
        <trans-unit id="b1c39119660f33e5be726c1652639e668fcdfcd8" translate="yes" xml:space="preserve">
          <source>Verifies that the given compiler is ABI-compatible with PyTorch.</source>
          <target state="translated">주어진 컴파일러가 PyTorch와 ABI 호환되는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="991eeb7d6acd3a1b90daf8e907ad4a1126fbf67e" translate="yes" xml:space="preserve">
          <source>Via a string and device ordinal:</source>
          <target state="translated">문자열 및 장치 서수를 통해 :</target>
        </trans-unit>
        <trans-unit id="78f371ae51565c626e223c43a305351baadcdfcb" translate="yes" xml:space="preserve">
          <source>Via a string:</source>
          <target state="translated">문자열을 통해 :</target>
        </trans-unit>
        <trans-unit id="3eb4e2b65c3b6adcd10d477d9e1ded0579505479" translate="yes" xml:space="preserve">
          <source>Video classification</source>
          <target state="translated">비디오 분류</target>
        </trans-unit>
        <trans-unit id="61ad6c7f7397fbdc6a6513e489917c12a6953f11" translate="yes" xml:space="preserve">
          <source>View this tensor as the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.view_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.view(other.size())&lt;/code&gt;.</source>
          <target state="translated">이 텐서를 &lt;code&gt;other&lt;/code&gt; 것과 같은 크기로 봅니다. &lt;code&gt;self.view_as(other)&lt;/code&gt; 는 &lt;code&gt;self.view(other.size())&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="68b460da28c94fa06be12e6f2242c79c2eca8bd2" translate="yes" xml:space="preserve">
          <source>Vision Layers</source>
          <target state="translated">비전 레이어</target>
        </trans-unit>
        <trans-unit id="8cd7ed1352bc6bdd613ec698de479a989aa9357b" translate="yes" xml:space="preserve">
          <source>Vision functions</source>
          <target state="translated">비전 기능</target>
        </trans-unit>
        <trans-unit id="2b4b5f5947eec0464172de099ad91010e86ae87b" translate="yes" xml:space="preserve">
          <source>VonMises</source>
          <target state="translated">VonMises</target>
        </trans-unit>
        <trans-unit id="e2415cb7f63df0c9de23362326ad3c37a9adfc96" translate="yes" xml:space="preserve">
          <source>W</source>
          <target state="translated">W</target>
        </trans-unit>
        <trans-unit id="46965f2770b3f862f5982cb5b877918de164026c" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride [1]}-2 \ times \ text {padding [1]} + \ text {kernel \ _size [1]}</target>
        </trans-unit>
        <trans-unit id="5b7347a76af1465d2b26baeed335756321992f36" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride [2]}-2 \ times \ text {padding [2]} + \ text {kernel \ _size [2]}</target>
        </trans-unit>
        <trans-unit id="06cfb3f342da0b25161bf11f15bf2d81bfb676a7" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride} [1]-2 \ times \ text {padding} [1] + \ text {dilation} [1] \ times (\ text { 커널 \ _size} [1]-1) + \ text {output \ _padding} [1] + 1</target>
        </trans-unit>
        <trans-unit id="144094cbaa937780b12d5cb8356e1fc4a6c692aa" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride} [2]-2 \ times \ text {padding} [2] + \ text {dilation} [2] \ times (\ text { 커널 \ _size} [2]-1) + \ text {output \ _padding} [2] + 1</target>
        </trans-unit>
        <trans-unit id="23da97705d0e8d5a571139859c1eefc7a25b59f7" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}</source>
          <target state="translated">W_ {out} = W_ {in} + \ text {padding \ _left} + \ text {padding \ _right}</target>
        </trans-unit>
        <trans-unit id="bf2c9a7a90367be7032bd4f58614c0c9ad3779f8" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} \times \text{upscale\_factor}</source>
          <target state="translated">W_ {out} = W_ {in} \ times \ text {upscale \ _factor}</target>
        </trans-unit>
        <trans-unit id="c5a97d192d1067a74706f036b87ed2b96b8d5895" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor W_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="dd2204cd0212ab9c7330f7c2e6a0b40cae1ae693" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 * \ text {padding [1]}-\ text {dilation [1]} \ times (\ text {kernel \ _size [1]}- 1)-1} {\ text {stride [1]}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="876c8e0e601e7178678d337712548c5cc7269f61" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [1]-\ text {dilation} [1] \ times (\ text {kernel \ _size} [1] -1)-1} {\ text {stride} [1]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="6e0657f1536a7a37c826480069677e9010835607" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [1]-\ text {kernel \ _size} [1]} {\ text {stride} [1]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="a11c0191402c7be2c31a4ea7dcec53e255776ef6" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [2]-\ text {dilation} [2] \ times (\ text {kernel \ _size} [2] -1)-1} {\ text {stride} [2]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="b4191f6bab2de7c1a0f2413fa95cad94e0003ac5" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [2]-\ text {kernel \ _size} [2]} {\ text {stride} [2]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="90b3ee06862d0af7ed835a4f57d3b9839c234435" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in}-\ text {kernel \ _size} [1]} {\ text {stride} [1]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="206613fe29a3beacd4aa9146df44d435eeec070f" translate="yes" xml:space="preserve">
          <source>Wait for all the kernels in this stream to complete.</source>
          <target state="translated">이 스트림의 모든 커널이 완료 될 때까지 기다리십시오.</target>
        </trans-unit>
        <trans-unit id="a3b76987c40f20668517b915def98113b24efd97" translate="yes" xml:space="preserve">
          <source>Waits for all kernels in all streams on a CUDA device to complete.</source>
          <target state="translated">CUDA 장치의 모든 스트림에있는 모든 커널이 완료 될 때까지 기다립니다.</target>
        </trans-unit>
        <trans-unit id="02d96942e51e668861c82b6ad6a1888bbaffdfc3" translate="yes" xml:space="preserve">
          <source>Waits for all provided futures to be complete, and returns the list of completed values.</source>
          <target state="translated">제공된 모든 Future가 완료 될 때까지 기다린 후 완료된 값 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0a15d8c4d33065db8154fb49b0e2d2a98624af19" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store, and throws an exception if the keys have not been set by the supplied &lt;code&gt;timeout&lt;/code&gt;.</source>
          <target state="translated">각 키 대기 &lt;code&gt;keys&lt;/code&gt; 저장소에 추가하고, 키가 제공된가 설정되지 않은 경우 예외가 발생 될 &lt;code&gt;timeout&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="24a824e1b1fcd89978ae586133c59626ca034951" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store. If not all keys are set before the &lt;code&gt;timeout&lt;/code&gt; (set during store initialization), then &lt;code&gt;wait&lt;/code&gt; will throw an exception.</source>
          <target state="translated">각 키에 대한 대기 &lt;code&gt;keys&lt;/code&gt; 저장소에 추가 할 수 있습니다. &lt;code&gt;timeout&lt;/code&gt; 이전에 모든 키가 설정되지 않은 경우 (저장소 초기화 중에 설정 됨) &lt;code&gt;wait&lt;/code&gt; 는 예외를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="bf737f8268ade30b29f4e304a6f74e8dc126217a" translate="yes" xml:space="preserve">
          <source>Waits for the event to complete.</source>
          <target state="translated">이벤트가 완료 될 때까지 기다립니다.</target>
        </trans-unit>
        <trans-unit id="ff9074c4aa9ff896546792138df1d27c0f9655c8" translate="yes" xml:space="preserve">
          <source>Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.</source>
          <target state="translated">이 이벤트에서 현재 캡처 된 모든 작업이 완료 될 때까지 기다립니다. 이렇게하면 이벤트가 완료 될 때까지 CPU 스레드가 진행되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="322f5a9cd5158cc9dec2b3e7da850004249c9e4e" translate="yes" xml:space="preserve">
          <source>We accumulate the gradients in the appropriate &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;torch.distributed.autograd.context&lt;/code&gt;&lt;/a&gt; on each of the nodes. The autograd context to be used is looked up given the &lt;code&gt;context_id&lt;/code&gt; that is passed in when &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="translated">각 노드 의 적절한 &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;torch.distributed.autograd.context&lt;/code&gt; &lt;/a&gt; 에 그라디언트를 누적합니다 . 사용할 autograd 컨텍스트 는 &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt; 가 호출 될 때 전달 되는 &lt;code&gt;context_id&lt;/code&gt; 가 주어 지면 조회 됩니다. 주어진 ID에 해당하는 유효한 autograd 컨텍스트가 없으면 오류가 발생합니다. &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt; API를 사용하여 누적 된 그라디언트를 검색 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="db56b28279fe3aea1467a7671b69e8c448f2218c" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;torch.nn.Linear&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="3f8d51566ad98edb597de3ba663338465830f744" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="2e48e5f00566ef4a4b3bcb10eab8d225acffc2e4" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="a3fb7d904d476590d98359b7241880b1be2f9a76" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="9fb0293b1874dcfe3feb591ccd49a1edaf2d5f3f" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Conv2d&lt;/code&gt; 와 동일한 인터페이스를 채택합니다 . 문서 는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="27cfadeddc9160f1498530cc3eb1f719bfd8434a" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Linear&lt;/code&gt; 와 동일한 인터페이스를 채택합니다 . 문서 는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="5105f862983064b0cf7594afba8d668c8f56e9ff" translate="yes" xml:space="preserve">
          <source>We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:</source>
          <target state="translated">추적과 스크립팅을 혼합 할 수 있습니다. 모델 일부의 특정 요구 사항에 맞게 추적 및 스크립팅을 구성 할 수 있습니다. 이 예를 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="6defebafebb2e25f76bcf411ab1ec455c032f365" translate="yes" xml:space="preserve">
          <source>We also do not support the following subsystems, though some may work out of the box:</source>
          <target state="translated">다음 하위 시스템도 지원하지 않지만 일부 하위 시스템은 즉시 작동 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3fa9a188f6de578d3bee11e42ad3ca8321060293" translate="yes" xml:space="preserve">
          <source>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with &lt;code&gt;torch.cat&lt;/code&gt;:</source>
          <target state="translated">내부 업데이트를 사용하지 않도록 코드를 수정하여이 문제를 해결할 수 있습니다. 대신 &lt;code&gt;torch.cat&lt;/code&gt; 을 사용 하여 결과 텐서를 외부에 빌드합니다 .</target>
        </trans-unit>
        <trans-unit id="7aac442ad82fddd2074ccf19e4043ee60a7cdfb0" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt; 의 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="4304f8e1f80a53c4c4788f88c99ef4dcb6d0bc02" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;torch.nn.ReLU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;torch.nn.ReLU&lt;/code&gt; &lt;/a&gt; 의 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="94fba3bcda49d77ec84730c68d223d594ab23cb3" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt; 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="a068c518b2d5e22e14c479569a01c9c9d06bceec" translate="yes" xml:space="preserve">
          <source>We highly recommend taking a look at the original paper for more details.</source>
          <target state="translated">자세한 내용은 원본 논문을 참조하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="86ad4838215218ac000a7d87b4b48e01c6961c5f" translate="yes" xml:space="preserve">
          <source>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in &lt;code&gt;references/video_classification&lt;/code&gt;.</source>
          <target state="translated">Kinetics-400에 대해 사전 훈련 된 동작 인식 모델을 제공합니다. 그들은 모두 &lt;code&gt;references/video_classification&lt;/code&gt; 에 제공된 스크립트로 훈련되었습니다 .</target>
        </trans-unit>
        <trans-unit id="e921c07692e5a21fc62c5f94070c73cf433f0bb7" translate="yes" xml:space="preserve">
          <source>We provide pre-trained models, using the PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt;&lt;code&gt;torch.utils.model_zoo&lt;/code&gt;&lt;/a&gt;. These can be constructed by passing &lt;code&gt;pretrained=True&lt;/code&gt;:</source>
          <target state="translated">PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt; &lt;code&gt;torch.utils.model_zoo&lt;/code&gt; 를&lt;/a&gt; 사용하여 사전 학습 된 모델을 제공합니다 . &lt;code&gt;pretrained=True&lt;/code&gt; 전달하여 구성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="62c9fd377e2aad0f66ac5c26c69e0a5867347f36" translate="yes" xml:space="preserve">
          <source>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</source>
          <target state="translated">순수 Python 프로그램에서 독립형 C ++ 프로그램과 같이 Python과 독립적으로 실행할 수있는 TorchScript 프로그램으로 모델을 점진적으로 전환하는 도구를 제공합니다. 이를 통해 Python의 익숙한 도구를 사용하여 PyTorch에서 모델을 훈련 한 다음 TorchScript를 통해 Python 프로그램이 성능 및 멀티 스레딩 이유로 불리 할 수있는 프로덕션 환경으로 모델을 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bd73422aa80da87d8ae75185414e27781227d850" translate="yes" xml:space="preserve">
          <source>We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.</source>
          <target state="translated">제공된 루트를 사용하여 autograd 그래프를 찾고 적절한 종속성을 계산합니다. 이 메서드는 전체 autograd 계산이 완료 될 때까지 차단됩니다.</target>
        </trans-unit>
        <trans-unit id="7ea805e8dac2601210cb6d8d378a9d7e2e8c1197" translate="yes" xml:space="preserve">
          <source>Weibull</source>
          <target state="translated">Weibull</target>
        </trans-unit>
        <trans-unit id="48996658259127412cd98e4a4e7b4a204e6d1b0a" translate="yes" xml:space="preserve">
          <source>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by &lt;code&gt;name&lt;/code&gt; (e.g. &lt;code&gt;'weight'&lt;/code&gt;) with two parameters: one specifying the magnitude (e.g. &lt;code&gt;'weight_g'&lt;/code&gt;) and one specifying the direction (e.g. &lt;code&gt;'weight_v'&lt;/code&gt;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every &lt;code&gt;forward()&lt;/code&gt; call.</source>
          <target state="translated">가중치 정규화는 가중치 텐서의 크기를 방향에서 분리하는 재 매개 변수화입니다. 이는 &lt;code&gt;name&lt;/code&gt; 지정된 매개 변수 (예 : &lt;code&gt;'weight'&lt;/code&gt; )를 두 개의 매개 변수로 대체합니다 . 하나는 크기를 지정하는 매개 변수 (예 : &lt;code&gt;'weight_g'&lt;/code&gt; )와 다른 하나는 방향을 지정하는 매개 변수 (예 : &lt;code&gt;'weight_v'&lt;/code&gt; )입니다. 가중치 정규화는 모든 &lt;code&gt;forward()&lt;/code&gt; 호출 전에 크기와 방향에서 가중치 텐서를 다시 계산하는 후크를 통해 구현됩니다 .</target>
        </trans-unit>
        <trans-unit id="d53879f401fe4a3643952a897298dc95fdaf8d7e" translate="yes" xml:space="preserve">
          <source>Weight:</source>
          <target state="translated">Weight:</target>
        </trans-unit>
        <trans-unit id="769bb19e615b7f8e2809e5882e2d05a18f57a531" translate="yes" xml:space="preserve">
          <source>When</source>
          <target state="translated">When</target>
        </trans-unit>
        <trans-unit id="d4a914f8dcec6b172849d1c8fb16401fbdbb7604" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when &lt;code&gt;align_corners = False&lt;/code&gt;. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at &lt;code&gt;-1&lt;/code&gt;. From version 1.3.0, under &lt;code&gt;align_corners = True&lt;/code&gt; all grid points along a unit dimension are considered to be at &lt;code&gt;`0&lt;/code&gt; (the center of the input image).</source>
          <target state="translated">경우 &lt;code&gt;align_corners = True&lt;/code&gt; , 2 차원 데이터를 1 차원 데이터, 3 차원 아핀 변환에 2 차원 아핀 변환 (즉, 공간 차원의 하나의 단위 크기를 갖는 경우) 의도 된 사용 사례 불명확하고, 없다. &lt;code&gt;align_corners = False&lt;/code&gt; 문제가되지 않습니다 . 버전 1.2.0까지는 단위 치수를 따르는 모든 그리드 점이 임의로 &lt;code&gt;-1&lt;/code&gt; 로 간주되었습니다 . 버전 1.3.0부터 &lt;code&gt;align_corners = True&lt;/code&gt; 아래 에서 단위 치수를 따르는 모든 그리드 포인트는 &lt;code&gt;`0&lt;/code&gt; (입력 이미지의 중심)에있는 것으로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="71f1f4c1d858ca7758a6c256f77db87995af50dc" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was &lt;code&gt;align_corners = True&lt;/code&gt;. Since then, the default behavior has been changed to &lt;code&gt;align_corners = False&lt;/code&gt;, in order to bring it in line with the default for &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">경우 &lt;code&gt;align_corners = True&lt;/code&gt; 그리드 위치가 입력 화상의 사이즈, 화소 사이즈에 의존하고 의해 샘플링 위치되도록 &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt; 다른 해상도에서 주어진 동일한 입력 다르다 (즉, 업 샘플링 또는 다운 샘플링 된 후). 버전 1.2.0까지의 기본 동작은 &lt;code&gt;align_corners = True&lt;/code&gt; 입니다. 그 이후로 기본 동작은 &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;interpolate()&lt;/code&gt; &lt;/a&gt; 의 기본값에 &lt;code&gt;align_corners = False&lt;/code&gt; 위해 align_corners = False 로 변경되었습니다 .</target>
        </trans-unit>
        <trans-unit id="393f1e49e22eb1a104936dd30d39047e787303b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;batch_size&lt;/code&gt; (default &lt;code&gt;1&lt;/code&gt;) is not &lt;code&gt;None&lt;/code&gt;, the data loader yields batched samples instead of individual samples. &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;drop_last&lt;/code&gt; arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify &lt;code&gt;batch_sampler&lt;/code&gt;, which yields a list of keys at a time.</source>
          <target state="translated">경우 &lt;code&gt;batch_size&lt;/code&gt; (디폴트 &lt;code&gt;1&lt;/code&gt; )하지없는 &lt;code&gt;None&lt;/code&gt; 데이터 로더 수율 대신 개별 샘플의 샘플을 일괄. &lt;code&gt;batch_size&lt;/code&gt; 및 &lt;code&gt;drop_last&lt;/code&gt; 인수는 데이터 로더가 데이터 세트 키의 배치를 가져 오는 방법을 지정하는 데 사용됩니다. 맵 스타일 데이터 세트의 경우 사용자는 한 번에 키 목록을 생성하는 &lt;code&gt;batch_sampler&lt;/code&gt; 를 대신 지정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="19d86416250dfee41099689ccdc474523b9de05a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, backward cannot be performed since &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; from the forward pass is required for the backward operation.</source>
          <target state="translated">경우 &lt;code&gt;compute_uv&lt;/code&gt; 가 = &lt;code&gt;False&lt;/code&gt; , 후방 캔 이후 수행 될 &lt;code&gt;U&lt;/code&gt; 및 &lt;code&gt;V&lt;/code&gt; 순방향 패스에서는 역방향 동작에 요구된다.</target>
        </trans-unit>
        <trans-unit id="2e150d7511a776c744a811c73ef08944e9bb6434" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;dim&lt;/code&gt; is given, a squeeze operation is done only in the given dimension. If &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">때 &lt;code&gt;dim&lt;/code&gt; 주어집니다, 스퀴즈 작업은 주어진 차원에서 이루어집니다. 경우 &lt;code&gt;input&lt;/code&gt; 형상이다 :</target>
        </trans-unit>
        <trans-unit id="b1120f7dfd992f23bbea9418ec57a1af2fe9af61" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a scalar value, the operation applied is:</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 스칼라 값이고, 동작은 적용 :</target>
        </trans-unit>
        <trans-unit id="66481b64a8fde71d4312aa50639d9ce101db880a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the operation applied is:</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 텐서이고, 동작은 적용 :</target>
        </trans-unit>
        <trans-unit id="25f39b6b9d40581f69f611ee72f68c18106be3d1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;exponent&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 텐서는,의 형태 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;exponent&lt;/code&gt; 있어야 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2903dec45ead9638a2d9f01c40302928711917b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;groups == in_channels&lt;/code&gt; and &lt;code&gt;out_channels == K * in_channels&lt;/code&gt;, where &lt;code&gt;K&lt;/code&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</source>
          <target state="translated">경우 &lt;code&gt;groups == in_channels&lt;/code&gt; 및 &lt;code&gt;out_channels == K * in_channels&lt;/code&gt; , &lt;code&gt;K&lt;/code&gt; 는 양의 정수이고,이 동작은 또한 깊이 방향으로 콘볼 루션 문헌 불린다.</target>
        </trans-unit>
        <trans-unit id="c1b8e1328795e35aa33cb3682bf9e0a128163cf6" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;input&lt;/code&gt; is on CUDA, &lt;a href=&quot;#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt; causes host-device synchronization.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 이 CUDA에 있을 때 &lt;a href=&quot;#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; &lt;/a&gt; 는 호스트-장치 동기화를 유발합니다.</target>
        </trans-unit>
        <trans-unit id="9fd8de791261b8fe5e3fd820c55e05ef945d4e88" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;module&lt;/code&gt; returns a scalar (i.e., 0-dimensional tensor) in &lt;code&gt;forward()&lt;/code&gt;, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</source>
          <target state="translated">되면 &lt;code&gt;module&lt;/code&gt; 의 스칼라 (즉, 0 차원 텐서)를 리턴 &lt;code&gt;forward()&lt;/code&gt; ,이 랩퍼는 각 장치의 결과를 포함하는, 데이터를 병렬로 사용되는 장치들의 수와 동일한 길이의 벡터를 리턴한다.</target>
        </trans-unit>
        <trans-unit id="14811e48d579473d679dcda9d3180b5365c2423a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shape of &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor</source>
          <target state="translated">때 &lt;code&gt;other&lt;/code&gt; 텐서이며, 모양 &lt;code&gt;other&lt;/code&gt; 있어야합니다 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; 기본 텐서의 모양</target>
        </trans-unit>
        <trans-unit id="6d6234d9ff0cd3ff058351e570bff6a3e90e8d1e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;other&lt;/code&gt; 텐서의 모양입니다 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; 해야합니다 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b23daf961076c6f771f3b98153804afb65cab2e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;size&lt;/code&gt; is given, it is the output size of the image &lt;code&gt;(h, w)&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;size&lt;/code&gt; 주어, 영상의 출력 크기 &lt;code&gt;(h, w)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d240a83215b54cebb671916345cdadc2427a5a89" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, the gradients on &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; and &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</source>
          <target state="translated">경우 &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 에서 그라디언트 &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; 및 &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; 이들 벡터로 후방에서 무시 될 것이다 수 부분 공간의 임의의 기준입니다.</target>
        </trans-unit>
        <trans-unit id="85caf16edf789cb76e0cd678a0b93ec725f0105b" translate="yes" xml:space="preserve">
          <source>When a model is trained on &lt;code&gt;M&lt;/code&gt; nodes with &lt;code&gt;batch=N&lt;/code&gt;, the gradient will be &lt;code&gt;M&lt;/code&gt; times smaller when compared to the same model trained on a single node with &lt;code&gt;batch=M*N&lt;/code&gt; (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart.</source>
          <target state="translated">모델이 &lt;code&gt;batch=N&lt;/code&gt; 인 &lt;code&gt;M&lt;/code&gt; 노드에서 학습 된 경우, 단일 노드에서 &lt;code&gt;batch=M*N&lt;/code&gt; 으로 학습 된 동일한 모델과 비교할 때 기울기가 &lt;code&gt;M&lt;/code&gt; 배 더 작아집니다 (다른 노드 간의 기울기가 평균화되기 때문). 로컬 교육과 비교하여 수학적으로 동등한 교육 과정을 얻으려면이 점을 고려해야합니다.</target>
        </trans-unit>
        <trans-unit id="794b9d3ddf3eff59a51409ca459125a99df4910e" translate="yes" xml:space="preserve">
          <source>When a non-sparse &lt;code&gt;param&lt;/code&gt; receives a non-sparse gradient during &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;torch.Tensor.backward()&lt;/code&gt;&lt;/a&gt;&lt;code&gt;param.grad&lt;/code&gt; is accumulated as follows.</source>
          <target state="translated">비 희소 &lt;code&gt;param&lt;/code&gt; &lt;a href=&quot;#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;torch.Tensor.backward()&lt;/code&gt; &lt;/a&gt; 동안 비 희소 그라디언트를 수신하면 &lt;code&gt;param.grad&lt;/code&gt; 는 다음과 같이 누적됩니다.</target>
        </trans-unit>
        <trans-unit id="746e7941b779e1590393a8195388add16038ba46" translate="yes" xml:space="preserve">
          <source>When a subclass is used with &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, each item in the dataset will be yielded from the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; iterator. When &lt;code&gt;num_workers &amp;gt; 0&lt;/code&gt;, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;get_worker_info()&lt;/code&gt;&lt;/a&gt;, when called in a worker process, returns information about the worker. It can be used in either the dataset&amp;rsquo;s &lt;code&gt;__iter__()&lt;/code&gt; method or the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; &amp;lsquo;s &lt;code&gt;worker_init_fn&lt;/code&gt; option to modify each copy&amp;rsquo;s behavior.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 와 함께 하위 클래스를 사용 하면 데이터 집합의 각 항목이 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 반복기 에서 생성됩니다 . 때 &lt;code&gt;num_workers &amp;gt; 0&lt;/code&gt; 이 종종 노동자에서 반환 된 데이터 중복을 피하기 위해 서로 독립적 사본을 구성하는 것이 바람직하므로, 각 작업자 프로세스는 DataSet 개체의 다른 복사본이있을 것이다. &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt; &lt;code&gt;get_worker_info()&lt;/code&gt; &lt;/a&gt; 는 작업자 프로세스에서 호출 될 때 작업자에 대한 정보를 반환합니다. 데이터 세트의 &lt;code&gt;__iter__()&lt;/code&gt; 메서드 또는 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;worker_init_fn&lt;/code&gt; 옵션에서 사용하여 각 복사본의 동작을 수정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4794503b3573f4db7c79a8a74163748fdeea10ab" translate="yes" xml:space="preserve">
          <source>When both &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;batch_sampler&lt;/code&gt; are &lt;code&gt;None&lt;/code&gt; (default value for &lt;code&gt;batch_sampler&lt;/code&gt; is already &lt;code&gt;None&lt;/code&gt;), automatic batching is disabled. Each sample obtained from the &lt;code&gt;dataset&lt;/code&gt; is processed with the function passed as the &lt;code&gt;collate_fn&lt;/code&gt; argument.</source>
          <target state="translated">두 경우 &lt;code&gt;batch_size&lt;/code&gt; 및 &lt;code&gt;batch_sampler&lt;/code&gt; 가 있습니다 &lt;code&gt;None&lt;/code&gt; (대한 기본 값 &lt;code&gt;batch_sampler&lt;/code&gt; 은 이미 없다 &lt;code&gt;None&lt;/code&gt; ), 자동 배치를 사용할 수 없습니다. &lt;code&gt;dataset&lt;/code&gt; 에서 얻은 각 샘플 은 &lt;code&gt;collate_fn&lt;/code&gt; 인수 로 전달 된 함수로 처리됩니다 .</target>
        </trans-unit>
        <trans-unit id="83b686ad466f2edcefa11e742fdd44b512b0dd23" translate="yes" xml:space="preserve">
          <source>When called in a worker, this returns an object guaranteed to have the following attributes:</source>
          <target state="translated">워커에서 호출되면 다음 속성이 보장되는 객체를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b6402d4d143ce0d83ed8c31ca1e7c6b83b35fb53" translate="yes" xml:space="preserve">
          <source>When called in the main process, this returns &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">메인 프로세스에서 호출되면 &lt;code&gt;None&lt;/code&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="bc06fbb059ebdabdac77c068a81b8512376f9ddd" translate="yes" xml:space="preserve">
          <source>When called with &lt;code&gt;dims&lt;/code&gt; of the list form, the given dimensions will be contracted in place of the last</source>
          <target state="translated">목록 양식의 &lt;code&gt;dims&lt;/code&gt; 부분으로 호출되면 주어진 크기가 마지막 크기 대신 축소됩니다.</target>
        </trans-unit>
        <trans-unit id="8f0991d5226cadceec16a5e2277a3d90d9a71503" translate="yes" xml:space="preserve">
          <source>When called with a non-negative integer argument &lt;code&gt;dims&lt;/code&gt; =</source>
          <target state="translated">음이 아닌 정수 인수로 호출 할 때 &lt;code&gt;dims&lt;/code&gt; =</target>
        </trans-unit>
        <trans-unit id="6abce014373fbb8ba1a8a6700a958f442096752d" translate="yes" xml:space="preserve">
          <source>When combined with TorchScript decorators, this decorator must be the outmost one.</source>
          <target state="translated">TorchScript 데코레이터와 결합 할 때이 데코레이터는 가장 바깥쪽에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="01692078c2de1ab235032349659ea1bad48180a7" translate="yes" xml:space="preserve">
          <source>When combined with static or class method, this decorator must be the inner one.</source>
          <target state="translated">정적 또는 클래스 메소드와 결합 할 때이 데코레이터는 내부 데코레이터 여야합니다.</target>
        </trans-unit>
        <trans-unit id="02e71c552b6cfbf5a63e34935049e68d7d217f7f" translate="yes" xml:space="preserve">
          <source>When creating a new &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt;, the following methods are available to &lt;code&gt;ctx&lt;/code&gt;.</source>
          <target state="translated">새 &lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt; 생성 할 때 &lt;code&gt;ctx&lt;/code&gt; 에 다음 메서드를 사용할 수 있습니다 . ㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="24460dec44dc8e232523a0b3b6cffc083f18ab5b" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt;&lt;code&gt;new_tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">데이터가 텐서 &lt;code&gt;x&lt;/code&gt; 이면 &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt; &lt;code&gt;new_tensor()&lt;/code&gt; &lt;/a&gt; 는 전달 된 데이터에서 '데이터'를 읽고 리프 변수를 생성합니다. 따라서 &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; 는 &lt;code&gt;x.clone().detach()&lt;/code&gt; 와 &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; 하고 tensor.new_tensor (x, requires_grad = True) 는 &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; 합니다. &lt;code&gt;clone()&lt;/code&gt; 및 &lt;code&gt;detach()&lt;/code&gt; 사용하는 동일한 방법 이 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="44313813a50d98d5898f4bddc561c8a3109b1400" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;torch.tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">데이터가 텐서 &lt;code&gt;x&lt;/code&gt; 인 경우 &lt;a href=&quot;#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; 는 전달 된 데이터에서 '데이터'를 읽고 리프 변수를 생성합니다. 따라서 &lt;code&gt;torch.tensor(x)&lt;/code&gt; 는 &lt;code&gt;x.clone().detach()&lt;/code&gt; 와 &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; 하고 torch.tensor (x, requires_grad = True) 는 &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; 합니다. &lt;code&gt;clone()&lt;/code&gt; 및 &lt;code&gt;detach()&lt;/code&gt; 사용하는 동일한 방법 이 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="4d87a694539e44d9bb0922817749f14565db02a2" translate="yes" xml:space="preserve">
          <source>When drawn without replacement, &lt;code&gt;num_samples&lt;/code&gt; must be lower than number of non-zero elements in &lt;code&gt;input&lt;/code&gt; (or the min number of non-zero elements in each row of &lt;code&gt;input&lt;/code&gt; if it is a matrix).</source>
          <target state="translated">대체하지 않고 그릴 때 &lt;code&gt;num_samples&lt;/code&gt; 는 &lt;code&gt;input&lt;/code&gt; 에서 0이 아닌 요소의 수 (또는 행렬 인 경우 &lt;code&gt;input&lt;/code&gt; 각 행에있는 0이 아닌 요소의 최소 수) 보다 작아야 합니다 .</target>
        </trans-unit>
        <trans-unit id="0bdf0c7df4d55ef87659ffa5921e0c1608596af5" translate="yes" xml:space="preserve">
          <source>When entering an autocast-enabled region, Tensors may be any type. You should not call &lt;code&gt;.half()&lt;/code&gt; on your model(s) or inputs when using autocasting.</source>
          <target state="translated">자동 캐스트 활성화 지역에 들어갈 때 Tensor는 모든 유형이 될 수 있습니다. &lt;code&gt;.half()&lt;/code&gt; 을 사용할 때 모델 또는 입력에 대해 .half () 를 호출하면 안됩니다 .</target>
        </trans-unit>
        <trans-unit id="52b1d0543e21755317d78418469b43a9d3b74143" translate="yes" xml:space="preserve">
          <source>When fetching from &lt;a href=&quot;#iterable-style-datasets&quot;&gt;iterable-style datasets&lt;/a&gt; with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;multi-processing&lt;/a&gt;, the &lt;code&gt;drop_last&lt;/code&gt; argument drops the last non-full batch of each worker&amp;rsquo;s dataset replica.</source>
          <target state="translated">에서 가져 오는 경우 &lt;a href=&quot;#iterable-style-datasets&quot;&gt;반복 가능한 스타일의 데이터 세트&lt;/a&gt; 와 &lt;a href=&quot;#multi-process-data-loading&quot;&gt;멀티 프로세싱&lt;/a&gt; 의 &lt;code&gt;drop_last&lt;/code&gt; 인수는 각 근로자의 데이터 세트 복제의 마지막이 아닌 전체 일괄 삭제합니다.</target>
        </trans-unit>
        <trans-unit id="aeab232209859fdb56ecfdd5e27289109b3779cd" translate="yes" xml:space="preserve">
          <source>When given an image of &lt;code&gt;Channels x Height x Width&lt;/code&gt;, it will apply &lt;code&gt;Softmax&lt;/code&gt; to each location</source>
          <target state="translated">&lt;code&gt;Channels x Height x Width&lt;/code&gt; 의 이미지가 주어지면 각 위치에 &lt;code&gt;Softmax&lt;/code&gt; 가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="722286111baf501df8f66886a8878dbc93c4e6e4" translate="yes" xml:space="preserve">
          <source>When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:</source>
          <target state="translated">last_epoch = -1이면 초기 lr을 lr로 설정합니다. 일정이 재귀 적으로 정의되기 때문에 학습률은 다른 운영자가이 스케줄러 외부에서 동시에 수정할 수 있습니다. 학습률이이 스케줄러에 의해서만 설정되는 경우 각 단계의 학습률은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9635ed5ff58b5f10f0a861bfe4da315b19a3d9ec" translate="yes" xml:space="preserve">
          <source>When manually importing this backend and invoking &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; with the corresponding backend name, the &lt;code&gt;torch.distributed&lt;/code&gt; package runs on the new backend.</source>
          <target state="translated">이 백엔드를 수동으로 가져오고 해당 백엔드 이름으로 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; 을 호출 하면 &lt;code&gt;torch.distributed&lt;/code&gt; 패키지가 새 백엔드에서 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="6c58952c2db520eb2d0c000e5b5179ffccb22124" translate="yes" xml:space="preserve">
          <source>When passed to the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script&lt;/code&gt;&lt;/a&gt; function, a &lt;code&gt;torch.nn.Module&lt;/code&gt;&amp;rsquo;s data is copied to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and the TorchScript compiler compiles the module. The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;, as well as any &lt;code&gt;@torch.jit.export&lt;/code&gt; methods.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수에 전달되면 &lt;code&gt;torch.nn.Module&lt;/code&gt; 의 데이터가 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 에&lt;/a&gt; 복사되고 TorchScript 컴파일러가 모듈을 컴파일합니다. 모듈의 &lt;code&gt;forward&lt;/code&gt; 는 기본적으로 컴파일됩니다. &lt;code&gt;forward&lt;/code&gt; 에서 호출 된 메서드 는 &lt;code&gt;forward&lt;/code&gt; 및 &lt;code&gt;@torch.jit.export&lt;/code&gt; 메서드 에서 사용되는 순서대로 느리게 컴파일됩니다 .</target>
        </trans-unit>
        <trans-unit id="98975218294f0d2cf1d9e027d0c9f18c335eccc0" translate="yes" xml:space="preserve">
          <source>When running on CUDA, &lt;code&gt;row * col&lt;/code&gt; must be less than</source>
          <target state="translated">CUDA에서 실행할 때 &lt;code&gt;row * col&lt;/code&gt; 은 다음보다 작아야합니다.</target>
        </trans-unit>
        <trans-unit id="6442e147505eb082b7d915b41a9300ebc0cdaab5" translate="yes" xml:space="preserve">
          <source>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</source>
          <target state="translated">scale_factor가 지정되면 recompute_scale_factor = True이면 scale_factor를 사용하여 output_size를 계산 한 다음 보간을위한 새 스케일을 추론하는 데 사용됩니다. recompute_scale_factor의 기본 동작은 1.6.0에서 False로 변경되었으며, 보간 계산에 scale_factor가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6134dc991dfe53224afc4b25750c288358d06db2" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;divisor&lt;/code&gt; tensor contains no zero elements, then &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;unfold&lt;/code&gt; operations are inverses of each other (up to constant divisor).</source>
          <target state="translated">때 &lt;code&gt;divisor&lt;/code&gt; 텐서 더 제로 요소가없는 다음 &lt;code&gt;fold&lt;/code&gt; 와 &lt;code&gt;unfold&lt;/code&gt; 작업 (정수 제수까지) 서로의 역수이다.</target>
        </trans-unit>
        <trans-unit id="ea57553addc94dbd66c0aab4dca41d371d835f2e" translate="yes" xml:space="preserve">
          <source>When the dtypes of inputs to an arithmetic operation (&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;) differ, we promote by finding the minimum dtype that satisfies the following rules:</source>
          <target state="translated">산술 연산 ( &lt;code&gt;add&lt;/code&gt; , &lt;code&gt;sub&lt;/code&gt; , &lt;code&gt;div&lt;/code&gt; , &lt;code&gt;mul&lt;/code&gt; ) 에 대한 입력의 dtype이 다를 경우 다음 규칙을 충족하는 최소 dtype을 찾아 승격합니다.</target>
        </trans-unit>
        <trans-unit id="b36d5c15d9cebdd9841a3cbecf930ef5be3e8fb3" translate="yes" xml:space="preserve">
          <source>When the input Tensor is a sparse tensor then the unspecifed values are treated as &lt;code&gt;-inf&lt;/code&gt;.</source>
          <target state="translated">입력 텐서가 희소 텐서이면 지정되지 않은 값은 &lt;code&gt;-inf&lt;/code&gt; 로 처리됩니다 .</target>
        </trans-unit>
        <trans-unit id="0306aa2f6b60a04c04ba2bb3e8bec0e8ed79e4a2" translate="yes" xml:space="preserve">
          <source>When the probability density function is differentiable with respect to its parameters, we only need &lt;code&gt;sample()&lt;/code&gt; and &lt;code&gt;log_prob()&lt;/code&gt; to implement REINFORCE:</source>
          <target state="translated">확률 밀도 함수가 매개 변수와 관련하여 미분 할 수있는 경우 REINFORCE를 구현 하려면 &lt;code&gt;sample()&lt;/code&gt; 및 &lt;code&gt;log_prob()&lt;/code&gt; 만 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="adc8c2e11d80dad257b6a86b35591121148d959c" translate="yes" xml:space="preserve">
          <source>When the shapes do not match, the shape of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is used as the shape for the returned output tensor</source>
          <target state="translated">모양이 일치하지 않으면 반환 된 출력 텐서 의 모양으로 &lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt; 의 모양 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0836fba3472a07dd87c4cfcc80c3a0f1b5a7eda0" translate="yes" xml:space="preserve">
          <source>When used in a &lt;code&gt;worker_init_fn&lt;/code&gt; passed over to &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;, this method can be useful to set up each worker process differently, for instance, using &lt;code&gt;worker_id&lt;/code&gt; to configure the &lt;code&gt;dataset&lt;/code&gt; object to only read a specific fraction of a sharded dataset, or use &lt;code&gt;seed&lt;/code&gt; to seed other libraries used in dataset code (e.g., NumPy).</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; 에&lt;/a&gt; 전달 된 &lt;code&gt;worker_init_fn&lt;/code&gt; 에서 사용되는 경우이 메서드는 예를 들어 &lt;code&gt;worker_id&lt;/code&gt; 를 사용하여 분할 된 &lt;code&gt;dataset&lt;/code&gt; 의 특정 부분 만 읽도록 데이터 집합 개체를 구성 하거나 &lt;code&gt;seed&lt;/code&gt; 를 사용 하여 다른 사람을 시드하는 등 각 작업자 프로세스를 다르게 설정하는 데 유용 할 수 있습니다. 데이터 세트 코드에 사용되는 라이브러리 (예 : NumPy).</target>
        </trans-unit>
        <trans-unit id="7a635c724cadbffe9cd8e217f6606360c059e6c0" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt;&lt;code&gt;BuildExtension&lt;/code&gt;&lt;/a&gt;, it is allowed to supply a dictionary for &lt;code&gt;extra_compile_args&lt;/code&gt; (rather than the usual list) that maps from languages (&lt;code&gt;cxx&lt;/code&gt; or &lt;code&gt;nvcc&lt;/code&gt;) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt; &lt;code&gt;BuildExtension&lt;/code&gt; 을&lt;/a&gt; 사용할 때 언어 ( &lt;code&gt;cxx&lt;/code&gt; 또는 &lt;code&gt;nvcc&lt;/code&gt; )에서 컴파일러에 제공 할 추가 컴파일러 플래그 목록으로 매핑되는 &lt;code&gt;extra_compile_args&lt;/code&gt; (일반 목록이 아닌)에 대한 사전 을 제공 할 수 있습니다. 이렇게하면 혼합 컴파일 중에 C ++ 및 CUDA 컴파일러에 다른 플래그를 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eb9c51be75e99cadf6963fd3f79e89b1c37968c6" translate="yes" xml:space="preserve">
          <source>When using an &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;multi-process data loading&lt;/a&gt;. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; documentations for how to achieve this.</source>
          <target state="translated">&lt;a href=&quot;#multi-process-data-loading&quot;&gt;다중 프로세스 데이터로드&lt;/a&gt; 와 함께 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 을 사용하는 경우 . 동일한 데이터 세트 개체가 각 작업자 프로세스에 복제되므로 데이터 중복을 방지하려면 복제본을 다르게 구성해야합니다. 이를 달성하는 방법 은 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 문서를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7635dd8102bfabbafa2a3b5c97436e5cb54d9ba1" translate="yes" xml:space="preserve">
          <source>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">CUDA 백엔드를 사용할 때이 작업은 쉽게 꺼지지 않는 역방향 패스에서 비 결정적 동작을 유발할 수 있습니다. 배경 은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;재현성에&lt;/a&gt; 대한 참고 사항을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="54dc7a473943cf2733a6d4da75adee3f4a210bcf" translate="yes" xml:space="preserve">
          <source>When viewing a profile created using &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt; in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt; appends sequence number information to the ranges it generates.</source>
          <target state="translated">Nvidia Visual Profiler에서 &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; &lt;/a&gt; 를 사용하여 생성 된 프로필을 볼 때 각 역방향 통과 작업을 해당하는 순방향 통과 작업과 연관시키는 것이 어려울 수 있습니다. 이 작업을 쉽게하기 위해 &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; &lt;/a&gt; 는 생성하는 범위에 시퀀스 번호 정보를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="79561f3cf985a5e4719b8247ff531adef7e29706" translate="yes" xml:space="preserve">
          <source>When writing TorchScript directly using &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See &lt;code&gt;Builtin Functions&lt;/code&gt; for a complete reference of available Pytorch tensor methods, modules, and functions.</source>
          <target state="translated">&lt;code&gt;@torch.jit.script&lt;/code&gt; 데코레이터를 사용하여 직접 TorchScript를 작성할 때 프로그래머는 TorchScript에서 지원되는 Python의 하위 집합 만 사용해야합니다. 이 섹션에서는 독립 실행 형 언어에 대한 언어 참조 인 것처럼 TorchScript에서 지원되는 내용을 문서화합니다. 이 참조에서 언급되지 않은 Python의 모든 기능은 TorchScript의 일부가 아닙니다. 사용 가능한 Pytorch 텐서 메서드, 모듈 및 함수에 대한 전체 참조는 &lt;code&gt;Builtin Functions&lt;/code&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="50bf66ea03e8d3f7a8a4f3f33c0d423a99cbb251" translate="yes" xml:space="preserve">
          <source>When you call &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; and then &lt;code&gt;load_state_dict()&lt;/code&gt; to avoid GPU RAM surge when loading a model checkpoint.</source>
          <target state="translated">GPU 텐서를 포함하는 파일에서 &lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; 를 호출하면 해당 텐서가 기본적으로 GPU에로드됩니다. 당신은 호출 할 수 있습니다 &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; 다음 &lt;code&gt;load_state_dict()&lt;/code&gt; 모델의 체크 포인트를로드 할 때 피하기 GPU의 RAM의 급증을.</target>
        </trans-unit>
        <trans-unit id="d4b9a73359398be175a62d4f2cbf23eca1eb7394" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 FFT 크기입니다. 동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환 사이 에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="540833bd1b024f45b94c5fe10bdcce398da41e2a" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 FFT 크기입니다. 동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="4fa609678777cb0f1d2bea5c36804f3e7f699ab3" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 IFFT 크기입니다. 동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="983301015460563605882b2d43f60b6f31ca13f7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 IFFT 크기입니다. 동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환 사이 에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="dcabf659c01386b96c4252ad5247bb9f50b66574" translate="yes" xml:space="preserve">
          <source>Where are my downloaded models saved?</source>
          <target state="translated">다운로드 한 모델은 어디에 저장됩니까?</target>
        </trans-unit>
        <trans-unit id="017d4a3464bb8236ca4dfe2a53c2f262d76de8ab" translate="yes" xml:space="preserve">
          <source>Which backend to use?</source>
          <target state="translated">사용할 백엔드는 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="3631991286d3139c1de9a41ef4982f8e5d886ce2" translate="yes" xml:space="preserve">
          <source>Which produces:</source>
          <target state="translated">어느 생산 :</target>
        </trans-unit>
        <trans-unit id="a68a67a970d91d390715c4a5723442211582200e" translate="yes" xml:space="preserve">
          <source>While Loops</source>
          <target state="translated">While 루프</target>
        </trans-unit>
        <trans-unit id="48f3d5bde3297b3b923d0f7759d754056d7f6c40" translate="yes" xml:space="preserve">
          <source>While it is assumed that &lt;code&gt;A&lt;/code&gt; is symmetric, &lt;code&gt;A.grad&lt;/code&gt; is not. To make sure that &lt;code&gt;A.grad&lt;/code&gt; is symmetric, so that &lt;code&gt;A - t * A.grad&lt;/code&gt; is symmetric in first-order optimization routines, prior to running &lt;code&gt;lobpcg&lt;/code&gt; we do the following symmetrization map: &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt;. The map is performed only when the &lt;code&gt;A&lt;/code&gt; requires gradients.</source>
          <target state="translated">&lt;code&gt;A&lt;/code&gt; 가 대칭 이라고 가정하지만 &lt;code&gt;A.grad&lt;/code&gt; 는 그렇지 않습니다. &lt;code&gt;A.grad&lt;/code&gt; 가 대칭 인지 확인하기 위해 &lt;code&gt;A - t * A.grad&lt;/code&gt; 가 1 차 최적화 루틴에서 대칭이되도록 &lt;code&gt;lobpcg&lt;/code&gt; 를 실행하기 전에 다음 대칭 맵을 수행합니다. &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt; . 맵은 &lt;code&gt;A&lt;/code&gt; 에 그라디언트가 필요할 때만 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="f69597c76f979c6f47613f7588234a4093ae500a" translate="yes" xml:space="preserve">
          <source>While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.</source>
          <target state="translated">항상 유효한 분해를 제공해야하지만 플랫폼간에 동일한 분해를 제공하지 않을 수 있습니다. 이는 LAPACK 구현에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="90f928cf1ec6eb30bffa5d56a4ee00f33a26ccf6" translate="yes" xml:space="preserve">
          <source>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</source>
          <target state="translated">수학적으로 log (softmax (x))와 동일하지만이 두 작업을 개별적으로 수행하는 것은 더 느리고 수치 적으로 불안정합니다. 이 함수는 대체 공식을 사용하여 출력과 기울기를 올바르게 계산합니다.</target>
        </trans-unit>
        <trans-unit id="adfc4c1cf043279d74d69b96592d62572f9a8648" translate="yes" xml:space="preserve">
          <source>Wide ResNet</source>
          <target state="translated">넓은 ResNet</target>
        </trans-unit>
        <trans-unit id="eec7e605ef42b4a1da3f9a4494e1414462efb313" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2</source>
          <target state="translated">넓은 ResNet-101-2</target>
        </trans-unit>
        <trans-unit id="ac86db1353b797ed1c0d73605a56146ab6cb1914" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&quot;Wide Residual Networks&quot;의&lt;/a&gt; Wide ResNet-101-2 모델</target>
        </trans-unit>
        <trans-unit id="b631c349f7132ef6c2a8879b09c961b30fce8aba" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2</source>
          <target state="translated">넓은 ResNet-50-2</target>
        </trans-unit>
        <trans-unit id="726f31b3e4c15ecbbc898cae3f9e8f73a3460020" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&quot;Wide Residual Networks&quot;의&lt;/a&gt; Wide ResNet-50-2 모델</target>
        </trans-unit>
        <trans-unit id="d2b826d3f7d8e2201135c671569ea283afb245af" translate="yes" xml:space="preserve">
          <source>Will result in:</source>
          <target state="translated">결과는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8fa871c4385dea4733bc80aea73b5f5364e4ef06" translate="yes" xml:space="preserve">
          <source>Windows FAQ</source>
          <target state="translated">Windows FAQ</target>
        </trans-unit>
        <trans-unit id="8ac66d0e68a85c8d292fec91f6cbcf0bd809b4e9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;bilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;bilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt; 샘플을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="05c8bfe5d3e24898d75d1b114e7f0550313f9961" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See below for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;linear&lt;/code&gt; , &lt;code&gt;bilinear&lt;/code&gt; , &lt;code&gt;bicubic&lt;/code&gt; , 및 &lt;code&gt;trilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 아래를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="9b1242167a707b16c47db2fe5f3c0130b4361833" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;linear&lt;/code&gt; , &lt;code&gt;bilinear&lt;/code&gt; 및 &lt;code&gt;trilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt; 샘플을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="343e3c563441d98e78b2af38a8ee16b2107d4b5b" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;mode='bicubic'&lt;/code&gt;, it&amp;rsquo;s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; if you want to reduce the overshoot when displaying the image.</source>
          <target state="translated">로 &lt;code&gt;mode='bicubic'&lt;/code&gt; , 이것은 오버 슈트의 원인 말하면 음수 값 또는 값보다 큰 이미지를 255 생성 할 수있다. 이미지를 표시 할 때 오버 슈트를 줄이려면 명시 적으로 &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; 호출 하십시오.</target>
        </trans-unit>
        <trans-unit id="670f4308b71dfa44f0ce3d603252c9a80431b7a1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;padding_idx&lt;/code&gt; set, the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from &lt;a href=&quot;#torch.nn.Embedding&quot;&gt;&lt;code&gt;Embedding&lt;/code&gt;&lt;/a&gt; is always zero.</source>
          <target state="translated">&lt;code&gt;padding_idx&lt;/code&gt; 를 설정 하면 padding_idx 의 임베딩 벡터 &lt;code&gt;padding_idx&lt;/code&gt; 모두 0으로 초기화됩니다. 그러나이 벡터는 사용자 정의 된 초기화 방법을 사용하여 나중에 수정할 수 있으므로 출력을 채우는 데 사용되는 벡터를 변경할 수 있습니다. &lt;a href=&quot;#torch.nn.Embedding&quot;&gt; &lt;code&gt;Embedding&lt;/code&gt; &lt;/a&gt; 의이 벡터에 대한 기울기 는 항상 0입니다.</target>
        </trans-unit>
        <trans-unit id="245747fecda85d5b71d626b9e2eb04627690bcb2" translate="yes" xml:space="preserve">
          <source>With &lt;em&gt;trace-based&lt;/em&gt; exporter, we get the result ONNX graph which unrolls the for loop:</source>
          <target state="translated">&lt;em&gt;추적 기반&lt;/em&gt; 내보내기를 사용 하면 for 루프를 펼치는 결과 ONNX 그래프를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="19fffb59deac1debbb0ed0b605bbe04aebbddb29" translate="yes" xml:space="preserve">
          <source>With the default arguments it uses the Euclidean norm over vectors along dimension</source>
          <target state="translated">기본 인수를 사용하면 차원을 따라 벡터에 대한 유클리드 노름을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="6aa512cf7aad097706fd2890b5de1202e2c6a97c" translate="yes" xml:space="preserve">
          <source>Within a Python process, the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;Global Interpreter Lock (GIL)&lt;/a&gt; prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument &lt;code&gt;num_workers&lt;/code&gt; to a positive integer.</source>
          <target state="translated">Python 프로세스 내에서 &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL (Global Interpreter Lock)&lt;/a&gt; 은 스레드간에 Python 코드를 완전히 병렬화하는 것을 방지합니다. 데이터로드로 계산 코드를 차단하는 것을 방지하기 위해 PyTorch는 단순히 인수 &lt;code&gt;num_workers&lt;/code&gt; 를 양의 정수로 설정하여 다중 프로세스 데이터로드를 수행 할 수있는 쉬운 전환을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="67793521614b6b44f958c708ad8988aeadfd2309" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length in the last dimension:</source>
          <target state="translated">출력 길이를 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 지정하지 않으면 입력이 마지막 차원에서 홀수 길이이기 때문에 출력이 제대로 왕복되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4263d953681d9ae30565f30202d1492e42e9e8ac" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length:</source>
          <target state="translated">출력 길이를 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 지정하지 않으면 입력이 홀수 길이이기 때문에 출력이 제대로 왕복되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2f5f94e39ac0f29eec4ef83cffba8ff93856c6cc" translate="yes" xml:space="preserve">
          <source>Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.</source>
          <target state="translated">작업자는 반복 끝에 도달하거나 반복기가 가비지 수집이되면 종료됩니다.</target>
        </trans-unit>
        <trans-unit id="275f19f1f1857e51454ed2397868a56594f54c36" translate="yes" xml:space="preserve">
          <source>Working with &lt;code&gt;collate_fn&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;collate_fn&lt;/code&gt; 작업</target>
        </trans-unit>
        <trans-unit id="7322e6cb1692d568f796bae37f8f8678f8c44133" translate="yes" xml:space="preserve">
          <source>Wrap most of you main script&amp;rsquo;s code within &lt;code&gt;if __name__ == '__main__':&lt;/code&gt; block, to make sure it doesn&amp;rsquo;t run again (most likely generating error) when each worker process is launched. You can place your dataset and &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; instance creation logic here, as it doesn&amp;rsquo;t need to be re-executed in workers.</source>
          <target state="translated">&lt;code&gt;if __name__ == '__main__':&lt;/code&gt; block 안에 대부분의 메인 스크립트 코드를 래핑 하여 각 작업자 프로세스가 시작될 때 다시 실행되지 않도록 (오류 발생 가능성이 가장 높음) 확인합니다. 작업자에서 다시 실행할 필요가 없으므로 여기에 데이터 세트 및 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 인스턴스 생성 로직을 배치 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1ecad6be71c469e57d25325f17d76f3b6c7e6652" translate="yes" xml:space="preserve">
          <source>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</source>
          <target state="translated">유효한 qconfig가있는 경우 리프 자식 모듈을 QuantWrapper로 래핑합니다.이 함수는 모듈의 자식을 inplace로 수정하고 입력 모듈을 래핑하는 새 모듈도 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5375f310fdbbbaceba22e923b5eb13a5f74fab7f" translate="yes" xml:space="preserve">
          <source>Wrapper around a &lt;code&gt;torch._C.Future&lt;/code&gt; which encapsulates an asynchronous execution of a callable, e.g. &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;. It also exposes a set of APIs to add callback functions and set results.</source>
          <target state="translated">호출 가능의 비동기 실행을 캡슐화하는 &lt;code&gt;torch._C.Future&lt;/code&gt; 주위의 래퍼 , 예 : &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt; . 또한 콜백 함수를 추가하고 결과를 설정하는 API 세트를 노출합니다.</target>
        </trans-unit>
        <trans-unit id="8df13c46adfe3a050db797a249e788f92b1e60e7" translate="yes" xml:space="preserve">
          <source>Wrapper around a CUDA event.</source>
          <target state="translated">CUDA 이벤트를 래퍼합니다.</target>
        </trans-unit>
        <trans-unit id="821f73849dcef0ef5418016b1e5d009bb01173fa" translate="yes" xml:space="preserve">
          <source>Wrapper around a CUDA stream.</source>
          <target state="translated">CUDA 스트림을 래퍼합니다.</target>
        </trans-unit>
        <trans-unit id="65acb3d4bdbad8b7e153886b6b2199ad76a756c8" translate="yes" xml:space="preserve">
          <source>Wrapper class for quantized operations.</source>
          <target state="translated">양자화 된 작업을위한 래퍼 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="8e2dfc8a3e67da8b6e1ca8fafe3e8afa2b3f0620" translate="yes" xml:space="preserve">
          <source>Wrapper that allows creation of class factories.</source>
          <target state="translated">클래스 팩토리 생성을 허용하는 래퍼.</target>
        </trans-unit>
        <trans-unit id="843c52018e11fda7799dc5989f496736df1f7d7b" translate="yes" xml:space="preserve">
          <source>Wraps another sampler to yield a mini-batch of indices.</source>
          <target state="translated">다른 샘플러를 래핑하여 인덱스의 미니 배치를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="2a31ffa99fd6d35b029d1f439707a5bd7c9b4da8" translate="yes" xml:space="preserve">
          <source>Writes all values from the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor. For each value in &lt;code&gt;src&lt;/code&gt;, its output index is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">텐서에서 모든 값을 기록 &lt;code&gt;src&lt;/code&gt; 에 &lt;code&gt;self&lt;/code&gt; 에 지정된 인덱스에서 &lt;code&gt;index&lt;/code&gt; 텐서. 의 각 값에 대해 &lt;code&gt;src&lt;/code&gt; , 출력 인덱스는 인덱스에 의해 지정되는 &lt;code&gt;src&lt;/code&gt; 을위한 &lt;code&gt;dimension != dim&lt;/code&gt; 과의 대응 값 &lt;code&gt;index&lt;/code&gt; 에 대한 &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5adf3c6baa6091132b20eef0ff93f0984499afa5" translate="yes" xml:space="preserve">
          <source>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</source>
          <target state="translated">텐서 보드가 사용할 log_dir의 이벤트 파일에 직접 항목을 씁니다.</target>
        </trans-unit>
        <trans-unit id="c032adc1ff629c9b66f22749ad667e6beadf144b" translate="yes" xml:space="preserve">
          <source>X</source>
          <target state="translated">X</target>
        </trans-unit>
        <trans-unit id="29822901b44f81c6464586704f28915142f932bc" translate="yes" xml:space="preserve">
          <source>X (Tensor): tensor of eigenvectors of size</source>
          <target state="translated">X (텐서) : 크기의 고유 벡터의 텐서</target>
        </trans-unit>
        <trans-unit id="030b21a9a5ef307806a06b78be434c1972173393" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = X ^ * [N_1-\ omega_1, \ dots, N_d-\ omega_d],</target>
        </trans-unit>
        <trans-unit id="ed1d6f505827ea7067cb8c300c0904dbfa5fbd7a" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = \ frac {1} {\ prod_ {i = 1} ^ d N_i} \ sum_ {n_1 = 0} ^ {N_1-1} \ dots \ sum_ {n_d = 0 } ^ {N_d-1} x [n_1, \ dots, n_d] e ^ {\ j \ 2 \ pi \ sum_ {i = 0} ^ d \ frac {\ omega_i n_i} {N_i}},</target>
        </trans-unit>
        <trans-unit id="746d94247ac643f86d1ee17dab642515e2ff9085" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = \ sum_ {n_1 = 0} ^ {N_1-1} \ dots \ sum_ {n_d = 0} ^ {N_d-1} x [n_1, \ dots, n_d] e ^ {-j \ 2 \ pi \ sum_ {i = 0} ^ d \ frac {\ omega_i n_i} {N_i}},</target>
        </trans-unit>
        <trans-unit id="a14f6ead4d7bc2b1f74caf9a0cc9b1f3d45fbb66" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = X[m, \text{n\_fft} - \omega]^*</source>
          <target state="translated">X [m, \ omega] = X [m, \ text {n \ _fft}-\ omega] ^ *</target>
        </trans-unit>
        <trans-unit id="4e6854a6a1b525f353a271b3c9b646d1b5c28d3f" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}% \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ % \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</source>
          <target state="translated">X [m, \ omega] = \ sum_ {k = 0} ^ {\ text {win \ _length-1}} % \ text {window} [k] \ \ text {input} [m \ times \ text {hop \ _length} + k] \ % \ exp \ left (-j \ frac {2 \ pi \ cdot \ omega k} {\ text {win \ _length}} \ right),</target>
        </trans-unit>
        <trans-unit id="14ebad2cda6dcca683250b526859348822d9d5c8" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. E.g.:</source>
          <target state="translated">예, 현재 ONNX opset 버전&amp;gt; = 11에 대해 지원됩니다. 예 :</target>
        </trans-unit>
        <trans-unit id="5684cedf8d1212771db72a004af08859ca152372" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:</source>
          <target state="translated">예, 현재 ONNX opset 버전&amp;gt; = 11에 대해 지원됩니다. ONNX는 opset 11에 시퀀스 개념을 도입했습니다. 목록과 유사하게 시퀀스는 임의 개수의 Tensor를 포함하는 데이터 유형입니다. SequenceInsert, SequenceAt 등과 같은 관련 연산자도 ONNX에 도입되었습니다. 그러나 루프 내 내부 목록 추가는 ONNX로 내보낼 수 없습니다. 이를 구현하려면 inplace add 연산자를 사용하십시오. 예 :</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="7c19fb2e314a3137e93fb758f531465aacc3456b" translate="yes" xml:space="preserve">
          <source>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</source>
          <target state="translated">또한 처음 n 개의 차원 만 희소하고 나머지 차원은 조밀 한 하이브리드 희소 텐서를 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="d617602dcc62272743d7a795be368e5ffbe5433d" translate="yes" xml:space="preserve">
          <source>You can also run the exported model with &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtime&lt;/a&gt;, you will need to install &lt;code&gt;ONNX Runtime&lt;/code&gt;: please &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;follow these instructions&lt;/a&gt;.</source>
          <target state="translated">당신은 또한에 내 보낸 모델을 실행할 수 있습니다 &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX 런타임을&lt;/a&gt; , 당신은 설치해야합니다 &lt;code&gt;ONNX Runtime&lt;/code&gt; : 제발 &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;다음 지침을 따르십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="18be7e0f13bf0aca1a5c2348aa103f8bb566787e" translate="yes" xml:space="preserve">
          <source>You can also use cosine annealing to a fixed value instead of linear annealing by setting &lt;code&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;anneal_strategy=&quot;cos&quot;&lt;/code&gt; 를 설정하여 선형 어닐링 대신 고정 값으로 코사인 어닐링을 사용할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c8c75fa187b181f202978d8a86ab28cca3e9eb36" translate="yes" xml:space="preserve">
          <source>You can also verify the protobuf using the &lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; library. You can install &lt;code&gt;ONNX&lt;/code&gt; with conda:</source>
          <target state="translated">&lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; 라이브러리를 사용하여 protobuf를 확인할 수도 있습니다. &lt;code&gt;ONNX&lt;/code&gt; 하여 ONNX를 설치할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ec539497ce517be7d2c9354eae2288ab7aed0f7d" translate="yes" xml:space="preserve">
          <source>You can construct a model with random weights by calling its constructor:</source>
          <target state="translated">생성자를 호출하여 무작위 가중치로 모델을 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1400fef33661543c72061ceed7d3f9e9ebc487ea" translate="yes" xml:space="preserve">
          <source>You can create your own registry by creating a new &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt;&lt;code&gt;ConstraintRegistry&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">새 &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt; &lt;code&gt;ConstraintRegistry&lt;/code&gt; &lt;/a&gt; 개체를 만들어 고유 한 레지스트리를 만들 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="decdc18078675c119e9394b80151d2e753fa566c" translate="yes" xml:space="preserve">
          <source>You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn&amp;rsquo;t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.</source>
          <target state="translated">여전히 옵션을 키워드 인수로 전달할 수 있습니다. 재정의하지 않은 그룹에서 기본값으로 사용됩니다. 이는 단일 옵션 만 변경하고 다른 모든 옵션은 파라미터 그룹간에 일관되게 유지하려는 경우에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="3431a2a1d98df776e7494a8e2a5ab89115af9a01" translate="yes" xml:space="preserve">
          <source>You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.</source>
          <target state="translated">텐서와 저장소를 모두 인수로 사용할 수 있습니다. 주어진 객체가 GPU에 할당되지 않은 경우 이는 작동하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="cc736ac07ea8c62fb5b125af6be01955a2077643" translate="yes" xml:space="preserve">
          <source>You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.</source>
          <target state="translated">total_steps에 대한 값을 제공하거나 epochs 및 steps_per_epoch 모두에 대한 값을 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="762890e45c5b225da275ae73984ed756702605c8" translate="yes" xml:space="preserve">
          <source>You should never try to change your model&amp;rsquo;s parameters after wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Because, when wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;, the constructor of &lt;code&gt;DistributedDataParallel&lt;/code&gt; will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&amp;rsquo;s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.</source>
          <target state="translated">&lt;code&gt;DistributedDataParallel&lt;/code&gt; 로 모델을 래핑 한 후에는 모델의 매개 변수를 변경해서는 안됩니다 . 와 모델 마무리 할 때, 때문에 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 ,의 생성자 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 은 건축 당시의 모델 자체의 모든 매개 변수에 추가 기울기 감소 기능을 등록합니다. 나중에 모델의 매개 변수를 변경하면 그래디언트 중복 함수가 더 이상 올바른 매개 변수 세트와 일치하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ccd4aee0db1253438f0c3a9d3c74816b28340f08" translate="yes" xml:space="preserve">
          <source>You&amp;rsquo;ll generally want to use &lt;a href=&quot;torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">일반적으로 대신 &lt;a href=&quot;torch.qr#torch.qr&quot;&gt; &lt;code&gt;torch.qr()&lt;/code&gt; &lt;/a&gt; 을 사용하고 싶을 것입니다.</target>
        </trans-unit>
        <trans-unit id="f259809289921b4be91b7ff0e29bbdb10551660f" translate="yes" xml:space="preserve">
          <source>Your models should also subclass this class.</source>
          <target state="translated">모델도이 클래스를 하위 클래스로 분류해야합니다.</target>
        </trans-unit>
        <trans-unit id="93d450611dc79948223d0cdb9f4a99610848c9d6" translate="yes" xml:space="preserve">
          <source>ZeroPad2d</source>
          <target state="translated">ZeroPad2d</target>
        </trans-unit>
        <trans-unit id="90bc5acc0a3b091bfe56eb668e9c84ac53428130" translate="yes" xml:space="preserve">
          <source>[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]]</source>
          <target state="translated">[* \ times \ text {normalized \ _shape} [0] \ times \ text {normalized \ _shape} [1] \ times \ ldots \ times \ text {normalized \ _shape} [-1]]</target>
        </trans-unit>
        <trans-unit id="b61210c8825bb88613c060ba48aae051333bd30a" translate="yes" xml:space="preserve">
          <source>[-1, 1]</source>
          <target state="translated">[-1, 1]</target>
        </trans-unit>
        <trans-unit id="ae17aa1eaf46c89eecaf929c74d8fda9a55db49b" translate="yes" xml:space="preserve">
          <source>[0, 1)</source>
          <target state="translated">[0, 1)</target>
        </trans-unit>
        <trans-unit id="c49d95c97e6b97b46f0fa87c4c3d5517b2dd2ac1" translate="yes" xml:space="preserve">
          <source>[0, C-1]</source>
          <target state="translated">[0, C-1]</target>
        </trans-unit>
        <trans-unit id="c00ab2adc7412d84a0750550969f7439fdbed02f" translate="yes" xml:space="preserve">
          <source>[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;code&gt;code&lt;/code&gt;. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant&amp;rsquo;s values.</source>
          <target state="translated">[0] &lt;code&gt;forward&lt;/code&gt; 메서드에 대한 내부 그래프의 예쁘게 인쇄 된 표현 (유효한 Python 구문)입니다 . &lt;code&gt;code&lt;/code&gt; 참조하십시오 . [1] [0] 출력의 CONSTANT.cN 형식을 따르는 ConstMap. [0] 출력의 인덱스는 기본 상수 값의 키입니다.</target>
        </trans-unit>
        <trans-unit id="bc47f02dcecd076fd210f14fb47e8772c796bc8d" translate="yes" xml:space="preserve">
          <source>[1] D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.</source>
          <target state="translated">[1] DW Griffin과 JS Lim,&amp;ldquo;수정 된 단시간 푸리에 변환으로부터 신호 추정&amp;rdquo;, IEEE Trans. ASSP, vol.32, no.2, pp.236-243, 1984 년 4 월.</target>
        </trans-unit>
        <trans-unit id="6682e58cfacdff6355a8741ff44261abb6d30565" translate="yes" xml:space="preserve">
          <source>[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)</source>
          <target state="translated">[1] 구체적인 분포 : 이산 랜덤 변수의 연속 완화 (Maddison et al, 2017)</target>
        </trans-unit>
        <trans-unit id="1abd3a692f41e45de17752978fa303a578688034" translate="yes" xml:space="preserve">
          <source>[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. &lt;a href=&quot;https://arxiv.org/abs/1907.06845&quot;&gt;https://arxiv.org/abs/1907.06845&lt;/a&gt;</source>
          <target state="translated">[1] 연속 베르누이 : 변형 오토 인코더, Loaiza-Ganem G 및 Cunningham JP, NeurIPS 2019의 퍼베이시브 오류 수정. &lt;a href=&quot;https://arxiv.org/abs/1907.06845&quot;&gt;https://arxiv.org/abs/1907.06845&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d064e85a047a7b9150d33ff887f20a63a320452" translate="yes" xml:space="preserve">
          <source>[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)</source>
          <target state="translated">[2] Gumbel-Softmax를 사용한 범주 재 매개 변수화 (Jang et al, 2017)</target>
        </trans-unit>
        <trans-unit id="9390898997c65a41ca007de79555abfa5f0ba5df" translate="yes" xml:space="preserve">
          <source>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</source>
          <target state="translated">[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) LOBPCG의 강력하고 효율적인 구현. SIAM J. Sci. 계산, 40 (5), C655-C676. (22 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08253d232026d76249c2db42c2940e24ba3bacf" translate="yes" xml:space="preserve">
          <source>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</source>
          <target state="translated">[Knyazev2001] Andrew V. Knyazev. (2001) 최적의 사전 조건화 된 고유 솔버를 향해 : 국부적으로 최적의 블록 사전 조건화 된 켤레 기울기 방법. SIAM J. Sci. 계산, 23 (2), 517-541. (25 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ff64b4c2d43518023be089f31430ef90034f605e" translate="yes" xml:space="preserve">
          <source>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</source>
          <target state="translated">[StathopoulosEtal2002] Andreas Stathopoulos와 Kesheng Wu. (2002) 일정한 동기화 요구 사항이있는 블록 직교 화 절차. SIAM J. Sci. Comput., 23 (6), 2165-2182. (18 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1e9b6692d359e3ebb28163afd3f06ce34d6b2df" translate="yes" xml:space="preserve">
          <source>[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</source>
          <target state="translated">[[-1.1819, -0.8899], [1.5813, 0.2274]]], 이름 = ( 'A', 'B1', 'B2'))</target>
        </trans-unit>
        <trans-unit id="38ac8c676253501b3fb6819ff9373ce6cfe5b055" translate="yes" xml:space="preserve">
          <source>\ ^*</source>
          <target state="translated">\ ^ *</target>
        </trans-unit>
        <trans-unit id="b9bb5838f4fb69b64d3191210035a54a11128eeb" translate="yes" xml:space="preserve">
          <source>\Delta\theta = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}</source>
          <target state="translated">\ 델타 \ theta = \ alpha r \ frac {\ partial \ log p (a | \ pi ^ \ theta (s))} {\ partial \ theta}</target>
        </trans-unit>
        <trans-unit id="80a1cbf9743b017c73b23f243284395f0c518670" translate="yes" xml:space="preserve">
          <source>\Gamma(\cdot)</source>
          <target state="translated">\Gamma(\cdot)</target>
        </trans-unit>
        <trans-unit id="15e33ea71862e7b6632cf90aebb8ab938c698f9d" translate="yes" xml:space="preserve">
          <source>\Phi(x)</source>
          <target state="translated">\Phi(x)</target>
        </trans-unit>
        <trans-unit id="2a6c118642891e41137cb68e1346d34dabbd128c" translate="yes" xml:space="preserve">
          <source>\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</source>
          <target state="translated">\ Vert x \ Vert _p = \ left (\ sum_ {i = 1} ^ n \ vert x_i \ vert ^ p \ right) ^ {1 / p}.</target>
        </trans-unit>
        <trans-unit id="f7c665b45932a814215e979bc2611080b4948e68" translate="yes" xml:space="preserve">
          <source>\alpha</source>
          <target state="translated">\alpha</target>
        </trans-unit>
        <trans-unit id="1eafe1d2e67a6ccfd6c43fcc6a7aba05feb684c2" translate="yes" xml:space="preserve">
          <source>\alpha = 1.6732632423543772848170429916717</source>
          <target state="translated">\ alpha = 1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="e9b40d7ba0060d884943279a8f77be052be116fb" translate="yes" xml:space="preserve">
          <source>\alpha/(\sqrt{v} + \epsilon)</source>
          <target state="translated">\alpha/(\sqrt{v} + \epsilon)</target>
        </trans-unit>
        <trans-unit id="cbe1c5b26860bdd639e58977c2072d9b65aece25" translate="yes" xml:space="preserve">
          <source>\alpha=1.6732632423543772848170429916717</source>
          <target state="translated">\alpha=1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="553ba8e7e5ec0531ddcc7d2c4cd048f424b3f52f" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \eta_t &amp;amp; = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right), &amp;amp; T_{cur} \neq (2k+1)T_{max}; \\ \eta_{t+1} &amp;amp; = \eta_{t} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 - \cos\left(\frac{1}{T_{max}}\pi\right)\right), &amp;amp; T_{cur} = (2k+1)T_{max}. \end{aligned}</source>
          <target state="translated">\ begin {aligned} \ eta_t &amp;amp; = \ eta_ {min} + \ frac {1} {2} (\ eta_ {max}-\ eta_ {min}) \ left (1 + \ cos \ left (\ frac {T_ {cur}} {T_ {max}} \ pi \ right) \ right) 및 T_ {cur} \ neq (2k + 1) T_ {max}; \\ \ eta_ {t + 1} &amp;amp; = \ eta_ {t} + \ frac {1} {2} (\ eta_ {max}-\ eta_ {min}) \ left (1-\ cos \ left (\ frac {1} {T_ {최대}} \ pi \ 오른쪽) \ 오른쪽), &amp;amp; T_ {cur} = (2k + 1) T_ {최대}. \ end {정렬}</target>
        </trans-unit>
        <trans-unit id="a6518676c0b93d25520b42e7f84e6cb06eb90098" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{if Symmetric:}&amp;amp;\\ &amp;amp;s = 2 \max(|x_\text{min}|, x_\text{max}) / \left( Q_\text{max} - Q_\text{min} \right) \\ &amp;amp;z = \begin{cases} 0 &amp;amp; \text{if dtype is qint8} \\ 128 &amp;amp; \text{otherwise} \end{cases}\\ \text{Otherwise:}&amp;amp;\\ &amp;amp;s = \left( x_\text{max} - x_\text{min} \right ) / \left( Q_\text{max} - Q_\text{min} \right ) \\ &amp;amp;z = Q_\text{min} - \text{round}(x_\text{min} / s) \end{aligned}</source>
          <target state="translated">\ begin {aligned} \ text {if Symmetric :} &amp;amp; \\ &amp;amp; s = 2 \ max (| x_ \ text {min} |, x_ \ text {max}) / \ left (Q_ \ text {max}-Q_ \ text {min} \ right) \\ &amp;amp; z = \ begin {cases} 0 &amp;amp; \ text {dtype이 qint8} \\ 128 &amp;amp; \ text {otherwise} \ end {cases} \\ \ text {기타 :} &amp;amp; \ \ &amp;amp; s = \ left (x_ \ text {max}-x_ \ text {min} \ right) / \ left (Q_ \ text {max}-Q_ \ text {min} \ right) \\ &amp;amp; z = Q_ \ text { min}-\ text {round} (x_ \ text {min} / s) \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="eef3f6c4a900c50c6ccae139e6900e18ddc9222e" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</source>
          <target state="translated">\ begin {정렬} \ text {out} (N_i, C_j, d, h, w) = {} &amp;amp; \ max_ {k = 0, \ ldots, kD-1} \ max_ {m = 0, \ ldots, kH -1} \ max_ {n = 0, \ ldots, kW-1} \\ &amp;amp; \ text {input} (N_i, C_j, \ text {stride [0]} \ times d + k, \ text {stride [1 ]} \ times h + m, \ text {stride [2]} \ times w + n) \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="3444423e4aa6a27dc2d4e17fde867139b4f23a7b" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</source>
          <target state="translated">\ begin {정렬} \ text {out} (N_i, C_j, d, h, w) = {} &amp;amp; \ sum_ {k = 0} ^ {kD-1} \ sum_ {m = 0} ^ {kH-1 } \ sum_ {n = 0} ^ {kW-1} \\ &amp;amp; \ frac {\ text {input} (N_i, C_j, \ text {stride} [0] \ times d + k, \ text {stride} [ 1] \ times h + m, \ text {stride} [2] \ times w + n)} {kD \ times kH \ times kW} \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="9b5fbddfbbd75f7006abb89ad6964ec0f13538c7" translate="yes" xml:space="preserve">
          <source>\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</source>
          <target state="translated">\ begin {aligned} out (N_i, C_j, h, w) = {} &amp;amp; \ max_ {m = 0, \ ldots, kH-1} \ max_ {n = 0, \ ldots, kW-1} \\ &amp;amp; \ text {input} (N_i, C_j, \ text {stride [0]} \ times h + m, \ text {stride [1]} \ times w + n) \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="68934623d4a064db7240ac0ed4e63262bd5a88d3" translate="yes" xml:space="preserve">
          <source>\begin{aligned} v_{t+1} &amp;amp; = \mu * v_{t} + \text{lr} * g_{t+1}, \\ p_{t+1} &amp;amp; = p_{t} - v_{t+1}. \end{aligned}</source>
          <target state="translated">\ begin {정렬} v_ {t + 1} &amp;amp; = \ mu * v_ {t} + \ text {lr} * g_ {t + 1}, \\ p_ {t + 1} &amp;amp; = p_ {t}-v_ {t + 1}. \ end {정렬}</target>
        </trans-unit>
        <trans-unit id="e5d65e65e32d2c2c154e2f6af937b11f165a63e3" translate="yes" xml:space="preserve">
          <source>\begin{aligned} v_{t+1} &amp;amp; = \mu * v_{t} + g_{t+1}, \\ p_{t+1} &amp;amp; = p_{t} - \text{lr} * v_{t+1}, \end{aligned}</source>
          <target state="translated">\ begin {정렬} v_ {t + 1} &amp;amp; = \ mu * v_ {t} + g_ {t + 1}, \\ p_ {t + 1} &amp;amp; = p_ {t}-\ text {lr} * v_ {t + 1}, \ end {정렬}</target>
        </trans-unit>
        <trans-unit id="4ea19e5cef880f790f54322d49c339a67ef7eba6" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} \\ i_t = \ sigma (W_ {ii} x_t + b_ {ii} + W_ {hi} h_ {t-1} + b_ {hi}) \\ f_t = \ sigma (W_ {if} x_t + b_ {if} + W_ {hf} h_ {t-1} + b_ {hf}) \\ g_t = \ tanh (W_ {ig} x_t + b_ {ig} + W_ {hg} h_ { t-1} + b_ {hg}) \\ o_t = \ sigma (W_ {io} x_t + b_ {io} + W_ {ho} h_ {t-1} + b_ {ho}) \\ c_t = f_t \ odot c_ {t-1} + i_t \ odot g_t \\ h_t = o_t \ odot \ tanh (c_t) \\ \ end {array}</target>
        </trans-unit>
        <trans-unit id="6b6cb34a158b8201fbf48f50465b1ece00a7eb5f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</source>
          <target state="translated">\ begin {array} {ll} \ min_X &amp;amp; \ | AX-B \ | _2. \ end {배열}</target>
        </trans-unit>
        <trans-unit id="9312cff9d66b9a6753766d98374f8fbc6cf98e2f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</source>
          <target state="translated">\ begin {array} {ll} \ min_X &amp;amp; \ | X \ | _2 &amp;amp; \ text {subject to} &amp;amp; AX = B. \ end {array}</target>
        </trans-unit>
        <trans-unit id="930f69e97734f3784bc6b17a4919f47bb6fa5660" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} i = \ sigma (W_ {ii} x + b_ {ii} + W_ {hi} h + b_ {hi}) \\ f = \ sigma (W_ {if} x + b_ { if} + W_ {hf} h + b_ {hf}) \\ g = \ tanh (W_ {ig} x + b_ {ig} + W_ {hg} h + b_ {hg}) \\ o = \ sigma ( W_ {io} x + b_ {io} + W_ {ho} h + b_ {ho}) \\ c '= f * c + i * g \\ h'= o * \ tanh (c ') \\ \ end {array}</target>
        </trans-unit>
        <trans-unit id="e6bea3b60c648ba0107ad5a273553cd4d585a9b3" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</source>
          <target state="translated">\ begin {array} {ll} r = \ sigma (W_ {ir} x + b_ {ir} + W_ {hr} h + b_ {hr}) \\ z = \ sigma (W_ {iz} x + b_ { iz} + W_ {hz} h + b_ {hz}) \\ n = \ tanh (W_ {in} x + b_ {in} + r * (W_ {hn} h + b_ {hn})) \\ h '= (1-z) * n + z * h \ end {array}</target>
        </trans-unit>
        <trans-unit id="e41fd097ab91cc853c05e5d0d9e0d37ff3dd7662" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</source>
          <target state="translated">\ begin {array} {ll} r_t = \ sigma (W_ {ir} x_t + b_ {ir} + W_ {hr} h _ {(t-1)} + b_ {hr}) \\ z_t = \ sigma (W_ {iz} x_t + b_ {iz} + W_ {hz} h _ {(t-1)} + b_ {hz}) \\ n_t = \ tanh (W_ {in} x_t + b_ {in} + r_t * (W_ {hn} h _ {(t-1)} + b_ {hn})) \\ h_t = (1-z_t) * n_t + z_t * h _ {(t-1)} \ end {array}</target>
        </trans-unit>
        <trans-unit id="083f0eacd92d5be23a46e2af20aa291d89fff082" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} x_\text{min} &amp;amp;= \begin{cases} \min(X) &amp;amp; \text{if~}x_\text{min} = \text{None} \\ \min\left(x_\text{min}, \min(X)\right) &amp;amp; \text{otherwise} \end{cases}\\ x_\text{max} &amp;amp;= \begin{cases} \max(X) &amp;amp; \text{if~}x_\text{max} = \text{None} \\ \max\left(x_\text{max}, \max(X)\right) &amp;amp; \text{otherwise} \end{cases}\\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} x_ \ text {min} &amp;amp; = \ begin {cases} \ min (X) &amp;amp; \ text {if ~} x_ \ text {min} = \ text {None} \\ \ min \ left (x_ \ text {min}, \ min (X) \ right) &amp;amp; \ text {otherwise} \ end {cases} \\ x_ \ text {max} &amp;amp; = \ begin {cases} \ max (X) &amp;amp; \ text {if ~} x_ \ text {max} = \ text {None} \\ \ max \ left (x_ \ text {max}, \ max (X) \ right) &amp;amp; \ text {otherwise} \ end {cases} \\ \ end {배열}</target>
        </trans-unit>
        <trans-unit id="afd260a0e4c76fcb26f5d6cc69c2624609e9b929" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} x_\text{min} = \begin{cases} \min(X) &amp;amp; \text{if~}x_\text{min} = \text{None} \\ (1 - c) x_\text{min} + c \min(X) &amp;amp; \text{otherwise} \end{cases}\\ x_\text{max} = \begin{cases} \max(X) &amp;amp; \text{if~}x_\text{max} = \text{None} \\ (1 - c) x_\text{max} + c \max(X) &amp;amp; \text{otherwise} \end{cases}\\ \end{array}</source>
          <target state="translated">\begin{array}{ll} x_\text{min} = \begin{cases} \min(X) &amp;amp; \text{if~}x_\text{min} = \text{None} \\ (1 - c) x_\text{min} + c \min(X) &amp;amp; \text{otherwise} \end{cases}\\ x_\text{max} = \begin{cases} \max(X) &amp;amp; \text{if~}x_\text{max} = \text{None} \\ (1 - c) x_\text{max} + c \max(X) &amp;amp; \text{otherwise} \end{cases}\\ \end{array}</target>
        </trans-unit>
        <trans-unit id="6499d503bfc00cadae1440b191c52a8632e2f8c4" translate="yes" xml:space="preserve">
          <source>\beta</source>
          <target state="translated">\beta</target>
        </trans-unit>
        <trans-unit id="6ceee6706c4f97ef457cd65c6ed19732a4a4c7e0" translate="yes" xml:space="preserve">
          <source>\delta^{(l-1)}_t</source>
          <target state="translated">\delta^{(l-1)}_t</target>
        </trans-unit>
        <trans-unit id="986673ab936df8ce1454d7dc93297bd7bbc51c4b" translate="yes" xml:space="preserve">
          <source>\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">\ ell (a, p, n) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_i = \ max \ {d (a_i, p_i)-d (a_i, n_i) + {\ rm 마진}, 0 \}</target>
        </trans-unit>
        <trans-unit id="4d38ace96b9720e15bef838db0ce85ef5b456092" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_n \ left [y_n \ cdot \ log \ sigma (x_n) + (1-y_n) \ cdot \ log (1-\ sigma (x_n)) \ right],</target>
        </trans-unit>
        <trans-unit id="ec2ad56a41af8d7cfc9a26f61f048cf6cbd7e78d" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_n \ left [y_n \ cdot \ log x_n + (1-y_n) \ cdot \ log ( 1-x_n) \ 오른쪽],</target>
        </trans-unit>
        <trans-unit id="de7b2379237fe388c3459990f0a508ab1bd86588" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_ {y_n} x_ {n, y_n}, \ quad w_ {c} = \ text { weight} [c] \ cdot \ mathbb {1} \ {c \ not = \ text {ignore \ _index} \},</target>
        </trans-unit>
        <trans-unit id="f6a58890a57b18854cfa1707f659a4fd6ff6e4cb" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n = \ left (x_n-y_n \ right) ^ 2,</target>
        </trans-unit>
        <trans-unit id="bbe073a5e1335cf9e8392124ee3bc939a82d8704" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n = \ left | x_n-y_n \ 오른쪽 |,</target>
        </trans-unit>
        <trans-unit id="295ee7beff5d61b430ac876642e47ec1ac559795" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text { 'mean';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="4d5ca23779a198f768fff42eaa5060e03279277c" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text { 'mean';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="3c1589dd2fb19090831cbb7f520baedd8d65a3e4" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text {`mean ';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text {`sum '.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="8ba1942b6f9513bd39cc8cb1f1c52037ed6d49d3" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ sum_ {n = 1} ^ N \ frac {1} {\ sum_ {n = 1} ^ N w_ {y_n}} l_n 및 \ text {축소 인 경우 } = \ text { 'mean';} \\ \ sum_ {n = 1} ^ N l_n, &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="5a3a211a3bdfba5d642037c3946f715f2ce73187" translate="yes" xml:space="preserve">
          <source>\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</source>
          <target state="translated">\ ell_c (x, y) = L_c = \ {l_ {1, c}, \ dots, l_ {N, c} \} ^ \ top, \ quad l_ {n, c} =-w_ {n, c} \ left [p_c y_ {n, c} \ cdot \ log \ sigma (x_ {n, c}) + (1-y_ {n, c}) \ cdot \ log (1-\ sigma (x_ {n, c })) \권리],</target>
        </trans-unit>
        <trans-unit id="38c29fec2dd06380b3c75f2ad0de63a4841510db" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min}</source>
          <target state="translated">\ eta_t = \ eta_ {분}</target>
        </trans-unit>
        <trans-unit id="669aeacd1d773163fae893a8ca1bc6fee6ec98cc" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)</source>
          <target state="translated">\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)</target>
        </trans-unit>
        <trans-unit id="4c0a99b42d5ab2de0c04cb285ece308fea6df103" translate="yes" xml:space="preserve">
          <source>\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)</source>
          <target state="translated">\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)</target>
        </trans-unit>
        <trans-unit id="ce41fe8b1089f12f8148fc05ab4748c0e9ba7592" translate="yes" xml:space="preserve">
          <source>\eta_t=\eta_{max}</source>
          <target state="translated">\eta_t=\eta_{max}</target>
        </trans-unit>
        <trans-unit id="2e3b3cc9e60fd68fa21c6331d4f52b4c13f61075" translate="yes" xml:space="preserve">
          <source>\eta_{max}</source>
          <target state="translated">\eta_{max}</target>
        </trans-unit>
        <trans-unit id="c04fbcdd9308d49d4c2300f2ebbb1e83c44e8b83" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target} * \text{input}</source>
          <target state="translated">\ exp (\ text {input})-\ text {target} * \ text {input}</target>
        </trans-unit>
        <trans-unit id="cd47433a58e73b2601de69c1a9744b288212fb5f" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target}*\text{input}</source>
          <target state="translated">\ exp (\ text {input})-\ text {target} * \ text {input}</target>
        </trans-unit>
        <trans-unit id="fe43eb66c8bdd27010ca6db1c98281c7d4c4731f" translate="yes" xml:space="preserve">
          <source>\exp^A = \sum_{k=0}^\infty A^k / k!.</source>
          <target state="translated">\ exp ^ A = \ sum_ {k = 0} ^ \ infty A ^ k / k !.</target>
        </trans-unit>
        <trans-unit id="14f6d7074093dd4d9ef81de502fd654605e4bb67" translate="yes" xml:space="preserve">
          <source>\forall i = d, \dots, d+k-1</source>
          <target state="translated">\ forall i = d, \ dots, d + k-1</target>
        </trans-unit>
        <trans-unit id="6aed51b4bd21bf9aaad2932aa936dab4bdd62611" translate="yes" xml:space="preserve">
          <source>\frac{1}{1-p}</source>
          <target state="translated">\frac{1}{1-p}</target>
        </trans-unit>
        <trans-unit id="037205eaa442691656469a316bf2dff320c2f572" translate="yes" xml:space="preserve">
          <source>\frac{1}{2} N (N - 1)</source>
          <target state="translated">\ frac {1} {2} N (N-1)</target>
        </trans-unit>
        <trans-unit id="64c94d13eeb330b494061e86538db66574ad0f7d" translate="yes" xml:space="preserve">
          <source>\frac{1}{3}</source>
          <target state="translated">\frac{1}{3}</target>
        </trans-unit>
        <trans-unit id="5947a169159fe867f85f3fd8b9690019b48152f5" translate="yes" xml:space="preserve">
          <source>\frac{1}{8}</source>
          <target state="translated">\frac{1}{8}</target>
        </trans-unit>
        <trans-unit id="f81c864ea46e4c42b16663b744993f9011be95f2" translate="yes" xml:space="preserve">
          <source>\frac{300}{100}=3</source>
          <target state="translated">\frac{300}{100}=3</target>
        </trans-unit>
        <trans-unit id="436bcf2181eea8fe79cdb015a5567ca1b8d69652" translate="yes" xml:space="preserve">
          <source>\frac{5}{3}</source>
          <target state="translated">\frac{5}{3}</target>
        </trans-unit>
        <trans-unit id="a9c2bfb5b8138830fd93a3a13faef54bc4dc94a2" translate="yes" xml:space="preserve">
          <source>\frac{m}{2} \leq</source>
          <target state="translated">\ frac {m} {2} \ leq</target>
        </trans-unit>
        <trans-unit id="b8fb95020958e9f0b58822f35b35fb490054c02e" translate="yes" xml:space="preserve">
          <source>\frac{p - 1}{2}</source>
          <target state="translated">\ frac {p-1} {2}</target>
        </trans-unit>
        <trans-unit id="67833ee2012ec1c6254b6c009dc72bf0dc48aa6d" translate="yes" xml:space="preserve">
          <source>\gamma</source>
          <target state="translated">\gamma</target>
        </trans-unit>
        <trans-unit id="27634ea2c473bc36e59149a7f0c457ffb292325a" translate="yes" xml:space="preserve">
          <source>\hat{x}</source>
          <target state="translated">\hat{x}</target>
        </trans-unit>
        <trans-unit id="b9e47e1f86f68634e0d0a997d4ea3952fae1e892" translate="yes" xml:space="preserve">
          <source>\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</source>
          <target state="translated">\ hat {x} _ \ text {new} = (1-\ text {momentum}) \ times \ hat {x} + \ text {momentum} \ times x_t</target>
        </trans-unit>
        <trans-unit id="770b7843ffee64a984041915fe8461fd98a76060" translate="yes" xml:space="preserve">
          <source>\in [0, \infty]</source>
          <target state="translated">\ in [0, \ infty]</target>
        </trans-unit>
        <trans-unit id="32c336e712ce8251fe81037413821c2c348ab43d" translate="yes" xml:space="preserve">
          <source>\inf</source>
          <target state="translated">\inf</target>
        </trans-unit>
        <trans-unit id="9b97f26fbeb1b84327c736de515f10980d9c7d68" translate="yes" xml:space="preserve">
          <source>\infty</source>
          <target state="translated">\infty</target>
        </trans-unit>
        <trans-unit id="b237071f96360004cf37b06340000864b59f54c3" translate="yes" xml:space="preserve">
          <source>\int y\,dx</source>
          <target state="translated">\ int y \, dx</target>
        </trans-unit>
        <trans-unit id="a10251c74fceb1b1b9e9c45471b613f216beb4a9" translate="yes" xml:space="preserve">
          <source>\int_a^b f = -\int_b^a f</source>
          <target state="translated">\ int_a ^ bf =-\ int_b ^ af</target>
        </trans-unit>
        <trans-unit id="b3931f1ce298c536432fd324b3a1ab4337120689" translate="yes" xml:space="preserve">
          <source>\lambda</source>
          <target state="translated">\lambda</target>
        </trans-unit>
        <trans-unit id="1b47c3b18de49455a713efe91ac03ec27aad59a5" translate="yes" xml:space="preserve">
          <source>\lbrace (i, i) \rbrace</source>
          <target state="translated">\ lbrace (i, i) \ rbrace</target>
        </trans-unit>
        <trans-unit id="0fa91d11567564172cce99f01e48d1e73a28bc31" translate="yes" xml:space="preserve">
          <source>\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</source>
          <target state="translated">\ left [0, 1, 2, \ dots, \ left \ lfloor \ frac {\ text {n \ _fft}} {2} \ right \ rfloor + 1 \ right]</target>
        </trans-unit>
        <trans-unit id="89ca5f286a6835c142dd1390cd67d2698dca7bcb" translate="yes" xml:space="preserve">
          <source>\left[\text{-clip\_value}, \text{clip\_value}\right]</source>
          <target state="translated">\ left [\ text {-clip \ _value}, \ text {clip \ _value} \ right]</target>
        </trans-unit>
        <trans-unit id="6ceb7ce51c6d9da5e34a800614788f55dff712c0" translate="yes" xml:space="preserve">
          <source>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</source>
          <target state="translated">\ left \ lceil \ frac {\ text {end}-\ text {start}} {\ text {step}} \ right \ rceil</target>
        </trans-unit>
        <trans-unit id="a74c86baea5a7bb180947e759ccafb91eee50e79" translate="yes" xml:space="preserve">
          <source>\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</source>
          <target state="translated">\ left \ lfloor \ frac {\ text {end}-\ text {start}} {\ text {step}} \ right \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="ed56196dc362da2ecbc9244bc832a7311739bb3d" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="de0088faf4547f5a22dd4cd77b23209a7dd8113e" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="88061aa72baf92fe1d129c82b73d16c37a6af8f0" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="f0a9328764eddcc1a62bf8fbb7c249505ed2c539" translate="yes" xml:space="preserve">
          <source>\leq</source>
          <target state="translated">\leq</target>
        </trans-unit>
        <trans-unit id="49abff7af14753dc3cdab27a1bf7f8ed5f2c2fa1" translate="yes" xml:space="preserve">
          <source>\leq 256</source>
          <target state="translated">\ leq 256</target>
        </trans-unit>
        <trans-unit id="a719b606b8fa813d6dc2397930551b66016965ba" translate="yes" xml:space="preserve">
          <source>\leq S</source>
          <target state="translated">\ leq S</target>
        </trans-unit>
        <trans-unit id="f7d8821758716417ac6285ab84b0661734c3439c" translate="yes" xml:space="preserve">
          <source>\leq T</source>
          <target state="translated">\ leq T</target>
        </trans-unit>
        <trans-unit id="9ca5d36772ef139985921c4d545788d48fddcf0c" translate="yes" xml:space="preserve">
          <source>\lfloor \frac{N_d}{2} \rfloor + 1</source>
          <target state="translated">\ lfloor \ frac {N_d} {2} \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="bc0c94864255e5978415bf4d290dee47e2677194" translate="yes" xml:space="preserve">
          <source>\lfloor\frac{\text{input planes}}{sT}\rfloor</source>
          <target state="translated">\ lfloor \ frac {\ text {input planes}} {sT} \ rfloor</target>
        </trans-unit>
        <trans-unit id="9878e94e87e5bd7098242aa36f29f409bc60ed2b" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</source>
          <target state="translated">\ lim_ {x \ to 0} \ frac {d} {dx} \ log (x) = \ infty</target>
        </trans-unit>
        <trans-unit id="26c67f72ffeaf83ec8c07ff1c00c57b7b4ce05e6" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \log (x) = -\infty</source>
          <target state="translated">\ lim_ {x \ to 0} \ log (x) =-\ infty</target>
        </trans-unit>
        <trans-unit id="d92b1f0baf6824880fb434c0077a618ba265ea33" translate="yes" xml:space="preserve">
          <source>\log (0) = -\infty</source>
          <target state="translated">\ log (0) =-\ infty</target>
        </trans-unit>
        <trans-unit id="99e33ed0cf12197cde63048ced916ea73cbc9c3e" translate="yes" xml:space="preserve">
          <source>\log(0)</source>
          <target state="translated">\log(0)</target>
        </trans-unit>
        <trans-unit id="9f7859c28e08a9c15994c8896e613dd27080b6a5" translate="yes" xml:space="preserve">
          <source>\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</source>
          <target state="translated">\ log (\ Gamma_ {p} (a)) = C + \ displaystyle \ sum_ {i = 1} ^ {p} \ log \ left (\ Gamma \ left (a-\ frac {i-1} {2} \맞아 맞아)</target>
        </trans-unit>
        <trans-unit id="c1e3a3daacbe46fc4916de547be4049f250f3288" translate="yes" xml:space="preserve">
          <source>\log(\text{Softmax}(x))</source>
          <target state="translated">\log(\text{Softmax}(x))</target>
        </trans-unit>
        <trans-unit id="322a0b1861bac0e7bc727021b82a0313c2a2b303" translate="yes" xml:space="preserve">
          <source>\log\left(e^x + e^y\right)</source>
          <target state="translated">\ log \ left (e ^ x + e ^ y \ right)</target>
        </trans-unit>
        <trans-unit id="8783c75c38eec2ddd68cac88f7dc5ac00e806274" translate="yes" xml:space="preserve">
          <source>\log_2\left(2^x + 2^y\right)</source>
          <target state="translated">\ log_2 \ left (2 ^ x + 2 ^ y \ right)</target>
        </trans-unit>
        <trans-unit id="8935b97a0f600209c234abf4d224c5fe967c9fa1" translate="yes" xml:space="preserve">
          <source>\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</source>
          <target state="translated">\ lvert \ text {input}-\ text {other} \ rvert \ leq \ texttt {atol} + \ texttt {rtol} \ times \ lvert \ text {other} \ rvert</target>
        </trans-unit>
        <trans-unit id="52c49baa710d814dcf054568b203f1298d137cf7" translate="yes" xml:space="preserve">
          <source>\mathbf{L}</source>
          <target state="translated">\mathbf{L}</target>
        </trans-unit>
        <trans-unit id="eafd018e3a8aa85682e4f0b33612ce6efed642ca" translate="yes" xml:space="preserve">
          <source>\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</source>
          <target state="translated">\ mathbf {W} _ {SN} = \ dfrac {\ mathbf {W}} {\ sigma (\ mathbf {W})}, \ sigma (\ mathbf {W}) = \ max _ {\ mathbf {h} : \ mathbf {h} \ ne 0} \ dfrac {\ | \ mathbf {W} \ mathbf {h} \ | _2} {\ | \ mathbf {h} \ | _2}</target>
        </trans-unit>
        <trans-unit id="0b4343943b1e977c3f21f3ea5b47966a6b6ad676" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma}</source>
          <target state="translated">\mathbf{\Sigma}</target>
        </trans-unit>
        <trans-unit id="250361cfd8c9755de3dafc641e9f127bd071fcd2" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</source>
          <target state="translated">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</target>
        </trans-unit>
        <trans-unit id="4e7e4ed1e422b1e8789d9ac447a022c6ca481196" translate="yes" xml:space="preserve">
          <source>\mathbf{\Sigma}^{-1}</source>
          <target state="translated">\mathbf{\Sigma}^{-1}</target>
        </trans-unit>
        <trans-unit id="201a130976ca261c5bc7ef67511ced9f15d7653a" translate="yes" xml:space="preserve">
          <source>\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</source>
          <target state="translated">\ mathbf {w} = g \ dfrac {\ mathbf {v}} {\ | \ mathbf {v} \ |}</target>
        </trans-unit>
        <trans-unit id="df511dffcdad49a67cd5945ee0ab7aef8bb4f9fc" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 0.01)</source>
          <target state="translated">\ mathcal {N} (0, 0.01)</target>
        </trans-unit>
        <trans-unit id="8f3ba18912099017c093331676f6188e744efdac" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 1)</source>
          <target state="translated">\ mathcal {N} (0, 1)</target>
        </trans-unit>
        <trans-unit id="50c7a47dd01b89919d5422d0295475337aab6ace" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, \text{std}^2)</source>
          <target state="translated">\ mathcal {N} (0, \ text {std} ^ 2)</target>
        </trans-unit>
        <trans-unit id="92c992916044275832483fbb98179a2d00ca4c1e" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(\text{mean}, \text{std}^2)</source>
          <target state="translated">\ mathcal {N} (\ text {평균}, \ text {std} ^ 2)</target>
        </trans-unit>
        <trans-unit id="87b1ef0bb4d4a0924cc95ef538f83b2bb3e53e42" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\sqrt{k}, \sqrt{k})</source>
          <target state="translated">\ mathcal {U} (-\ sqrt {k}, \ sqrt {k})</target>
        </trans-unit>
        <trans-unit id="d7f6ed3e182dc030701ad2deb57bf7caa5b88d6f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\text{bound}, \text{bound})</source>
          <target state="translated">\ mathcal {U} (-\ text {바운드}, \ text {바운드})</target>
        </trans-unit>
        <trans-unit id="325a9ed78efc110eaab4160ef2c97f9ac713df84" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-a, a)</source>
          <target state="translated">\ mathcal {U} (-a, a)</target>
        </trans-unit>
        <trans-unit id="b28ca9812620dfef2c686761b7aa8334acb0312f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(0, 1)</source>
          <target state="translated">\ mathcal {U} (0, 1)</target>
        </trans-unit>
        <trans-unit id="b9712b8b025515dc1dea5f46b97d3c4fe634d954" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(\text{lower}, \text{upper})</source>
          <target state="translated">\ mathcal {U} (\ text {lower}, \ text {upper})</target>
        </trans-unit>
        <trans-unit id="e216b98083013f9a980a8176c0057ec95ff5e691" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(a, b)</source>
          <target state="translated">\ mathcal {U} (a, b)</target>
        </trans-unit>
        <trans-unit id="2df7cbd2cee6b04f561cf080750aba56226af4cb" translate="yes" xml:space="preserve">
          <source>\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\ mathrm {erfc} (x) = 1-\ frac {2} {\ sqrt {\ pi}} \ int_ {0} ^ {x} e ^ {-t ^ 2} dt</target>
        </trans-unit>
        <trans-unit id="20a69b6b57674088939e407b3b5098441f752171" translate="yes" xml:space="preserve">
          <source>\mathrm{erfinv}(\mathrm{erf}(x)) = x</source>
          <target state="translated">\ mathrm {erfinv} (\ mathrm {erf} (x)) = x</target>
        </trans-unit>
        <trans-unit id="56d825970c3fb1fe7543e6c61686e2830fac7d9f" translate="yes" xml:space="preserve">
          <source>\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\ mathrm {erf} (x) = \ frac {2} {\ sqrt {\ pi}} \ int_ {0} ^ {x} e ^ {-t ^ 2} dt</target>
        </trans-unit>
        <trans-unit id="5cc0da2aee8ceedc2aa92a9553e40d60b942829b" translate="yes" xml:space="preserve">
          <source>\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}</source>
          <target state="translated">\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}</target>
        </trans-unit>
        <trans-unit id="3e9c0d2c84e11d8338d1dac942d5dede72bd1acf" translate="yes" xml:space="preserve">
          <source>\min(input.size(-1), input.size(-2))</source>
          <target state="translated">\ min (input.size (-1), input.size (-2))</target>
        </trans-unit>
        <trans-unit id="3a4e56595df1d02f21e5c3ff7b0e9d80921858ff" translate="yes" xml:space="preserve">
          <source>\mu</source>
          <target state="translated">\mu</target>
        </trans-unit>
        <trans-unit id="c8e2d1a0bf50a27d43ade30cfb048d99feb31ad1" translate="yes" xml:space="preserve">
          <source>\odot</source>
          <target state="translated">\odot</target>
        </trans-unit>
        <trans-unit id="73b077a63e22815fe5c8ee82dab9894be842b19c" translate="yes" xml:space="preserve">
          <source>\omega</source>
          <target state="translated">\omega</target>
        </trans-unit>
        <trans-unit id="72166555a6db55785fc6fbe9a7c5bbe72be28db8" translate="yes" xml:space="preserve">
          <source>\otimes</source>
          <target state="translated">\otimes</target>
        </trans-unit>
        <trans-unit id="bd84e490efa5f32ec0f75c3ea25a6efc19cfaa88" translate="yes" xml:space="preserve">
          <source>\pi^\theta</source>
          <target state="translated">\pi^\theta</target>
        </trans-unit>
        <trans-unit id="6dc2ada78a76bad95d3b921558b1195549985eab" translate="yes" xml:space="preserve">
          <source>\prod(\text{kernel\_size})</source>
          <target state="translated">\prod(\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="02cea321dbc3a3edfc9cc1780dfe84f694c585f0" translate="yes" xml:space="preserve">
          <source>\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</source>
          <target state="translated">\ psi (x) = \ frac {d} {dx} \ ln \ left (\ Gamma \ left (x \ right) \ right) = \ frac {\ Gamma '(x)} {\ Gamma (x)}</target>
        </trans-unit>
        <trans-unit id="b30c154a70c78f1c5dcecce58dde2e9ff4ec56aa" translate="yes" xml:space="preserve">
          <source>\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</source>
          <target state="translated">\ psi ^ {(n)} (x) = \ frac {d ^ {(n)}} {dx ^ {(n)}} \ psi (x)</target>
        </trans-unit>
        <trans-unit id="69c15416b63fd933850978302bcda59da879774e" translate="yes" xml:space="preserve">
          <source>\sigma</source>
          <target state="translated">\sigma</target>
        </trans-unit>
        <trans-unit id="bfe16f27ebc966df6f10ba356a1547b6e7242dd7" translate="yes" xml:space="preserve">
          <source>\sqrt{2}</source>
          <target state="translated">\sqrt{2}</target>
        </trans-unit>
        <trans-unit id="358161536f25000be6a773604fcf4a28afd7b7bd" translate="yes" xml:space="preserve">
          <source>\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</source>
          <target state="translated">\ sqrt {\ frac {2} {1 + \ text {negative \ _slope} ^ 2}}</target>
        </trans-unit>
        <trans-unit id="64126f7d8d3d661d4e29a191268f98bf759903a3" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^K N_i}</source>
          <target state="translated">\ sqrt {\ prod_ {i = 1} ^ K N_i}</target>
        </trans-unit>
        <trans-unit id="976e9a0789eb95323325b92e6edc3b432c6754b8" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^d N_i}</source>
          <target state="translated">\ sqrt {\ prod_ {i = 1} ^ d N_i}</target>
        </trans-unit>
        <trans-unit id="9312c2748ccad7c25f8d92bc855a5a0b38989a51" translate="yes" xml:space="preserve">
          <source>\star</source>
          <target state="translated">\star</target>
        </trans-unit>
        <trans-unit id="3df82d7a797b96797b79d72f235bc018cb8155c9" translate="yes" xml:space="preserve">
          <source>\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</source>
          <target state="translated">\ sum_ {t =-\ infty} ^ {\ infty} | w | ^ 2 [nt \ times hop \ _length] \ cancel {=} 0</target>
        </trans-unit>
        <trans-unit id="a12be8b8923c0e92cce3f35968e39960bc665833" translate="yes" xml:space="preserve">
          <source>\tanh</source>
          <target state="translated">\tanh</target>
        </trans-unit>
        <trans-unit id="053658991aeb9a94a57c16cbe979538b3eca3b46" translate="yes" xml:space="preserve">
          <source>\texttt{n\_classes}</source>
          <target state="translated">\texttt{n\_classes}</target>
        </trans-unit>
        <trans-unit id="79df4825cae48291f72d91cc9840f2adcb2716c4" translate="yes" xml:space="preserve">
          <source>\texttt{result[i]}</source>
          <target state="translated">\texttt{result[i]}</target>
        </trans-unit>
        <trans-unit id="8a01c6904694a6040cbbe4d31b35e67c5fb8c5bc" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p\_tensor[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p\_tensor[i]})</target>
        </trans-unit>
        <trans-unit id="248f05fb27cede89d993bc3bb9750c4fde2467c2" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p})</target>
        </trans-unit>
        <trans-unit id="15ca5fa99d2411cc7a336f5df78382fd090360c9" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{self[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{self[i]})</target>
        </trans-unit>
        <trans-unit id="f4308a3c6285dab040bdf907914e0906e5918d43" translate="yes" xml:space="preserve">
          <source>\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</source>
          <target state="translated">\ text {CELU} (x) = \ max (0, x) + \ min (0, \ alpha * (\ exp (x / \ alpha)-1))</target>
        </trans-unit>
        <trans-unit id="1c5fbaf107a1bd80dcb32b25718623eef6cabd9f" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</source>
          <target state="translated">\ text {ELU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x&amp;gt; 0 \\ \ alpha * (\ exp (x)-1), &amp;amp; \ text {if} x \ leq 0 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="ee2c000c326328286929b5abe78d5f28ecc355ca" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</source>
          <target state="translated">\ text {ELU} (x) = \ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1))</target>
        </trans-unit>
        <trans-unit id="1ae2cb648d379e90a17c82c0bca8393180166a13" translate="yes" xml:space="preserve">
          <source>\text{GELU}(x) = x * \Phi(x)</source>
          <target state="translated">\ text {GELU} (x) = x * \ Phi (x)</target>
        </trans-unit>
        <trans-unit id="98d15f980c451db5bf1f8450a31ea12ac80b261b" translate="yes" xml:space="preserve">
          <source>\text{GLU}(a, b) = a \otimes \sigma(b)</source>
          <target state="translated">\ text {GLU} (a, b) = a \ otimes \ sigma (b)</target>
        </trans-unit>
        <trans-unit id="2b0fd43cdc620c274316d22c100c5a1cedf8758f" translate="yes" xml:space="preserve">
          <source>\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {HardShrink} (x) = \ begin {cases} x, &amp;amp; \ text {if} x&amp;gt; \ lambda \\ x, &amp;amp; \ text {if} x &amp;lt;-\ lambda \\ 0, &amp;amp; \ text {그렇지 않으면 } \ end {cases}</target>
        </trans-unit>
        <trans-unit id="905f89b6d5fe72acb1c8befdbd1fbd394ccc9c79" translate="yes" xml:space="preserve">
          <source>\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</source>
          <target state="translated">\ text {HardTanh} (x) = \ begin {cases} 1 &amp;amp; \ text {if} x&amp;gt; 1 \\ -1 &amp;amp; \ text {if} x &amp;lt;-1 \\ x &amp;amp; \ text {그렇지 않으면} \\ \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c0b2885acfe7c1d78f13cbb375808a8a70622e9c" translate="yes" xml:space="preserve">
          <source>\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\ text {Hardsigmoid} (x) = \ begin {cases} 0 &amp;amp; \ text {if ~} x \ le -3, \\ 1 &amp;amp; \ text {if ~} x \ ge +3, \\ x / 6 + 1/2 &amp;amp; \ text {otherwise} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="3707baa731a2a1d93fe314a962f04a7440710fa1" translate="yes" xml:space="preserve">
          <source>\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\ text {Hardswish} (x) = \ begin {cases} 0 &amp;amp; \ text {if ~} x \ le -3, \\ x &amp;amp; \ text {if ~} x \ ge +3, \\ x \ cdot ( x + 3) / 6 &amp;amp; \ text {otherwise} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="f4d15ebefe7344cee7608a90993d80b6ed04f631" translate="yes" xml:space="preserve">
          <source>\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {LeakyRELU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x \ geq 0 \\ \ text {negative \ _slope} \ times x, &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="68988527b0b6237352fc1f66e3f8116ea81dd84a" translate="yes" xml:space="preserve">
          <source>\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</source>
          <target state="translated">\ text {LeakyReLU} (x) = \ max (0, x) + \ text {negative \ _slope} * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="51a86607b1de32af3fd6faa8dfe21281f15bc36b" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</source>
          <target state="translated">\ text {LogSigmoid} (x) = \ log \ left (\ frac {1} {1 + \ exp (-x)} \ right)</target>
        </trans-unit>
        <trans-unit id="a4939da8f3a870de43d45395bdbb72f415fd6a5c" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</source>
          <target state="translated">\ text {LogSigmoid} (x_i) = \ log \ left (\ frac {1} {1 + \ exp (-x_i)} \ right)</target>
        </trans-unit>
        <trans-unit id="a58af29fea78e99ffe2b4d0392c259d952558379" translate="yes" xml:space="preserve">
          <source>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</source>
          <target state="translated">\ text {LogSoftmax} (x_ {i}) = \ log \ left (\ frac {\ exp (x_i)} {\ sum_j \ exp (x_j)} \ right)</target>
        </trans-unit>
        <trans-unit id="96135669cb29b74a14e76462df05bf97eef0ca84" translate="yes" xml:space="preserve">
          <source>\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</source>
          <target state="translated">\ text {MultiHead} (Q, K, V) = \ text {Concat} (head_1, \ dots, head_h) W ^ O \ text {where} head_i = \ text {Attention} (QW_i ^ Q, KW_i ^ K, VW_i ^ V)</target>
        </trans-unit>
        <trans-unit id="ccbab99b824f6a91f1e1bf82a1a0fdaf575b1bb6" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {PReLU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x \ geq 0 \\ ax, &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="caa4e9686c6c337eb30002e3bdcb6a130c86d148" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</source>
          <target state="translated">\ text {PReLU} (x) = \ max (0, x) + \ text {weight} * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="5b347087764b6a93bb6f3ca448de0809ea32466b" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</source>
          <target state="translated">\ text {PReLU} (x) = \ max (0, x) + a * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="8ab73665e19333454b7dcdadd14407c6f4eaf162" translate="yes" xml:space="preserve">
          <source>\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {RReLU} (x) = \ begin {cases} x &amp;amp; \ text {if} x \ geq 0 \\ ax &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="ca059eb696d6eaad86cfafe12af59ddef8aeb677" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(0,x), 6)</source>
          <target state="translated">\ text {ReLU6} (x) = \ min (\ max (0, x), 6)</target>
        </trans-unit>
        <trans-unit id="be84129fb353e250c9169d66a9b03062f62f4151" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</source>
          <target state="translated">\ text {ReLU6} (x) = \ min (\ max (x_0, x), q (6))</target>
        </trans-unit>
        <trans-unit id="3bfab1dc2c67446ab1cfb128a9c2bc89afb1f336" translate="yes" xml:space="preserve">
          <source>\text{ReLU}</source>
          <target state="translated">\text{ReLU}</target>
        </trans-unit>
        <trans-unit id="08bf65cb7cffc00d2f4597ea4d30a22a63976e74" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x) = (x)^+ = \max(0, x)</source>
          <target state="translated">\ text {ReLU} (x) = (x) ^ + = \ max (0, x)</target>
        </trans-unit>
        <trans-unit id="836fcdcc8abf1c798414718b995cf2a357c6c5a8" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x)= \max(x_0, x)</source>
          <target state="translated">\ text {ReLU} (x) = \ max (x_0, x)</target>
        </trans-unit>
        <trans-unit id="cab6677fddc14bf3ddd26b8fe4fe9e7d772b3259" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\ text {SELU} (x) = \ text {scale} * (\ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1)))</target>
        </trans-unit>
        <trans-unit id="0eacb9a4c9b22d795a671d9085d3576f65f88226" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\ text {SELU} (x) = 배율 * (\ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1)))</target>
        </trans-unit>
        <trans-unit id="14b5832965c3a723de26aea464be1a7e1207b0e7" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {시그 모이 드} (x) = \ frac {1} {1 + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="dd6633f7f328cd88d5fded389f4a69b2a11f8bed" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {시그 모이 드} (x) = \ sigma (x) = \ frac {1} {1 + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="fa68ac444b17ef606bcb6fc0587fbb0c08130721" translate="yes" xml:space="preserve">
          <source>\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x + \lambda, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {SoftShrinkage} (x) = \ begin {cases} x-\ lambda, &amp;amp; \ text {if} x&amp;gt; \ lambda \\ x + \ lambda, &amp;amp; \ text {if} x &amp;lt;-\ lambda \\ 0 , &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="16ab4540ec544f6557be83896c80fd5ae2348d4a" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{ 1 + |x|}</source>
          <target state="translated">\ text {SoftSign} (x) = \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="b4e8d4178c0a5a0115f54178dd9083d36fddd624" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{1 + |x|}</source>
          <target state="translated">\ text {SoftSign} (x) = \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="a2f536bf3a8bd1033dd978de393c848edde7120d" translate="yes" xml:space="preserve">
          <source>\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</source>
          <target state="translated">\ text {Softmax} (x_ {i}) = \ frac {\ exp (x_i)} {\ sum_j \ exp (x_j)}</target>
        </trans-unit>
        <trans-unit id="5b81ade15ec338b468467f739fb0a7a9ffbb69c9" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x) = \text{Softmax}(-x)</source>
          <target state="translated">\ text {Softmin} (x) = \ text {Softmax} (-x)</target>
        </trans-unit>
        <trans-unit id="f07f1bdd8c02f9e56e3ec0dac4ae00ebee846aae" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</source>
          <target state="translated">\ text {Softmin} (x_ {i}) = \ frac {\ exp (-x_i)} {\ sum_j \ exp (-x_j)}</target>
        </trans-unit>
        <trans-unit id="2b624f5a535d4c37714f3d81447e39c785f806f7" translate="yes" xml:space="preserve">
          <source>\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</source>
          <target state="translated">\ text {Softplus} (x) = \ frac {1} {\ beta} * \ log (1 + \ exp (\ beta * x))</target>
        </trans-unit>
        <trans-unit id="c0fd3a2a1e0641ec623557f2f43fda5f7c89dc6c" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \tanh(x)</source>
          <target state="translated">\ text {Tanhshrink} (x) = x-\ tanh (x)</target>
        </trans-unit>
        <trans-unit id="995fcea13ed41fcda67e8f62153baff1abf82a33" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \text{Tanh}(x)</source>
          <target state="translated">\ text {Tanhshrink} (x) = x-\ text {Tanh} (x)</target>
        </trans-unit>
        <trans-unit id="d0334b6a3fe1d679df6ff4bfb438db0b64230682" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh} (x) = \ tanh (x) = \ frac {\ exp (x)-\ exp (-x)} {\ exp (x) + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="75a10715f4ce8ac09932b779e24b1a0a5468348f" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh} (x) = \ tanh (x) = \ frac {\ exp (x)-\ exp (-x)} {\ exp (x) + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="f169080e8d1eb07929eead4219c50bde74fb23da" translate="yes" xml:space="preserve">
          <source>\text{batch1} \mathbin{@} \text{batch2}</source>
          <target state="translated">\ text {batch1} \ mathbin {@} \ text {batch2}</target>
        </trans-unit>
        <trans-unit id="229245bb08fb0569d7993b5cea15398607e69900" translate="yes" xml:space="preserve">
          <source>\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}</source>
          <target state="translated">\ text {bound} = \ text {gain} \ times \ sqrt {\ frac {3} {\ text {fan \ _mode}}}</target>
        </trans-unit>
        <trans-unit id="31ee1b43627512c3204fd0ea19131e5c35c60dc9" translate="yes" xml:space="preserve">
          <source>\text{gamma}^{\text{cycle iterations}}</source>
          <target state="translated">\ text {감마} ^ {\ text {사이클 반복}}</target>
        </trans-unit>
        <trans-unit id="3e2f5fb306be83947e6aad79d83c918b084bbee4" translate="yes" xml:space="preserve">
          <source>\text{in\_channels}</source>
          <target state="translated">\text{in\_channels}</target>
        </trans-unit>
        <trans-unit id="26c469932ffffddf5378f023f97a627052bcde45" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;gt; \text{other}</source>
          <target state="translated">\ text {입력}&amp;gt; \ text {기타}</target>
        </trans-unit>
        <trans-unit id="c15cd245b2e6c1904e5b0a29dc06bb37cb23bb5e" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;lt; \text{other}</source>
          <target state="translated">\ text {입력} &amp;lt;\ text {기타}</target>
        </trans-unit>
        <trans-unit id="f395bedc77d1db475982b0b075906a247b84c2eb" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target} * \log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log (\ text {input} + \ text {eps})</target>
        </trans-unit>
        <trans-unit id="686d55b3f5e0392f4e8fcfd75b77c03037718ecf" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target}*\log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log (\ text {input} + \ text {eps})</target>
        </trans-unit>
        <trans-unit id="6da1a408fc1384eb95e3208a559d3a3e0f03e99f" translate="yes" xml:space="preserve">
          <source>\text{input} = Q R</source>
          <target state="translated">\ text {input} = QR</target>
        </trans-unit>
        <trans-unit id="c4dd474d77793c7a92038a0755b05fe0ba4adf17" translate="yes" xml:space="preserve">
          <source>\text{input} = V \text{diag}(e) V^T</source>
          <target state="translated">\ text {input} = V \ text {diag} (e) V ^ T</target>
        </trans-unit>
        <trans-unit id="e721787652f4fc0b6f66950d386917c38d8957f8" translate="yes" xml:space="preserve">
          <source>\text{input} \geq \text{other}</source>
          <target state="translated">\ text {입력} \ geq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="3cbe8a936fc7cb3a9fdf09ad329713fbe61317e4" translate="yes" xml:space="preserve">
          <source>\text{input} \leq \text{other}</source>
          <target state="translated">\ text {입력} \ leq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="3901704e8ee508e9522d87ef1f88b52ef027532b" translate="yes" xml:space="preserve">
          <source>\text{input} \neq \text{other}</source>
          <target state="translated">\ text {입력} \ neq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="ccf6648a02673a0bbbd4f96889ba48888a0e40f4" translate="yes" xml:space="preserve">
          <source>\text{input}[i, j]</source>
          <target state="translated">\ text {입력} [i, j]</target>
        </trans-unit>
        <trans-unit id="47eeab2292dd874946bd7e66f27cc66b11eafc5e" translate="yes" xml:space="preserve">
          <source>\text{input}_{i}</source>
          <target state="translated">\text{input}_{i}</target>
        </trans-unit>
        <trans-unit id="dd2a901c455f604c6af98c920667ce74d801ef8c" translate="yes" xml:space="preserve">
          <source>\text{input}_{i} / \text{other}_{i}</source>
          <target state="translated">\ text {input} _ {i} / \ text {기타} _ {i}</target>
        </trans-unit>
        <trans-unit id="c920fc92a2f279db08ab80d5217f4a1714bd40d6" translate="yes" xml:space="preserve">
          <source>\text{i}^{th}</source>
          <target state="translated">\text{i}^{th}</target>
        </trans-unit>
        <trans-unit id="6ca0545c1ccedc4ff77a3d4acb74e3770c0fbbdd" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]})</source>
          <target state="translated">\ text {커널 \ _size [0]}, \ text {커널 \ _size [1]})</target>
        </trans-unit>
        <trans-unit id="9229970eeb833df18b71461a132cd3b0ce98ef3a" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</source>
          <target state="translated">\ text {kernel \ _size [0]}, \ text {kernel \ _size [1]}, \ text {kernel \ _size [2]})</target>
        </trans-unit>
        <trans-unit id="15869cb2da85e3f8dc3a924d02c0d13f080d42d1" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size})</source>
          <target state="translated">\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="80b2d9d0a1aff139a212f622b578c27a2cb6c126" translate="yes" xml:space="preserve">
          <source>\text{k}^{th}</source>
          <target state="translated">\text{k}^{th}</target>
        </trans-unit>
        <trans-unit id="0f84b2b9c4507f4c38935799e7145beab872e3ff" translate="yes" xml:space="preserve">
          <source>\text{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})</source>
          <target state="translated">\ text {logcumsumexp} (x) _ {ij} = \ log \ sum \ limits_ {j = 0} ^ {i} \ exp (x_ {ij})</target>
        </trans-unit>
        <trans-unit id="e4fff2f19ba639fc2ee13a3cda790993ad87fdfe" translate="yes" xml:space="preserve">
          <source>\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})</source>
          <target state="translated">\ text {logsumexp} (x) _ {i} = \ log \ sum_j \ exp (x_ {ij})</target>
        </trans-unit>
        <trans-unit id="e990f63d798552bb3d1da254e1d0c3c11424c266" translate="yes" xml:space="preserve">
          <source>\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</source>
          <target state="translated">\ text {loss} = \ frac {\ sum ^ {N} _ {i = 1} loss (i, class [i])} {\ sum ^ {N} _ {i = 1} weight [class [i] ]}</target>
        </trans-unit>
        <trans-unit id="65ca80a02b47d73c67e5a3e63c9b314787018d8d" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</source>
          <target state="translated">\ text {손실} (x, 클래스) =-\ log \ left (\ frac {\ exp (x [클래스])} {\ sum_j \ exp (x [j])} \ right) = -x [클래스] + \ log \ left (\ sum_j \ exp (x [j]) \ right)</target>
        </trans-unit>
        <trans-unit id="019a711532dd32707cd8b1b52a85f9dcf92bf993" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</source>
          <target state="translated">\ text {손실} (x, 클래스) = 가중치 [클래스] \ left (-x [클래스] + \ log \ left (\ sum_j \ exp (x [j]) \ right) \ right)</target>
        </trans-unit>
        <trans-unit id="ffd0b22341cc6c018e0b5728d553ab798985dbfc" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp;amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp;amp; \text{if } y = -1 \end{cases}</source>
          <target state="translated">\ text {loss} (x, y) = \ begin {cases} 1-\ cos (x_1, x_2), &amp;amp; \ text {if} y = 1 \\ \ max (0, \ cos (x_1, x_2)- \ text {margin}), &amp;amp; \ text {if} y = -1 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c59e75cdc56fc3f83b35cdb9f628a1f6dc39d344" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {1} {n} \ sum_ {i} z_ {i}</target>
        </trans-unit>
        <trans-unit id="b06e3acda76e096ac3266b690a1f15f3c86a036c" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {\ sum_i \ max (0, \ text {margin}-x [y] + x [i])) ^ p} {\ text {x.size} ( 0)}</target>
        </trans-unit>
        <trans-unit id="932b5db2aa0a39e3cde91ee0926767654acd252f" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {\ sum_i \ max (0, w [y] * (\ text {margin}-x [y] + x [i])) ^ p)} {\ 텍스트 {x.size} (0)}</target>
        </trans-unit>
        <trans-unit id="1be1a15f43a5fe29e30cefc8a77697b52665cc11" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</source>
          <target state="translated">\ text {loss} (x, y) = \ sum_i \ frac {\ log (1 + \ exp (-y [i] * x [i]))} {\ text {x.nelement} ()}</target>
        </trans-unit>
        <trans-unit id="be8b3997d164f84206bbdb59fc30b16e4df28e7b" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss} (x, y) = \ sum_ {ij} \ frac {\ max (0, 1-(x [y [j]]-x [i]))} {\ text {x.size} (0)}</target>
        </trans-unit>
        <trans-unit id="ed773e5223ed4e8e7a2085c415f1e22caa9ee61a" translate="yes" xml:space="preserve">
          <source>\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</source>
          <target state="translated">\ text {손실} (x1, x2, y) = \ max (0, -y * (x1-x2) + \ text {margin})</target>
        </trans-unit>
        <trans-unit id="1a70593a5153f0c40ba1299b72f0b57903179f3e" translate="yes" xml:space="preserve">
          <source>\text{other}_{i}</source>
          <target state="translated">\text{other}_{i}</target>
        </trans-unit>
        <trans-unit id="17f7519844253d6013b5ffbe16cec0f08dcc6ff0" translate="yes" xml:space="preserve">
          <source>\text{out} = -1 \times \text{input}</source>
          <target state="translated">\ text {out} = -1 \ times \ text {input}</target>
        </trans-unit>
        <trans-unit id="90a41651feeb08c3eedd09a310ec016d8b14ad0c" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {mat1} _i \ mathbin {@} \ text {mat2} _i)</target>
        </trans-unit>
        <trans-unit id="bb3213012476cae8440fbe19a6e07cd71a7793bd" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {mat} \ mathbin {@} \ text {vec})</target>
        </trans-unit>
        <trans-unit id="e949532f9deb6bd6d752ab7f572576835541d193" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {vec1} \ otimes \ text {vec2})</target>
        </trans-unit>
        <trans-unit id="0358e27d2d15a38fee22f3930f0704830355a8e2" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j</source>
          <target state="translated">\ text {out} = \ text {abs} \ cdot \ cos (\ text {angle}) + \ text {abs} \ cdot \ sin (\ text {angle}) \ cdot j</target>
        </trans-unit>
        <trans-unit id="36eb11cd26daa728869e9dc0ff4a77afceb36308" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{alpha} \times \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {alpha} \ times \ text {other}</target>
        </trans-unit>
        <trans-unit id="256d0e45774d81cdb6896827af46bd880155747c" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {other}</target>
        </trans-unit>
        <trans-unit id="b32a02ba7ef69d1c7496b06c7c87e64e5599ad7d" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)</source>
          <target state="translated">\ text {out} (N_i, C_j, l) = \ frac {1} {k} \ sum_ {m = 0} ^ {k-1} \ text {input} (N_i, C_j, \ text {stride} \ 곱하기 l + m)</target>
        </trans-unit>
        <trans-unit id="6ffc018e43b40a0d8b659cfa4eb86a17bb9a021a" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out} (N_i, C _ {\ text {out} _j}) = \ text {bias} (C _ {\ text {out} _j}) + \ sum_ {k = 0} ^ {C _ {\ text { in}}-1} \ text {weight} (C _ {\ text {out} _j}, k) \ star \ text {input} (N_i, k)</target>
        </trans-unit>
        <trans-unit id="1329f06a718ae43bb8cf775c59ab239d1fb3b8cd" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out} (N_i, C _ {\ text {out} _j}) = \ text {bias} (C _ {\ text {out} _j}) + \ sum_ {k = 0} ^ {C_ {in}- 1} \ text {weight} (C _ {\ text {out} _j}, k) \ star \ text {input} (N_i, k)</target>
        </trans-unit>
        <trans-unit id="43e8084d30aefee2bbf06206dd0a8ab14834163f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \begin{cases} \text{x}_i &amp;amp; \text{if } \text{condition}_i \\ \text{y}_i &amp;amp; \text{otherwise} \\ \end{cases}</source>
          <target state="translated">\ text {out} _i = \ begin {cases} \ text {x} _i &amp;amp; \ text {if} \ text {condition} _i \\ \ text {y} _i &amp;amp; \ text {otherwise} \\ \ end {cases }</target>
        </trans-unit>
        <trans-unit id="887bfc6ee868fe88195f2b1a697c2be49d6ee9a3" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</source>
          <target state="translated">\ text {out} _i = \ beta \ \ text {input} _i + \ alpha \ (\ text {batch1} _i \ mathbin {@} \ text {batch2} _i)</target>
        </trans-unit>
        <trans-unit id="65617250468b820b83705d0c5a443bdbe573e367" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \frac{\text{input}_i}{\text{other}_i}</source>
          <target state="translated">\ text {out} _i = \ frac {\ text {input} _i} {\ text {other} _i}</target>
        </trans-unit>
        <trans-unit id="07bf2f98a46000c9d46f464978022925a2bd7e3f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ frac {\ text {tensor1} _i} {\ text {tensor2} _i}</target>
        </trans-unit>
        <trans-unit id="3da52d8260a7cb2e43645767f57dc80dcb7dfe10" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ text {tensor1} _i \ times \ text {tensor2} _i</target>
        </trans-unit>
        <trans-unit id="62f87f35ab6f8ad57797570d5573c2164ad3b265" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ mathbin {@} \ text {mat2} _i</target>
        </trans-unit>
        <trans-unit id="7a9589802a74e5f4ebdc7b3bf9edf66a572b8b2d" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \times \text{other}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ times \ text {other} _i</target>
        </trans-unit>
        <trans-unit id="598abd50041125894fc9d56c763e555dc53626ff" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{other} \times \text{input}_i</source>
          <target state="translated">\ text {out} _i = \ text {other} \ times \ text {input} _i</target>
        </trans-unit>
        <trans-unit id="f19bf9db2f65318c8d3279e96d9b9001602fb09a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{self} ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = \ text {self} ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="a7a2515cc1e93998b3a18636546dddfa38daeec0" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</source>
          <target state="translated">\ text {out} _i = \ text {start} _i + \ text {weight} _i \ times (\ text {end} _i-\ text {start} _i)</target>
        </trans-unit>
        <trans-unit id="0cb57186f10cc53449c5a3667b3e9e3171f06840" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ \text{exponent}</source>
          <target state="translated">\ text {out} _i = x_i ^ \ text {exponent}</target>
        </trans-unit>
        <trans-unit id="fc2ce21ca05c23e8481f3ba74137e106e861116a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = x_i ^ {\ text {지수} _i}</target>
        </trans-unit>
        <trans-unit id="199c9bafa2de3cddd25186555e3341b49eafa3be" translate="yes" xml:space="preserve">
          <source>\text{out}_i \sim \text{Poisson}(\text{input}_i)</source>
          <target state="translated">\ text {out} _i \ sim \ text {Poisson} (\ text {input} _i)</target>
        </trans-unit>
        <trans-unit id="ec5b93008f32dc9277b22a2479b439d557449a63" translate="yes" xml:space="preserve">
          <source>\text{out}_{i+1} = \text{out}_i + \text{step}.</source>
          <target state="translated">\ text {out} _ {i + 1} = \ text {out} _i + \ text {step}.</target>
        </trans-unit>
        <trans-unit id="ebe2d70cc579db08dc5e67ffe414031300d09f4c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}</source>
          <target state="translated">\ text {out} _ {i} = I_0 (\ text {input} _ {i}) = \ sum_ {k = 0} ^ {\ infty} \ frac {(\ text {input} _ {i} ^ 2 / 4) ^ k} {(k!) ^ 2}</target>
        </trans-unit>
        <trans-unit id="6b1003c5523ecc63d8ba6aacefacf06606afa128" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="f24a159b70e10e5357a57a49e58cd32d187c432a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="21103602f770a5f7ee7fc1da1e89140f56599227" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="84def1c39988847c83528f8fb8f04adcd967d458" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="0f2819fdd8d29dbcba0965986f63a77a316be87c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {1 + e ^ {-\ text {input} _ {i}}}</target>
        </trans-unit>
        <trans-unit id="e639a53f131fc76fecc291100aefd605f12a0eda" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {\ sqrt {\ text {input} _ {i}}}</target>
        </trans-unit>
        <trans-unit id="46ce5ca183590faeb94bd046c494af519bde768a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{\text{input}_{i}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {\ text {input} _ {i}}</target>
        </trans-unit>
        <trans-unit id="4427bc0696c242442c6213bd5a82d54aee1a2769" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1</source>
          <target state="translated">\ text {out} _ {i} = \ left \ lceil \ text {input} _ {i} \ right \ rceil = \ left \ lfloor \ text {input} _ {i} \ right \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="15280a01ee3fbee59abcbcd2395fe2dc53197b84" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor</source>
          <target state="translated">\ text {out} _ {i} = \ left \ lfloor \ text {input} _ {i} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="8713f714ca9f011825a27f3a64b5103537d7c3a8" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \log \Gamma(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ log \ Gamma (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="f106314a05b463158c27ca8e8540a802938b4b7b" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \operatorname{sgn}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ operatorname {sgn} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="6d66baddaec726fdba3a80fb59890298bae46f57" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sin(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sin (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="2029dbca4086f039d8d89647025660d63e959250" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sin^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sin ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="29f2eee43262fe3393a73b473efb38b2970a7d4d" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sinh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sinh (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="4f9ac0337247d9d45d980c107144e23646dfd988" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sinh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ sinh ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="a48b925f8709a2a6a8ef052050d18b589d5123a2" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sqrt{\text{input}_{i}^{2} + \text{other}_{i}^{2}}</source>
          <target state="translated">\ text {out} _ {i} = \ sqrt {\ text {input} _ {i} ^ {2} + \ text {other} _ {i} ^ {2}}</target>
        </trans-unit>
        <trans-unit id="a6adf034e3ba47318aad9958d92aa5caaeae39a3" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \sqrt{\text{input}_{i}}</source>
          <target state="translated">\ text {out} _ {i} = \ sqrt {\ text {input} _ {i}}</target>
        </trans-unit>
        <trans-unit id="3563e2ffc1ebfbc800c1dd7b12d56f17e2f84012" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tan(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tan (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="c84ff0d5cfb4e3368d70e9d3853ac2f7da77df34" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tan^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tan ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="339ee2ac3b749c3430c326bb801e434eb7dd88eb" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tanh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tanh (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="0bbbd60b143b37df1c047a994b490f440b3b4a12" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \tanh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ tanh ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="b2b840e12a553c229a2511c216662f4659dc95dc" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \text{input}_{i} - \left\lfloor |\text{input}_{i}| \right\rfloor * \operatorname{sgn}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ text {input} _ {i}-\ left \ lfloor | \ text {input} _ {i} | \ right \ rfloor * \ operatorname {sgn} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="07e6f589bfadaeb684ef52c979559ff70dd53446" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = angle(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = 각도 (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="3208732ebcf3371420c5abc0afe1728e365bcaab" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = conj(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = conj (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="b2990a218cc833a6ec80d5667b3951afb3dfc0ec" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = |\text{input}_{i}|</source>
          <target state="translated">\ text {out} _ {i} = | \ text {input} _ {i} |</target>
        </trans-unit>
        <trans-unit id="c6aa4724db682413c4a0d9e7da789301bde04767" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} \sim \mathcal{N}(0, 1)</source>
          <target state="translated">\ text {out} _ {i} \ sim \ mathcal {N} (0, 1)</target>
        </trans-unit>
        <trans-unit id="19b54f107e367ae800728c182219b0a6d113a55d" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} \ sim \ mathrm {Bernoulli} (p = \ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="0ff6f3181c37c49b168dc5bb2c5512278c4d8cdd" translate="yes" xml:space="preserve">
          <source>\text{out}_{{i+1}} = \text{out}_{i} + \text{step}</source>
          <target state="translated">\ text {out} _ {{i + 1}} = \ text {out} _ {i} + \ text {step}</target>
        </trans-unit>
        <trans-unit id="22972795246817e25ed6cd9e7f9534bb009c4be5" translate="yes" xml:space="preserve">
          <source>\text{padding\_back}</source>
          <target state="translated">\text{padding\_back}</target>
        </trans-unit>
        <trans-unit id="8f171d7dcd7097c55030a1f2855ec480075028b7" translate="yes" xml:space="preserve">
          <source>\text{padding\_bottom}</source>
          <target state="translated">\text{padding\_bottom}</target>
        </trans-unit>
        <trans-unit id="c61e815e96b628670818ef2117895748429c8232" translate="yes" xml:space="preserve">
          <source>\text{padding\_front}</source>
          <target state="translated">\text{padding\_front}</target>
        </trans-unit>
        <trans-unit id="db98a807cd5008387cbab3fe6e2aec41450b4604" translate="yes" xml:space="preserve">
          <source>\text{padding\_front}, \text{padding\_back})</source>
          <target state="translated">\ text {padding \ _front}, \ text {padding \ _back})</target>
        </trans-unit>
        <trans-unit id="71d180502f18ec5d773aee314a7a4111878c2b75" translate="yes" xml:space="preserve">
          <source>\text{padding\_left}</source>
          <target state="translated">\text{padding\_left}</target>
        </trans-unit>
        <trans-unit id="d6ccab1e4d0ca305991dad9a66b238c71e38726d" translate="yes" xml:space="preserve">
          <source>\text{padding\_right}</source>
          <target state="translated">\text{padding\_right}</target>
        </trans-unit>
        <trans-unit id="822c531daaae9a83e8cb9610863aa10968e367cb" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}</source>
          <target state="translated">\text{padding\_top}</target>
        </trans-unit>
        <trans-unit id="c863b416ef996ee31809e50a0bf7888a3fdd7ad8" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}, \text{padding\_bottom}</source>
          <target state="translated">\ text {padding \ _top}, \ text {padding \ _bottom}</target>
        </trans-unit>
        <trans-unit id="4dfb970fd31b03f94d4e4f1e23a1347628359b17" translate="yes" xml:space="preserve">
          <source>\text{padding\_top}, \text{padding\_bottom})</source>
          <target state="translated">\ text {padding \ _top}, \ text {padding \ _bottom})</target>
        </trans-unit>
        <trans-unit id="2940a7cd2f158af987520c8f34af379096300028" translate="yes" xml:space="preserve">
          <source>\text{scale} = 1.0507009873554804934193349852946</source>
          <target state="translated">\ text {스케일} = 1.0507009873554804934193349852946</target>
        </trans-unit>
        <trans-unit id="509bb45bc88eeba17ab4e18a7b0718847be009ba" translate="yes" xml:space="preserve">
          <source>\text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}</source>
          <target state="translated">\ text {silu} (x) = x * \ sigma (x), \ text {여기서} \ sigma (x) \ text {는 로지스틱 시그 모이 드입니다.}</target>
        </trans-unit>
        <trans-unit id="19a93b01d26977b01f0022e9ad1395871a79c6d2" translate="yes" xml:space="preserve">
          <source>\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</source>
          <target state="translated">\ text {similarity} = \ dfrac {x_1 \ cdot x_2} {\ max (\ Vert x_1 \ Vert _2 \ cdot \ Vert x_2 \ Vert _2, \ epsilon)}</target>
        </trans-unit>
        <trans-unit id="64fe2ce749a0723d9a9566b76783f0dec79a7cd9" translate="yes" xml:space="preserve">
          <source>\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}.</source>
          <target state="translated">\ text {similarity} = \ dfrac {x_1 \ cdot x_2} {\ max (\ Vert x_1 \ Vert _2 \ cdot \ Vert x_2 \ Vert _2, \ epsilon)}.</target>
        </trans-unit>
        <trans-unit id="1e184ebb3380e76e50f3e3cc8c8ba0cdbf171178" translate="yes" xml:space="preserve">
          <source>\text{spatial\_size}</source>
          <target state="translated">\text{spatial\_size}</target>
        </trans-unit>
        <trans-unit id="30d98357c0cf5aadd64e4c5685312bfb74e6ff43" translate="yes" xml:space="preserve">
          <source>\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}</source>
          <target state="translated">\ text {std} = \ frac {\ text {gain}} {\ sqrt {\ text {fan \ _mode}}}</target>
        </trans-unit>
        <trans-unit id="0bba3586f43350ca4e47d9af0d460338cdf45084" translate="yes" xml:space="preserve">
          <source>\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}</source>
          <target state="translated">\ text {std} = \ text {gain} \ times \ sqrt {\ frac {2} {\ text {fan \ _in} + \ text {fan \ _out}}}</target>
        </trans-unit>
        <trans-unit id="8ee7a6294338ef4ec43db737a79e7a5c5827ad5d" translate="yes" xml:space="preserve">
          <source>\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]</source>
          <target state="translated">\ text {stride} [i] = \ text {stride} [i + 1] \ times \ text {size} [i + 1]</target>
        </trans-unit>
        <trans-unit id="9ae213028d9116087df1c48df38a24906070e968" translate="yes" xml:space="preserve">
          <source>\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})</source>
          <target state="translated">\ text {target} * \ log (\ text {target})-\ text {target} + 0.5 * \ log (2 * \ pi * \ text {target})</target>
        </trans-unit>
        <trans-unit id="40e87ce770ee226cb8037a67d055f5046fdf9036" translate="yes" xml:space="preserve">
          <source>\text{target} \sim \mathrm{Poisson}(\text{input}) \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input}) + \log(\text{target!})</source>
          <target state="translated">\ text {target} \ sim \ mathrm {Poisson} (\ text {input}) \ text {loss} (\ text {input}, \ text {target}) = \ text {input}-\ text {target} * \ log (\ text {input}) + \ log (\ text {target!})</target>
        </trans-unit>
        <trans-unit id="ee61c683da63904db4a3e121bdf113933043f4d7" translate="yes" xml:space="preserve">
          <source>\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</source>
          <target state="translated">\ text {target} * \ log (\ text {target})-\ text {target} + 0.5 * \ log (2 \ pi \ text {target}).</target>
        </trans-unit>
        <trans-unit id="0f6bd28cfcaee12c0d20055b4b4afd7bf6b48113" translate="yes" xml:space="preserve">
          <source>\text{tensor1} / \text{tensor2}</source>
          <target state="translated">\ text {tensor1} / \ text {tensor2}</target>
        </trans-unit>
        <trans-unit id="075aa43f8957c1474a6931e41c96dcb7c5d686d6" translate="yes" xml:space="preserve">
          <source>\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \times eigenvectors[:, j + 1]</source>
          <target state="translated">\ text {진정한 고유 벡터} [j + 1] = 고유 벡터 [:, j]-i \ times 고유 벡터 [:, j + 1]</target>
        </trans-unit>
        <trans-unit id="dbbcfbb1d0a265eefc03e882be62153c1a959077" translate="yes" xml:space="preserve">
          <source>\text{true eigenvector}[j] = eigenvectors[:, j] + i \times eigenvectors[:, j + 1]</source>
          <target state="translated">\ text {진정한 고유 벡터} [j] = 고유 벡터 [:, j] + i \ times 고유 벡터 [:, j + 1]</target>
        </trans-unit>
        <trans-unit id="63386bdc7be9c10bbab18ac478e13f97470e863e" translate="yes" xml:space="preserve">
          <source>\text{val}</source>
          <target state="translated">\text{val}</target>
        </trans-unit>
        <trans-unit id="d6db479a087e6e41362768b25784ba255c6716a3" translate="yes" xml:space="preserve">
          <source>\text{vec1} \otimes \text{vec2}</source>
          <target state="translated">\ text {vec1} \ otimes \ text {vec2}</target>
        </trans-unit>
        <trans-unit id="64e1f89a912ee4486e65f62e989e6b10c06342f9" translate="yes" xml:space="preserve">
          <source>\text{win\_length} &amp;lt; \text{n\_fft}</source>
          <target state="translated">\ text {승리 \ _length} &amp;lt;\ text {n \ _fft}</target>
        </trans-unit>
        <trans-unit id="18afc597cccde66cf5c2fa4706aeb0646f3723b2" translate="yes" xml:space="preserve">
          <source>\text{window\_length} + 1</source>
          <target state="translated">\ text {창 \ _ 길이} + 1</target>
        </trans-unit>
        <trans-unit id="848da5fb0b7bd079124401e74232d55ffe67400b" translate="yes" xml:space="preserve">
          <source>\text{{heaviside}}(input, values) = \begin{cases} 0, &amp;amp; \text{if input &amp;lt; 0}\\ values, &amp;amp; \text{if input == 0}\\ 1, &amp;amp; \text{if input &amp;gt; 0} \end{cases}</source>
          <target state="translated">\ text {{heaviside}} (입력, 값) = \ begin {cases} 0, &amp;amp; \ text {입력 &amp;lt;0} \\ 값, &amp;amp; \ text {입력 == 0} \\ 1, &amp;amp; \ text {입력&amp;gt; 0} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="f6688d2b60281ad8fefcc0fea715a9f936e08052" translate="yes" xml:space="preserve">
          <source>\text{{out}}_i = \text{trunc} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right)</source>
          <target state="translated">\ text {{out}} _ i = \ text {trunc} \ left (\ frac {{\ text {{input}} _ i}} {{\ text {{other}} _ i}} \ right)</target>
        </trans-unit>
        <trans-unit id="b390b7373d30b8df651a5b7c8366caf960e3ef34" translate="yes" xml:space="preserve">
          <source>\text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i</source>
          <target state="translated">\ text {{out}} _ i = \ text {{input}} _ i-\ text {{alpha}} \ times \ text {{other}} _ i</target>
        </trans-unit>
        <trans-unit id="cb005d76f9f2e394a770c2562c2e150a413b3216" translate="yes" xml:space="preserve">
          <source>\theta</source>
          <target state="translated">\theta</target>
        </trans-unit>
        <trans-unit id="48eecc0a7bfa64d8c32f97e0fc816b2205b04bb8" translate="yes" xml:space="preserve">
          <source>\{0, \ldots, K-1\}</source>
          <target state="translated">\{0, \ldots, K-1\}</target>
        </trans-unit>
        <trans-unit id="86f7e437faa5a7fce15d1ddcb9eaeaea377667b8" translate="yes" xml:space="preserve">
          <source>a</source>
          <target state="translated">a</target>
        </trans-unit>
        <trans-unit id="985ea09fbb51e15e4429dc741d64e71fe6b0efab" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object</source>
          <target state="translated">&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; 의&lt;/a&gt; 객체</target>
        </trans-unit>
        <trans-unit id="d162d7e8bd1d5dda8190b029ffd491226ce6290d" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tuple&lt;/code&gt; of three ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the depth dimension, the second &lt;code&gt;int&lt;/code&gt; for the height dimension and the third &lt;code&gt;int&lt;/code&gt; for the width dimension</source>
          <target state="translated">3 개의 int로 구성된 &lt;code&gt;tuple&lt;/code&gt; &amp;ndash;이 경우 첫 번째 &lt;code&gt;int&lt;/code&gt; 는 깊이 차원에 사용되며 두 번째 &lt;code&gt;int&lt;/code&gt; 는 높이 차원에, 세 번째 &lt;code&gt;int&lt;/code&gt; 는 너비 차원에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="5e87ba64be191e5fb677cabf542708af2183633d" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tuple&lt;/code&gt; of two ints &amp;ndash; in which case, the first &lt;code&gt;int&lt;/code&gt; is used for the height dimension, and the second &lt;code&gt;int&lt;/code&gt; for the width dimension</source>
          <target state="translated">두 정수 의 &lt;code&gt;tuple&lt;/code&gt; &amp;ndash;이 경우 첫 번째 &lt;code&gt;int&lt;/code&gt; 는 높이 차원에 사용되고 두 번째 &lt;code&gt;int&lt;/code&gt; 는 너비 차원에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9af60a637080c119529287cf450a17dc51cdd322" translate="yes" xml:space="preserve">
          <source>a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}</source>
          <target state="translated">a = \ text {gain} \ times \ sqrt {\ frac {6} {\ text {fan \ _in} + \ text {fan \ _out}}}</target>
        </trans-unit>
        <trans-unit id="a18d36abd55ff2f8af389fa82ac5cf243f710013" translate="yes" xml:space="preserve">
          <source>a Tensor containing the result of module(input) located on output_device</source>
          <target state="translated">output_device에 위치한 모듈 (입력)의 결과를 포함하는 Tensor</target>
        </trans-unit>
        <trans-unit id="890f00f76ee1d37642ba29392aeb8c4a40b22625" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)</source>
          <target state="translated">[-inf, 0) 범위의 값을 가진 입력과 동일한 차원 및 모양의 텐서</target>
        </trans-unit>
        <trans-unit id="2003464995dec65fd9f3cf1a1961d2e3b1715419" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input with values in the range [0, 1]</source>
          <target state="translated">[0, 1] 범위의 값을 가진 입력과 동일한 차원 및 모양의 Tensor</target>
        </trans-unit>
        <trans-unit id="6fae14cf3c01e74728e043358ac48d17d6d7ff1f" translate="yes" xml:space="preserve">
          <source>a Tensor of the same dimension and shape as the input, with values in the range [0, 1]</source>
          <target state="translated">[0, 1] 범위의 값을 갖는 입력과 동일한 차원 및 모양의 텐서</target>
        </trans-unit>
        <trans-unit id="0fa1afd2e30272a3a23ae385fdbd6176134150f6" translate="yes" xml:space="preserve">
          <source>a class with the highest probability for each example</source>
          <target state="translated">각 예에 대해 가장 높은 확률을 가진 클래스</target>
        </trans-unit>
        <trans-unit id="45f2f8dde4763de997a5fc5976cf3a9430e5dc9f" translate="yes" xml:space="preserve">
          <source>a dictionary containing a whole state of the module</source>
          <target state="translated">모듈의 전체 상태를 포함하는 사전</target>
        </trans-unit>
        <trans-unit id="00a9b884880389883482dc751408a90f0cb9ee68" translate="yes" xml:space="preserve">
          <source>a dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export.</source>
          <target state="translated">입력 / 출력의 동적 축을 지정하는 사전 :-KEY : 입력 및 / 또는 출력 이름-VALUE : 주어진 키에 대한 동적 축의 색인 및 잠재적으로 내 보낸 동적 축에 사용될 이름. 일반적으로 값은 다음 방법 중 하나 또는 둘 모두의 조합에 따라 정의됩니다. (1). 제공된 입력의 동적 축을 지정하는 정수 목록입니다. 이 시나리오에서는 자동화 된 이름이 생성되고 내보내기 중에 제공된 입력 / 출력의 동적 축에 적용됩니다. 또는 (2). 해당 입 / 출력의 동적 축 인덱스에서 내보내기 중에 해당 입 / 출력 축에 적용하려는 이름으로 매핑을 지정하는 내부 사전입니다.</target>
        </trans-unit>
        <trans-unit id="fc0ea09618fde72378f33c4a6ddb59c0e4510cd0" translate="yes" xml:space="preserve">
          <source>a function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes:</source>
          <target state="translated">반복 프로세스를 추적하는 기능. 지정되면 LOBPCG 인스턴스를 인수로 사용하여 각 반복 단계에서 호출됩니다. LOBPCG 인스턴스는 다음 속성에서 반복 프로세스의 전체 상태를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="f41119365c981a28ba7e837a621a43e97595a3e6" translate="yes" xml:space="preserve">
          <source>a handle that can be used to remove the added hook by calling &lt;code&gt;handle.remove()&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;handle.remove()&lt;/code&gt; 를 호출하여 추가 된 후크를 제거하는 데 사용할 수있는 핸들</target>
        </trans-unit>
        <trans-unit id="ae26d64a3b3e80ec8246fb0560aeac16291f28f8" translate="yes" xml:space="preserve">
          <source>a list of available entrypoint names</source>
          <target state="translated">사용 가능한 진입 점 이름 목록</target>
        </trans-unit>
        <trans-unit id="9f1c5e47605b815eb437073ec1603759477ea676" translate="yes" xml:space="preserve">
          <source>a reference to the execution of &lt;code&gt;func&lt;/code&gt;. The value &lt;code&gt;T&lt;/code&gt; can only be accessed by forcing completion of &lt;code&gt;func&lt;/code&gt; through &lt;code&gt;torch.jit.wait&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 의 실행에 대한 참조 . &lt;code&gt;T&lt;/code&gt; 값 은 &lt;code&gt;torch.jit.wait&lt;/code&gt; 를 통해 &lt;code&gt;func&lt;/code&gt; 를 강제로 완료해야만 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="33218b2df65cce0092049e58dd8bb1cb81e976cb" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimension</source>
          <target state="translated">단일 &lt;code&gt;int&lt;/code&gt; &amp;ndash;이 경우 깊이, 높이 및 너비 치수에 동일한 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="850561b0cef31ac05a0c3f4be1e02622fadced9d" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the depth, height and width dimensions</source>
          <target state="translated">단일 &lt;code&gt;int&lt;/code&gt; &amp;ndash;이 경우 깊이, 높이 및 너비 치수에 동일한 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="a73b9f710ff83671961055a44ffd5cbbcae7d407" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimension</source>
          <target state="translated">단일 &lt;code&gt;int&lt;/code&gt; &amp;ndash;이 경우 높이 및 너비 치수에 동일한 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="13675467768e46f60e5b4830e00591213f355dd1" translate="yes" xml:space="preserve">
          <source>a single &lt;code&gt;int&lt;/code&gt; &amp;ndash; in which case the same value is used for the height and width dimensions</source>
          <target state="translated">단일 &lt;code&gt;int&lt;/code&gt; &amp;ndash;이 경우 높이 및 너비 치수에 동일한 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="779476176932b5394791dab09ad1d1763eee6c5b" translate="yes" xml:space="preserve">
          <source>a tensor located on &lt;code&gt;destination&lt;/code&gt; device, that is a result of concatenating &lt;code&gt;tensors&lt;/code&gt; along &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 을 따라 &lt;code&gt;tensors&lt;/code&gt; 를 연결 한 결과 인 &lt;code&gt;destination&lt;/code&gt; 장치 에있는 텐서 .</target>
        </trans-unit>
        <trans-unit id="db8d55dadf017c53a5cb22f260f3267b482ce8b0" translate="yes" xml:space="preserve">
          <source>a tensor of shape &lt;code&gt;Size([max(input) + 1])&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; is non-empty, else &lt;code&gt;Size(0)&lt;/code&gt;</source>
          <target state="translated">모양의 텐서 &lt;code&gt;Size([max(input) + 1])&lt;/code&gt; 경우 &lt;code&gt;input&lt;/code&gt; 그렇지 않은 빈 &lt;code&gt;Size(0)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a1ce48932d1afaa2c16e715f71dcc93e8712b7a3" translate="yes" xml:space="preserve">
          <source>a tuple containing &lt;code&gt;out&lt;/code&gt; tensors, each containing a chunk of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">함유하는 튜플 &lt;code&gt;out&lt;/code&gt; 텐서는 각각의 청크 함유 &lt;code&gt;tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ab2e28106a5b9ebc089b8e609fd50c57563064f9" translate="yes" xml:space="preserve">
          <source>a tuple containing &lt;code&gt;out&lt;/code&gt; tensors, each containing a copy of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">함유하는 튜플 &lt;code&gt;out&lt;/code&gt; 텐서는, 각각의 사본 함유 &lt;code&gt;tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3a204de89b91437261225e8faf7e8b031ac8eb66" translate="yes" xml:space="preserve">
          <source>a tuple containing chunks of &lt;code&gt;tensor&lt;/code&gt;, placed on &lt;code&gt;devices&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;devices&lt;/code&gt; 배치 된 &lt;code&gt;tensor&lt;/code&gt; 청크를 포함하는 튜플 .</target>
        </trans-unit>
        <trans-unit id="fc7fda381188f8a86d844f77a91f9092c0f7ea9a" translate="yes" xml:space="preserve">
          <source>a tuple containing copies of &lt;code&gt;tensor&lt;/code&gt;, placed on &lt;code&gt;devices&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;devices&lt;/code&gt; 배치 된 &lt;code&gt;tensor&lt;/code&gt; 복사본을 포함하는 튜플 .</target>
        </trans-unit>
        <trans-unit id="3cffaf9b9bcbb730e4c1694aa1ce58215c898bc4" translate="yes" xml:space="preserve">
          <source>above), and</source>
          <target state="translated">위) 및</target>
        </trans-unit>
        <trans-unit id="82451b41fd7878180b6aa2b54e369cbec4e8032c" translate="yes" xml:space="preserve">
          <source>abs</source>
          <target state="translated">abs</target>
        </trans-unit>
        <trans-unit id="da4dbfbc4fdc56237451cf5e9f9933b2b64f1ba6" translate="yes" xml:space="preserve">
          <source>absolute</source>
          <target state="translated">absolute</target>
        </trans-unit>
        <trans-unit id="8379b4f061ef5249954793884b3e333722e3a581" translate="yes" xml:space="preserve">
          <source>according to the</source>
          <target state="translated">에 따르면</target>
        </trans-unit>
        <trans-unit id="328f0742cf74e8ffdb253376b1803615055e215b" translate="yes" xml:space="preserve">
          <source>acos</source>
          <target state="translated">acos</target>
        </trans-unit>
        <trans-unit id="cae11b2912fc7a669ec8e6fd1ef1797da899527b" translate="yes" xml:space="preserve">
          <source>acosh() -&amp;gt; Tensor</source>
          <target state="translated">acosh ()-&amp;gt; 텐서</target>
        </trans-unit>
        <trans-unit id="43b9ab0810a93aacba6924ceb0586f3a5ded5277" translate="yes" xml:space="preserve">
          <source>acosh_() -&amp;gt; Tensor</source>
          <target state="translated">acosh_ ()-&amp;gt; 텐서</target>
        </trans-unit>
        <trans-unit id="188c404e728c43b5646acbca5ba76bfe689b737f" translate="yes" xml:space="preserve">
          <source>across all input channels. If called with &lt;code&gt;nn.PReLU(nChannels)&lt;/code&gt;, a separate</source>
          <target state="translated">모든 입력 채널에서. &lt;code&gt;nn.PReLU(nChannels)&lt;/code&gt; 와 함께 호출되면 별도의</target>
        </trans-unit>
        <trans-unit id="6bb3255719c068be2327cb70a2b8e43a35888f5b" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool1d</source>
          <target state="translated">adaptive_avg_pool1d</target>
        </trans-unit>
        <trans-unit id="c61fa142ae65a4935debc614087fb58ff3a6df54" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool2d</source>
          <target state="translated">adaptive_avg_pool2d</target>
        </trans-unit>
        <trans-unit id="14ce4a9f26f8969b6890e407e6f51988efbeea4c" translate="yes" xml:space="preserve">
          <source>adaptive_avg_pool3d</source>
          <target state="translated">adaptive_avg_pool3d</target>
        </trans-unit>
        <trans-unit id="3aaf9eadafea10463817e8d201b7f07b7265e014" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool1d</source>
          <target state="translated">adaptive_max_pool1d</target>
        </trans-unit>
        <trans-unit id="d3d98fd5af00d45465427af1e01b6955b7cc0483" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool2d</source>
          <target state="translated">adaptive_max_pool2d</target>
        </trans-unit>
        <trans-unit id="a3a21897a04cb4aa98f34167bb5bef9a6168c500" translate="yes" xml:space="preserve">
          <source>adaptive_max_pool3d</source>
          <target state="translated">adaptive_max_pool3d</target>
        </trans-unit>
        <trans-unit id="58d1bbce297de3c304a9fefc3b483181872a5c6b" translate="yes" xml:space="preserve">
          <source>add</source>
          <target state="translated">add</target>
        </trans-unit>
        <trans-unit id="5a96c9fe756bb4bf51deba8e7d88183c67947c4a" translate="yes" xml:space="preserve">
          <source>add (nonzero alpha not supported)</source>
          <target state="translated">추가 (0이 아닌 알파는 지원되지 않음)</target>
        </trans-unit>
        <trans-unit id="d8cab2345ee84bff05fb1af92dc904a85a08122e" translate="yes" xml:space="preserve">
          <source>add_relu</source>
          <target state="translated">add_relu</target>
        </trans-unit>
        <trans-unit id="be4a7c50ca01a7fc792facb9bd955bb98f5efd0f" translate="yes" xml:space="preserve">
          <source>add_scalar</source>
          <target state="translated">add_scalar</target>
        </trans-unit>
        <trans-unit id="d22b6f87e06eb3844a28e850f32f0a7a889d92fe" translate="yes" xml:space="preserve">
          <source>addmm</source>
          <target state="translated">addmm</target>
        </trans-unit>
        <trans-unit id="015bd815072238008dba22c71b1f12d4a0e9ce46" translate="yes" xml:space="preserve">
          <source>affine_grid</source>
          <target state="translated">affine_grid</target>
        </trans-unit>
        <trans-unit id="8bde722d88ff10934184bb1cc67a307b1553065a" translate="yes" xml:space="preserve">
          <source>after a restart. Default: 1.</source>
          <target state="translated">다시 시작한 후. 기본값 : 1.</target>
        </trans-unit>
        <trans-unit id="186f4cfeddd96df3f01de118db573839422f502a" translate="yes" xml:space="preserve">
          <source>after restart, set</source>
          <target state="translated">다시 시작한 후 설정</target>
        </trans-unit>
        <trans-unit id="5dc5329726e33c5c40a0fde2a6492a766965bdc3" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Cat&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Cat&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="cff8a9de6aa7fa5fc4f28c651af38f742f1fcebf" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._DependentProperty&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._DependentProperty&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="c6de7425cb1a6932219fdf4542beb8455ae1f2ce" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._GreaterThan&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._GreaterThan&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="27d8f16c0fa611de1e5b1506b6023adc7a8d2f87" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._GreaterThanEq&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._GreaterThanEq&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="4e2f7434e9b90fc50cc6286977410f04d80e91a9" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._HalfOpenInterval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._HalfOpenInterval&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="91d112cab3026dc93f2178836f77313f2d104de5" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._IntegerInterval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._IntegerInterval&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="6d32f64a8596e02e3341c04582c89898ba004bc0" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Interval&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Interval&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="6faf50bed58cc08be17966a6d90c9e06a9764c5f" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._LessThan&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._LessThan&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="5120abef070748d281f11bc2b98b4ec00cc7739e" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;torch.distributions.constraints._Stack&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributions.constraints._Stack&lt;/code&gt; 의 별칭</target>
        </trans-unit>
        <trans-unit id="a4f813ae14b83732a4864c611af5b3566fb3ec62" translate="yes" xml:space="preserve">
          <source>alias of &lt;code&gt;typing.Tuple&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;typing.Tuple&lt;/code&gt; 별칭입니다 .</target>
        </trans-unit>
        <trans-unit id="068aa03cad6f28f79ba1111c747f21ad5b02ebe5" translate="yes" xml:space="preserve">
          <source>all_gather</source>
          <target state="translated">all_gather</target>
        </trans-unit>
        <trans-unit id="d02036fc361150eb3b815512675dec0cfae0d0f3" translate="yes" xml:space="preserve">
          <source>all_reduce</source>
          <target state="translated">all_reduce</target>
        </trans-unit>
        <trans-unit id="d26207411b688464018bef94d89557e73d634e59" translate="yes" xml:space="preserve">
          <source>all_to_all</source>
          <target state="translated">all_to_all</target>
        </trans-unit>
        <trans-unit id="a690d72de3b70e796e4e33522d5b7688578c24c7" translate="yes" xml:space="preserve">
          <source>allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype</source>
          <target state="translated">허용되는 값은 torch.qint8 및 torch.quint8입니다. quant_min 및 quant_max의 값은 dtype과 일치하도록 선택해야합니다.</target>
        </trans-unit>
        <trans-unit id="d554177a50129be7b474c04ec4d6704a4b3ab4b7" translate="yes" xml:space="preserve">
          <source>along &lt;code&gt;dim&lt;/code&gt;, using the trapezoid rule.</source>
          <target state="translated">사다리꼴 규칙을 사용하여 &lt;code&gt;dim&lt;/code&gt; 을 따라 .</target>
        </trans-unit>
        <trans-unit id="76cd6d3a1710d97fef46a230a4cc6240eb2a1380" translate="yes" xml:space="preserve">
          <source>along &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 따라 .</target>
        </trans-unit>
        <trans-unit id="6554d1a5e11c7976a696c18e79f0814b9396668a" translate="yes" xml:space="preserve">
          <source>along dimension &lt;code&gt;dim&lt;/code&gt; is transformed as</source>
          <target state="translated">치수를 따라 &lt;code&gt;dim&lt;/code&gt; 은 다음과 같이 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="be76331b95dfc399cd776d2fc68021e0db03cc4f" translate="yes" xml:space="preserve">
          <source>alpha</source>
          <target state="translated">alpha</target>
        </trans-unit>
        <trans-unit id="4b077706cb8a0c2ebeb6471729485bac1a733acf" translate="yes" xml:space="preserve">
          <source>alpha_dropout</source>
          <target state="translated">alpha_dropout</target>
        </trans-unit>
        <trans-unit id="cffa50a32cb13a240d705317bcec65dd1f31b6ad" translate="yes" xml:space="preserve">
          <source>and</source>
          <target state="translated">and</target>
        </trans-unit>
        <trans-unit id="ea11961808d60532cba9bb74eca2efc9028432b5" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;L&lt;/code&gt; represents a sequence length.</source>
          <target state="translated">그리고 &lt;code&gt;L&lt;/code&gt; 은 시퀀스 길이를 나타낸다.</target>
        </trans-unit>
        <trans-unit id="51be83d9e5138cf7da772101995cb9cef885eb19" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;dim = i&lt;/code&gt;, then &lt;code&gt;index&lt;/code&gt; must be an</source>
          <target state="translated">및 &lt;code&gt;dim = i&lt;/code&gt; 다음 &lt;code&gt;index&lt;/code&gt; 이어야</target>
        </trans-unit>
        <trans-unit id="7274e61465c020300a30dd75f2b7c77288b1872f" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;grid&lt;/code&gt; with shape</source>
          <target state="translated">및 &lt;code&gt;grid&lt;/code&gt; 모양</target>
        </trans-unit>
        <trans-unit id="c3e9497e8d92dbb47e859e474f20c5ff68bc011d" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">및 &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="69906b9a4a5287952e8b1f59e1b066d2de6ab17e" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;out&lt;/code&gt; will be a matrix of size</source>
          <target state="translated">및 &lt;code&gt;out&lt;/code&gt; 크기의 행렬 것</target>
        </trans-unit>
        <trans-unit id="bfbe811b15a7be917e54a7bd6d548093f12e6464" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;out&lt;/code&gt; will have the same size as &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">그리고 &lt;code&gt;out&lt;/code&gt; 같은 크기해야합니다 &lt;code&gt;index&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e8884a6fb0ff6189ded0963ddb98c79c7d9aeb69" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;solution&lt;/code&gt; is the solution</source>
          <target state="translated">및 &lt;code&gt;solution&lt;/code&gt; 의 해결책</target>
        </trans-unit>
        <trans-unit id="deb10848a6c3d0e28f26841e04fd508759c1568a" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;vec2&lt;/code&gt; is a vector of size</source>
          <target state="translated">및 &lt;code&gt;vec2&lt;/code&gt; 크기의 벡터이다</target>
        </trans-unit>
        <trans-unit id="8516d1daf3054c5229e8207676fe21041fa9e2b0" translate="yes" xml:space="preserve">
          <source>and a &lt;code&gt;Tensor&lt;/code&gt; label</source>
          <target state="translated">및 &lt;code&gt;Tensor&lt;/code&gt; 레이블</target>
        </trans-unit>
        <trans-unit id="7a32db4530d76c7069a29b591cae084dc90bcd99" translate="yes" xml:space="preserve">
          <source>and a labels tensor</source>
          <target state="translated">및 레이블 텐서</target>
        </trans-unit>
        <trans-unit id="d9a100e0a04720d0e08f421ee521b4c07271e73c" translate="yes" xml:space="preserve">
          <source>and a margin with a value greater than</source>
          <target state="translated">값이 다음보다 큰 여백</target>
        </trans-unit>
        <trans-unit id="9bef7a9ca9492d76a989ba472a5134335b532cc1" translate="yes" xml:space="preserve">
          <source>and a matrix</source>
          <target state="translated">및 행렬</target>
        </trans-unit>
        <trans-unit id="994f7a70392302396190552425356c44897b163a" translate="yes" xml:space="preserve">
          <source>and all but the last dimension are the same shape as the input.</source>
          <target state="translated">마지막 차원을 제외한 모든 것은 입력과 동일한 모양입니다.</target>
        </trans-unit>
        <trans-unit id="d3efa22a01fb352021085e134597a06af8fdcd21" translate="yes" xml:space="preserve">
          <source>and assumes</source>
          <target state="translated">그리고 가정</target>
        </trans-unit>
        <trans-unit id="02e70de252e8382255cf467699d2e8c35f5e1033" translate="yes" xml:space="preserve">
          <source>and loading from an iterable-style dataset is roughly equivalent with:</source>
          <target state="translated">반복 가능한 스타일 데이터 세트에서로드하는 것은 대략 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="44fe67e18a7b0c61be3bcd65d4873d81ff165fe4" translate="yes" xml:space="preserve">
          <source>and multiple right-hand sides</source>
          <target state="translated">및 여러 오른쪽</target>
        </trans-unit>
        <trans-unit id="6b843bb82aff68f312d79ab6cf03a56ace808f2d" translate="yes" xml:space="preserve">
          <source>and one of the following modes is used: - &lt;code&gt;linear&lt;/code&gt; - &lt;code&gt;bilinear&lt;/code&gt; - &lt;code&gt;bicubic&lt;/code&gt; - &lt;code&gt;trilinear&lt;/code&gt;</source>
          <target state="translated">다음 모드 중 하나를 사용한다 : - &lt;code&gt;linear&lt;/code&gt; - &lt;code&gt;bilinear&lt;/code&gt; - &lt;code&gt;bicubic&lt;/code&gt; - &lt;code&gt;trilinear&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="34c169d448724e62adaa41082c03ac52ab54fff0" translate="yes" xml:space="preserve">
          <source>and output</source>
          <target state="translated">및 출력</target>
        </trans-unit>
        <trans-unit id="422022e7df4fc1e7e88e8f2a905326d532895a06" translate="yes" xml:space="preserve">
          <source>and restarts at step</source>
          <target state="translated">단계에서 다시 시작</target>
        </trans-unit>
        <trans-unit id="14b190aab2ad2654706b74c2be95acb6da7f2850" translate="yes" xml:space="preserve">
          <source>and scalar output</source>
          <target state="translated">및 스칼라 출력</target>
        </trans-unit>
        <trans-unit id="b34851d4c502e28d96eafba7331ab90e1898bc8d" translate="yes" xml:space="preserve">
          <source>and so forth. If increasing is True, the columns are</source>
          <target state="translated">기타 등등. 증가가 True이면 열은</target>
        </trans-unit>
        <trans-unit id="146cd69984268e3dc94fa6880d4aabdda441d4be" translate="yes" xml:space="preserve">
          <source>and standard deviation</source>
          <target state="translated">및 표준 편차</target>
        </trans-unit>
        <trans-unit id="5bba86a9b83c5aa670a1f3d4c691ac866067b19d" translate="yes" xml:space="preserve">
          <source>and target</source>
          <target state="translated">및 대상</target>
        </trans-unit>
        <trans-unit id="9124e7eb2a80bd8e99f0ee162bc6c10019f76e56" translate="yes" xml:space="preserve">
          <source>and target tensor</source>
          <target state="translated">및 타겟 텐서</target>
        </trans-unit>
        <trans-unit id="adaffd67d43c3d4bd29c2d4e89911618672f35ec" translate="yes" xml:space="preserve">
          <source>and the LU factorization of A, in order as a namedtuple &lt;code&gt;solution, LU&lt;/code&gt;.</source>
          <target state="translated">그리고 A의 LU 분해 &lt;code&gt;solution, LU&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3c8e722a8235feec4386550dd50bdb353aabf9b3" translate="yes" xml:space="preserve">
          <source>and the elements of</source>
          <target state="translated">및 요소</target>
        </trans-unit>
        <trans-unit id="6058dacca42ad9e7a4c0849aef311147f2ca8455" translate="yes" xml:space="preserve">
          <source>and the total loss functions is</source>
          <target state="translated">총 손실 함수는</target>
        </trans-unit>
        <trans-unit id="0f0f37057f618e969e7c100dbe104d0a83309871" translate="yes" xml:space="preserve">
          <source>and vector</source>
          <target state="translated">및 벡터</target>
        </trans-unit>
        <trans-unit id="f12fb281ea5401324cfef7c940cff938f57fc26a" translate="yes" xml:space="preserve">
          <source>and we will be able to step into the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function. To disable the TorchScript compiler for a specific function, see &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">그리고 우리는 일반적인 파이썬 함수로 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수로 들어갈 수 있습니다. 특정 함수에 대해 TorchScript 컴파일러를 비활성화하려면 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="29748c4ec2f5a11f19b122c91c764f2c283a295d" translate="yes" xml:space="preserve">
          <source>and x2 has shape</source>
          <target state="translated">x2는 모양이 있습니다</target>
        </trans-unit>
        <trans-unit id="abe7ad204a8ebc99e055716a1499632a1e21a7ef" translate="yes" xml:space="preserve">
          <source>and zero point</source>
          <target state="translated">그리고 영점</target>
        </trans-unit>
        <trans-unit id="11a9215ec620218c8e8d409263469091431ff0d4" translate="yes" xml:space="preserve">
          <source>angle</source>
          <target state="translated">angle</target>
        </trans-unit>
        <trans-unit id="4ad4ad16aa212e130ee9bd5ac32ae35842a975fe" translate="yes" xml:space="preserve">
          <source>arange</source>
          <target state="translated">arange</target>
        </trans-unit>
        <trans-unit id="a538840857b54aaa96ea8ec3ac8fb4a13708f87e" translate="yes" xml:space="preserve">
          <source>arbitrary shapes with a total of</source>
          <target state="translated">총을 가진 임의의 모양</target>
        </trans-unit>
        <trans-unit id="86c852d13e59fa71b388491fc4ce6baa16edb892" translate="yes" xml:space="preserve">
          <source>are assumed to be 1 and not referenced from</source>
          <target state="translated">1로 간주되고 참조되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e4876ebe475fcdaae7801e2c6555b7737319caff" translate="yes" xml:space="preserve">
          <source>are computed as:</source>
          <target state="translated">다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="f920c82e9a5117ed628937a9379bf48075b0898c" translate="yes" xml:space="preserve">
          <source>are learnable affine transform parameters of &lt;code&gt;normalized_shape&lt;/code&gt; if &lt;code&gt;elementwise_affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;elementwise_affine&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 &lt;code&gt;normalized_shape&lt;/code&gt; 의 학습 가능한 아핀 변환 매개 변수입니다 . 표준 편차는 &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; 와 동일한 편향 추정기를 통해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="e538aee8f67210d75bc513a9c556cb3f2c22456c" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 크기 &lt;code&gt;C&lt;/code&gt; (여기서 &lt;code&gt;C&lt;/code&gt; 는 입력 크기) 의 학습 가능한 매개 변수 벡터입니다 . 표준 편차는 &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; 와 동일한 편향 추정기를 통해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="425eb1a701280f929d1aa755a3bd9afffe042bd8" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size &lt;code&gt;C&lt;/code&gt; (where &lt;code&gt;C&lt;/code&gt; is the input size). By default, the elements of</source>
          <target state="translated">크기 &lt;code&gt;C&lt;/code&gt; 의 학습 가능한 매개 변수 벡터입니다 (여기서 &lt;code&gt;C&lt;/code&gt; 는 입력 크기). 기본적으로</target>
        </trans-unit>
        <trans-unit id="86f4f24ef6637d895465729e95b5618cb38c12be" translate="yes" xml:space="preserve">
          <source>are learnable parameter vectors of size C (where C is the input size) if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 크기 C (여기서 C는 입력 크기)의 학습 가능한 매개 변수 벡터입니다 . 표준 편차는 &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; 와 동일한 편향 추정기를 통해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="8ff6a70926b2f700851f7fc8c20c594a8e76e9c3" translate="yes" xml:space="preserve">
          <source>are learnable per-channel affine transform parameter vectors of size &lt;code&gt;num_channels&lt;/code&gt; if &lt;code&gt;affine&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 &lt;code&gt;num_channels&lt;/code&gt; 크기의 학습 가능한 채널 별 아핀 변환 매개 변수 벡터입니다 . 표준 편차는 &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; 와 동일한 편향 추정기를 통해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="75bf5f4b4eb9c04180b6f81312057902bb428ee6" translate="yes" xml:space="preserve">
          <source>are returned because the real-to-complex Fourier transform satisfies the conjugate symmetry, i.e.,</source>
          <target state="translated">실수-복소 푸리에 변환이 켤레 대칭을 충족하기 때문에 반환됩니다. 즉,</target>
        </trans-unit>
        <trans-unit id="b9344438a3a3542ca4e36035afaf2dc5ecaf2be7" translate="yes" xml:space="preserve">
          <source>are sampled from</source>
          <target state="translated">에서 샘플링</target>
        </trans-unit>
        <trans-unit id="2865bbee8d449f1d185c550ec6b590c22b6957d4" translate="yes" xml:space="preserve">
          <source>are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt;.</source>
          <target state="translated">표준 편차는 &lt;code&gt;torch.var(input, unbiased=False)&lt;/code&gt; 해당하는 편향된 추정기를 통해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="8ee644b4632f03c7ed8cd56a6d8d3b0a6de1f9ba" translate="yes" xml:space="preserve">
          <source>are set to 1 and the elements of</source>
          <target state="translated">1로 설정되고</target>
        </trans-unit>
        <trans-unit id="07c77f1bda818de0208d523f8899b3173d0eae6a" translate="yes" xml:space="preserve">
          <source>are tensors of arbitrary shapes with a total of</source>
          <target state="translated">임의의 형태의 텐서는 총</target>
        </trans-unit>
        <trans-unit id="d1792693a82797275a29dc724b1fdb08fe5cef51" translate="yes" xml:space="preserve">
          <source>are the dimensions of the matrix.</source>
          <target state="translated">행렬의 차원입니다.</target>
        </trans-unit>
        <trans-unit id="eb06c7309fb6a43a67a1cdf5bde8c81dbe8d348d" translate="yes" xml:space="preserve">
          <source>are the input, forget, cell, and output gates, respectively.</source>
          <target state="translated">각각 입력, 잊어 버림, 셀 및 출력 게이트입니다.</target>
        </trans-unit>
        <trans-unit id="1ab04c3136643cd3b0d7e5db6c93dfabb34510d0" translate="yes" xml:space="preserve">
          <source>are the minimum and maximum of the quantized data type.</source>
          <target state="translated">양자화 된 데이터 유형의 최소 및 최대입니다.</target>
        </trans-unit>
        <trans-unit id="1d1f1537fbffe574251e7df78749efa3f5feee9a" translate="yes" xml:space="preserve">
          <source>are the only supported values.</source>
          <target state="translated">지원되는 유일한 값입니다.</target>
        </trans-unit>
        <trans-unit id="8ef8bf39209d2f2fce9681f8054b40e4806ff7f6" translate="yes" xml:space="preserve">
          <source>are the parameters,</source>
          <target state="translated">매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="9b2535542a411be559b351451bde244b4f033b2f" translate="yes" xml:space="preserve">
          <source>are the reset, update, and new gates, respectively.</source>
          <target state="translated">각각 재설정, 업데이트 및 새 게이트입니다.</target>
        </trans-unit>
        <trans-unit id="05b98babf9d8d0da7974d92dffc5a3f4ea246a9c" translate="yes" xml:space="preserve">
          <source>are then computed as:</source>
          <target state="translated">그런 다음 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="d4665f1e2ffa54be784f9b474fe768613b821365" translate="yes" xml:space="preserve">
          <source>argmax</source>
          <target state="translated">argmax</target>
        </trans-unit>
        <trans-unit id="4538ebc5d50e576bcdd8095d274394500be1d3e0" translate="yes" xml:space="preserve">
          <source>argmin</source>
          <target state="translated">argmin</target>
        </trans-unit>
        <trans-unit id="0a960f0a485739de9ec7e0394dd5e46da6afde09" translate="yes" xml:space="preserve">
          <source>as below</source>
          <target state="translated">아래</target>
        </trans-unit>
        <trans-unit id="908b35dcf789b151ab044796ae0006bab35551e6" translate="yes" xml:space="preserve">
          <source>as described above</source>
          <target state="translated">위에서 설명한대로</target>
        </trans-unit>
        <trans-unit id="98d3a6b6e515d500e9df2fee1f684162fcc532ee" translate="yes" xml:space="preserve">
          <source>as explicit separate matrices.</source>
          <target state="translated">명시 적 별도의 행렬로.</target>
        </trans-unit>
        <trans-unit id="196302a3f9515657b0c55dab8c474613624309d3" translate="yes" xml:space="preserve">
          <source>as the &lt;code&gt;target&lt;/code&gt; for each value of a 1D tensor of size &lt;code&gt;minibatch&lt;/code&gt;; if &lt;code&gt;ignore_index&lt;/code&gt; is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).</source>
          <target state="translated">는 AS &lt;code&gt;target&lt;/code&gt; 사이즈의 1D 텐서의 각 값 &lt;code&gt;minibatch&lt;/code&gt; ; 만약 &lt;code&gt;ignore_index&lt;/code&gt; 가 지정되어,이 기준은이 클래스 인덱스 (이 인덱스가 반드시 클래스 범위가 아니더라도 좋다)를 수용한다.</target>
        </trans-unit>
        <trans-unit id="c3e04b10767441eb9b4b0a57a2d62cbb77d5cdad" translate="yes" xml:space="preserve">
          <source>as vec norm when dim is None</source>
          <target state="translated">dim이 None 일 때 vec norm으로</target>
        </trans-unit>
        <trans-unit id="73688580af35401988ca63c730bbff475def44d3" translate="yes" xml:space="preserve">
          <source>as:</source>
          <target state="translated">as:</target>
        </trans-unit>
        <trans-unit id="6480588dae849f3cd69c8be01b990f84ce70a375" translate="yes" xml:space="preserve">
          <source>as_strided</source>
          <target state="translated">as_strided</target>
        </trans-unit>
        <trans-unit id="f6b3e9a1435d3f432e7a6b9003583871fcb26de3" translate="yes" xml:space="preserve">
          <source>asin</source>
          <target state="translated">asin</target>
        </trans-unit>
        <trans-unit id="0217022a2cbba3beab97d37baf8f0e78d8404ee5" translate="yes" xml:space="preserve">
          <source>asynchronous operation - when &lt;code&gt;async_op&lt;/code&gt; is set to True. The collective operation function returns a distributed request object. In general, you don&amp;rsquo;t need to create it manually and it is guaranteed to support two methods:</source>
          <target state="translated">비동기 작업 &lt;code&gt;async_op&lt;/code&gt; 이 True로 설정된 경우 . 집합 연산 함수는 분산 요청 객체를 반환합니다. 일반적으로 수동으로 생성 할 필요가 없으며 다음 두 가지 방법을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="0366fd6a49c84920645a67d8439d5e2e6075abe4" translate="yes" xml:space="preserve">
          <source>at each cycle iteration.</source>
          <target state="translated">각주기 반복에서.</target>
        </trans-unit>
        <trans-unit id="f077504d04fa1bc96b042497b434fd33fabdf43d" translate="yes" xml:space="preserve">
          <source>atan</source>
          <target state="translated">atan</target>
        </trans-unit>
        <trans-unit id="12dad2bbce60ee2bc83c285aa39dca395465f305" translate="yes" xml:space="preserve">
          <source>atol</source>
          <target state="translated">atol</target>
        </trans-unit>
        <trans-unit id="cfacbf356c29ecafbd9f709c475179843abdd31e" translate="yes" xml:space="preserve">
          <source>attn_mask: 2D mask</source>
          <target state="translated">attn_mask : 2D 마스크</target>
        </trans-unit>
        <trans-unit id="cc13d40f3fdefe8666e6601dc54517fbb0641b8a" translate="yes" xml:space="preserve">
          <source>attn_output:</source>
          <target state="translated">attn_output:</target>
        </trans-unit>
        <trans-unit id="4cac88fb84e0dd512ca9b04f9392aa70146ed77a" translate="yes" xml:space="preserve">
          <source>attn_output_weights:</source>
          <target state="translated">attn_output_weights:</target>
        </trans-unit>
        <trans-unit id="87792e7cfa548e657093e5c536428e801de627fe" translate="yes" xml:space="preserve">
          <source>avg_pool1d</source>
          <target state="translated">avg_pool1d</target>
        </trans-unit>
        <trans-unit id="8454dbf5a788a4fc7b69874c4fe549fff6d01e4a" translate="yes" xml:space="preserve">
          <source>avg_pool2d</source>
          <target state="translated">avg_pool2d</target>
        </trans-unit>
        <trans-unit id="99a1901d4567084ff3e326ca553c35a49cbcc708" translate="yes" xml:space="preserve">
          <source>avg_pool3d</source>
          <target state="translated">avg_pool3d</target>
        </trans-unit>
        <trans-unit id="e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98" translate="yes" xml:space="preserve">
          <source>b</source>
          <target state="translated">b</target>
        </trans-unit>
        <trans-unit id="ab598131b37324cb1420d88178e195226078e8ec" translate="yes" xml:space="preserve">
          <source>b_{c} = a_{c}\left(k + \frac{\alpha}{n} \sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}</source>
          <target state="translated">b_ {c} = a_ {c} \ left (k + \ frac {\ alpha} {n} \ sum_ {c '= \ max (0, cn / 2)} ^ {\ min (N-1, c + n / 2)} a_ {c '} ^ 2 \ right) ^ {-\ beta}</target>
        </trans-unit>
        <trans-unit id="07949b7c55cab1a81db23f348c3409b06fc9ee75" translate="yes" xml:space="preserve">
          <source>baddbmm</source>
          <target state="translated">baddbmm</target>
        </trans-unit>
        <trans-unit id="779455ee3bde8494d9629b353e17b19e92357ba8" translate="yes" xml:space="preserve">
          <source>barrier</source>
          <target state="translated">barrier</target>
        </trans-unit>
        <trans-unit id="1405df66cbe219b0bf6355bc3d60361a8376b6b4" translate="yes" xml:space="preserve">
          <source>base</source>
          <target state="translated">base</target>
        </trans-unit>
        <trans-unit id="115bac8f958d7eaf610aede34fad2562c346e505" translate="yes" xml:space="preserve">
          <source>batch size</source>
          <target state="translated">배치 크기</target>
        </trans-unit>
        <trans-unit id="31442bf4302dd7b34e796318f57912b743ccccf0" translate="yes" xml:space="preserve">
          <source>batch1</source>
          <target state="translated">batch1</target>
        </trans-unit>
        <trans-unit id="1d1b6777e9c5a97a6a039ea824d68ee915cc1491" translate="yes" xml:space="preserve">
          <source>batch2</source>
          <target state="translated">batch2</target>
        </trans-unit>
        <trans-unit id="b30c0788a62514c66ada13cce11022313c9e5df0" translate="yes" xml:space="preserve">
          <source>batch_norm</source>
          <target state="translated">batch_norm</target>
        </trans-unit>
        <trans-unit id="793787e7648c00d5d9f56d9f3ee2a1d2314216d8" translate="yes" xml:space="preserve">
          <source>being an orthogonal matrix or batch of orthogonal matrices and</source>
          <target state="translated">직교 행렬 또는 직교 행렬의 배치이고</target>
        </trans-unit>
        <trans-unit id="b50848b7bdcbc01d3b72f5bb1ce5b7a00a5aad61" translate="yes" xml:space="preserve">
          <source>being an upper triangular matrix or batch of upper triangular matrices.</source>
          <target state="translated">상위 삼각 행렬 또는 상위 삼각 행렬의 배치입니다.</target>
        </trans-unit>
        <trans-unit id="4928df7f06c7ee461e27540fb51dac9bf4b670ad" translate="yes" xml:space="preserve">
          <source>beta is an optional parameter that defaults to 1.</source>
          <target state="translated">beta는 기본값이 1 인 선택적 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="736c1fbfd886e877040ee4134960bc63ce05262d" translate="yes" xml:space="preserve">
          <source>between two distributions.</source>
          <target state="translated">두 분포 사이.</target>
        </trans-unit>
        <trans-unit id="e1e8289afa6b54e410bc2304308fe48e83a8ef02" translate="yes" xml:space="preserve">
          <source>bias</source>
          <target state="translated">bias</target>
        </trans-unit>
        <trans-unit id="5b74ed2accf3a37671aad25b7b91bed84cad9a6e" translate="yes" xml:space="preserve">
          <source>bias:</source>
          <target state="translated">bias:</target>
        </trans-unit>
        <trans-unit id="a1ceb3e5387c09cb212ce6ee3c3c1a476297155d" translate="yes" xml:space="preserve">
          <source>bilinear</source>
          <target state="translated">bilinear</target>
        </trans-unit>
        <trans-unit id="2e1157f96f8af393a261e4d71a43aa637f30cb60" translate="yes" xml:space="preserve">
          <source>binary answer to whether &lt;code&gt;module&lt;/code&gt; is pruned.</source>
          <target state="translated">&lt;code&gt;module&lt;/code&gt; 이 정리 되었는지 여부에 대한 이진 답변 입니다.</target>
        </trans-unit>
        <trans-unit id="74c77efd9714f8e8eecdc286f7d29c392001890c" translate="yes" xml:space="preserve">
          <source>binary_cross_entropy</source>
          <target state="translated">binary_cross_entropy</target>
        </trans-unit>
        <trans-unit id="b6c72f603883e86a6cb8021afc246ea5ec1382f5" translate="yes" xml:space="preserve">
          <source>binary_cross_entropy_with_logits</source>
          <target state="translated">binary_cross_entropy_with_logits</target>
        </trans-unit>
        <trans-unit id="0569ae912e387dbc988dee842bf17cdd4e437f63" translate="yes" xml:space="preserve">
          <source>bits</source>
          <target state="translated">bits</target>
        </trans-unit>
        <trans-unit id="b3334e38fd49d5a88ca98cd30878d427a4ddbf06" translate="yes" xml:space="preserve">
          <source>bitshift</source>
          <target state="translated">bitshift</target>
        </trans-unit>
        <trans-unit id="e4e1043a1b984700e67896c1396be3aa962489d5" translate="yes" xml:space="preserve">
          <source>blank=0</source>
          <target state="translated">blank=0</target>
        </trans-unit>
        <trans-unit id="4d46077c78ecf87402848fc9f920b72cde953b02" translate="yes" xml:space="preserve">
          <source>bound</source>
          <target state="translated">bound</target>
        </trans-unit>
        <trans-unit id="7717af821763c86c1249c0045aa20cfbed718b66" translate="yes" xml:space="preserve">
          <source>box AP</source>
          <target state="translated">상자 AP</target>
        </trans-unit>
        <trans-unit id="aa06eab1f6ebf63aafea56f4652eca504ae7b58f" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the ground-truth boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt;</source>
          <target state="translated">상자 ( &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ) : &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 형식의 실측 상자 ( &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;H&lt;/code&gt; 와 &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;W&lt;/code&gt; 사이의 값)</target>
        </trans-unit>
        <trans-unit id="bf13e93de62c26c00d7b1d7846899f810d8062c3" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the ground-truth boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values of &lt;code&gt;x&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; and values of &lt;code&gt;y&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;</source>
          <target state="translated">상자 ( &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ) : &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 형식의 실측 값 상자 , &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;W&lt;/code&gt; 사이 의 &lt;code&gt;x&lt;/code&gt; 값과 &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;H&lt;/code&gt; 사이 의 &lt;code&gt;y&lt;/code&gt; 값</target>
        </trans-unit>
        <trans-unit id="4523bad4432d4b614f44a286645e98445b5fdb92" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the predicted boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt;</source>
          <target state="translated">상자 ( &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ) : &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 형식 의 예측 상자 , &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;H&lt;/code&gt; 와 &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;W&lt;/code&gt; 사이의 값</target>
        </trans-unit>
        <trans-unit id="99db80efa8d4fb3770cb46ce8d01afcb432bd25a" translate="yes" xml:space="preserve">
          <source>boxes (&lt;code&gt;FloatTensor[N, 4]&lt;/code&gt;): the predicted boxes in &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; format, with values of &lt;code&gt;x&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; and values of &lt;code&gt;y&lt;/code&gt; between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;H&lt;/code&gt;</source>
          <target state="translated">상자 ( &lt;code&gt;FloatTensor[N, 4]&lt;/code&gt; ) : &lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt; 형식 의 예측 상자 , &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;W&lt;/code&gt; 사이 의 &lt;code&gt;x&lt;/code&gt; 값과 &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;H&lt;/code&gt; 사이 의 &lt;code&gt;y&lt;/code&gt; 값</target>
        </trans-unit>
        <trans-unit id="b58ccb7871e0261dfe6de9e4f5274543ebbebeb8" translate="yes" xml:space="preserve">
          <source>broadcast</source>
          <target state="translated">broadcast</target>
        </trans-unit>
        <trans-unit id="8ab478517af17507ee846089a366bb3e5480b24d" translate="yes" xml:space="preserve">
          <source>but slower. Default: &lt;em&gt;False&lt;/em&gt;. See &lt;a href=&quot;https://arxiv.org/pdf/1707.06990.pdf&quot;&gt;&amp;ldquo;paper&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">그러나 느리다. 기본값 : &lt;em&gt;False&lt;/em&gt; . &lt;a href=&quot;https://arxiv.org/pdf/1707.06990.pdf&quot;&gt;&quot;종이&quot;&lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="408158643ed564c72fa0921826f8294d71ccbf7c" translate="yes" xml:space="preserve">
          <source>by</source>
          <target state="translated">by</target>
        </trans-unit>
        <trans-unit id="3e8a60d5f4e9887048f1c82dc39ab1673602aa1e" translate="yes" xml:space="preserve">
          <source>by summing the overlapping values. Similar to &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt;, the arguments must satisfy</source>
          <target state="translated">겹치는 값을 더합니다. &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt; 와 유사하게 인수는 다음을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="84a516841ba77a5b4648de2cd0dfcb30ea46dbb4" translate="yes" xml:space="preserve">
          <source>c</source>
          <target state="translated">c</target>
        </trans-unit>
        <trans-unit id="7ba642b78cae54611d3ce19d3a3b9315a889b623" translate="yes" xml:space="preserve">
          <source>c &amp;gt; 1</source>
          <target state="translated">c&amp;gt; 1</target>
        </trans-unit>
        <trans-unit id="a695701dc8bcc878e55225bb88a2be5e0250b7f9" translate="yes" xml:space="preserve">
          <source>c = (u u^T)^{{-1}} b</source>
          <target state="translated">c = (uu ^ T) ^ {{-1}} b</target>
        </trans-unit>
        <trans-unit id="325406a6e81b2954fe93c1849e849987a40ad22c" translate="yes" xml:space="preserve">
          <source>c = (u^T u)^{{-1}} b</source>
          <target state="translated">c = (u ^ T u) ^ {{-1}} b</target>
        </trans-unit>
        <trans-unit id="9db33fa0cd63d5bcc994a2d10aee7aa1c6cf5234" translate="yes" xml:space="preserve">
          <source>c = 1</source>
          <target state="translated">c = 1</target>
        </trans-unit>
        <trans-unit id="75e4c1ce919ca4246ccdfcf0f90143995bdcf09e" translate="yes" xml:space="preserve">
          <source>c_t</source>
          <target state="translated">c_t</target>
        </trans-unit>
        <trans-unit id="52a612a982a939b9f63856d2f4f4f8c12f2509f8" translate="yes" xml:space="preserve">
          <source>can be adjusted using &lt;code&gt;min_val&lt;/code&gt; and &lt;code&gt;max_val&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;min_val&lt;/code&gt; 및 &lt;code&gt;max_val&lt;/code&gt; 을 사용하여 조정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4d83e8bdc70262378c30c2dddd5537deeb8a96c5" translate="yes" xml:space="preserve">
          <source>can be avoided if one sets &lt;code&gt;reduction = 'sum'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;reduction = 'sum'&lt;/code&gt; 설정하면 피할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="90bd565efaf349a2fc8e4dc779050d0325ec50a1" translate="yes" xml:space="preserve">
          <source>can be avoided if sets &lt;code&gt;reduction = 'sum'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;reduction = 'sum'&lt;/code&gt; 설정하면 피할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="3c65ddc9ba0e2d191602d3c173ef6a188ce419ee" translate="yes" xml:space="preserve">
          <source>can be precisely described as:</source>
          <target state="translated">다음과 같이 정확하게 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8634627c53b593a107a2ce1c2b724bc47ea5e431" translate="yes" xml:space="preserve">
          <source>can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding &lt;code&gt;eigenvalues[j]&lt;/code&gt; is a real number, column &lt;code&gt;eigenvectors[:, j]&lt;/code&gt; is the eigenvector corresponding to &lt;code&gt;eigenvalues[j]&lt;/code&gt;. If the corresponding &lt;code&gt;eigenvalues[j]&lt;/code&gt; and &lt;code&gt;eigenvalues[j + 1]&lt;/code&gt; form a complex conjugate pair, then the true eigenvectors can be computed as</source>
          <target state="translated">다음과 같이 해당 고유 값의 정규화 된 (단위 길이) 고유 벡터를 계산하는 데 사용할 수 있습니다. 해당 &lt;code&gt;eigenvalues[j]&lt;/code&gt; 가 실수이면 열 &lt;code&gt;eigenvectors[:, j]&lt;/code&gt; 는 고유 &lt;code&gt;eigenvalues[j]&lt;/code&gt; 해당하는 고유 벡터 입니다. 대응하는 &lt;code&gt;eigenvalues[j]&lt;/code&gt; 및 &lt;code&gt;eigenvalues[j + 1]&lt;/code&gt; 이 복소 켤레 쌍을 형성하면 실제 고유 벡터는 다음과 같이 계산 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9d989e8d27dc9e0ec3389fc855f142c3d40f0c50" translate="yes" xml:space="preserve">
          <source>cat</source>
          <target state="translated">cat</target>
        </trans-unit>
        <trans-unit id="613af80c25dfbc9ef75ce605280571b9a518d632" translate="yes" xml:space="preserve">
          <source>ceil</source>
          <target state="translated">ceil</target>
        </trans-unit>
        <trans-unit id="8cc51bda059df43f07da456966946b85e1e39fed" translate="yes" xml:space="preserve">
          <source>celu</source>
          <target state="translated">celu</target>
        </trans-unit>
        <trans-unit id="a4ab4f21160cf5b0a8b70fd62605f209cd8eb378" translate="yes" xml:space="preserve">
          <source>clamp</source>
          <target state="translated">clamp</target>
        </trans-unit>
        <trans-unit id="0b03af5027f5174c31ba5792d65334dc4b86d0c4" translate="yes" xml:space="preserve">
          <source>clamp_max</source>
          <target state="translated">clamp_max</target>
        </trans-unit>
        <trans-unit id="c71e6f6650b2c00996a576acfebb2e5c2c9133bc" translate="yes" xml:space="preserve">
          <source>clamp_min</source>
          <target state="translated">clamp_min</target>
        </trans-unit>
        <trans-unit id="af6ab064a94fb3e9fd590a99e474c2ed6fe59e97" translate="yes" xml:space="preserve">
          <source>clear()</source>
          <target state="translated">clear()</target>
        </trans-unit>
        <trans-unit id="dde02ec247dcb158d65d0da1bdd9301502952f4f" translate="yes" xml:space="preserve">
          <source>clip_value</source>
          <target state="translated">clip_value</target>
        </trans-unit>
        <trans-unit id="5dc3fded50a44a781130cc74e32147cfb27fb2b1" translate="yes" xml:space="preserve">
          <source>colors:</source>
          <target state="translated">colors:</target>
        </trans-unit>
        <trans-unit id="ec3d37cfb39e6ddfa966aa2a0e1cc364f09c1aac" translate="yes" xml:space="preserve">
          <source>columns (when specified) or &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">열 (지정된 경우) 또는 &lt;code&gt;1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d8f2c317f524276e700bfcb20a9ddc27b06eedc5" translate="yes" xml:space="preserve">
          <source>columns of &lt;code&gt;input&lt;/code&gt; are linearly independent. This behavior will propably change once QR supports pivoting.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 열 은 일차적으로 독립적입니다. 이 동작은 QR이 피벗을 지원하면 적절하게 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="a3ed4cce473629e46512b31377e0c26f8c611bf9" translate="yes" xml:space="preserve">
          <source>columns represent the principal directions</source>
          <target state="translated">열은 주요 방향을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="805682fdf2befd0f72b7d16894fd58f0d033c065" translate="yes" xml:space="preserve">
          <source>columns.</source>
          <target state="translated">columns.</target>
        </trans-unit>
        <trans-unit id="49ba358c3272c2db40fc6ab2c103669678628b68" translate="yes" xml:space="preserve">
          <source>concat</source>
          <target state="translated">concat</target>
        </trans-unit>
        <trans-unit id="afad9a69767e7c24ecda5af9187202e1b4b070b2" translate="yes" xml:space="preserve">
          <source>condition</source>
          <target state="translated">condition</target>
        </trans-unit>
        <trans-unit id="f607ae1e89eb5c8e88a5312b0347f07cd629f09e" translate="yes" xml:space="preserve">
          <source>conduct; niter must be a nonnegative integer, and defaults to 2</source>
          <target state="translated">행위; niter는 음이 아닌 정수 여야하며 기본값은 2입니다.</target>
        </trans-unit>
        <trans-unit id="0713af050ff5d1185e252e762af4f2837f6692ed" translate="yes" xml:space="preserve">
          <source>containing the window</source>
          <target state="translated">창 포함</target>
        </trans-unit>
        <trans-unit id="d75270e5e6de30e18e4c2d9c2c4833c4b41b71ea" translate="yes" xml:space="preserve">
          <source>contains the eigenvalues of</source>
          <target state="translated">고유 값 포함</target>
        </trans-unit>
        <trans-unit id="edf1afc933ef93b4de4eecc542db49d0c5ac72fd" translate="yes" xml:space="preserve">
          <source>contains the solution. If</source>
          <target state="translated">솔루션을 포함합니다. 만약</target>
        </trans-unit>
        <trans-unit id="963bb6a82e1a90627d6c03382ec4d1cf449f21be" translate="yes" xml:space="preserve">
          <source>conv1d</source>
          <target state="translated">conv1d</target>
        </trans-unit>
        <trans-unit id="6c6a564ef490d5000c80c05e48c88c09ac2644d2" translate="yes" xml:space="preserve">
          <source>conv2d</source>
          <target state="translated">conv2d</target>
        </trans-unit>
        <trans-unit id="37994178ccdc1cb8e941708bda9ab3d1d68ab700" translate="yes" xml:space="preserve">
          <source>conv3d</source>
          <target state="translated">conv3d</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
