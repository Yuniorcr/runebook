<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="c543b3c390ed1ce750f7a99ce30092a4fe11d2d6" translate="yes" xml:space="preserve">
          <source>NaN values in &lt;code&gt;grid&lt;/code&gt; would be interpreted as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">NaN values in &lt;code&gt;grid&lt;/code&gt; would be interpreted as &lt;code&gt;-1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="709a23220f2c3d64d1e1d6d18c4d5280f8d82fca" translate="yes" xml:space="preserve">
          <source>Name</source>
          <target state="translated">Name</target>
        </trans-unit>
        <trans-unit id="0585ffb115d09e675d597261b20f2d75a3eb03a7" translate="yes" xml:space="preserve">
          <source>Name propagation semantics</source>
          <target state="translated">Name propagation semantics</target>
        </trans-unit>
        <trans-unit id="817cc3c2ad5448f94a4d253bbcac8dd6cc2d1cf9" translate="yes" xml:space="preserve">
          <source>Named Tensors</source>
          <target state="translated">Named Tensors</target>
        </trans-unit>
        <trans-unit id="e90e823237a9f4d7fe13c9da511106ba9aee2f03" translate="yes" xml:space="preserve">
          <source>Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support &amp;ldquo;broadcasting by name&amp;rdquo; rather than &amp;ldquo;broadcasting by position&amp;rdquo;.</source>
          <target state="translated">Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support &amp;ldquo;broadcasting by name&amp;rdquo; rather than &amp;ldquo;broadcasting by position&amp;rdquo;.</target>
        </trans-unit>
        <trans-unit id="6e3e9a74ab4dd7c6ec7d57c873cd59674e123d29" translate="yes" xml:space="preserve">
          <source>Named Tensors operator coverage</source>
          <target state="translated">Named Tensors operator coverage</target>
        </trans-unit>
        <trans-unit id="00f19d99a1a121465f53068cd3e37c47ef1fad72" translate="yes" xml:space="preserve">
          <source>Named Tuples</source>
          <target state="translated">명명 된 튜플</target>
        </trans-unit>
        <trans-unit id="d906a9f233a4d60e7ff3266a36abd411378985f0" translate="yes" xml:space="preserve">
          <source>Named dimensions</source>
          <target state="translated">Named dimensions</target>
        </trans-unit>
        <trans-unit id="d69a85347956dd430a196f336efedfda312c73cf" translate="yes" xml:space="preserve">
          <source>Named dimensions, like regular Tensor dimensions, are ordered. &lt;code&gt;tensor.names[i]&lt;/code&gt; is the name of dimension &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">Named dimensions, like regular Tensor dimensions, are ordered. &lt;code&gt;tensor.names[i]&lt;/code&gt; is the name of dimension &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="87ade27d950d661d18340b5de9b3d2e0c2886884" translate="yes" xml:space="preserve">
          <source>Named tensor API reference</source>
          <target state="translated">Named tensor API reference</target>
        </trans-unit>
        <trans-unit id="554d6e43a251766628f65b287dfa2b6bb568c6f8" translate="yes" xml:space="preserve">
          <source>Named tensors can coexist with unnamed tensors; named tensors are instances of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. Unnamed tensors have &lt;code&gt;None&lt;/code&gt;-named dimensions. Named tensors do not require all dimensions to be named.</source>
          <target state="translated">Named tensors can coexist with unnamed tensors; named tensors are instances of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;. Unnamed tensors have &lt;code&gt;None&lt;/code&gt; -named dimensions. Named tensors do not require all dimensions to be named.</target>
        </trans-unit>
        <trans-unit id="9babc66ca4089f8e16cccf70d5e5125ac942af22" translate="yes" xml:space="preserve">
          <source>Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called &lt;em&gt;name inference&lt;/em&gt;. More formally, name inference consists of the following two steps:</source>
          <target state="translated">Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called &lt;em&gt;name inference&lt;/em&gt;. More formally, name inference consists of the following two steps:</target>
        </trans-unit>
        <trans-unit id="bc702e382ad1ddf0605ed2f8380fd2e579529d82" translate="yes" xml:space="preserve">
          <source>Namely, joining processes sequentially implies they will terminate sequentially. If they don&amp;rsquo;t, and the first process does not terminate, the process termination will go unnoticed. Also, there are no native facilities for error propagation.</source>
          <target state="translated">즉, 프로세스를 순차적으로 결합한다는 것은 순차적으로 종료된다는 것을 의미합니다. 그렇지 않고 첫 번째 프로세스가 종료되지 않으면 프로세스 종료가 눈에 띄지 않게됩니다. 또한 오류 전파를위한 기본 기능이 없습니다.</target>
        </trans-unit>
        <trans-unit id="ce14504f9d8c7c9312b2871ff6009782be1ad451" translate="yes" xml:space="preserve">
          <source>Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv&lt;/a&gt;).</source>
          <target state="translated">Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv&lt;/a&gt;).</target>
        </trans-unit>
        <trans-unit id="1589b41367a72480492d7a5904aab37522c48d37" translate="yes" xml:space="preserve">
          <source>Negative log likelihood loss with Poisson distribution of target.</source>
          <target state="translated">Negative log likelihood loss with Poisson distribution of target.</target>
        </trans-unit>
        <trans-unit id="e2c678e1dc9e8fcb17d534c7469483a491efdcab" translate="yes" xml:space="preserve">
          <source>NegativeBinomial</source>
          <target state="translated">NegativeBinomial</target>
        </trans-unit>
        <trans-unit id="b7f2cb3940a28574771032f71b83c2599f143244" translate="yes" xml:space="preserve">
          <source>Neither &lt;code&gt;sampler&lt;/code&gt; nor &lt;code&gt;batch_sampler&lt;/code&gt; is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.</source>
          <target state="translated">어느 쪽도 &lt;code&gt;sampler&lt;/code&gt; 도 &lt;code&gt;batch_sampler&lt;/code&gt; 는 이러한 데이터 세트는 키 또는 인덱스의 어떤 개념이 없기 때문에, 반복 가능한 스타일의 데이터 세트와 호환되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9821f42c2d297ad4796746e3fa14e55cb3e99e22" translate="yes" xml:space="preserve">
          <source>Nesterov momentum is based on the formula from &lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf&quot;&gt;On the importance of initialization and momentum in deep learning&lt;/a&gt;.</source>
          <target state="translated">Nesterov 모멘텀은 &lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf&quot;&gt;딥 러닝에서 초기화 및 모멘텀의 중요성&lt;/a&gt; 에 대한 공식을 기반으로합니다 .</target>
        </trans-unit>
        <trans-unit id="53ebc572b4a44802ba114729f07bdaaf5409a9d7" translate="yes" xml:space="preserve">
          <source>Network</source>
          <target state="translated">Network</target>
        </trans-unit>
        <trans-unit id="813b5b686d397a3b63ce050a22e4bcaa735af1e8" translate="yes" xml:space="preserve">
          <source>New API:</source>
          <target state="translated">New API:</target>
        </trans-unit>
        <trans-unit id="cc7e29ed592d2dbd4407e7babc4a099e18fb72ad" translate="yes" xml:space="preserve">
          <source>New distribution instance with batch dimensions expanded to &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="translated">배치 차원이 &lt;code&gt;batch_size&lt;/code&gt; 로 확장 된 새 배포 인스턴스 .</target>
        </trans-unit>
        <trans-unit id="540f7186c4d530bac579f69bb157fd5e121529cc" translate="yes" xml:space="preserve">
          <source>NewType</source>
          <target state="translated">NewType</target>
        </trans-unit>
        <trans-unit id="0013bf26fef8048809165bc5fc20da2af938e96c" translate="yes" xml:space="preserve">
          <source>No expressions except method definitions are allowed in the body of the class.</source>
          <target state="translated">No expressions except method definitions are allowed in the body of the class.</target>
        </trans-unit>
        <trans-unit id="7a05537029cdbc40cf12ae1af3fd5798f2e0fb52" translate="yes" xml:space="preserve">
          <source>No support for inheritance or any other polymorphism strategy, except for inheriting from &lt;code&gt;object&lt;/code&gt; to specify a new-style class.</source>
          <target state="translated">No support for inheritance or any other polymorphism strategy, except for inheriting from &lt;code&gt;object&lt;/code&gt; to specify a new-style class.</target>
        </trans-unit>
        <trans-unit id="16585665bdbf85c2a46ca04dcdf6f3b3c87a4c04" translate="yes" xml:space="preserve">
          <source>No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.</source>
          <target state="translated">No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.</target>
        </trans-unit>
        <trans-unit id="3a122cf075b0ba371ed052ceb0f012b712c9d692" translate="yes" xml:space="preserve">
          <source>Node 1: &lt;em&gt;(IP: 192.168.1.1, and has a free port: 1234)&lt;/em&gt;</source>
          <target state="translated">Node 1: &lt;em&gt;(IP: 192.168.1.1, and has a free port: 1234)&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="d7c20c7d93b71387bce45a0f532f94aceec659b1" translate="yes" xml:space="preserve">
          <source>Node 2:</source>
          <target state="translated">Node 2:</target>
        </trans-unit>
        <trans-unit id="2200871a1ac28c0b5dd797edb44ce5069b3ba408" translate="yes" xml:space="preserve">
          <source>Nominal typing is in development, but structural typing is not</source>
          <target state="translated">공칭 타이핑은 개발 중이지만 구조 타이핑은 아닙니다.</target>
        </trans-unit>
        <trans-unit id="56c04398e9678198426e682b1f9d96427b74b23b" translate="yes" xml:space="preserve">
          <source>Nominal vs structural subtyping</source>
          <target state="translated">공칭 vs 구조적 서브 타이핑</target>
        </trans-unit>
        <trans-unit id="b89eadfaef5d8d82b020885869aa96878170cfaa" translate="yes" xml:space="preserve">
          <source>Non-ATen operators</source>
          <target state="translated">비 ATEN 연산자</target>
        </trans-unit>
        <trans-unit id="05ac3d4369dff83cd256460b1c7ab61b3d6873b9" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (other)</source>
          <target state="translated">비선형 활성화 (기타)</target>
        </trans-unit>
        <trans-unit id="37e8f09f37b4ba586b29c8d43264a706f392c534" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (weighted sum, nonlinearity)</source>
          <target state="translated">비선형 활성화 (가중 합, 비선형 성)</target>
        </trans-unit>
        <trans-unit id="90f4c5f6b363fba151454b4dc3811eace77006ea" translate="yes" xml:space="preserve">
          <source>Non-linear activation functions</source>
          <target state="translated">비선형 활성화 함수</target>
        </trans-unit>
        <trans-unit id="32cfd0f165a52f907e24f0d0d41b54665a307df6" translate="yes" xml:space="preserve">
          <source>Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in &lt;a href=&quot;#use-of-python-values&quot;&gt;Use of Python Values&lt;/a&gt;.</source>
          <target state="translated">비 지역 변수는 함수가 정의 될 때 컴파일 타임에 Python 값으로 확인됩니다. 그런 다음 이러한 값은 &lt;a href=&quot;#use-of-python-values&quot;&gt;Python 값 사용에&lt;/a&gt; 설명 된 규칙을 사용하여 TorchScript 값으로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="6eef6648406c333a4035cd5e60d0bf2ecf2606d7" translate="yes" xml:space="preserve">
          <source>None</source>
          <target state="translated">None</target>
        </trans-unit>
        <trans-unit id="8acafd3007c1ab1f15fdb73fa725fb8e794923ba" translate="yes" xml:space="preserve">
          <source>None if &lt;code&gt;join&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;ProcessContext&lt;/code&gt; if &lt;code&gt;join&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;join&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 없음 , &lt;code&gt;join&lt;/code&gt; 이 &lt;code&gt;False&lt;/code&gt; 인 경우 &lt;code&gt;ProcessContext&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ab71bc2480c32bc39f483a12c34cd4676886028d" translate="yes" xml:space="preserve">
          <source>None, module is modified inplace with added observer modules and forward_hooks</source>
          <target state="translated">없음, 추가 된 관찰자 모듈 및 forward_hooks를 사용하여 모듈이 제자리에서 수정되었습니다. ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="e4fa32a75bd8c135dd060f289888f83fb70a2d47" translate="yes" xml:space="preserve">
          <source>None, module is modified inplace with qconfig attached</source>
          <target state="translated">없음, 모듈은 qconfig가 연결된 상태로 제자리에서 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="45e118d0563ea8581f830f46e85b60ae714faae4" translate="yes" xml:space="preserve">
          <source>Normal</source>
          <target state="translated">Normal</target>
        </trans-unit>
        <trans-unit id="46e38cc5e7e0b2b170857112460a732303fdb5e3" translate="yes" xml:space="preserve">
          <source>Normalization Layers</source>
          <target state="translated">정규화 레이어</target>
        </trans-unit>
        <trans-unit id="746e3a4c0d98d2df15064221df23fa7793e56038" translate="yes" xml:space="preserve">
          <source>Normalization functions</source>
          <target state="translated">정규화 기능</target>
        </trans-unit>
        <trans-unit id="c4639d9330cdd597089734b95310026444f4de7e" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fae17255f7a6267cdf6f973a75edad225dcf4d82" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="37141e0de089638ade05f2fc80c5c6e1a4b3e953" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d960941bb7a853f66a4f14f14d7f53d0f7022466" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역변환 ( &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="7b81cd890e0116574dbf7776b7567225bd21e39c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e18f2dcd89118b40d295cc2d0f570dae9ee63571" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="20885ecc112a0654f2fd1749899c2aade057f18f" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="36f2290f1751dd4fd40961edc0b9d58c6d126bef" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f07c249cb9e5f2751b249e4c52c89f4d38befc44" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d3864acfcc3775dda73f1b742d452bc185e4b45c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="290617bd60b4381fb793ad7f08e51d3f4c292b8a" translate="yes" xml:space="preserve">
          <source>Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.</source>
          <target state="translated">일반적으로 사용자가 함수와 상호 작용하는 유일한 방법은 하위 클래스를 만들고 새 작업을 정의하는 것입니다. 이것은 torch.autograd를 확장하는 데 권장되는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="ac15bca5bbea06cd32f056c8cbf634e4f1faea41" translate="yes" xml:space="preserve">
          <source>Not implemented</source>
          <target state="translated">구현되지 않음</target>
        </trans-unit>
        <trans-unit id="df8f8315e0c299c571aa4226e66410f1097a424b" translate="yes" xml:space="preserve">
          <source>Not providing a value for &lt;code&gt;steps&lt;/code&gt; is deprecated. For backwards compatibility, not providing a value for &lt;code&gt;steps&lt;/code&gt; will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for &lt;code&gt;steps&lt;/code&gt; will throw a runtime error.</source>
          <target state="translated">&lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않는 것은 더 이상 사용되지 않습니다. 이전 버전과의 호환성을 위해 &lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않으면 요소가 100 개인 텐서가 생성됩니다. 이 동작은 문서화 된 함수 시그니처에 반영되지 않으며 의존해서는 안됩니다. 향후 PyTorch 릴리스에서 &lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않으면 런타임 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2c924e3088204ee77ba681f72be3444357932fca" translate="yes" xml:space="preserve">
          <source>Note</source>
          <target state="translated">Note</target>
        </trans-unit>
        <trans-unit id="a73e4c5274c790b0c43ddacd3a38f0f1878acefd" translate="yes" xml:space="preserve">
          <source>Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):</source>
          <target state="translated">주기의 총 단계 수는 다음 두 가지 방법 중 하나로 결정될 수 있습니다 (우선 순위에 따라 나열 됨).</target>
        </trans-unit>
        <trans-unit id="da7038b6159ab37164aeb7442907c881108aff7e" translate="yes" xml:space="preserve">
          <source>Note that</source>
          <target state="translated">참고</target>
        </trans-unit>
        <trans-unit id="5433b2e634d55f555a9239d7a13b88b55129e9b2" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt; before Python version 3.6) does not preserve the order of the merged mapping.</source>
          <target state="translated">다른 정렬되지 않은 매핑 유형 (예 : Python 버전 3.6 이전 의 Python의 일반 &lt;code&gt;dict&lt;/code&gt; &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt; 사용 하는 update () 는 병합 된 매핑의 순서를 유지하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a1dc440ad60db846cad7f7d0a7ce1260c129ea8c" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt;) does not preserve the order of the merged mapping.</source>
          <target state="translated">다른 정렬되지 않은 매핑 유형 (예 : Python의 일반 &lt;code&gt;dict&lt;/code&gt; ) 을 &lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt; 하는 update () 는 병합 된 매핑의 순서를 유지하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="42c994d9673f83cc70c78e02c5d9a230c6566874" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; in &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt; are used to &lt;strong&gt;instantiate&lt;/strong&gt; a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is</source>
          <target state="translated">참고 &lt;code&gt;*args&lt;/code&gt; 와 &lt;code&gt;**kwargs&lt;/code&gt; 로 의 &lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt; 사용됩니다 &lt;strong&gt;인스턴스화&lt;/strong&gt; 모델을. 모델을로드 한 후 모델로 수행 할 수있는 작업을 어떻게 알 수 있습니까? 제안 된 워크 플로는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="2d1e0d8414eaec791533b2f0922dd4ade580351c" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt; reductions are not available when using the &lt;code&gt;NCCL&lt;/code&gt; backend.</source>
          <target state="translated">참고 &lt;code&gt;BAND&lt;/code&gt; , &lt;code&gt;BOR&lt;/code&gt; 및 &lt;code&gt;BXOR&lt;/code&gt; 의 사용할 때 감소를 사용할 수없는 &lt;code&gt;NCCL&lt;/code&gt; 의 백엔드.</target>
        </trans-unit>
        <trans-unit id="0bd438191e4b3887c615246e19e8361b84bd83a7" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; and &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; is redundant. We can thus compute the forward transform without considering negative frequencies:</source>
          <target state="translated">참고 &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; 및 &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; 중복된다. 따라서 음의 주파수를 고려하지 않고 순방향 변환을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ee47ccc0fe1d0f9aca7dffbff0fc6bd81769f2e4" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;len(output_tensor_list)&lt;/code&gt; needs to be the same for all the distributed processes calling this function.</source>
          <target state="translated">참고 &lt;code&gt;len(output_tensor_list)&lt;/code&gt; 요구가이 함수를 호출하는 모든 분산 된 프로세스에 대해 동일합니다.</target>
        </trans-unit>
        <trans-unit id="de81d60cb1e06955cef77dc7af826512edffc0f1" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;total_count&lt;/code&gt; need not be specified if only &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.log_prob&quot;&gt;&lt;code&gt;log_prob()&lt;/code&gt;&lt;/a&gt; is called (see example below)</source>
          <target state="translated">참고 &lt;code&gt;total_count&lt;/code&gt; 경우에만 필요가 지정 될 수 없습니다 &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.log_prob&quot;&gt; &lt;code&gt;log_prob()&lt;/code&gt; &lt;/a&gt; 호출됩니다 (아래 예 참조)</target>
        </trans-unit>
        <trans-unit id="7ad0240a1b6c1c4077cd6f27931fdf77ffb68b6b" translate="yes" xml:space="preserve">
          <source>Note that QConfig needs to contain observer &lt;strong&gt;classes&lt;/strong&gt; (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.</source>
          <target state="translated">QConfig에는 관찰자 &lt;strong&gt;클래스&lt;/strong&gt; (예 : MinMaxObserver) 또는 구체적인 관찰자 인스턴스 자체가 아닌 호출시 인스턴스를 반환하는 콜 러블 이 포함 되어야합니다. 양자화 준비 기능은 각 레이어에 대해 관찰자를 여러 번 인스턴스화합니다.</target>
        </trans-unit>
        <trans-unit id="1ccf01e0451a8d6abc36c37e011e58952b42e20e" translate="yes" xml:space="preserve">
          <source>Note that QConfigDynamic needs to contain observer &lt;strong&gt;classes&lt;/strong&gt; (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers.</source>
          <target state="translated">QConfigDynamic에는 관찰자 &lt;strong&gt;클래스&lt;/strong&gt; (예 : MinMaxObserver) 또는 구체적인 관찰자 인스턴스 자체가 아니라 호출시 인스턴스를 반환하는 콜 러블 이 포함 되어야합니다. 양자화 기능은 각 레이어에 대해 관찰자를 여러 번 인스턴스화합니다.</target>
        </trans-unit>
        <trans-unit id="d9706617db521f3c057495c1501777340ff2c154" translate="yes" xml:space="preserve">
          <source>Note that automatic rank assignment is not supported anymore in the latest distributed package and &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">자동 순위 할당은 최신 배포 패키지에서 더 이상 지원 되지 않으며 &lt;code&gt;group_name&lt;/code&gt; 도 더 이상 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d90621e782b71803aaf8f760cde86adc6ccf606c" translate="yes" xml:space="preserve">
          <source>Note that deterministic operations tend to have worse performance than non-deterministic operations.</source>
          <target state="translated">결정적 작업은 비 결정적 작업보다 성능이 떨어지는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="b7948c066b35cd06338c8b76a0036f57024ca522" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;input_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt;, since the function scatters the result from every single GPU in the group. To interpret each element of &lt;code&gt;input_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;output_tensor_list[j]&lt;/code&gt; of rank k receives the reduce-scattered result from &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">함수가 그룹의 모든 단일 GPU에서 결과를 분산 &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt; &lt;code&gt;input_tensor_lists&lt;/code&gt; 의 각 요소의 크기는 world_size * len (output_tensor_list) 입니다. 각 요소 해석하는 &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; 것을 참고 &lt;code&gt;output_tensor_list[j]&lt;/code&gt; 랭크는 K의 감소로부터 산란 결과 수신 &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f1fa921e9c60fd6eb9bf5d36ace2910b625e1764" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;output_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt;, since the function all gathers the result from every single GPU in the group. To interpret each element of &lt;code&gt;output_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;input_tensor_list[j]&lt;/code&gt; of rank k will be appear in &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;output_tensor_lists&lt;/code&gt; 의 각 요소의 크기는 &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt; . 모든 함수는 그룹의 모든 단일 GPU에서 결과를 수집하기 때문입니다. 각 요소 해석하는 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 것을 주목 &lt;code&gt;input_tensor_list[j]&lt;/code&gt; 랭크는 K의에 표시한다 &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a779c7a4b4446c6b4a026c9792070b87e0802a90" translate="yes" xml:space="preserve">
          <source>Note that if there will be a lot of tensors shared, this strategy will keep a large number of file descriptors open most of the time. If your system has low limits for the number of open file descriptors, and you can&amp;rsquo;t raise them, you should use the &lt;code&gt;file_system&lt;/code&gt; strategy.</source>
          <target state="translated">많은 텐서가 공유되는 경우이 전략은 대부분의 시간 동안 많은 수의 파일 설명자를 열어 둡니다. 시스템에 열린 파일 설명자 수에 대한 제한이 낮고이를 올릴 수없는 경우 &lt;code&gt;file_system&lt;/code&gt; 전략을 사용해야합니다 .</target>
        </trans-unit>
        <trans-unit id="261fa08253a2249def1244a03fcd9b8841a48f8d" translate="yes" xml:space="preserve">
          <source>Note that multicast address is not supported anymore in the latest distributed package. &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">멀티 캐스트 주소는 최신 배포 패키지에서 더 이상 지원되지 않습니다. &lt;code&gt;group_name&lt;/code&gt; 도 더 이상 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b2830ad219b13cbec38ff7f3f532533bf250586f" translate="yes" xml:space="preserve">
          <source>Note that non-integer &lt;code&gt;step&lt;/code&gt; is subject to floating point rounding errors when comparing against &lt;code&gt;end&lt;/code&gt;; to avoid inconsistency, we advise adding a small epsilon to &lt;code&gt;end&lt;/code&gt; in such cases.</source>
          <target state="translated">정수가 아닌 &lt;code&gt;step&lt;/code&gt; 는 &lt;code&gt;end&lt;/code&gt; 와 비교할 때 부동 소수점 반올림 오류의 영향을받습니다 . 피할 불일치에, 우리가 할 수있는 작은 엡실론 추가 조언한다 &lt;code&gt;end&lt;/code&gt; 이러한 경우를.</target>
        </trans-unit>
        <trans-unit id="112ac3f4ac80a498de80217376157fc61531fc0a" translate="yes" xml:space="preserve">
          <source>Note that one should use &lt;code&gt;cache_size=1&lt;/code&gt; when it comes to &lt;code&gt;NaN/Inf&lt;/code&gt; values.</source>
          <target state="translated">&lt;code&gt;NaN/Inf&lt;/code&gt; 값에 관해서는 &lt;code&gt;cache_size=1&lt;/code&gt; 을 사용해야 합니다.</target>
        </trans-unit>
        <trans-unit id="0bc75f2496d9d7af4d1e5ae3151f63a061a03230" translate="yes" xml:space="preserve">
          <source>Note that the &lt;code&gt;.event_shape&lt;/code&gt; of a &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;TransformedDistribution&lt;/code&gt;&lt;/a&gt; is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;TransformedDistribution&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;.event_shape&lt;/code&gt; 는 기본 분포와 해당 변환의 최대 모양입니다. 변환은 이벤트간에 상관 관계를 도입 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="fe417e45eca8b56b8a391ee5d65f646c54c00fc4" translate="yes" xml:space="preserve">
          <source>Note that the input to LongTensor is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:</source>
          <target state="translated">LongTensor에 대한 입력은 인덱스 튜플 목록이 아닙니다. 이런 식으로 인덱스를 작성하려면 희소 생성자에 전달하기 전에 전치해야합니다.</target>
        </trans-unit>
        <trans-unit id="7e0f39780c8cac4e285c8ec3c405982d7cafc31f" translate="yes" xml:space="preserve">
          <source>Note that these cases may in fact be traceable in the future.</source>
          <target state="translated">이러한 경우는 실제로 향후 추적이 가능할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="59bdfd4be005baa7a143e9e7af0a324461ee7947" translate="yes" xml:space="preserve">
          <source>Note that this enumerates over all batched tensors in lock-step &lt;code&gt;[[0, 0], [1, 1], &amp;hellip;]&lt;/code&gt;. With &lt;code&gt;expand=False&lt;/code&gt;, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, &lt;code&gt;[[0], [1], ..&lt;/code&gt;.</source>
          <target state="translated">이것은 잠금 단계 &lt;code&gt;[[0, 0], [1, 1], &amp;hellip;]&lt;/code&gt; 에서 모든 일괄 처리 된 텐서를 열거 합니다. 로 &lt;code&gt;expand=False&lt;/code&gt; , 열거 0 희미한 따라 발생하지만, 나머지 배치 기준 치수 싱글 인 상태 &lt;code&gt;[[0], [1], ..&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="752392a2ff1e2a39cc0db5f6e4585276ad451ffb" translate="yes" xml:space="preserve">
          <source>Note that this function is simply doing &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt;. Using that &lt;code&gt;isinstance&lt;/code&gt; check is better for typechecking with mypy, and more explicit - so it&amp;rsquo;s recommended to use that instead of &lt;code&gt;is_tensor&lt;/code&gt;.</source>
          <target state="translated">이 함수는 단순히 &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt; 합니다. 해당 &lt;code&gt;isinstance&lt;/code&gt; 검사를 사용하는 것이 mypy로 유형 검사를 수행하는 데 더 좋고 더 명시 &lt;code&gt;is_tensor&lt;/code&gt; 대신 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="a5b8b84abd5cbd1ff532fd7ae1e19baa9b3b1a28" translate="yes" xml:space="preserve">
          <source>Note that this function requires Python 3.4 or higher.</source>
          <target state="translated">이 함수에는 Python 3.4 이상이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="e898adc76bed905448d3327e88134b4893e4bf98" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;matplotlib&lt;/code&gt; package.</source>
          <target state="translated">이를 위해서는 &lt;code&gt;matplotlib&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="ba4e51216b174df96efb807dbfef6adf290a07f2" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;moviepy&lt;/code&gt; package.</source>
          <target state="translated">여기에는 &lt;code&gt;moviepy&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="da8c941a1c4dc21607ac70d896e4a7cb648a3554" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;pillow&lt;/code&gt; package.</source>
          <target state="translated">&lt;code&gt;pillow&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="38c3af3645ee3d8c1e85f997b98d93ebdc11ac44" translate="yes" xml:space="preserve">
          <source>Note that when &lt;code&gt;tracker&lt;/code&gt; stores Tensor objects from the LOBPCG instance, it must make copies of these.</source>
          <target state="translated">&lt;code&gt;tracker&lt;/code&gt; 가 LOBPCG 인스턴스에서 Tensor 개체를 저장할 때 이러한 개체의 복사본을 만들어야합니다.</target>
        </trans-unit>
        <trans-unit id="76d32111f7bcbce3e8104b2ef5376761705e1942" translate="yes" xml:space="preserve">
          <source>Note: A full example to apply nn.Transformer module for the word language model is available in &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt;</source>
          <target state="translated">참고 : 단어 언어 모델에 nn.Transformer 모듈을 적용하는 전체 예제는 &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt; 에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2acb81c5ebbb77d4e03260d44dbd15da614d8dba" translate="yes" xml:space="preserve">
          <source>Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode.</source>
          <target state="translated">참고 : 변환기 모델의 다중 헤드 어텐션 아키텍처로 인해 변환기의 출력 시퀀스 길이는 디코딩의 입력 시퀀스 (즉, 대상) 길이와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="ea3f023c9638ee2f77985080cfee0c3149276897" translate="yes" xml:space="preserve">
          <source>Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.</source>
          <target state="translated">참고 : 모델로드는 일반적인 사용 사례이지만 토크 나이저, 손실 함수 등과 같은 다른 개체를로드하는데도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fec631db96f94a854d531dcf201c32e0e5b12fec" translate="yes" xml:space="preserve">
          <source>Note: When beta is set to 0, this is equivalent to &lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt;&lt;code&gt;L1Loss&lt;/code&gt;&lt;/a&gt;. Passing a negative value in for beta will result in an exception.</source>
          <target state="translated">참고 : 베타가 0으로 설정되면 &lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt; &lt;code&gt;L1Loss&lt;/code&gt; &lt;/a&gt; 와 동일합니다 . 베타에 음수 값을 전달하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="9b7c2d4ed1b493db790041907421b97fbf38eae9" translate="yes" xml:space="preserve">
          <source>Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with &lt;code&gt;True&lt;/code&gt; are not allowed to attend while &lt;code&gt;False&lt;/code&gt; values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of &lt;code&gt;True&lt;/code&gt; will be ignored while the position with the value of &lt;code&gt;False&lt;/code&gt; will be unchanged.</source>
          <target state="translated">참고 : [src / tgt / memory] _mask는 위치 i가 마스크되지 않은 위치에 참석할 수 있도록합니다. ByteTensor가 제공되면 0이 아닌 위치는 참석할 수 없지만 0 위치는 변경되지 않습니다. BoolTensor가 제공되면 &lt;code&gt;True&lt;/code&gt; 인 위치는 참석할 수 없으며 &lt;code&gt;False&lt;/code&gt; 값은 변경되지 않습니다. FloatTensor가 제공되면주의 가중치에 추가됩니다. [src / tgt / memory] _key_padding_mask는주의에 의해 무시 될 키에 지정된 요소를 제공합니다. ByteTensor가 제공되면 0이 아닌 위치는 무시되고 0 위치는 변경되지 않습니다. BoolTensor가 제공되면 &lt;code&gt;True&lt;/code&gt; 값의 위치는 무시되고 &lt;code&gt;False&lt;/code&gt; 값의 위치는 무시됩니다. 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="70440046a3dc2e079f23ee1c57dfa76669b732aa" translate="yes" xml:space="preserve">
          <source>Notes</source>
          <target state="translated">Notes</target>
        </trans-unit>
        <trans-unit id="4425723195c4a3b16b476f69800fa3cf16abb9b4" translate="yes" xml:space="preserve">
          <source>Notice that if</source>
          <target state="translated">만약</target>
        </trans-unit>
        <trans-unit id="d1656ebe55b849d2893f4a0cf92db35aea6025bc" translate="yes" xml:space="preserve">
          <source>Notice that operators can also have associated &lt;code&gt;blocks&lt;/code&gt;, namely the &lt;code&gt;prim::Loop&lt;/code&gt; and &lt;code&gt;prim::If&lt;/code&gt; operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.</source>
          <target state="translated">연산자는 또한 연관된 &lt;code&gt;blocks&lt;/code&gt; , 즉 &lt;code&gt;prim::Loop&lt;/code&gt; 및 &lt;code&gt;prim::If&lt;/code&gt; 연산자를 가질 수 있습니다 . 그래프 출력에서 ​​이러한 연산자는 쉽게 디버깅 할 수 있도록 동등한 소스 코드 양식을 반영하도록 형식이 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="04a5b9046bd473cb428ae07e5d6a160de265bc5d" translate="yes" xml:space="preserve">
          <source>Notice that the symmetric element &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; is omitted. At the Nyquist frequency &lt;code&gt;T[-2] == T[2]&lt;/code&gt; is it&amp;rsquo;s own symmetric pair, and therefore must always be real-valued.</source>
          <target state="translated">대칭 요소 &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; 가 생략되었습니다. Nyquist 주파수에서 &lt;code&gt;T[-2] == T[2]&lt;/code&gt; 는 자체 대칭 쌍이므로 항상 실수 값이어야합니다.</target>
        </trans-unit>
        <trans-unit id="1956334a42d3e178212344356997b21e347fa273" translate="yes" xml:space="preserve">
          <source>Now PyTorch is able to export &lt;code&gt;elu&lt;/code&gt; operator.</source>
          <target state="translated">이제 PyTorch는 &lt;code&gt;elu&lt;/code&gt; 연산자 를 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a30a0ffebbaef4efa26c2a00eda09cf59167dfe6" translate="yes" xml:space="preserve">
          <source>Now the exported ONNX graph becomes:</source>
          <target state="translated">이제 내 보낸 ONNX 그래프는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="27759972b02a15b63bf671d015c9407939f14fb3" translate="yes" xml:space="preserve">
          <source>Numerical gradient checking</source>
          <target state="translated">수치 적 그래디언트 검사</target>
        </trans-unit>
        <trans-unit id="08a914cde05039694ef0194d9ee79ff9a79dde33" translate="yes" xml:space="preserve">
          <source>O</source>
          <target state="translated">O</target>
        </trans-unit>
        <trans-unit id="58e9484683a456865672c8a4517b530e0f41cd11" translate="yes" xml:space="preserve">
          <source>ONLY INDICES:</source>
          <target state="translated">유일한 지표 :</target>
        </trans-unit>
        <trans-unit id="b335243efa572b5f0beb01acb45b42a02be187b5" translate="yes" xml:space="preserve">
          <source>ONNX</source>
          <target state="translated">ONNX</target>
        </trans-unit>
        <trans-unit id="98099fd5d344320bb061ab40d4f82b1271288a2a" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN</source>
          <target state="translated">ONNX_ATEN</target>
        </trans-unit>
        <trans-unit id="7fd76316b691135ff7ad4e41541a37a2b2ed8c3d" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN_FALLBACK</source>
          <target state="translated">ONNX_ATEN_FALLBACK</target>
        </trans-unit>
        <trans-unit id="0976054bd35910dc334e574eb3e3c16bf4dc9b17" translate="yes" xml:space="preserve">
          <source>ONNX_FALLTHROUGH</source>
          <target state="translated">ONNX_FALLTHROUGH</target>
        </trans-unit>
        <trans-unit id="946938795b376f71c3a327dfe4ec5463d0103fa6" translate="yes" xml:space="preserve">
          <source>Object Detection, Instance Segmentation and Person Keypoint Detection</source>
          <target state="translated">물체 감지, 인스턴스 분할 및 사람 키포인트 감지</target>
        </trans-unit>
        <trans-unit id="56b40c7447fb6cdfcf37a82cf91030f8b0adc145" translate="yes" xml:space="preserve">
          <source>Observer classes have usually reasonable default arguments, but they can be overwritten with &lt;code&gt;with_args&lt;/code&gt; method (that behaves like functools.partial):</source>
          <target state="translated">관찰자 클래스는 일반적으로 합리적인 기본 인수를 갖지만 &lt;code&gt;with_args&lt;/code&gt; 메서드 (functools.partial처럼 동작 함) 로 덮어 쓸 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="05df80fc8f6cd937ee6fca4db89715da3aeb05ee" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the moving average of the min and max values.</source>
          <target state="translated">최소값과 최대 값의 이동 평균을 기반으로 양자화 매개 변수를 계산하기위한 관찰자 모듈.</target>
        </trans-unit>
        <trans-unit id="0e6416b8d542683480294d9aa23272830844da59" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the running min and max values.</source>
          <target state="translated">실행중인 최소 및 최대 값을 기반으로 양자화 매개 변수를 계산하기위한 관찰자 모듈.</target>
        </trans-unit>
        <trans-unit id="06100f7170e7f76076139e20082a1eb3afab8d09" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the running per channel min and max values.</source>
          <target state="translated">채널당 실행 최소 및 최대 값을 기반으로 양자화 매개 변수를 계산하기위한 관찰자 모듈.</target>
        </trans-unit>
        <trans-unit id="77e203a6c2d75205e18b618179c387b4e62870c5" translate="yes" xml:space="preserve">
          <source>Observer that doesn&amp;rsquo;t do anything and just passes its configuration to the quantized module&amp;rsquo;s &lt;code&gt;.from_float()&lt;/code&gt;.</source>
          <target state="translated">아무것도하지 않고 그 구성을 양자화 된 모듈의 &lt;code&gt;.from_float()&lt;/code&gt; 전달하는 옵저버 .</target>
        </trans-unit>
        <trans-unit id="0618a214d16e39be75c6315a11d086a7dcbea1b6" translate="yes" xml:space="preserve">
          <source>Observers</source>
          <target state="translated">Observers</target>
        </trans-unit>
        <trans-unit id="8bf3249a3a8cece6d4343af20c9431018c926b24" translate="yes" xml:space="preserve">
          <source>Obtaining log-probabilities in a neural network is easily achieved by adding a &lt;code&gt;LogSoftmax&lt;/code&gt; layer in the last layer of your network. You may use &lt;code&gt;CrossEntropyLoss&lt;/code&gt; instead, if you prefer not to add an extra layer.</source>
          <target state="translated">네트워크 의 마지막 계층에 &lt;code&gt;LogSoftmax&lt;/code&gt; 계층 을 추가하면 신경망에서 로그 확률을 쉽게 얻을 수 있습니다. 추가 레이어를 추가하지 않으 &lt;code&gt;CrossEntropyLoss&lt;/code&gt; 대신 CrossEntropyLoss 를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b98c7204267732ffe5bc04ce20cbadbe8642e52b" translate="yes" xml:space="preserve">
          <source>Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you&amp;rsquo;re evaluating. If the profiler outputs don&amp;rsquo;t help, you could try looking at the result of &lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;nvprof&lt;/code&gt;. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline.</source>
          <target state="translated">물론 현실은 훨씬 더 복잡하며 평가하는 모델의 부분에 따라 스크립트가이 두 극단 중 하나가 아닐 수도 있습니다. 프로파일 러 출력이 도움이되지 않으면 &lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt; &lt;/a&gt; 하여 torch.autograd.profiler.emit_nvtx () 의 결과 를 &lt;code&gt;nvprof&lt;/code&gt; 있습니다. 그러나 NVTX 오버 헤드가 매우 높고 종종 심하게 왜곡 된 타임 라인을 제공한다는 점을 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="c37dc6bfd2614888384f13ab211221f9b141d421" translate="yes" xml:space="preserve">
          <source>Old API:</source>
          <target state="translated">이전 API :</target>
        </trans-unit>
        <trans-unit id="fe1a8467f796e691ae75cd019ca47d231225ebf5" translate="yes" xml:space="preserve">
          <source>On CUDA 10.1, set environment variable &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt;. This may affect performance.</source>
          <target state="translated">CUDA 10.1에서 환경 변수 &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt; 을 설정합니다 . 이는 성능에 영향을 미칠 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="87f5d31208b13882b13d7cef323f2a640539b60a" translate="yes" xml:space="preserve">
          <source>On CUDA 10.2 or later, set environment variable (note the leading colon symbol) &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; or &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt;.</source>
          <target state="translated">CUDA 10.2 이상에서는 환경 변수 (선행 콜론 기호 참고) &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; 또는 &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt; 를 설정 합니다.</target>
        </trans-unit>
        <trans-unit id="36e5819afd89f0fa826f96c64796ff1578b24bfd" translate="yes" xml:space="preserve">
          <source>On Unix, &lt;code&gt;fork()&lt;/code&gt; is the default &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt; start method. Using &lt;code&gt;fork()&lt;/code&gt;, child workers typically can access the &lt;code&gt;dataset&lt;/code&gt; and Python argument functions directly through the cloned address space.</source>
          <target state="translated">Unix에서 &lt;code&gt;fork()&lt;/code&gt; 는 기본 &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt; &lt;code&gt;multiprocessing&lt;/code&gt; &lt;/a&gt; 시작 방법입니다. &lt;code&gt;fork()&lt;/code&gt; 사용하면 하위 작업자는 일반적으로 복제 된 주소 공간을 통해 직접 &lt;code&gt;dataset&lt;/code&gt; 및 Python 인수 함수에 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="7e2ae33ff3c9afba44a2228dce980df719a2e771" translate="yes" xml:space="preserve">
          <source>On Windows, &lt;code&gt;spawn()&lt;/code&gt; is the default &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt; start method. Using &lt;code&gt;spawn()&lt;/code&gt;, another interpreter is launched which runs your main script, followed by the internal worker function that receives the &lt;code&gt;dataset&lt;/code&gt;, &lt;code&gt;collate_fn&lt;/code&gt; and other arguments through &lt;a href=&quot;https://docs.python.org/3/library/pickle.html#module-pickle&quot;&gt;&lt;code&gt;pickle&lt;/code&gt;&lt;/a&gt; serialization.</source>
          <target state="translated">Windows에서 &lt;code&gt;spawn()&lt;/code&gt; 은 기본 &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt; &lt;code&gt;multiprocessing&lt;/code&gt; &lt;/a&gt; 시작 방법입니다. &lt;code&gt;spawn()&lt;/code&gt; 사용하면 기본 스크립트를 실행하는 또 다른 인터프리터가 시작되고 그 뒤에 &lt;code&gt;dataset&lt;/code&gt; , &lt;code&gt;collate_fn&lt;/code&gt; 및 기타 인수를 &lt;a href=&quot;https://docs.python.org/3/library/pickle.html#module-pickle&quot;&gt; &lt;code&gt;pickle&lt;/code&gt; &lt;/a&gt; 직렬화를 통해 수신하는 내부 작업자 함수가 실행됩니다 .</target>
        </trans-unit>
        <trans-unit id="b433d4e11b599fd15dbc04960281f9eb287632d1" translate="yes" xml:space="preserve">
          <source>On each window, the function computed is:</source>
          <target state="translated">각 창에서 계산되는 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a84bfeb86e4aa154caeb1f582b53ed62fd133141" translate="yes" xml:space="preserve">
          <source>On modules, methods must be compiled before they can be called. The TorchScript compiler recursively compiles methods it sees when compiling other methods. By default, compilation starts on the &lt;code&gt;forward&lt;/code&gt; method. Any methods called by &lt;code&gt;forward&lt;/code&gt; will be compiled, and any methods called by those methods, and so on. To start compilation at a method other than &lt;code&gt;forward&lt;/code&gt;, use the &lt;a href=&quot;jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator (&lt;code&gt;forward&lt;/code&gt; implicitly is marked &lt;code&gt;@torch.jit.export&lt;/code&gt;).</source>
          <target state="translated">모듈에서 메서드는 호출되기 전에 컴파일되어야합니다. TorchScript 컴파일러는 다른 메서드를 컴파일 할 때 표시되는 메서드를 재귀 적으로 컴파일합니다. 기본적으로 컴파일은 &lt;code&gt;forward&lt;/code&gt; 메서드 에서 시작됩니다 . &lt;code&gt;forward&lt;/code&gt; 에 의해 호출 된 모든 메서드와 해당 메서드에 의해 호출되는 메서드 등이 컴파일됩니다. &lt;code&gt;forward&lt;/code&gt; 이외의 메서드에서 컴파일을 시작하려면 &lt;a href=&quot;jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt; 데코레이터를 사용합니다 ( &lt;code&gt;forward&lt;/code&gt; 은 암시 적으로 &lt;code&gt;@torch.jit.export&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="a488ba8f17e355abbbc16eb50cb56f628e0ef099" translate="yes" xml:space="preserve">
          <source>On the other hand, invoking &lt;code&gt;trace&lt;/code&gt; with module&amp;rsquo;s instance (e.g. &lt;code&gt;my_module&lt;/code&gt;) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.</source>
          <target state="translated">반면에 모듈의 인스턴스 (예 : &lt;code&gt;my_module&lt;/code&gt; )로 &lt;code&gt;trace&lt;/code&gt; 를 호출 하면 새 모듈이 생성되고 매개 변수가 새 모듈에 올바르게 복사되므로 필요한 경우 그래디언트를 누적 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b392f645cfdecae166313100197dfb1e06d02b4b" translate="yes" xml:space="preserve">
          <source>Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).</source>
          <target state="translated">모든 DDP 프로세스가 결합되면 컨텍스트 관리자는 마지막으로 결합 된 프로세스에 해당하는 모델을 모든 프로세스에 브로드 캐스트하여 모델이 모든 프로세스에서 동일한 지 확인합니다 (DDP에 의해 보장됨).</target>
        </trans-unit>
        <trans-unit id="3ebcd2d39bb7401d6e980dad866025a71193ae15" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for Caffe2:</source>
          <target state="translated">설치가 완료되면 Caffe2 용 백엔드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c7da4cd77b5400566e4f52a1d8995e48e44e2095" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for ONNX Runtime:</source>
          <target state="translated">설치가 완료되면 ONNX 런타임 용 백엔드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="181dde386ac4e48b01ea4c9ac2766b9fdca3442c" translate="yes" xml:space="preserve">
          <source>Once you&amp;rsquo;ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.</source>
          <target state="translated">TensorBoard를 설치하면 이러한 유틸리티를 사용하여 TensorBoard UI 내에서 시각화 할 수 있도록 PyTorch 모델 및 메트릭을 디렉터리에 로깅 할 수 있습니다. 스칼라, 이미지, 히스토그램, 그래프 및 임베딩 시각화는 모두 PyTorch 모델과 텐서, Caffe2 네트 및 블롭에 대해 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="fa0ae9a8c41213fbda341d9975cb44fc985cb402" translate="yes" xml:space="preserve">
          <source>One can either give a &lt;code&gt;scale_factor&lt;/code&gt; or the target output &lt;code&gt;size&lt;/code&gt; to calculate the output size. (You cannot give both, as it is ambiguous)</source>
          <target state="translated">하나 중 하나 줄 수 &lt;code&gt;scale_factor&lt;/code&gt; 또는 목표 출력 &lt;code&gt;size&lt;/code&gt; 출력 크기를 계산한다. (모호하기 때문에 둘 다 줄 수는 없습니다)</target>
        </trans-unit>
        <trans-unit id="f84825776234c0c4615edbbd6631eafc8684c310" translate="yes" xml:space="preserve">
          <source>One cannot specify both positional args &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; and keyword args &lt;code&gt;rename_map&lt;/code&gt;.</source>
          <target state="translated">위치 인수 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 과 키워드 args &lt;code&gt;rename_map&lt;/code&gt; 을 모두 지정할 수는 없습니다 .</target>
        </trans-unit>
        <trans-unit id="f99ff2b17e8a666214d13b388ce8036710cb6b91" translate="yes" xml:space="preserve">
          <source>One way to automatically catch many errors in traces is by using &lt;code&gt;check_inputs&lt;/code&gt; on the &lt;code&gt;torch.jit.trace()&lt;/code&gt; API. &lt;code&gt;check_inputs&lt;/code&gt; takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:</source>
          <target state="translated">자동 트레이스 많은 오류를 포착하는 한 가지 방법은 사용하는 것입니다 &lt;code&gt;check_inputs&lt;/code&gt; 을 온 &lt;code&gt;torch.jit.trace()&lt;/code&gt; API. &lt;code&gt;check_inputs&lt;/code&gt; 는 계산을 다시 추적하고 결과를 확인하는 데 사용할 입력 튜플 목록을 가져옵니다. 예를 들면 :</target>
        </trans-unit>
        <trans-unit id="19248650fc8fc3134816a8981b1c84826b1d56cc" translate="yes" xml:space="preserve">
          <source>OneHotCategorical</source>
          <target state="translated">OneHotCategorical</target>
        </trans-unit>
        <trans-unit id="e9f428e38cd970cf00e186de12fa06466a0c70e3" translate="yes" xml:space="preserve">
          <source>Only 2D input is supported for quantized inputs</source>
          <target state="translated">양자화 된 입력에는 2D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="e86d4094fa43c487d3762de38c606afefec7936e" translate="yes" xml:space="preserve">
          <source>Only 2D inputs are supported</source>
          <target state="translated">2D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="daede8c2e196c09bd4c265e562b51d50f0840292" translate="yes" xml:space="preserve">
          <source>Only 2D/3D input is supported for quantized inputs</source>
          <target state="translated">양자화 된 입력에는 2D / 3D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="27966aeec004be97d5e6f5f602ef5deb9549007c" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;torch.quint8&lt;/code&gt; is supported for the input data type.</source>
          <target state="translated">입력 데이터 유형에는 &lt;code&gt;torch.quint8&lt;/code&gt; 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="04aa0322b01e7ae421d1285e24a5ecee9dd8da63" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;zeros&lt;/code&gt; is supported for the &lt;code&gt;padding_mode&lt;/code&gt; argument.</source>
          <target state="translated">&lt;code&gt;padding_mode&lt;/code&gt; 인수 에는 &lt;code&gt;zeros&lt;/code&gt; 만 지원됩니다 .</target>
        </trans-unit>
        <trans-unit id="63c087cd6938fb9b66dc8e1e4aed57dffdb285a6" translate="yes" xml:space="preserve">
          <source>Only CUDA ops are eligible for autocasting.</source>
          <target state="translated">CUDA 작업 만 자동 캐스팅에 적합합니다. ㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="a1a79ba7e0818fab9dabcadf1f32212012a954c6" translate="yes" xml:space="preserve">
          <source>Only leaf Tensors will have their &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated during a call to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt;. To get &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated for non-leaf Tensors, you can use &lt;a href=&quot;#torch.Tensor.retain_grad&quot;&gt;&lt;code&gt;retain_grad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">만 잎 텐서가있을 것이다 &lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt; 호출하는 동안 채워 &lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; . 리프가 아닌 Tensor에 대해 &lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; 를&lt;/a&gt; 채우 려면 &lt;a href=&quot;#torch.Tensor.retain_grad&quot;&gt; &lt;code&gt;retain_grad()&lt;/code&gt; &lt;/a&gt; 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4e5b1e16b907585048a530098a587f0e967c17c8" translate="yes" xml:space="preserve">
          <source>Only leaf Tensors will have their &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated during a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt;. To get &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated for non-leaf Tensors, you can use &lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt;&lt;code&gt;retain_grad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">만 잎 텐서가있을 것이다 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt; 호출하는 동안 채워 &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; . 리프가 아닌 Tensor에 대해 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; 를&lt;/a&gt; 채우 려면 &lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt; &lt;code&gt;retain_grad()&lt;/code&gt; &lt;/a&gt; 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="214baf780986ab4b5685dfad680eb394c92a2724" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend are currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 및 gloo 백엔드 만 지원되는 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="db8dc40130a88665cd5bf8f3be6d7ea249aff179" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 및 gloo 백엔드 만 지원됩니다. 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="df8cfb4a9929de36ffa3e1c6452d25a578580d0f" translate="yes" xml:space="preserve">
          <source>Only nccl backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 백엔드 만 지원됩니다. 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="d7f15fe4ed58d21e3568885b930a49d8cb28d993" translate="yes" xml:space="preserve">
          <source>Only one of &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt;&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt;&lt;code&gt;precision_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt; can be specified.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt; &lt;code&gt;covariance_matrix&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt; &lt;code&gt;precision_matrix&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; &lt;/a&gt; 중 하나만 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02ff51aa952480752b675430ae860b4c527dade3" translate="yes" xml:space="preserve">
          <source>Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an &lt;code&gt;out=...&lt;/code&gt; Tensor are allowed in autocast-enabled regions, but won&amp;rsquo;t go through autocasting. For example, in an autocast-enabled region &lt;code&gt;a.addmm(b, c)&lt;/code&gt; can autocast, but &lt;code&gt;a.addmm_(b, c)&lt;/code&gt; and &lt;code&gt;a.addmm(b, c, out=d)&lt;/code&gt; cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.</source>
          <target state="translated">out-of-place ops 및 Tensor 메서드 만 사용할 수 있습니다. &lt;code&gt;out=...&lt;/code&gt; Tensor 를 명시 적으로 제공하는 인플레 이스 변형 및 호출 은 자동 전송이 활성화 된 지역에서 허용되지만 자동 전송을 거치지 않습니다. 예를 들어, 자동 캐스트 활성화 영역에서 &lt;code&gt;a.addmm(b, c)&lt;/code&gt; 는 자동 캐스트 할 수 있지만 &lt;code&gt;a.addmm_(b, c)&lt;/code&gt; 및 &lt;code&gt;a.addmm(b, c, out=d)&lt;/code&gt; 는 자동 캐스트 할 수 없습니다. 최상의 성능과 안정성을 위해 자동 전송이 활성화 된 지역에서 위치가없는 작업을 선호합니다.</target>
        </trans-unit>
        <trans-unit id="6f7a0a50221e4f4092d5689875e749bacffbc3f1" translate="yes" xml:space="preserve">
          <source>Only the GPU of &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; on the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">순위가 &lt;code&gt;dst&lt;/code&gt; 인 프로세스에서 &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; 의 GPU 만 최종 결과를받습니다.</target>
        </trans-unit>
        <trans-unit id="c024a1c0a5c1c600879124a8ffc9c018f3eeaa1c" translate="yes" xml:space="preserve">
          <source>Only the following modes are supported for the quantized inputs:</source>
          <target state="translated">양자화 된 입력에는 다음 모드 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="4abce74e0ec5249c62cdd843742dc1fcd44cf957" translate="yes" xml:space="preserve">
          <source>Only the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">순위가 &lt;code&gt;dst&lt;/code&gt; 인 프로세스 만 최종 결과를받습니다.</target>
        </trans-unit>
        <trans-unit id="465e942c4c87f62f9475a2c554cc5cf2204bbefd" translate="yes" xml:space="preserve">
          <source>Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that dynamic lookups are not available.</source>
          <target state="translated">튜플, 목록 및 변수 만 JIT 입력 / 출력으로 지원됩니다. 사전과 문자열도 허용되지만 사용하지 않는 것이 좋습니다. 사용자는 dict 입력을주의 깊게 확인해야하며 동적 조회를 사용할 수 없다는 점을 염두에 두어야합니다.</target>
        </trans-unit>
        <trans-unit id="c63b5ae248ca6bf9edcb58a14545a2ec8ac9edff" translate="yes" xml:space="preserve">
          <source>Only works with &lt;code&gt;torch.per_tensor_affine&lt;/code&gt; quantization scheme.</source>
          <target state="translated">&lt;code&gt;torch.per_tensor_affine&lt;/code&gt; 양자화 체계 에서만 작동합니다 .</target>
        </trans-unit>
        <trans-unit id="b82189b88bcf4f6d2bb2ec50d464f322cc6fbe77" translate="yes" xml:space="preserve">
          <source>Only works with &lt;code&gt;torch.per_tensor_symmetric&lt;/code&gt; quantization scheme</source>
          <target state="translated">&lt;code&gt;torch.per_tensor_symmetric&lt;/code&gt; 양자화 체계 에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="4315fcaa2c448f9408ac0b9bd62d96495ec80efb" translate="yes" xml:space="preserve">
          <source>Op Eligibility</source>
          <target state="translated">운영 자격</target>
        </trans-unit>
        <trans-unit id="8e1bcca5493f841f86f549bb09efaabbc0b197b6" translate="yes" xml:space="preserve">
          <source>Op-Specific Behavior</source>
          <target state="translated">작업 별 동작</target>
        </trans-unit>
        <trans-unit id="a14e13faab81577c3e1bd1a5724a666f83008292" translate="yes" xml:space="preserve">
          <source>Opens an nvprof trace file and parses autograd annotations.</source>
          <target state="translated">nvprof 추적 파일을 열고 autograd 주석을 구문 분석합니다.</target>
        </trans-unit>
        <trans-unit id="d0f4a3ec3e4ad0aecc8e89f342dde91d8ebe6688" translate="yes" xml:space="preserve">
          <source>Operator Export Type</source>
          <target state="translated">운영자 내보내기 유형</target>
        </trans-unit>
        <trans-unit id="bb7373849dc3f047bbc13778dd3b1cb1da43b12b" translate="yes" xml:space="preserve">
          <source>OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph:</source>
          <target state="translated">OperatorExportTypes.ONNX : 모든 작업이 일반 ONNX 작업 (ONNX 네임 스페이스 포함)으로 내보내집니다. OperatorExportTypes.ONNX_ATEN : 모든 작업이 ATen 작업 (aten 네임 스페이스 포함)으로 내보내집니다. OperatorExportTypes.ONNX_ATEN_FALLBACK : ATen 작업이 ONNX에서 지원되지 않거나 해당 기호가 누락 된 경우 ATen 작업으로 폴백합니다. 등록 된 작업은 정기적으로 ONNX로 내보내집니다. 그래프 예 :</target>
        </trans-unit>
        <trans-unit id="e90414358dbfff0a68e4eb5d68a16978cf197d5a" translate="yes" xml:space="preserve">
          <source>Operators</source>
          <target state="translated">Operators</target>
        </trans-unit>
        <trans-unit id="ae7fe79b856a95e009187cbf46df9c67d852304a" translate="yes" xml:space="preserve">
          <source>Ops called with an explicit &lt;code&gt;dtype=...&lt;/code&gt; argument are not eligible, and will produce output that respects the &lt;code&gt;dtype&lt;/code&gt; argument.</source>
          <target state="translated">명시 적 &lt;code&gt;dtype=...&lt;/code&gt; 인수로 호출 된 Ops는 적합하지 않으며 &lt;code&gt;dtype&lt;/code&gt; 인수 를 존중하는 출력을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="f728860bd944be6f6762552490261e98b6019554" translate="yes" xml:space="preserve">
          <source>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they&amp;rsquo;re downstream from autocasted ops.</source>
          <target state="translated">아래에 나열되지 않은 작업은 자동 전송을 거치지 않습니다. 입력에 의해 정의 된 유형으로 실행됩니다. 그러나 자동 캐스팅은 목록에없는 작업이 자동 캐스팅 된 작업의 다운 스트림 인 경우 실행되는 유형을 계속 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1e2c18bb8c291433dee05fffb46fe3081033ce4f" translate="yes" xml:space="preserve">
          <source>Ops that can autocast to &lt;code&gt;float16&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;float16&lt;/code&gt; 으로 자동 캐스팅 할 수있는 작업</target>
        </trans-unit>
        <trans-unit id="d41b349395acc2ee22829c823ba6e5272a03f36e" translate="yes" xml:space="preserve">
          <source>Ops that can autocast to &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;float32&lt;/code&gt; 로 자동 캐스팅 할 수있는 작업</target>
        </trans-unit>
        <trans-unit id="de95d5ff9adfbfe0e08db9c9f3168e2ebb01f3f5" translate="yes" xml:space="preserve">
          <source>Ops that promote to the widest input type</source>
          <target state="translated">가장 광범위한 입력 유형으로 승격하는 작업</target>
        </trans-unit>
        <trans-unit id="c518b3a6d985b38fecd8953dbd43a11b6da0e4b9" translate="yes" xml:space="preserve">
          <source>Ops that run in &lt;code&gt;float64&lt;/code&gt; or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.</source>
          <target state="translated">에서 실행하는 것이 옵스 &lt;code&gt;float64&lt;/code&gt; 또는 비 부동 소수점 dtypes 자격이 없으며, 자동 시전 가능 여부를 이러한 형태로 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="326510735a447ec63da9abec360c7b0441bdd180" translate="yes" xml:space="preserve">
          <source>Optional Type Refinement</source>
          <target state="translated">선택적 유형 개선</target>
        </trans-unit>
        <trans-unit id="21826226f40329aa4cc3679f6cd2d7ad202e0147" translate="yes" xml:space="preserve">
          <source>Optional values &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; are scaling factors on the outer product between &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and the added matrix &lt;code&gt;input&lt;/code&gt; respectively.</source>
          <target state="translated">선택적 값 &lt;code&gt;beta&lt;/code&gt; 와 &lt;code&gt;alpha&lt;/code&gt; 는 각각 &lt;code&gt;vec1&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 사이의 외적 과 추가 된 행렬 &lt;code&gt;input&lt;/code&gt; 에 대한 배율 인수 입니다.</target>
        </trans-unit>
        <trans-unit id="14cf9a30a86cd6abc4768ba63268809b6a642a72" translate="yes" xml:space="preserve">
          <source>Optionally set the Torch Hub directory used to save downloaded models &amp;amp; weights.</source>
          <target state="translated">다운로드 한 모델 및 무게를 저장하는 데 사용되는 Torch Hub 디렉토리를 선택적으로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="3cdb2fb789b07586dcdc33ff9c3ec35d1394e04a" translate="yes" xml:space="preserve">
          <source>Optionally, you can give non-equal weighting on the classes by passing a 1D &lt;code&gt;weight&lt;/code&gt; tensor into the constructor.</source>
          <target state="translated">선택적 으로 생성자에 1D &lt;code&gt;weight&lt;/code&gt; 텐서를 전달하여 클래스에 동일하지 않은 가중치를 부여 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="657998dbdb24b52285f38e822b9084533640dfff" translate="yes" xml:space="preserve">
          <source>Ordinarily, &amp;ldquo;automatic mixed precision training&amp;rdquo; uses &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;torch.cuda.amp.autocast&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;torch.cuda.amp.GradScaler&lt;/code&gt;&lt;/a&gt; together, as shown in the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;Automatic Mixed Precision examples&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;Automatic Mixed Precision recipe&lt;/a&gt;. However, &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;autocast&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;GradScaler&lt;/code&gt;&lt;/a&gt; are modular, and may be used separately if desired.</source>
          <target state="translated">일반적으로 '자동 혼합 정밀도 훈련'은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;자동 혼합 정밀도 예제&lt;/a&gt; 및 &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;자동 혼합 정밀도 레시피에&lt;/a&gt; 표시된대로 &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt; &lt;code&gt;torch.cuda.amp.autocast&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;torch.cuda.amp.GradScaler&lt;/code&gt; 를&lt;/a&gt; 함께 사용합니다 . 그러나, &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt; &lt;code&gt;autocast&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;GradScaler&lt;/code&gt; 은&lt;/a&gt; 모듈러이고, 원하는 경우 별도로 사용할 수있다.</target>
        </trans-unit>
        <trans-unit id="ae88acefda9740ca025e9b1e8d82500694df6312" translate="yes" xml:space="preserve">
          <source>Orphan</source>
          <target state="translated">Orphan</target>
        </trans-unit>
        <trans-unit id="6e6a6f2086bb5fe5dbfd17d8d5f502d48759834b" translate="yes" xml:space="preserve">
          <source>Other</source>
          <target state="translated">Other</target>
        </trans-unit>
        <trans-unit id="d51aeaedae53d1689ff4812f653d88f9f69f9530" translate="yes" xml:space="preserve">
          <source>Other NCCL environment variables</source>
          <target state="translated">기타 NCCL 환경 변수</target>
        </trans-unit>
        <trans-unit id="b41b4ea22c0549444f4374445d8b5be41ed6c7a3" translate="yes" xml:space="preserve">
          <source>Other Operations</source>
          <target state="translated">기타 작업</target>
        </trans-unit>
        <trans-unit id="b4fb6252944f2406950d09755e09e265942a94cb" translate="yes" xml:space="preserve">
          <source>Other dimensions of &lt;code&gt;input&lt;/code&gt; that are not explicitly moved remain in their original order and appear at the positions not specified in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="translated">명시 적으로 이동되지 않은 &lt;code&gt;input&lt;/code&gt; 다른 차원은 원래 순서로 유지되고 &lt;code&gt;destination&lt;/code&gt; 지정되지 않은 위치에 나타납니다 .</target>
        </trans-unit>
        <trans-unit id="48ae709d89b00cce606c898dd9a79282ec9c0bb8" translate="yes" xml:space="preserve">
          <source>Otherwise, &lt;code&gt;.grad&lt;/code&gt; is created with rowmajor-contiguous strides.</source>
          <target state="translated">그렇지 않으면 &lt;code&gt;.grad&lt;/code&gt; 는 rowmajor-contiguous strides로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="9b1d762f3a800a1e6b0f118d4aa509412d043de1" translate="yes" xml:space="preserve">
          <source>Otherwise, if &lt;code&gt;map_location&lt;/code&gt; is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).</source>
          <target state="translated">그렇지 않으면 &lt;code&gt;map_location&lt;/code&gt; 이 dict이면 파일 (키)에 나타나는 위치 태그를 저장소 (값)를 넣을 위치를 지정하는 태그로 다시 매핑하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9f6ea88f9397a13956243e283a62e08c4833e59a" translate="yes" xml:space="preserve">
          <source>Otherwise, it will not be possible to view &lt;code&gt;self&lt;/code&gt; tensor as &lt;code&gt;shape&lt;/code&gt; without copying it (e.g., via &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;). When it is unclear whether a &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; can be performed, it is advisable to use &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which returns a view if the shapes are compatible, and copies (equivalent to calling &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;) otherwise.</source>
          <target state="translated">그렇지 않으면 &lt;code&gt;self&lt;/code&gt; 텐서를 복사하지 않고 &lt;code&gt;shape&lt;/code&gt; 으로 볼 수 없습니다 (예 : &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt; 를 통해 ). &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 를 수행 할 수 있는지 여부가 확실하지 않은 경우 모양이 호환되는 경우 뷰를 반환하고 그렇지 않으면 복사 ( &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt; 를 호출하는 것과 동일 &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 하는 reshape () 를 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="5cbc6561d27e47c3ea5b5fa6d9459eb9629b7617" translate="yes" xml:space="preserve">
          <source>Otherwise:</source>
          <target state="translated">Otherwise:</target>
        </trans-unit>
        <trans-unit id="d386f5cc6857d8a2d7baf372e44b4d1117dfed1a" translate="yes" xml:space="preserve">
          <source>Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.</source>
          <target state="translated">우리의 해결책은 BCELoss가 로그 함수 출력을 -100보다 크거나 같도록 클램프하는 것입니다. 이런 식으로 우리는 항상 유한 손실 값과 선형 역방향 방법을 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02600672b80ab0a1c4955d9a3e0f68bbce6d3eed" translate="yes" xml:space="preserve">
          <source>Our sparse tensor format permits &lt;em&gt;uncoalesced&lt;/em&gt; sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. Uncoalesced tensors permit us to implement certain operators more efficiently.</source>
          <target state="translated">우리의 희소 텐서 형식 은 인덱스에 중복 좌표가있을 수있는 &lt;em&gt;통합되지 않은&lt;/em&gt; 희소 텐서를 허용 합니다. 이 경우 해당 인덱스의 값은 모든 중복 값 항목의 합계라는 의미입니다. 통합되지 않은 텐서는 특정 연산자를보다 효율적으로 구현할 수 있도록합니다.</target>
        </trans-unit>
        <trans-unit id="48c4c6b206793564023af6432f79ea36266139f9" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="99a0b833417509afcf74b89d81969fd3e6454e0f" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="f02f9ec70b467240dbc6d49d1fba7b03c1442801" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="5a8086fbdf63aa63eb1e982f564377b659f098f0" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt;&lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt; &lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="73076ad1c2f9be4b4d38d73b62e6c3e6c045bec2" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt;&lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt; &lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="e013688ad79fc825d3497306dc29bb552d0e34c4" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="725700a259ff1b2b017f60b4e74698aa49714d29" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="6398fea054e2780d871397e980d3cf8c7e956e4c" translate="yes" xml:space="preserve">
          <source>Out-place version of &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;index_put_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;index_put_()&lt;/code&gt; &lt;/a&gt; 의 외부 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="33c36eb13432b5622fd7de9ba502b6b4e6d48ded" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 의 외적 .</target>
        </trans-unit>
        <trans-unit id="615e0188218c976163bf2ec4e7c83bb336305665" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;. If &lt;code&gt;input&lt;/code&gt; is a vector of size</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 의 외적 . &lt;code&gt;input&lt;/code&gt; 이 크기 벡터 인 경우</target>
        </trans-unit>
        <trans-unit id="67a0062a0687696c335a2c7d0e459405eba90f74" translate="yes" xml:space="preserve">
          <source>Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; contains the all_gather result that resides on the GPU of &lt;code&gt;input_tensor_list[i]&lt;/code&gt;.</source>
          <target state="translated">출력 목록. 집합의 출력에 사용할 각 GPU에 올바른 크기의 텐서를 포함해야합니다. 예를 들어 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 에는 &lt;code&gt;input_tensor_list[i]&lt;/code&gt; 의 GPU에 상주하는 all_gather 결과가 포함 됩니다.</target>
        </trans-unit>
        <trans-unit id="1b572830da9a955103a98dbcea6dc96cd3c1bf56" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;function&lt;/code&gt; on &lt;code&gt;*args&lt;/code&gt;</source>
          <target state="translated">실행의 출력 &lt;code&gt;function&lt;/code&gt; 에 &lt;code&gt;*args&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90de97722219af193315e02c2b185d1c1a5a4186" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;functions&lt;/code&gt; sequentially on &lt;code&gt;*inputs&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;*inputs&lt;/code&gt; 순차적으로 실행되는 &lt;code&gt;functions&lt;/code&gt; 출력</target>
        </trans-unit>
        <trans-unit id="613a57f34002b7db7e7eb40fb879941fd5f0eb57" translate="yes" xml:space="preserve">
          <source>Output shape: &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">출력 모양 : &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f6a2e936babf4473798688e71160bef838804c5c" translate="yes" xml:space="preserve">
          <source>Output tensors (on different GPUs) to receive the result of the operation.</source>
          <target state="translated">연산 결과를 받기 위해 텐서를 출력합니다 (다른 GPU에서).</target>
        </trans-unit>
        <trans-unit id="be4dd5eb977c617cc4374f35ce4dcfe3424e6c52" translate="yes" xml:space="preserve">
          <source>Output1:</source>
          <target state="translated">Output1:</target>
        </trans-unit>
        <trans-unit id="099f2ae15e5d444369fa820ff7e04ecd1eccb38d" translate="yes" xml:space="preserve">
          <source>Output2:</source>
          <target state="translated">Output2:</target>
        </trans-unit>
        <trans-unit id="f3c8c95c5e534bcd2ea0034a0d83177efa6923f4" translate="yes" xml:space="preserve">
          <source>Output:</source>
          <target state="translated">Output:</target>
        </trans-unit>
        <trans-unit id="8063b576f684099c293104f0277e7a1383d61e53" translate="yes" xml:space="preserve">
          <source>Output: &lt;code&gt;(*, embedding_dim)&lt;/code&gt;, where &lt;code&gt;*&lt;/code&gt; is the input shape</source>
          <target state="translated">출력 : &lt;code&gt;(*, embedding_dim)&lt;/code&gt; , 여기서 &lt;code&gt;*&lt;/code&gt; 는 입력 모양입니다.</target>
        </trans-unit>
        <trans-unit id="7b14dfe86f44f01989354331a43a653a7f8d7f72" translate="yes" xml:space="preserve">
          <source>Output: A Tensor of shape</source>
          <target state="translated">출력 : 형태의 텐서</target>
        </trans-unit>
        <trans-unit id="4a4fbbd55c6c3ccbf6bf7220f45ac1790ff9bb33" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If :attr:&lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 기본적으로 스칼라. : attr : &lt;code&gt;reduction&lt;/code&gt; 이 &lt;code&gt;'none'&lt;/code&gt; 인 경우</target>
        </trans-unit>
        <trans-unit id="7166320ecb31bd6a3586040c60443c5f16f96651" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 기본적으로 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 다음,</target>
        </trans-unit>
        <trans-unit id="88a34ee124c80fdb97be4c7994066abce0b11c8e" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 다음,</target>
        </trans-unit>
        <trans-unit id="816914c299df40485ecf46a1e795a1eb404d91d8" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then same shape as the input</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 입력으로하고 동일한 형상을</target>
        </trans-unit>
        <trans-unit id="573c4c1298793f227765c23e85deebaee6066af6" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then the same size as the target:</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; , 대상으로 다음 같은 크기는 :</target>
        </trans-unit>
        <trans-unit id="6ee76fa98a639a0ac269010b7032639b0a171647" translate="yes" xml:space="preserve">
          <source>Outputs:</source>
          <target state="translated">Outputs:</target>
        </trans-unit>
        <trans-unit id="19620445bec40ebeb61b840841c0effef04f34be" translate="yes" xml:space="preserve">
          <source>Outputs: (h_1, c_1)</source>
          <target state="translated">출력 : (h_1, c_1)</target>
        </trans-unit>
        <trans-unit id="9b4d142414484eb98f10e629a2f06df85748b811" translate="yes" xml:space="preserve">
          <source>Outputs: h&amp;rsquo;</source>
          <target state="translated">출력 : h '</target>
        </trans-unit>
        <trans-unit id="511c97e64880de564b0676893af9fcc8a480a2e2" translate="yes" xml:space="preserve">
          <source>Outputs: output, (h_n, c_n)</source>
          <target state="translated">출력 : 출력, (h_n, c_n)</target>
        </trans-unit>
        <trans-unit id="a459eb30ab99f3ae5b662cb99c1f4215b146179c" translate="yes" xml:space="preserve">
          <source>Outputs: output, h_n</source>
          <target state="translated">출력 : output, h_n</target>
        </trans-unit>
        <trans-unit id="3454926b31857082d753c8156d0ffd5035b9d6b1" translate="yes" xml:space="preserve">
          <source>Overloaded function.</source>
          <target state="translated">과부하 된 기능.</target>
        </trans-unit>
        <trans-unit id="23672df91d237894c7bd8c7afdec0a1cd14fb6f6" translate="yes" xml:space="preserve">
          <source>Owner Share RRef with User</source>
          <target state="translated">사용자와 소유자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="511993d3c99719e38a6779073019dacd7178ddb9" translate="yes" xml:space="preserve">
          <source>P</source>
          <target state="translated">P</target>
        </trans-unit>
        <trans-unit id="4a6bac0b7fcbb3ba201bd5676355ccb5034a278a" translate="yes" xml:space="preserve">
          <source>P(x) = \dfrac{1}{\text{to} - \text{from}}</source>
          <target state="translated">P (x) = \ dfrac {1} {\ text {to}-\ text {from}}</target>
        </trans-unit>
        <trans-unit id="964bd5656c7a016ef0c9ada7382da7ca1fee27a4" translate="yes" xml:space="preserve">
          <source>PRODUCT</source>
          <target state="translated">PRODUCT</target>
        </trans-unit>
        <trans-unit id="768bfcbbf6df966e333cf0230ff5aeca88678ff9" translate="yes" xml:space="preserve">
          <source>PReLU</source>
          <target state="translated">PReLU</target>
        </trans-unit>
        <trans-unit id="c6672b1c1e9f2629ba013dde01df96129b495f92" translate="yes" xml:space="preserve">
          <source>PackedSequence</source>
          <target state="translated">PackedSequence</target>
        </trans-unit>
        <trans-unit id="c0577f7163ed4ff21523cd27bc5236f92de452c9" translate="yes" xml:space="preserve">
          <source>Packs a Tensor containing padded sequences of variable length.</source>
          <target state="translated">패딩 처리 된 가변 길이 시퀀스를 포함하는 Tensor를 압축합니다.</target>
        </trans-unit>
        <trans-unit id="65d415f11a63c4c34ebb5de81f2e889c9328e53e" translate="yes" xml:space="preserve">
          <source>Packs a list of variable length Tensors</source>
          <target state="translated">가변 길이 Tensor 목록을 압축합니다.</target>
        </trans-unit>
        <trans-unit id="019020b0e5b242a0cc1a8528059f3ad86084afa9" translate="yes" xml:space="preserve">
          <source>Pad a list of variable length Tensors with &lt;code&gt;padding_value&lt;/code&gt;</source>
          <target state="translated">가변 길이 텐서 목록을 &lt;code&gt;padding_value&lt;/code&gt; 로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="2560c7ddae8fa20bff911361bd8035aef865bce1" translate="yes" xml:space="preserve">
          <source>Padding Layers</source>
          <target state="translated">패딩 레이어</target>
        </trans-unit>
        <trans-unit id="119765b1620180622c6f6bb53002149703be9254" translate="yes" xml:space="preserve">
          <source>Padding mode:</source>
          <target state="translated">패딩 모드 :</target>
        </trans-unit>
        <trans-unit id="c28c7b4763bce482c5c0e969cee27313a7b7a4b3" translate="yes" xml:space="preserve">
          <source>Padding size:</source>
          <target state="translated">패딩 크기 :</target>
        </trans-unit>
        <trans-unit id="01c1aebd02a3849d24632e98070e328ff731c754" translate="yes" xml:space="preserve">
          <source>Pads a packed batch of variable length sequences.</source>
          <target state="translated">가변 길이 시퀀스의 패킹 된 배치를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="5f2ca3b7c48057f8a6249de795ad4677ba74df59" translate="yes" xml:space="preserve">
          <source>Pads tensor.</source>
          <target state="translated">패드 텐서.</target>
        </trans-unit>
        <trans-unit id="340e6f3d3040c075fcd53240ac5a1540bc713e60" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with a constant value.</source>
          <target state="translated">입력 텐서 경계를 상수 값으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="66887cd786c809b7e475e385b383407a6d7e005a" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with zero.</source>
          <target state="translated">입력 텐서 경계를 0으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="15f49157ae62175a9c8bfe59b0dd171af5f13c80" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using replication of the input boundary.</source>
          <target state="translated">입력 경계의 복제를 사용하여 입력 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="2095464e9318d466281e389dabe7d857b2607c1d" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using the reflection of the input boundary.</source>
          <target state="translated">입력 경계의 반사를 사용하여 입력 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="9b861161343780fb206bb4b2ff66ca3fba1cab61" translate="yes" xml:space="preserve">
          <source>PairwiseDistance</source>
          <target state="translated">PairwiseDistance</target>
        </trans-unit>
        <trans-unit id="1ad9f67d0f855f646efb3775c7a4778a3cc5a138" translate="yes" xml:space="preserve">
          <source>Parallelism</source>
          <target state="translated">Parallelism</target>
        </trans-unit>
        <trans-unit id="f699f295e5ae4ac633cfa18437fed38d028b3fdb" translate="yes" xml:space="preserve">
          <source>Parameter</source>
          <target state="translated">Parameter</target>
        </trans-unit>
        <trans-unit id="cb5ac90fcc6e04cb5c8cae65265e51f0ab35a0b7" translate="yes" xml:space="preserve">
          <source>Parameter names except the first must EXACTLY match the names in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">첫 번째를 제외한 매개 변수 이름은 &lt;code&gt;forward&lt;/code&gt; 의 이름과 정확히 일치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="4891fc0302ab937e6c97e91f093534d4afb88b1e" translate="yes" xml:space="preserve">
          <source>Parameter ordering does NOT necessarily match what is in &lt;code&gt;VariableType.h&lt;/code&gt;, tensors (inputs) are always first, then non-tensor arguments.</source>
          <target state="translated">매개 변수 순서가 &lt;code&gt;VariableType.h&lt;/code&gt; 에 있는 것과 반드시 ​​일치하는 것은 아닙니다 . 텐서 (입력)는 항상 먼저, 그다음에는 비 텐서 인수입니다.</target>
        </trans-unit>
        <trans-unit id="88be3e0f29cc772e39dc2a268a55384b430d8f22" translate="yes" xml:space="preserve">
          <source>ParameterDict</source>
          <target state="translated">ParameterDict</target>
        </trans-unit>
        <trans-unit id="63777906465127a1bb8f4e009ca405717880743f" translate="yes" xml:space="preserve">
          <source>ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods.</source>
          <target state="translated">ParameterDict는 일반 Python 사전처럼 인덱싱 할 수 있지만 여기에 포함 된 매개 변수는 제대로 등록되어 있으며 모든 Module 메서드에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="acdf35da4c553e1dfc2210109486c37c589cbf86" translate="yes" xml:space="preserve">
          <source>ParameterList</source>
          <target state="translated">ParameterList</target>
        </trans-unit>
        <trans-unit id="a975eea30db9fa05003e3b5097688bd49ec7e01b" translate="yes" xml:space="preserve">
          <source>Parameters</source>
          <target state="translated">Parameters</target>
        </trans-unit>
        <trans-unit id="b2ef1af91a76a3fb94590270b4864284563ecce3" translate="yes" xml:space="preserve">
          <source>Parameters are &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; subclasses, that have a very special property when used with &lt;code&gt;Module&lt;/code&gt; s - when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in &lt;code&gt;parameters()&lt;/code&gt; iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as &lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt;&lt;code&gt;Parameter&lt;/code&gt;&lt;/a&gt;, these temporaries would get registered too.</source>
          <target state="translated">매개 변수는 &lt;code&gt;Module&lt;/code&gt; 과 함께 사용할 때 매우 특별한 속성을 갖는 &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 하위 클래스입니다. 모듈 속성으로 할당되면 자동으로 매개 변수 목록에 추가되고 &lt;code&gt;parameters()&lt;/code&gt; 반복자 에 나타납니다 . Tensor를 할당하는 것은 그러한 효과가 없습니다. 이는 모델에서 RNN의 마지막 숨겨진 상태와 같은 일부 임시 상태를 캐시하려고 할 수 있기 때문입니다. &lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt; &lt;code&gt;Parameter&lt;/code&gt; &lt;/a&gt; 와 같은 클래스가 없으면 이러한 임시도 등록됩니다.</target>
        </trans-unit>
        <trans-unit id="3df6923b905b81224f8a63a634516931831a9327" translate="yes" xml:space="preserve">
          <source>Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.</source>
          <target state="translated">매개 변수는 프로세스간에 브로드 캐스트되지 않습니다. 모듈은 기울기에 대해 전체 축소 단계를 수행하고 모든 프로세스에서 동일한 방식으로 최적화 프로그램에 의해 수정 될 것이라고 가정합니다. 버퍼 (예 : BatchNorm 통계)는 랭크 0 프로세스의 모듈에서 매 반복마다 시스템의 다른 모든 복제본으로 브로드 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="794cc4ffc58cd8aa6cf549921b36b20e1749013e" translate="yes" xml:space="preserve">
          <source>Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don&amp;rsquo;t satisfy those properties are sets and iterators over values of dictionaries.</source>
          <target state="translated">매개 변수는 실행간에 일관된 결정적 순서가있는 컬렉션으로 지정되어야합니다. 이러한 속성을 충족하지 않는 개체의 예로는 사전 값에 대한 집합 및 반복기가 있습니다.</target>
        </trans-unit>
        <trans-unit id="63ed4981dcd5565ccc67fe6943dae94af81704fd" translate="yes" xml:space="preserve">
          <source>Pareto</source>
          <target state="translated">Pareto</target>
        </trans-unit>
        <trans-unit id="a70edfd33f03d14b9304a9ca1fe474eaf0817702" translate="yes" xml:space="preserve">
          <source>Parsing the local_rank argument</source>
          <target state="translated">local_rank 인수 구문 분석</target>
        </trans-unit>
        <trans-unit id="48a79b6f6f692b43d8485194d95ec3b0c6abe1d9" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layer.</source>
          <target state="translated">인코더 레이어를 통해 입력을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="74a6cfbb8f181d4db771aa9745afb6080e4ee4fe" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layers in turn.</source>
          <target state="translated">입력을 인코더 레이어를 통해 차례로 전달합니다.</target>
        </trans-unit>
        <trans-unit id="4d0cc1f642a2a736b4718ae462ea2ac809ed76b7" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer in turn.</source>
          <target state="translated">입력 (및 마스크)을 디코더 계층을 통해 차례로 전달합니다.</target>
        </trans-unit>
        <trans-unit id="8c65d8ab6bdf0e2280cb1ffcf102aefaac492303" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer.</source>
          <target state="translated">디코더 레이어를 통해 입력 (및 마스크)을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="fd97c67f36d1e3c87f8ab45030bd4f215e7035cc" translate="yes" xml:space="preserve">
          <source>Passing -1 as the size for a dimension means not changing the size of that dimension.</source>
          <target state="translated">차원의 크기로 -1을 전달하면 해당 차원의 크기가 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1ade9bd239bdf873c59edb2cc4d6f0dd2841681d" translate="yes" xml:space="preserve">
          <source>Passing &lt;code&gt;new_scale&lt;/code&gt; sets the scale directly.</source>
          <target state="translated">&lt;code&gt;new_scale&lt;/code&gt; 을 전달 하면 배율이 직접 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="6f36a4168bfec1245f47f9820a4b54d62ffb6bc8" translate="yes" xml:space="preserve">
          <source>Pathwise derivative</source>
          <target state="translated">경로 적 도함수</target>
        </trans-unit>
        <trans-unit id="ea3c8179bd96f54267bf75b132502b2bb1730b13" translate="yes" xml:space="preserve">
          <source>Pattern Matching Assignments</source>
          <target state="translated">패턴 매칭 할당</target>
        </trans-unit>
        <trans-unit id="246efe0491937cc86746284b090916b8754123a3" translate="yes" xml:space="preserve">
          <source>Per-parameter options</source>
          <target state="translated">매개 변수 별 옵션</target>
        </trans-unit>
        <trans-unit id="bbb4559fe386a24723c5dad9bdcdef77441cfdce" translate="yes" xml:space="preserve">
          <source>Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If &lt;code&gt;graceful=True&lt;/code&gt;, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if &lt;code&gt;graceful=False&lt;/code&gt;, this is a local shutdown, and it does not wait for other RPC processes to reach this method.</source>
          <target state="translated">RPC 에이전트를 종료 한 다음 RPC 에이전트를 제거합니다. 이렇게하면 로컬 에이전트가 미해결 요청을 수락하지 않고 모든 RPC 스레드를 종료하여 RPC 프레임 워크를 종료합니다. 경우에 &lt;code&gt;graceful=True&lt;/code&gt; ,이 모든 로컬 및 원격 RPC 프로세스가이 방법을 도달 할 때까지 차단하고 완전한 모든 뛰어난 작품을 기다립니다. 그렇지 않으면 &lt;code&gt;graceful=False&lt;/code&gt; 이면 로컬 종료이며 다른 RPC 프로세스가이 메서드에 도달 할 때까지 기다리지 않습니다.</target>
        </trans-unit>
        <trans-unit id="000d790b0603ede49c70b8206287b63e59d6ce01" translate="yes" xml:space="preserve">
          <source>Performs</source>
          <target state="translated">Performs</target>
        </trans-unit>
        <trans-unit id="ab6a68d7b5ed065f1d2b1e10ab32e7ca4fa89132" translate="yes" xml:space="preserve">
          <source>Performs Tensor dtype and/or device conversion. A &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; are inferred from the arguments of &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">Tensor dtype 및 / 또는 장치 변환을 수행합니다. &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; 가&lt;/a&gt; 의 인수에서 추론 &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="148ba906f736c7be8f3690226a95e0fc3751c601" translate="yes" xml:space="preserve">
          <source>Performs a &amp;ldquo;true&amp;rdquo; division like Python 3. See &lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt;&lt;code&gt;torch.floor_divide()&lt;/code&gt;&lt;/a&gt; for floor division.</source>
          <target state="translated">Python 3과 같은 &quot;진정한&quot;분할을 수행합니다 . 바닥 분할 은 &lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt; &lt;code&gt;torch.floor_divide()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ca1d45c075edfbcf89b6d16ec3eabd49d65b9ace" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에서 행렬의 배치 행렬-행렬 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="1d3416bea00d8ee005b5f0e7621baf85cbbbc8e4" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">&lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에서 행렬의 배치 행렬-행렬 곱을 수행합니다 . &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="6629b05e96ef663c5b3cf02ed167a20014b205ec" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</source>
          <target state="translated">감소 된 추가 단계를 사용하여 &lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 (모든 행렬 곱셈이 첫 번째 차원을 따라 누적 됨).</target>
        </trans-unit>
        <trans-unit id="1710424b09b25bd3fe48dce1c221f445326275f1" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension). &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">감소 된 추가 단계를 사용하여 &lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 (모든 행렬 곱셈이 첫 번째 차원을 따라 누적 됨). &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="6860592c3b78f43f0a520489d41db750bddd29e5" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mat2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="df9a63e54344ddcdcb571d3c78f6a26c0d733eac" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="e06b6aa87fc19869b8dd410c826fba4f794df727" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;mat1&lt;/code&gt; 및 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="bc47b86c1cbb06be939cda2d15d03119065f2793" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;. The matrix &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">행렬 &lt;code&gt;mat1&lt;/code&gt; 및 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 . 행렬 &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="ec815028dc3febded0f07da8c4424505ed14884e" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the sparse matrix &lt;code&gt;mat1&lt;/code&gt; and dense matrix &lt;code&gt;mat2&lt;/code&gt;. Similar to &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt;&lt;code&gt;torch.mm()&lt;/code&gt;&lt;/a&gt;, If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="translated">희소 행렬 &lt;code&gt;mat1&lt;/code&gt; 과 조밀 행렬 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 . 유사 &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt; &lt;code&gt;torch.mm()&lt;/code&gt; &lt;/a&gt; , 경우 &lt;code&gt;mat1&lt;/code&gt; A는</target>
        </trans-unit>
        <trans-unit id="0fcbec8807d0db14810ce48d50247513762338dc" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;input&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;input&lt;/code&gt; 과 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="fd688cf71edd9657ad9f3dba62a61d9bb7db410e" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;mat&lt;/code&gt; 와 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="0d3f478da414f374637a9f8efee9102a8950a0aa" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;. The vector &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">행렬 &lt;code&gt;mat&lt;/code&gt; 와 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 . 벡터 &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="0efe20233a22c5777e16f0f9f23f0594b1694e45" translate="yes" xml:space="preserve">
          <source>Performs a single optimization step (parameter update).</source>
          <target state="translated">단일 최적화 단계를 수행합니다 (매개 변수 업데이트).</target>
        </trans-unit>
        <trans-unit id="eb4301873be53ea1f5d5d9d23cf8670a4bf294c6" translate="yes" xml:space="preserve">
          <source>Performs a single optimization step.</source>
          <target state="translated">단일 최적화 단계를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="097227fe1294d426da153667fbb89e50d7a799f6" translate="yes" xml:space="preserve">
          <source>Performs dtype and/or device conversion on &lt;code&gt;self.data&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self.data&lt;/code&gt; 에 대해 dtype 및 / 또는 장치 변환을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="50008568c0d385fc9c65f3ac3f0c22f33a0fd7e8" translate="yes" xml:space="preserve">
          <source>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</source>
          <target state="translated">낮은 순위 행렬, 이러한 행렬의 배치 또는 희소 행렬에 대해 선형 PCA (주성분 분석)를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="20ee59ad32c7c62194b8cc1470dab04b5aeb1063" translate="yes" xml:space="preserve">
          <source>Performs the element-wise division of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">수행의 소자 현명한 분할 &lt;code&gt;tensor1&lt;/code&gt; 의해 &lt;code&gt;tensor2&lt;/code&gt; , 스칼라 곱에 의한 결과 &lt;code&gt;value&lt;/code&gt; 과 추가로 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="20dfc56a802418cc6cb883565e29a8149e17cc74" translate="yes" xml:space="preserve">
          <source>Performs the element-wise multiplication of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">수행의 요소 와이즈 곱 &lt;code&gt;tensor1&lt;/code&gt; 의해 &lt;code&gt;tensor2&lt;/code&gt; , 스칼라 곱에 의한 결과 &lt;code&gt;value&lt;/code&gt; 과 추가로 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5b3394929b52d1d2c8a062a547e9e7818f342005" translate="yes" xml:space="preserve">
          <source>Performs the operation.</source>
          <target state="translated">작업을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="b8d3d151e5b759fb242c35ceb1f1a9e038806294" translate="yes" xml:space="preserve">
          <source>Performs the outer-product of vectors &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and adds it to the matrix &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">벡터 &lt;code&gt;vec1&lt;/code&gt; 및 &lt;code&gt;vec2&lt;/code&gt; 의 외적을 수행하고 이를 행렬 &lt;code&gt;input&lt;/code&gt; 추가합니다 .</target>
        </trans-unit>
        <trans-unit id="7775a934f392a3fd32435cb3afba7f94cd7e9125" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the dimension order in the &lt;code&gt;other&lt;/code&gt; tensor, adding size-one dims for any new names.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; 텐서 의 차원 순서와 일치 하도록 &lt;code&gt;self&lt;/code&gt; 텐서의 차원 을 변경하여 새 이름에 대해 크기 1을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="172d57479053b5b2d1caa6f27506a79abfa3d89b" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the order specified in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;, adding size-one dims for any new names.</source>
          <target state="translated">순서를 무작위로 바꾸어 넣은의 크기 &lt;code&gt;self&lt;/code&gt; 텐서에 지정된 순서에 맞게 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 새로운 이름을 크기 하나 희미을 추가.</target>
        </trans-unit>
        <trans-unit id="89073f3c192cb17be2601e291aa1534b084f3029" translate="yes" xml:space="preserve">
          <source>PixelShuffle</source>
          <target state="translated">PixelShuffle</target>
        </trans-unit>
        <trans-unit id="c026434074e7feb786c10d360bdf80aa00a3000a" translate="yes" xml:space="preserve">
          <source>Platform-specific behaviors</source>
          <target state="translated">플랫폼 별 동작</target>
        </trans-unit>
        <trans-unit id="46d351806d607c75c00d9e7addd52a82379f8961" translate="yes" xml:space="preserve">
          <source>Please checkout &lt;a href=&quot;#tracing-vs-scripting&quot;&gt;Tracing vs Scripting&lt;/a&gt;.</source>
          <target state="translated">체크 아웃하십시오 &lt;a href=&quot;#tracing-vs-scripting&quot;&gt;스크립팅 대 추적을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="77243e0c917ac356b20b9bd68dd15bf839ebd531" translate="yes" xml:space="preserve">
          <source>Please ensure that &lt;code&gt;device_ids&lt;/code&gt; argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the &lt;code&gt;device_ids&lt;/code&gt; needs to be &lt;code&gt;[args.local_rank]&lt;/code&gt;, and &lt;code&gt;output_device&lt;/code&gt; needs to be &lt;code&gt;args.local_rank&lt;/code&gt; in order to use this utility</source>
          <target state="translated">&lt;code&gt;device_ids&lt;/code&gt; 인수가 코드가 작동 할 유일한 GPU 기기 ID로 설정되어 있는지 확인하세요 . 이것은 일반적으로 프로세스의 로컬 순위입니다. 즉, 이 유틸리티를 사용 하려면 &lt;code&gt;device_ids&lt;/code&gt; 는 &lt;code&gt;[args.local_rank]&lt;/code&gt; 이고 &lt;code&gt;output_device&lt;/code&gt; 는 &lt;code&gt;args.local_rank&lt;/code&gt; 여야 합니다.</target>
        </trans-unit>
        <trans-unit id="489042bcd064b62307292513642ef8adc6ce3919" translate="yes" xml:space="preserve">
          <source>Please refer to &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed Overview&lt;/a&gt; for a brief introduction to all features related to distributed training.</source>
          <target state="translated">분산 교육과 관련된 모든 기능에 대한 간략한 소개는 &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch 분산 개요&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="17832db8653d0843df90a5d67009e3924ef34997" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;expand&lt;/code&gt;.</source>
          <target state="translated">참조하십시오 &lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용은 &lt;code&gt;expand&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5ec18ad55b74e4a4762c499e962c30b802cc51eb" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;view&lt;/code&gt;.</source>
          <target state="translated">참조하십시오 &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용 &lt;code&gt;view&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6ce3ed1431a0c41a218668fa1492f351b3c1863d" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;reshape&lt;/code&gt;.</source>
          <target state="translated">참조하시기 바랍니다 &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용은 &lt;code&gt;reshape&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="037ad39d2570fc5f7861f69bb41699739bb0d316" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt; for more documentation on ReLU.</source>
          <target state="translated">ReLU에 대한 자세한 문서는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4353aa1de0244f4e4d17336cf0860de21b27bcfd" translate="yes" xml:space="preserve">
          <source>Point-to-point communication</source>
          <target state="translated">지점 간 통신</target>
        </trans-unit>
        <trans-unit id="f2954403f1437b0d34c50ea2ba4d35acd2ad8ef0" translate="yes" xml:space="preserve">
          <source>Pointwise Ops</source>
          <target state="translated">Pointwise Ops</target>
        </trans-unit>
        <trans-unit id="7f6c43edfe5b4aaaf229cadb3d90c7f7c42e39d6" translate="yes" xml:space="preserve">
          <source>Poisson</source>
          <target state="translated">Poisson</target>
        </trans-unit>
        <trans-unit id="fa7dfe92888800753b97007c7d56433a35b2c1ff" translate="yes" xml:space="preserve">
          <source>Poisson negative log likelihood loss.</source>
          <target state="translated">포아송 음의 로그 우도 손실입니다.</target>
        </trans-unit>
        <trans-unit id="080ef63a7d510bdbbbb0c01b547bfd9466aafda2" translate="yes" xml:space="preserve">
          <source>PoissonNLLLoss</source>
          <target state="translated">PoissonNLLLoss</target>
        </trans-unit>
        <trans-unit id="8774a7b82c80db34246fbe30ad918e5ef44c7d79" translate="yes" xml:space="preserve">
          <source>Pool type:</source>
          <target state="translated">수영장 유형 :</target>
        </trans-unit>
        <trans-unit id="58ef016bfefe7d0fa87dbd8a9397011926b50b05" translate="yes" xml:space="preserve">
          <source>Pooling functions</source>
          <target state="translated">풀링 기능</target>
        </trans-unit>
        <trans-unit id="4f14f8bf99ac13b99d1b436dd62fcdf8537ddee0" translate="yes" xml:space="preserve">
          <source>Pooling layers</source>
          <target state="translated">풀링 레이어</target>
        </trans-unit>
        <trans-unit id="0b3306afd19e4390fd2c7bf86d987f82d98b6188" translate="yes" xml:space="preserve">
          <source>Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.</source>
          <target state="translated">중첩 된 범위 범위 스택에서 범위를 제거합니다. 종료 된 범위의 0부터 시작하는 깊이를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7280aa9b126e519bb5acdcab7417841a30bb930a" translate="yes" xml:space="preserve">
          <source>Possible values are:</source>
          <target state="translated">가능한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1774bc5c596634bc97c87b3b60bbe553b38e8300" translate="yes" xml:space="preserve">
          <source>Prefer &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; over &lt;code&gt;binary_cross_entropy&lt;/code&gt;</source>
          <target state="translated">선호 &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; 이상 &lt;code&gt;binary_cross_entropy&lt;/code&gt; 을</target>
        </trans-unit>
        <trans-unit id="1167b0995920e6f31e81abce0609fca20a13f6e0" translate="yes" xml:space="preserve">
          <source>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</source>
          <target state="translated">양자화 교정 또는 양자화 인식 학습을 위해 모델 사본을 준비하고이를 양자화 된 버전으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="17bb7ea4855300bdce06f568a4c1e23392bcdee5" translate="yes" xml:space="preserve">
          <source>Prepares a copy of the model for quantization calibration or quantization-aware training.</source>
          <target state="translated">양자화 교정 또는 양자화 인식 학습을 위해 모델 사본을 준비합니다.</target>
        </trans-unit>
        <trans-unit id="3c85cac74e37970ecc55d37819ce7376a3e2e8cd" translate="yes" xml:space="preserve">
          <source>Preparing model for quantization</source>
          <target state="translated">양자화를위한 모델 준비</target>
        </trans-unit>
        <trans-unit id="6175db74ea439762cf31ac47bf2800599471e037" translate="yes" xml:space="preserve">
          <source>Pretrained weights can either be stored locally in the github repo, or loadable by &lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt;&lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt;&lt;/a&gt;. If less than 2GB, it&amp;rsquo;s recommended to attach it to a &lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;project release&lt;/a&gt; and use the url from the release. In the example above &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; handles &lt;code&gt;pretrained&lt;/code&gt;, alternatively you can put the following logic in the entrypoint definition.</source>
          <target state="translated">사전 훈련 된 가중치는 github 저장소에 로컬로 저장하거나 &lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt; &lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt; &lt;/a&gt; 로드 할 수 있습니다 . 2GB 미만인 경우 &lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;프로젝트 릴리스&lt;/a&gt; 에 첨부하고 릴리스 의 URL을 사용하는 것이 좋습니다 . 위의 예에서 &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; 의 핸들이 &lt;code&gt;pretrained&lt;/code&gt; , 또는 당신은 엔트리 포인트 정의에 다음과 같은 논리를 넣을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c189714d06cfa7c01d24df029a774f3f30e4981e" translate="yes" xml:space="preserve">
          <source>Primarily used for quantization to float16 which doesn&amp;rsquo;t require determining ranges.</source>
          <target state="translated">주로 범위를 결정할 필요가없는 float16에 대한 양자화에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="fd6f86c9793df0ba1099fc411b3e7ec52a6a1cfe" translate="yes" xml:space="preserve">
          <source>Print Statements</source>
          <target state="translated">명세서 인쇄</target>
        </trans-unit>
        <trans-unit id="fee7456cc782e1f8058fb173d31005a875e61eb8" translate="yes" xml:space="preserve">
          <source>Prints an EventList as a nicely formatted table.</source>
          <target state="translated">EventList를 멋지게 형식화 된 테이블로 인쇄합니다.</target>
        </trans-unit>
        <trans-unit id="06e3b604d03b7cb14c38bef03c1ff6819f9e40ec" translate="yes" xml:space="preserve">
          <source>Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer&amp;rsquo;s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling &lt;code&gt;scheduler.step()&lt;/code&gt;) before the optimizer&amp;rsquo;s update (calling &lt;code&gt;optimizer.step()&lt;/code&gt;), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling &lt;code&gt;scheduler.step()&lt;/code&gt; at the wrong time.</source>
          <target state="translated">PyTorch 1.1.0 이전에는 최적화 프로그램 업데이트 전에 학습률 스케줄러가 호출 될 것으로 예상되었습니다. 1.1.0은 BC를 깨는 방식으로이 동작을 변경했습니다. 최적화 프로그램을 업데이트하기 전에 학습률 스케줄러 ( &lt;code&gt;scheduler.step()&lt;/code&gt; 호출)를 사용하는 경우 ( &lt;code&gt;optimizer.step()&lt;/code&gt; 호출 ) 학습률 일정의 첫 번째 값을 건너 뜁니다. PyTorch 1.1.0으로 업그레이드 한 후 결과를 재현 할 수없는 경우 잘못된 시간에 &lt;code&gt;scheduler.step()&lt;/code&gt; 을 호출하고 있는지 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="7444118490ce8aa2e0fbbbea8ea2ca65e38de32d" translate="yes" xml:space="preserve">
          <source>Probability distributions - torch.distributions</source>
          <target state="translated">확률 분포-torch.distributions</target>
        </trans-unit>
        <trans-unit id="d36d84ef2381c76d6d53fffa7008829b5eaeb1fc" translate="yes" xml:space="preserve">
          <source>Process Group Backend</source>
          <target state="translated">프로세스 그룹 백엔드</target>
        </trans-unit>
        <trans-unit id="40e121339893a371c4a1b2c140642c9b6a34e9bd" translate="yes" xml:space="preserve">
          <source>Produces several warnings and a graph which simply returns the input:</source>
          <target state="translated">몇 가지 경고와 단순히 입력을 반환하는 그래프를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="31da1961bbfc7c4ddeb384d30515cc288fec6568" translate="yes" xml:space="preserve">
          <source>Profiler</source>
          <target state="translated">Profiler</target>
        </trans-unit>
        <trans-unit id="a445fe395c6c6d32b8f6c47c9d18f8e8ae1263f5" translate="yes" xml:space="preserve">
          <source>Profiling RPC-based Workloads</source>
          <target state="translated">RPC 기반 워크로드 프로파일 링</target>
        </trans-unit>
        <trans-unit id="6f204147095385ad851c3a0b168d9ac27a7540b0" translate="yes" xml:space="preserve">
          <source>Promotion Examples:</source>
          <target state="translated">프로모션 예 :</target>
        </trans-unit>
        <trans-unit id="d29f4bd5abe504fd6f4217c42fe5687ce1f0ffba" translate="yes" xml:space="preserve">
          <source>Propagate qconfig through the module hierarchy and assign &lt;code&gt;qconfig&lt;/code&gt; attribute on each leaf module</source>
          <target state="translated">모듈 계층을 통해 qconfig를 전파 하고 각 리프 모듈에 &lt;code&gt;qconfig&lt;/code&gt; 속성을 할당 합니다.</target>
        </trans-unit>
        <trans-unit id="51f45d4a44ddb55152733d7d5ec938ec682205f1" translate="yes" xml:space="preserve">
          <source>Proposed by G. Hinton in his &lt;a href=&quot;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;course&lt;/a&gt;.</source>
          <target state="translated">그의 &lt;a href=&quot;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;과정&lt;/a&gt; 에서 G. Hinton에 의해 제 안됨 .</target>
        </trans-unit>
        <trans-unit id="17a62537e52031c55b875c513ecbb2c241853032" translate="yes" xml:space="preserve">
          <source>Protocol Scenarios</source>
          <target state="translated">프로토콜 시나리오</target>
        </trans-unit>
        <trans-unit id="7a477b80f752e81ae51a71148e25e03d00004834" translate="yes" xml:space="preserve">
          <source>Provides a skeleton for customization requiring the overriding of methods such as &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt;&lt;code&gt;apply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt; &lt;code&gt;apply()&lt;/code&gt; &lt;/a&gt; 와 같은 메서드를 재정의해야하는 사용자 지정을위한 스켈레톤을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="df76ae939d5d588eb6f5766984f7e5d52eb50216" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor at random.</source>
          <target state="translated">무작위로 텐서에서 (현재 정리되지 않은) 단위를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="368202ce04421132a9166203cfc137eb74e9e21c" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</source>
          <target state="translated">L1- 노름이 가장 낮은 단위를 0으로 제거하여 텐서에서 (현재 정리되지 않은) 단위를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="5b5612215ed58f8a7a594acd4880fde95d901f0f" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor at random.</source>
          <target state="translated">무작위로 텐서에서 전체 (현재 제거되지 않은) 채널을 정리합니다.</target>
        </trans-unit>
        <trans-unit id="a802bde3db7f577abbb1b538eee51db0b51ddf30" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.</source>
          <target state="translated">Ln- 노름을 기반으로 텐서에서 전체 (현재 제거되지 않은) 채널을 정리합니다.</target>
        </trans-unit>
        <trans-unit id="2306b2b0d296e475c246073f14ea3e13b1639fc6" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 에서 사전 계산 된 마스크를 적용하여 &lt;code&gt;mask&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8222739be42aac5d12f65e68e036f42e19a41197" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 에서 사전 계산 된 마스크를 적용하여 &lt;code&gt;mask&lt;/code&gt; . 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="12f145dd28935b207164778834f46d05e62d324e" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random.</source>
          <target state="translated">무작위로 선택된 지정된 &lt;code&gt;dim&lt;/code&gt; 을 따라 지정된 &lt;code&gt;amount&lt;/code&gt; 의 (현재 제거되지 않은) 채널을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서 를 제거합니다 .</target>
        </trans-unit>
        <trans-unit id="1993775f18835672e6ea2ee6df5cc4b95f07d89b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">무작위로 선택된 지정된 &lt;code&gt;dim&lt;/code&gt; 을 따라 지정된 &lt;code&gt;amount&lt;/code&gt; 의 (현재 제거되지 않은) 채널을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서 를 제거합니다 . 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="e4cf8aaa8979161ded3d9362964d7e1da3ab5bd2" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 에 &lt;code&gt;module&lt;/code&gt; 지정된 제거하여 &lt;code&gt;amount&lt;/code&gt; 지정된 따라 (현재 unpruned) 채널 &lt;code&gt;dim&lt;/code&gt; 최저 L``n`` 규범으로한다.</target>
        </trans-unit>
        <trans-unit id="e5e671a44c6d84c3fbfdc3856808100dbb577dde" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 에 &lt;code&gt;module&lt;/code&gt; 지정된 제거하여 &lt;code&gt;amount&lt;/code&gt; 지정된 따라 (현재 unpruned) 채널 &lt;code&gt;dim&lt;/code&gt; 최저 L``n`` 규범으로한다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="e414308272708bb6595a21b037cefd9b491135bf" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random.</source>
          <target state="translated">임의로 선택된 (현재 정리되지 않은) 단위 의 지정된 &lt;code&gt;amount&lt;/code&gt; 을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="0d1f0e7025ef360a1a8db642ffde843b0c686100" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">임의로 선택된 (현재 정리되지 않은) 단위 의 지정된 &lt;code&gt;amount&lt;/code&gt; 을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="0e2ae92cb48940de59dcf3dcc3eee3fc2761f74b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm.</source>
          <target state="translated">L1- 노름이 가장 낮은 지정된 &lt;code&gt;amount&lt;/code&gt; (현재 정리되지 않은) 단위를 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="3ceb680b02bb9fc923a4db9b23afd99bcf23bd57" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">L1- 노름이 가장 낮은 지정된 &lt;code&gt;amount&lt;/code&gt; (현재 정리되지 않은) 단위를 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="8ee04a977907bca0b936aebe6227e9be8b0a5084" translate="yes" xml:space="preserve">
          <source>Pruning itself is NOT undone or reversed!</source>
          <target state="translated">가지 치기 자체는 취소되거나 취소되지 않습니다!</target>
        </trans-unit>
        <trans-unit id="fae06ff3f9da62952a413ca3bf10b3455201b027" translate="yes" xml:space="preserve">
          <source>PruningContainer</source>
          <target state="translated">PruningContainer</target>
        </trans-unit>
        <trans-unit id="784e725830e98a189da61d3afccea45b6e5a9d0f" translate="yes" xml:space="preserve">
          <source>Publishing models</source>
          <target state="translated">모델 게시</target>
        </trans-unit>
        <trans-unit id="ae8ea7cada42ba595cd41ec28d2ff26795c6d771" translate="yes" xml:space="preserve">
          <source>Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.</source>
          <target state="translated">범위를 중첩 된 범위 범위 스택으로 푸시합니다. 시작된 범위의 0부터 시작하는 깊이를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="256db3aa08282f6add3cb3226d3554daf2bee937" translate="yes" xml:space="preserve">
          <source>Puts values from the tensor &lt;code&gt;value&lt;/code&gt; into the tensor &lt;code&gt;self&lt;/code&gt; using the indices specified in &lt;a href=&quot;#torch.Tensor.indices&quot;&gt;&lt;code&gt;indices&lt;/code&gt;&lt;/a&gt; (which is a tuple of Tensors). The expression &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; is equivalent to &lt;code&gt;tensor[indices] = value&lt;/code&gt;. Returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">인덱스에 지정된 &lt;a href=&quot;#torch.Tensor.indices&quot;&gt; &lt;code&gt;indices&lt;/code&gt; &lt;/a&gt; ( 텐서 의 튜플)를 사용하여 텐서 &lt;code&gt;value&lt;/code&gt; 을 텐서 &lt;code&gt;self&lt;/code&gt; 에 넣습니다. &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; 표현식 은 &lt;code&gt;tensor[indices] = value&lt;/code&gt; 와 동일 합니다 . &lt;code&gt;self&lt;/code&gt; 를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="c148aded3d47eba6bad059e92b2038d207b0a750" translate="yes" xml:space="preserve">
          <source>Putting it all together</source>
          <target state="translated">함께 모아서</target>
        </trans-unit>
        <trans-unit id="98a2e30eeb85238d061519d74935e170e44c72cb" translate="yes" xml:space="preserve">
          <source>PyTorch</source>
          <target state="translated">PyTorch</target>
        </trans-unit>
        <trans-unit id="2837c35a89ca2ff1375acbd6ae4015154d808597" translate="yes" xml:space="preserve">
          <source>PyTorch Contribution Guide</source>
          <target state="translated">PyTorch 기여 가이드</target>
        </trans-unit>
        <trans-unit id="0de6ec530c1988d0af9e4ce7eb73eef1189976b6" translate="yes" xml:space="preserve">
          <source>PyTorch Functions and Modules</source>
          <target state="translated">PyTorch 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="6517675a341634e77bfecd3ddf631a647d857475" translate="yes" xml:space="preserve">
          <source>PyTorch Governance</source>
          <target state="translated">PyTorch 거버넌스</target>
        </trans-unit>
        <trans-unit id="d35b21e83a6b1a4ee2adc066dff9672eedab98a1" translate="yes" xml:space="preserve">
          <source>PyTorch Governance | Persons of Interest</source>
          <target state="translated">PyTorch 거버넌스 | 관심있는 사람</target>
        </trans-unit>
        <trans-unit id="3a2bd7a8f704e6cdbe374466d0208d799f0651cc" translate="yes" xml:space="preserve">
          <source>PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some numeric differences. Depending on model structure, these differences may be negligible, but they can also cause major divergences in behavior (especially on untrained models.) We allow Caffe2 to call directly to Torch implementations of operators, to help you smooth over these differences when precision is important, and to also document these differences.</source>
          <target state="translated">PyTorch 및 ONNX 백엔드 (Caffe2, ONNX 런타임 등)에는 종종 약간의 숫자 차이가있는 연산자 구현이 있습니다. 모델 구조에 따라 이러한 차이는 무시할 수있을 수 있지만 동작에 큰 차이를 유발할 수도 있습니다 (특히 훈련되지 않은 모델에서). Caffe2가 연산자의 Torch 구현을 직접 호출하여 정밀도가 중요한 경우 이러한 차이를 부드럽게 처리 할 수 ​​있습니다. , 그리고 이러한 차이점을 문서화합니다.</target>
        </trans-unit>
        <trans-unit id="6148268ed386643e6b3a594818c58fc07ad9c665" translate="yes" xml:space="preserve">
          <source>PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)</source>
          <target state="translated">PyTorch 분산 패키지는 Linux (안정), MacOS (안정) 및 Windows (프로토 타입)를 지원합니다. Linux의 경우 기본적으로 Gloo 및 NCCL 백엔드가 빌드되고 배포 된 PyTorch에 포함됩니다 (CUDA로 빌드하는 경우에만 NCCL). MPI는 소스에서 PyTorch를 빌드하는 경우에만 포함될 수있는 선택적 백엔드입니다. (예 : MPI가 설치된 호스트에 PyTorch 빌드)</target>
        </trans-unit>
        <trans-unit id="9456a74dbb38bfce73f52a1992187228bd2d00fa" translate="yes" xml:space="preserve">
          <source>PyTorch documentation</source>
          <target state="translated">PyTorch 문서</target>
        </trans-unit>
        <trans-unit id="d19d474346c5b08d13d3339e3bf4feebcd91f549" translate="yes" xml:space="preserve">
          <source>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</source>
          <target state="translated">PyTorch는 GPU 및 CPU를 사용하는 딥 러닝에 최적화 된 텐서 라이브러리입니다.</target>
        </trans-unit>
        <trans-unit id="899246dc869a567118284070768757e2fc424c84" translate="yes" xml:space="preserve">
          <source>PyTorch on XLA Devices</source>
          <target state="translated">XLA 장치의 PyTorch</target>
        </trans-unit>
        <trans-unit id="c93898f1a89d832ed7049068dfa7cb16c03bf3ed" translate="yes" xml:space="preserve">
          <source>PyTorch preserves storage sharing across serialization. See &lt;code&gt;preserve-storage-sharing&lt;/code&gt; for more details.</source>
          <target state="translated">PyTorch는 직렬화 과정에서 스토리지 공유를 보존합니다. 자세한 내용은 &lt;code&gt;preserve-storage-sharing&lt;/code&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="27b7e9176c7a6b31ba4779baa5f868bead4ef214" translate="yes" xml:space="preserve">
          <source>PyTorch provides two global &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt;&lt;code&gt;ConstraintRegistry&lt;/code&gt;&lt;/a&gt; objects that link &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; objects to &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.</source>
          <target state="translated">PyTorch는 &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt; 개체를 &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt; &lt;code&gt;Transform&lt;/code&gt; &lt;/a&gt; 개체에 연결 하는 두 개의 전역 &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt; &lt;code&gt;ConstraintRegistry&lt;/code&gt; &lt;/a&gt; 개체를 제공 합니다. 이러한 객체는 입력 제약 조건과 반환 변환 모두이지만 bijectivity에 대한 보장이 다릅니다.</target>
        </trans-unit>
        <trans-unit id="ccb71e60b710059778503566d9b6733540438582" translate="yes" xml:space="preserve">
          <source>PyTorch ships with two builtin backends: &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; and &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt;. Additional ones can be registered using the &lt;code&gt;register_backend()&lt;/code&gt; function.</source>
          <target state="translated">PyTorch는 &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; 및 &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt; 의 두 가지 내장 백엔드와 함께 제공 됩니다. &lt;code&gt;register_backend()&lt;/code&gt; 함수를 사용하여 추가로 등록 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2d5a1e0600b8abe6782e41ec54cae91372de9186" translate="yes" xml:space="preserve">
          <source>Python 2 does not support Ellipsis but one may use a string literal instead (&lt;code&gt;'...'&lt;/code&gt;).</source>
          <target state="translated">파이썬 2는 줄임표를 지원하지 않지만 대신 문자열 리터럴 ( &lt;code&gt;'...'&lt;/code&gt; )을 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b5ec6c002217ad80d40ef02a1f4366196fae59b7" translate="yes" xml:space="preserve">
          <source>Python 3 type hints can be used in place of &lt;code&gt;torch.jit.annotate&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.annotate&lt;/code&gt; 대신 Python 3 유형 힌트를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0711b02500c16372db7e037b0db241051fe28013" translate="yes" xml:space="preserve">
          <source>Python API</source>
          <target state="translated">파이썬 API</target>
        </trans-unit>
        <trans-unit id="cad6fbd004ed44f2a0389e7ad1a4cfa89ba1d3fc" translate="yes" xml:space="preserve">
          <source>Python Functions and Modules</source>
          <target state="translated">Python 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="c9209054e624163b69a9eb976a32495c0a33865f" translate="yes" xml:space="preserve">
          <source>Python Language Reference Comparison</source>
          <target state="translated">Python 언어 참조 비교</target>
        </trans-unit>
        <trans-unit id="e4ec2cbb65019f4837950da585dfbc7781b88d91" translate="yes" xml:space="preserve">
          <source>Python Language Reference Coverage</source>
          <target state="translated">Python 언어 참조 범위</target>
        </trans-unit>
        <trans-unit id="fd58a092a5203baf1b73956b039089eaeff86afb" translate="yes" xml:space="preserve">
          <source>Python classes can be used in TorchScript if they are annotated with &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt;, similar to how you would declare a TorchScript function:</source>
          <target state="translated">Python 클래스는 TorchScript 함수를 선언하는 방법과 유사하게 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 로 주석이 달린 경우 TorchScript에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="dcb69ff04adf584813a764fcff415e81178eed67" translate="yes" xml:space="preserve">
          <source>Python enums can be used in TorchScript without any extra annotation or code:</source>
          <target state="translated">Python 열거 형은 추가 주석이나 코드없이 TorchScript에서 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="21aa60d14a5d3bf9941c64223102e72e43cd7db3" translate="yes" xml:space="preserve">
          <source>Python-defined Constants</source>
          <target state="translated">Python 정의 상수</target>
        </trans-unit>
        <trans-unit id="bca513fe1a1ac6b508002e4d70e9b04a95755456" translate="yes" xml:space="preserve">
          <source>Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.</source>
          <target state="translated">Pytorch Hub는 연구 재현성을 촉진하도록 설계된 사전 훈련 된 모델 저장소입니다.</target>
        </trans-unit>
        <trans-unit id="4a3b71fd725a5570aa333bd5a258f51e9ad067c8" translate="yes" xml:space="preserve">
          <source>Pytorch Hub provides convenient APIs to explore all available models in hub through &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;, show docstring and examples through &lt;a href=&quot;#torch.hub.help&quot;&gt;&lt;code&gt;torch.hub.help()&lt;/code&gt;&lt;/a&gt; and load the pre-trained models using &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Pytorch Hub는 &lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt; 통해 허브에서 사용 가능한 모든 모델을 탐색하고 torch.hub.help () 를 통해 docstring 및 예제를 표시하며 &lt;a href=&quot;#torch.hub.help&quot;&gt; &lt;code&gt;torch.hub.help()&lt;/code&gt; &lt;/a&gt; 를 사용하여 사전 &lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt; 된 모델을로드 할 수있는 편리한 API를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="71ea6e31bfc2cea7da3ec5ce3389fe77a6457276" translate="yes" xml:space="preserve">
          <source>Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple &lt;code&gt;hubconf.py&lt;/code&gt; file;</source>
          <target state="translated">Pytorch Hub는 간단한 &lt;code&gt;hubconf.py&lt;/code&gt; 파일 을 추가하여 사전 훈련 된 모델 (모델 정의 및 사전 훈련 된 가중치)을 github 저장소에 게시 하는 것을 지원 합니다.</target>
        </trans-unit>
        <trans-unit id="c3156e00d3c2588c639e0d3cf6821258b05761c7" translate="yes" xml:space="preserve">
          <source>Q</source>
          <target state="translated">Q</target>
        </trans-unit>
        <trans-unit id="817b3643a8c9250c5615063227303b8c5da71cbe" translate="yes" xml:space="preserve">
          <source>Q: Does ONNX support implicit scalar datatype casting?</source>
          <target state="translated">Q : ONNX는 암시 적 스칼라 데이터 유형 캐스팅을 지원합니까?</target>
        </trans-unit>
        <trans-unit id="e999540844f438b9780d0892a8883a34284bbec7" translate="yes" xml:space="preserve">
          <source>Q: How do I store attributes on a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;?</source>
          <target state="translated">Q : &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 에&lt;/a&gt; 속성을 어떻게 저장 합니까?</target>
        </trans-unit>
        <trans-unit id="71cd3f9caa34814ac682f01c8799e67876c37c7b" translate="yes" xml:space="preserve">
          <source>Q: How to export models with loops in it?</source>
          <target state="translated">Q : 루프가있는 모델을 내보내는 방법은 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="3ab78ce25486eb77e84e3dce117cf425f112dd6c" translate="yes" xml:space="preserve">
          <source>Q: I have exported my lstm model, but its input size seems to be fixed?</source>
          <target state="translated">Q : lstm 모델을 내보냈는데 입력 크기가 고정 된 것 같습니다.</target>
        </trans-unit>
        <trans-unit id="a12de1b8a4f8718726ec01c0631665899c7a3116" translate="yes" xml:space="preserve">
          <source>Q: I would like to trace module&amp;rsquo;s method but I keep getting this error:</source>
          <target state="translated">Q : 모듈의 메서드를 추적하고 싶지만이 오류가 계속 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2b00cd8e47e4d783f5be1951f1dd20a7ce719f21" translate="yes" xml:space="preserve">
          <source>Q: I would like to train a model on GPU and do inference on CPU. What are the best practices?</source>
          <target state="translated">Q : GPU에서 모델을 훈련시키고 CPU에서 추론을하고 싶습니다. 모범 사례는 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="470067c814eb1ea3d524592caed0d6e62ae92c6f" translate="yes" xml:space="preserve">
          <source>Q: Is tensor in-place indexed assignment like &lt;code&gt;data[index] = new_data&lt;/code&gt; supported?</source>
          <target state="translated">Q : &lt;code&gt;data[index] = new_data&lt;/code&gt; 와 같은 텐서 내부 색인 할당이 지원 되나요?</target>
        </trans-unit>
        <trans-unit id="e79884b3f980f1ad19cca2c799bbefd52769c3b3" translate="yes" xml:space="preserve">
          <source>Q: Is tensor list exportable to ONNX?</source>
          <target state="translated">Q : 텐서 목록을 ONNX로 내보낼 수 있습니까?</target>
        </trans-unit>
        <trans-unit id="892976819297325dacfe2454b5f3511112ebff7a" translate="yes" xml:space="preserve">
          <source>QFunctional</source>
          <target state="translated">QFunctional</target>
        </trans-unit>
        <trans-unit id="3a8e72c5cc093d92b3cfbbcce054a2f1c557acd3" translate="yes" xml:space="preserve">
          <source>Q_\text{max}</source>
          <target state="translated">Q_\text{max}</target>
        </trans-unit>
        <trans-unit id="d92ca2583a7c56daca5b83e5f89026fff5289864" translate="yes" xml:space="preserve">
          <source>Q_\text{min}</source>
          <target state="translated">Q_\text{min}</target>
        </trans-unit>
        <trans-unit id="97f0eded6b0e44ee7ab93339b4fa67092a5342fa" translate="yes" xml:space="preserve">
          <source>Quantization</source>
          <target state="translated">Quantization</target>
        </trans-unit>
        <trans-unit id="7b80176f26e12114d8e0a0da100ea2dd78841526" translate="yes" xml:space="preserve">
          <source>Quantization configuration should be assigned preemptively to individual submodules in &lt;code&gt;.qconfig&lt;/code&gt; attribute.</source>
          <target state="translated">양자화 구성은 &lt;code&gt;.qconfig&lt;/code&gt; 속성의 개별 하위 모듈에 선제 적으로 할당되어야 합니다.</target>
        </trans-unit>
        <trans-unit id="ac5d2b1d677d49a0bd86ef0862a7927fd9191c9e" translate="yes" xml:space="preserve">
          <source>Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the &lt;a href=&quot;quantization#quantization-doc&quot;&gt;Quantization&lt;/a&gt; documentation.</source>
          <target state="translated">양자화는 계산을 수행하고 부동 소수점 정밀도보다 낮은 비트 폭으로 텐서를 저장하는 기술을 말합니다. PyTorch는 텐서 및 채널당 비대칭 선형 양자화를 모두 지원합니다. PyTorch에서 양자화 함수를 사용하는 방법에 대한 자세한 내용은 &lt;a href=&quot;quantization#quantization-doc&quot;&gt;양자화&lt;/a&gt; 문서 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="18a85a5632065b99cfe45a2efad2f1ae99da1046" translate="yes" xml:space="preserve">
          <source>Quantize</source>
          <target state="translated">Quantize</target>
        </trans-unit>
        <trans-unit id="f8fde939a75393ebe92d80b644e2d353e60f89e3" translate="yes" xml:space="preserve">
          <source>Quantize stub module, before calibration, this is same as an observer, it will be swapped as &lt;code&gt;nnq.Quantize&lt;/code&gt; in &lt;code&gt;convert&lt;/code&gt;.</source>
          <target state="translated">퀀 스터브 모듈은 교정 전, 이것과 같이 바꾼 것, 관찰자와 동일 &lt;code&gt;nnq.Quantize&lt;/code&gt; 으로 &lt;code&gt;convert&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="567337a1768d995cea83bd61b738d0b3fde9158e" translate="yes" xml:space="preserve">
          <source>Quantize the input float model with post training static quantization.</source>
          <target state="translated">사후 훈련 정적 양자화로 입력 부동 모델을 양자화합니다.</target>
        </trans-unit>
        <trans-unit id="5dc78d5962844144dbb78dcd34c444f89d2360d5" translate="yes" xml:space="preserve">
          <source>Quantized Functions</source>
          <target state="translated">양자화 된 함수</target>
        </trans-unit>
        <trans-unit id="93bd0dbffb4b3754bf685731ec3211818537c4b4" translate="yes" xml:space="preserve">
          <source>Quantized model.</source>
          <target state="translated">양자화 된 모델.</target>
        </trans-unit>
        <trans-unit id="22040c5f1ace6596f37ed53e5feee71111db31d0" translate="yes" xml:space="preserve">
          <source>Quantizes an incoming tensor</source>
          <target state="translated">들어오는 텐서를 양자화합니다.</target>
        </trans-unit>
        <trans-unit id="7ad43145efa235468f5902d61cd285ed9b73e61e" translate="yes" xml:space="preserve">
          <source>Quasi-random sampling</source>
          <target state="translated">준 랜덤 샘플링</target>
        </trans-unit>
        <trans-unit id="06576556d1ad802f247cad11ae748be47b70cd9c" translate="yes" xml:space="preserve">
          <source>R</source>
          <target state="translated">R</target>
        </trans-unit>
        <trans-unit id="a4a12b6d13143948a19488e0d1cd0864bcfcd487" translate="yes" xml:space="preserve">
          <source>R(2+1)D-18 network</source>
          <target state="translated">R (2 + 1) D-18 네트워크</target>
        </trans-unit>
        <trans-unit id="6d51f7562aadcec1a3d0cd7c60d6435cbf2ea1a0" translate="yes" xml:space="preserve">
          <source>R3D-18 network</source>
          <target state="translated">R3D-18 네트워크</target>
        </trans-unit>
        <trans-unit id="c5db7969dcd30635e5d7867040b6cc76158dd175" translate="yes" xml:space="preserve">
          <source>RAW</source>
          <target state="translated">RAW</target>
        </trans-unit>
        <trans-unit id="2fe8814f679602ef8677e9b846ae9cc63646e90c" translate="yes" xml:space="preserve">
          <source>RNN</source>
          <target state="translated">RNN</target>
        </trans-unit>
        <trans-unit id="419fa7ef1354a7471cfefa40385138f81850a613" translate="yes" xml:space="preserve">
          <source>RNNBase</source>
          <target state="translated">RNNBase</target>
        </trans-unit>
        <trans-unit id="22c95097b5426fe43997d6eb11be7ab8c5097b78" translate="yes" xml:space="preserve">
          <source>RNNCell</source>
          <target state="translated">RNNCell</target>
        </trans-unit>
        <trans-unit id="c3282cbbcba660116d62c007822229220838a1c6" translate="yes" xml:space="preserve">
          <source>RPC</source>
          <target state="translated">RPC</target>
        </trans-unit>
        <trans-unit id="b75c8242fd289be2b08bd6f1908eeb3affe74a47" translate="yes" xml:space="preserve">
          <source>RReLU</source>
          <target state="translated">RReLU</target>
        </trans-unit>
        <trans-unit id="e908d326bea7dd110e9821c1f738fe31ee60caf9" translate="yes" xml:space="preserve">
          <source>RRef</source>
          <target state="translated">RRef</target>
        </trans-unit>
        <trans-unit id="e98cb22d66aa1860c5d982443eb2d7e84c9630f1" translate="yes" xml:space="preserve">
          <source>RRef Lifetime</source>
          <target state="translated">RRef 수명</target>
        </trans-unit>
        <trans-unit id="c36d9458e0d1baae7403b5ce642a78b09e4acc7f" translate="yes" xml:space="preserve">
          <source>Raises</source>
          <target state="translated">Raises</target>
        </trans-unit>
        <trans-unit id="d92af57a28c421f8d8438229f0b21202604e4ae3" translate="yes" xml:space="preserve">
          <source>Raises &lt;code&gt;RuntimeError&lt;/code&gt; if &lt;a href=&quot;https://ninja-build.org/&quot;&gt;ninja&lt;/a&gt; build system is not available on the system, does nothing otherwise.</source>
          <target state="translated">발생시킵니다 &lt;code&gt;RuntimeError&lt;/code&gt; 에는 경우 &lt;a href=&quot;https://ninja-build.org/&quot;&gt;닌자&lt;/a&gt; 빌드 시스템은 시스템에서 사용할 수 없습니다, 그렇지 않으면 아무것도하지 않는다.</target>
        </trans-unit>
        <trans-unit id="fe6245a65da1996031b65e42c6ba6186170ca87f" translate="yes" xml:space="preserve">
          <source>Raises ValueError if the value is not present.</source>
          <target state="translated">값이 없으면 ValueError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="eef14c4912a222577af3c87a94916617431f0107" translate="yes" xml:space="preserve">
          <source>Random Number Generator</source>
          <target state="translated">난수 생성기</target>
        </trans-unit>
        <trans-unit id="aedcea91bbd64d179e04e09799d9cf9271d320c0" translate="yes" xml:space="preserve">
          <source>Random sampling</source>
          <target state="translated">무작위 샘플링</target>
        </trans-unit>
        <trans-unit id="4a94ba04a9499be153f8aa33a992c875d347dc19" translate="yes" xml:space="preserve">
          <source>Random sampling creation ops are listed under &lt;a href=&quot;#random-sampling&quot;&gt;Random sampling&lt;/a&gt; and include: &lt;a href=&quot;generated/torch.rand#torch.rand&quot;&gt;&lt;code&gt;torch.rand()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.rand_like#torch.rand_like&quot;&gt;&lt;code&gt;torch.rand_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt;&lt;code&gt;torch.randn()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randn_like#torch.randn_like&quot;&gt;&lt;code&gt;torch.randn_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randint#torch.randint&quot;&gt;&lt;code&gt;torch.randint()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randint_like#torch.randint_like&quot;&gt;&lt;code&gt;torch.randint_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randperm#torch.randperm&quot;&gt;&lt;code&gt;torch.randperm()&lt;/code&gt;&lt;/a&gt; You may also use &lt;a href=&quot;generated/torch.empty#torch.empty&quot;&gt;&lt;code&gt;torch.empty()&lt;/code&gt;&lt;/a&gt; with the &lt;a href=&quot;#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; methods to create &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; s with values sampled from a broader range of distributions.</source>
          <target state="translated">랜덤 샘플링 만들기 작전이 아래에 나열됩니다 &lt;a href=&quot;#random-sampling&quot;&gt;랜덤 샘플링&lt;/a&gt; 과 같습니다 &lt;a href=&quot;generated/torch.rand#torch.rand&quot;&gt; &lt;code&gt;torch.rand()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.rand_like#torch.rand_like&quot;&gt; &lt;code&gt;torch.rand_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt; &lt;code&gt;torch.randn()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randn_like#torch.randn_like&quot;&gt; &lt;code&gt;torch.randn_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randint#torch.randint&quot;&gt; &lt;code&gt;torch.randint()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randint_like#torch.randint_like&quot;&gt; &lt;code&gt;torch.randint_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randperm#torch.randperm&quot;&gt; &lt;code&gt;torch.randperm()&lt;/code&gt; &lt;/a&gt; 당신은 또한 할 수있다 사용 &lt;a href=&quot;generated/torch.empty#torch.empty&quot;&gt; &lt;code&gt;torch.empty()&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;#inplace-random-sampling&quot;&gt;인 곳에 랜덤 샘플링&lt;/a&gt; 만드는 방법 &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 분포의보다 넓은 범위에서 샘플 값들.</target>
        </trans-unit>
        <trans-unit id="1033bbebd194535f79c8257b5bf0911dcfc67f5b" translate="yes" xml:space="preserve">
          <source>RandomStructured</source>
          <target state="translated">RandomStructured</target>
        </trans-unit>
        <trans-unit id="1bc526966e9f4a0fc0dc8703f5e03b9b477135e9" translate="yes" xml:space="preserve">
          <source>RandomUnstructured</source>
          <target state="translated">RandomUnstructured</target>
        </trans-unit>
        <trans-unit id="464ae7dc9e362bdfac200fc767cbef5d1f028f9c" translate="yes" xml:space="preserve">
          <source>Randomized leaky ReLU.</source>
          <target state="translated">무작위 누출 ReLU.</target>
        </trans-unit>
        <trans-unit id="9e67c8ff6a6fe1b402c9707f9ca4025ad7e992ff" translate="yes" xml:space="preserve">
          <source>Randomly masks out entire channels (a channel is a feature map, e.g. the</source>
          <target state="translated">전체 채널을 무작위로 마스킹합니다 (채널은 기능 맵입니다.</target>
        </trans-unit>
        <trans-unit id="6b76ae477338397aa80a17779eb1a87460d96c9e" translate="yes" xml:space="preserve">
          <source>Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:</source>
          <target state="translated">데이터 세트를 주어진 길이의 겹치지 않는 새 데이터 세트로 무작위로 분할합니다. 재현 가능한 결과를 위해 생성기를 선택적으로 수정합니다. 예 :</target>
        </trans-unit>
        <trans-unit id="f5ee3860ad6da586f04541bda24f379685849499" translate="yes" xml:space="preserve">
          <source>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the</source>
          <target state="translated">전체 채널을 무작위로 제로화합니다 (채널은 2D 기능 맵입니다.</target>
        </trans-unit>
        <trans-unit id="d25942032b09a691f7d96817937bb37b8fc61cbd" translate="yes" xml:space="preserve">
          <source>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the</source>
          <target state="translated">전체 채널을 무작위로 제로화합니다 (채널은 3D 기능 맵입니다.</target>
        </trans-unit>
        <trans-unit id="1e57b50527a8568c0445a4ecc1addecd8c4a65f8" translate="yes" xml:space="preserve">
          <source>Randomness in multi-process data loading</source>
          <target state="translated">다중 프로세스 데이터로드의 임의성</target>
        </trans-unit>
        <trans-unit id="e887e4a3161eab98177808f95ed573db9bd91832" translate="yes" xml:space="preserve">
          <source>Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to &lt;code&gt;world_size&lt;/code&gt;.</source>
          <target state="translated">순위는 분산 프로세스 그룹 내의 각 프로세스에 할당 된 고유 식별자입니다. 항상 0에서 &lt;code&gt;world_size&lt;/code&gt; 범위의 연속 정수 입니다.</target>
        </trans-unit>
        <trans-unit id="db193220cd72bbc96216458a492d5965637b4287" translate="yes" xml:space="preserve">
          <source>Rather, this directly calls the underlying LAPACK function &lt;code&gt;?geqrf&lt;/code&gt; which produces a sequence of &amp;lsquo;elementary reflectors&amp;rsquo;.</source>
          <target state="translated">오히려 이것은 '기본 리플렉터'시퀀스를 생성하는 기본 LAPACK 함수 &lt;code&gt;?geqrf&lt;/code&gt; 를 직접 호출합니다 .</target>
        </trans-unit>
        <trans-unit id="5a864fcb6762d236bc276a85137cd0fcd44b3001" translate="yes" xml:space="preserve">
          <source>ReLU</source>
          <target state="translated">ReLU</target>
        </trans-unit>
        <trans-unit id="5290128df38d58d4be3bdda2dec90bacff114b1c" translate="yes" xml:space="preserve">
          <source>ReLU6</source>
          <target state="translated">ReLU6</target>
        </trans-unit>
        <trans-unit id="3294ad6b1eda298355d27264864a84bb153e4e5b" translate="yes" xml:space="preserve">
          <source>Real values are finite when they are not NaN, negative infinity, or infinity. Complex values are finite when both their real and imaginary parts are finite.</source>
          <target state="translated">실수 값은 NaN, 음의 무한대 또는 무한대가 아닌 경우 유한합니다. 실수 부분과 허수 부분이 모두 유한 할 때 복잡한 값은 유한합니다.</target>
        </trans-unit>
        <trans-unit id="6bf7f7c605a0ed647c478d22e23c8b7c103aaf78" translate="yes" xml:space="preserve">
          <source>Real-to-complex Discrete Fourier Transform.</source>
          <target state="translated">실수에서 복잡한 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="4cb84ee223d1d7f38f89544406d3d898f06cdafb" translate="yes" xml:space="preserve">
          <source>Rearranges elements in a tensor of shape</source>
          <target state="translated">텐서 모양으로 요소를 재정렬합니다.</target>
        </trans-unit>
        <trans-unit id="d4d632c1063de7870f16352716202b1f2a8c81c1" translate="yes" xml:space="preserve">
          <source>Receives a tensor asynchronously.</source>
          <target state="translated">텐서를 비동기 적으로받습니다.</target>
        </trans-unit>
        <trans-unit id="2f89320a075e26566ba50731edf2f47338c4432f" translate="yes" xml:space="preserve">
          <source>Receives a tensor synchronously.</source>
          <target state="translated">텐서를 동 기적으로받습니다.</target>
        </trans-unit>
        <trans-unit id="d3031eb6413ee9483cf91490238bc6c5cdb0c9c8" translate="yes" xml:space="preserve">
          <source>Reconstruct an event from an IPC handle on the given device.</source>
          <target state="translated">주어진 장치의 IPC 핸들에서 이벤트를 재구성합니다.</target>
        </trans-unit>
        <trans-unit id="9f19550117b40c6d2db06a55311fcd7ca6f091f5" translate="yes" xml:space="preserve">
          <source>Recorded event.</source>
          <target state="translated">녹화 된 이벤트.</target>
        </trans-unit>
        <trans-unit id="16d8e6844c3490f438ed2b3c2bf4939db0fa6454" translate="yes" xml:space="preserve">
          <source>Records an event.</source>
          <target state="translated">이벤트를 기록합니다.</target>
        </trans-unit>
        <trans-unit id="8c8da4978a756400f2a253bb3ea49d3f8d06e10d" translate="yes" xml:space="preserve">
          <source>Records operation history and defines formulas for differentiating ops.</source>
          <target state="translated">작업 내역을 기록하고 작업을 차별화하기위한 공식을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="8417f5f78b257e51c68ac1ce329dbe8dc58722b1" translate="yes" xml:space="preserve">
          <source>Records the event in a given stream.</source>
          <target state="translated">주어진 스트림에 이벤트를 기록합니다.</target>
        </trans-unit>
        <trans-unit id="6d3b40b37dfe8fdad50303f26863dfa5944ffb83" translate="yes" xml:space="preserve">
          <source>Recurrent Layers</source>
          <target state="translated">반복 레이어</target>
        </trans-unit>
        <trans-unit id="437a7c4fe39d00898697f5eea2075a348d8b6eda" translate="yes" xml:space="preserve">
          <source>Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.</source>
          <target state="translated">텐서 목록을 전체 그룹으로 줄이고 분산시킵니다. 현재 nccl 백엔드 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="62a542ed033585ebe95fd9e187a5f08c926cf3da" translate="yes" xml:space="preserve">
          <source>Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a &amp;lsquo;patience&amp;rsquo; number of epochs, the learning rate is reduced.</source>
          <target state="translated">메트릭 개선이 중지되면 학습률을 줄입니다. 모델은 학습이 정체되면 학습률을 2 ~ 10 배로 줄이는 것이 도움이됩니다. 이 스케줄러는 메트릭 수량을 읽고 '인내'에포크 수가 개선되지 않으면 학습률이 감소합니다.</target>
        </trans-unit>
        <trans-unit id="2e154454ad02bd98ccb4e04b626318e2ca6ecea3" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines in such a way that all get the final result.</source>
          <target state="translated">모든 머신에서 최종 결과를 얻을 수있는 방식으로 텐서 데이터를 줄입니다.</target>
        </trans-unit>
        <trans-unit id="0845d8a3b333d7e6bdeae11a45e93d821e227572" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.</source>
          <target state="translated">모든 머신에서 최종 결과를 얻을 수있는 방식으로 텐서 데이터를 줄입니다. 이 함수는 모든 노드의 텐서 수를 줄이고 각 텐서는 서로 다른 GPU에 상주합니다. 따라서 텐서 목록의 입력 텐서는 GPU 텐서 여야합니다. 또한 텐서 목록의 각 텐서는 다른 GPU에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="68d09fcde5e8e7a3c4ca4ab69d90b3bd77b87c7d" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines.</source>
          <target state="translated">모든 머신에서 텐서 데이터를 줄입니다.</target>
        </trans-unit>
        <trans-unit id="c6916d4b854bb48bab1b470966a0bd89b2429552" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data on multiple GPUs across all machines. Each tensor in &lt;code&gt;tensor_list&lt;/code&gt; should reside on a separate GPU</source>
          <target state="translated">모든 머신에서 여러 GPU의 텐서 데이터를 줄입니다. &lt;code&gt;tensor_list&lt;/code&gt; 의 각 텐서 는 별도의 GPU에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="72826c34e0b98307364b7bcd04a0655be2f7559a" translate="yes" xml:space="preserve">
          <source>Reduces, then scatters a list of tensors to all processes in a group.</source>
          <target state="translated">축소 한 다음 텐서 목록을 그룹의 모든 프로세스에 분산시킵니다.</target>
        </trans-unit>
        <trans-unit id="44eef8c0a66c5ce295026ad03e889f17e08ed29c" translate="yes" xml:space="preserve">
          <source>Reducing with the addition operation is the same as using &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt;&lt;code&gt;scatter_add_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">더하기 연산으로 줄이기는 &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt; &lt;code&gt;scatter_add_()&lt;/code&gt; &lt;/a&gt; 를 사용하는 것과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="1b6d087fa2831e91be3ac6de64b3da6a95b4c118" translate="yes" xml:space="preserve">
          <source>Reduction Ops</source>
          <target state="translated">감소 작업</target>
        </trans-unit>
        <trans-unit id="da9dd3248fef822f41edb238f214a556d8fa5e69" translate="yes" xml:space="preserve">
          <source>Reduction is not yet implemented for the CUDA backend.</source>
          <target state="translated">CUDA 백엔드에는 아직 감소가 구현되지 않았습니다.</target>
        </trans-unit>
        <trans-unit id="45c3dc1c7731c6185824876ed514e54f71bacb64" translate="yes" xml:space="preserve">
          <source>Reference:</source>
          <target state="translated">Reference:</target>
        </trans-unit>
        <trans-unit id="5d20d0fee3b91643dd8d272ac33d01ca95179d82" translate="yes" xml:space="preserve">
          <source>References</source>
          <target state="translated">References</target>
        </trans-unit>
        <trans-unit id="9d1e4e7d27b519b1da3d7266c9c87d7861741080" translate="yes" xml:space="preserve">
          <source>References:</source>
          <target state="translated">References:</target>
        </trans-unit>
        <trans-unit id="9368e2ec97261508237a4befedcf1e3dc15edd5f" translate="yes" xml:space="preserve">
          <source>References::</source>
          <target state="translated">References::</target>
        </trans-unit>
        <trans-unit id="ba0ffdee70f599751e1059c2737dfaa0253ed5bb" translate="yes" xml:space="preserve">
          <source>Refines the dimension names of &lt;code&gt;self&lt;/code&gt; according to &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">의 정제 차원 이름 &lt;code&gt;self&lt;/code&gt; 에 따라 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fd83bfc89e8951867aebce2ddbcf9aebb755b1db" translate="yes" xml:space="preserve">
          <source>Refining is a special case of renaming that &amp;ldquo;lifts&amp;rdquo; unnamed dimensions. A &lt;code&gt;None&lt;/code&gt; dim can be refined to have any name; a named dim can only be refined to have the same name.</source>
          <target state="translated">미세 조정은 이름이 지정되지 않은 치수를 &quot;리프트&quot;하는 이름을 바꾸는 특별한 경우입니다. &lt;code&gt;None&lt;/code&gt; 희미한는 어떤 이름을 가지고 정제 할 수있다; 명명 된 dim은 동일한 이름을 갖도록 미세 조정할 수만 있습니다.</target>
        </trans-unit>
        <trans-unit id="5acc200962ac427c960517361870d2ca95708a0c" translate="yes" xml:space="preserve">
          <source>ReflectionPad1d</source>
          <target state="translated">ReflectionPad1d</target>
        </trans-unit>
        <trans-unit id="1970d048b8b11bdbb537bc4467b97dde9cbcbcf6" translate="yes" xml:space="preserve">
          <source>ReflectionPad2d</source>
          <target state="translated">ReflectionPad2d</target>
        </trans-unit>
        <trans-unit id="b0e2ded4176f1e93c24c6bb28a1f246d0f065d46" translate="yes" xml:space="preserve">
          <source>Registers a &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; subclass in this registry. Usage:</source>
          <target state="translated">이 레지스트리에 &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt; 서브 클래스를 등록합니다 . 용법:</target>
        </trans-unit>
        <trans-unit id="44e5f9796886b9f39a08e9c3d313df29a4afdc69" translate="yes" xml:space="preserve">
          <source>Registers a backward hook on the module.</source>
          <target state="translated">모듈에 역방향 후크를 등록합니다.</target>
        </trans-unit>
        <trans-unit id="780fedc50fbec8a5ee2093bca221ccb17c71d8a2" translate="yes" xml:space="preserve">
          <source>Registers a backward hook.</source>
          <target state="translated">역방향 후크를 등록합니다.</target>
        </trans-unit>
        <trans-unit id="859ad11a7a9b00f2dc1648434120e2c30f756c3a" translate="yes" xml:space="preserve">
          <source>Registers a forward hook on the module.</source>
          <target state="translated">모듈에 포워드 후크를 등록합니다.</target>
        </trans-unit>
        <trans-unit id="c1cc6d46047e39e672054a7b43c380ab89aa516c" translate="yes" xml:space="preserve">
          <source>Registers a forward pre-hook on the module.</source>
          <target state="translated">모듈에 정방향 사전 후크를 등록합니다.</target>
        </trans-unit>
        <trans-unit id="d2f50deb4525028ab1c8e505fb5f42b3b1805265" translate="yes" xml:space="preserve">
          <source>Registry to link constraints to transforms.</source>
          <target state="translated">제약 조건을 변환에 연결하는 레지스트리.</target>
        </trans-unit>
        <trans-unit id="994be4aa8c70d405e9665f571a19642fba261dfd" translate="yes" xml:space="preserve">
          <source>Reinterprets some of the batch dims of a distribution as event dims.</source>
          <target state="translated">배포의 일부 배치 희미 함을 이벤트 희미 함으로 재 해석합니다.</target>
        </trans-unit>
        <trans-unit id="c4e38689980bd0eb675729a3f5eeff33277a75da" translate="yes" xml:space="preserve">
          <source>RelaxedBernoulli</source>
          <target state="translated">RelaxedBernoulli</target>
        </trans-unit>
        <trans-unit id="17435d6989481bd254182b89c973e12b3560805c" translate="yes" xml:space="preserve">
          <source>RelaxedOneHotCategorical</source>
          <target state="translated">RelaxedOneHotCategorical</target>
        </trans-unit>
        <trans-unit id="017e050dc2c81440963780dd987527e102d00635" translate="yes" xml:space="preserve">
          <source>Release memory ASAP in the consumer.</source>
          <target state="translated">소비자의 메모리를 최대한 빨리 해제하십시오.</target>
        </trans-unit>
        <trans-unit id="c78aa00da732269786895ce2069aaa539d4c52f2" translate="yes" xml:space="preserve">
          <source>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in &lt;code&gt;nvidia-smi&lt;/code&gt;.</source>
          <target state="translated">다른 GPU 애플리케이션에서 사용하고 &lt;code&gt;nvidia-smi&lt;/code&gt; 에서 볼 수 있도록 현재 캐싱 할당자가 보유하고있는 사용되지 않은 모든 캐시 메모리를 해제합니다 .</target>
        </trans-unit>
        <trans-unit id="6aabe5246532aa5e226dacac93b7040def81f191" translate="yes" xml:space="preserve">
          <source>Remote Reference Protocol</source>
          <target state="translated">원격 참조 프로토콜</target>
        </trans-unit>
        <trans-unit id="d2611051acb1fb592d77daa5cae8fbc141634b65" translate="yes" xml:space="preserve">
          <source>Remove all items from the ModuleDict.</source>
          <target state="translated">ModuleDict에서 모든 항목을 제거하십시오.</target>
        </trans-unit>
        <trans-unit id="19890e3b8941dee97b6730d3bdd4b5d7d4da71b3" translate="yes" xml:space="preserve">
          <source>Remove all items from the ParameterDict.</source>
          <target state="translated">ParameterDict에서 모든 항목을 제거하십시오.</target>
        </trans-unit>
        <trans-unit id="42e212ea49cffd4d66b50c00fb3332336dc13f4d" translate="yes" xml:space="preserve">
          <source>Remove key from the ModuleDict and return its module.</source>
          <target state="translated">ModuleDict에서 키를 제거하고 해당 모듈을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="1225726758ca9313c0c5ad3ef2ebcc4ac3f41d5b" translate="yes" xml:space="preserve">
          <source>Remove key from the ParameterDict and return its parameter.</source>
          <target state="translated">ParameterDict에서 키를 제거하고 해당 매개 변수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="9168342afe82b61ecfcba245e5a3e85066d9cae2" translate="yes" xml:space="preserve">
          <source>Removes a tensor dimension.</source>
          <target state="translated">텐서 차원을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="10551f8140eae7eec1c2f790aa2c398a8a2c225b" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module and the pruning method from the forward hook.</source>
          <target state="translated">모듈에서 프 루닝 재 매개 변수화를 제거하고 포워드 후크에서 프 루닝 메소드를 제거합니다.</target>
        </trans-unit>
        <trans-unit id="4d39954d7304877c790bae592a897de9b12d0614" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named &lt;code&gt;name&lt;/code&gt; remains permanently pruned, and the parameter named &lt;code&gt;name+'_orig'&lt;/code&gt; is removed from the parameter list. Similarly, the buffer named &lt;code&gt;name+'_mask'&lt;/code&gt; is removed from the buffers.</source>
          <target state="translated">모듈에서 프 루닝 재 매개 변수화를 제거하고 포워드 후크에서 프 루닝 메소드를 제거합니다. &lt;code&gt;name&lt;/code&gt; 이라는 이름 의 정리 된 매개 변수 는 영구적으로 정리 된 상태로 남아 있으며 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 이름 의 매개 변수는 매개 변수 목록에서 제거됩니다. 마찬가지로 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 이름 의 버퍼가 버퍼에서 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="c59a012a002ebcdeb37a7a4b34651f264175236e" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module. The pruned parameter named &lt;code&gt;name&lt;/code&gt; remains permanently pruned, and the parameter named &lt;code&gt;name+'_orig'&lt;/code&gt; is removed from the parameter list. Similarly, the buffer named &lt;code&gt;name+'_mask'&lt;/code&gt; is removed from the buffers.</source>
          <target state="translated">모듈에서 정리 다시 매개 변수화를 제거합니다. &lt;code&gt;name&lt;/code&gt; 이라는 이름 의 정리 된 매개 변수 는 영구적으로 정리 된 상태로 남아 있으며 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 이름 의 매개 변수는 매개 변수 목록에서 제거됩니다. 마찬가지로 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 이름 의 버퍼가 버퍼에서 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="dc2f6b46c8953513899753dc6f37ab21a947d81f" translate="yes" xml:space="preserve">
          <source>Removes the spectral normalization reparameterization from a module.</source>
          <target state="translated">모듈에서 스펙트럼 정규화 다시 매개 변수화를 제거합니다.</target>
        </trans-unit>
        <trans-unit id="e6f3f85d165e444aba5b7212e4c107777c615fa6" translate="yes" xml:space="preserve">
          <source>Removes the weight normalization reparameterization from a module.</source>
          <target state="translated">모듈에서 가중치 정규화 다시 매개 변수화를 제거합니다.</target>
        </trans-unit>
        <trans-unit id="52a78431ddbd753c3acf927411e5dc4eadd51b45" translate="yes" xml:space="preserve">
          <source>Renames dimension names of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 차원 이름을 바꿉니다 .</target>
        </trans-unit>
        <trans-unit id="1e15bfbedf002065481094064ab67496cead9c00" translate="yes" xml:space="preserve">
          <source>Render matplotlib figure into an image and add it to summary.</source>
          <target state="translated">matplotlib 그림을 이미지로 렌더링하고 요약에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="d4424e2e484e1d0e2228176a1b9d133567150454" translate="yes" xml:space="preserve">
          <source>Repeat elements of a tensor.</source>
          <target state="translated">텐서의 요소를 반복합니다.</target>
        </trans-unit>
        <trans-unit id="c71c8380a78ab9cf6beaa41ffbcea1de55f181da" translate="yes" xml:space="preserve">
          <source>Repeated tensor which has the same shape as input, except along the</source>
          <target state="translated">입력과 같은 모양을 가진 반복 된 텐서</target>
        </trans-unit>
        <trans-unit id="fb8854cff80d6a6dd0521042f630c62a94943f9c" translate="yes" xml:space="preserve">
          <source>Repeats this tensor along the specified dimensions.</source>
          <target state="translated">지정된 차원을 따라이 텐서를 반복합니다.</target>
        </trans-unit>
        <trans-unit id="b48641d4be70735a27677d2b7a657eb8c5a36bbf" translate="yes" xml:space="preserve">
          <source>Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.</source>
          <target state="translated">지정된 모듈을 동적 가중치 전용 양자화 버전으로 교체하고 양자화 모델을 출력합니다.</target>
        </trans-unit>
        <trans-unit id="d717a0451eef9c7694fd101a42a92bb95d3c0a0f" translate="yes" xml:space="preserve">
          <source>ReplicationPad1d</source>
          <target state="translated">ReplicationPad1d</target>
        </trans-unit>
        <trans-unit id="7134ff298e79c741cf90df7d64ae3929d67c6005" translate="yes" xml:space="preserve">
          <source>ReplicationPad2d</source>
          <target state="translated">ReplicationPad2d</target>
        </trans-unit>
        <trans-unit id="b3ace69bd0bb68cc7d2ded9c8edec593b271b884" translate="yes" xml:space="preserve">
          <source>ReplicationPad3d</source>
          <target state="translated">ReplicationPad3d</target>
        </trans-unit>
        <trans-unit id="e41aa1fbbb423bb33022752fd330ea4ede8e16b6" translate="yes" xml:space="preserve">
          <source>Reproducibility</source>
          <target state="translated">Reproducibility</target>
        </trans-unit>
        <trans-unit id="f2e21cfb2562445f4e03149dcac964dc2b3217e6" translate="yes" xml:space="preserve">
          <source>ResNeXt</source>
          <target state="translated">ResNeXt</target>
        </trans-unit>
        <trans-unit id="135c48b06dbed678d0d867cebf0cf2de161d70cb" translate="yes" xml:space="preserve">
          <source>ResNeXt-101 32x8d model from &lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;Aggregated Residual Transformation for Deep Neural Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">ResNeXt-101 32x8d 모델 &lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&quot;심층 신경망을위한 집계 된 잔차 변환&quot;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd1a95f8f26475fcde5af625d8ad155d8daa61e2" translate="yes" xml:space="preserve">
          <source>ResNeXt-101-32x8d</source>
          <target state="translated">ResNeXt-101-32x8d</target>
        </trans-unit>
        <trans-unit id="6345fcacef60a6153337313629690096bdc6aa78" translate="yes" xml:space="preserve">
          <source>ResNeXt-50 32x4d model from &lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;Aggregated Residual Transformation for Deep Neural Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&quot;심층 신경망을위한 집계 된 잔차 변환&quot;의&lt;/a&gt; ResNeXt-50 32x4d 모델</target>
        </trans-unit>
        <trans-unit id="e6816178c1f62a28d2e7d31075a846209cc1ee81" translate="yes" xml:space="preserve">
          <source>ResNeXt-50-32x4d</source>
          <target state="translated">ResNeXt-50-32x4d</target>
        </trans-unit>
        <trans-unit id="78fa6ef9716ebb9b06e34fb7e8ef3e1ee1ff74a3" translate="yes" xml:space="preserve">
          <source>ResNet</source>
          <target state="translated">ResNet</target>
        </trans-unit>
        <trans-unit id="5e9c332cfed41849a28e63e75aa34789f743a723" translate="yes" xml:space="preserve">
          <source>ResNet (2+1)D</source>
          <target state="translated">ResNet (2 + 1) D</target>
        </trans-unit>
        <trans-unit id="870e5dde5df25034d3630c3aac9cb2c4bf4e76fc" translate="yes" xml:space="preserve">
          <source>ResNet 3D</source>
          <target state="translated">ResNet 3D</target>
        </trans-unit>
        <trans-unit id="da256605d586d089e9e7223940cb0055eadbd1ee" translate="yes" xml:space="preserve">
          <source>ResNet 3D 18</source>
          <target state="translated">ResNet 3D 18</target>
        </trans-unit>
        <trans-unit id="f8475b358157d99679b66bf1c03461ee4befbc51" translate="yes" xml:space="preserve">
          <source>ResNet MC 18</source>
          <target state="translated">ResNet MC 18</target>
        </trans-unit>
        <trans-unit id="1f7d19436196d570e8d470413429de7c74b3c3f2" translate="yes" xml:space="preserve">
          <source>ResNet Mixed Convolution</source>
          <target state="translated">ResNet 혼합 회선</target>
        </trans-unit>
        <trans-unit id="65817ac0c28cdf514ee310ccdae57eb30ff37866" translate="yes" xml:space="preserve">
          <source>ResNet-101</source>
          <target state="translated">ResNet-101</target>
        </trans-unit>
        <trans-unit id="6cdb79fa3540b98aee4fbbedd859f85dfa5878e1" translate="yes" xml:space="preserve">
          <source>ResNet-101 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&quot;이미지 인식을위한 딥 레지 듀얼 학습&quot;의&lt;/a&gt; ResNet-101 모델</target>
        </trans-unit>
        <trans-unit id="ad50fc165163aca8aff90e2401f355f0d73cb36d" translate="yes" xml:space="preserve">
          <source>ResNet-152</source>
          <target state="translated">ResNet-152</target>
        </trans-unit>
        <trans-unit id="ce82bef112318844175b33dd6318fe126f6f7716" translate="yes" xml:space="preserve">
          <source>ResNet-152 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&quot;이미지 인식을위한 딥 레지 듀얼 학습&quot;의&lt;/a&gt; ResNet-152 모델</target>
        </trans-unit>
        <trans-unit id="a95e0d1f4879bbdecde67b2d824acd91e0a2a593" translate="yes" xml:space="preserve">
          <source>ResNet-18</source>
          <target state="translated">ResNet-18</target>
        </trans-unit>
        <trans-unit id="0c0f62f5b13f506e60ccc5621518d2252a8fa85e" translate="yes" xml:space="preserve">
          <source>ResNet-18 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&quot;이미지 인식을위한 딥 레지 듀얼 학습&quot;의&lt;/a&gt; ResNet-18 모델</target>
        </trans-unit>
        <trans-unit id="a7751035efc6144aca1732aa23112dd3aecb6bc5" translate="yes" xml:space="preserve">
          <source>ResNet-34</source>
          <target state="translated">ResNet-34</target>
        </trans-unit>
        <trans-unit id="e8def5ecc7cddff157e2b586142f5fdf856d05a5" translate="yes" xml:space="preserve">
          <source>ResNet-34 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&quot;이미지 인식을위한 딥 레지 듀얼 학습&quot;의&lt;/a&gt; ResNet-34 모델</target>
        </trans-unit>
        <trans-unit id="27444a897bb1c912cdd3b04f0a83fd62a5d2c590" translate="yes" xml:space="preserve">
          <source>ResNet-50</source>
          <target state="translated">ResNet-50</target>
        </trans-unit>
        <trans-unit id="238ac2833cd54b6df3e18e9c24a824061fd3a21c" translate="yes" xml:space="preserve">
          <source>ResNet-50 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&quot;이미지 인식을위한 딥 레지 듀얼 학습&quot;의&lt;/a&gt; ResNet-50 모델</target>
        </trans-unit>
        <trans-unit id="6f2d9cd833caae8dbee1966d8ab0943fddb27fd0" translate="yes" xml:space="preserve">
          <source>ResNext</source>
          <target state="translated">ResNext</target>
        </trans-unit>
        <trans-unit id="d6989ed48031dfd0f342adca72920ca82d14ffc5" translate="yes" xml:space="preserve">
          <source>Resets parameter data pointer so that they can use faster code paths.</source>
          <target state="translated">더 빠른 코드 경로를 사용할 수 있도록 매개 변수 데이터 포인터를 재설정합니다.</target>
        </trans-unit>
        <trans-unit id="48147200a8245c840b5cf1d24511816f59e2613d" translate="yes" xml:space="preserve">
          <source>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</source>
          <target state="translated">지정된 장치에 대해 캐싱 할당자가 관리하는 최대 GPU 메모리 추적의 시작점을 재설정합니다.</target>
        </trans-unit>
        <trans-unit id="055ff3b94f1675cb0c91b05c26e8f21a16e0aac4" translate="yes" xml:space="preserve">
          <source>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</source>
          <target state="translated">주어진 장치에 대해 텐서가 차지하는 최대 GPU 메모리를 추적하는 시작점을 재설정합니다.</target>
        </trans-unit>
        <trans-unit id="5146907f6db4f1b0f9869f7b411013406ae86ae8" translate="yes" xml:space="preserve">
          <source>Resizes &lt;code&gt;self&lt;/code&gt; tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.</source>
          <target state="translated">크기를 조정 &lt;code&gt;self&lt;/code&gt; 지정된 크기에 텐서를. 요소 수가 현재 저장소 크기보다 크면 기본 저장소 크기가 새 요소 수에 맞게 조정됩니다. 요소 수가 적 으면 기본 스토리지가 변경되지 않습니다. 기존 요소는 보존되지만 새 메모리는 초기화되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="70492132da5a47c3477a96c6627fc5933e85e1e1" translate="yes" xml:space="preserve">
          <source>Resizes the &lt;code&gt;self&lt;/code&gt; tensor to be the same size as the specified &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;. This is equivalent to &lt;code&gt;self.resize_(tensor.size())&lt;/code&gt;.</source>
          <target state="translated">의 크기를 조정 &lt;code&gt;self&lt;/code&gt; 지정된와 같은 크기로 텐서 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; . 이것은 &lt;code&gt;self.resize_(tensor.size())&lt;/code&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="dc2cdc93dc037b80e1ebd8434da768aef6d0fcaf" translate="yes" xml:space="preserve">
          <source>Result is &lt;code&gt;-inf&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; has zero log determinant, and is &lt;code&gt;nan&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; has negative determinant.</source>
          <target state="translated">결과는 &lt;code&gt;-inf&lt;/code&gt; 경우 &lt;code&gt;input&lt;/code&gt; 제로 로그 결정을 보유하고, &lt;code&gt;nan&lt;/code&gt; 경우 &lt;code&gt;input&lt;/code&gt; 부정적인 결정을 갖는다.</target>
        </trans-unit>
        <trans-unit id="8ae09aed030eb1a83f83c301c7fc275b3c8a0647" translate="yes" xml:space="preserve">
          <source>RetinaNet</source>
          <target state="translated">RetinaNet</target>
        </trans-unit>
        <trans-unit id="a1e229ed770b9619ca67b1b0aef6ec1a40fa1776" translate="yes" xml:space="preserve">
          <source>RetinaNet ResNet-50 FPN</source>
          <target state="translated">RetinaNet ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="2f7a9d4a0bc6f5ce0cbca61c540daaa5837025ee" translate="yes" xml:space="preserve">
          <source>Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given &lt;code&gt;context_id&lt;/code&gt; as part of the distributed autograd backward pass.</source>
          <target state="translated">분산 된 autograd 역방향 패스의 일부로 주어진 &lt;code&gt;context_id&lt;/code&gt; 에 해당하는 제공된 컨텍스트에 누적 된 해당 Tensor에 대한 적절한 그라디언트로 Tensor에서 맵을 검색합니다 .</target>
        </trans-unit>
        <trans-unit id="1ece76093eabcbca0a40b151ec0259b426ce567d" translate="yes" xml:space="preserve">
          <source>Retrieves the value associated with the given &lt;code&gt;key&lt;/code&gt; in the store. If &lt;code&gt;key&lt;/code&gt; is not present in the store, the function will wait for &lt;code&gt;timeout&lt;/code&gt;, which is defined when initializing the store, before throwing an exception.</source>
          <target state="translated">저장소 의 지정된 &lt;code&gt;key&lt;/code&gt; 와 관련된 값을 검색합니다 . 경우 &lt;code&gt;key&lt;/code&gt; 저장소에 존재하지 않는 함수는 기다리는 &lt;code&gt;timeout&lt;/code&gt; 예외를 발생하기 전에, 매장을 초기화 할 때 정의된다.</target>
        </trans-unit>
        <trans-unit id="24f096b221f9534bcad007f2b5a32b490950b5b5" translate="yes" xml:space="preserve">
          <source>Return</source>
          <target state="translated">Return</target>
        </trans-unit>
        <trans-unit id="9cae8b09f0118269a649c5008d17c8253bec9782" translate="yes" xml:space="preserve">
          <source>Return &lt;code&gt;True&lt;/code&gt; if this &lt;code&gt;Future&lt;/code&gt; is done. A &lt;code&gt;Future&lt;/code&gt; is done if it has a result or an exception.</source>
          <target state="translated">이 &lt;code&gt;Future&lt;/code&gt; 가 완료 되면 &lt;code&gt;True&lt;/code&gt; 를 반환 합니다. &lt;code&gt;Future&lt;/code&gt; 는 결과 또는 예외가있는 경우 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="402831e8533c0acc0b144ded8789c976ea34ab74" translate="yes" xml:space="preserve">
          <source>Return a tensor of elements selected from either &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt;, depending on &lt;code&gt;condition&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;condition&lt;/code&gt; 에 따라 &lt;code&gt;x&lt;/code&gt; 또는 &lt;code&gt;y&lt;/code&gt; 에서 선택한 요소의 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="91f0e9dea6ad89a9cf4c50eeccd2244407a0e3b6" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict key/value pairs.</source>
          <target state="translated">ModuleDict 키 / 값 쌍의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="53758c5a8d9838ee15c5a1139ddc298268e3f550" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict keys.</source>
          <target state="translated">ModuleDict 키의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="75b2716bbd52d76a593565ce26e635f35a65eba3" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict values.</source>
          <target state="translated">ModuleDict 값의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="39aa7eccaf084effaa1d95b2523664d87adadc63" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict key/value pairs.</source>
          <target state="translated">ParameterDict 키 / 값 쌍의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="36d08ddba7858cda4d75cdfaa632b92a0d271817" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict keys.</source>
          <target state="translated">ParameterDict 키의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0eb7abc74094e832f39c777a2b9fdb3353665e0c" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict values.</source>
          <target state="translated">ParameterDict 값의 이터 러블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6fb5e3008c47afba229895a4aa4b8cfa03845a94" translate="yes" xml:space="preserve">
          <source>Return the next floating-point value after &lt;code&gt;input&lt;/code&gt; towards &lt;code&gt;other&lt;/code&gt;, elementwise.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; 요소에 대한 &lt;code&gt;input&lt;/code&gt; 후 다음 부동 소수점 값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="c457486fbf0726f2b8521c0379e30203a9c59a56" translate="yes" xml:space="preserve">
          <source>Return the recommended gain value for the given nonlinearity function. The values are as follows:</source>
          <target state="translated">주어진 비선형 성 함수에 권장되는 이득 값을 반환합니다. 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="689416efbdc5cadbcaa00d5d52249a2a0b1ad5c6" translate="yes" xml:space="preserve">
          <source>Return the singular value decomposition &lt;code&gt;(U, S, V)&lt;/code&gt; of a matrix, batches of matrices, or a sparse matrix</source>
          <target state="translated">행렬, 행렬 배치 또는 희소 행렬 의 특이 값 분해 &lt;code&gt;(U, S, V)&lt;/code&gt; 를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="41b1fb407b7fa442b77381701968fb175362cf78" translate="yes" xml:space="preserve">
          <source>Return type</source>
          <target state="translated">반품 유형</target>
        </trans-unit>
        <trans-unit id="1254906d712ed370ba1cdf6cdc5b06fa43eea6c3" translate="yes" xml:space="preserve">
          <source>Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as &lt;code&gt;resize_&lt;/code&gt; / &lt;code&gt;resize_as_&lt;/code&gt; / &lt;code&gt;set_&lt;/code&gt; / &lt;code&gt;transpose_&lt;/code&gt;) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as &lt;code&gt;zero_&lt;/code&gt; / &lt;code&gt;copy_&lt;/code&gt; / &lt;code&gt;add_&lt;/code&gt;) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.</source>
          <target state="translated">반환 된 Tensor는 원래 저장소와 동일한 저장소를 공유합니다. 둘 중 하나에 대한 내부 수정 사항이 표시되며 정확성 검사에서 오류가 발생할 수 있습니다. 중요 참고 : 이전 에는 반환 된 텐서에 대한 내부 크기 / 보폭 / 스토리지 변경 (예 : &lt;code&gt;resize_&lt;/code&gt; / &lt;code&gt;resize_as_&lt;/code&gt; / &lt;code&gt;set_&lt;/code&gt; / &lt;code&gt;transpose_&lt;/code&gt; )도 원래 텐서를 업데이트합니다. 이제 이러한 내부 변경 사항은 더 이상 원래 텐서를 업데이트하지 않고 대신 오류를 트리거합니다. 희소 텐서의 경우 : 반환 된 텐서에 대한 &lt;code&gt;add_&lt;/code&gt; &lt;code&gt;zero_&lt;/code&gt; 인덱스 / 값 변경 (예 : zero_ / &lt;code&gt;copy_&lt;/code&gt; / add_ )은 더 이상 원래 텐서를 업데이트하지 않고 대신 오류를 트리거합니다.</target>
        </trans-unit>
        <trans-unit id="24bbb3acca42b36ec251562d04ed070ce8e00154" translate="yes" xml:space="preserve">
          <source>Returned by &lt;a href=&quot;#torch.multiprocessing.spawn&quot;&gt;&lt;code&gt;spawn()&lt;/code&gt;&lt;/a&gt; when called with &lt;code&gt;join=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;join=False&lt;/code&gt; 로 호출되면 &lt;a href=&quot;#torch.multiprocessing.spawn&quot;&gt; &lt;code&gt;spawn()&lt;/code&gt; &lt;/a&gt; 의해 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="996cd4205ef23a24fd77a57ded3d3ea1bc26418b" translate="yes" xml:space="preserve">
          <source>Returned tensor</source>
          <target state="translated">반환 된 텐서</target>
        </trans-unit>
        <trans-unit id="9582a02f141fc4b345b2936eba691cd0654efebc" translate="yes" xml:space="preserve">
          <source>Returns</source>
          <target state="translated">Returns</target>
        </trans-unit>
        <trans-unit id="bececec8df4be65f442e8a241d0d72275aefac53" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if all processes have been joined successfully, &lt;code&gt;False&lt;/code&gt; if there are more processes that need to be joined.</source>
          <target state="translated">모든 프로세스가 성공적으로 조인되면 &lt;code&gt;True&lt;/code&gt; 를 반환하고 조인 해야하는 프로세스가 더 있으면 &lt;code&gt;False&lt;/code&gt; 를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="11cfd066b647ca21ea710fa73097f97cb102e1f7" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if the &lt;a href=&quot;https://ninja-build.org/&quot;&gt;ninja&lt;/a&gt; build system is available on the system, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">반환 &lt;code&gt;True&lt;/code&gt; 경우 생성 &lt;a href=&quot;https://ninja-build.org/&quot;&gt;닌자&lt;/a&gt; 빌드 시스템은 시스템에서 사용할 수 있습니다 &lt;code&gt;False&lt;/code&gt; 그렇지.</target>
        </trans-unit>
        <trans-unit id="196db82d442085adba66ce72a40bf4c6b883e9c5" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if the distributed package is available. Otherwise, &lt;code&gt;torch.distributed&lt;/code&gt; does not expose any other APIs. Currently, &lt;code&gt;torch.distributed&lt;/code&gt; is available on Linux, MacOS and Windows. Set &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; to enable it when building PyTorch from source. Currently, the default value is &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; for Linux and Windows, &lt;code&gt;USE_DISTRIBUTED=0&lt;/code&gt; for MacOS.</source>
          <target state="translated">배포 된 패키지를 사용할 수 있으면 &lt;code&gt;True&lt;/code&gt; 를 반환 합니다. 그렇지 않으면 &lt;code&gt;torch.distributed&lt;/code&gt; 는 다른 API를 노출하지 않습니다. 현재 &lt;code&gt;torch.distributed&lt;/code&gt; 는 Linux, MacOS 및 Windows에서 사용할 수 있습니다. 설정 &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; 은 소스에서 PyTorch를 구축 할 때를 활성화합니다. 현재 기본값은 &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; 리눅스와 윈도우를위한 &lt;code&gt;USE_DISTRIBUTED=0&lt;/code&gt; 맥 OS에 대한.</target>
        </trans-unit>
        <trans-unit id="616465fba98082b69d414e38f74b13cab726f4ca" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if your system supports flushing denormal numbers and it successfully configures flush denormal mode. &lt;a href=&quot;#torch.set_flush_denormal&quot;&gt;&lt;code&gt;set_flush_denormal()&lt;/code&gt;&lt;/a&gt; is only supported on x86 architectures supporting SSE3.</source>
          <target state="translated">시스템이 비정규 번호 플러시를 지원하고 비정규 플러시 모드를 성공적으로 구성하면 &lt;code&gt;True&lt;/code&gt; 를 반환 합니다. &lt;a href=&quot;#torch.set_flush_denormal&quot;&gt; &lt;code&gt;set_flush_denormal()&lt;/code&gt; &lt;/a&gt; 은 SSE3를 지원하는 x86 아키텍처에서만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="65ca52a0d52509836247baf62510fc9195f808a9" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;self&lt;/code&gt; tensor as a NumPy &lt;code&gt;ndarray&lt;/code&gt;. This tensor and the returned &lt;code&gt;ndarray&lt;/code&gt; share the same underlying storage. Changes to &lt;code&gt;self&lt;/code&gt; tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서를 NumPy &lt;code&gt;ndarray&lt;/code&gt; 로 반환합니다 . 이 텐서와 반환 된 &lt;code&gt;ndarray&lt;/code&gt; 는 동일한 기본 스토리지를 공유합니다. &lt;code&gt;self&lt;/code&gt; 텐서의 변경 사항 은 &lt;code&gt;ndarray&lt;/code&gt; 에 반영 되며 그 반대의 경우도 마찬가지입니다.</target>
        </trans-unit>
        <trans-unit id="606042b612f01e1413a4be37fdcf4bc9fb55a2fd" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s offset in the underlying storage in terms of number of storage elements (not bytes).</source>
          <target state="translated">스토리지 요소 수 (바이트가 아님) 측면에서 기본 스토리지의 &lt;code&gt;self&lt;/code&gt; 텐서 오프셋을 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="f2368e1fd9ddecc9751151ce685ba540173983ea" translate="yes" xml:space="preserve">
          <source>Returns NVCC gencode flags this library were compiled with.</source>
          <target state="translated">이 라이브러리가 컴파일 된 NVCC gencode 플래그를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="bcee86aa4247be4fa57afaf338977b171a646e64" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;obj&lt;/code&gt; is a PyTorch storage object.</source>
          <target state="translated">&lt;code&gt;obj&lt;/code&gt; 가 PyTorch 저장소 개체 이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="0c0fb790a1f7d7b5df52c8eb6b44b136b1ef5094" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;obj&lt;/code&gt; is a PyTorch tensor.</source>
          <target state="translated">&lt;code&gt;obj&lt;/code&gt; 가 PyTorch 텐서 이면 True를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="598b755e55cee356b7f7a395b6039f5d177fd41e" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;self&lt;/code&gt; tensor is contiguous in memory in the order specified by memory format.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서가 메모리 형식에 지정된 순서대로 메모리에서 연속적 이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="f80a083f5b5de080482f9bd320c35bee376edd5c" translate="yes" xml:space="preserve">
          <source>Returns True if all elements in each row of the tensor in the given dimension &lt;code&gt;dim&lt;/code&gt; are True, False otherwise.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 텐서의 각 행에있는 모든 요소가 True이면 True를 반환하고 그렇지 않으면 False를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="10febb846509f6f2bc8c4b9a8a6d5f7a4d94b02e" translate="yes" xml:space="preserve">
          <source>Returns True if all elements in the tensor are True, False otherwise.</source>
          <target state="translated">텐서의 모든 요소가 True이면 True를 반환하고 그렇지 않으면 False를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="ec249a00c3df799b548b2828fcdbdce219624299" translate="yes" xml:space="preserve">
          <source>Returns True if any elements in each row of the tensor in the given dimension &lt;code&gt;dim&lt;/code&gt; are True, False otherwise.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 텐서의 각 행에있는 요소 가 True이면 True를, 그렇지 않으면 False를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="19164100a9af1a366aaf5bd12a4e88efb4270d73" translate="yes" xml:space="preserve">
          <source>Returns True if any elements in the tensor are True, False otherwise.</source>
          <target state="translated">텐서의 요소가 True이면 True를 반환하고 그렇지 않으면 False를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="93fef2bb2e23cd032c2bc66acb9ba2af786fd6c1" translate="yes" xml:space="preserve">
          <source>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</source>
          <target state="translated">두 텐서가 정확히 동일한 메모리 (동일한 저장소, 오프셋, 크기 및 스트라이드)를 가리키면 True를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="dfeaffdf583441339db6cd69904330215eafe368" translate="yes" xml:space="preserve">
          <source>Returns True if the &lt;code&gt;input&lt;/code&gt; is a single element tensor which is not equal to zero after type conversions.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 이 유형 변환 후 0이 아닌 단일 요소 텐서 이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="c533fa6ff5203fb73d711ade459c89e619986758" translate="yes" xml:space="preserve">
          <source>Returns True if the &lt;code&gt;input&lt;/code&gt; is a single element tensor which is not equal to zero after type conversions. i.e. not equal to &lt;code&gt;torch.tensor([0.])&lt;/code&gt; or &lt;code&gt;torch.tensor([0])&lt;/code&gt; or &lt;code&gt;torch.tensor([False])&lt;/code&gt;. Throws a &lt;code&gt;RuntimeError&lt;/code&gt; if &lt;code&gt;torch.numel() != 1&lt;/code&gt; (even in case of sparse tensors).</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 이 유형 변환 후 0이 아닌 단일 요소 텐서 이면 True를 반환 합니다. 즉, &lt;code&gt;torch.tensor([0.])&lt;/code&gt; 또는 &lt;code&gt;torch.tensor([0])&lt;/code&gt; 또는 &lt;code&gt;torch.tensor([False])&lt;/code&gt; 와 같지 않습니다 . 예외 &lt;code&gt;RuntimeError&lt;/code&gt; 에이 경우 &lt;code&gt;torch.numel() != 1&lt;/code&gt; (에도 희소 텐서의 경우).</target>
        </trans-unit>
        <trans-unit id="4546d55dc55434836d7155a943fded11ace1b513" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;input&lt;/code&gt; is a complex data type i.e., one of &lt;code&gt;torch.complex64&lt;/code&gt;, and &lt;code&gt;torch.complex128&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 데이터 유형이 복합 데이터 유형 인 경우, 즉 &lt;code&gt;torch.complex64&lt;/code&gt; 및 &lt;code&gt;torch.complex128&lt;/code&gt; 중 하나 인 경우 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="5ed711035f4f45a68bdc409be57f591a1203d447" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;input&lt;/code&gt; is a floating point data type i.e., one of &lt;code&gt;torch.float64&lt;/code&gt;, &lt;code&gt;torch.float32&lt;/code&gt; and &lt;code&gt;torch.float16&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 데이터 유형이 부동 소수점 데이터 유형, 즉 &lt;code&gt;torch.float64&lt;/code&gt; , &lt;code&gt;torch.float32&lt;/code&gt; 및 &lt;code&gt;torch.float16&lt;/code&gt; 중 하나 인 경우 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="5ad196c93762cde87d752d5a63b5607aaf9d27a9" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a complex data type.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 데이터 유형이 복합 데이터 유형 이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="47b01dfd12ff21d88fce8263233ddd92fb4ed8af" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a floating point data type.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 데이터 유형이 부동 소수점 데이터 유형 이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="81de301f63a48cfa0447acb0616f4e3a8050d38a" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a signed data type.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 데이터 유형이 부호 있는 데이터 유형이면 True를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="2b3d22229996fcea50652425a0ceef76726b9f91" translate="yes" xml:space="preserve">
          <source>Returns True if the global deterministic flag is turned on.</source>
          <target state="translated">글로벌 결정적 플래그가 켜져 있으면 True를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b5073df54cb09fc1a6407537c2e76e0147566896" translate="yes" xml:space="preserve">
          <source>Returns True if the global deterministic flag is turned on. Refer to &lt;a href=&quot;torch.set_deterministic#torch.set_deterministic&quot;&gt;&lt;code&gt;torch.set_deterministic()&lt;/code&gt;&lt;/a&gt; documentation for more details.</source>
          <target state="translated">글로벌 결정적 플래그가 켜져 있으면 True를 반환합니다. 자세한 내용은 &lt;a href=&quot;torch.set_deterministic#torch.set_deterministic&quot;&gt; &lt;code&gt;torch.set_deterministic()&lt;/code&gt; &lt;/a&gt; 문서를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="c4032dc2622bbc19b42824ae9a76253643ed72f9" translate="yes" xml:space="preserve">
          <source>Returns a 1-D tensor of size</source>
          <target state="translated">크기의 1 차원 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d2604008505b663501ba7ccd9df67158c244b1f4" translate="yes" xml:space="preserve">
          <source>Returns a 1-dimensional view of each input tensor with zero dimensions.</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 1 차원 뷰를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e25d75bccca92cff61915e5211fa5ae4cd4e83b9" translate="yes" xml:space="preserve">
          <source>Returns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 1 차원 뷰를 반환합니다. 차원이 하나 이상있는 입력 텐서는있는 그대로 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="f9b98594557024de39affd4c070595a178dad181" translate="yes" xml:space="preserve">
          <source>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</source>
          <target state="translated">대각선에 1이 있고 다른 곳에 0이있는 2 차원 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="1a93e2128169bc51774f54bd53d2eaec67c329a8" translate="yes" xml:space="preserve">
          <source>Returns a 2-dimensional view of each each input tensor with zero dimensions.</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 2 차원 뷰를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5d4b7116a250c6d886720ec99aa88d8cbeb63c35" translate="yes" xml:space="preserve">
          <source>Returns a 2-dimensional view of each each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 2 차원 뷰를 반환합니다. 차원이 두 개 이상인 입력 텐서는있는 그대로 반환됩니다. : param input : : type input : Tensor 또는 Tensor 목록</target>
        </trans-unit>
        <trans-unit id="5b208e9701580107f4e0baee12e48579cf84134e" translate="yes" xml:space="preserve">
          <source>Returns a 3-dimensional view of each each input tensor with zero dimensions.</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 3 차원 뷰를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="9be625c9292e0cee5fd9c2f9e9f3ae65376309fe" translate="yes" xml:space="preserve">
          <source>Returns a 3-dimensional view of each each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors</source>
          <target state="translated">차원이 0 인 각 입력 텐서의 3 차원 뷰를 반환합니다. 차원이 3 개 이상인 입력 텐서는있는 그대로 반환됩니다. : param input : : type input : Tensor 또는 Tensor 목록</target>
        </trans-unit>
        <trans-unit id="ca90d78c7ebd1efe1d3973c488edc158aa0b5d0e" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; object representing this distribution&amp;rsquo;s support.</source>
          <target state="translated">이 분포의 지원을 나타내는 &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt; 객체를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="ab61e9a8b5883304b4c24f113af9c2e2c089bc01" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object to a list of the passed in Futures.</source>
          <target state="translated">&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 에서 전달 된 목록에 Future 객체를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="cff2424065898889ca4c8ceba9391c00be8f3c4e" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object that can be waited on. When completed, the return value of &lt;code&gt;func&lt;/code&gt; on &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;kwargs&lt;/code&gt; can be retrieved from the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">기다릴 수 있는 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 객체를 반환합니다 . 완료되면 &lt;code&gt;args&lt;/code&gt; 및 &lt;code&gt;kwargs&lt;/code&gt; 에 대한 &lt;code&gt;func&lt;/code&gt; 의 반환 값을 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 객체 에서 검색 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="3012b7f55211365d28ddf4ab3e32e9d56916febf" translate="yes" xml:space="preserve">
          <source>Returns a CPU copy of this storage if it&amp;rsquo;s not already on the CPU</source>
          <target state="translated">이 스토리지가 아직 CPU에 있지 않은 경우이 스토리지의 CPU 사본을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="310ef068d2b0176ea9cd4ecb440e8d929300b581" translate="yes" xml:space="preserve">
          <source>Returns a DLPack representing the tensor.</source>
          <target state="translated">텐서를 나타내는 DLPack을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d270dbf83984f38c1be89175b564ec60887173d4" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the current scale, or 1.0 if scaling is disabled.</source>
          <target state="translated">현재 스케일을 포함하는 Python float를 반환하거나, 스케일링이 비활성화 된 경우 1.0을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="79f467145c77b5bb1f3f5bd6e7fabcaca51e20bd" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the scale backoff factor.</source>
          <target state="translated">스케일 백 오프 인자를 포함하는 Python 부동 소수점을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="fea0c1596123398a7d55be21927faed71a6ab193" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the scale growth factor.</source>
          <target state="translated">스케일 성장 인자를 포함하는 Python 부동 소수점을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7844351802a857a8df7b5d5cbf2c31ca4dbaa9b0" translate="yes" xml:space="preserve">
          <source>Returns a Python int containing the growth interval.</source>
          <target state="translated">성장 간격을 포함하는 Python int를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="8d50286946851edf8b261b9ee4ffcf514e96ce11" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;0&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 가 &lt;code&gt;0&lt;/code&gt; 으로 채워진 Tensor를 반환합니다 . 기본적으로 반환 된 Tensor에는이 Tensor와 동일한 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="0ab36bf75992b3e189a293c78e7fdb358c585560" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;1&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 가 &lt;code&gt;1&lt;/code&gt; 로 채워진 Tensor를 반환합니다 . 기본적으로 반환 된 Tensor에는이 Tensor와 동일한 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="aff928dda115238bd0307d57f02acbba9a324cfb" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; 로 채워진 크기 &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 의 Tensor를 반환합니다 . 기본적으로 반환 된 Tensor에는이 Tensor와 동일한 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="545a5b97b3133069c0e8dcc8e1ce77397a1e785a" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with uninitialized data. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">초기화되지 않은 데이터로 채워진 크기 &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 의 Tensor를 반환합니다 . 기본적으로 반환 된 Tensor에는이 Tensor와 동일한 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="e1bd30076d669876272e68e1e793c79f23bddd95" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as the Tensor &lt;code&gt;other&lt;/code&gt;. When &lt;code&gt;non_blocking&lt;/code&gt;, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When &lt;code&gt;copy&lt;/code&gt; is set, a new Tensor is created even when the Tensor already matches the desired conversion.</source>
          <target state="translated">텐서 동일로 반환 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 텐서로 &lt;code&gt;other&lt;/code&gt; . 때 &lt;code&gt;non_blocking&lt;/code&gt; 하는 CUDA 텐서에 고정 된 메모리와 CPU 텐서 변환 호스트 가능한 경우, 예를 들어, 관련하여 비동기 적으로 변환하려고합니다. 때 &lt;code&gt;copy&lt;/code&gt; 설정되어 텐서가 이미 원하는 변환을 일치하는 경우에도, 새로운 텐서가 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="7ff74c2d1bff3e0ce5952d69b71bd9a1e607f1aa" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with the specified &lt;a href=&quot;#torch.Tensor.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; and (optional) &lt;code&gt;dtype&lt;/code&gt;. If &lt;code&gt;dtype&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; it is inferred to be &lt;code&gt;self.dtype&lt;/code&gt;. When &lt;code&gt;non_blocking&lt;/code&gt;, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When &lt;code&gt;copy&lt;/code&gt; is set, a new Tensor is created even when the Tensor already matches the desired conversion.</source>
          <target state="translated">지정된 &lt;a href=&quot;#torch.Tensor.device&quot;&gt; &lt;code&gt;device&lt;/code&gt; &lt;/a&gt; 및 (선택 사항) &lt;code&gt;dtype&lt;/code&gt; 이 있는 Tensor를 반환합니다 . 경우 &lt;code&gt;dtype&lt;/code&gt; 없습니다 &lt;code&gt;None&lt;/code&gt; 은 것으로 추정된다 &lt;code&gt;self.dtype&lt;/code&gt; . 때 &lt;code&gt;non_blocking&lt;/code&gt; 하는 CUDA 텐서에 고정 된 메모리와 CPU 텐서 변환 호스트 가능한 경우, 예를 들어, 관련하여 비동기 적으로 변환하려고합니다. 때 &lt;code&gt;copy&lt;/code&gt; 설정되어 텐서가 이미 원하는 변환을 일치하는 경우에도, 새로운 텐서가 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="da569f76cd483df9524efcb31fcf2ea99e390904" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with the specified &lt;code&gt;dtype&lt;/code&gt;</source>
          <target state="translated">지정된 &lt;code&gt;dtype&lt;/code&gt; 을 가진 Tensor를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="da2702897f096fbbfeecf34312faf90efede2de3" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating if CUDA is currently available.</source>
          <target state="translated">CUDA를 현재 사용할 수 있는지 여부를 나타내는 부울을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6724b6006e294e6e49650c83c7e2700124a4da5f" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating if CUDNN is currently available.</source>
          <target state="translated">CUDNN을 현재 사용할 수 있는지 여부를 나타내는 부울을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="623f6de5f91be71aa002bb6ab0ba269d8348512a" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating whether this instance is enabled.</source>
          <target state="translated">이 인스턴스가 활성화되었는지 여부를 나타내는 부울을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="005b46932efde89028d44d377485c52b269ce9de" translate="yes" xml:space="preserve">
          <source>Returns a byte tensor of &lt;code&gt;sample_shape + batch_shape&lt;/code&gt; indicating whether each event in value satisfies this constraint.</source>
          <target state="translated">value의 각 이벤트가이 제약 조건을 충족하는지 여부를 나타내는 &lt;code&gt;sample_shape + batch_shape&lt;/code&gt; 의 바이트 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="e15fd6f4de5dc5fea9ffd2c5465a552199dc217b" translate="yes" xml:space="preserve">
          <source>Returns a contiguous in memory tensor containing the same data as &lt;code&gt;self&lt;/code&gt; tensor. If &lt;code&gt;self&lt;/code&gt; tensor is already in the specified memory format, this function returns the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 와 동일한 데이터를 포함하는 연속 메모리 텐서를 반환합니다 . 경우 &lt;code&gt;self&lt;/code&gt; 텐서가 지정된 메모리 포맷에 이미이 함수는 반환 &lt;code&gt;self&lt;/code&gt; 텐서를.</target>
        </trans-unit>
        <trans-unit id="27f4f3450faf371eb6306a54c86ee05877b30941" translate="yes" xml:space="preserve">
          <source>Returns a contraction of a and b over multiple dimensions.</source>
          <target state="translated">여러 차원에 걸쳐 a와 b의 축소를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="28370d731f31276aa7e934060964f2e577e96a76" translate="yes" xml:space="preserve">
          <source>Returns a copy of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 복사본을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="3318b58b5db87c6ad9827c719c9a9b9dbdc40b78" translate="yes" xml:space="preserve">
          <source>Returns a copy of the tensor in &lt;code&gt;torch.mkldnn&lt;/code&gt; layout.</source>
          <target state="translated">&lt;code&gt;torch.mkldnn&lt;/code&gt; 레이아웃 에서 텐서의 복사본을 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="f6881bd2839553790195cb237970005b22687467" translate="yes" xml:space="preserve">
          <source>Returns a copy of this object in CPU memory.</source>
          <target state="translated">CPU 메모리에있는이 개체의 복사본을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="bd445c8d669710f667e23d46ff8038e303167c04" translate="yes" xml:space="preserve">
          <source>Returns a copy of this object in CUDA memory.</source>
          <target state="translated">CUDA 메모리에있는이 객체의 복사본을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2d7501c9a5a1714d3c3684f56c90bd8b61fff5d3" translate="yes" xml:space="preserve">
          <source>Returns a copy of this storage</source>
          <target state="translated">이 저장소의 사본을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7d518eb5a72df432375d8394da8f628d15fd3b22" translate="yes" xml:space="preserve">
          <source>Returns a dictionary containing a whole state of the module.</source>
          <target state="translated">모듈의 전체 상태를 포함하는 사전을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7da6f33bf2a479dba2b7480dac5819fb18cbbc2f" translate="yes" xml:space="preserve">
          <source>Returns a dictionary from argument names to &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.</source>
          <target state="translated">인수 이름 에서이 분포의 각 인수에 의해 충족되어야 하는 &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt; 객체에 대한 사전을 반환합니다 . 텐서가 아닌 인수는이 딕셔너리에 나타날 필요가 없습니다.</target>
        </trans-unit>
        <trans-unit id="e46b2212af694a636c66b6d3e83683486d2db4d8" translate="yes" xml:space="preserve">
          <source>Returns a dictionary of CUDA memory allocator statistics for a given device.</source>
          <target state="translated">주어진 장치에 대한 CUDA 메모리 할당 자 통계 사전을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="915db01bc1c22d7e1017e2a0ebc05f0845ed5843" translate="yes" xml:space="preserve">
          <source>Returns a human-readable printout of the current memory allocator statistics for a given device.</source>
          <target state="translated">주어진 장치에 대한 현재 메모리 할당 자 통계의 사람이 읽을 수있는 인쇄물을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="dbff7cc734b7142fc6847934bd6e1e33f653b796" translate="yes" xml:space="preserve">
          <source>Returns a list containing the elements of this storage</source>
          <target state="translated">이 스토리지의 요소를 포함하는 목록을 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="97fd9a7e3427d9329387c86b5fc1fc50b3513d44" translate="yes" xml:space="preserve">
          <source>Returns a list of ByteTensor representing the random number states of all devices.</source>
          <target state="translated">모든 장치의 난수 상태를 나타내는 ByteTensor 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="dd4681a72abe05d09c99ad2246c5c6f58da6423c" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the &lt;code&gt;k&lt;/code&gt; th smallest element of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">리턴 namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 은 IS &lt;code&gt;k&lt;/code&gt; 의 각 행의 최소 요소 번째 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ee8ea74a50bcff655454904543731546a0784b94" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the &lt;code&gt;k&lt;/code&gt; th smallest element of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each element found.</source>
          <target state="translated">리턴 namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 은 IS &lt;code&gt;k&lt;/code&gt; 의 각 행의 최소 요소 번째 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 발견 된 각 요소의 인덱스 위치입니다.</target>
        </trans-unit>
        <trans-unit id="1e386080072251f678a51090a4553c8d16a045fa" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative maximum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 요소의 누적 최대가 &lt;code&gt;input&lt;/code&gt; 차원에서 &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="126e4b96929c50182f17c4bb4a58b7d0fe049898" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative maximum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 요소의 누적 최대가 &lt;code&gt;input&lt;/code&gt; 차원에서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 차원 &lt;code&gt;dim&lt;/code&gt; 에서 찾은 각 최대 값의 인덱스 위치입니다 .</target>
        </trans-unit>
        <trans-unit id="0588c1c3069407fd276a9514a421bf82e6313f49" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative minimum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 요소의 누적 최소 &lt;code&gt;input&lt;/code&gt; 차원에서 &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c882e37440105ee95efe87d7c7a08229fc1a4c09" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative minimum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 요소의 누적 최소 &lt;code&gt;input&lt;/code&gt; 차원에서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 차원 &lt;code&gt;dim&lt;/code&gt; 에서 찾은 각 최대 값의 인덱스 위치입니다 .</target>
        </trans-unit>
        <trans-unit id="967b821b8f0b7a1f96a4f7d0f792de5f0185c446" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the maximum value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found (argmax).</source>
          <target state="translated">리턴 namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 의 각 행의 최대 값 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 발견 된 각 최대 값 (argmax)의 인덱스 위치입니다.</target>
        </trans-unit>
        <trans-unit id="340d25e61485b289574a04d557988b6aad2f299c" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the median value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each median value found.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 의 각 행의 중간 값 인 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 발견 된 각 중앙값의 인덱스 위치입니다.</target>
        </trans-unit>
        <trans-unit id="6b24dd1ba3e9b5a8840a923ed9f1831a7b9b04bb" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the minimum value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each minimum value found (argmin).</source>
          <target state="translated">리턴 namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 의 각 행의 최소 값이고 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; . 그리고 &lt;code&gt;indices&lt;/code&gt; 는 발견 된 각 최소값 (argmin)의 인덱스 위치입니다.</target>
        </trans-unit>
        <trans-unit id="bc8471de2c101d3651a6d7a76fd12e82843e6661" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the mode value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, i.e.</source>
          <target state="translated">namedtuple 리턴 &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 의 각 행의 모드 값 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; , 즉</target>
        </trans-unit>
        <trans-unit id="a4cc2f3a360a06df266083c92ef6ae91e2b318f1" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the mode value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, i.e. a value which appears most often in that row, and &lt;code&gt;indices&lt;/code&gt; is the index location of each mode value found.</source>
          <target state="translated">리턴 namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; &lt;code&gt;values&lt;/code&gt; 의 각 행의 모드 값 &lt;code&gt;input&lt;/code&gt; 주어진 차원에서 텐서 &lt;code&gt;dim&lt;/code&gt; 즉 그 행에 자주 나타나며, 값, &lt;code&gt;indices&lt;/code&gt; 각각의 모드 값의 인덱스 위치를 찾을 수있다.</target>
        </trans-unit>
        <trans-unit id="55daf74ff9a749b34582eb6dc5a7e9467d0b17d0" translate="yes" xml:space="preserve">
          <source>Returns a new 1-D tensor which indexes the &lt;code&gt;input&lt;/code&gt; tensor according to the boolean mask &lt;code&gt;mask&lt;/code&gt; which is a &lt;code&gt;BoolTensor&lt;/code&gt;.</source>
          <target state="translated">반환 새로운 1-D 텐서되는 인덱싱 &lt;code&gt;input&lt;/code&gt; 부울 마스크에있어서 텐서 &lt;code&gt;mask&lt;/code&gt; A는 &lt;code&gt;BoolTensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cc996281b6d8b730ea490f35416e253843087c8c" translate="yes" xml:space="preserve">
          <source>Returns a new SparseTensor with values from Tensor &lt;code&gt;input&lt;/code&gt; filtered by indices of &lt;code&gt;mask&lt;/code&gt; and values are ignored. &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; must have the same shape.</source>
          <target state="translated">&lt;code&gt;mask&lt;/code&gt; 인덱스로 필터링 된 Tensor &lt;code&gt;input&lt;/code&gt; 값이있는 새 SparseTensor를 반환 하고 값은 무시됩니다. &lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mask&lt;/code&gt; 는 같은 모양이어야합니다.</target>
        </trans-unit>
        <trans-unit id="650add904aa71e494da203b253a7e5daaa3f4c87" translate="yes" xml:space="preserve">
          <source>Returns a new Tensor with &lt;code&gt;data&lt;/code&gt; as the tensor data. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">&lt;code&gt;data&lt;/code&gt; 를 텐서 데이터로 사용하여 새 텐서를 반환합니다 . 기본적으로 반환 된 Tensor에는이 Tensor와 동일한 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="6f5b88777af1b3fb2f4856550ea09e3d3ade90f2" translate="yes" xml:space="preserve">
          <source>Returns a new Tensor, detached from the current graph.</source>
          <target state="translated">현재 그래프에서 분리 된 새 Tensor를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="adfa10a0d579afab7d4a1e2dd4d592e805a24bbd" translate="yes" xml:space="preserve">
          <source>Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to &lt;code&gt;batch_shape&lt;/code&gt;. This method calls &lt;a href=&quot;tensors#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand&lt;/code&gt;&lt;/a&gt; on the distribution&amp;rsquo;s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in &lt;code&gt;__init__.py&lt;/code&gt;, when an instance is first created.</source>
          <target state="translated">&lt;code&gt;batch_shape&lt;/code&gt; 로 확장 된 배치 차원으로 새 배포 인스턴스를 반환하거나 파생 클래스에서 제공하는 기존 인스턴스를 채 웁니다 . 이 메서드 호출 은 분포의 모수를 &lt;a href=&quot;tensors#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand&lt;/code&gt; &lt;/a&gt; 합니다. 따라서 이것은 확장 된 배포 인스턴스에 대해 새 메모리를 할당하지 않습니다. 또한 인스턴스가 처음 생성 될 때 &lt;code&gt;__init__.py&lt;/code&gt; 에서 인수 확인 또는 매개 변수 브로드 캐스팅을 반복하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="b6cad31d8dba22f0073db8447536a302dee9ee8b" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing imaginary values of the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 허수 값을 포함하는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="a474356ccaab506872384349a8c7d3cc842edfca" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing imaginary values of the &lt;code&gt;self&lt;/code&gt; tensor. The returned tensor and &lt;code&gt;self&lt;/code&gt; share the same underlying storage.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 허수 값을 포함하는 새 텐서를 반환합니다 . 반환 된 텐서와 &lt;code&gt;self&lt;/code&gt; 는 동일한 기본 저장소를 공유합니다.</target>
        </trans-unit>
        <trans-unit id="0417e03c0fc21d8aa9cfd0ad69033f35d283ab8f" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing real values of the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 실제 값을 포함하는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="ecd577300f229280c9c5bc1fe369db0f8475652e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing real values of the &lt;code&gt;self&lt;/code&gt; tensor. The returned tensor and &lt;code&gt;self&lt;/code&gt; share the same underlying storage.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 실제 값을 포함하는 새 텐서를 반환합니다 . 반환 된 텐서와 &lt;code&gt;self&lt;/code&gt; 는 동일한 기본 저장소를 공유합니다.</target>
        </trans-unit>
        <trans-unit id="84c3a649630732036f386759b3c1fe0f4082f284" translate="yes" xml:space="preserve">
          <source>Returns a new tensor that is a narrowed version of &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 의 축소 버전 인 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="446ae7f4205e67fe0ead00f78a291ab93a7e711a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor that is a narrowed version of &lt;code&gt;input&lt;/code&gt; tensor. The dimension &lt;code&gt;dim&lt;/code&gt; is input from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;start + length&lt;/code&gt;. The returned tensor and &lt;code&gt;input&lt;/code&gt; tensor share the same underlying storage.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 의 축소 버전 인 새 텐서를 반환합니다 . 치수 &lt;code&gt;dim&lt;/code&gt; 은 &lt;code&gt;start&lt;/code&gt; 부터 &lt;code&gt;start + length&lt;/code&gt; 까지 입력됩니다 . 반환 된 텐서 및 &lt;code&gt;input&lt;/code&gt; 텐서는 동일한 기본 저장소를 공유합니다.</target>
        </trans-unit>
        <trans-unit id="d6897347512206bded239af0259e282422aa1908" translate="yes" xml:space="preserve">
          <source>Returns a new tensor which indexes the &lt;code&gt;input&lt;/code&gt; tensor along dimension &lt;code&gt;dim&lt;/code&gt; using the entries in &lt;code&gt;index&lt;/code&gt; which is a &lt;code&gt;LongTensor&lt;/code&gt;.</source>
          <target state="translated">반환 인덱스 새로운 텐서 &lt;code&gt;input&lt;/code&gt; 크기에 따라 텐서가 &lt;code&gt;dim&lt;/code&gt; 의 항목을 사용하여 &lt;code&gt;index&lt;/code&gt; A는 &lt;code&gt;LongTensor&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="d9c1e11a5f9f21aaf09f7c09b28d3efe9b79c1c9" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with a dimension of size one inserted at the specified position.</source>
          <target state="translated">지정된 위치에 1 차원 크기가 삽입 된 새 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="3224c262166ef14a2d46031572904d6b7dc8f710" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element is &lt;code&gt;finite&lt;/code&gt; or not.</source>
          <target state="translated">각 요소가 &lt;code&gt;finite&lt;/code&gt; 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="95e1f8a1ccb8a8da884d7e84169520f79117ec5e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is &amp;ldquo;close&amp;rdquo; to the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소가 &lt;code&gt;other&lt;/code&gt; 의 해당 요소에 &quot;가까운&quot;지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="4be234d2b824b3afbb8b6e3c023a9678a2fef898" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is &amp;ldquo;close&amp;rdquo; to the corresponding element of &lt;code&gt;other&lt;/code&gt;. Closeness is defined as:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소가 &lt;code&gt;other&lt;/code&gt; 의 해당 요소에 &quot;가까운&quot;지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 . 친밀 성은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="e9a96751e78a2d8505653806cfbbeebe8f0da987" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is NaN or not.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소 가 NaN인지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="f12a3acfcbbec29775876cc40c95d5a00ba9dd4a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소 가 NaN인지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 . 실수 및 / 또는 허수 부분이 NaN 인 경우 복잡한 값은 NaN으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="c183c7e7c59ca8cd11a1eb4078b588d714403acc" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is real-valued or not.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소 가 실수 인지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1d49a6f7124bd5cd437a7a8fee3ecaa6dd817cb5" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is real-valued or not. All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 각 요소 가 실수 인지 여부를 나타내는 부울 요소가있는 새 텐서를 반환합니다 . 모든 실제 값 유형은 실제 값으로 간주됩니다. 허수 부가 0 일 때 복잡한 값은 실수로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="fc57cc29526229bfe21bab4f3ac0fa280da67107" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in degrees to radians.</source>
          <target state="translated">Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in degrees to radians.</target>
        </trans-unit>
        <trans-unit id="e88584aef7be0fb19dcf3caab156e7c17acae22e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in radians to degrees.</source>
          <target state="translated">Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in radians to degrees.</target>
        </trans-unit>
        <trans-unit id="1fd6a07571fec21877bbfab5cd7f129520d284d3" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; rounded to the closest integer.</source>
          <target state="translated">Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; rounded to the closest integer.</target>
        </trans-unit>
        <trans-unit id="fbe9fedb3aa9a3f8d1c80fddb5c3023ad4eb0562" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the arcsine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the arcsine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e7301245f47a696980faac9e988f16c20bfda8b7" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the arctangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the arctangent of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="48ba42f2193edb51a142cab5570b31530effd843" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the ceil of the elements of &lt;code&gt;input&lt;/code&gt;, the smallest integer greater than or equal to each element.</source>
          <target state="translated">Returns a new tensor with the ceil of the elements of &lt;code&gt;input&lt;/code&gt; , the smallest integer greater than or equal to each element.</target>
        </trans-unit>
        <trans-unit id="ec380dfec0f03433bc5e3eb0dc32821c6d58c884" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the cosine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b0f7755628d00f3662eb39d02532aaee3f952862" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices.</source>
          <target state="translated">Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices.</target>
        </trans-unit>
        <trans-unit id="1c910712ac5b779edd94af13b8675d26b2ec9292" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.</source>
          <target state="translated">Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.</target>
        </trans-unit>
        <trans-unit id="303a77ef9e4e8ebea411b724b70cc880893ce1e5" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the exponential of the elements minus 1 of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the exponential of the elements minus 1 of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc8c5f6e874a9d0acbadee7550724c3fd5e88f46" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the exponential of the elements of the input tensor &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the exponential of the elements of the input tensor &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="66422619029217fb49c5540ea658003ab25d0251" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the floor of the elements of &lt;code&gt;input&lt;/code&gt;, the largest integer less than or equal to each element.</source>
          <target state="translated">Returns a new tensor with the floor of the elements of &lt;code&gt;input&lt;/code&gt; , the largest integer less than or equal to each element.</target>
        </trans-unit>
        <trans-unit id="54289275fc2999c18e519269b9c836fadd53ce09" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="60d4e278d41229d94159bb88a5a80098f0f17944" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="529a1d5ed854492ced22f20ca045b03cdb98828a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9966d85e500f92c101949dbcee56d3622b0b7c74" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the inverse hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="016516e7b6362bc2420ae10e09ad46e7409d58cb" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the inverse hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8624b8361034217c02a1c5e88fca1edddd5ce5dd" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the inverse hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="dfd8b16427d7265384f4ab1669de0b012956b1e0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logarithm to the base 10 of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the logarithm to the base 10 of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ca59edbe372ab054262e278a40d31de7be114996" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logarithm to the base 2 of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the logarithm to the base 2 of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="768981f99c3160fc5f2a6b954d047547f7f51a33" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="83c02ed817fb8ffcff4e801d08d12130cb9f1cab" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; is clamped to [eps, 1 - eps] when eps is not None. When eps is None and &lt;code&gt;input&lt;/code&gt; &amp;lt; 0 or &lt;code&gt;input&lt;/code&gt; &amp;gt; 1, the function will yields NaN.</source>
          <target state="translated">Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; is clamped to [eps, 1 - eps] when eps is not None. When eps is None and &lt;code&gt;input&lt;/code&gt; &amp;lt; 0 or &lt;code&gt;input&lt;/code&gt; &amp;gt; 1, the function will yields NaN.</target>
        </trans-unit>
        <trans-unit id="743fb44188d39b917a57a969a7a2bdfa54f07c5c" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the natural logarithm of (1 + &lt;code&gt;input&lt;/code&gt;).</source>
          <target state="translated">Returns a new tensor with the natural logarithm of (1 + &lt;code&gt;input&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="7802c5deb620e1b95914f599e220dfe558ee03c0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the natural logarithm of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the natural logarithm of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c9bebfac39f90f81173963e0ca1f126dea57da57" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the negative of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the negative of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a0595e3a74577061b95243faf09a3387b6e217a7" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the reciprocal of the elements of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">Returns a new tensor with the reciprocal of the elements of &lt;code&gt;input&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="40e790541f395ea8a715d66e7db7815ecda1464f" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the reciprocal of the square-root of each of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the reciprocal of the square-root of each of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="82f5d4bb888952c73ce51e1bac717329cb2869c0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the same data as the &lt;code&gt;self&lt;/code&gt; tensor but of a different &lt;code&gt;shape&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the same data as the &lt;code&gt;self&lt;/code&gt; tensor but of a different &lt;code&gt;shape&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="736d1d1a205da1b8f4c4709de8f6235951658a7e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the sigmoid of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the sigmoid of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5e2b36eb81199cdbe569cc67684536ac9defc3a0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the signs of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the signs of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="47f02b09b0f6a0ab4b9c1f34438e6706d93ade59" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the sine of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="75f7c6fa74ae5240454bfc9ef7f507262ffe9ed2" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the square of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the square of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b2f28004537b23138d7c295a5914cf3e65ab6fd0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the square-root of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the square-root of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="baf6cf4928d6ab113308458333184cd46b1dbec9" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the tangent of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c51c660d43995f5399bc591984337fa3aa3dd1db" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the truncated integer values of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a new tensor with the truncated integer values of the elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="71a9122ea30596ae8f8e0cfb84c73297b46f8b8e" translate="yes" xml:space="preserve">
          <source>Returns a new view of the &lt;code&gt;self&lt;/code&gt; tensor with singleton dimensions expanded to a larger size.</source>
          <target state="translated">Returns a new view of the &lt;code&gt;self&lt;/code&gt; tensor with singleton dimensions expanded to a larger size.</target>
        </trans-unit>
        <trans-unit id="04f242c1b997d817d7997eb7a445d6e3823e9344" translate="yes" xml:space="preserve">
          <source>Returns a partial view of &lt;code&gt;input&lt;/code&gt; with the its diagonal elements with respect to &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt; appended as a dimension at the end of the shape.</source>
          <target state="translated">Returns a partial view of &lt;code&gt;input&lt;/code&gt; with the its diagonal elements with respect to &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt; appended as a dimension at the end of the shape.</target>
        </trans-unit>
        <trans-unit id="d3032b88cf4f390e50cfecedba4f33edc6cf5043" translate="yes" xml:space="preserve">
          <source>Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; for details.</source>
          <target state="translated">Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; for details.</target>
        </trans-unit>
        <trans-unit id="21aab95a18e188886df073751e7d106cd6f9db59" translate="yes" xml:space="preserve">
          <source>Returns a random permutation of integers from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;n - 1&lt;/code&gt;.</source>
          <target state="translated">Returns a random permutation of integers from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;n - 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="af613cf5c56817cafaa6ac101470a6f4a4596731" translate="yes" xml:space="preserve">
          <source>Returns a result tensor where each</source>
          <target state="translated">Returns a result tensor where each</target>
        </trans-unit>
        <trans-unit id="38827e466d04bcb489dba158a3d8570409e939d7" translate="yes" xml:space="preserve">
          <source>Returns a set of sharing strategies supported on a current system.</source>
          <target state="translated">현재 시스템에서 지원되는 공유 전략 세트를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a40e726bb013fd09820ae8ca46b5c0798f7ebbbb" translate="yes" xml:space="preserve">
          <source>Returns a snapshot of the CUDA memory allocator state across all devices.</source>
          <target state="translated">모든 장치에서 CUDA 메모리 할당 자 상태의 스냅 샷을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a13c7d88cad174382717397efbd3cfcb039a6695" translate="yes" xml:space="preserve">
          <source>Returns a sparse copy of the tensor. PyTorch supports sparse tensors in &lt;a href=&quot;sparse#sparse-docs&quot;&gt;coordinate format&lt;/a&gt;.</source>
          <target state="translated">Returns a sparse copy of the tensor. PyTorch supports sparse tensors in &lt;a href=&quot;sparse#sparse-docs&quot;&gt;coordinate format&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="d0a96d92340e65cc2a84c22d4d7b7ff9aa5fc4fe" translate="yes" xml:space="preserve">
          <source>Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</source>
          <target state="translated">Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</target>
        </trans-unit>
        <trans-unit id="597e1ef5a6ebc351d6f9d2b405bdd9101ff81520" translate="yes" xml:space="preserve">
          <source>Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. This graph will be preprocessed to inline all function and method calls. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</source>
          <target state="translated">Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. This graph will be preprocessed to inline all function and method calls. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</target>
        </trans-unit>
        <trans-unit id="bbb10017036639ab22d4a14313f02dc34ac957a3" translate="yes" xml:space="preserve">
          <source>Returns a tensor containing the indices of all non-zero elements of &lt;code&gt;input&lt;/code&gt;. Each row in the result contains the indices of a non-zero element in &lt;code&gt;input&lt;/code&gt;. The result is sorted lexicographically, with the last index changing the fastest (C-style).</source>
          <target state="translated">Returns a tensor containing the indices of all non-zero elements of &lt;code&gt;input&lt;/code&gt; . Each row in the result contains the indices of a non-zero element in &lt;code&gt;input&lt;/code&gt; . The result is sorted lexicographically, with the last index changing the fastest (C-style).</target>
        </trans-unit>
        <trans-unit id="92387ca99fbc648117949c0806c35a077690e5b2" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</source>
          <target state="translated">Returns a tensor filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</target>
        </trans-unit>
        <trans-unit id="a3052366ee17d89fee66f156e0077cbbe80ebcf2" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random numbers from a normal distribution with mean &lt;code&gt;0&lt;/code&gt; and variance &lt;code&gt;1&lt;/code&gt; (also called the standard normal distribution).</source>
          <target state="translated">Returns a tensor filled with random numbers from a normal distribution with mean &lt;code&gt;0&lt;/code&gt; and variance &lt;code&gt;1&lt;/code&gt; (also called the standard normal distribution).</target>
        </trans-unit>
        <trans-unit id="071f27bb3cbfbdac0a3664bb5e1e9b570135a766" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random numbers from a uniform distribution on the interval</source>
          <target state="translated">Returns a tensor filled with random numbers from a uniform distribution on the interval</target>
        </trans-unit>
        <trans-unit id="f6ccb0d318d2aea6d35f4cd94e4134bbdd4f3636" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt; , with the same size as &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5053ad71aa6f4e2e9cdde0c5980a9be98d8e91ca" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.zeros_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt; , with the same size as &lt;code&gt;input&lt;/code&gt; . &lt;code&gt;torch.zeros_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="53e8d496bf6d2306beb97ee488fa23a9b84ced51" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt; , with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="19bc36fafde43e2e6ce27a396db27c69f93641b5" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt; , with the same size as &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="45f89a095c06d9b0650d201a4622d88088b3f6da" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.ones_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt; , with the same size as &lt;code&gt;input&lt;/code&gt; . &lt;code&gt;torch.ones_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="380e4f75952135cee58ac0cb34dec6379c82103a" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt; , with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2619f65f9f0c07b8f2554ca2a356bbee1fb49067" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data.</source>
          <target state="translated">Returns a tensor filled with uninitialized data.</target>
        </trans-unit>
        <trans-unit id="6c9d253aa1bcbf44289c2a22a9efa2be86cdae22" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;stride&lt;/code&gt; respectively. &lt;code&gt;torch.empty_strided(size, stride)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(size).as_strided(size, stride)&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;stride&lt;/code&gt; respectively. &lt;code&gt;torch.empty_strided(size, stride)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(size).as_strided(size, stride)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="480a441694f9fb4e384f61d0c285c3346aba2867" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5a9df9f8590e35dc409160c4e4da50e8473fc470" translate="yes" xml:space="preserve">
          <source>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</source>
          <target state="translated">Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</target>
        </trans-unit>
        <trans-unit id="ef9879439829f78c493eca54baa6bd3b9da4b6a9" translate="yes" xml:space="preserve">
          <source>Returns a tensor of the same size as &lt;code&gt;input&lt;/code&gt; with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in &lt;code&gt;input&lt;/code&gt; i.e.,</source>
          <target state="translated">Returns a tensor of the same size as &lt;code&gt;input&lt;/code&gt; with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in &lt;code&gt;input&lt;/code&gt; i.e.,</target>
        </trans-unit>
        <trans-unit id="dee1e5e90adcef58cad4a12a8016b3c535cc3f0d" translate="yes" xml:space="preserve">
          <source>Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3c3720e104640e1187905d7635a3f6f3703b6e8e" translate="yes" xml:space="preserve">
          <source>Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt;. The given dimensions &lt;code&gt;dim0&lt;/code&gt; and &lt;code&gt;dim1&lt;/code&gt; are swapped.</source>
          <target state="translated">Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt; . The given dimensions &lt;code&gt;dim0&lt;/code&gt; and &lt;code&gt;dim1&lt;/code&gt; are swapped.</target>
        </trans-unit>
        <trans-unit id="bc2a3afc167973637c7f4b6c69bc81e1a7b7c261" translate="yes" xml:space="preserve">
          <source>Returns a tensor where each row contains &lt;code&gt;num_samples&lt;/code&gt; indices sampled from the multinomial probability distribution located in the corresponding row of tensor &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor where each row contains &lt;code&gt;num_samples&lt;/code&gt; indices sampled from the multinomial probability distribution located in the corresponding row of tensor &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="bfd4b9d9504c67da6e3c81783a008d55415316b9" translate="yes" xml:space="preserve">
          <source>Returns a tensor where each sub-tensor of &lt;code&gt;input&lt;/code&gt; along dimension &lt;code&gt;dim&lt;/code&gt; is normalized such that the &lt;code&gt;p&lt;/code&gt;-norm of the sub-tensor is lower than the value &lt;code&gt;maxnorm&lt;/code&gt;</source>
          <target state="translated">Returns a tensor where each sub-tensor of &lt;code&gt;input&lt;/code&gt; along dimension &lt;code&gt;dim&lt;/code&gt; is normalized such that the &lt;code&gt;p&lt;/code&gt; -norm of the sub-tensor is lower than the value &lt;code&gt;maxnorm&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a3a6b5909320cbc074af2544b9626c6f4bda5986" translate="yes" xml:space="preserve">
          <source>Returns a tensor with all the dimensions of &lt;code&gt;input&lt;/code&gt; of size &lt;code&gt;1&lt;/code&gt; removed.</source>
          <target state="translated">Returns a tensor with all the dimensions of &lt;code&gt;input&lt;/code&gt; of size &lt;code&gt;1&lt;/code&gt; removed.</target>
        </trans-unit>
        <trans-unit id="d4dca01252775c3a510432e13b2a50bd68aaad47" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt;, but with the specified shape.</source>
          <target state="translated">Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt; , but with the specified shape.</target>
        </trans-unit>
        <trans-unit id="10ea5ae4aa9d17fa5857d769438a3aa3d3b7e12d" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt;, but with the specified shape. When possible, the returned tensor will be a view of &lt;code&gt;input&lt;/code&gt;. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</source>
          <target state="translated">Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt; , but with the specified shape. When possible, the returned tensor will be a view of &lt;code&gt;input&lt;/code&gt; . Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</target>
        </trans-unit>
        <trans-unit id="859366eeae9c89a2d4641ddcda6c14627157bccd" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;self&lt;/code&gt; but with the specified shape. This method returns a view if &lt;code&gt;shape&lt;/code&gt; is compatible with the current shape. See &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">Returns a tensor with the same data and number of elements as &lt;code&gt;self&lt;/code&gt; but with the specified shape. This method returns a view if &lt;code&gt;shape&lt;/code&gt; is compatible with the current shape. See &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt; on when it is possible to return a view.</target>
        </trans-unit>
        <trans-unit id="4ddaf7cc67581deb40ac49cd798acd828948f3ec" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same shape as Tensor &lt;code&gt;input&lt;/code&gt; filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</source>
          <target state="translated">Returns a tensor with the same shape as Tensor &lt;code&gt;input&lt;/code&gt; filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</target>
        </trans-unit>
        <trans-unit id="f4d7f431450f08dca6e6bf02ccea73d644849cfb" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="03ce7caeebb968b62c8d34b90b828d1ce8423f2f" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. &lt;code&gt;torch.full_like(input, fill_value)&lt;/code&gt; is equivalent to &lt;code&gt;torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt; . &lt;code&gt;torch.full_like(input, fill_value)&lt;/code&gt; is equivalent to &lt;code&gt;torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c4b1fea8767410865166f5ee9867a018bc11da8d" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1.</source>
          <target state="translated">Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1.</target>
        </trans-unit>
        <trans-unit id="ea94290136057228a379eae661032b4dbc67bcaf" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1. &lt;code&gt;torch.randn_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1. &lt;code&gt;torch.randn_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0152430be2c09d5c5103cb904fde37d1ace3b55a" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a uniform distribution on the interval</source>
          <target state="translated">Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a uniform distribution on the interval</target>
        </trans-unit>
        <trans-unit id="823c3c765d13781fa9b963395a22444e3b621c72" translate="yes" xml:space="preserve">
          <source>Returns a tuple of 1-D tensors, one for each dimension in &lt;code&gt;input&lt;/code&gt;, each containing the indices (in that dimension) of all non-zero elements of &lt;code&gt;input&lt;/code&gt; .</source>
          <target state="translated">Returns a tuple of 1-D tensors, one for each dimension in &lt;code&gt;input&lt;/code&gt; , each containing the indices (in that dimension) of all non-zero elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fcd5e2fc8161aab0c115280174b5bb6b68911439" translate="yes" xml:space="preserve">
          <source>Returns a tuple of all slices along a given dimension, already without it.</source>
          <target state="translated">Returns a tuple of all slices along a given dimension, already without it.</target>
        </trans-unit>
        <trans-unit id="f9c24c4caad4a1ea9a709ef4d25a5add23c5abc6" translate="yes" xml:space="preserve">
          <source>Returns a tuple of tensors as &lt;code&gt;(the pivots, the L tensor, the U tensor)&lt;/code&gt;.</source>
          <target state="translated">Returns a tuple of tensors as &lt;code&gt;(the pivots, the L tensor, the U tensor)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cad07505f53debb40a34d79ccc9f58807d93a22d" translate="yes" xml:space="preserve">
          <source>Returns a tuple of:</source>
          <target state="translated">Returns a tuple of:</target>
        </trans-unit>
        <trans-unit id="48ec84de6060a777e4b133c0cc4cbb5c195b63f4" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor.</source>
          <target state="translated">Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor.</target>
        </trans-unit>
        <trans-unit id="cbd534c7cdca6e762bf6f1efb8dbc311c8d5604b" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</source>
          <target state="translated">Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d5219316aa3ad16a6268bf6cb076669b071e63b0" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor.</source>
          <target state="translated">Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor.</target>
        </trans-unit>
        <trans-unit id="feec717f015dd3a3543847bb4922909ce0df9768" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</source>
          <target state="translated">Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b19c812d92695d71cc123c4309dfdfb970e00990" translate="yes" xml:space="preserve">
          <source>Returns a view of the original tensor which contains all slices of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; from &lt;code&gt;self&lt;/code&gt; tensor in the dimension &lt;code&gt;dimension&lt;/code&gt;.</source>
          <target state="translated">Returns a view of the original tensor which contains all slices of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; from &lt;code&gt;self&lt;/code&gt; tensor in the dimension &lt;code&gt;dimension&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="af681be32c7d06253ca1c1b67c18e2b4ecd6f876" translate="yes" xml:space="preserve">
          <source>Returns a view of the original tensor with its dimensions permuted.</source>
          <target state="translated">Returns a view of the original tensor with its dimensions permuted.</target>
        </trans-unit>
        <trans-unit id="92e60f922f107927ce27990d6a2076704c10ee26" translate="yes" xml:space="preserve">
          <source>Returns an IPC handle of this event. If not recorded yet, the event will use the current device.</source>
          <target state="translated">이 이벤트의 IPC 핸들을 반환합니다. 아직 녹화되지 않은 경우 이벤트는 현재 장치를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="c55cff2ceacf5489627870098b732610149ca753" translate="yes" xml:space="preserve">
          <source>Returns an fp32 Tensor by dequantizing a quantized Tensor</source>
          <target state="translated">Returns an fp32 Tensor by dequantizing a quantized Tensor</target>
        </trans-unit>
        <trans-unit id="8a760468481f662aec69f0d66a0af21b8d01fcc9" translate="yes" xml:space="preserve">
          <source>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</source>
          <target state="translated">Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</target>
        </trans-unit>
        <trans-unit id="ac68eb8bdb577f36ea46f58e6159bc7bdc815476" translate="yes" xml:space="preserve">
          <source>Returns an iterator over all modules in the network.</source>
          <target state="translated">Returns an iterator over all modules in the network.</target>
        </trans-unit>
        <trans-unit id="94fc7e2a384a791953b1e205101832ccf08471ae" translate="yes" xml:space="preserve">
          <source>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</source>
          <target state="translated">Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</target>
        </trans-unit>
        <trans-unit id="06d88a90e624553bccb897dec896158a9e8cbe10" translate="yes" xml:space="preserve">
          <source>Returns an iterator over immediate children modules.</source>
          <target state="translated">Returns an iterator over immediate children modules.</target>
        </trans-unit>
        <trans-unit id="710ada787bca81e41a3d40e51f40a7c77bf13dde" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</source>
          <target state="translated">Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</target>
        </trans-unit>
        <trans-unit id="6eff830e357832045671934c63ceb5cb2fcb5978" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module buffers.</source>
          <target state="translated">Returns an iterator over module buffers.</target>
        </trans-unit>
        <trans-unit id="b1b975207d4ac5172ef30d4d877fa0c9ee98199a" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</source>
          <target state="translated">Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</target>
        </trans-unit>
        <trans-unit id="aa5e4dda23bfdd55410c837a325f1f9ec0f370cb" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module parameters.</source>
          <target state="translated">Returns an iterator over module parameters.</target>
        </trans-unit>
        <trans-unit id="48fce4dac8987007e7e689346df86ef5aaaf8d0c" translate="yes" xml:space="preserve">
          <source>Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="25a01c60e025b196abbdae1e228f044292cc80b3" translate="yes" xml:space="preserve">
          <source>Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.empty_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt; . &lt;code&gt;torch.empty_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ab487b617ce0598b5cf2a7578485a234f9dec28c" translate="yes" xml:space="preserve">
          <source>Returns cosine similarity between</source>
          <target state="translated">Returns cosine similarity between</target>
        </trans-unit>
        <trans-unit id="e36c4e3a7a9f1d367dee6c46085960ff3fa5e136" translate="yes" xml:space="preserve">
          <source>Returns cosine similarity between x1 and x2, computed along dim.</source>
          <target state="translated">Returns cosine similarity between x1 and x2, computed along dim.</target>
        </trans-unit>
        <trans-unit id="e9ee7bb84b7690f6d9a479786aae93fb8de0a027" translate="yes" xml:space="preserve">
          <source>Returns cublasHandle_t pointer to current cuBLAS handle</source>
          <target state="translated">현재 cuBLAS 핸들에 대한 cublasHandle_t 포인터를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="415079d9dd9bf0c3a5f12e73dab28b75c28128b7" translate="yes" xml:space="preserve">
          <source>Returns either a complex tensor of size</source>
          <target state="translated">Returns either a complex tensor of size</target>
        </trans-unit>
        <trans-unit id="a4b4474f9674a58aa04ea930d85c7a5dc2c0abdc" translate="yes" xml:space="preserve">
          <source>Returns entropy of distribution, batched over batch_shape.</source>
          <target state="translated">batch_shape에 배치 된 분포 엔트로피를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="dfc3ffdf3e62461b544a4b95b691f57d57a7486b" translate="yes" xml:space="preserve">
          <source>Returns list CUDA architectures this library was compiled for.</source>
          <target state="translated">이 라이브러리가 컴파일 된 CUDA 아키텍처 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="efcc116701cc62cfdac8737d78b2800ae566068b" translate="yes" xml:space="preserve">
          <source>Returns perplexity of distribution, batched over batch_shape.</source>
          <target state="translated">batch_shape에 일괄 처리 된 배포의 복잡성을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="bdbba2c4c769cf86fe1ade7dbe8c1a8c12ab8f3b" translate="yes" xml:space="preserve">
          <source>Returns scaled outputs. If this instance of &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;GradScaler&lt;/code&gt;&lt;/a&gt; is not enabled, outputs are returned unmodified.</source>
          <target state="translated">스케일링 된 출력을 반환합니다. 이 &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;GradScaler&lt;/code&gt; &lt;/a&gt; 인스턴스 가 활성화되지 않은 경우 출력은 수정되지 않은 상태로 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="468f8e57e6feeaa4ca9cfb85917c23990eabfb8c" translate="yes" xml:space="preserve">
          <source>Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be &lt;code&gt;(cardinality,) + batch_shape + event_shape&lt;/code&gt; (where &lt;code&gt;event_shape = ()&lt;/code&gt; for univariate distributions).</source>
          <target state="translated">이산 분포에서 지원하는 모든 값을 포함하는 텐서를 반환합니다. 결과는 차원 0에 대해 열거되므로 결과의 모양은 &lt;code&gt;(cardinality,) + batch_shape + event_shape&lt;/code&gt; (일 변량 분포의 경우 &lt;code&gt;event_shape = ()&lt;/code&gt; )가됩니다.</target>
        </trans-unit>
        <trans-unit id="d89b2ba3d69dbf0c5536d8dba9d4a8221b96833a" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors. See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</source>
          <target state="translated">Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors. See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</target>
        </trans-unit>
        <trans-unit id="3a64cb8b4d560048c2d03eaad56133c0e7564a88" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt;. See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</source>
          <target state="translated">Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt; . See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</target>
        </trans-unit>
        <trans-unit id="615a0842a40834e6013343600f3be5a6a5475da4" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors.</source>
          <target state="translated">Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors.</target>
        </trans-unit>
        <trans-unit id="6d1fdb76adea0e220c4916182503a08888092e2a" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt;.</source>
          <target state="translated">Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="bf82a1ecc16fe682cb2b7f0a986e6561051951df" translate="yes" xml:space="preserve">
          <source>Returns the &lt;code&gt;k&lt;/code&gt; largest elements of the given &lt;code&gt;input&lt;/code&gt; tensor along a given dimension.</source>
          <target state="translated">주어진 차원을 따라 주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 &lt;code&gt;k&lt;/code&gt; 개의 가장 큰 요소를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="8bed4d210c184a7836660a5c8e790f223aef9984" translate="yes" xml:space="preserve">
          <source>Returns the Generator state as a &lt;code&gt;torch.ByteTensor&lt;/code&gt;.</source>
          <target state="translated">Generator 상태를 &lt;code&gt;torch.ByteTensor&lt;/code&gt; 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b326f89b0009d9a68d7ef03ea15be0ad2d1ff47f" translate="yes" xml:space="preserve">
          <source>Returns the LU solve of the linear system</source>
          <target state="translated">선형 시스템의 LU 풀이를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="f7e41a542d6bed1cb096c1ed65295fbd9d0b0caa" translate="yes" xml:space="preserve">
          <source>Returns the address of the first element of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 첫 번째 요소 주소를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="2dcc3de7b8c8581e1cdbe092b04332f6662ee7fa" translate="yes" xml:space="preserve">
          <source>Returns the backend of the given process group.</source>
          <target state="translated">주어진 프로세스 그룹의 백엔드를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0ff2118aeefe8e264108f6bd90694f18e03d42e7" translate="yes" xml:space="preserve">
          <source>Returns the cross product of vectors in dimension &lt;code&gt;dim&lt;/code&gt; of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">반환 차원 벡터의 외적은 &lt;code&gt;dim&lt;/code&gt; 의 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ac3e83f04f1e5b5b04b0171c84cf8b30b1da0e51" translate="yes" xml:space="preserve">
          <source>Returns the cumulative density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">평가에서의 누적 밀도 / 질량 함수 리턴 &lt;code&gt;value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3ea80ae8957ba72ac8ddc6e085e6f544e8ae8c05" translate="yes" xml:space="preserve">
          <source>Returns the cumulative product of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">차원 &lt;code&gt;dim&lt;/code&gt; 에있는 &lt;code&gt;input&lt;/code&gt; 요소의 누적 곱을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="672aa95c90df23603089d78e5a986ac0c7ee6f37" translate="yes" xml:space="preserve">
          <source>Returns the cumulative sum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">차원 &lt;code&gt;dim&lt;/code&gt; 에있는 &lt;code&gt;input&lt;/code&gt; 요소의 누적 합계를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="543b4e544f696735e42e91a48b1a05b0d0275c5e" translate="yes" xml:space="preserve">
          <source>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</source>
          <target state="translated">주어진 장치에 대해 캐싱 할당자가 관리하는 현재 GPU 메모리 (바이트)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="353af166fcd46553479e01ad58f7b83a7e79e6b9" translate="yes" xml:space="preserve">
          <source>Returns the current GPU memory occupied by tensors in bytes for a given device.</source>
          <target state="translated">주어진 장치에 대해 텐서가 차지하는 현재 GPU 메모리를 바이트 단위로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="96ed2eb11bac807a9796664f42b21924320e0776" translate="yes" xml:space="preserve">
          <source>Returns the current random seed of the current GPU.</source>
          <target state="translated">현재 GPU의 현재 임의 시드를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e5e450695734676a7d659d903827db864e4c16d8" translate="yes" xml:space="preserve">
          <source>Returns the current strategy for sharing CPU tensors.</source>
          <target state="translated">CPU 텐서를 공유하기위한 현재 전략을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="67bc3c1de929ca35a480af3381de6be2f62d0bd8" translate="yes" xml:space="preserve">
          <source>Returns the currently selected &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for a given device.</source>
          <target state="translated">주어진 장치에 대해 현재 선택된 &lt;a href=&quot;#torch.cuda.Stream&quot;&gt; &lt;code&gt;Stream&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="214e82956aaf7597b71c309353db304a3f55c7e8" translate="yes" xml:space="preserve">
          <source>Returns the default &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for a given device.</source>
          <target state="translated">주어진 장치에 대한 기본 &lt;a href=&quot;#torch.cuda.Stream&quot;&gt; &lt;code&gt;Stream&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="59028083782bdbe4c0f1ae0141a2a0d44a986b01" translate="yes" xml:space="preserve">
          <source>Returns the index of a currently selected device.</source>
          <target state="translated">현재 선택된 장치의 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="711b80d2744d16d4fcb6dea10d90a382b486d415" translate="yes" xml:space="preserve">
          <source>Returns the indices of the buckets to which each value in the &lt;code&gt;input&lt;/code&gt; belongs, where the boundaries of the buckets are set by &lt;code&gt;boundaries&lt;/code&gt;.</source>
          <target state="translated">각 값되는 버킷의 인덱스를 반환 &lt;code&gt;input&lt;/code&gt; 버킷의 경계가 설정 속한 &lt;code&gt;boundaries&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8f4a40b7b47151439ff4a946635582981d1e9915" translate="yes" xml:space="preserve">
          <source>Returns the indices of the buckets to which each value in the &lt;code&gt;input&lt;/code&gt; belongs, where the boundaries of the buckets are set by &lt;code&gt;boundaries&lt;/code&gt;. Return a new tensor with the same size as &lt;code&gt;input&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="translated">각 값되는 버킷의 인덱스를 반환 &lt;code&gt;input&lt;/code&gt; 버킷의 경계가 설정 속한 &lt;code&gt;boundaries&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; 과 크기가 같은 새 텐서를 반환합니다 . 경우 &lt;code&gt;right&lt;/code&gt; (기본값) False입니다, 다음 왼쪽 경계가 닫힙니다. 보다 공식적으로 반환 된 인덱스는 다음 규칙을 충족합니다.</target>
        </trans-unit>
        <trans-unit id="8e89c9544d1eb7458a38f74393c854c8ecda06cb" translate="yes" xml:space="preserve">
          <source>Returns the indices of the lower triangular part of a &lt;code&gt;row&lt;/code&gt;-by- &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</source>
          <target state="translated">리턴의 하 삼각 부분의 인덱스 &lt;code&gt;row&lt;/code&gt; -by- &lt;code&gt;col&lt;/code&gt; 첫 번째 행에 모든 인덱스 및 상기 제 2 열은 열 좌표를 포함 행의 좌표를 포함하는 2 바이 N 텐서의 매트릭스.</target>
        </trans-unit>
        <trans-unit id="2a481df57e472778d81eb9d047f3b776a831dacb" translate="yes" xml:space="preserve">
          <source>Returns the indices of the lower triangular part of a &lt;code&gt;row&lt;/code&gt;-by- &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.</source>
          <target state="translated">리턴의 하 삼각 부분의 인덱스 &lt;code&gt;row&lt;/code&gt; -by- &lt;code&gt;col&lt;/code&gt; 첫 번째 행에 모든 인덱스 및 상기 제 2 열은 열 좌표를 포함 행의 좌표를 포함하는 2 바이 N 텐서의 매트릭스. 인덱스는 행과 열을 기준으로 정렬됩니다.</target>
        </trans-unit>
        <trans-unit id="36d1fcc471f42fd072f6a49d3437bd2fb723f49b" translate="yes" xml:space="preserve">
          <source>Returns the indices of the maximum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 최대 값 인덱스를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="27e8f13c991f81ab6eaf5ea019cec0319d74f97f" translate="yes" xml:space="preserve">
          <source>Returns the indices of the maximum values of a tensor across a dimension.</source>
          <target state="translated">차원에서 텐서의 최대 값 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="02a6d25f17c18c20395b7dbfe38496f775ac3649" translate="yes" xml:space="preserve">
          <source>Returns the indices of the minimum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 최소값 인덱스를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="c7c9c6f3bffff8c19d01d21b7330abaf2fed0de4" translate="yes" xml:space="preserve">
          <source>Returns the indices of the minimum values of a tensor across a dimension.</source>
          <target state="translated">차원에서 텐서의 최소값 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6112a2a2de721ce77e0fbceb09a5d99e7c8b1b09" translate="yes" xml:space="preserve">
          <source>Returns the indices of the upper triangular part of a &lt;code&gt;row&lt;/code&gt; by &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</source>
          <target state="translated">리턴의 상위 삼각형 부분의 인덱스를 &lt;code&gt;row&lt;/code&gt; 함으로써 &lt;code&gt;col&lt;/code&gt; 첫 번째 행은 모든 인덱스 및 상기 제 2 열은 열 좌표를 포함 행의 좌표를 포함하는 2 바이 N 텐서의 매트릭스.</target>
        </trans-unit>
        <trans-unit id="0c610a94f21c0e29b9389ef4636aaf00f7bcfcca" translate="yes" xml:space="preserve">
          <source>Returns the indices of the upper triangular part of a &lt;code&gt;row&lt;/code&gt; by &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.</source>
          <target state="translated">리턴의 상위 삼각형 부분의 인덱스를 &lt;code&gt;row&lt;/code&gt; 함으로써 &lt;code&gt;col&lt;/code&gt; 첫 번째 행은 모든 인덱스 및 상기 제 2 열은 열 좌표를 포함 행의 좌표를 포함하는 2 바이 N 텐서의 매트릭스. 인덱스는 행과 열을 기준으로 정렬됩니다.</target>
        </trans-unit>
        <trans-unit id="c38cd5cab4c0cbb6855ee937ca43e344a716d2eb" translate="yes" xml:space="preserve">
          <source>Returns the indices that sort a tensor along a given dimension in ascending order by value.</source>
          <target state="translated">주어진 차원을 따라 텐서를 값별로 오름차순으로 정렬하는 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="12d3b5fef61c4a9ef299d2984d5e7f942e44587a" translate="yes" xml:space="preserve">
          <source>Returns the information about the current &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; iterator worker process.</source>
          <target state="translated">현재 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 반복기 작업자 프로세스 에 대한 정보를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="528f25a31d271ea8c283f1c3f903d83097bb316f" translate="yes" xml:space="preserve">
          <source>Returns the initial seed for generating random numbers as a Python &lt;code&gt;long&lt;/code&gt;.</source>
          <target state="translated">난수를 생성하기위한 초기 시드를 Python &lt;code&gt;long&lt;/code&gt; 으로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b3583d38607467b25587c688245c573295c38448" translate="yes" xml:space="preserve">
          <source>Returns the initial seed for generating random numbers.</source>
          <target state="translated">난수 생성을위한 초기 시드를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d3aa84ac404b56cbb6665579407061f3f1829f6d" translate="yes" xml:space="preserve">
          <source>Returns the inverse &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; of this transform. This should satisfy &lt;code&gt;t.inv.inv is t&lt;/code&gt;.</source>
          <target state="translated">역이 반환 &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt; &lt;code&gt;Transform&lt;/code&gt; &lt;/a&gt; 의이 변환. 이것은 &lt;code&gt;t.inv.inv is t&lt;/code&gt; 를 만족해야합니다 .</target>
        </trans-unit>
        <trans-unit id="c1aef1f541873f46619d10b39465c4cebb0a9884" translate="yes" xml:space="preserve">
          <source>Returns the inverse cumulative density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">반환에서 평가 역 누적 밀도 / 질량 함수 &lt;code&gt;value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f756dfcdc4b88f47cc8afe02c13bb828b05c63e7" translate="yes" xml:space="preserve">
          <source>Returns the log of summed exponentials of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 합산 지수 로그를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b6389ff7f37373c9ca57e71fc477f0c668bda055" translate="yes" xml:space="preserve">
          <source>Returns the log of summed exponentials of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. The computation is numerically stabilized.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 합산 지수 로그를 반환합니다 . 계산은 수치 적으로 안정화됩니다.</target>
        </trans-unit>
        <trans-unit id="cf79c17bd23741493f06836ca1da65ec3e27f2d9" translate="yes" xml:space="preserve">
          <source>Returns the log of the probability density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">평가에서 확률 밀도 / 질량 로그 함수의 리턴 &lt;code&gt;value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d7b57cf20e91ac970ff715302367553ab5d606f0" translate="yes" xml:space="preserve">
          <source>Returns the logarithm of the cumulative summation of the exponentiation of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 요소의 지수에 대한 누적 합계의 로그를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="ece733a42c60a3b1bb785f7b96e5ca90ed6c9354" translate="yes" xml:space="preserve">
          <source>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices &lt;code&gt;input&lt;/code&gt;, the other elements of the result tensor &lt;code&gt;out&lt;/code&gt; are set to 0.</source>
          <target state="translated">행렬의 아래쪽 삼각형 부분 (2 차원 텐서) 또는 행렬의 배치 &lt;code&gt;input&lt;/code&gt; 을 반환합니다 . 결과 텐서 &lt;code&gt;out&lt;/code&gt; 의 다른 요소는 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="903bb5ad470677dd96718bed7dd424b4eea58c19" translate="yes" xml:space="preserve">
          <source>Returns the matrix exponential. Supports batched input. For a matrix &lt;code&gt;A&lt;/code&gt;, the matrix exponential is defined as</source>
          <target state="translated">행렬 지수를 반환합니다. 일괄 입력을 지원합니다. 행렬 &lt;code&gt;A&lt;/code&gt; 의 경우 행렬 지수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="57632c5d6ea2c48ab4c7ad8c0de751aa58bc8ad6" translate="yes" xml:space="preserve">
          <source>Returns the matrix norm or vector norm of a given tensor.</source>
          <target state="translated">주어진 텐서의 행렬 노름 또는 벡터 노름을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="aa8212443e4c775af178e39c5b9d95062b7f280a" translate="yes" xml:space="preserve">
          <source>Returns the matrix product of the</source>
          <target state="translated">다음의 행렬 곱을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2056ceddf487b6aeac66141f77f1593dd5d51a33" translate="yes" xml:space="preserve">
          <source>Returns the matrix raised to the power &lt;code&gt;n&lt;/code&gt; for square matrices.</source>
          <target state="translated">제곱 행렬에 대해 &lt;code&gt;n&lt;/code&gt; 거듭 제곱 된 행렬 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7a715df6cd530cefa49cad9ca42363f5dcc48be7" translate="yes" xml:space="preserve">
          <source>Returns the matrix raised to the power &lt;code&gt;n&lt;/code&gt; for square matrices. For batch of matrices, each individual matrix is raised to the power &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="translated">제곱 행렬에 대해 &lt;code&gt;n&lt;/code&gt; 거듭 제곱 된 행렬 을 반환합니다 . 행렬 배치의 경우 각 개별 행렬은 &lt;code&gt;n&lt;/code&gt; 제곱 합니다.</target>
        </trans-unit>
        <trans-unit id="03824d09bb5c54243f197f9c9ba3e110276ea20d" translate="yes" xml:space="preserve">
          <source>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</source>
          <target state="translated">주어진 장치에 대해 캐싱 할당자가 관리하는 최대 GPU 메모리 (바이트)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0caf8ade3d9c0d603d539d45bc8b1a8d76a7591a" translate="yes" xml:space="preserve">
          <source>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</source>
          <target state="translated">주어진 장치에 대해 텐서가 차지하는 최대 GPU 메모리 (바이트)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2592d5f82bbbc822a97e96b68b786a9c78bb926c" translate="yes" xml:space="preserve">
          <source>Returns the maximum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 최대 값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="fa13bd7559d027b3477eb7ddbb196e1171a0bca9" translate="yes" xml:space="preserve">
          <source>Returns the maximum value of each slice of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension(s) &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 조각의 최대 값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="bca8e17597f7810508efae907eae755779bfdb1a" translate="yes" xml:space="preserve">
          <source>Returns the mean of the distribution.</source>
          <target state="translated">분포의 평균을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="809ee94cff5a85854481052825bc00d99efb4eb6" translate="yes" xml:space="preserve">
          <source>Returns the mean value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 평균값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b95b8ed42b947df36f019475ad19d862230385ee" translate="yes" xml:space="preserve">
          <source>Returns the mean value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행의 평균값을 반환합니다 . &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오.</target>
        </trans-unit>
        <trans-unit id="f2abf9fb9cf56489f6a00a784d4d458d7895d090" translate="yes" xml:space="preserve">
          <source>Returns the median value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 중앙값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="8878f4d09befec1d421d046a625ff001883087d7" translate="yes" xml:space="preserve">
          <source>Returns the minimum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 최소값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="0a0cee886c48025d366605a4d62185c2c9000596" translate="yes" xml:space="preserve">
          <source>Returns the minimum value of each slice of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension(s) &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 조각의 최소값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1b3e9ec11a895a649216c83f8ca9adf589921f7c" translate="yes" xml:space="preserve">
          <source>Returns the number of GPUs available.</source>
          <target state="translated">사용 가능한 GPU 수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="fd09e78e1b1393b57b6e81bd22e6b7089ac87df8" translate="yes" xml:space="preserve">
          <source>Returns the number of dimensions of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 차원 수를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="074c1198678efc321484286e167225dc00032bc7" translate="yes" xml:space="preserve">
          <source>Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by &lt;code&gt;set()&lt;/code&gt; and &lt;code&gt;add()&lt;/code&gt; since one key is used to coordinate all the workers using the store.</source>
          <target state="translated">저장소에 설정된 키 수를 반환합니다. 저장소를 사용하는 모든 작업자를 조정하는 데 하나의 키가 사용되기 때문에이 숫자는 일반적으로 &lt;code&gt;set()&lt;/code&gt; 및 &lt;code&gt;add()&lt;/code&gt; 에 의해 추가 된 키 수보다 하나 더 큽니다 .</target>
        </trans-unit>
        <trans-unit id="77a1f23eab350f2cc1dff14c648c4ff6203e6c58" translate="yes" xml:space="preserve">
          <source>Returns the number of processes in the current process group</source>
          <target state="translated">현재 프로세스 그룹의 프로세스 수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c6ca89b8a3b13965452fa2d1cea6ac87f72d7a23" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for inter-op parallelism on CPU (e.g.</source>
          <target state="translated">CPU에서 inter-op 병렬 처리에 사용되는 스레드 수를 반환합니다 (예 :</target>
        </trans-unit>
        <trans-unit id="d8b5e8f68adc8148c1f9e13acbeae8fd51bfefc0" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter)</source>
          <target state="translated">CPU (예 : JIT 인터프리터)에서 inter-op 병렬 처리에 사용되는 스레드 수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7c423a746fa97a8c78c6c6a0bea67a7ee6cacc04" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for parallelizing CPU operations</source>
          <target state="translated">CPU 작업을 병렬화하는 데 사용되는 스레드 수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="76a9d76393683122be08a7059b22ee9487de8af7" translate="yes" xml:space="preserve">
          <source>Returns the numerical rank of a 2-D tensor.</source>
          <target state="translated">2 차원 텐서의 숫자 순위를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="915a8c083943d2bf3a4491860c8c37370cb6d81a" translate="yes" xml:space="preserve">
          <source>Returns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;input&lt;/code&gt; is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues.</source>
          <target state="translated">2 차원 텐서의 숫자 순위를 반환합니다. 행렬 순위를 계산하는 방법은 기본적으로 SVD를 사용하여 수행됩니다. 경우 &lt;code&gt;symmetric&lt;/code&gt; 인 &lt;code&gt;True&lt;/code&gt; 다음 &lt;code&gt;input&lt;/code&gt; 대칭 인 것으로 가정하고, 랭크의 계산은 고유 값을 구함으로써 수행된다.</target>
        </trans-unit>
        <trans-unit id="b9021b2a01727f502181983387c837330544dbe0" translate="yes" xml:space="preserve">
          <source>Returns the p-norm of (&lt;code&gt;input&lt;/code&gt; - &lt;code&gt;other&lt;/code&gt;)</source>
          <target state="translated">( &lt;code&gt;input&lt;/code&gt; - &lt;code&gt;other&lt;/code&gt; ) 의 p- 노름을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0db967e67efe6e63a87c0ce474cb74d7038305af" translate="yes" xml:space="preserve">
          <source>Returns the product of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 곱을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="a7140a19943936f9d48bbfc1f4b6d873ec50ed19" translate="yes" xml:space="preserve">
          <source>Returns the product of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행의 곱을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="971be3e3aa0bf4f890c81c3791eba6b84f38ff83" translate="yes" xml:space="preserve">
          <source>Returns the q-th quantiles of all elements in the &lt;code&gt;input&lt;/code&gt; tensor, doing a linear interpolation when the q-th quantile lies between two data points.</source>
          <target state="translated">q 번째 분위수가 두 데이터 포인트 사이에있을 때 선형 보간을 수행 하여 &lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 q 번째 분위수를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1aa60514b18ba933ebc0d3b97778472f133a31e9" translate="yes" xml:space="preserve">
          <source>Returns the q-th quantiles of each row of the &lt;code&gt;input&lt;/code&gt; tensor along the dimension &lt;code&gt;dim&lt;/code&gt;, doing a linear interpolation when the q-th quantile lies between two data points. By default, &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; resulting in the &lt;code&gt;input&lt;/code&gt; tensor being flattened before computation.</source>
          <target state="translated">q 번째 분위수가 두 데이터 포인트 사이에있을 때 선형 보간을 수행 하여 차원 &lt;code&gt;dim&lt;/code&gt; 을 따라 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 q 번째 분위수를 반환합니다 . 기본적으로 &lt;code&gt;dim&lt;/code&gt; 인 &lt;code&gt;None&lt;/code&gt; 결과 &lt;code&gt;input&lt;/code&gt; 텐서 존재가 연산하기 전에 평탄화 없다.</target>
        </trans-unit>
        <trans-unit id="855eae4c3b5ff9439f790aa02a1e5edcb2f9666b" translate="yes" xml:space="preserve">
          <source>Returns the quantization scheme of a given QTensor.</source>
          <target state="translated">주어진 QTensor의 양자화 체계를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7d9c1cd994cab4ee1d9aadcaf26ce62a26fd9235" translate="yes" xml:space="preserve">
          <source>Returns the random number generator state as a &lt;code&gt;torch.ByteTensor&lt;/code&gt;.</source>
          <target state="translated">난수 생성기 상태를 &lt;code&gt;torch.ByteTensor&lt;/code&gt; 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="5e5816534688f7e46f4af65ebc7314df9d504c09" translate="yes" xml:space="preserve">
          <source>Returns the random number generator state of the specified GPU as a ByteTensor.</source>
          <target state="translated">지정된 GPU의 난수 생성기 상태를 ByteTensor로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="627a3c7f5879a8778db877ff3b00bbcfbf03056f" translate="yes" xml:space="preserve">
          <source>Returns the rank of current process group</source>
          <target state="translated">현재 프로세스 그룹의 순위를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4525ac2f89a3ac5d0c306f28833d2a44e3540a50" translate="yes" xml:space="preserve">
          <source>Returns the real and the imaginary parts together as one tensor of the same shape of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">실수 부분과 허수 부분을 같은 &lt;code&gt;input&lt;/code&gt; 형태의 하나의 텐서로 함께 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7e3148edb8aa985705a099b8ae7ab9879b62393f" translate="yes" xml:space="preserve">
          <source>Returns the result of running &lt;code&gt;func&lt;/code&gt; with &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;kwargs&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;args&lt;/code&gt; 및 &lt;code&gt;kwargs&lt;/code&gt; 로 &lt;code&gt;func&lt;/code&gt; 를 실행 한 결과를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="64531174d404b10b93897ed2b88aba389572d7db" translate="yes" xml:space="preserve">
          <source>Returns the return value of &lt;code&gt;optimizer.step(*args, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;optimizer.step(*args, **kwargs)&lt;/code&gt; 의 반환 값을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="6acc79cd313eeed62eea5f7e0ca05acecdcbc531" translate="yes" xml:space="preserve">
          <source>Returns the shape of a single sample (without batching).</source>
          <target state="translated">단일 샘플의 모양을 반환합니다 (배치 제외).</target>
        </trans-unit>
        <trans-unit id="4121b94d8bee0076d793dcc54679dd64a1c94a20" translate="yes" xml:space="preserve">
          <source>Returns the shape over which parameters are batched.</source>
          <target state="translated">매개 변수가 일괄 처리되는 모양을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e4dd6ff47ca4bddd8efc777b8ef0c3a2c983568d" translate="yes" xml:space="preserve">
          <source>Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.</source>
          <target state="translated">해당되는 경우 야 코비 행렬식의 부호를 반환합니다. 일반적으로 이것은 bijective 변환에만 의미가 있습니다.</target>
        </trans-unit>
        <trans-unit id="7d3d453cc3ee6cad8567bb601546cc4fa1aa1364" translate="yes" xml:space="preserve">
          <source>Returns the size in bytes of an individual element.</source>
          <target state="translated">개별 요소의 크기 (바이트)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="38820622e4a658519544ad686f76dee59368c29c" translate="yes" xml:space="preserve">
          <source>Returns the size of the &lt;code&gt;self&lt;/code&gt; tensor. The returned value is a subclass of &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;&lt;code&gt;tuple&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 크기를 반환합니다 . 반환 된 값은 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt; &lt;code&gt;tuple&lt;/code&gt; &lt;/a&gt; 의 하위 클래스입니다 .</target>
        </trans-unit>
        <trans-unit id="11ff079b4e3d88cd650436812653217a81a77a46" translate="yes" xml:space="preserve">
          <source>Returns the standard deviation of the distribution.</source>
          <target state="translated">분포의 표준 편차를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="159dfd0ce2a90b4d47c871e593a62731a632dc99" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation and mean of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 표준 편차와 평균을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b541928a3e9600b386da860747f66bb8139c4d0a" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation and mean of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">차원 &lt;code&gt;dim&lt;/code&gt; 에있는 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 표준 편차와 평균을 반환합니다 . &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오.</target>
        </trans-unit>
        <trans-unit id="8bbd91280991e2a9737689547bbddca952d63287" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 표준 편차를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="e297a06301ae289868c8331b63443bb83703e3b8" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">차원 &lt;code&gt;dim&lt;/code&gt; 에있는 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 표준 편차를 반환합니다 . &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오.</target>
        </trans-unit>
        <trans-unit id="8527f000d10d08462f55a7df699877bf3a2b4a09" translate="yes" xml:space="preserve">
          <source>Returns the state of the optimizer as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">옵티마이 저의 상태를 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; &lt;/a&gt; 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="ddacfefa1e98c3901d53623a3db23fafb076c7fc" translate="yes" xml:space="preserve">
          <source>Returns the state of the scaler as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;. It contains five entries:</source>
          <target state="translated">스케일러의 상태를 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; &lt;/a&gt; 로 반환합니다 . 여기에는 5 개의 항목이 있습니다.</target>
        </trans-unit>
        <trans-unit id="cbae1c6724a7b43e4ce8327e553e968ab36fa3db" translate="yes" xml:space="preserve">
          <source>Returns the state of the scheduler as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">스케줄러의 상태를 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; &lt;/a&gt; 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1ebb6fa0d71baf28bcf461777f1c5e425a2744c2" translate="yes" xml:space="preserve">
          <source>Returns the stride of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 의 보폭을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b5f0fbc903ffee7b0004fd1f9bcd5996f44017b1" translate="yes" xml:space="preserve">
          <source>Returns the sum of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 합을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1863758d4c0f9cfda8c72c22853117ba70ebdee5" translate="yes" xml:space="preserve">
          <source>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</source>
          <target state="translated">NaN (Not a Numbers)을 0으로 처리하여 모든 요소의 합계를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="14e6166614886b7770d3ecf48dce1bfb59192744" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of SparseTensor &lt;code&gt;input&lt;/code&gt; in the given dimensions &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them. When sum over all &lt;code&gt;sparse_dim&lt;/code&gt;, this method returns a Tensor instead of SparseTensor.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 SparseTensor &lt;code&gt;input&lt;/code&gt; 의 각 행의 합계를 반환합니다 . &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오. 모든 &lt;code&gt;sparse_dim&lt;/code&gt; 에 대한 합계 일 때이 메서드는 SparseTensor 대신 Tensor를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="bf49e8cc86ca6a6600ce29a25884a785a376abc6" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, treating Not a Numbers (NaNs) as zero. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에있는 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행의 합계를 반환하고 NaN (Not a Numbers)을 0으로 처리합니다. &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오.</target>
        </trans-unit>
        <trans-unit id="4d434d5c1c4d2f8c246ee3548fb97e54d9889e29" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행의 합계를 반환합니다 . &lt;code&gt;dim&lt;/code&gt; 이 차원 목록 이면 모두 축소하십시오.</target>
        </trans-unit>
        <trans-unit id="28abb051bc478158b8e05bc1054f16151a20a7cb" translate="yes" xml:space="preserve">
          <source>Returns the sum of the elements of the diagonal of the input 2-D matrix.</source>
          <target state="translated">입력 2 차원 행렬의 대각선 요소의 합을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="628247234e04df2fd2dd0c458a3e5e95270ff4e1" translate="yes" xml:space="preserve">
          <source>Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;item()&lt;/code&gt;&lt;/a&gt;. Tensors are automatically moved to the CPU first if necessary.</source>
          <target state="translated">텐서를 (중첩 된) 목록으로 반환합니다. 스칼라의 경우 &lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;item()&lt;/code&gt; &lt;/a&gt; 과 마찬가지로 표준 Python 숫자가 반환 됩니다. 텐서는 필요한 경우 먼저 자동으로 CPU로 이동합니다.</target>
        </trans-unit>
        <trans-unit id="035dc69af946dc6fb060b51cabe8b5996eafc4da" translate="yes" xml:space="preserve">
          <source>Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.</source>
          <target state="translated">이벤트가 기록 된 후 end_event가 기록되기 전에 경과 된 시간 (밀리 초)을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="94ea64072a9591f059633c957623237cd647f97a" translate="yes" xml:space="preserve">
          <source>Returns the total number of elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 의 총 요소 수를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="1d3970a743c03ce9d28802ddc4f6588b09bb7c8b" translate="yes" xml:space="preserve">
          <source>Returns the type if &lt;code&gt;dtype&lt;/code&gt; is not provided, else casts this object to the specified type.</source>
          <target state="translated">&lt;code&gt;dtype&lt;/code&gt; 이 제공되지 않으면 형식을 반환하고 , 그렇지 않으면이 개체를 지정된 형식으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="2fea27b4bcbff9f0edb11c8bfcd39329450a9b92" translate="yes" xml:space="preserve">
          <source>Returns the type of the underlying storage.</source>
          <target state="translated">기본 저장소의 유형을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4ac509092325d5bcdffb0cc3612c7cf497735d46" translate="yes" xml:space="preserve">
          <source>Returns the underlying storage.</source>
          <target state="translated">기본 저장소를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="32fcdf754b9cb5a228ef02efc503187163097afb" translate="yes" xml:space="preserve">
          <source>Returns the unique elements of the input tensor.</source>
          <target state="translated">입력 텐서의 고유 요소를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="26e4d829967fdd7e5612995ff5e9602634c0689a" translate="yes" xml:space="preserve">
          <source>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices &lt;code&gt;input&lt;/code&gt;, the other elements of the result tensor &lt;code&gt;out&lt;/code&gt; are set to 0.</source>
          <target state="translated">행렬의 위쪽 삼각형 부분 (2 차원 텐서) 또는 행렬의 배치 &lt;code&gt;input&lt;/code&gt; 을 반환합니다 . 결과 텐서 &lt;code&gt;out&lt;/code&gt; 의 다른 요소는 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="f17ab21227456440a4bca3f2fb221dbfad6b68d3" translate="yes" xml:space="preserve">
          <source>Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see &lt;a href=&quot;#torch.Tensor.tolist&quot;&gt;&lt;code&gt;tolist()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 텐서의 값을 표준 Python 숫자로 반환합니다. 이것은 요소가 하나 인 텐서에서만 작동합니다. 다른 경우는 &lt;a href=&quot;#torch.Tensor.tolist&quot;&gt; &lt;code&gt;tolist()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="f1cc620c9c632e9d45f1d98e6550b6c3781efad4" translate="yes" xml:space="preserve">
          <source>Returns the variance and mean of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 분산과 평균을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="9ff1d1aa2591afcd14de03b53682da2b82cb9cc7" translate="yes" xml:space="preserve">
          <source>Returns the variance and mean of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행에 대한 분산과 평균을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="af4e03a6f47c5a06e9dc95d8b7b0827b12d10bac" translate="yes" xml:space="preserve">
          <source>Returns the variance of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 텐서 에있는 모든 요소의 분산을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="0298d938b904aa1e59745be5e3318e09914487aa" translate="yes" xml:space="preserve">
          <source>Returns the variance of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에서 &lt;code&gt;input&lt;/code&gt; 텐서 의 각 행의 분산을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="fdcaff1a07dc530f557ad4bcd05c34a11bb994e8" translate="yes" xml:space="preserve">
          <source>Returns the variance of the distribution.</source>
          <target state="translated">분포의 분산을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="88e4f4506a554a4b89da7d55d5cfa7515f220fb9" translate="yes" xml:space="preserve">
          <source>Returns the version of cuDNN</source>
          <target state="translated">cuDNN의 버전을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="9c3ac5c69999b0a023804bb506ededc917b6c131" translate="yes" xml:space="preserve">
          <source>Returns this tensor as the same shape as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.reshape_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.reshape(other.sizes())&lt;/code&gt;. This method returns a view if &lt;code&gt;other.sizes()&lt;/code&gt; is compatible with the current shape. See &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">이 텐서를 &lt;code&gt;other&lt;/code&gt; 와 같은 모양으로 반환합니다 . &lt;code&gt;self.reshape_as(other)&lt;/code&gt; 는 &lt;code&gt;self.reshape(other.sizes())&lt;/code&gt; 와 동일합니다 . &lt;code&gt;other.sizes()&lt;/code&gt; 가 현재 모양과 호환되는 경우이 메서드는 뷰를 반환합니다 . 뷰를 반환 할 수있는 경우 &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="586000b57d8c3b25d201cf41023bcdd3dcb6f713" translate="yes" xml:space="preserve">
          <source>Returns this tensor cast to the type of the given tensor.</source>
          <target state="translated">이 텐서 캐스트를 주어진 텐서의 유형으로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="ba9ebe14b855b8e57d04a421d5dedae584879e50" translate="yes" xml:space="preserve">
          <source>Returns total time spent on CPU obtained as a sum of all self times across all the events.</source>
          <target state="translated">모든 이벤트에서 모든 자체 시간의 합계로 얻은 CPU에 소요 된 총 시간을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="88c329059251d00815688b5e671bb43e44ab8c23" translate="yes" xml:space="preserve">
          <source>Returns true if &lt;code&gt;self.data&lt;/code&gt; stored on a gpu</source>
          <target state="translated">&lt;code&gt;self.data&lt;/code&gt; 가 GPU에 저장된 경우 true를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="69352252bee0873e3fb0a65734cef7e6482fff70" translate="yes" xml:space="preserve">
          <source>Returns true if &lt;code&gt;self.data&lt;/code&gt; stored on in pinned memory</source>
          <target state="translated">경우에 true를 돌려줍니다 &lt;code&gt;self.data&lt;/code&gt; 고정 된 메모리에 저장</target>
        </trans-unit>
        <trans-unit id="373aeaf23062f8c5ecaff73e9be00cb54c4e37f3" translate="yes" xml:space="preserve">
          <source>Returns true if this tensor resides in pinned memory.</source>
          <target state="translated">이 텐서가 고정 된 메모리에 있으면 true를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c8e3480d7d70ecfc7b3c25ecaed7108c83b90b62" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with CUDA support. Note that this doesn&amp;rsquo;t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.</source>
          <target state="translated">PyTorch가 CUDA 지원으로 빌드되었는지 여부를 반환합니다. 이것이 반드시 CUDA를 사용할 수 있다는 의미는 아닙니다. 이 PyTorch 바이너리가 CUDA 드라이버와 장치가 작동하는 머신에서 실행된다면 우리는 그것을 사용할 수있을 것입니다.</target>
        </trans-unit>
        <trans-unit id="98fd0f653522d8480237ac570476e6435a37e06f" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with MKL support.</source>
          <target state="translated">PyTorch가 MKL 지원으로 빌드되었는지 여부를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="620b15262eed97044ef3e21b5235ee100fef8185" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with MKL-DNN support.</source>
          <target state="translated">PyTorch가 MKL-DNN 지원으로 빌드되었는지 여부를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="957ab7585c4317f19aa9cfbd05dcb5165584e9c9" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with OpenMP support.</source>
          <target state="translated">PyTorch가 OpenMP 지원으로 빌드되었는지 여부를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="57fa332c1c053f80c3eff6f20003e44a174f7e50" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</source>
          <target state="translated">PyTorch가 _GLIBCXX_USE_CXX11_ABI = 1로 빌드되었는지 여부를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4f1dfa1c8e9678eac6dccc5f732ccd2cbc520cad" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch&amp;rsquo;s CUDA state has been initialized.</source>
          <target state="translated">PyTorch의 CUDA 상태가 초기화되었는지 여부를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="770f265f08ee812cf8c52d62b5ca51586f3291bd" translate="yes" xml:space="preserve">
          <source>Returns whether or not the current node is the owner of this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">현재 노드가이 &lt;code&gt;RRef&lt;/code&gt; 소유자인지 여부를 리턴합니다 .</target>
        </trans-unit>
        <trans-unit id="65ba60da18a01009c75c582854b7076b39cbf39e" translate="yes" xml:space="preserve">
          <source>Returns whether this &lt;code&gt;RRef&lt;/code&gt; has been confirmed by the owner. &lt;code&gt;OwnerRRef&lt;/code&gt; always returns true, while &lt;code&gt;UserRRef&lt;/code&gt; only returns true when the owner knowns about this &lt;code&gt;UserRRef&lt;/code&gt;.</source>
          <target state="translated">이 &lt;code&gt;RRef&lt;/code&gt; 가 소유자에 의해 확인 되었는지 여부를 리턴합니다 . &lt;code&gt;OwnerRRef&lt;/code&gt; 는 항상 true를 반환하는 반면 &lt;code&gt;UserRRef&lt;/code&gt; 는 소유자가이 &lt;code&gt;UserRRef&lt;/code&gt; 에 대해 알고있는 경우에만 true를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="a0f5e59205b44dcceb0ac6759e21e032f1501bc8" translate="yes" xml:space="preserve">
          <source>Returns worker information of the node that owns this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">이 &lt;code&gt;RRef&lt;/code&gt; 를 소유 한 노드의 작업자 정보를 리턴 합니다.</target>
        </trans-unit>
        <trans-unit id="4edad2b2ca35229024c67f3f0d3bff7d93093b61" translate="yes" xml:space="preserve">
          <source>Returns worker name of the node that owns this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">이 &lt;code&gt;RRef&lt;/code&gt; 를 소유 한 노드의 작업자 이름을 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="7749fcf802c472b6c2f5bd0556805e456ffd5674" translate="yes" xml:space="preserve">
          <source>Returns:</source>
          <target state="translated">Returns:</target>
        </trans-unit>
        <trans-unit id="614439f097ad878a8635d81ef81858113ba66ffa" translate="yes" xml:space="preserve">
          <source>Returns: self</source>
          <target state="translated">반환 : self</target>
        </trans-unit>
        <trans-unit id="63fbd9687bea5bf2237b15e3392da627d72bce65" translate="yes" xml:space="preserve">
          <source>Reverse the order of a n-D tensor along given axis in dims.</source>
          <target state="translated">주어진 축을 따라 nD 텐서의 순서를 희미하게 반전합니다.</target>
        </trans-unit>
        <trans-unit id="7787cde8d62a04058bf7bd3bc0e17bb68ebcb122" translate="yes" xml:space="preserve">
          <source>Right now all parameters have to be on a single device. This will be improved in the future.</source>
          <target state="translated">지금은 모든 매개 변수가 단일 장치에 있어야합니다. 이것은 향후 개선 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="c79c0386989b48b4178b173f296df3e3c68180f2" translate="yes" xml:space="preserve">
          <source>Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it&amp;rsquo;s a no-op.</source>
          <target state="translated">지금은 모듈이 GPU에 있고 cuDNN이 활성화 된 경우에만 작동합니다. 그렇지 않으면 작동하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bf85deb85c247cf504b5145a433d00570a94475a" translate="yes" xml:space="preserve">
          <source>Roll the tensor along the given dimension(s).</source>
          <target state="translated">주어진 차원을 따라 텐서를 굴립니다.</target>
        </trans-unit>
        <trans-unit id="61764ff63ec4f189d73879d14b23769ece78814a" translate="yes" xml:space="preserve">
          <source>Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.</source>
          <target state="translated">주어진 차원을 따라 텐서를 굴립니다. 마지막 위치를 넘어 이동 한 요소는 첫 번째 위치에 다시 도입됩니다. 치수가 지정되지 않은 경우 텐서는 롤링 전에 평평해진 다음 원래 모양으로 복원됩니다.</target>
        </trans-unit>
        <trans-unit id="8f164f4fea8febd9cad272e82ab328161a3c6dcb" translate="yes" xml:space="preserve">
          <source>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</source>
          <target state="translated">dims 축으로 지정된 평면에서 nD 텐서를 90도 회전합니다.</target>
        </trans-unit>
        <trans-unit id="8dfaeb09d713bebac16f06e9df8c867ae92ec360" translate="yes" xml:space="preserve">
          <source>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k &amp;gt; 0, and from the second towards the first for k &amp;lt; 0.</source>
          <target state="translated">dims 축으로 지정된 평면에서 nD 텐서를 90도 회전합니다. 회전 방향은 k&amp;gt; 0 인 경우 첫 번째 축에서 두 번째 축을 향하고 k &amp;lt;0 인 경우 두 번째 축에서 첫 번째 축을 향합니다.</target>
        </trans-unit>
        <trans-unit id="8f997250f8d2d3174e19a84c2938437affe12ee1" translate="yes" xml:space="preserve">
          <source>Rule of thumb</source>
          <target state="translated">경험의 법칙</target>
        </trans-unit>
        <trans-unit id="f35ba84a98c82f892526356da88061a3501723bb" translate="yes" xml:space="preserve">
          <source>Run it on the command line with</source>
          <target state="translated">명령 줄에서 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="312956888e10f6c9efa0be100898714e9d49a550" translate="yes" xml:space="preserve">
          <source>Running a loaded model:</source>
          <target state="translated">로드 된 모델 실행 :</target>
        </trans-unit>
        <trans-unit id="5359eea6ab0e87db9479836bb9761e7ad6a157fe" translate="yes" xml:space="preserve">
          <source>Runtime characteristics</source>
          <target state="translated">런타임 특성</target>
        </trans-unit>
        <trans-unit id="02aa629c8b16cd17a44f3a0efec2feed43937642" translate="yes" xml:space="preserve">
          <source>S</source>
          <target state="translated">S</target>
        </trans-unit>
        <trans-unit id="d7c8ea15b98297abf6f0c48aa6d1cd297394d77c" translate="yes" xml:space="preserve">
          <source>S ** 2 / (m - 1)</source>
          <target state="translated">S ** 2 / (m-1)</target>
        </trans-unit>
        <trans-unit id="dc71df7ac9f3ac27b7d12ac3903965de330de133" translate="yes" xml:space="preserve">
          <source>S = \text{max target length, if shape is } (N, S)</source>
          <target state="translated">S = \ text {모양이} 인 경우 최대 대상 길이 (N, S)</target>
        </trans-unit>
        <trans-unit id="1678c8d5a5e1b27b84ba49edaa49f332453af05c" translate="yes" xml:space="preserve">
          <source>S=\text{num\_layers} * \text{num\_directions}</source>
          <target state="translated">S = \ text {num \ _layers} * \ text {num \ _directions}</target>
        </trans-unit>
        <trans-unit id="d4cd3df709d03592b7820bf91be2a174166e078a" translate="yes" xml:space="preserve">
          <source>SELU</source>
          <target state="translated">SELU</target>
        </trans-unit>
        <trans-unit id="bad45f89fe2643171a9f0af482912ef53c58c8d3" translate="yes" xml:space="preserve">
          <source>SMART mode algorithm</source>
          <target state="translated">SMART 모드 알고리즘</target>
        </trans-unit>
        <trans-unit id="2c798885ebfbe383227dbd5c2205277f8af9d524" translate="yes" xml:space="preserve">
          <source>SUM</source>
          <target state="translated">SUM</target>
        </trans-unit>
        <trans-unit id="6f0eaf31574764cd755cbd6a055a6fcbc66e5dae" translate="yes" xml:space="preserve">
          <source>SWA has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot;&gt;Averaging Weights Leads to Wider Optima and Better Generalization&lt;/a&gt;.</source>
          <target state="translated">SWA는 &lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot;&gt;Averaging Weights Leads to Wider Optima and Better Generalization&lt;/a&gt; 에서 제안되었습니다 .</target>
        </trans-unit>
        <trans-unit id="f84f75a03bd05c4bba727eb6c0d6fe360d00d818" translate="yes" xml:space="preserve">
          <source>SWA learning rate schedules</source>
          <target state="translated">SWA 학습률 일정</target>
        </trans-unit>
        <trans-unit id="2e4fbb0a9e252120bad1b2cf3ddef7f9a08b3c65" translate="yes" xml:space="preserve">
          <source>Same as &lt;a href=&quot;#torch.Tensor.narrow&quot;&gt;&lt;code&gt;Tensor.narrow()&lt;/code&gt;&lt;/a&gt; except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling &lt;code&gt;`narrow_copy&lt;/code&gt; with &lt;code&gt;`dimemsion &amp;gt; self.sparse_dim()`&lt;/code&gt; will return a copy with the relevant dense dimension narrowed, and &lt;code&gt;`self.shape`&lt;/code&gt; updated accordingly.</source>
          <target state="translated">공유 스토리지가 아닌 복사본을 반환하는 것을 제외하고 는 &lt;a href=&quot;#torch.Tensor.narrow&quot;&gt; &lt;code&gt;Tensor.narrow()&lt;/code&gt; &lt;/a&gt; 와 동일 합니다. 이것은 주로 공유 스토리지 좁은 방법이없는 희소 텐서 용입니다. 호출 &lt;code&gt;`narrow_copy&lt;/code&gt; 와 &lt;code&gt;`dimemsion &amp;gt; self.sparse_dim()`&lt;/code&gt; 관련 밀도의 축소 차원과 함께 사본을 반환합니다 &lt;code&gt;`self.shape`&lt;/code&gt; 따라 업데이트.</target>
        </trans-unit>
        <trans-unit id="12f6402d6b42f6b20df7c7e5514d35048980475e" translate="yes" xml:space="preserve">
          <source>Sampled tensor of same shape as &lt;code&gt;logits&lt;/code&gt; from the Gumbel-Softmax distribution. If &lt;code&gt;hard=True&lt;/code&gt;, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">Gumbel-Softmax 분포에서 &lt;code&gt;logits&lt;/code&gt; 과 동일한 모양의 샘플링 된 텐서 . 경우 &lt;code&gt;hard=True&lt;/code&gt; , 반환 된 샘플 그렇지 않으면 확률 분포 될 것이다, 한 뜨겁에서 1 합이 &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0f11d31dedd1a06aeb7fd6c168cfb8046bb4af97" translate="yes" xml:space="preserve">
          <source>Sampler that restricts data loading to a subset of the dataset.</source>
          <target state="translated">데이터로드를 데이터 세트의 하위 집합으로 제한하는 샘플러입니다.</target>
        </trans-unit>
        <trans-unit id="46c4eda8f0682c6a4eca16cae1bf03c979256658" translate="yes" xml:space="preserve">
          <source>Samples are binary (0 or 1). They take the value &lt;code&gt;1&lt;/code&gt; with probability &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; with probability &lt;code&gt;1 - p&lt;/code&gt;.</source>
          <target state="translated">샘플은 이진 (0 또는 1)입니다. 그들은 값을 받아 &lt;code&gt;1&lt;/code&gt; 확률로 &lt;code&gt;p&lt;/code&gt; 와 &lt;code&gt;0&lt;/code&gt; 을 확률로 &lt;code&gt;1 - p&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="290387605cb78533c282d0e9d73fda591d532885" translate="yes" xml:space="preserve">
          <source>Samples are integers from</source>
          <target state="translated">샘플은 다음의 정수입니다.</target>
        </trans-unit>
        <trans-unit id="58b74a9d8f020ad3d0017c06dac17a64698c94e6" translate="yes" xml:space="preserve">
          <source>Samples are logits of values in (0, 1). See [1] for more details.</source>
          <target state="translated">샘플은 (0, 1) 값의 로짓입니다. 자세한 내용은 [1]을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="038b1821087530df9b32220979af778b78c446a1" translate="yes" xml:space="preserve">
          <source>Samples are non-negative integers [0,</source>
          <target state="translated">샘플은 음이 아닌 정수 [0,</target>
        </trans-unit>
        <trans-unit id="19b7a569829985718d97274f88b37fe1917c9e92" translate="yes" xml:space="preserve">
          <source>Samples are nonnegative integers, with a pmf given by</source>
          <target state="translated">샘플은 음이 아닌 정수이며 pmf는</target>
        </trans-unit>
        <trans-unit id="c447ff4f78d86ac69f689268060727181fbe9807" translate="yes" xml:space="preserve">
          <source>Samples are one-hot coded vectors of size &lt;code&gt;probs.size(-1)&lt;/code&gt;.</source>
          <target state="translated">샘플은 &lt;code&gt;probs.size(-1)&lt;/code&gt; 크기의 원-핫 코딩 된 벡터입니다 .</target>
        </trans-unit>
        <trans-unit id="19104e555389bebae6c56e80fa764ee086be702a" translate="yes" xml:space="preserve">
          <source>Samples elements from &lt;code&gt;[0,..,len(weights)-1]&lt;/code&gt; with given probabilities (weights).</source>
          <target state="translated">주어진 확률 (가중치) 로 &lt;code&gt;[0,..,len(weights)-1]&lt;/code&gt; 에서 요소를 샘플링 합니다.</target>
        </trans-unit>
        <trans-unit id="9e9e089066c3d2af4277a5ab0054e52473f2c07f" translate="yes" xml:space="preserve">
          <source>Samples elements randomly from a given list of indices, without replacement.</source>
          <target state="translated">대체하지 않고 주어진 인덱스 목록에서 요소를 무작위로 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="24e35b463a3cf98fd449e886fc4e86f3a0dab5f2" translate="yes" xml:space="preserve">
          <source>Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify &lt;code&gt;num_samples&lt;/code&gt; to draw.</source>
          <target state="translated">요소를 무작위로 샘플링합니다. 대체하지 않는 경우 셔플 된 데이터 세트에서 샘플링합니다. 대체를 사용하는 경우 사용자는 그릴 &lt;code&gt;num_samples&lt;/code&gt; 를 지정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="09f894ef8557b505ad9a0b93997e1c4496877755" translate="yes" xml:space="preserve">
          <source>Samples elements sequentially, always in the same order.</source>
          <target state="translated">요소를 항상 동일한 순서로 순차적으로 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="22779ddf6cbe4d8bc9ff5fafe1e2aa1b1ff778a6" translate="yes" xml:space="preserve">
          <source>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means &lt;code&gt;0&lt;/code&gt; follows a Cauchy distribution.</source>
          <target state="translated">Cauchy (Lorentz) 분포의 샘플. 평균이 &lt;code&gt;0&lt;/code&gt; 인 독립 정규 분포 확률 변수의 비율 분포 는 코시 분포를 따릅니다.</target>
        </trans-unit>
        <trans-unit id="2cb9d9b0dcc4dfe9421ee8a8fe988bef0860c270" translate="yes" xml:space="preserve">
          <source>Samples from a Gumbel Distribution.</source>
          <target state="translated">Gumbel 분포의 샘플.</target>
        </trans-unit>
        <trans-unit id="f796da3db0834b82a9b597a027df4fe0a3710f34" translate="yes" xml:space="preserve">
          <source>Samples from a Pareto Type 1 distribution.</source>
          <target state="translated">Pareto Type 1 분포의 샘플.</target>
        </trans-unit>
        <trans-unit id="e40b18d593f8292cd1929002560bba0658812862" translate="yes" xml:space="preserve">
          <source>Samples from a two-parameter Weibull distribution.</source>
          <target state="translated">2- 모수 Weibull 분포의 표본입니다.</target>
        </trans-unit>
        <trans-unit id="870cb0523222c3bafa82fc03370a9998186a6c8a" translate="yes" xml:space="preserve">
          <source>Samples from the Gumbel-Softmax distribution (&lt;a href=&quot;https://arxiv.org/abs/1611.00712&quot;&gt;Link 1&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;Link 2&lt;/a&gt;) and optionally discretizes.</source>
          <target state="translated">Gumbel-Softmax 분포 ( &lt;a href=&quot;https://arxiv.org/abs/1611.00712&quot;&gt;Link 1 &lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;Link 2&lt;/a&gt; )의 샘플 이며 선택적으로 이산화됩니다.</target>
        </trans-unit>
        <trans-unit id="bb9c2993371e955a4cc850805ced703ca0a14469" translate="yes" xml:space="preserve">
          <source>Save an offline version of this module for use in a separate process.</source>
          <target state="translated">별도의 프로세스에서 사용할 수 있도록이 모듈의 오프라인 버전을 저장하십시오.</target>
        </trans-unit>
        <trans-unit id="c01b4e6f82d950b9eb933dee7fd7ccb2a95a9e86" translate="yes" xml:space="preserve">
          <source>Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using &lt;code&gt;torch::jit::load(filename)&lt;/code&gt; or into the Python API with &lt;a href=&quot;torch.jit.load#torch.jit.load&quot;&gt;&lt;code&gt;torch.jit.load&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">별도의 프로세스에서 사용할 수 있도록이 모듈의 오프라인 버전을 저장하십시오. 저장된 모듈은이 모듈의 모든 메서드, 하위 모듈, 매개 변수 및 속성을 직렬화합니다. &lt;code&gt;torch::jit::load(filename)&lt;/code&gt; 사용하여 C ++ API에 로드 하거나 &lt;a href=&quot;torch.jit.load#torch.jit.load&quot;&gt; &lt;code&gt;torch.jit.load&lt;/code&gt; &lt;/a&gt; 를 사용하여 Python API에 로드 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ae2ed5816db3fac279ce9e6772770274eeecffb6" translate="yes" xml:space="preserve">
          <source>Saves an object to a disk file.</source>
          <target state="translated">개체를 디스크 파일에 저장합니다.</target>
        </trans-unit>
        <trans-unit id="4568659c737ca6dd6d7cb2b416c756b4fa3d4b2e" translate="yes" xml:space="preserve">
          <source>Saves given tensors for a future call to &lt;code&gt;backward()&lt;/code&gt;.</source>
          <target state="translated">앞으로 &lt;code&gt;backward()&lt;/code&gt; 호출을 위해 주어진 텐서를 저장 합니다.</target>
        </trans-unit>
        <trans-unit id="122590ee88a0a7371836192fc7f0f566864870ca" translate="yes" xml:space="preserve">
          <source>Say we have a model like:</source>
          <target state="translated">다음과 같은 모델이 있다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="66da7860d1535f34a78cf5749db3d68876d558e1" translate="yes" xml:space="preserve">
          <source>Scatters a list of tensors to all processes in a group.</source>
          <target state="translated">텐서 목록을 그룹의 모든 프로세스에 분산시킵니다.</target>
        </trans-unit>
        <trans-unit id="106090197649a7e069056e240a9ad14c57a1cbfa" translate="yes" xml:space="preserve">
          <source>Scatters tensor across multiple GPUs.</source>
          <target state="translated">여러 GPU에 텐서를 분산시킵니다.</target>
        </trans-unit>
        <trans-unit id="b687feeb25ef199e522b82d65d8c03c4535af2a9" translate="yes" xml:space="preserve">
          <source>Score function</source>
          <target state="translated">점수 기능</target>
        </trans-unit>
        <trans-unit id="ca23ef4c78c4b30679a7feed50e583d779b937a2" translate="yes" xml:space="preserve">
          <source>Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.</source>
          <target state="translated">변환을 반전하고 기본 분포의 점수와 로그 abs det jacobian을 사용하여 점수를 계산하여 샘플에 점수를 매 깁니다.</target>
        </trans-unit>
        <trans-unit id="8003f43b6ada6148476432121a9ade7662e59807" translate="yes" xml:space="preserve">
          <source>ScriptFunction</source>
          <target state="translated">ScriptFunction</target>
        </trans-unit>
        <trans-unit id="45d3892e0529a64b6123a79e03cc7e0f2432a98c" translate="yes" xml:space="preserve">
          <source>ScriptModule</source>
          <target state="translated">ScriptModule</target>
        </trans-unit>
        <trans-unit id="3fb95588d29ea3da9d0dc7585a2020bf65e56741" translate="yes" xml:space="preserve">
          <source>Scripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.</source>
          <target state="translated">스크립팅 된 함수는 추적 된 함수를 호출 할 수 있습니다. 이것은 단순 피드 포워드 모델 주위에서 제어 흐름을 사용해야 할 때 특히 유용합니다. 예를 들어 시퀀스 대 시퀀스 모델의 빔 검색은 일반적으로 스크립트로 작성되지만 추적을 사용하여 생성 된 인코더 모듈을 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="898752673b10b861d8ace49b60919a48cd4eac78" translate="yes" xml:space="preserve">
          <source>Scripting a function or &lt;code&gt;nn.Module&lt;/code&gt; will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">함수 또는 &lt;code&gt;nn.Module&lt;/code&gt; 을 스크립팅 하면 소스 코드를 검사하고 TorchScript 컴파일러를 사용하여 TorchScript 코드로 컴파일하고 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; 을&lt;/a&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="af454137fb34eaadd60d3d10157722e0668fe3b2" translate="yes" xml:space="preserve">
          <source>Scripting a function or &lt;code&gt;nn.Module&lt;/code&gt; will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt;. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the &lt;a href=&quot;../jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt;.</source>
          <target state="translated">함수 또는 &lt;code&gt;nn.Module&lt;/code&gt; 을 스크립팅 하면 소스 코드를 검사하고 TorchScript 컴파일러를 사용하여 TorchScript 코드로 컴파일하고 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; 을&lt;/a&gt; 반환합니다 . TorchScript 자체는 Python 언어의 하위 집합이므로 Python의 모든 기능이 작동하는 것은 아니지만 텐서에서 계산하고 제어 종속 작업을 수행하기에 충분한 기능을 제공합니다. 전체 가이드는 &lt;a href=&quot;../jit_language_reference#language-reference&quot;&gt;TorchScript 언어 참조를 참조하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="033419356306d2d296e24d272874b31ebedcd66e" translate="yes" xml:space="preserve">
          <source>Scripting an &lt;code&gt;nn.Module&lt;/code&gt; by default will compile the &lt;code&gt;forward&lt;/code&gt; method and recursively compile any methods, submodules, and functions called by &lt;code&gt;forward&lt;/code&gt;. If a &lt;code&gt;nn.Module&lt;/code&gt; only uses features supported in TorchScript, no changes to the original module code should be necessary. &lt;code&gt;script&lt;/code&gt; will construct &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that has copies of the attributes, parameters, and methods of the original module.</source>
          <target state="translated">스크립팅 &lt;code&gt;nn.Module&lt;/code&gt; 을 기본으로하면 컴파일 &lt;code&gt;forward&lt;/code&gt; 방법을 재귀 적 호출 어떤 방법, 서브 모듈 및 기능 컴파일 &lt;code&gt;forward&lt;/code&gt; . 경우 &lt;code&gt;nn.Module&lt;/code&gt; 는 단지 TorchScript에서 지원되는 기능을 사용하여, 원래의 모듈 코드에 대한 변경 사항은 필요 없습니다. &lt;code&gt;script&lt;/code&gt; 는 원래 모듈의 속성, 매개 변수 및 메소드의 사본이있는 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 을 구성 합니다.</target>
        </trans-unit>
        <trans-unit id="19791ed5c10ea9f7f4c0c92eb2869473896cff09" translate="yes" xml:space="preserve">
          <source>Search the distribution in the histogram for optimal min/max values.</source>
          <target state="translated">최적의 최소 / 최대 값에 대한 히스토그램의 분포를 검색합니다.</target>
        </trans-unit>
        <trans-unit id="2b7757dc0bce246d1f7d37c65a1b7323d32d2496" translate="yes" xml:space="preserve">
          <source>Second, some operators will produce different values depending on whether or not they are coalesced or not (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.sparse.FloatTensor._indices&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._indices()&lt;/code&gt;&lt;/a&gt;, as well as &lt;a href=&quot;tensors#torch.Tensor.sparse_mask&quot;&gt;&lt;code&gt;torch.Tensor.sparse_mask()&lt;/code&gt;&lt;/a&gt;). These operators are prefixed by an underscore to indicate that they reveal internal implementation details and should be used with care, since code that works with coalesced sparse tensors may not work with uncoalesced sparse tensors; generally speaking, it is safest to explicitly coalesce before working with these operators.</source>
          <target state="translated">둘째, 일부 사업자들은 여부가 합체되어 여부 (예를 들어,에 따라 다른 값을 생성합니다 &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.sparse.FloatTensor._indices&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._indices()&lt;/code&gt; &lt;/a&gt; 뿐만 아니라 &lt;a href=&quot;tensors#torch.Tensor.sparse_mask&quot;&gt; &lt;code&gt;torch.Tensor.sparse_mask()&lt;/code&gt; &lt;/a&gt; ). 이 연산자는 내부 구현 세부 사항을 나타 내기 위해 밑줄이 접두어로 붙으며 통합 된 희소 텐서와 함께 작동하는 코드는 통합되지 않은 희소 텐서와 작동하지 않을 수 있으므로주의해서 사용해야합니다. 일반적으로 이러한 연산자와 함께 작업하기 전에 명시 적으로 병합하는 것이 가장 안전합니다.</target>
        </trans-unit>
        <trans-unit id="a158648517089dfe2e8140b70f8bef8fa8b3034b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#dataloader-collate-fn&quot;&gt;this section&lt;/a&gt; on more about &lt;code&gt;collate_fn&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;collate_fn&lt;/code&gt; 에 대한 자세한 내용은 &lt;a href=&quot;#dataloader-collate-fn&quot;&gt;이 섹션&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="36c23f7f90f14da5821a9b8b12151f9fc9d50660" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#dataset-types&quot;&gt;Dataset Types&lt;/a&gt; for more details on these two types of datasets and how &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; interacts with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;Multi-process data loading&lt;/a&gt;.</source>
          <target state="translated">이 두 가지 유형의 데이터 세트에 대한 자세한 내용과 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 이 &lt;a href=&quot;#multi-process-data-loading&quot;&gt;다중 프로세스 데이터로드&lt;/a&gt; 와 상호 작용 하는 방법에 대한 자세한 내용 은 &lt;a href=&quot;#dataset-types&quot;&gt;데이터 세트 유형&lt;/a&gt; 을 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="b547fa358f4d375a4ada3bd99e7498b8afae4202" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#module-torch.utils.data&quot;&gt;&lt;code&gt;torch.utils.data&lt;/code&gt;&lt;/a&gt; documentation page for more details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;#module-torch.utils.data&quot;&gt; &lt;code&gt;torch.utils.data&lt;/code&gt; &lt;/a&gt; 문서 페이지를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="5b6dced15fa8a0213a82f02ae1effe79ccf48d16" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; for restrictions on tensor names.</source>
          <target state="translated">텐서 이름에 대한 제한 사항 은 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="243a5526f4842ad1af50687b538ec9b7b7b4698f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.cuda.max_memory_allocated&quot;&gt;&lt;code&gt;max_memory_allocated()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;#torch.cuda.max_memory_allocated&quot;&gt; &lt;code&gt;max_memory_allocated()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="18ad60e0a411bdc88fa0f09bd839bc7ff6f95e28" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.cuda.max_memory_cached&quot;&gt;&lt;code&gt;max_memory_cached()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;#torch.cuda.max_memory_cached&quot;&gt; &lt;code&gt;max_memory_cached()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="d23b93420a80f6bb1829feef82f05084851c1675" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">자세한 내용과 출력 형태는 &lt;a href=&quot;#torch.nn.quantized.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="b27704ec42ef1503a1d419e6619ba79f9a13ebfb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">자세한 내용과 출력 형태는 &lt;a href=&quot;#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="56a1d3982f38299aa1d9205f01ea00cd34354fef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">자세한 내용과 출력 형태는 &lt;a href=&quot;#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="b4ccb661511817b74aa1b611837a5a3335536f1d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.checkpoint.checkpoint&quot;&gt;&lt;code&gt;checkpoint()&lt;/code&gt;&lt;/a&gt; on how checkpointing works.</source>
          <target state="translated">체크 포인트가 작동하는 방식에 대해서는 &lt;a href=&quot;#torch.utils.checkpoint.checkpoint&quot;&gt; &lt;code&gt;checkpoint()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="592148f1f545aa4c74a139446df1dc46e1b6f5ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt; for a description of arguments omitted below.</source>
          <target state="translated">아래에서 생략 된 인수에 대한 설명은 &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3896f3dcf8c1d0bd4419fbc3e2eb2617d32af3a0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt; &lt;code&gt;Dataset&lt;/code&gt; &lt;/a&gt; 를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="1b587694da52c7658cba1e5d21c7a0bf9aad6739" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 을 참조하세요.</target>
        </trans-unit>
        <trans-unit id="dd3e6f221b9ffca56a618c8ce4986b9d9a3b9a59" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#variable-resolution&quot;&gt;Variable Resolution&lt;/a&gt; for how variables are resolved.</source>
          <target state="translated">&lt;a href=&quot;#variable-resolution&quot;&gt;변수가 해결&lt;/a&gt; 되는 방법 은 변수 해결 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="6663536c00122040c87a44bdc2b745caee1f1423" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../jit#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; for details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;../jit#inspecting-code&quot;&gt;코드 검사&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="eb8c27e43e09f15fd524c6c5a38cd014825e6c23" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">뷰를 반환 할 수있는 경우 &lt;a href=&quot;../tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0410d55962a7fc08aaf5d7b12288cb6edf33cd4b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="ced4292c46e2d0f9084a0d4bbafd060bcaf8de87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="d557ccd80fb3258ddffbbb8174134a02d6cf179c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="789fcdc7d8a9ef51de9ddf2ef8954e307a7dd712" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.add#torch.add&quot;&gt;&lt;code&gt;torch.add()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.add#torch.add&quot;&gt; &lt;code&gt;torch.add()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="37a8639e7cd42640a040dc7726bd5f806eb0bd35" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addbmm#torch.addbmm&quot;&gt;&lt;code&gt;torch.addbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addbmm#torch.addbmm&quot;&gt; &lt;code&gt;torch.addbmm()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="d031d4e4498ba97f52d6d80a33dad228fb9f2cbe" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addcdiv#torch.addcdiv&quot;&gt;&lt;code&gt;torch.addcdiv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addcdiv#torch.addcdiv&quot;&gt; &lt;code&gt;torch.addcdiv()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="565ec468f6c035b17c0ac0bb82ee2cbcd8edaca7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addcmul#torch.addcmul&quot;&gt;&lt;code&gt;torch.addcmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addcmul#torch.addcmul&quot;&gt; &lt;code&gt;torch.addcmul()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="b2d04f2a20233d7e081134dcc2710d272865025a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt; &lt;code&gt;torch.addmm()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="fbb6e45db46d337e88fb01997ddfd0d38b1e36e5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addmv#torch.addmv&quot;&gt;&lt;code&gt;torch.addmv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addmv#torch.addmv&quot;&gt; &lt;code&gt;torch.addmv()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="0415452e192f39adc1dfb989387d7c4f3dd04593" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addr#torch.addr&quot;&gt;&lt;code&gt;torch.addr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.addr#torch.addr&quot;&gt; &lt;code&gt;torch.addr()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="7040368b881cece8154506daeab47e7b26bc4465" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt;&lt;code&gt;torch.allclose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt; &lt;code&gt;torch.allclose()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="b5b8be1187577d5e5a0b32510f1f32129626cca8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.amax#torch.amax&quot;&gt;&lt;code&gt;torch.amax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.amax#torch.amax&quot;&gt; &lt;code&gt;torch.amax()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="bac9b8922811f35c234bf464a4494dbef74933b2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.amin#torch.amin&quot;&gt;&lt;code&gt;torch.amin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.amin#torch.amin&quot;&gt; &lt;code&gt;torch.amin()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="d5518b7da2603866919ecc93573fb7693cbe1d28" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;torch.angle()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;torch.angle()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="6d6f36d28f663492799df5b5fcb97f2c82893849" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arccos#torch.arccos&quot;&gt;&lt;code&gt;torch.arccos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arccos#torch.arccos&quot;&gt; &lt;code&gt;torch.arccos()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="95b1cd872f49253eed8e157b208ed8426d2a08f0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arccosh#torch.arccosh&quot;&gt;&lt;code&gt;torch.arccosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arccosh#torch.arccosh&quot;&gt; &lt;code&gt;torch.arccosh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="20d7592e91b8ac1020ec98e06fb5bdf07f1891bc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arcsin#torch.arcsin&quot;&gt;&lt;code&gt;torch.arcsin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arcsin#torch.arcsin&quot;&gt; &lt;code&gt;torch.arcsin()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="92063535c0e36748fc07d3768b27df3f9db98d33" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arcsinh#torch.arcsinh&quot;&gt;&lt;code&gt;torch.arcsinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arcsinh#torch.arcsinh&quot;&gt; &lt;code&gt;torch.arcsinh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="f471a52f4701848a5c13052260382f54ba5f2bf2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arctan#torch.arctan&quot;&gt;&lt;code&gt;torch.arctan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arctan#torch.arctan&quot;&gt; &lt;code&gt;torch.arctan()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="6352e358921d4176f31796abd8e2e1d94c2c4807" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arctanh#torch.arctanh&quot;&gt;&lt;code&gt;torch.arctanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.arctanh#torch.arctanh&quot;&gt; &lt;code&gt;torch.arctanh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="8a3005660e169df440b195c1013e466fc0dc0f85" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argmax#torch.argmax&quot;&gt;&lt;code&gt;torch.argmax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.argmax#torch.argmax&quot;&gt; &lt;code&gt;torch.argmax()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="9336c76dce7b82519130c38591a8f4bc511802b3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argmin#torch.argmin&quot;&gt;&lt;code&gt;torch.argmin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.argmin#torch.argmin&quot;&gt; &lt;code&gt;torch.argmin()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="8db3f6696a6f968e7692af8c5ef5b20e20027f77" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argsort#torch.argsort&quot;&gt;&lt;code&gt;torch.argsort()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.argsort#torch.argsort&quot;&gt; &lt;code&gt;torch.argsort()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="4a2ff963efa23ea4ed3e29949ad7d7c3a624603f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.as_strided#torch.as_strided&quot;&gt;&lt;code&gt;torch.as_strided()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.as_strided#torch.as_strided&quot;&gt; &lt;code&gt;torch.as_strided()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="885b7e053cac4178633ab2cdae8c61527522af3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="4b825056a4e3742071a4388ba1c9dd880e2f684d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="85ed8161a1e7cf3d498c0e025910cad264dbb99e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="7c4e94754d2f650204d6da4aafe2bedeb278399d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atan2#torch.atan2&quot;&gt;&lt;code&gt;torch.atan2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.atan2#torch.atan2&quot;&gt; &lt;code&gt;torch.atan2()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="ed04a790c582bea4569ce9ba2a1f5fb49e96591b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="2f172a9267504e77b4582e306093325cd390dbe6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.baddbmm#torch.baddbmm&quot;&gt;&lt;code&gt;torch.baddbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.baddbmm#torch.baddbmm&quot;&gt; &lt;code&gt;torch.baddbmm()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="0ee51350ddf5c14b20523cca547052313b6da137" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt;&lt;code&gt;torch.bernoulli()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt; &lt;code&gt;torch.bernoulli()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="fa6bd5fd2fc0d4cbf209b32b685dcd15549e17a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bincount#torch.bincount&quot;&gt;&lt;code&gt;torch.bincount()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bincount#torch.bincount&quot;&gt; &lt;code&gt;torch.bincount()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="1330e38a3bff864a896452adaef91e316142ee7a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_and#torch.bitwise_and&quot;&gt;&lt;code&gt;torch.bitwise_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bitwise_and#torch.bitwise_and&quot;&gt; &lt;code&gt;torch.bitwise_and()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="b232e0fb96bd322c2458574164c04bbfc68e696b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_not#torch.bitwise_not&quot;&gt;&lt;code&gt;torch.bitwise_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bitwise_not#torch.bitwise_not&quot;&gt; &lt;code&gt;torch.bitwise_not()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="49bd1ecf94a298ed7136ddaa7b129d79af6f4f58" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_or#torch.bitwise_or&quot;&gt;&lt;code&gt;torch.bitwise_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bitwise_or#torch.bitwise_or&quot;&gt; &lt;code&gt;torch.bitwise_or()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="74f2dbe9a2909c1557b61d4ca27fc1dfa0ee2a38" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_xor#torch.bitwise_xor&quot;&gt;&lt;code&gt;torch.bitwise_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bitwise_xor#torch.bitwise_xor&quot;&gt; &lt;code&gt;torch.bitwise_xor()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="50e2d514fed6ae782003300fa73df0e3bb831323" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bmm#torch.bmm&quot;&gt;&lt;code&gt;torch.bmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.bmm#torch.bmm&quot;&gt; &lt;code&gt;torch.bmm()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="1c1407ae846f94e16c4554a9aed2a962bd7d5b41" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ceil#torch.ceil&quot;&gt;&lt;code&gt;torch.ceil()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ceil#torch.ceil&quot;&gt; &lt;code&gt;torch.ceil()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="0ee9005d9f7612dda3673351dab212c7597eacf9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky#torch.cholesky&quot;&gt;&lt;code&gt;torch.cholesky()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cholesky#torch.cholesky&quot;&gt; &lt;code&gt;torch.cholesky()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="9234f12a7920c48c92b0b92789cbf7f520e6cce2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky_inverse#torch.cholesky_inverse&quot;&gt;&lt;code&gt;torch.cholesky_inverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cholesky_inverse#torch.cholesky_inverse&quot;&gt; &lt;code&gt;torch.cholesky_inverse()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="95412d4a68f7be2f448d2f6abdef3ba1039693ce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky_solve#torch.cholesky_solve&quot;&gt;&lt;code&gt;torch.cholesky_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cholesky_solve#torch.cholesky_solve&quot;&gt; &lt;code&gt;torch.cholesky_solve()&lt;/code&gt; &lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="70d52135d5fe5109fdf4b68140efc30ed3ff15ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.chunk#torch.chunk&quot;&gt;&lt;code&gt;torch.chunk()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.chunk#torch.chunk&quot;&gt; &lt;code&gt;torch.chunk()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="9d2a8fa141c274f1ecc888c9dee97f508028c310" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="db643765ea3bdabe789a609a784f6f21e8b15ff6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.clone#torch.clone&quot;&gt;&lt;code&gt;torch.clone()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.clone#torch.clone&quot;&gt; &lt;code&gt;torch.clone()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="2c5dee157b17c3fc64e10f3c111962a223627009" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.conj#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.conj#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="00b6992059c4a2ecf9c5ba83b4b4d6367e100b3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cos#torch.cos&quot;&gt;&lt;code&gt;torch.cos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cos#torch.cos&quot;&gt; &lt;code&gt;torch.cos()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="5f760d7b54a866c270d1f4f0fee69afc86260ce1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cosh#torch.cosh&quot;&gt;&lt;code&gt;torch.cosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cosh#torch.cosh&quot;&gt; &lt;code&gt;torch.cosh()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="2e6ca2ba899e5a5ed0308b11c6dec0f921c1c386" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.count_nonzero#torch.count_nonzero&quot;&gt;&lt;code&gt;torch.count_nonzero()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.count_nonzero#torch.count_nonzero&quot;&gt; &lt;code&gt;torch.count_nonzero()&lt;/code&gt; &lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3154a66df758bedd23c1ca8d445818980c877056" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cross#torch.cross&quot;&gt;&lt;code&gt;torch.cross()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cross#torch.cross&quot;&gt; &lt;code&gt;torch.cross()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="c33b1f8ea95f5374185b2beef8bdc0333006ca40" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cummax#torch.cummax&quot;&gt;&lt;code&gt;torch.cummax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cummax#torch.cummax&quot;&gt; &lt;code&gt;torch.cummax()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="b7a6c4b7502006ffffe7f61bc361df0a8f5c3485" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cummin#torch.cummin&quot;&gt;&lt;code&gt;torch.cummin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cummin#torch.cummin&quot;&gt; &lt;code&gt;torch.cummin()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="175fd7991a76a77d1e7947c94b32c3900db2a22a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cumprod#torch.cumprod&quot;&gt;&lt;code&gt;torch.cumprod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cumprod#torch.cumprod&quot;&gt; &lt;code&gt;torch.cumprod()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="ecda9d651f57ce0a80f553759df7118f9fc98f17" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cumsum#torch.cumsum&quot;&gt;&lt;code&gt;torch.cumsum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.cumsum#torch.cumsum&quot;&gt; &lt;code&gt;torch.cumsum()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="e4c2bb557dabd8b8d6424a1086a8f0e49082dcc1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.deg2rad#torch.deg2rad&quot;&gt;&lt;code&gt;torch.deg2rad()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.deg2rad#torch.deg2rad&quot;&gt; &lt;code&gt;torch.deg2rad()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="74fe8f78b0f15bbe31849b729e7353cfc88dda04" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="87d829bc645704d604ad02215c0bb4c96717d9da" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diag#torch.diag&quot;&gt;&lt;code&gt;torch.diag()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.diag#torch.diag&quot;&gt; &lt;code&gt;torch.diag()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="d17eea2fe4310acb0e6689f3868245d146ae748a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="07ddff47af47155cb8195d49fd74ab5f110f0de6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diagflat#torch.diagflat&quot;&gt;&lt;code&gt;torch.diagflat()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.diagflat#torch.diagflat&quot;&gt; &lt;code&gt;torch.diagflat()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="e9871639ca6ca069867ff9833f0d0bad21e81d3f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="fd7c0b43d807db12fbff5fc78c8898e1347cf1a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.digamma#torch.digamma&quot;&gt;&lt;code&gt;torch.digamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.digamma#torch.digamma&quot;&gt; &lt;code&gt;torch.digamma()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="ec5b576fe8744daae4dc7030555211156b888d46" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.dist#torch.dist&quot;&gt;&lt;code&gt;torch.dist()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.dist#torch.dist&quot;&gt; &lt;code&gt;torch.dist()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="8ab62666946a8333d7379a60a2d1d9d3ae225fa2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="0cfd78ad9784ac679c7a2b9b5bca896325ba89a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.divide#torch.divide&quot;&gt;&lt;code&gt;torch.divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.divide#torch.divide&quot;&gt; &lt;code&gt;torch.divide()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="593f13a108fe5ed0675dcd7c508f6c35b99b0b10" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.dot#torch.dot&quot;&gt;&lt;code&gt;torch.dot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.dot#torch.dot&quot;&gt; &lt;code&gt;torch.dot()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="59a8ed7a18a443621255817cd4ad0bf4a0e632ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.eig#torch.eig&quot;&gt;&lt;code&gt;torch.eig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.eig#torch.eig&quot;&gt; &lt;code&gt;torch.eig()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="5ab6c51079e17cb89630c1e4a91f1d3454d90b6f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.eq#torch.eq&quot;&gt;&lt;code&gt;torch.eq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.eq#torch.eq&quot;&gt; &lt;code&gt;torch.eq()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="4813e89ec9293f99dfa5025547b461fb432f69d3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.equal#torch.equal&quot;&gt;&lt;code&gt;torch.equal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.equal#torch.equal&quot;&gt; &lt;code&gt;torch.equal()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="3f7360444c9397fc1d5c5b1bc067d9af3c6e9edd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erf#torch.erf&quot;&gt;&lt;code&gt;torch.erf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.erf#torch.erf&quot;&gt; &lt;code&gt;torch.erf()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="08fb634ad4f6c40b3935c0b3378501a7339f8fbe" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erfc#torch.erfc&quot;&gt;&lt;code&gt;torch.erfc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.erfc#torch.erfc&quot;&gt; &lt;code&gt;torch.erfc()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="2bd6c425dc9b646c2b2bed4edf1c5a988917c21f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erfinv#torch.erfinv&quot;&gt;&lt;code&gt;torch.erfinv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.erfinv#torch.erfinv&quot;&gt; &lt;code&gt;torch.erfinv()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="ad29dad56d468c778a3c249cc44c87db316a44e6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.exp#torch.exp&quot;&gt;&lt;code&gt;torch.exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.exp#torch.exp&quot;&gt; &lt;code&gt;torch.exp()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="6b6fbfd00b7b75b48f9ccfcacaa40ee11ef98750" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.expm1#torch.expm1&quot;&gt;&lt;code&gt;torch.expm1()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.expm1#torch.expm1&quot;&gt; &lt;code&gt;torch.expm1()&lt;/code&gt; &lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4bbb47f4968cd75aa8ca7739f121cb837ee30c39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="a9a77b8be814999a1e18a3bcfe6fdc2b3f1b3bf0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fix#torch.fix&quot;&gt;&lt;code&gt;torch.fix()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.fix#torch.fix&quot;&gt; &lt;code&gt;torch.fix()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="aad979493ad2ea8c7660622a96be7811d0a81b87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.flip#torch.flip&quot;&gt;&lt;code&gt;torch.flip()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.flip#torch.flip&quot;&gt; &lt;code&gt;torch.flip()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="bad831a531ebee9d67a2612a1d0e6978cc7ed27b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fliplr#torch.fliplr&quot;&gt;&lt;code&gt;torch.fliplr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.fliplr#torch.fliplr&quot;&gt; &lt;code&gt;torch.fliplr()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="87f6a8078b8a3f6f5a4247460381f4e94549ef39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.flipud#torch.flipud&quot;&gt;&lt;code&gt;torch.flipud()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.flipud#torch.flipud&quot;&gt; &lt;code&gt;torch.flipud()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="399317938ad3be99c06e85c37e27d75701458b99" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.floor#torch.floor&quot;&gt;&lt;code&gt;torch.floor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.floor#torch.floor&quot;&gt; &lt;code&gt;torch.floor()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="bcdf651ea00fbe14dab01c29ec405b9ced982377" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.floor_divide#torch.floor_divide&quot;&gt;&lt;code&gt;torch.floor_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.floor_divide#torch.floor_divide&quot;&gt; &lt;code&gt;torch.floor_divide()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="2efd0d46fdaf4a4f7fc46d9063330efa363c69ce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fmod#torch.fmod&quot;&gt;&lt;code&gt;torch.fmod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.fmod#torch.fmod&quot;&gt; &lt;code&gt;torch.fmod()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="eb5589e94e3540a14e8e7b3fac4f8fdf408c5f2f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.frac#torch.frac&quot;&gt;&lt;code&gt;torch.frac()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.frac#torch.frac&quot;&gt; &lt;code&gt;torch.frac()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="fe3455e99f31e058dbdc692b113c1359d89488d7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.gather#torch.gather&quot;&gt;&lt;code&gt;torch.gather()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.gather#torch.gather&quot;&gt; &lt;code&gt;torch.gather()&lt;/code&gt; &lt;/a&gt; 참조</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
