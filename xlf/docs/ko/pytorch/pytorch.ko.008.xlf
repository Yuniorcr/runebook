<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="14ee5344d06a238323bb82692b3ba5195a694523" translate="yes" xml:space="preserve">
          <source>The RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.</source>
          <target state="translated">RPC 패키지는 또한 애플리케이션이 호출 수신자 측에서 주어진 함수를 처리하는 방법을 지정할 수 있도록하는 데코레이터를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="d97e45dd1ab295237092f3a7f655e6e94838339b" translate="yes" xml:space="preserve">
          <source>The RPC tutorials introduce users to the RPC framework, provide several example applications using &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; APIs, and demonstrate how to use &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;the profiler&lt;/a&gt; to profile RPC-based workloads.</source>
          <target state="translated">RPC 자습서는 사용자에게 RPC 프레임 워크를 소개하고 &lt;a href=&quot;#distributed-rpc-framework&quot;&gt;torch.distributed.rpc&lt;/a&gt; API를 사용하는 여러 예제 애플리케이션을 제공 &lt;a href=&quot;https://pytorch.org/docs/stable/autograd.html#profiler&quot;&gt;하며 프로파일 러&lt;/a&gt; 를 사용 하여 RPC 기반 워크로드를 프로파일 링 하는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="1362003f90d57874aad22424c41e2c89849947f1" translate="yes" xml:space="preserve">
          <source>The RRef design note covers the design of the &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) protocol used to refer to values on remote workers by the framework.</source>
          <target state="translated">RRef 디자인 노트는 프레임 워크에서 원격 작업자의 값을 참조하는 데 사용되는 &lt;a href=&quot;#rref&quot;&gt;RRef&lt;/a&gt; (Remote REFerence) 프로토콜 의 디자인을 다룹니다 .</target>
        </trans-unit>
        <trans-unit id="e195940e5abd6ebbfad00ac76917c59c49c9651e" translate="yes" xml:space="preserve">
          <source>The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft function.</source>
          <target state="translated">STFT는 입력의 짧은 겹치는 창에 대한 푸리에 변환을 계산합니다. 이것은 시간에 따라 변화하는 신호의 주파수 성분을 제공합니다. 이 함수의 인터페이스는 &lt;a href=&quot;https://librosa.org/doc/latest/generated/librosa.stft.html&quot;&gt;librosa&lt;/a&gt; stft 함수를 모델로 합니다.</target>
        </trans-unit>
        <trans-unit id="56e279d91a2e6e157edb87095248831fbd79e513" translate="yes" xml:space="preserve">
          <source>The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:</source>
          <target state="translated">SummaryWriter 클래스는 TensorBoard의 소비 및 시각화를 위해 데이터를 기록하는 기본 항목입니다. 예를 들면 :</target>
        </trans-unit>
        <trans-unit id="e88b194ff4e6feceb2bfaaa1ec38472db3a49b02" translate="yes" xml:space="preserve">
          <source>The TensorPipe agent, which is the default, leverages &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;the TensorPipe library&lt;/a&gt;, which provides a natively point-to-point communication primitive specifically suited for machine learning that fundamentally addresses some of the limitations of Gloo. Compared to Gloo, it has the advantage of being asynchronous, which allows a large number of transfers to occur simultaneously, each at their own speed, without blocking each other. It will only open pipes between pairs of nodes when needed, on demand, and when one node fails only its incident pipes will be closed, while all other ones will keep working as normal. In addition, it is able to support multiple different transports (TCP, of course, but also shared memory, NVLink, InfiniBand, &amp;hellip;) and can automatically detect their availability and negotiate the best transport to use for each pipe.</source>
          <target state="translated">기본값 인 TensorPipe 에이전트 &lt;a href=&quot;https://github.com/pytorch/tensorpipe&quot;&gt;는 TensorPipe 라이브러리를&lt;/a&gt; 활용 합니다., 이는 Gloo의 몇 가지 한계를 근본적으로 해결하는 머신 러닝에 특히 적합한 기본 지점 간 통신 기본 요소를 제공합니다. Gloo와 비교할 때 비동기식이라는 장점이있어 서로를 차단하지 않고 각각 자체 속도로 많은 수의 전송이 동시에 발생할 수 있습니다. 필요할 때 노드 쌍 사이의 파이프 만 열고 한 노드가 실패하면 사고 파이프 만 닫히고 다른 모든 파이프는 정상적으로 작동합니다. 또한 여러 다른 전송 (물론 TCP는 물론 공유 메모리, NVLink, InfiniBand 등)을 지원할 수 있으며 자동으로 가용성을 감지하고 각 파이프에 사용할 최상의 전송을 협상 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d362a2db0f7abde4a8479b2b7feada60ead120fd" translate="yes" xml:space="preserve">
          <source>The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.</source>
          <target state="translated">TensorPipe 백엔드는 PyTorch v1.6에 도입되었으며 활발하게 개발되고 있습니다. 현재로서는 CPU 텐서 만 지원하며 곧 GPU 지원이 제공 될 예정입니다. Gloo와 마찬가지로 TCP 기반 전송과 함께 제공됩니다. 또한 매우 높은 대역폭을 달성하기 위해 여러 소켓 및 스레드에서 대형 텐서를 자동으로 청크하고 다중화 할 수 있습니다. 상담원은 개입없이 스스로 최상의 운송 수단을 선택할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fe34bfd2269a025104eb8b4a08ae444d0c47f764" translate="yes" xml:space="preserve">
          <source>The TorchScript compiler needs to know the types of &lt;code&gt;module attributes&lt;/code&gt;. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">TorchScript 컴파일러는 &lt;code&gt;module attributes&lt;/code&gt; 의 유형을 알아야 합니다 . 대부분의 유형은 멤버 값에서 유추 할 수 있습니다. 빈 목록과 사전은 유형을 추론 할 수 없으며 &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526 스타일&lt;/a&gt; 클래스 주석으로 주석을 달아야합니다. 유형을 추론 할 수없고 명시 적으로 주석을 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 경우 결과 ScriptModule에 속성으로 추가되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d526d524819b552e08647a3450928e08c0be85f0" translate="yes" xml:space="preserve">
          <source>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows</source>
          <target state="translated">COCO val2017에서 평가 된 사전 훈련 된 모델의 정확도는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fd6ae7d26b925d9ddede1463aabdca8219c04ca1" translate="yes" xml:space="preserve">
          <source>The algorithm used for interpolation is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">보간에 사용되는 알고리즘은 &lt;code&gt;mode&lt;/code&gt; 의해 결정됩니다 .</target>
        </trans-unit>
        <trans-unit id="50402614be52e24dbcb07aaaf1f84990228308aa" translate="yes" xml:space="preserve">
          <source>The algorithm used for upsampling is determined by &lt;code&gt;mode&lt;/code&gt;.</source>
          <target state="translated">업 샘플링에 사용되는 알고리즘은 &lt;code&gt;mode&lt;/code&gt; 의해 결정됩니다 .</target>
        </trans-unit>
        <trans-unit id="9dbbacc61ab643f749eabb1318de7d252379ec8c" translate="yes" xml:space="preserve">
          <source>The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.</source>
          <target state="translated">업 샘플링에 사용할 수있는 알고리즘은 각각 3D, 4D 및 5D 입력 Tensor에 대해 가장 가까운 이웃 및 선형, 쌍 선형, 쌍 입방 형 및 삼선 형입니다.</target>
        </trans-unit>
        <trans-unit id="bdd6673496c6fd17d43bfb63694d67b1a23e7074" translate="yes" xml:space="preserve">
          <source>The approximate decimal resolution of this type, i.e., &lt;code&gt;10**-precision&lt;/code&gt;.</source>
          <target state="translated">이 유형의 대략적인 10 진수 해상도, 즉 &lt;code&gt;10**-precision&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="11e64a47029713e51445813986a3824ac2548079" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; 인수 는 고려할 대각선을 제어합니다. 경우 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; = 0의 주 대각선 위의 모든 요소가 유지된다. 양수 값은 주 대각선 위의 많은 대각선을 제외하고 마찬가지로 음수 값은 주 대각선 아래의 많은 대각선을 포함합니다. 주 대각선은 인덱스 집합입니다.</target>
        </trans-unit>
        <trans-unit id="2720b921eb619ce89ed4f91beabc1047581eca82" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider. If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; 인수 는 고려할 대각선을 제어합니다. 경우 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; = 0, 주 대각선 아래의 모든 요소가 유지된다. 양수 값은 주 대각선 위의 대각선을 포함하고 마찬가지로 음수 값은 주 대각선 아래의 많은 대각선을 제외합니다. 주 대각선은 인덱스 집합입니다.</target>
        </trans-unit>
        <trans-unit id="eff44d81484d8a3edd9c1b438e09ac7f19f85991" translate="yes" xml:space="preserve">
          <source>The argument &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; controls which diagonal to consider:</source>
          <target state="translated">&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; 인수 는 고려할 대각선을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="5cfcdff9c158bfd56a616516f4454be040717f37" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">인수 &lt;code&gt;offset&lt;/code&gt; 은 고려할 대각선을 제어합니다. 경우 &lt;code&gt;offset&lt;/code&gt; = 0의 주 대각선 위의 모든 요소가 유지된다. 양수 값은 주 대각선 위의 많은 대각선을 제외하고 마찬가지로 음수 값은 주 대각선 아래의 많은 대각선을 포함합니다. 주 대각선은 인덱스 집합입니다.</target>
        </trans-unit>
        <trans-unit id="d114863c5d0c0b07094ca0c20e0cd672c5d3f900" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider. If &lt;code&gt;offset&lt;/code&gt; = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices</source>
          <target state="translated">인수 &lt;code&gt;offset&lt;/code&gt; 은 고려할 대각선을 제어합니다. 경우 &lt;code&gt;offset&lt;/code&gt; = 0, 주 대각선 아래의 모든 요소가 유지된다. 양수 값은 주 대각선 위의 대각선을 포함하고 마찬가지로 음수 값은 주 대각선 아래의 많은 대각선을 제외합니다. 주 대각선은 인덱스 집합입니다.</target>
        </trans-unit>
        <trans-unit id="19b889e5a4344dcdd3129928b40b0f46befb2501" translate="yes" xml:space="preserve">
          <source>The argument &lt;code&gt;offset&lt;/code&gt; controls which diagonal to consider:</source>
          <target state="translated">인수 &lt;code&gt;offset&lt;/code&gt; 은 고려할 대각선을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="54707c812db6f29dfe9835066cfce0408bd514d9" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;. However, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this instead returns the results multiplied by</source>
          <target state="translated">인수 사양은 &lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 와 거의 동일합니다 . 그러나 &lt;code&gt;normalized&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; 로 설정 되면 대신 결과를 곱한 결과를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="3bf649a8dfc260fcc9e9737948bc65802a397444" translate="yes" xml:space="preserve">
          <source>The argument specifications are almost identical with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;. Similar to &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;, if &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by multiplying it with</source>
          <target state="translated">인수 사양은 &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 와 거의 동일합니다 . 유사하게 &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 경우, &lt;code&gt;normalized&lt;/code&gt; 로 설정 &lt;code&gt;True&lt;/code&gt; , 이것은 함께 곱한 결과를 정규화</target>
        </trans-unit>
        <trans-unit id="cf4c102338f979040fb6ff061754881df7061681" translate="yes" xml:space="preserve">
          <source>The backend of the given process group as a lower case string.</source>
          <target state="translated">주어진 프로세스 그룹의 백엔드 (소문자 문자열).</target>
        </trans-unit>
        <trans-unit id="e19263dbfb8f739b55183f8eb835a3eac6ee2887" translate="yes" xml:space="preserve">
          <source>The backend options class for &lt;code&gt;ProcessGroupAgent&lt;/code&gt;, which is derived from &lt;code&gt;RpcBackendOptions&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;RpcBackendOptions&lt;/code&gt; 에서 파생 된 &lt;code&gt;ProcessGroupAgent&lt;/code&gt; 의 백엔드 옵션 클래스입니다 .</target>
        </trans-unit>
        <trans-unit id="ced0f1f27d8dec6f68219ca478db12401a16eb9d" translate="yes" xml:space="preserve">
          <source>The backend options for &lt;code&gt;TensorPipeAgent&lt;/code&gt;, derived from &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">대한 백엔드 옵션 &lt;code&gt;TensorPipeAgent&lt;/code&gt; 에서 파생 &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt; &lt;code&gt;RpcBackendOptions&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="502ce5e4564798f5189d3ade0d0a77ccd7a4b686" translate="yes" xml:space="preserve">
          <source>The backward method does not support sparse and complex inputs. It works only when &lt;code&gt;B&lt;/code&gt; is not provided (i.e. &lt;code&gt;B == None&lt;/code&gt;). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</source>
          <target state="translated">역방향 방법은 희소 및 복합 입력을 지원하지 않습니다. &lt;code&gt;B&lt;/code&gt; 가 제공되지 않은 경우에만 작동합니다 (예 : &lt;code&gt;B == None&lt;/code&gt; ). 우리는 확장을 위해 적극적으로 작업하고 있으며 알고리즘의 세부 사항은 즉시 게시 될 예정입니다.</target>
        </trans-unit>
        <trans-unit id="977219b95a63d49a9a83d568761b5dea94f71247" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used locally.</source>
          <target state="translated">배치 크기는 로컬에서 사용되는 GPU 수보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="23b97210326ef320e2d1633f1e69b26cc12a15dc" translate="yes" xml:space="preserve">
          <source>The batch size should be larger than the number of GPUs used.</source>
          <target state="translated">배치 크기는 사용 된 GPU 수보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="1822f0d582e28f7680fb7e40c87465297eaceb93" translate="yes" xml:space="preserve">
          <source>The behavior depends on the dimensionality of the tensors as follows:</source>
          <target state="translated">동작은 다음과 같이 텐서의 차원에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2f9d3a99640c5733b646d772d2660cd0aaa3e59f" translate="yes" xml:space="preserve">
          <source>The behavior of the model changes depending if it is in training or evaluation mode.</source>
          <target state="translated">모델의 동작은 학습 또는 평가 모드에 따라 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="671bda2b5e08e92d9c6a9bee65efdef513a603fd" translate="yes" xml:space="preserve">
          <source>The boolean argument &lt;code&gt;eigenvectors&lt;/code&gt; defines computation of both eigenvectors and eigenvalues or eigenvalues only.</source>
          <target state="translated">부울 인수 &lt;code&gt;eigenvectors&lt;/code&gt; 는 고유 벡터와 고유 값 또는 고유 값의 계산을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="a803e781482443f16488bb7ddd92d4c8599d93d8" translate="yes" xml:space="preserve">
          <source>The boolean option &lt;code&gt;sorted&lt;/code&gt; if &lt;code&gt;True&lt;/code&gt;, will make sure that the returned &lt;code&gt;k&lt;/code&gt; elements are themselves sorted</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 &lt;code&gt;sorted&lt;/code&gt; 부울 옵션 은 반환 된 &lt;code&gt;k&lt;/code&gt; 요소가 자체적으로 정렬되도록합니다.</target>
        </trans-unit>
        <trans-unit id="7230c4dcc10bf6edcaa500d739b7ece219bab321" translate="yes" xml:space="preserve">
          <source>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</source>
          <target state="translated">캐싱 할당자는 텐서가 할당 된 스트림 만 인식합니다. 인식으로 인해 이미 하나의 스트림에서만 텐서의 수명주기를 올바르게 관리합니다. 그러나 텐서를 원본 스트림과 다른 스트림에서 사용하면 할당자가 예기치 않게 메모리를 재사용 할 수 있습니다. 이 메서드를 호출하면 할당자가 어떤 스트림이 텐서를 사용했는지 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="750a4192b0cf8d28e81899b152edaaa9a146f0c9" translate="yes" xml:space="preserve">
          <source>The case when</source>
          <target state="translated">경우</target>
        </trans-unit>
        <trans-unit id="1a15a1c2561662d55e6adec5b59058d1e70d3911" translate="yes" xml:space="preserve">
          <source>The columns of the output matrix are elementwise powers of the input vector</source>
          <target state="translated">출력 행렬의 열은 입력 벡터의 요소 별 거듭 제곱입니다.</target>
        </trans-unit>
        <trans-unit id="ce6592c2eac0498a5cc7f983f7cee34a07c28d68" translate="yes" xml:space="preserve">
          <source>The constructor of &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; can be called without argument, in which case the class is created for the pytorch default dtype (as returned by &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt;&lt;code&gt;torch.get_default_dtype()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt; 의 생성자는 인수없이 호출 될 수 있으며,이 경우 클래스는 pytorch 기본 dtype ( &lt;a href=&quot;generated/torch.get_default_dtype#torch.get_default_dtype&quot;&gt; &lt;code&gt;torch.get_default_dtype()&lt;/code&gt; &lt;/a&gt; 의해 반환 됨 )에 대해 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="c4ae76d927f18889b6dd2cecc1dcf754fe0ab549" translate="yes" xml:space="preserve">
          <source>The contents of a tensor can be accessed and modified using Python&amp;rsquo;s indexing and slicing notation:</source>
          <target state="translated">텐서의 내용은 Python의 인덱싱 및 슬라이싱 표기법을 사용하여 액세스하고 수정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="68f6bd6745cd407cbd762333af1947bd80a473cd" translate="yes" xml:space="preserve">
          <source>The context managers &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;torch.no_grad()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt;&lt;code&gt;torch.enable_grad()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;torch.set_grad_enabled()&lt;/code&gt;&lt;/a&gt; are helpful for locally disabling and enabling gradient computation. See &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;Locally disabling gradient computation&lt;/a&gt; for more details on their usage. These context managers are thread local, so they won&amp;rsquo;t work if you send work to another thread using the &lt;code&gt;threading&lt;/code&gt; module, etc.</source>
          <target state="translated">컨텍스트 관리자 &lt;a href=&quot;generated/torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;torch.no_grad()&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/torch.enable_grad#torch.enable_grad&quot;&gt; &lt;code&gt;torch.enable_grad()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;torch.set_grad_enabled()&lt;/code&gt; &lt;/a&gt; 는 로컬에서 그래디언트 계산을 비활성화하고 활성화하는 데 유용합니다. 사용법에 대한 자세한 내용 은 &lt;a href=&quot;autograd#locally-disable-grad&quot;&gt;로컬로 그라디언트 계산 비활성화를&lt;/a&gt; 참조하십시오 . 이러한 컨텍스트 관리자는 스레드 로컬이므로 &lt;code&gt;threading&lt;/code&gt; 모듈 등을 사용하여 다른 스레드로 작업을 보내면 작동하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="6ca40d5babc478c27564467be749f0f09c316678" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;n&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="translated">Hermitian 입력 값의 올바른 해석은 &lt;code&gt;n&lt;/code&gt; 에서 제공하는 원래 데이터의 길이에 따라 다릅니다 . 이는 각 입력 모양이 홀수 또는 짝수 길이 신호에 해당 할 수 있기 때문입니다. 기본적으로 신호는 짝수 길이로 간주되며 홀수 신호는 제대로 왕복하지 않습니다. 따라서 항상 신호 길이 &lt;code&gt;n&lt;/code&gt; 을 전달하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="15f3e882aab04b4ad9da40b79636691c5dd75e80" translate="yes" xml:space="preserve">
          <source>The correct interpretation of the Hermitian input depends on the length of the original data, as given by &lt;code&gt;s&lt;/code&gt;. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape &lt;code&gt;s&lt;/code&gt;.</source>
          <target state="translated">Hermitian 입력 값의 올바른 해석은 &lt;code&gt;s&lt;/code&gt; 로 주어진 원래 데이터의 길이에 따라 다릅니다 . 이는 각 입력 모양이 홀수 또는 짝수 길이 신호에 해당 할 수 있기 때문입니다. 기본적으로 신호는 짝수 길이로 간주되며 홀수 신호는 제대로 왕복하지 않습니다. 따라서 항상 신호 모양 &lt;code&gt;s&lt;/code&gt; 를 전달하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="b2f2b30024847cce229c4bda1e1b5ab964a9eaf4" translate="yes" xml:space="preserve">
          <source>The criterion only considers a contiguous block of non-negative targets that starts at the front.</source>
          <target state="translated">기준은 앞쪽에서 시작하는 음이 아닌 대상의 연속 블록 만 고려합니다.</target>
        </trans-unit>
        <trans-unit id="e11b9195cb3521aafd237a69ce124b8d788f6886" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">현재 구현에는 많은 작업을 수행 하는 복잡한 &lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 대해 제시된 동작이 없습니다 . 일부 실패의 경우 &lt;code&gt;grad_input&lt;/code&gt; 및 &lt;code&gt;grad_output&lt;/code&gt; 은 입력 및 출력의 하위 집합에 대한 기울기 만 포함합니다. 이러한 &lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; 의&lt;/a&gt; 경우 특정 입력 또는 출력에서 ​​직접 &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt; 사용 하여 필요한 그라데이션을 가져와야 합니다.</target>
        </trans-unit>
        <trans-unit id="e2bf74abb2f8760663c776314dd933141deb8a40" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">현재 구현에는 많은 작업을 수행 하는 복잡한 &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 대해 제시된 동작이 없습니다 . 일부 실패의 경우 &lt;code&gt;grad_input&lt;/code&gt; 및 &lt;code&gt;grad_output&lt;/code&gt; 은 입력 및 출력의 하위 집합에 대한 기울기 만 포함합니다. 이러한 &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; 의&lt;/a&gt; 경우 특정 입력 또는 출력에서 ​​직접 &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt; 사용 하여 필요한 그라데이션을 가져와야 합니다.</target>
        </trans-unit>
        <trans-unit id="41f3fc6117e6365cbfdd81a5124ee1886a82a466" translate="yes" xml:space="preserve">
          <source>The current implementation will not have the presented behavior for complex &lt;code&gt;Module&lt;/code&gt; that perform many operations. In some failure cases, &lt;code&gt;grad_input&lt;/code&gt; and &lt;code&gt;grad_output&lt;/code&gt; will only contain the gradients for a subset of the inputs and outputs. For such &lt;code&gt;Module&lt;/code&gt;, you should use &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt;&lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt;&lt;/a&gt; directly on a specific input or output to get the required gradients.</source>
          <target state="translated">현재 구현에는 많은 작업을 수행 하는 복잡한 &lt;code&gt;Module&lt;/code&gt; 대해 제시된 동작이 없습니다 . 일부 실패의 경우 &lt;code&gt;grad_input&lt;/code&gt; 및 &lt;code&gt;grad_output&lt;/code&gt; 은 입력 및 출력의 하위 집합에 대한 기울기 만 포함합니다. 이러한 &lt;code&gt;Module&lt;/code&gt; 의 경우 특정 입력 또는 출력에서 ​​직접 &lt;a href=&quot;../autograd#torch.Tensor.register_hook&quot;&gt; &lt;code&gt;torch.Tensor.register_hook()&lt;/code&gt; &lt;/a&gt; 사용 하여 필요한 그라데이션을 가져와야 합니다.</target>
        </trans-unit>
        <trans-unit id="0fcbf7f37e10aae7ed7b4feee2a1e7bd2fefc4c8" translate="yes" xml:space="preserve">
          <source>The default floating point dtype is initially &lt;code&gt;torch.float32&lt;/code&gt;.</source>
          <target state="translated">기본 부동 소수점 dtype은 처음에 &lt;code&gt;torch.float32&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="fed344676665a74468953d310a0df7afafaa24e3" translate="yes" xml:space="preserve">
          <source>The default floating point tensor type is initially &lt;code&gt;torch.FloatTensor&lt;/code&gt;.</source>
          <target state="translated">기본 부동 소수점 텐서 유형은 초기에 &lt;code&gt;torch.FloatTensor&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="a80bd362bedb3fcb6747d9fd75b4961eb0cdb1f6" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">이산 푸리에 변환은 분리 가능하므로 여기서 &lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; 은 두 개의 1 차원 &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 호출 과 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="68e62e57e835a4d91bc43ca48c95e3efa63535c9" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to two one-dimensional &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; calls:</source>
          <target state="translated">이산 푸리에 변환은 분리 가능하므로 여기서 &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; 은 두 개의 1 차원 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 호출 과 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="1984f4c44a0a732d169d5d6a5c5f08cad523bc50" translate="yes" xml:space="preserve">
          <source>The discrete Fourier transform is separable, so &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt; here is equivalent to a combination of &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">이산 푸리에 변환은 분리 가능하므로 여기서 &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; 은 &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 의 조합과 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="6def4e7a63f51a47c9f6b51fb1bb8c54875ce717" translate="yes" xml:space="preserve">
          <source>The distance swap is described in detail in the paper &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/a&gt; by V. Balntas, E. Riba et al.</source>
          <target state="translated">거리 스왑은 V. Balntas, E. Riba et al.의 &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;삼중 항 손실이있는 얕은 컨벌루션 특징 설명자 학습&lt;/a&gt; 논문에 자세히 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="e0ab87bc03e5a5294329af8c45b84a5da57986c5" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.</source>
          <target state="translated">분산 RPC 프레임 워크는 원격으로 기능을 쉽게 실행하고 실제 데이터를 복사하지 않고 원격 개체 참조를 지원하며 autograd 및 최적화 API를 제공하여 RPC 경계에서 투명하게 뒤로 실행하고 매개 변수를 업데이트합니다. 이러한 기능은 네 가지 API 세트로 ​​분류 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="58cfec2b510135f93e0701fc204e248929b783f2" translate="yes" xml:space="preserve">
          <source>The distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.</source>
          <target state="translated">분산 RPC 프레임 워크는 원격 통신을 허용하는 일련의 기본 요소를 통해 다중 머신 모델 학습을위한 메커니즘과 여러 머신에 분할 된 모델을 자동으로 구별하는 상위 수준 API를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="10060f95e0764042598d146fc06132ab3dfae090" translate="yes" xml:space="preserve">
          <source>The distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.</source>
          <target state="translated">분산 autograd 디자인 노트는 모델 병렬 학습과 같은 애플리케이션에 유용한 RPC 기반 분산 autograd 프레임 워크의 디자인을 다룹니다.</target>
        </trans-unit>
        <trans-unit id="dafbad2c73a3de536f4d90e5d5fa2fe3c91b9950" translate="yes" xml:space="preserve">
          <source>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; (by explicitly creating the store as an alternative to specifying &lt;code&gt;init_method&lt;/code&gt;.) There are 3 choices for Key-Value Stores: &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">분산 패키지는 분산 키-값 저장소와 함께 제공되며,이 저장소는 그룹의 프로세스간에 정보를 공유하고 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; 에서 분산 패키지 를 초기화하는 데 사용할 수 있습니다 ( 대안으로 저장소를 명시 적으로 생성하여 지정 &lt;code&gt;init_method&lt;/code&gt; 를 ) 키 - 값 저장을위한 3 개 선택이 있습니다. &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; 는&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6db33b80979f7c8f4114d1e3c176aabac7ef279b" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">피제수와 제수는 정수와 부동 소수점 숫자를 모두 포함 할 수 있습니다. 나머지는 피제수 &lt;code&gt;input&lt;/code&gt; 과 같은 부호를 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="77ad1382d4823bfe1261c0964c88050a7ad3d117" translate="yes" xml:space="preserve">
          <source>The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">피제수와 제수는 정수와 부동 소수점 숫자를 모두 포함 할 수 있습니다. 나머지는 제수 &lt;code&gt;other&lt;/code&gt; 와 같은 부호를 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="3c8c32198d8c41dc6ec53302edb97fbbfa15ce0d" translate="yes" xml:space="preserve">
          <source>The division by</source>
          <target state="translated">구분</target>
        </trans-unit>
        <trans-unit id="d4d19d774aea3434987d4b4cece2b5f99cec9991" translate="yes" xml:space="preserve">
          <source>The dlpack shares the tensors memory. Note that each dlpack can only be consumed once.</source>
          <target state="translated">dlpack은 텐서 메모리를 공유합니다. 각 dlpack은 한 번만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a37e21cd1a669630ae636a9ca39104d114477d0e" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic cosine is &lt;code&gt;[1, inf)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for &lt;code&gt;+ INF&lt;/code&gt; for which the output is mapped to &lt;code&gt;+ INF&lt;/code&gt;.</source>
          <target state="translated">역 쌍곡선 코사인의 영역은 &lt;code&gt;[1, inf)&lt;/code&gt; 범위를 벗어난 값 은 출력이 &lt;code&gt;+ INF&lt;/code&gt; 매핑되는 &lt;code&gt;+ INF&lt;/code&gt; 를 제외하고 &lt;code&gt;NaN&lt;/code&gt; 에 매핑됩니다 .</target>
        </trans-unit>
        <trans-unit id="b9eae4d9c836ec727b1fd15ef73922995c7e82e1" translate="yes" xml:space="preserve">
          <source>The domain of the inverse hyperbolic tangent is &lt;code&gt;(-1, 1)&lt;/code&gt; and values outside this range will be mapped to &lt;code&gt;NaN&lt;/code&gt;, except for the values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; for which the output is mapped to &lt;code&gt;+/-INF&lt;/code&gt; respectively.</source>
          <target state="translated">역 쌍곡 탄젠트의 영역은 &lt;code&gt;(-1, 1)&lt;/code&gt; &lt;code&gt;NaN&lt;/code&gt; 범위를 벗어난 값 은 출력이 각각 &lt;code&gt;+/-INF&lt;/code&gt; 에 매핑되는 값 &lt;code&gt;1&lt;/code&gt; 과 &lt;code&gt;-1&lt;/code&gt; 을 제외하고 NaN 에 매핑됩니다 .</target>
        </trans-unit>
        <trans-unit id="e24f7a4d115e62a42f56d551f1451d37a5e25266" translate="yes" xml:space="preserve">
          <source>The dynamic control flow is captured correctly. We can verify in backends with different loop range.</source>
          <target state="translated">동적 제어 흐름이 올바르게 캡처됩니다. 루프 범위가 다른 백엔드에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a418864112d90d162f2d2d68264a78b5661b0c49" translate="yes" xml:space="preserve">
          <source>The eigenvalues are returned in ascending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.</source>
          <target state="translated">고유 값은 오름차순으로 반환됩니다. 경우 &lt;code&gt;input&lt;/code&gt; 매트릭스의 배치는, 그 배치의 각 행렬의 고유 값은 오름차순으로 반환된다.</target>
        </trans-unit>
        <trans-unit id="70c72529d7b55dc5ab2b127843a9a42c7179e611" translate="yes" xml:space="preserve">
          <source>The elements are sorted into equal width bins between &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;. If &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; are both zero, the minimum and maximum values of the data are used.</source>
          <target state="translated">요소는 &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; 과 &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; 사이의 동일한 너비 빈으로 정렬됩니다 . 경우 &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; 모두 제로인 데이터의 최대 값과 최소값이 사용된다.</target>
        </trans-unit>
        <trans-unit id="ef67a3a1e1ceed752fed1303fe693fb7bb4731f9" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;Backend.UNDEFINED&lt;/code&gt; is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</source>
          <target state="translated">&lt;code&gt;Backend.UNDEFINED&lt;/code&gt; 항목 이 있지만 일부 필드의 초기 값으로 만 사용됩니다. 사용자는 직접 사용하거나 존재한다고 가정해서는 안됩니다.</target>
        </trans-unit>
        <trans-unit id="c0ed2b956d8f2913276ea868cc2cc1c195e28811" translate="yes" xml:space="preserve">
          <source>The example script above produces the graph:</source>
          <target state="translated">위의 예제 스크립트는 그래프를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="8850a69d0e77cfdf6128ac9dcc486c371b38e541" translate="yes" xml:space="preserve">
          <source>The export fails because PyTorch does not support exporting &lt;code&gt;elu&lt;/code&gt; operator. We find &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; in &lt;code&gt;VariableType.h&lt;/code&gt;. This means &lt;code&gt;elu&lt;/code&gt; is an ATen operator. We check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;, and confirm that &lt;code&gt;Elu&lt;/code&gt; is standardized in ONNX. We add the following lines to &lt;code&gt;symbolic_opset9.py&lt;/code&gt;:</source>
          <target state="translated">PyTorch는 &lt;code&gt;elu&lt;/code&gt; 연산자 내보내기를 지원하지 않기 때문에 내보내기가 실패합니다 . 우리는 찾을 &lt;code&gt;virtual Tensor elu(const Tensor &amp;amp; input, Scalar alpha, bool inplace) const override;&lt;/code&gt; 에 &lt;code&gt;VariableType.h&lt;/code&gt; . 이것은 &lt;code&gt;elu&lt;/code&gt; 가 ATen 연산자 임을 의미 합니다. 우리는 확인 &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX 연산자 목록을&lt;/a&gt; 하고 있음을 확인 &lt;code&gt;Elu&lt;/code&gt; ONNX에서 표준화됩니다. &lt;code&gt;symbolic_opset9.py&lt;/code&gt; 에 다음 줄을 추가합니다 .</target>
        </trans-unit>
        <trans-unit id="f9a2121d113f583a300c4eeb7ced28dd018af26f" translate="yes" xml:space="preserve">
          <source>The fact that gradients need to be computed for a Tensor do not mean that the &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; attribute will be populated, see &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt;&lt;code&gt;is_leaf&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">Tensor에 대해 그라디언트를 계산해야한다는 사실이 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt; 속성이 채워지는 것을 의미하지는 않습니다 . 자세한 내용 은 &lt;a href=&quot;autograd#torch.Tensor.is_leaf&quot;&gt; &lt;code&gt;is_leaf&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="221c8b91c0e460724cf31fb2005c0c9e54071204" translate="yes" xml:space="preserve">
          <source>The first call to add for a given &lt;code&gt;key&lt;/code&gt; creates a counter associated with &lt;code&gt;key&lt;/code&gt; in the store, initialized to &lt;code&gt;amount&lt;/code&gt;. Subsequent calls to add with the same &lt;code&gt;key&lt;/code&gt; increment the counter by the specified &lt;code&gt;amount&lt;/code&gt;. Calling &lt;code&gt;add()&lt;/code&gt; with a key that has already been set in the store by &lt;code&gt;set()&lt;/code&gt; will result in an exception.</source>
          <target state="translated">주어진 &lt;code&gt;key&lt;/code&gt; 에 대해 추가하는 첫 번째 호출 은 &lt;code&gt;amount&lt;/code&gt; 로 초기화 된 저장소의 &lt;code&gt;key&lt;/code&gt; 와 관련된 카운터를 만듭니다 . 동일한 &lt;code&gt;key&lt;/code&gt; 추가하기위한 후속 호출 은 카운터를 지정된 &lt;code&gt;amount&lt;/code&gt; 증가시킵니다 . &lt;code&gt;set()&lt;/code&gt; 의해 이미 저장소에 설정된 키로 &lt;code&gt;add()&lt;/code&gt; 를 호출 하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d04a7a0c088c359fab554cd1a6049882b564ff2e" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph.</source>
          <target state="translated">첫 번째 매개 변수는 항상 내 보낸 ONNX 그래프입니다.</target>
        </trans-unit>
        <trans-unit id="26a3da676366966fcc7fbfc997f2f64e6ee31c9d" translate="yes" xml:space="preserve">
          <source>The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in &lt;code&gt;VariableType.h&lt;/code&gt;, because dispatch is done with keyword arguments.</source>
          <target state="translated">첫 번째 매개 변수는 항상 내 보낸 ONNX 그래프입니다. 디스패치는 키워드 인수로 수행되므로 매개 변수 이름은 &lt;code&gt;VariableType.h&lt;/code&gt; 의 이름과 정확히 일치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="5964988cdbc48443556dc471facbf25fa24c6e63" translate="yes" xml:space="preserve">
          <source>The following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a &lt;code&gt;Tensor&lt;/code&gt; as an argument or a return value, the destination worker will try to create a &lt;code&gt;Tensor&lt;/code&gt; with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.</source>
          <target state="translated">다음 API를 사용하면 사용자가 원격으로 함수를 실행하고 원격 데이터 개체에 대한 참조 (RRef)를 만들 수 있습니다. 이러한 API에서 &lt;code&gt;Tensor&lt;/code&gt; 를 인수 또는 반환 값으로 전달할 때 대상 작업자는 동일한 메타 (예 : 모양, 보폭 등) 로 &lt;code&gt;Tensor&lt;/code&gt; 를 만들려고합니다 . 소스 및 대상 작업자의 장치 목록이 일치하지 않으면 충돌 할 수 있으므로 의도적으로 CUDA 텐서 전송을 허용하지 않습니다. 이러한 경우 애플리케이션은 항상 명시 적으로 입력 텐서를 호출자의 CPU로 이동하고 필요한 경우이를 호출 수신자의 원하는 장치로 이동할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0331e44de903db51dab5949d62e369ce763fc069" translate="yes" xml:space="preserve">
          <source>The following Python Expressions are supported.</source>
          <target state="translated">다음 Python 표현식이 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="f123fb71a763a700ae8133c3cf978d6e15341a63" translate="yes" xml:space="preserve">
          <source>The following factory functions support named tensors:</source>
          <target state="translated">다음 팩토리 함수는 명명 된 텐서를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="690193766b4768be386db05b742730b7dbdf34e9" translate="yes" xml:space="preserve">
          <source>The following methods are unique to &lt;a href=&quot;#torch.BoolTensor&quot;&gt;&lt;code&gt;torch.BoolTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">다음 메서드는 &lt;a href=&quot;#torch.BoolTensor&quot;&gt; &lt;code&gt;torch.BoolTensor&lt;/code&gt; 에&lt;/a&gt; 고유합니다 .</target>
        </trans-unit>
        <trans-unit id="2fcc9561d17b3f2c0c3db2c65c4b45e1fd39309f" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will act deterministically when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">다음과 같은 일반적으로 비 결정적 작업은 &lt;code&gt;d=True&lt;/code&gt; 일 때 결정적으로 작동합니다 .</target>
        </trans-unit>
        <trans-unit id="6c856d87fb36615ab5d3e65d2705a558ef7fc7ab" translate="yes" xml:space="preserve">
          <source>The following normally-nondeterministic operations will throw a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;d=True&lt;/code&gt;:</source>
          <target state="translated">일반적으로 비 결정적 다음 작업은 &lt;code&gt;d=True&lt;/code&gt; 일 때 &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt; &lt;code&gt;RuntimeError&lt;/code&gt; 를&lt;/a&gt; 발생 시킵니다 .</target>
        </trans-unit>
        <trans-unit id="10cc3c798c883edec11b18c9b813ac5a197463fe" translate="yes" xml:space="preserve">
          <source>The following operators are supported:</source>
          <target state="translated">다음 연산자가 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="116feb31ceb0c75370eb04add5159eb16382e56b" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in PyTorch 1.8. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt;&lt;code&gt;torch.fft.fftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt; 함수 는 더 이상 사용되지 않으며 PyTorch 1.8에서 제거됩니다. 대신 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; 를 가져 오고 &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../fft#torch.fft.fftn&quot;&gt; &lt;code&gt;torch.fft.fftn()&lt;/code&gt; &lt;/a&gt; 호출 하여 새로운 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; 모듈 함수를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="18bbed5d4e21fbcafc8624ec507383dc653a9737" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.ifft&quot;&gt;&lt;code&gt;torch.ifft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt;&lt;code&gt;torch.fft.ifftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.ifft&quot;&gt; &lt;code&gt;torch.ifft()&lt;/code&gt; &lt;/a&gt; 함수 는 더 이상 사용되지 않으며 향후 PyTorch 릴리스에서 제거됩니다. 대신 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; 를 가져 오고 &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../fft#torch.fft.ifftn&quot;&gt; &lt;code&gt;torch.fft.ifftn()&lt;/code&gt; &lt;/a&gt; 호출 하여 새로운 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; 모듈 함수를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="b1a40cf129fc842a7fe39cfe2023111384b05a6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;torch.irfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt;&lt;code&gt;torch.fft.irfft()&lt;/code&gt;&lt;/a&gt; for one-sided input, or &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt;&lt;code&gt;torch.fft.ifft()&lt;/code&gt;&lt;/a&gt; for two-sided input.</source>
          <target state="translated">&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;torch.irfft()&lt;/code&gt; &lt;/a&gt; 함수 는 더 이상 사용되지 않으며 향후 PyTorch 릴리스에서 제거됩니다. 새로운 사용 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft의&lt;/a&gt; 대신 가져, 모듈 기능 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft을&lt;/a&gt; 하고 호출 &lt;a href=&quot;../fft#torch.fft.irfft&quot;&gt; &lt;code&gt;torch.fft.irfft()&lt;/code&gt; &lt;/a&gt; 일방의 입력이나 대 &lt;a href=&quot;../fft#torch.fft.ifft&quot;&gt; &lt;code&gt;torch.fft.ifft()&lt;/code&gt; &lt;/a&gt; 양면 입력.</target>
        </trans-unit>
        <trans-unit id="a0db51d208a07883c44d7be8c9786454460ee522" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;#torch.rfft&quot;&gt;&lt;code&gt;torch.rfft()&lt;/code&gt;&lt;/a&gt; is deprecated and will be removed in a future PyTorch release. Use the new &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; module functions, instead, by importing &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft&lt;/a&gt; and calling &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt;&lt;code&gt;torch.fft.rfft()&lt;/code&gt;&lt;/a&gt; for one-sided output, or &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt;&lt;code&gt;torch.fft.fft()&lt;/code&gt;&lt;/a&gt; for two-sided output.</source>
          <target state="translated">&lt;a href=&quot;#torch.rfft&quot;&gt; &lt;code&gt;torch.rfft()&lt;/code&gt; &lt;/a&gt; 함수 는 더 이상 사용되지 않으며 향후 PyTorch 릴리스에서 제거됩니다. 새로운 사용 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft의&lt;/a&gt; 대신 가져, 모듈 기능 &lt;a href=&quot;../fft#torch-fft-module&quot;&gt;torch.fft을&lt;/a&gt; 하고 호출 &lt;a href=&quot;../fft#torch.fft.rfft&quot;&gt; &lt;code&gt;torch.fft.rfft()&lt;/code&gt; &lt;/a&gt; 한면 출력 또는 대 &lt;a href=&quot;../fft#torch.fft.fft&quot;&gt; &lt;code&gt;torch.fft.fft()&lt;/code&gt; &lt;/a&gt; 양면 출력.</target>
        </trans-unit>
        <trans-unit id="78c07c2cfdea987b1b930691f54fd99adb7a7932" translate="yes" xml:space="preserve">
          <source>The function is defined as:</source>
          <target state="translated">함수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="81e6543a378b1e27df259854cebe693b6ae023b6" translate="yes" xml:space="preserve">
          <source>The gated linear unit. Computes:</source>
          <target state="translated">게이트 선형 장치. 계산 :</target>
        </trans-unit>
        <trans-unit id="c30476379351f558b5c5fb5c388947e81c5eddf9" translate="yes" xml:space="preserve">
          <source>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying &lt;code&gt;gradient&lt;/code&gt;. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">그래프는 체인 규칙을 사용하여 구분됩니다. 텐서가 스칼라가 아니고 (즉, 데이터에 두 개 이상의 요소가 있음) 그래디언트가 필요한 경우 함수는 추가로 &lt;code&gt;gradient&lt;/code&gt; 를 지정해야 합니다 . 미분 함수 wrt &lt;code&gt;self&lt;/code&gt; 의 기울기를 포함하는 일치하는 유형 및 위치의 텐서 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="63a016154b7848c498c0189a691b12aee6391726" translate="yes" xml:space="preserve">
          <source>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">후크는 인수를 수정해서는 안되지만 선택적으로 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt; 대신 사용될 새 그라디언트를 반환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d3b8c649dc7d5a1913425cba9308cbaac351ab8a" translate="yes" xml:space="preserve">
          <source>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</source>
          <target state="translated">후크는 Tensor에 대한 그라디언트가 계산 될 때마다 호출됩니다. 후크에는 다음 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="91b8909b6b0301d53bc827787fcd07621d744139" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">후크는 &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt; 가 출력을 계산 한 후 매번 호출됩니다 . 다음과 같은 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="ea6266d19428a156d329cc252aacfffb34135bcf" translate="yes" xml:space="preserve">
          <source>The hook will be called every time after &lt;code&gt;forward()&lt;/code&gt; has computed an output. It should have the following signature:</source>
          <target state="translated">후크는 &lt;code&gt;forward()&lt;/code&gt; 가 출력을 계산 한 후 매번 호출됩니다 . 다음과 같은 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="3027e825f33c77d15aef5c4a000dacdcd543639b" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is invoked. It should have the following signature:</source>
          <target state="translated">후크는 &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt; 가 호출 되기 전에 매번 호출됩니다. 다음과 같은 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="658103272c0bdd847853bfee8c1acd84bd1873a4" translate="yes" xml:space="preserve">
          <source>The hook will be called every time before &lt;code&gt;forward()&lt;/code&gt; is invoked. It should have the following signature:</source>
          <target state="translated">후크는 &lt;code&gt;forward()&lt;/code&gt; 가 호출 되기 전에 매번 호출됩니다. 다음과 같은 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="b7317d93b9ecea6baa18606aa1761b42a13618f5" translate="yes" xml:space="preserve">
          <source>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</source>
          <target state="translated">후크는 모듈 입력에 대한 기울기가 계산 될 때마다 호출됩니다. 후크에는 다음 서명이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="47b9323acd4c956c6840fecee45ba64f27f04f5f" translate="yes" xml:space="preserve">
          <source>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute &amp;ndash; that is, contain a small number of assigned labels.</source>
          <target state="translated">아이디어는 자주 액세스하는 클러스터 (가장 빈번한 레이블을 포함하는 첫 번째 클러스터와 같이)도 계산 비용이 저렴해야한다는 것입니다. 즉, 할당 된 레이블의 수가 적어야합니다.</target>
        </trans-unit>
        <trans-unit id="fd1e3da6ac802ac499dbc929d3c01e7d8d1e592e" translate="yes" xml:space="preserve">
          <source>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</source>
          <target state="translated">구현은 Halko et al, 2009의 Algorithm 5.1을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="b39161781b1b7b35ca70ad2826501e304e14740d" translate="yes" xml:space="preserve">
          <source>The implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.</source>
          <target state="translated">구현은 다음을 기반으로합니다. Bader, P .; Blanes, S .; Casas, F. 최적화 된 테일러 다항식 근사를 사용하여 행렬 지수 계산. 수학 2019, 7, 1174.</target>
        </trans-unit>
        <trans-unit id="d9d89b5b4a75085b7eb81dcbff4669f9ff584606" translate="yes" xml:space="preserve">
          <source>The implementation of SVD on CPU uses the LAPACK routine &lt;code&gt;?gesdd&lt;/code&gt; (a divide-and-conquer algorithm) instead of &lt;code&gt;?gesvd&lt;/code&gt; for speed. Analogously, the SVD on GPU uses the MAGMA routine &lt;code&gt;gesdd&lt;/code&gt; as well.</source>
          <target state="translated">CPU에서 SVD를 구현할 &lt;code&gt;?gesvd&lt;/code&gt; 속도를 위해 ? gesvd 대신 LAPACK 루틴 &lt;code&gt;?gesdd&lt;/code&gt; (분할 및 정복 알고리즘)를 사용 합니다. 마찬가지로 GPU의 SVD는 MAGMA 루틴 &lt;code&gt;gesdd&lt;/code&gt; 도 사용합니다.</target>
        </trans-unit>
        <trans-unit id="c05db592ff41dfc6119d9765d1f98df7bbeac593" translate="yes" xml:space="preserve">
          <source>The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.</source>
          <target state="translated">객체 감지, 인스턴스 분할 및 키포인트 감지를위한 모델 구현은 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="2ee40d087e274de07c646cdfb7f973b9dab8f9cf" translate="yes" xml:space="preserve">
          <source>The inferred dtype for python floats in &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">파이썬에 대한 유추 된 dtype은 &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; 떠 있습니다.</target>
        </trans-unit>
        <trans-unit id="6c9a1b1a8c22e228f94e607d9ba23a7a9895221c" translate="yes" xml:space="preserve">
          <source>The input &lt;code&gt;window_length&lt;/code&gt; is a positive integer controlling the returned window size. &lt;code&gt;periodic&lt;/code&gt; flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;. Therefore, if &lt;code&gt;periodic&lt;/code&gt; is true, the</source>
          <target state="translated">입력 &lt;code&gt;window_length&lt;/code&gt; 는 반환 된 창 크기를 제어하는 ​​양의 정수입니다. &lt;code&gt;periodic&lt;/code&gt; 플래그는 반환 된 창이 대칭 창에서 마지막 중복 값을 잘라 내고 &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt; 와 같은 함수를 사용하여 주기적 창으로 사용할 준비가되었는지 여부를 결정합니다 . 따라서 &lt;code&gt;periodic&lt;/code&gt; 이 참이면</target>
        </trans-unit>
        <trans-unit id="dffb8d48e1eff38cec4958715ef4e4bea17673f9" translate="yes" xml:space="preserve">
          <source>The input channels are separated into &lt;code&gt;num_groups&lt;/code&gt; groups, each containing &lt;code&gt;num_channels / num_groups&lt;/code&gt; channels. The mean and standard-deviation are calculated separately over the each group.</source>
          <target state="translated">입력 채널은 &lt;code&gt;num_groups&lt;/code&gt; 그룹 으로 구분되며 각 그룹에는 &lt;code&gt;num_channels / num_groups&lt;/code&gt; 채널이 포함됩니다. 평균 및 표준 편차는 각 그룹에 대해 별도로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="24d9bce9a9af270abcd63358a34e29c31cf80c6f" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</source>
          <target state="translated">입력에는 모듈에 제공된 위치 인수 만 포함됩니다. 키워드 인수는 후크에 전달되지 않고 전달에만 &lt;code&gt;forward&lt;/code&gt; 됩니다. 후크는 입력을 수정할 수 있습니다. 사용자는 후크에서 튜플 또는 단일 수정 된 값을 반환 할 수 있습니다. 단일 값이 반환되면 값을 튜플에 래핑합니다 (해당 값이 이미 튜플이 아닌 경우).</target>
        </trans-unit>
        <trans-unit id="501f21d86b3222c931438d1d37f49ef014fe1070" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">입력에는 모듈에 제공된 위치 인수 만 포함됩니다. 키워드 인수는 후크에 전달되지 않고 전달에만 &lt;code&gt;forward&lt;/code&gt; 됩니다. 후크는 출력을 수정할 수 있습니다. 입력 inplace를 수정할 수 있지만 &lt;a href=&quot;#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt; 가 호출 된 후에 호출되기 때문에 forward에는 영향을주지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="d84cfbddddc9e1c2d0a053623e51f4c9b23ed343" translate="yes" xml:space="preserve">
          <source>The input contains only the positional arguments given to the module. Keyword arguments won&amp;rsquo;t be passed to the hooks and only to the &lt;code&gt;forward&lt;/code&gt;. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after &lt;code&gt;forward()&lt;/code&gt; is called.</source>
          <target state="translated">입력에는 모듈에 제공된 위치 인수 만 포함됩니다. 키워드 인수는 후크에 전달되지 않고 전달에만 &lt;code&gt;forward&lt;/code&gt; 됩니다. 후크는 출력을 수정할 수 있습니다. 입력 inplace를 수정할 수 있지만 &lt;code&gt;forward()&lt;/code&gt; 가 호출 된 후에 호출되기 때문에 forward에는 영향을주지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="e0bbaa069be68586598b46c411d0f2665ffc0e9d" translate="yes" xml:space="preserve">
          <source>The input data is assumed to be of the form &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt;. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</source>
          <target state="translated">입력 데이터는 &lt;code&gt;minibatch x channels x [optional depth] x [optional height] x width&lt;/code&gt; 합니다. 따라서 공간 입력의 경우 4D Tensor를 예상하고 체적 입력의 경우 5D Tensor를 예상합니다.</target>
        </trans-unit>
        <trans-unit id="855506544cad1eeac05a21efea97a3853e2dceb9" translate="yes" xml:space="preserve">
          <source>The input dimensions are interpreted in the form: &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt;.</source>
          <target state="translated">입력 치수는 &lt;code&gt;mini-batch x channels x [optional depth] x [optional height] x width&lt;/code&gt; 형식으로 해석됩니다 .</target>
        </trans-unit>
        <trans-unit id="ff7b890633934634eb44c66753eb478acaced44e" translate="yes" xml:space="preserve">
          <source>The input is assumed to be a low-rank matrix.</source>
          <target state="translated">입력은 낮은 순위 행렬로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="aa72bb59a173177382b9660fee81f62bda47bdae" translate="yes" xml:space="preserve">
          <source>The input quantization parameters are propagated to the output.</source>
          <target state="translated">입력 양자화 매개 변수는 출력으로 전파됩니다.</target>
        </trans-unit>
        <trans-unit id="7c8e7facaf470e54c3381c46dad3dd11548e58de" translate="yes" xml:space="preserve">
          <source>The input quantization parameters propagate to the output.</source>
          <target state="translated">입력 양자화 매개 변수는 출력으로 전파됩니다.</target>
        </trans-unit>
        <trans-unit id="9c93ac513b2302a4f83a43405b9a47bcb2309e67" translate="yes" xml:space="preserve">
          <source>The input to the model is expected to be a list of tensors, each of shape &lt;code&gt;[C, H, W]&lt;/code&gt;, one for each image, and should be in &lt;code&gt;0-1&lt;/code&gt; range. Different images can have different sizes.</source>
          <target state="translated">모델에 대한 입력 은 각 이미지에 대해 하나씩 &lt;code&gt;[C, H, W]&lt;/code&gt; 모양의 텐서 목록이 될 것으로 예상 되며 &lt;code&gt;0-1&lt;/code&gt; 범위에 있어야합니다 . 이미지마다 크기가 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="33129e152019831a4d6451e1dcb0878f56c1bef2" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.&lt;/code&gt; prefix for some operations. See example usage below.</source>
          <target state="translated">이 클래스의 인스턴스는 &lt;code&gt;torch.&lt;/code&gt; 대신 사용할 수 있습니다 . 일부 작업의 접두사. 아래 사용 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="351e07f8a31af7d82fc6045df4c793836de62f62" translate="yes" xml:space="preserve">
          <source>The instance of this class can be used instead of the &lt;code&gt;torch.ops.quantized&lt;/code&gt; prefix. See example usage below.</source>
          <target state="translated">이 클래스의 인스턴스는 &lt;code&gt;torch.ops.quantized&lt;/code&gt; 접두사 대신 사용할 수 있습니다 . 아래 사용 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="6c3dcf3c5f75afd5957f352f9917b83b90088988" translate="yes" xml:space="preserve">
          <source>The interface for specifying operator definitions is a Prototype feature; adventurous users should note that the APIs will probably change in a future interface.</source>
          <target state="translated">운영자 정의를 지정하기위한 인터페이스는 프로토 타입 기능입니다. 모험심이 강한 사용자는 API가 향후 인터페이스에서 변경 될 수 있다는 점에 유의해야합니다.</target>
        </trans-unit>
        <trans-unit id="3bb29d8ff742c3f78ac5a2da01f22239940307d9" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수의 역은 &lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="3ff786d8f2651e07ce1136605dc8809d77fc8a6a" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수의 역은 &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="bd26976415ff02dd321f8aabd58b78a55df77549" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수의 역은 &lt;a href=&quot;torch.irfft#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="51e2eb96b40c3aad396917c7dd98de9a6585beea" translate="yes" xml:space="preserve">
          <source>The inverse of this function is &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수의 역은 &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="4b9e7887dd437999f146f02a0f60028f96d84bf3" translate="yes" xml:space="preserve">
          <source>The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express neural net models. In particular, TorchScript supports:</source>
          <target state="translated">TorchScript와 전체 Python 언어의 가장 큰 차이점은 TorchScript는 신경망 모델을 표현하는 데 필요한 작은 유형 집합 만 지원한다는 것입니다. 특히 TorchScript는 다음을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="3b1d6d882f09df5877abb8ef200d99a4552edbcc" translate="yes" xml:space="preserve">
          <source>The largest representable number.</source>
          <target state="translated">가장 큰 숫자입니다.</target>
        </trans-unit>
        <trans-unit id="63b23adb215af0fe9eaa51ef9153973e0c53546f" translate="yes" xml:space="preserve">
          <source>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</source>
          <target state="translated">마지막 용어는 Stirling 공식으로 생략하거나 근사화 할 수 있습니다. 근사는 1보다 큰 목표 값에 사용됩니다. 목표 값이 1보다 작거나 같은 경우 손실에 0이 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="08f3358598c31282b2114da38bcad5d74e704334" translate="yes" xml:space="preserve">
          <source>The locations are used in the order of</source>
          <target state="translated">위치는 순서대로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9ebd3a640891ee47a9a38fbd56c89724e57e2af2" translate="yes" xml:space="preserve">
          <source>The loss can be described as:</source>
          <target state="translated">손실은 다음과 같이 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f21bd0e4e809cda5ba13223c1dfb3494e56f4f3e" translate="yes" xml:space="preserve">
          <source>The loss function for</source>
          <target state="translated">손실 함수</target>
        </trans-unit>
        <trans-unit id="86081d5fc32148129e542f3dc9192fb2857da205" translate="yes" xml:space="preserve">
          <source>The loss function for each pair of samples in the mini-batch is:</source>
          <target state="translated">미니 배치의 각 샘플 쌍에 대한 손실 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bb7d036d43546cb725a6bf9bf29c5cc2076ac418" translate="yes" xml:space="preserve">
          <source>The loss function for each sample in the mini-batch is:</source>
          <target state="translated">미니 배치의 각 샘플에 대한 손실 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="48f8488012ce60a4d13e1aebd7fa0ba10061eaf3" translate="yes" xml:space="preserve">
          <source>The loss function for each sample is:</source>
          <target state="translated">각 샘플의 손실 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c43bc97db5e0527c42aa84dc0f06c5fb6d69ddbc" translate="yes" xml:space="preserve">
          <source>The loss function then becomes:</source>
          <target state="translated">손실 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="dbd64b8b1e6f8a768dc1cc623e35d46784438d27" translate="yes" xml:space="preserve">
          <source>The losses are averaged across observations for each minibatch. If the &lt;code&gt;weight&lt;/code&gt; argument is specified then this is a weighted average:</source>
          <target state="translated">손실은 각 미니 배치에 대한 관측에서 평균화됩니다. 경우 &lt;code&gt;weight&lt;/code&gt; 인수가 지정되어이 가중 평균 :</target>
        </trans-unit>
        <trans-unit id="187e00ce183312dd132d6826870b8e0983ae12a8" translate="yes" xml:space="preserve">
          <source>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</source>
          <target state="translated">행렬의 아래쪽 삼각형 부분은 대각선 위와 아래의 요소로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="cba1d04dc7071338e4468867933f6f030f4f19fd" translate="yes" xml:space="preserve">
          <source>The machine with rank 0 will be used to set up all connections.</source>
          <target state="translated">랭크 0의 머신은 모든 연결을 설정하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f6b0320b620362b6deb5bb70025cb38b632f6a57" translate="yes" xml:space="preserve">
          <source>The main trick for &lt;code&gt;hard&lt;/code&gt; is to do &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hard&lt;/code&gt; 의 주요 트릭 은 &lt;code&gt;y_hard - y_soft.detach() + y_soft&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="61caa3335965d2f3a59a97a54b15a39417eaac38" translate="yes" xml:space="preserve">
          <source>The max-pooling operation is applied in</source>
          <target state="translated">최대 풀링 작업이 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="d65dca627767b86d15d82ece9fa3008acf19101f" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups.</source>
          <target state="translated">평균 및 표준 편차는 동일한 프로세스 그룹의 모든 미니 배치에 대해 차원별로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="c3242427612d3b3c3e0cb9845d6bf7db1596d5b9" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension over the mini-batches and</source>
          <target state="translated">평균 및 표준 편차는 미니 배치에 대해 차원별로 계산되며</target>
        </trans-unit>
        <trans-unit id="5661df9ad4405c387ea95d88a69a9fe1650b1552" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.</source>
          <target state="translated">평균 및 표준 편차는 미니 배치의 각 객체에 대해 차원별로 개별적으로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="29fab6215a0696acbd3e84d8cb442b324881c861" translate="yes" xml:space="preserve">
          <source>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by &lt;code&gt;normalized_shape&lt;/code&gt;.</source>
          <target state="translated">평균 및 표준 편차는 &lt;code&gt;normalized_shape&lt;/code&gt; 로 지정된 모양이어야하는 마지막 특정 숫자 차원에 대해 별도로 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="8a930aa85422123ed7ce795a3af91990ae6ee8f7" translate="yes" xml:space="preserve">
          <source>The mean operation still operates over all the elements, and divides by</source>
          <target state="translated">평균 연산은 여전히 ​​모든 요소에 대해 작동하며 다음으로 나눕니다.</target>
        </trans-unit>
        <trans-unit id="619af861d215de1b87a6226cb50c28f6e623bb30" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements in the dimension &lt;code&gt;dim&lt;/code&gt;. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">중앙값은 차원 &lt;code&gt;dim&lt;/code&gt; 에 짝수의 요소가있는 &lt;code&gt;input&lt;/code&gt; 텐서에 대해 고유하지 않습니다 . 이 경우 두 중앙값 중 더 낮은 값이 반환됩니다. 모두 중앙값의 평균을 계산하는 &lt;code&gt;input&lt;/code&gt; , 사용 &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;q=0.5&lt;/code&gt; 대신.</target>
        </trans-unit>
        <trans-unit id="5d7cf56bfc5cedceb3ca00313d8c3d414937caef" translate="yes" xml:space="preserve">
          <source>The median is not unique for &lt;code&gt;input&lt;/code&gt; tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians in &lt;code&gt;input&lt;/code&gt;, use &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;q=0.5&lt;/code&gt; instead.</source>
          <target state="translated">중앙값은 요소 수가 짝수 인 &lt;code&gt;input&lt;/code&gt; 텐서에 대해 고유하지 않습니다 . 이 경우 두 중앙값 중 더 낮은 값이 반환됩니다. 모두 중앙값의 평균을 계산하는 &lt;code&gt;input&lt;/code&gt; , 사용 &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;q=0.5&lt;/code&gt; 대신.</target>
        </trans-unit>
        <trans-unit id="3568ef228089a375953fda3fd0c97aca0ace05a3" translate="yes" xml:space="preserve">
          <source>The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.</source>
          <target state="translated">모델은 모든 블록에서 두 배 더 큰 병목 채널 수를 제외하고는 ResNet과 동일합니다. 외부 1x1 컨볼 루션의 채널 수는 동일합니다. 예를 들어 ResNet-50의 마지막 블록에는 2048-512-2048 채널이 있고 Wide ResNet-50-2에는 2048-1024-2048이 있습니다.</target>
        </trans-unit>
        <trans-unit id="f15c86c9dfb63d5c112adb09bde9cb61a9fe3f2e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.</source>
          <target state="translated">모델은 학습 중에 RPN 및 R-CNN에 대한 분류 및 회귀 손실과 키포인트 손실을 포함 하는 &lt;code&gt;Dict[Tensor]&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="065007e637ab5dd9f6a9a41f5c632d9aa1aa3f5e" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss.</source>
          <target state="translated">모델은 훈련 중에 RPN과 R-CNN 모두에 대한 분류 및 회귀 손실과 마스크 손실을 포함 하는 &lt;code&gt;Dict[Tensor]&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="a0c3a4815182bd38a59e8d48bd1acda3676df3d3" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses for both the RPN and the R-CNN.</source>
          <target state="translated">모델은 훈련 중에 RPN과 R-CNN 모두에 대한 분류 및 회귀 손실을 포함 하는 &lt;code&gt;Dict[Tensor]&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="25fbe42f5bebb67d721733ebaa470925353914ab" translate="yes" xml:space="preserve">
          <source>The model returns a &lt;code&gt;Dict[Tensor]&lt;/code&gt; during training, containing the classification and regression losses.</source>
          <target state="translated">모델은 훈련 중에 분류 및 회귀 손실을 포함 하는 &lt;code&gt;Dict[Tensor]&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="32c7b059b3336b886e2737e75a7a90bf14890a11" translate="yes" xml:space="preserve">
          <source>The models expect a list of &lt;code&gt;Tensor[C, H, W]&lt;/code&gt;, in the range &lt;code&gt;0-1&lt;/code&gt;. The models internally resize the images so that they have a minimum size of &lt;code&gt;800&lt;/code&gt;. This option can be changed by passing the option &lt;code&gt;min_size&lt;/code&gt; to the constructor of the models.</source>
          <target state="translated">모델 은 &lt;code&gt;0-1&lt;/code&gt; 범위 의 &lt;code&gt;Tensor[C, H, W]&lt;/code&gt; 목록을 예상합니다 . 모델은 내부적으로 최소 크기가 &lt;code&gt;800&lt;/code&gt; 이되도록 이미지 크기를 조정합니다 . 이 옵션은 &lt;code&gt;min_size&lt;/code&gt; 옵션 을 모델 생성자 에 전달하여 변경할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b8cd112b9466880b251d727f36c9b5a8099f6121" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for detection:</source>
          <target state="translated">모델 하위 패키지에는 감지를위한 다음 모델 아키텍처에 대한 정의가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="2fd7064c0a6dbdb00e5c05b4bcf7bb996635bfb3" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for image classification:</source>
          <target state="translated">모델 서브 패키지는 이미지 분류를위한 다음 모델 아키텍처에 대한 정의를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="9a82a240d4b1d0867d7d72a1a3f555d48973b4c7" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions for the following model architectures for semantic segmentation:</source>
          <target state="translated">모델 서브 패키지는 의미 론적 분할을위한 다음 모델 아키텍처에 대한 정의를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="18bc003911e615e5dba9603961f512f27acb297a" translate="yes" xml:space="preserve">
          <source>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</source>
          <target state="translated">모델 서브 패키지는 이미지 분류, 픽셀 단위 시맨틱 분할, 객체 감지, 인스턴스 분할, 사람 키포인트 감지 및 비디오 분류를 포함하여 다양한 작업을 처리하기위한 모델의 정의를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="3cf46d13a21b259a13e6ce62948cca48ddbc78f6" translate="yes" xml:space="preserve">
          <source>The modes available for resizing are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only), &lt;code&gt;area&lt;/code&gt;</source>
          <target state="translated">크기 조정에 사용할 수있는 모드는 &lt;code&gt;nearest&lt;/code&gt; , &lt;code&gt;linear&lt;/code&gt; (3D 전용), &lt;code&gt;bilinear&lt;/code&gt; , 쌍 &lt;code&gt;bicubic&lt;/code&gt; (4D 전용), &lt;code&gt;trilinear&lt;/code&gt; (5D 전용), &lt;code&gt;area&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28d8cfd32d3249e2763e181ffc749144e0ea24a2" translate="yes" xml:space="preserve">
          <source>The modes available for upsampling are: &lt;code&gt;nearest&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt; (3D-only), &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt; (4D-only), &lt;code&gt;trilinear&lt;/code&gt; (5D-only)</source>
          <target state="translated">업 샘플링에 사용할 수있는 모드는 &lt;code&gt;nearest&lt;/code&gt; , &lt;code&gt;linear&lt;/code&gt; (3D 전용), &lt;code&gt;bilinear&lt;/code&gt; , 쌍 &lt;code&gt;bicubic&lt;/code&gt; (4D 전용), &lt;code&gt;trilinear&lt;/code&gt; (5D 전용)입니다.</target>
        </trans-unit>
        <trans-unit id="18b2caf95e73a1c791145d0b79ff330ee08dd6df" translate="yes" xml:space="preserve">
          <source>The module can be accessed as an attribute using the given name.</source>
          <target state="translated">모듈은 주어진 이름을 사용하여 속성으로 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="00f10b867e105cafb015ef514a5b43106e8a7867" translate="yes" xml:space="preserve">
          <source>The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">모듈의 &lt;code&gt;forward&lt;/code&gt; 는 기본적으로 컴파일됩니다. 에서 호출 방법 &lt;code&gt;forward&lt;/code&gt; 느리게 그들이에 사용되는 순서로 컴파일 &lt;code&gt;forward&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f00cd6c1142679bd0fe1c3ebd4ca78600af96a2c" translate="yes" xml:space="preserve">
          <source>The name of the worker.</source>
          <target state="translated">근로자의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="fba68059efcba7726021d64196e7ad3e2e4fcb96" translate="yes" xml:space="preserve">
          <source>The named tensor API is a prototype feature and subject to change.</source>
          <target state="translated">명명 된 텐서 API는 프로토 타입 기능이며 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ed535f6a7c3020c739db9c59caec32bca959d97b" translate="yes" xml:space="preserve">
          <source>The named tensor API is experimental and subject to change.</source>
          <target state="translated">명명 된 텐서 API는 실험적이며 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36555b023721ec5738f0c183a135318d3c9a4466" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss.</source>
          <target state="translated">음의 로그 우도 손실입니다.</target>
        </trans-unit>
        <trans-unit id="80880349d7cc0a615947c8d7ecadbb60eab216c2" translate="yes" xml:space="preserve">
          <source>The negative log likelihood loss. It is useful to train a classification problem with &lt;code&gt;C&lt;/code&gt; classes.</source>
          <target state="translated">음의 로그 우도 손실입니다. &lt;code&gt;C&lt;/code&gt; 클래스 로 분류 문제를 훈련하는 것이 유용합니다 .</target>
        </trans-unit>
        <trans-unit id="07e33a3c723549700a6c484c42061c0b83f6423c" translate="yes" xml:space="preserve">
          <source>The new backend derives from &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; and registers the backend name and the instantiating interface through &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; when imported.</source>
          <target state="translated">새 백엔드는 &lt;code&gt;c10d.ProcessGroup&lt;/code&gt; 에서 파생되며 가져올 때 &lt;code&gt;torch.distributed.Backend.register_backend()&lt;/code&gt; 통해 백엔드 이름과 인스턴스화 인터페이스를 등록합니다 .</target>
        </trans-unit>
        <trans-unit id="5ef80d57536dd4f889b111944ebd026c0916a56b" translate="yes" xml:space="preserve">
          <source>The new usage looks like this:</source>
          <target state="translated">새로운 사용법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bfa45b6345caf33366ff2d44c58d6e6325263da6" translate="yes" xml:space="preserve">
          <source>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</source>
          <target state="translated">노름은 단일 벡터로 연결된 것처럼 모든 그라디언트에서 함께 계산됩니다. 그라디언트는 제자리에서 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="760d37d3c11fd75fa728a8e00ba020ac1e72a10c" translate="yes" xml:space="preserve">
          <source>The normalization parameters are different from the image classification ones, and correspond to the mean and std from Kinetics-400.</source>
          <target state="translated">정규화 매개 변수는 이미지 분류 매개 변수와 다르며 Kinetics-400의 평균 및 표준에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="6b4b699c449424f5a0ee9c963faafecf0fd0d061" translate="yes" xml:space="preserve">
          <source>The number of bins (size 1) is one larger than the largest value in &lt;code&gt;input&lt;/code&gt; unless &lt;code&gt;input&lt;/code&gt; is empty, in which case the result is a tensor of size 0. If &lt;code&gt;minlength&lt;/code&gt; is specified, the number of bins is at least &lt;code&gt;minlength&lt;/code&gt; and if &lt;code&gt;input&lt;/code&gt; is empty, then the result is tensor of size &lt;code&gt;minlength&lt;/code&gt; filled with zeros. If &lt;code&gt;n&lt;/code&gt; is the value at position &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;out[n] += weights[i]&lt;/code&gt; if &lt;code&gt;weights&lt;/code&gt; is specified else &lt;code&gt;out[n] += 1&lt;/code&gt;.</source>
          <target state="translated">빈들의 수는 (크기 1) 최대 값보다 하나 큰 &lt;code&gt;input&lt;/code&gt; 않는 &lt;code&gt;input&lt;/code&gt; 하면 결과가 크기 0의 텐서 인 경우, 비어 &lt;code&gt;minlength&lt;/code&gt; 지정되어, 빈들의 개수는 최소이다 &lt;code&gt;minlength&lt;/code&gt; IF 및 &lt;code&gt;input&lt;/code&gt; 비어 있으면 결과는 0으로 채워진 &lt;code&gt;minlength&lt;/code&gt; 크기의 텐서입니다 . 경우 &lt;code&gt;n&lt;/code&gt; 위치의 값 &lt;code&gt;i&lt;/code&gt; , &lt;code&gt;out[n] += weights[i]&lt;/code&gt; 경우 &lt;code&gt;weights&lt;/code&gt; 지정한 다른 &lt;code&gt;out[n] += 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="792cc1a58f0a3a46d19118877b6d13c493cb98ec" translate="yes" xml:space="preserve">
          <source>The number of bits occupied by the type.</source>
          <target state="translated">유형이 차지하는 비트 수입니다.</target>
        </trans-unit>
        <trans-unit id="280e9a9c6268d27aec18f9552a0a0e023b3a5d6c" translate="yes" xml:space="preserve">
          <source>The number of keys present in the store.</source>
          <target state="translated">상점에있는 키의 수입니다.</target>
        </trans-unit>
        <trans-unit id="cb1db44aaa6eaf0c2b9f7e2d3594ec089fe36151" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests.</source>
          <target state="translated">&lt;code&gt;TensorPipeAgent&lt;/code&gt; 가 요청을 실행 하는 데 사용하는 스레드 풀의 스레드 수입니다 .</target>
        </trans-unit>
        <trans-unit id="20a68b367a08bf827b8cb29b9a7a5775403402ae" translate="yes" xml:space="preserve">
          <source>The number of threads in the thread-pool used by ProcessGroupAgent.</source>
          <target state="translated">ProcessGroupAgent에서 사용하는 스레드 풀의 스레드 수입니다.</target>
        </trans-unit>
        <trans-unit id="25f85e0ff2381c99b81c5c68406ed2a516dd1318" translate="yes" xml:space="preserve">
          <source>The numerical properties of a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; can be accessed through either the &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; or the &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">(A)의 수치 속성 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 는&lt;/a&gt; 하나를 통해 액세스 할 수 &lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.torch.iinfo&quot;&gt; &lt;code&gt;torch.iinfo&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="d8e759a2b7197a6d6914a25b2e4bccfa94711180" translate="yes" xml:space="preserve">
          <source>The operation applied is:</source>
          <target state="translated">적용되는 작업은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="0b61e3d8bf7f65d3c8a372be2ffe4aa181113744" translate="yes" xml:space="preserve">
          <source>The operation is defined as:</source>
          <target state="translated">작업은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="416029866582b442a7cfd38c944f5ccc41de6ea3" translate="yes" xml:space="preserve">
          <source>The operator set above is sufficient to export the following models:</source>
          <target state="translated">위의 연산자 세트는 다음 모델을 내보내는 데 충분합니다.</target>
        </trans-unit>
        <trans-unit id="97549a235d386a02f6c874f871b2a3fc368e849a" translate="yes" xml:space="preserve">
          <source>The order of norm. inf refers to &lt;code&gt;float('inf')&lt;/code&gt;, numpy&amp;rsquo;s &lt;code&gt;inf&lt;/code&gt; object, or any equivalent object. The following norms can be calculated:</source>
          <target state="translated">규범의 순서. inf는 &lt;code&gt;float('inf')&lt;/code&gt; , numpy의 &lt;code&gt;inf&lt;/code&gt; 객체 또는 이와 동등한 객체를 나타냅니다. 다음 표준을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="587438e00b276c5c07c33c3b833d1847c428edb1" translate="yes" xml:space="preserve">
          <source>The original &lt;code&gt;module&lt;/code&gt; with the converted &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers. If the original &lt;code&gt;module&lt;/code&gt; is a &lt;code&gt;BatchNorm*D&lt;/code&gt; layer, a new &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layer object will be returned instead.</source>
          <target state="translated">변환 된 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 레이어 가있는 원래 &lt;code&gt;module&lt;/code&gt; . 원래 &lt;code&gt;module&lt;/code&gt; 이 &lt;code&gt;BatchNorm*D&lt;/code&gt; 레이어 인 경우 새 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 레이어 개체가 대신 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="1c43e721e428fefa0921064ef9d974f9d9c3710a" translate="yes" xml:space="preserve">
          <source>The original module with the spectral norm hook</source>
          <target state="translated">스펙트럼 표준 후크가있는 원래 모듈</target>
        </trans-unit>
        <trans-unit id="1c8ee61f168965198d32cd38694cc41b385bdcc7" translate="yes" xml:space="preserve">
          <source>The original module with the weight norm hook</source>
          <target state="translated">무게 표준 후크가있는 원래 모듈</target>
        </trans-unit>
        <trans-unit id="a7c71aa7150538e239f1ed9763931d65c1aab1d1" translate="yes" xml:space="preserve">
          <source>The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">출력은 모든 입력 크기에 대해 D x H x W 크기입니다. 출력 기능의 수는 입력 평면의 수와 같습니다.</target>
        </trans-unit>
        <trans-unit id="8d13bc6b68893874e15ef23c3e8bf09f29542e58" translate="yes" xml:space="preserve">
          <source>The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">출력은 모든 입력 크기에 대해 H x W 크기입니다. 출력 기능의 수는 입력 평면의 수와 같습니다.</target>
        </trans-unit>
        <trans-unit id="dd1aa83704336b7c60e0ac15205527d8852065d5" translate="yes" xml:space="preserve">
          <source>The output of the &lt;code&gt;model&lt;/code&gt; callable when called with the given &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt;.</source>
          <target state="translated">주어진 &lt;code&gt;*args&lt;/code&gt; 및 &lt;code&gt;**kwargs&lt;/code&gt; 로 호출 될 때 호출 가능한 &lt;code&gt;model&lt;/code&gt; 의 출력입니다 .</target>
        </trans-unit>
        <trans-unit id="2624c3f93c8a5ba7f8db76475b9b2160784ad8be" translate="yes" xml:space="preserve">
          <source>The output size is H, for any input size. The number of output features is equal to the number of input planes.</source>
          <target state="translated">출력 크기는 모든 입력 크기에 대해 H입니다. 출력 기능의 수는 입력 평면의 수와 같습니다.</target>
        </trans-unit>
        <trans-unit id="aaed500780776d71aa7aa3beac0d23a2ad0bee52" translate="yes" xml:space="preserve">
          <source>The output tuple size must match the outputs of &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">출력 튜플 크기는 &lt;code&gt;forward&lt;/code&gt; 의 출력과 일치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="69fc4cd3bd84d50752da22ec0fa2da75a028b6c8" translate="yes" xml:space="preserve">
          <source>The package needs to be initialized using the &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; function before calling any other methods. This blocks until all processes have joined.</source>
          <target state="translated">다른 메소드를 호출하기 전에 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 패키지를 초기화해야 합니다. 모든 프로세스가 결합 될 때까지 차단됩니다.</target>
        </trans-unit>
        <trans-unit id="a24470d7f3dafd97610b1a97cfc4e625080f5d77" translate="yes" xml:space="preserve">
          <source>The padding size by which to pad some dimensions of &lt;code&gt;input&lt;/code&gt; are described starting from the last dimension and moving forward.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 일부 차원을 채우는 패딩 크기 는 마지막 차원에서 시작하여 앞으로 나아갑니다.</target>
        </trans-unit>
        <trans-unit id="b3d894892755f58e334d42f2a6bb03ad3c9c1029" translate="yes" xml:space="preserve">
          <source>The parallelized &lt;code&gt;module&lt;/code&gt; must have its parameters and buffers on &lt;code&gt;device_ids[0]&lt;/code&gt; before running this &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="translated">병렬화 된 &lt;code&gt;module&lt;/code&gt; 은 이 &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt; 모듈을 실행하기 전에 &lt;code&gt;device_ids[0]&lt;/code&gt; 에 매개 변수와 버퍼가 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="611179cf7c23e5fa87e86dc65fc6c2d7edcfcdbf" translate="yes" xml:space="preserve">
          <source>The parameter can be accessed as an attribute using given name.</source>
          <target state="translated">매개 변수는 주어진 이름을 사용하여 속성으로 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3673b833c2a753da5c92e7b2bd7867575cba0695" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; can either be:</source>
          <target state="translated">&lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; 매개 변수 는 다음 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bb60e522db72c109467c75d771e518ca64062c0c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can each be an &lt;code&gt;int&lt;/code&gt; or a one-element tuple.</source>
          <target state="translated">&lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; 매개 변수 는 각각 &lt;code&gt;int&lt;/code&gt; 또는 단일 요소 튜플 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="df694fbe631671fb53535c127328a8f9b5ccfdfe" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; can either be:</source>
          <target state="translated">&lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; 매개 변수 는 다음 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0d35449e64c1f14944b10572266aa02084f0787f" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt; can either be:</source>
          <target state="translated">&lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; , &lt;code&gt;dilation&lt;/code&gt; 매개 변수 는 다음 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="16e542ef2d9e6480cf0213002d643e04ad39afd9" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;output_padding&lt;/code&gt; can either be:</source>
          <target state="translated">&lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; , &lt;code&gt;output_padding&lt;/code&gt; 매개 변수 는 다음 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9df0d4116a648b1a73d8fd1ad856ffa0c5b125d9" translate="yes" xml:space="preserve">
          <source>The parameters represented by a single vector</source>
          <target state="translated">단일 벡터로 표시되는 매개 변수</target>
        </trans-unit>
        <trans-unit id="16c253cfff05379e83f73283d75913a9d3bc5eb0" translate="yes" xml:space="preserve">
          <source>The pivots returned by the function are 1-indexed. If &lt;code&gt;pivot&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the returned pivots is a tensor filled with zeros of the appropriate size.</source>
          <target state="translated">함수가 반환하는 피벗은 1 인덱스입니다. 경우 &lt;code&gt;pivot&lt;/code&gt; 입니다 &lt;code&gt;False&lt;/code&gt; , 다음 반환 된 피벗는 적절한 크기의 0으로 채워 텐서이다.</target>
        </trans-unit>
        <trans-unit id="4600b83baa50f6eee75af80c6cdb275baf5a4820" translate="yes" xml:space="preserve">
          <source>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</source>
          <target state="translated">감지, 인스턴스 분할 및 키포인트 감지를 위해 사전 학습 된 모델은 torchvision의 분류 모델로 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="bacc13981715b20b015df75a9953a0a7c1d88d4c" translate="yes" xml:space="preserve">
          <source>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt;. The classes that the pre-trained model outputs are the following, in order:</source>
          <target state="translated">사전 훈련 된 모델은 파스칼 VOC 데이터 세트에있는 20 개 범주에 대해 COCO train2017의 하위 집합에서 훈련되었습니다. &lt;code&gt;references/segmentation/coco_utils.py&lt;/code&gt; 에서 하위 집합이 어떻게 선택되었는지에 대한 자세한 정보를 볼 수 있습니다 . 사전 훈련 된 모델이 출력하는 클래스는 순서대로 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="5f5bcce5c35434b7def7a2c2abecc7855a7ff6ae" translate="yes" xml:space="preserve">
          <source>The process for obtaining the values of &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;std&lt;/code&gt; is roughly equivalent to:</source>
          <target state="translated">&lt;code&gt;mean&lt;/code&gt; 및 &lt;code&gt;std&lt;/code&gt; 값을 얻는 과정 은 대략 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cb1f1b1b62d82d262fa3ee527ff2235c6fe33cf0" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt;. Therefore, derivatives are not always existent, and exist for a constant rank only &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt;. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">의사 역은 행렬 &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/0117004&quot;&gt;[1]&lt;/a&gt; 의 요소에서 반드시 연속 함수일 필요는 없습니다 . 따라서 도함수는 항상 존재하는 것은 아니며 일정한 순위에 대해서만 존재합니다 &lt;a href=&quot;https://www.jstor.org/stable/2156365&quot;&gt;[2]&lt;/a&gt; . 그러나이 방법은 SVD 결과를 사용한 구현으로 인해 역 전파가 가능하며 불안정 할 수 있습니다. 내부적으로 SVD를 사용하기 때문에 Double-backward도 불안정합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="df88bf331dfcd873fd61e2a961e2a4565dca71ab" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;input&lt;/code&gt; of dimensions</source>
          <target state="translated">차원 &lt;code&gt;input&lt;/code&gt; 의 의사 역</target>
        </trans-unit>
        <trans-unit id="801cd7b0289a0cab3f1164523d810bc285cdd10c" translate="yes" xml:space="preserve">
          <source>The published models should be at least in a branch/tag. It can&amp;rsquo;t be a random commit.</source>
          <target state="translated">게시 된 모델은 최소한 분기 / 태그에 있어야합니다. 무작위 커밋이 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="346d775a68239ce50fc9c648f8e7ac9c30934f97" translate="yes" xml:space="preserve">
          <source>The range of the linear region</source>
          <target state="translated">선형 영역의 범위</target>
        </trans-unit>
        <trans-unit id="a67bc4760995b69a50ba7bb512777bd75a29786d" translate="yes" xml:space="preserve">
          <source>The rank of the process group -1, if not part of the group</source>
          <target state="translated">그룹의 일부가 아닌 경우 프로세스 그룹의 순위 -1</target>
        </trans-unit>
        <trans-unit id="1640b5e94a24bbde9b57f55563952bd38f53f1c7" translate="yes" xml:space="preserve">
          <source>The real-to-complex Fourier transform results follow conjugate symmetry:</source>
          <target state="translated">실수-복소 푸리에 변환 결과는 켤레 대칭을 따릅니다.</target>
        </trans-unit>
        <trans-unit id="f3326dbe4c730be437e49c0a1205659d772531b4" translate="yes" xml:space="preserve">
          <source>The regular implementation uses the (more common in PyTorch) &lt;code&gt;torch.long&lt;/code&gt; dtype.</source>
          <target state="translated">일반 구현은 (PyTorch에서 더 일반적) &lt;code&gt;torch.long&lt;/code&gt; dtype을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="f42ad6925b2f350139e05854ee56333bfbf81bfa" translate="yes" xml:space="preserve">
          <source>The relation of &lt;code&gt;(U, S, V)&lt;/code&gt; to PCA is as follows:</source>
          <target state="translated">&lt;code&gt;(U, S, V)&lt;/code&gt; 와 PCA 의 관계는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="0dee4722eb608774d930f39be3f46f18bcb0b430" translate="yes" xml:space="preserve">
          <source>The result will never require gradient.</source>
          <target state="translated">결과에는 그라디언트가 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6b0575a2f172061283c10c05f0db13db17166b95" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;alexnet.onnx&lt;/code&gt; is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument &lt;code&gt;verbose=True&lt;/code&gt; causes the exporter to print out a human-readable representation of the network:</source>
          <target state="translated">결과 &lt;code&gt;alexnet.onnx&lt;/code&gt; 는 내 보낸 모델 (이 경우 AlexNet)의 네트워크 구조와 매개 변수를 모두 포함하는 바이너리 protobuf 파일입니다. 키워드 인수 &lt;code&gt;verbose=True&lt;/code&gt; 는 내보내기가 사람이 읽을 수있는 네트워크 표현을 인쇄하도록합니다.</target>
        </trans-unit>
        <trans-unit id="3c02f08f3d1024ae629cdcc31caf60626bc7a365" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;out&lt;/code&gt; tensor shares it&amp;rsquo;s underlying storage with the &lt;code&gt;input&lt;/code&gt; tensor, so changing the content of one would change the content of the other.</source>
          <target state="translated">결과로 &lt;code&gt;out&lt;/code&gt; 텐서는 &lt;code&gt;input&lt;/code&gt; 텐서 와 기본 저장소를 공유 하므로 하나의 내용을 변경하면 다른 내용이 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="6758477f8561982cc2515cdbf897a47792390375" translate="yes" xml:space="preserve">
          <source>The resulting recording of &lt;code&gt;nn.Module.forward&lt;/code&gt; or &lt;code&gt;nn.Module&lt;/code&gt; produces &lt;code&gt;ScriptModule&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;nn.Module.forward&lt;/code&gt; 또는 &lt;code&gt;nn.Module&lt;/code&gt; 의 결과 기록 은 &lt;code&gt;ScriptModule&lt;/code&gt; 을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="094611e344ec0108ea463a9054c21e208ca824dd" translate="yes" xml:space="preserve">
          <source>The resulting recording of a standalone function produces &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="translated">독립 실행 형 함수의 결과 기록은 &lt;code&gt;ScriptFunction&lt;/code&gt; 을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="14a7d916a182796f96dea780975d504da8d70e49" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object can come from &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; constructor. The example below shows directly using the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt;&lt;code&gt;then()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">반환 된 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 객체는 &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 생성자 에서 올 수 있습니다 . 아래 예제는 &lt;a href=&quot;futures#torch.futures.Future.then&quot;&gt; &lt;code&gt;then()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; 를&lt;/a&gt; 직접 사용하는 것을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="35586b86275e1afc45c84727656b22c56c0dfc4c" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;out&lt;/code&gt; tensor only has values 0 or 1 and is of the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">반환 &lt;code&gt;out&lt;/code&gt; 텐서는 0 또는 1 값만 가지며 &lt;code&gt;input&lt;/code&gt; 과 같은 모양 입니다.</target>
        </trans-unit>
        <trans-unit id="0c1ca22658d19c73ead093d5a482f3f0748946cd" translate="yes" xml:space="preserve">
          <source>The returned Tensor&amp;rsquo;s data will be of size &lt;code&gt;T x B x *&lt;/code&gt;, where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence and &lt;code&gt;B&lt;/code&gt; is the batch size. If &lt;code&gt;batch_first&lt;/code&gt; is True, the data will be transposed into &lt;code&gt;B x T x *&lt;/code&gt; format.</source>
          <target state="translated">반환 된 Tensor 데이터의 크기는 &lt;code&gt;T x B x *&lt;/code&gt; 이며, 여기서 &lt;code&gt;T&lt;/code&gt; 는 가장 긴 시퀀스의 길이이고 &lt;code&gt;B&lt;/code&gt; 는 배치 크기입니다. 경우 &lt;code&gt;batch_first&lt;/code&gt; 가 True 인으로, 데이터는 전치된다 &lt;code&gt;B x T x *&lt;/code&gt; 형식.</target>
        </trans-unit>
        <trans-unit id="a86d643d348528641013d71d94cfc9c2436b5f5a" translate="yes" xml:space="preserve">
          <source>The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride &lt;code&gt;(1, m)&lt;/code&gt; instead of &lt;code&gt;(m, 1)&lt;/code&gt;.</source>
          <target state="translated">반환 된 행렬은 입력 행렬의 보폭에 관계없이 항상 전치됩니다. 즉, &lt;code&gt;(m, 1)&lt;/code&gt; 대신 stride &lt;code&gt;(1, m)&lt;/code&gt; )가 있습니다.</target>
        </trans-unit>
        <trans-unit id="e874ec4d7da2420c99c0303e4f7a958324a670ae" translate="yes" xml:space="preserve">
          <source>The returned tensor and &lt;code&gt;ndarray&lt;/code&gt; share the same memory. Modifications to the tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa. The returned tensor is not resizable.</source>
          <target state="translated">반환 된 텐서와 &lt;code&gt;ndarray&lt;/code&gt; 는 동일한 메모리를 공유합니다. 텐서에 대한 수정은 &lt;code&gt;ndarray&lt;/code&gt; 에 반영 되며 그 반대의 경우도 마찬가지입니다. 반환 된 텐서는 크기를 조정할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="93c4d85268a064ece907fbc8a5535a6f396d9a4e" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor</source>
          <target state="translated">반환 된 텐서는 원래 텐서와 동일한 저장소를 사용 하지 &lt;strong&gt;않습니다.&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="52f5aa4074a60eef379e1658572f1bb6509ef7da" translate="yes" xml:space="preserve">
          <source>The returned tensor does &lt;strong&gt;not&lt;/strong&gt; use the same storage as the original tensor. If &lt;code&gt;out&lt;/code&gt; has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</source>
          <target state="translated">반환 된 텐서는 원래 텐서와 동일한 저장소를 사용 하지 &lt;strong&gt;않습니다&lt;/strong&gt; . &lt;code&gt;out&lt;/code&gt; 의 모양이 예상과 다른 경우 자동으로 올바른 모양으로 변경하여 필요한 경우 기본 저장소를 다시 할당합니다.</target>
        </trans-unit>
        <trans-unit id="f46100c8ee7a830bab0c89e9873612006ce16b25" translate="yes" xml:space="preserve">
          <source>The returned tensor has the same number of dimensions as the original tensor (&lt;code&gt;input&lt;/code&gt;). The &lt;code&gt;dim&lt;/code&gt;th dimension has the same size as the length of &lt;code&gt;index&lt;/code&gt;; other dimensions have the same size as in the original tensor.</source>
          <target state="translated">반환 된 텐서는 원래 텐서 ( &lt;code&gt;input&lt;/code&gt; ) 와 동일한 수의 차원을 갖습니다 . &lt;code&gt;dim&lt;/code&gt; 번째 차원의 길이와 동일한 크기를 갖는 &lt;code&gt;index&lt;/code&gt; ; 다른 차원은 원래 텐서와 같은 크기입니다.</target>
        </trans-unit>
        <trans-unit id="c72a07e12ac6ea766a45eac39941a4c32ee2a049" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions</source>
          <target state="translated">반환 된 텐서는 동일한 데이터를 공유하고 요소 수가 동일해야하지만 크기가 다를 수 있습니다. 텐서가 표시 되려면 새보기 크기가 원래 크기 및 보폭과 호환되어야합니다. 즉, 각각의 새보기 차원이 원래 차원의 부분 공간이거나 원래 차원에 걸쳐 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="e400aae54e1364ef7f0ae0e4394533a68ef83da8" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the same underlying data with this tensor.</source>
          <target state="translated">반환 된 텐서는이 텐서와 동일한 기본 데이터를 공유합니다.</target>
        </trans-unit>
        <trans-unit id="172826726b64a692f9e7f61dbdcbf5a0b8b36f39" translate="yes" xml:space="preserve">
          <source>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</source>
          <target state="translated">반환 된 텐서는 입력 텐서와 저장소를 공유하므로 하나의 내용을 변경하면 다른 내용도 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="28c770021d54b212e8c62be1e09939a0369312ae" translate="yes" xml:space="preserve">
          <source>The rows of &lt;code&gt;input&lt;/code&gt; do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 행은 1로 합할 필요는 없지만 (이 경우 값을 가중치로 사용) 음이 아니고 유한해야하며 합이 0이 아니어야합니다.</target>
        </trans-unit>
        <trans-unit id="1eff8f651f1c1e81084414db9ad8e86ff7d386f5" translate="yes" xml:space="preserve">
          <source>The second argument can be a number or a tensor whose shape is &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the first argument.</source>
          <target state="translated">두 번째 인수는 첫 번째 인수로 모양을 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅&lt;/a&gt; 할 수있는 숫자 또는 텐서 일 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="35bad0572344e9ca340bcfea7b7e08eb1fb48094" translate="yes" xml:space="preserve">
          <source>The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">텐서의 모양은 가변 인수 &lt;code&gt;size&lt;/code&gt; 에 의해 정의됩니다 .</target>
        </trans-unit>
        <trans-unit id="44ad26e34a9ec24516da93c641da630606e76045" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt; don&amp;rsquo;t need to match, but the total number of elements in each tensor need to be the same.</source>
          <target state="translated">&lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt; 과 &lt;a href=&quot;torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; &lt;/a&gt; 의 모양은 일치 할 필요는 없지만 각 텐서의 총 요소 수는 동일해야합니다.</target>
        </trans-unit>
        <trans-unit id="7ab58315d98cc62bd1c43b5269219135f914a361" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; , &lt;code&gt;tensor1&lt;/code&gt; 및 &lt;code&gt;tensor2&lt;/code&gt; 의 모양은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅 &lt;/a&gt;가능 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="472d04a7b4ef19856846b867c2431ae86dfeee09" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; 형태는 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;방송 가능&lt;/a&gt; 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="4bb8d54bf4e66fa1d51318971fbd84c5dd471f64" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;tensor1&lt;/code&gt;, and &lt;code&gt;tensor2&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; , &lt;code&gt;tensor1&lt;/code&gt; 및 &lt;code&gt;tensor2&lt;/code&gt; 의 모양은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅 &lt;/a&gt;가능 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="3c2b5afb57ab65d651f6e8673ddc0bb16bc251d9" translate="yes" xml:space="preserve">
          <source>The shapes of &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;. If &lt;code&gt;weight&lt;/code&gt; is a tensor, then the shapes of &lt;code&gt;weight&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, and &lt;code&gt;end&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;start&lt;/code&gt; 과 &lt;code&gt;end&lt;/code&gt; 의 모양은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;방송 가능&lt;/a&gt; 해야합니다 . 경우 &lt;code&gt;weight&lt;/code&gt; 텐서의 다음 모양입니다 &lt;code&gt;weight&lt;/code&gt; , &lt;code&gt;start&lt;/code&gt; 과 &lt;code&gt;end&lt;/code&gt; 있어야합니다 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="30ae8ec04282dfa89f820e2638cc76a4c5308051" translate="yes" xml:space="preserve">
          <source>The shapes of the &lt;code&gt;mask&lt;/code&gt; tensor and the &lt;code&gt;input&lt;/code&gt; tensor don&amp;rsquo;t need to match, but they must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;mask&lt;/code&gt; 텐서와 &lt;code&gt;input&lt;/code&gt; 텐서 의 모양은 일치 할 필요는 없지만 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅 가능&lt;/a&gt; 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="80ed96242b937c96d491a6608007e2c80ef6fbfe" translate="yes" xml:space="preserve">
          <source>The singular values are returned in descending order. If &lt;code&gt;input&lt;/code&gt; is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</source>
          <target state="translated">특이 값은 내림차순으로 반환됩니다. 경우 &lt;code&gt;input&lt;/code&gt; 매트릭스의 배치는, 그 배치의 각 행렬의 특이 값을 내림차순으로 반환된다.</target>
        </trans-unit>
        <trans-unit id="86c71484bcb812cb0f2e7260fb264f82086dbacb" translate="yes" xml:space="preserve">
          <source>The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for &lt;code&gt;offset&lt;/code&gt; other than</source>
          <target state="translated">새 행렬의 크기는 마지막 입력 차원 크기의 지정된 대각선이되도록 계산됩니다. 동안 그 주 &lt;code&gt;offset&lt;/code&gt; 이외의</target>
        </trans-unit>
        <trans-unit id="0739eb19e36226e98c11fd96e8291d8a209e3a7e" translate="yes" xml:space="preserve">
          <source>The smallest positive representable number.</source>
          <target state="translated">표현할 수있는 가장 작은 양수입니다.</target>
        </trans-unit>
        <trans-unit id="cf0617c3e175f01c2b40e71995108022f6b64f95" translate="yes" xml:space="preserve">
          <source>The smallest representable number (typically &lt;code&gt;-max&lt;/code&gt;).</source>
          <target state="translated">표현 가능한 가장 작은 수 (일반적으로 &lt;code&gt;-max&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="27d91cd31d6556c601166e6b57bbeb8fc2cd13fe" translate="yes" xml:space="preserve">
          <source>The smallest representable number such that &lt;code&gt;1.0 + eps != 1.0&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;1.0 + eps != 1.0&lt;/code&gt; 과 같이 표현 가능한 가장 작은 숫자 입니다.</target>
        </trans-unit>
        <trans-unit id="ff2ebfdbbf9420dca19c2e2c489305049c84e69e" translate="yes" xml:space="preserve">
          <source>The smallest representable number.</source>
          <target state="translated">표현 가능한 가장 작은 숫자입니다.</target>
        </trans-unit>
        <trans-unit id="6e4db462bf55509b809c59ea2298379ae2dba77f" translate="yes" xml:space="preserve">
          <source>The sources in &lt;code&gt;cuda_sources&lt;/code&gt; are concatenated into a separate &lt;code&gt;.cu&lt;/code&gt; file and prepended with &lt;code&gt;torch/types.h&lt;/code&gt;, &lt;code&gt;cuda.h&lt;/code&gt; and &lt;code&gt;cuda_runtime.h&lt;/code&gt; includes. The &lt;code&gt;.cpp&lt;/code&gt; and &lt;code&gt;.cu&lt;/code&gt; files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in &lt;code&gt;cuda_sources&lt;/code&gt; per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the &lt;code&gt;cpp_sources&lt;/code&gt; (and include its name in &lt;code&gt;functions&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;cuda_sources&lt;/code&gt; 의 소스 는 별도의 &lt;code&gt;.cu&lt;/code&gt; 파일 로 연결되고 앞에 &lt;code&gt;torch/types.h&lt;/code&gt; , &lt;code&gt;cuda.h&lt;/code&gt; 및 &lt;code&gt;cuda_runtime.h&lt;/code&gt; 포함이 추가 됩니다. &lt;code&gt;.cpp&lt;/code&gt; 과 &lt;code&gt;.cu&lt;/code&gt; 파일은 별도로 컴파일하지만, 궁극적으로 하나의 라이브러리에 연결되어 있습니다. &lt;code&gt;cuda_sources&lt;/code&gt; 자체의 함수에 대해서는 바인딩이 생성되지 않습니다 . CUDA 커널에 바인딩하려면이를 호출하는 C ++ 함수를 생성하고 &lt;code&gt;cpp_sources&lt;/code&gt; 중 하나에서이 C ++ 함수를 선언하거나 정의 해야합니다 (그리고 해당 이름을 &lt;code&gt;functions&lt;/code&gt; 에 포함 ).</target>
        </trans-unit>
        <trans-unit id="f06fdc0c2350c66f3a9d82727c760926c8016f15" translate="yes" xml:space="preserve">
          <source>The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the &lt;code&gt;run_fn&lt;/code&gt;. However, the logic has no way to anticipate if the user will move Tensors to a new device within the &lt;code&gt;run_fn&lt;/code&gt; itself. Therefore, if you move Tensors to a new device (&amp;ldquo;new&amp;rdquo; meaning not belonging to the set of [current device + devices of Tensor arguments]) within &lt;code&gt;run_fn&lt;/code&gt;, deterministic output compared to non-checkpointed passes is never guaranteed.</source>
          <target state="translated">보관 한 논리는 저장하고 현재 장치와 모든 CUDA 텐서 인수의 장치에 대한 RNG 상태로 복원 &lt;code&gt;run_fn&lt;/code&gt; 을 . 그러나 논리는 사용자가 &lt;code&gt;run_fn&lt;/code&gt; 자체 내에서 Tensor를 새 장치로 이동할지 여부를 예측할 방법이 없습니다 . 새로운 장치 텐서 움직이면 따라서, 내 ( &quot;새로운&quot;[텐서 인수 현재 장치 + 디바이스]의 집합에 속하지 않는 의미) &lt;code&gt;run_fn&lt;/code&gt; 보장 절대로 비 체크 포인트 패스에 비해 결정적 출력.</target>
        </trans-unit>
        <trans-unit id="b7f3ab8a2df042d6e3025c18a145e6e14f77af25" translate="yes" xml:space="preserve">
          <source>The sum operation still operates over all the elements, and divides by</source>
          <target state="translated">합계 연산은 여전히 ​​모든 요소에 대해 작동하며 다음으로 나눕니다.</target>
        </trans-unit>
        <trans-unit id="b2531e155fe9e77a5cfae31abf7b6f59ffb9fcaa" translate="yes" xml:space="preserve">
          <source>The support of third-party backend is experimental and subject to change.</source>
          <target state="translated">타사 백엔드 지원은 실험적이며 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2873a12c84aba239b7842369e12bb64236d9f288" translate="yes" xml:space="preserve">
          <source>The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.</source>
          <target state="translated">텐서는 dlpack에 표시된 객체와 메모리를 공유합니다. 각 dlpack은 한 번만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dc114e497003f25e6fca05f82fcba607f7d78078" translate="yes" xml:space="preserve">
          <source>The tensors &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">텐서의 &lt;code&gt;condition&lt;/code&gt; , &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;y&lt;/code&gt; 있어야 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="ab7ff2ed427b3e162ddc9c1bc3618ade11c4bc8e" translate="yes" xml:space="preserve">
          <source>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</source>
          <target state="translated">토치 패키지에는 다차원 텐서에 대한 데이터 구조가 포함되어 있으며 이에 대한 수학적 연산이 정의됩니다. 또한 Tensor 및 임의 유형의 효율적인 직렬화를위한 많은 유틸리티와 기타 유용한 유틸리티를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="437652102ffae6443117ed89a3eae4a8f5b41da3" translate="yes" xml:space="preserve">
          <source>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</source>
          <target state="translated">추적 프로그램은 추적 된 계산에서 여러 문제가있는 패턴에 대한 경고를 생성합니다. 예를 들어 Tensor의 슬라이스 (뷰)에 제자리 할당을 포함하는 함수를 추적합니다.</target>
        </trans-unit>
        <trans-unit id="89850de90f112bfc18c8cdc61311b3872ebf2872" translate="yes" xml:space="preserve">
          <source>The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter &lt;code&gt;dynamic_axes&lt;/code&gt; in export api.</source>
          <target state="translated">추적 프로그램은 그래프에 예제 입력 모양을 기록합니다. 모델이 동적 형태의 입력을 받아야하는 경우 내보내기 API에서 &lt;code&gt;dynamic_axes&lt;/code&gt; 매개 변수를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f2328cc72d77d5be4ac31362bf776708e1354f25" translate="yes" xml:space="preserve">
          <source>The unreduced (i.e. with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) loss can be described as:</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; 되지 않은 (즉, 감소 가 &lt;code&gt;'none'&lt;/code&gt; 으로 설정된 ) 손실은 다음과 같이 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="83a2adc9d7ff925b785017e994bf1c2dfff219e3" translate="yes" xml:space="preserve">
          <source>The unreduced loss (i.e., with &lt;code&gt;reduction&lt;/code&gt; set to &lt;code&gt;'none'&lt;/code&gt;) can be described as:</source>
          <target state="translated">감소되지 않은 손실 (즉, &lt;code&gt;reduction&lt;/code&gt; 가 &lt;code&gt;'none'&lt;/code&gt; 로 설정 됨 )은 다음과 같이 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="55b435ee6855f1f093ca66cf72abe8bd2c30eb03" translate="yes" xml:space="preserve">
          <source>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</source>
          <target state="translated">행렬의 위쪽 삼각형 부분은 대각선 위의 요소로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="8ff79d6204f8beff86c1883967aad41f978dae9c" translate="yes" xml:space="preserve">
          <source>The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.</source>
          <target state="translated">이 유틸리티는 노드 당 하나 이상의 프로세스가 생성되는 단일 노드 분산 교육에 사용할 수 있습니다. 이 유틸리티는 CPU 훈련 또는 GPU 훈련에 사용할 수 있습니다. 유틸리티가 GPU 학습에 사용되는 경우 각 분산 프로세스는 단일 GPU에서 작동합니다. 이를 통해 잘 개선 된 단일 노드 훈련 성능을 얻을 수 있습니다. 또한 잘 개선 된 다중 노드 분산 훈련 성능을 위해 각 노드에서 여러 프로세스를 생성하여 다중 노드 분산 훈련에도 사용할 수 있습니다. 이는 직접 GPU를 지원하는 여러 Infiniband 인터페이스가있는 시스템에 특히 유용합니다. 모든 인터페이스를 집계 된 통신 대역폭에 사용할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="a8baea02cc4125823d2a9295d67b336747f5d060" translate="yes" xml:space="preserve">
          <source>The value held by this &lt;code&gt;Future&lt;/code&gt;. If the function (callback or RPC) creating the value has thrown an error, this &lt;code&gt;wait&lt;/code&gt; method will also throw an error.</source>
          <target state="translated">이 &lt;code&gt;Future&lt;/code&gt; 가 보유한 가치 . 값을 생성하는 함수 (콜백 또는 RPC)에서 오류가 발생하면이 &lt;code&gt;wait&lt;/code&gt; 메서드도 오류를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="4c32d99e8bca557bc1750470ea69ae3bfb345050" translate="yes" xml:space="preserve">
          <source>The values of this class are lowercase strings, e.g., &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;. They can be accessed as attributes, e.g., &lt;code&gt;Backend.NCCL&lt;/code&gt;.</source>
          <target state="translated">이 클래스의 값은 소문자 문자열입니다 (예 : &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; ) . 속성 (예 : &lt;code&gt;Backend.NCCL&lt;/code&gt; ) 으로 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2d09c2230d5453858141229e7b0087efe234f737" translate="yes" xml:space="preserve">
          <source>The values of this class can be accessed as attributes, e.g., &lt;code&gt;ReduceOp.SUM&lt;/code&gt;. They are used in specifying strategies for reduction collectives, e.g., &lt;a href=&quot;#torch.distributed.reduce&quot;&gt;&lt;code&gt;reduce()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="translated">이 클래스의 값은 속성 (예 : &lt;code&gt;ReduceOp.SUM&lt;/code&gt; ) 으로 액세스 할 수 있습니다 . 감소 집단에 대한 전략을 지정하는 데 사용됩니다 &lt;a href=&quot;#torch.distributed.reduce&quot;&gt; &lt;code&gt;reduce()&lt;/code&gt; &lt;/a&gt; 예 : reduce () , &lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt; &lt;code&gt;all_reduce_multigpu()&lt;/code&gt; &lt;/a&gt; 등 ) .</target>
        </trans-unit>
        <trans-unit id="539dcafbce59fbba03d94008a7b26e37638c0f1b" translate="yes" xml:space="preserve">
          <source>The world size of the process group -1, if not part of the group</source>
          <target state="translated">그룹의 일부가 아닌 경우 프로세스 그룹 -1의 세계 크기</target>
        </trans-unit>
        <trans-unit id="a5d66dffec3f1eab9a22e5606deb2eaebb37f157" translate="yes" xml:space="preserve">
          <source>Then &lt;code&gt;dynamic axes&lt;/code&gt; can be defined either as:</source>
          <target state="translated">그런 다음 &lt;code&gt;dynamic axes&lt;/code&gt; 다음 중 하나로 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cacf045e7460da89c00e2f44ce242ff34d557d4d" translate="yes" xml:space="preserve">
          <source>Then for any (supported) &lt;code&gt;input&lt;/code&gt; tensor the following equality holds:</source>
          <target state="translated">그런 다음 (지원되는) &lt;code&gt;input&lt;/code&gt; 텐서에 대해 다음 동등성이 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="61c6ce8b4e502ae0f9cf574180a84ce71379a45a" translate="yes" xml:space="preserve">
          <source>Then run the following code in two different processes:</source>
          <target state="translated">그런 다음 두 가지 다른 프로세스에서 다음 코드를 실행합니다.</target>
        </trans-unit>
        <trans-unit id="1ce92152c376045e006c7df7e500a93b25016945" translate="yes" xml:space="preserve">
          <source>Then, you can run:</source>
          <target state="translated">그런 다음 다음을 실행할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9157fc1a5d0f6b3459cf4d13b451acd4e1ae988a" translate="yes" xml:space="preserve">
          <source>There are 2 main ways to initialize a process group:</source>
          <target state="translated">프로세스 그룹을 초기화하는 두 가지 주요 방법이 있습니다.</target>
        </trans-unit>
        <trans-unit id="989aa62e15f82179c46dd7bca86e4177e7b7bf45" translate="yes" xml:space="preserve">
          <source>There are a few main ways to create a tensor, depending on your use case.</source>
          <target state="translated">사용 사례에 따라 텐서를 만드는 몇 가지 주요 방법이 있습니다.</target>
        </trans-unit>
        <trans-unit id="448eaecf17e019efecc19b1280b7ffb9043292f0" translate="yes" xml:space="preserve">
          <source>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</source>
          <target state="translated">Tensor에 정의 된 내부 랜덤 샘플링 함수도 몇 가지 더 있습니다. 해당 문서를 참조하려면 클릭하십시오.</target>
        </trans-unit>
        <trans-unit id="4a50a4f02b41f0e4e7768bdf9e0d8e8e8f9ca103" translate="yes" xml:space="preserve">
          <source>There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:</source>
          <target state="translated">일부 버전의 cuDNN 및 CUDA에서 RNN 함수에 대해 알려진 비결 정성 문제가 있습니다. 다음 환경 변수를 설정하여 결정적 동작을 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="94f93e5cb61707d9a9f9f8c1e8bae87549a0661b" translate="yes" xml:space="preserve">
          <source>There are more examples in &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;symbolic_opset9.py&lt;/a&gt; , &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset10.py&quot;&gt;symbolic_opset10.py&lt;/a&gt; 에 더 많은 예제가 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e7b2c38d0b8818a513a12fc7c7e90b9479638b6e" translate="yes" xml:space="preserve">
          <source>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</source>
          <target state="translated">주어진 Python 함수 / 모듈의 추적이 기본 코드를 나타내지 않는 경우가 몇 가지 있습니다. 이러한 경우에는 다음이 포함될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7b0666c05baae6bc9dd7801107606ece55efb258" translate="yes" xml:space="preserve">
          <source>There are two main usages:</source>
          <target state="translated">두 가지 주요 용도가 있습니다.</target>
        </trans-unit>
        <trans-unit id="7b78c278fc5d3de5d25f299a98858079b45aa6eb" translate="yes" xml:space="preserve">
          <source>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired &lt;code&gt;world_size&lt;/code&gt;. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</source>
          <target state="translated">TCP를 사용하여 초기화하는 방법에는 두 가지가 있습니다. 둘 다 모든 프로세스에서 도달 할 수있는 네트워크 주소와 원하는 &lt;code&gt;world_size&lt;/code&gt; 가 필요 합니다 . 첫 번째 방법은 순위 0 프로세스에 속하는 주소를 지정해야합니다. 이 초기화 방법을 사용하려면 모든 프로세스에 수동으로 지정된 순위가 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="f2b97adc02fc81457ca80fb753c694becd93e274" translate="yes" xml:space="preserve">
          <source>There is a subtlety in using the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;My recurrent network doesn&amp;rsquo;t work with data parallelism&lt;/a&gt; section in FAQ for details.</source>
          <target state="translated">사용에 미묘가 &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; A의 패턴 &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 에 싸여 &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt; . 자세한 내용 은 FAQ의 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;내 반복 네트워크가 데이터 병렬 처리&lt;/a&gt; 에서 작동하지 않음 섹션을 참조하세요.</target>
        </trans-unit>
        <trans-unit id="6bedff7a4f5803a09c69737d429b263e4a4e07c0" translate="yes" xml:space="preserve">
          <source>Therefore, indexing &lt;code&gt;output&lt;/code&gt; at the last dimension (column dimension) gives all values within a certain block.</source>
          <target state="translated">따라서 마지막 차원 (열 차원)의 인덱싱 &lt;code&gt;output&lt;/code&gt; 은 특정 블록 내의 모든 값을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="220cff043cb62e0e4cb5366907ebd80ead3bf366" translate="yes" xml:space="preserve">
          <source>Therefore, to invert an &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;normalized&lt;/code&gt; and &lt;code&gt;onesided&lt;/code&gt; arguments should be set identically for &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, and preferably a &lt;code&gt;signal_sizes&lt;/code&gt; is given to avoid size mismatch. See the example below for a case of size mismatch.</source>
          <target state="translated">따라서, 반전 &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 상기 &lt;code&gt;normalized&lt;/code&gt; 및 &lt;code&gt;onesided&lt;/code&gt; 인수에 대해 동일하게 설정되어야 &lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; , 바람직하게는 &lt;code&gt;signal_sizes&lt;/code&gt; 는 피 크기 불일치로 주어진다. 크기가 일치하지 않는 경우 아래 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="11de0478b61c8d1fd83658744b437989c835c987" translate="yes" xml:space="preserve">
          <source>These are the basic building block for graphs</source>
          <target state="translated">그래프의 기본 구성 요소입니다.</target>
        </trans-unit>
        <trans-unit id="8ca4969bd4e4273b5349ab2f675d16c1de44f187" translate="yes" xml:space="preserve">
          <source>These backends include:</source>
          <target state="translated">이러한 백엔드에는 다음이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="3dd026a9fc19a074410394c15ee56c930e5686d0" translate="yes" xml:space="preserve">
          <source>These types and features from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module are unavailble in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; 모듈 의 이러한 유형 및 기능은 TorchScript에서 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="aca9bd729c973c4d3bc4eb7f3f8d4e953c82edfd" translate="yes" xml:space="preserve">
          <source>These unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.</source>
          <target state="translated">이들은 루프를 풀고 튜플의 각 멤버에 대한 본문을 생성합니다. 본문은 각 구성원에 대해 올바르게 유형 검사해야합니다.</target>
        </trans-unit>
        <trans-unit id="5042b3fea81612a742984a1fb55be5b675720283" translate="yes" xml:space="preserve">
          <source>Third-party backends</source>
          <target state="translated">타사 백엔드</target>
        </trans-unit>
        <trans-unit id="f9ab4d0d07c5bfd671637ba9723d67603253bcae" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;momentum&lt;/code&gt; argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is</source>
          <target state="translated">이 &lt;code&gt;momentum&lt;/code&gt; 인수는 옵티 마이저 클래스 및 기존의 모멘텀 개념에서 사용되는 인수와 다릅니다. 수학적으로 여기에서 통계를 실행하기위한 업데이트 규칙은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4d057f12e63c19fa40f83b9e371a09dbaf3641ca" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;setuptools.build_ext&lt;/code&gt; subclass takes care of passing the minimum required compiler flags (e.g. &lt;code&gt;-std=c++14&lt;/code&gt;) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</source>
          <target state="translated">이 &lt;code&gt;setuptools.build_ext&lt;/code&gt; 서브 클래스는 필요한 최소 컴파일러 플래그 (예 : &lt;code&gt;-std=c++14&lt;/code&gt; )와 혼합 C ++ / CUDA 컴파일 (및 일반적으로 CUDA 파일 지원)을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="48b0bfc66718d9ed75062a9f5d57a687fd7749c9" translate="yes" xml:space="preserve">
          <source>This API is in beta and may change in the near future.</source>
          <target state="translated">이 API는 베타 버전이며 가까운 시일 내에 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="33a32d5fcbf1c45d06b8358ae2591cd5047cff16" translate="yes" xml:space="preserve">
          <source>This allows better BC support for &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt;. In &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;, the version number will be saved as in the attribute &lt;code&gt;_metadata&lt;/code&gt; of the returned state dict, and thus pickled. &lt;code&gt;_metadata&lt;/code&gt; is a dictionary with keys that follow the naming convention of state dict. See &lt;code&gt;_load_from_state_dict&lt;/code&gt; on how to use this information in loading.</source>
          <target state="translated">이를 통해 &lt;a href=&quot;#torch.nn.Module.load_state_dict&quot;&gt; &lt;code&gt;load_state_dict()&lt;/code&gt; &lt;/a&gt; 대한 BC 지원이 향상 됩니다. 에서는 &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; 의 버전 번호 속성 같이 저장 될 &lt;code&gt;_metadata&lt;/code&gt; 절인 따라서 반환 상태 딕셔너리, 그리고. &lt;code&gt;_metadata&lt;/code&gt; 는 상태 dict의 명명 규칙을 따르는 키가있는 사전입니다. 로드 &lt;code&gt;_load_from_state_dict&lt;/code&gt; 정보를 사용하는 방법 은 _load_from_state_dict 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="09b607c5ebd52b97aa3c6cf95a474cc71af1ac47" translate="yes" xml:space="preserve">
          <source>This allows for different samples to have variable amounts of target classes.</source>
          <target state="translated">이를 통해 다양한 샘플이 다양한 양의 대상 클래스를 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dc869bc2254cbb18e88bb0a8c7e186c575d69c2e" translate="yes" xml:space="preserve">
          <source>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</source>
          <target state="translated">이것은 또한 연관된 ​​매개 변수와 버퍼를 다른 오브젝트로 만듭니다. 따라서 모듈이 최적화되는 동안 GPU에 상주 할 경우 옵티 마이저를 구성하기 전에 호출해야합니다.</target>
        </trans-unit>
        <trans-unit id="cd79f292616deed21e20e4449558742671a5f03e" translate="yes" xml:space="preserve">
          <source>This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set &lt;code&gt;use_external_data_format&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; to successfully export such models.</source>
          <target state="translated">이 인수를 사용하면 대형 모델을 ONNX로 내보낼 수 있습니다. 2GB보다 큰 모델은 protobuf 크기 제한으로 인해 하나의 파일로 내보낼 수 없습니다. 이러한 모델을 성공적으로 내보내 려면 사용자가 &lt;code&gt;use_external_data_format&lt;/code&gt; 을 &lt;code&gt;True&lt;/code&gt; 로 설정해야 합니다.</target>
        </trans-unit>
        <trans-unit id="d9aed446ae3aa4dc28d8d65d1c929ff54cb2a74f" translate="yes" xml:space="preserve">
          <source>This attribute is &lt;code&gt;None&lt;/code&gt; by default and becomes a Tensor the first time a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; computes gradients for &lt;code&gt;self&lt;/code&gt;. The attribute will then contain the gradients computed and future calls to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; will accumulate (add) gradients into it.</source>
          <target state="translated">이 속성은 기본적으로 &lt;code&gt;None&lt;/code&gt; 이며 &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; 호출이 &lt;code&gt;self&lt;/code&gt; 에 대한 기울기를 처음으로 계산할 때 Tensor가됩니다 . 그러면 속성은 계산 된 그라디언트를 포함하고 앞으로 &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; 호출은 그라디언트 를 축적 (추가)합니다.</target>
        </trans-unit>
        <trans-unit id="27eaef81d3fd1f9e704b27f64a092d8671cfb020" translate="yes" xml:space="preserve">
          <source>This can be called as</source>
          <target state="translated">이것은 다음과 같이 부를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e5eeeb2d9e42f39a239e76c72a9314e3136e5540" translate="yes" xml:space="preserve">
          <source>This can then be visualized with TensorBoard, which should be installable and runnable with:</source>
          <target state="translated">그런 다음 TensorBoard를 사용하여 시각화 할 수 있으며 다음을 사용하여 설치 및 실행할 수 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="2306bf42c44895d1fc76396938e439c137c2dffb" translate="yes" xml:space="preserve">
          <source>This class can be directly called to parse the string, e.g., &lt;code&gt;Backend(backend_str)&lt;/code&gt; will check if &lt;code&gt;backend_str&lt;/code&gt; is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; returns &lt;code&gt;&quot;gloo&quot;&lt;/code&gt;.</source>
          <target state="translated">이 클래스는 문자열을 구문 분석하기 위해 직접 호출 될 수 있습니다. 예를 들어 &lt;code&gt;Backend(backend_str)&lt;/code&gt; 는 &lt;code&gt;backend_str&lt;/code&gt; 이 유효한지 확인하고 그렇다면 구문 분석 된 소문자 문자열을 반환합니다. 또한 대문자 문자열을 허용합니다. 예를 들어 &lt;code&gt;Backend(&quot;GLOO&quot;)&lt;/code&gt; 는 &lt;code&gt;&quot;gloo&quot;&lt;/code&gt; 를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="5ef03a595a40d9415bbad765363aad2d964d5b16" translate="yes" xml:space="preserve">
          <source>This class does not provide a &lt;code&gt;forward&lt;/code&gt; hook. Instead, you must use one of the underlying functions (e.g. &lt;code&gt;add&lt;/code&gt;).</source>
          <target state="translated">이 클래스는 &lt;code&gt;forward&lt;/code&gt; 후크를 제공하지 않습니다 . 대신 기본 함수 중 하나를 사용해야합니다 (예 : &lt;code&gt;add&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="dc115d83b6af36150f881200c064c4d238e34462" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="translated">이 클래스 대신 &lt;code&gt;interpolate()&lt;/code&gt; 가 사용되지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="c13ac423de7a3c762fee4c3a9510054ffdd5d384" translate="yes" xml:space="preserve">
          <source>This class is deprecated in favor of &lt;code&gt;interpolate()&lt;/code&gt;. It is equivalent to &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">이 클래스 대신 &lt;code&gt;interpolate()&lt;/code&gt; 가 사용되지 않습니다 . 그것은에 해당 &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f63c41dd97936df60be517dad06b50e25bfe14d0" translate="yes" xml:space="preserve">
          <source>This class uses &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; in order to retrieve the gradients for specific parameters.</source>
          <target state="translated">이 클래스는 특정 매개 변수에 대한 그라디언트를 검색하기 위해 &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt; 를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="1862d470ef8a5cd214866c60dc3b2e18ed9571b7" translate="yes" xml:space="preserve">
          <source>This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().</source>
          <target state="translated">이 집합체는 전체 그룹이이 함수에 들어갈 때까지, async_op이 False이거나 wait ()에서 비동기 작업 핸들이 호출 될 때까지 프로세스를 차단합니다.</target>
        </trans-unit>
        <trans-unit id="43ff3e72065cb78dedbfc0cb5795ef6d313a86b8" translate="yes" xml:space="preserve">
          <source>This composition also works for &lt;code&gt;nn.Module&lt;/code&gt;s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</source>
          <target state="translated">이 구성은 &lt;code&gt;nn.Module&lt;/code&gt; 에서도 작동하며 , 스크립트 모듈의 메서드에서 호출 할 수있는 추적을 사용하여 하위 모듈을 생성하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d56b8d3c3a6cd08249ac0400c9bd4fa666010028" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given &lt;code&gt;module&lt;/code&gt; by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</source>
          <target state="translated">이 컨테이너 는 배치 차원에서 청킹하여 지정된 장치에서 입력을 분할 하여 지정된 &lt;code&gt;module&lt;/code&gt; 의 응용 프로그램을 병렬화 합니다 (다른 개체는 장치 당 한 번 복사 됨). 포워드 패스에서 모듈은 각 장치에 복제되고 각 복제본은 입력의 일부를 처리합니다. 뒤로 통과하는 동안 각 복제본의 그라디언트가 원래 모듈에 합산됩니다.</target>
        </trans-unit>
        <trans-unit id="3e99fa6e39b3592c4eea5075ef0e0720a6948ff1" translate="yes" xml:space="preserve">
          <source>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</source>
          <target state="translated">이 컨테이너는 배치 차원에서 청킹하여 지정된 장치에서 입력을 분할하여 지정된 모듈의 응용 프로그램을 병렬화합니다. 모듈은 각 시스템과 각 장치에 복제되며 이러한 각 복제는 입력의 일부를 처리합니다. 역방향 패스 동안 각 노드의 기울기가 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="c09652531ac8f4b111870bcb71b813e31279457c" translate="yes" xml:space="preserve">
          <source>This context manager is thread local; it will not affect computation in other threads.</source>
          <target state="translated">이 컨텍스트 관리자는 스레드 로컬입니다. 다른 스레드의 계산에는 영향을주지 않습니다.</target>
        </trans-unit>
        <trans-unit id="26ef7031b1f75f10e86902c0f567ce05a4ea44b5" translate="yes" xml:space="preserve">
          <source>This context manager will keep track of already-joined DDP processes, and &amp;ldquo;shadow&amp;rdquo; the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.</source>
          <target state="translated">이 컨텍스트 관리자는 이미 조인 된 DDP 프로세스를 추적하고 조인되지 않은 DDP 프로세스에 의해 생성 된 것과 일치하는 집합 적 통신 작업을 삽입하여 정방향 및 역방향 패스를 &quot;그림자&quot;합니다. 이렇게하면 각 집단 호출이 이미 조인 된 DDP 프로세스에 의해 해당 호출이 있는지 확인하여 프로세스 전반에 걸쳐 불균등 한 입력으로 훈련 할 때 발생하는 중단 또는 오류를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="09d1e555fd19e686f48ac7306fa2525b334ba962" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt;&lt;code&gt;nn.LogSoftmax()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;nn.NLLLoss()&lt;/code&gt;&lt;/a&gt; in one single class.</source>
          <target state="translated">이 기준은 &lt;a href=&quot;generated/torch.nn.logsoftmax#torch.nn.LogSoftmax&quot;&gt; &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;nn.NLLLoss()&lt;/code&gt; &lt;/a&gt; 를 하나의 단일 클래스로 결합합니다.</target>
        </trans-unit>
        <trans-unit id="925d7f6dcda18d5ae787b2ea0d4e4cf02702d590" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;log_softmax&lt;/code&gt; and &lt;code&gt;nll_loss&lt;/code&gt; in a single function.</source>
          <target state="translated">이 기준은 단일 함수에서 &lt;code&gt;log_softmax&lt;/code&gt; 및 &lt;code&gt;nll_loss&lt;/code&gt; 를 결합 합니다.</target>
        </trans-unit>
        <trans-unit id="c7ea31e3bc10ddc7b5db55c0a7c9d23739bfa2db" translate="yes" xml:space="preserve">
          <source>This criterion combines &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; and &lt;code&gt;nn.NLLLoss()&lt;/code&gt; in one single class.</source>
          <target state="translated">이 기준은 &lt;code&gt;nn.LogSoftmax()&lt;/code&gt; 및 &lt;code&gt;nn.NLLLoss()&lt;/code&gt; 를 하나의 단일 클래스로 결합합니다.</target>
        </trans-unit>
        <trans-unit id="4a017ebd876454e242cede3ce02b356f5f295daa" translate="yes" xml:space="preserve">
          <source>This criterion expects a &lt;code&gt;target&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; of the same size as the &lt;code&gt;input&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">이 기준 은 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; 와 동일한 크기 의 &lt;code&gt;target&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; 를 예상합니다 .</target>
        </trans-unit>
        <trans-unit id="c10376150f504915e5bd847798be0f3a52dc669c" translate="yes" xml:space="preserve">
          <source>This criterion expects a class index in the range</source>
          <target state="translated">이 기준은 범위의 클래스 색인을 예상합니다.</target>
        </trans-unit>
        <trans-unit id="4c181079125dc2e25b97bd2d3da70b20b81a75a7" translate="yes" xml:space="preserve">
          <source>This decorator also works with RRef helpers, i.e., . &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 데코레이터는 RRef 도우미, 즉. &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_sync&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_sync()&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributed.rpc.RRef.rpc_async&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.rpc_async()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributed.rpc.RRef.remote&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.remote()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="ba581390e34d5e57c67e068b92dac8e4709b3ad6" translate="yes" xml:space="preserve">
          <source>This decorator indicates that a method on an &lt;code&gt;nn.Module&lt;/code&gt; is used as an entry point into a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and should be compiled.</source>
          <target state="translated">이 데코레이터는 &lt;code&gt;nn.Module&lt;/code&gt; 의 메서드 가 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 의 진입 점으로 사용되고 컴파일되어야 함을 나타냅니다 .</target>
        </trans-unit>
        <trans-unit id="174b3d4b823448bf6302b0b53449be4c6d5ad12e" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</source>
          <target state="translated">이 데코레이터는 함수 또는 메서드가 무시되고 Python 함수로 남겨 져야 함을 컴파일러에 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="6d10437bc80b062c46cb5ff8be0bb49322ddb59f" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">이 데코레이터는 함수 또는 메서드가 무시되고 Python 함수로 남겨 져야 함을 컴파일러에 나타냅니다. 이를 통해 아직 TorchScript와 호환되지 않는 코드를 모델에 남겨 둘 수 있습니다. TorchScript에서 호출되면 무시 된 함수가 Python 인터프리터로 호출을 전달합니다. 무시 된 기능이있는 모델은 내보낼 수 없습니다. 사용 &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt; 대신.</target>
        </trans-unit>
        <trans-unit id="1508df53ef2be98f34c189e45c94ca9319ce75f2" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</source>
          <target state="translated">이 데코레이터는 함수 또는 메서드를 무시하고 예외 발생으로 대체해야 함을 컴파일러에 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="75d6e9d0a82393986fc62b4441accfe0e14ee3a9" translate="yes" xml:space="preserve">
          <source>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.</source>
          <target state="translated">이 데코레이터는 함수 또는 메서드를 무시하고 예외 발생으로 대체해야 함을 컴파일러에 나타냅니다. 이를 통해 아직 TorchScript와 호환되지 않는 코드를 모델에 남겨두고 모델을 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8952c5fbe2a12cf21923d9bc22bc2c081674eb1e" translate="yes" xml:space="preserve">
          <source>This defines</source>
          <target state="translated">이것은 정의</target>
        </trans-unit>
        <trans-unit id="e40f466c8c675241a4989cdf735bce5f4ddce6bc" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?orgqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;LAPACK documentation for orgqr&lt;/a&gt; for further details.</source>
          <target state="translated">이것은 기본 LAPACK 함수 &lt;code&gt;?orgqr&lt;/code&gt; 직접 호출합니다 . 자세한 내용 &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-orgqr&quot;&gt;은 orgqr에&lt;/a&gt; 대한 LAPACK 문서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="9fb3c5a1f24bdd83af6fd14c07d4d7ef76ec7527" translate="yes" xml:space="preserve">
          <source>This directly calls the underlying LAPACK function &lt;code&gt;?ormqr&lt;/code&gt;. See &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;LAPACK documentation for ormqr&lt;/a&gt; for further details.</source>
          <target state="translated">이것은 기본 LAPACK 함수 &lt;code&gt;?ormqr&lt;/code&gt; 직접 호출합니다 . 자세한 내용 &lt;a href=&quot;https://software.intel.com/en-us/mkl-developer-reference-c-ormqr&quot;&gt;은 ormqr에&lt;/a&gt; 대한 LAPACK 문서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="d7d3fb60afba5b578d1ed91b3b2d20aba1664bb0" translate="yes" xml:space="preserve">
          <source>This error usually means that the method you are tracing uses a module&amp;rsquo;s parameters and you are passing the module&amp;rsquo;s method instead of the module instance (e.g. &lt;code&gt;my_module_instance.forward&lt;/code&gt; vs &lt;code&gt;my_module_instance&lt;/code&gt;).</source>
          <target state="translated">이 오류는 일반적으로 추적중인 메소드가 모듈의 매개 변수를 사용하고 모듈 인스턴스 대신 모듈의 메소드를 전달 함을 의미합니다 (예 : &lt;code&gt;my_module_instance.forward&lt;/code&gt; 대 &lt;code&gt;my_module_instance&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="c753cc91ef9f274f764a4ae41747b732c7fe2261" translate="yes" xml:space="preserve">
          <source>This feature is in beta, and its design and implementation may change in the future.</source>
          <target state="translated">이 기능은 베타 버전이며 향후 설계 및 구현이 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b7614e828d8ac51033f6b1d436da1e7ec19a65d5" translate="yes" xml:space="preserve">
          <source>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; object by accessing its &lt;code&gt;.data&lt;/code&gt; attribute.</source>
          <target state="translated">이 함수는 최소 2 차원이있는 모든 입력을받습니다. 이를 적용하여 레이블을 포장하고 RNN의 출력을 함께 사용하여 손실을 직접 계산할 수 있습니다. Tensor는 &lt;code&gt;.data&lt;/code&gt; 속성 에 액세스 하여 &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt; 객체 에서 검색 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="97ac98bd75da52df5620120c1ca469a7f25204b3" translate="yes" xml:space="preserve">
          <source>This function accumulates gradients in the leaves - you might need to zero &lt;code&gt;.grad&lt;/code&gt; attributes or set them to &lt;code&gt;None&lt;/code&gt; before calling it. See &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;Default gradient layouts&lt;/a&gt; for details on the memory layout of accumulated gradients.</source>
          <target state="translated">이 함수는 나뭇잎에 그라디언트를 누적합니다. &lt;code&gt;.grad&lt;/code&gt; 속성 을 0으로 설정하거나 호출하기 전에 &lt;code&gt;None&lt;/code&gt; 으로 설정해야 할 수 있습니다. 누적 된 그라디언트의 메모리 레이아웃에 대한 자세한 내용 은 &lt;a href=&quot;autograd#default-grad-layouts&quot;&gt;기본 그라디언트 레이아웃&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5919d90354daacc619c8dc367427cc669c661c6e" translate="yes" xml:space="preserve">
          <source>This function behaves exactly like &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt;&lt;code&gt;load_inline()&lt;/code&gt;&lt;/a&gt; is identical to &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수는 &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt; 와 똑같이 작동 하지만 소스를 파일 이름이 아닌 문자열로 사용합니다. 이러한 문자열의 동작 후 빌드 디렉토리에 파일로 저장됩니다 &lt;a href=&quot;#torch.utils.cpp_extension.load_inline&quot;&gt; &lt;code&gt;load_inline()&lt;/code&gt; &lt;/a&gt; 동일 &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4742c80d8253ffcd737e46fcd5dffcfb774bfb9a" translate="yes" xml:space="preserve">
          <source>This function calculates all eigenvalues (and vectors) of &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">이 함수는 &lt;code&gt;input&lt;/code&gt; 모든 고유 값 (및 벡터)을 계산 하여</target>
        </trans-unit>
        <trans-unit id="f591ac1d01d2706090dd5158a48cfafbf7078fcd" translate="yes" xml:space="preserve">
          <source>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the &lt;code&gt;ord&lt;/code&gt; parameter.</source>
          <target state="translated">이 함수는 감소 차원의 수와 &lt;code&gt;ord&lt;/code&gt; 매개 변수 의 값에 따라 8 가지 다른 유형의 행렬 노름 중 하나 또는 무한한 수의 벡터 노름 중 하나를 계산할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a99e80c93a86e3189bf95a219e4fc70666128f91" translate="yes" xml:space="preserve">
          <source>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</source>
          <target state="translated">이 함수는 버전 0.4.1에서 서명을 변경했습니다. 이전 서명으로 호출하면 오류가 발생하거나 잘못된 결과가 반환 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2742ba18db0e477006e0d4a879a88ee8aaa73a1c" translate="yes" xml:space="preserve">
          <source>This function checks if all &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; satisfy the condition:</source>
          <target state="translated">이 함수는 모든 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; 가 조건을 충족 하는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="4ad58a0a805b7dcf7c19d64d7ab1a5b0774c3bf4" translate="yes" xml:space="preserve">
          <source>This function does exact same thing as &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt; in the forward, except that it supports backward for sparse matrix &lt;code&gt;mat1&lt;/code&gt;. &lt;code&gt;mat1&lt;/code&gt; need to have &lt;code&gt;sparse_dim = 2&lt;/code&gt;. Note that the gradients of &lt;code&gt;mat1&lt;/code&gt; is a coalesced sparse tensor.</source>
          <target state="translated">이 함수는 희소 행렬 &lt;code&gt;mat1&lt;/code&gt; 에 대해 뒤로 지원한다는 점을 제외하면 앞으로 &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt; &lt;code&gt;torch.addmm()&lt;/code&gt; &lt;/a&gt; 와 똑같은 작업을 수행 합니다 . &lt;code&gt;mat1&lt;/code&gt; 에는 &lt;code&gt;sparse_dim = 2&lt;/code&gt; 가 있어야합니다 . &lt;code&gt;mat1&lt;/code&gt; 의 기울기 는 합쳐진 희소 텐서입니다.</target>
        </trans-unit>
        <trans-unit id="b687a9d19b48f08afdcc0c1bcf3778a2f67002f3" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;.</source>
          <target state="translated">이 기능은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;방송&lt;/a&gt; 되지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="6eaaf4ac25e2420bf8da7dbd7b25891750193e61" translate="yes" xml:space="preserve">
          <source>This function does not &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcast&lt;/a&gt;. For broadcasting matrix products, see &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt;&lt;code&gt;torch.matmul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 기능은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;방송&lt;/a&gt; 되지 않습니다 . 브로드 캐스팅 매트릭스 제품에 대해서는 &lt;a href=&quot;torch.matmul#torch.matmul&quot;&gt; &lt;code&gt;torch.matmul()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1273ec3288d9debd903d3f9e04111f9b0d5a8d6b" translate="yes" xml:space="preserve">
          <source>This function does not check if the factorization was successful or not if &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; since the status of the factorization is present in the third element of the return tuple.</source>
          <target state="translated">경우 인수 분해의 성공 아니었다면이 기능은 확인하지 않습니다 &lt;code&gt;get_infos&lt;/code&gt; 가 있다 &lt;code&gt;True&lt;/code&gt; 인수 분해의 상태가 반환 튜플의 세 번째 요소에 존재하기 때문이다.</target>
        </trans-unit>
        <trans-unit id="68c688ded2e6ac8d7e4721168f69f94608270e24" translate="yes" xml:space="preserve">
          <source>This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (&lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt;) can optimize the formula for you.</source>
          <target state="translated">이 함수는 주어진 표현식을 최적화하지 않으므로 동일한 계산에 대해 다른 공식이 더 빨리 실행되거나 메모리를 덜 소비 할 수 있습니다. opt_einsum ( &lt;a href=&quot;https://optimized-einsum.readthedocs.io/en/stable/&quot;&gt;https://optimized-einsum.readthedocs.io/en/stable/&lt;/a&gt; ) 과 같은 프로젝트 는 수식을 최적화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f44824ec132a2e35c26075b94e4bfe941180a2db" translate="yes" xml:space="preserve">
          <source>This function doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">이 함수는 NLLLoss와 직접 작동하지 않습니다. NLLLoss는 Softmax와 자체 사이에서 Log가 계산 될 것으로 예상합니다. 대신 log_softmax를 사용하십시오 (더 빠르고 더 나은 수치 속성을 가짐).</target>
        </trans-unit>
        <trans-unit id="4c40534c7583c126c2b6a9ea9f9ce426b4120e2b" translate="yes" xml:space="preserve">
          <source>This function is a front-end to the following LOBPCG algorithms selectable via &lt;code&gt;method&lt;/code&gt; argument:</source>
          <target state="translated">이 함수는 &lt;code&gt;method&lt;/code&gt; 인수 를 통해 선택할 수있는 다음 LOBPCG 알고리즘에 대한 프론트 엔드입니다 .</target>
        </trans-unit>
        <trans-unit id="773eb7977e3d589f5a2a946752611ffc03872d99" translate="yes" xml:space="preserve">
          <source>This function is deprecated and may be removed in a future release. It can be implemented using &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; as &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is not zero, and as &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; when &lt;code&gt;beta&lt;/code&gt; is zero.</source>
          <target state="translated">이 기능은 더 이상 사용되지 않으며 향후 릴리스에서 제거 될 수 있습니다. 이를 사용하여 구현 될 수 &lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 같이 &lt;code&gt;alpha * torch.outer(vec1, vec2) + beta * input&lt;/code&gt; 할 때 &lt;code&gt;beta&lt;/code&gt; 0이 아닌, 그리고 &lt;code&gt;alpha * torch.outer(vec1, vec2)&lt;/code&gt; 때 &lt;code&gt;beta&lt;/code&gt; 제로이다.</target>
        </trans-unit>
        <trans-unit id="ecdb962fda9e357b596ac207e636ee312e395193" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future PyTorch release. Use &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">이 함수는 더 이상 사용되지 않으며 향후 PyTorch 릴리스에서 제거됩니다. 대신 &lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="32ba15aae127598880c645601e7ae7f8189c6c82" translate="yes" xml:space="preserve">
          <source>This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python&amp;rsquo;s range builtin. Instead, use &lt;a href=&quot;torch.arange#torch.arange&quot;&gt;&lt;code&gt;torch.arange()&lt;/code&gt;&lt;/a&gt;, which produces values in [start, end).</source>
          <target state="translated">이 함수는 더 이상 사용되지 않으며 동작이 Python의 범위 내장과 일치하지 않기 때문에 향후 릴리스에서 제거 될 예정입니다. 대신 [start, end)에 값을 생성 하는 &lt;a href=&quot;torch.arange#torch.arange&quot;&gt; &lt;code&gt;torch.arange()&lt;/code&gt; &lt;/a&gt; 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="cd06f9dfe663c3caf3762160bc99450c126a39a9" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.functional.interpolate(...)&lt;/code&gt; 와 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="1632348f060a8b22bffa765800a936ba8981c455" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e11fc8a8842739d9e68d3ea9f36d243b26e33ee2" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.functional.interpolate(..., mode='nearest')&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="409b1beff6de19c9e0fc8534a682b00b07c2667b" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.quantized.functional.interpolate(...)&lt;/code&gt; 와 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="f088ca0f34a9882f6e67d133eea7956f99a05c61" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="a1e1948ca54d505e36aa98bcd4ccf28bf3f09403" translate="yes" xml:space="preserve">
          <source>This function is deprecated in favor of &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt;&lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt;&lt;/a&gt;. This is equivalent with &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt;.</source>
          <target state="translated">이 함수 대신 &lt;a href=&quot;#torch.nn.quantized.functional.interpolate&quot;&gt; &lt;code&gt;torch.nn.quantized.functional.interpolate()&lt;/code&gt; &lt;/a&gt; 가 사용되지 않습니다 . 이것은 &lt;code&gt;nn.quantized.functional.interpolate(..., mode='nearest')&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="6111b83606e5f76d2d19a4fd63d2df2a457f3738" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique#torch.unique&quot;&gt;&lt;code&gt;torch.unique()&lt;/code&gt;&lt;/a&gt; in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to &lt;code&gt;std::unique&lt;/code&gt; in C++.</source>
          <target state="translated">이 함수는 연속 된 중복 값만 제거한다는 점 에서 &lt;a href=&quot;torch.unique#torch.unique&quot;&gt; &lt;code&gt;torch.unique()&lt;/code&gt; &lt;/a&gt; 와 다릅니다 . 이 의미는 C ++의 &lt;code&gt;std::unique&lt;/code&gt; 와 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="c84bc16a342354e3160aa386d7ad206e77f3ce71" translate="yes" xml:space="preserve">
          <source>This function is different from &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; in the sense that this function also eliminates non-consecutive duplicate values.</source>
          <target state="translated">이 함수는 연속적이지 않은 중복 값도 제거한다는 점 에서 &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt; 와 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="42a1f69345a02122fc091a770868079bb8638971" translate="yes" xml:space="preserve">
          <source>This function is differentiable, so gradients will flow back from the result of this operation to &lt;code&gt;input&lt;/code&gt;. To create a tensor without an autograd relationship to &lt;code&gt;input&lt;/code&gt; see &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt;&lt;code&gt;detach()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수는 미분 할 수 있으므로 그래디언트는이 작업의 결과에서 &lt;code&gt;input&lt;/code&gt; 으로 다시 흐릅니다 . &lt;code&gt;input&lt;/code&gt; 대한 autograd 관계없이 텐서를 만들려면 &lt;a href=&quot;../autograd#torch.Tensor.detach&quot;&gt; &lt;code&gt;detach()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0fb9aa5da82ff7de2350851193131992c19c52a8" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">이 기능은 동일하다 &lt;code&gt;scipy.spatial.distance.cdist(input,&amp;rsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; 경우,</target>
        </trans-unit>
        <trans-unit id="cc680c0e90287ae914b5b4470b9652c693f1647a" translate="yes" xml:space="preserve">
          <source>This function is equivalent to &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; if</source>
          <target state="translated">이 기능은 동일하다 &lt;code&gt;scipy.spatial.distance.pdist(input, &amp;lsquo;minkowski&amp;rsquo;, p=p)&lt;/code&gt; 경우,</target>
        </trans-unit>
        <trans-unit id="ff6a78f486235a2c4ee2bb0612d674661defb9c5" translate="yes" xml:space="preserve">
          <source>This function is here for legacy reasons, may be removed from nn.Functional in the future.</source>
          <target state="translated">이 기능은 레거시 이유로 여기에 있으며 향후 nn.Functional에서 제거 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4ffbcd1ffd822d93e92f3e882d1d47ebf27a02de" translate="yes" xml:space="preserve">
          <source>This function is implemented only for nonnegative integers</source>
          <target state="translated">이 함수는 음이 아닌 정수에 대해서만 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="4af64360b4b0bf60d1db0c5268a216c0039ae031" translate="yes" xml:space="preserve">
          <source>This function is more accurate than &lt;a href=&quot;torch.log#torch.log&quot;&gt;&lt;code&gt;torch.log()&lt;/code&gt;&lt;/a&gt; for small values of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">이 함수는 &lt;code&gt;input&lt;/code&gt; 값이 작은 경우 &lt;a href=&quot;torch.log#torch.log&quot;&gt; &lt;code&gt;torch.log()&lt;/code&gt; &lt;/a&gt; 보다 정확 합니다.</target>
        </trans-unit>
        <trans-unit id="f9a118660ffcea349bb4668844a1f15351a99bbb" translate="yes" xml:space="preserve">
          <source>This function is not defined for &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; yet.</source>
          <target state="translated">이 함수는 아직 &lt;code&gt;torch.cuda.Tensor&lt;/code&gt; 에 대해 정의 되지 않았습니다.</target>
        </trans-unit>
        <trans-unit id="69c6cf6408f0b3c6058b2d09b95171f65a52d054" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt;&lt;code&gt;affine_grid()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">이 함수는 종종 &lt;a href=&quot;#torch.nn.functional.affine_grid&quot;&gt; &lt;code&gt;affine_grid()&lt;/code&gt; &lt;/a&gt; 와 함께 사용되어 &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; 를 구축 합니다 .</target>
        </trans-unit>
        <trans-unit id="3bda1093b38d267fe7d744a3f96bb51ca111b18d" translate="yes" xml:space="preserve">
          <source>This function is often used in conjunction with &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; to build &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; .</source>
          <target state="translated">이 함수는 종종 &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt; 과 함께 사용되어 &lt;a href=&quot;https://arxiv.org/abs/1506.02025&quot;&gt;Spatial Transformer Networks&lt;/a&gt; 를 구축 합니다 .</target>
        </trans-unit>
        <trans-unit id="d26ea75bdd03743d020bb8fd3f0b4f05e5fca44d" translate="yes" xml:space="preserve">
          <source>This function only works with CPU tensors and should not be used in code sections that require high performance.</source>
          <target state="translated">이 함수는 CPU 텐서에서만 작동하며 고성능이 필요한 코드 섹션에서는 사용하지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="7ed8f6139c0d2640189a9321e1da51e7b538d228" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;max(dim=0)&lt;/code&gt;</source>
          <target state="translated">이 함수는 &lt;code&gt;max(dim=0)&lt;/code&gt; 과 달리 결정 론적 (하위) 기울기를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="49eba71d4c9b31d15ee67772326e88d1462e9393" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;median(dim=0)&lt;/code&gt;</source>
          <target state="translated">이 함수는 &lt;code&gt;median(dim=0)&lt;/code&gt; 과 달리 결정 론적 (하위) 기울기를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d9251c4cd19a437c8f31a2c44be4bdf2e2282d65" translate="yes" xml:space="preserve">
          <source>This function produces deterministic (sub)gradients unlike &lt;code&gt;min(dim=0)&lt;/code&gt;</source>
          <target state="translated">이 함수는 &lt;code&gt;min(dim=0)&lt;/code&gt; 과 달리 결정적 (하위) 기울기를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="7d3e2fb10adcbb7f259d94a424b7b3b695d7efa3" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e.</source>
          <target state="translated">이 함수는 다중 선형 표현식을 계산하는 방법을 제공합니다 (예 :</target>
        </trans-unit>
        <trans-unit id="6dd319ec8d2bd7cb7695497b2e2c0c7da18093d6" translate="yes" xml:space="preserve">
          <source>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</source>
          <target state="translated">이 함수는 아인슈타인 합산 규칙을 ​​사용하여 다중 선형 표현식 (즉, 곱의 합)을 계산하는 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="464a58780f1e7ad5a78398d14f9e91d688f64e19" translate="yes" xml:space="preserve">
          <source>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</source>
          <target state="translated">이 기능을 사용하려면 기본 그룹의 모든 프로세스 (즉, 분산 작업의 일부인 모든 프로세스)가 그룹의 구성원이 아니더라도이 기능에 들어가야합니다. 또한 모든 프로세스에서 동일한 순서로 그룹을 만들어야합니다.</target>
        </trans-unit>
        <trans-unit id="a52b7a21105c565c661abb413198331f8bc8a45e" translate="yes" xml:space="preserve">
          <source>This function returns a Tensor of size &lt;code&gt;T x B x *&lt;/code&gt; or &lt;code&gt;B x T x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</source>
          <target state="translated">이 함수는 &lt;code&gt;T x B x *&lt;/code&gt; 또는 &lt;code&gt;B x T x *&lt;/code&gt; 크기의 Tensor를 반환합니다. 여기서 &lt;code&gt;T&lt;/code&gt; 는 가장 긴 시퀀스의 길이입니다. 이 함수는 시퀀스에있는 모든 텐서의 후행 차원과 유형이 동일하다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="c177e0164ff9028b4584fc5df4d1e3daee9e253a" translate="yes" xml:space="preserve">
          <source>This function returns a handle with a method &lt;code&gt;handle.remove()&lt;/code&gt; that removes the hook from the module.</source>
          <target state="translated">이 함수는 모듈에서 후크를 제거하는 &lt;code&gt;handle.remove()&lt;/code&gt; 메서드가있는 핸들을 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="a0496aaa431bc80a1684ef66937522b9d485ea0d" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</source>
          <target state="translated">This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the nearly optimal approximation of a singular value decomposition of a centered matrix</target>
        </trans-unit>
        <trans-unit id="eda5e94a6c116342560b85fa8d7ac690f3e68684" translate="yes" xml:space="preserve">
          <source>This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</source>
          <target state="translated">This function returns a namedtuple &lt;code&gt;(U, S, V)&lt;/code&gt; which is the singular value decomposition of a input real matrix or batches of real matrices &lt;code&gt;input&lt;/code&gt; such that</target>
        </trans-unit>
        <trans-unit id="5aeb212c2592eaa5e981d2676890741ba536e05d" translate="yes" xml:space="preserve">
          <source>This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</source>
          <target state="translated">This function returns eigenvalues and eigenvectors of a real symmetric matrix &lt;code&gt;input&lt;/code&gt; or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</target>
        </trans-unit>
        <trans-unit id="2f955323e5a9f295f213a25f7ecc7dec98b5caa2" translate="yes" xml:space="preserve">
          <source>This function returns the solution to the system of linear equations represented by</source>
          <target state="translated">This function returns the solution to the system of linear equations represented by</target>
        </trans-unit>
        <trans-unit id="7d7a84c0650abce4d3f9c668ace3ede1befabda4" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</source>
          <target state="translated">This function&amp;rsquo;s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.</target>
        </trans-unit>
        <trans-unit id="c29a81af2b62737c72764612d782901c3f2eb4d7" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt;&lt;code&gt;Dropout&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;a href=&quot;torch.nn.dropout#torch.nn.Dropout&quot;&gt; &lt;code&gt;Dropout&lt;/code&gt; &lt;/a&gt;, &lt;code&gt;BatchNorm&lt;/code&gt; , etc.</target>
        </trans-unit>
        <trans-unit id="a56c6998fe86297bbe85815fb835af9e00568e26" translate="yes" xml:space="preserve">
          <source>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt;, &lt;code&gt;BatchNorm&lt;/code&gt;, etc.</source>
          <target state="translated">This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. &lt;code&gt;Dropout&lt;/code&gt; , &lt;code&gt;BatchNorm&lt;/code&gt; , etc.</target>
        </trans-unit>
        <trans-unit id="4f8efe32a81509d5ad4a6e1e586b778004252c39" translate="yes" xml:space="preserve">
          <source>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</source>
          <target state="translated">This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="16cf55bdf2b28322a9b3129eb62ae4e3fc4f60f8" translate="yes" xml:space="preserve">
          <source>This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from &lt;a href=&quot;https://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111&quot;&gt;here&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e5d17292f2576cd2e36da2cca0252f9780e064b5" translate="yes" xml:space="preserve">
          <source>This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</source>
          <target state="translated">This invariant is maintained throughout &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt; class, and all functions that construct a &lt;code&gt;:class:PackedSequence&lt;/code&gt; in PyTorch (i.e., they only pass in tensors conforming to this constraint).</target>
        </trans-unit>
        <trans-unit id="0dcba97fb0267a928459453464ca8f56762a0535" translate="yes" xml:space="preserve">
          <source>This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</source>
          <target state="translated">This is TorchScript&amp;rsquo;s compilation of the code for the &lt;code&gt;forward&lt;/code&gt; method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</target>
        </trans-unit>
        <trans-unit id="78aa499d132d9b73782bab5e2924798d784eafb1" translate="yes" xml:space="preserve">
          <source>This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</source>
          <target state="translated">This is a &lt;strong&gt;Prototype&lt;/strong&gt; function.</target>
        </trans-unit>
        <trans-unit id="554d0b95763033953c158f9820e38365e62ff2d9" translate="yes" xml:space="preserve">
          <source>This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is a generalized version of &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt; &lt;code&gt;torch.hann_window()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="27da13545c38e0ad916bfb253add6c85dbfec69d" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly.</source>
          <target state="translated">This is a low-level function for calling LAPACK directly.</target>
        </trans-unit>
        <trans-unit id="baa9ff1557d00c12989b3535b16426973504c66e" translate="yes" xml:space="preserve">
          <source>This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</source>
          <target state="translated">This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in &lt;a href=&quot;https://software.intel.com/en-us/node/521004&quot;&gt;LAPACK documentation for geqrf&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bed0909b657a7fcd560ee9c527be9120da241148" translate="yes" xml:space="preserve">
          <source>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt;&lt;code&gt;set_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;, which checks for contiguity, or &lt;a href=&quot;#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;, which copies data if needed. To change the size in-place with custom strides, see &lt;a href=&quot;#torch.Tensor.set_&quot;&gt; &lt;code&gt;set_()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f3b5e88eb67e346fdbdf0577eada21e1b383ed17" translate="yes" xml:space="preserve">
          <source>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</source>
          <target state="translated">This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</target>
        </trans-unit>
        <trans-unit id="f5b9f918ec25aa03605059dcc957950d2df96994" translate="yes" xml:space="preserve">
          <source>This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</source>
          <target state="translated">This is a no-op if the tensor is already of the correct type. This is equivalent to &lt;code&gt;self.type(tensor.type())&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be9417a8a097120c57731208ecd0b2e0d9e0a45b" translate="yes" xml:space="preserve">
          <source>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</source>
          <target state="translated">This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</target>
        </trans-unit>
        <trans-unit id="584f2eb806dc59e0bb28f52080bdd509e138734b" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="3187ce0a4f646a0f02690921fbe6d7114d95c7e3" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 1d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="3a2d1424088d04fbdd7c029c2c5a864c5f132531" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="ee8eabed86139ec26051c16d9d19973c0fd6b09c" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="12efc861b006501b2b4700d386ad4f25b9ff0f7e" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="98a705e3678fe480b570b51f1f965a2f3d037e83" translate="yes" xml:space="preserve">
          <source>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</source>
          <target state="translated">This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module.</target>
        </trans-unit>
        <trans-unit id="fd838137a8dd7616f8151e3b75e72da4794d6fa0" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</source>
          <target state="translated">This is a variant of &lt;a href=&quot;generated/torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist.</target>
        </trans-unit>
        <trans-unit id="57134b77fa81442b10e02b41eaf3243756f7f5ee" translate="yes" xml:space="preserve">
          <source>This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt;. See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt;&lt;code&gt;torch.quantile()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is a variant of &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt; that &amp;ldquo;ignores&amp;rdquo; &lt;code&gt;NaN&lt;/code&gt; values, computing the quantiles &lt;code&gt;q&lt;/code&gt; as if &lt;code&gt;NaN&lt;/code&gt; values in &lt;code&gt;input&lt;/code&gt; did not exist. If all values in a reduced row are &lt;code&gt;NaN&lt;/code&gt; then the quantiles for that reduction will be &lt;code&gt;NaN&lt;/code&gt; . See the documentation for &lt;a href=&quot;torch.quantile#torch.quantile&quot;&gt; &lt;code&gt;torch.quantile()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="55c07d098e34194632b960dfc4bd298727835fac" translate="yes" xml:space="preserve">
          <source>This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</source>
          <target state="translated">This is always &lt;code&gt;True&lt;/code&gt; for CUDA tensors.</target>
        </trans-unit>
        <trans-unit id="16c88bb46cbf5ef3f448ca4fe755467e88b5e578" translate="yes" xml:space="preserve">
          <source>This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt;&lt;code&gt;torch.Tensor.repeat()&lt;/code&gt;&lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt;.</source>
          <target state="translated">This is different from &lt;a href=&quot;../tensors#torch.Tensor.repeat&quot;&gt; &lt;code&gt;torch.Tensor.repeat()&lt;/code&gt; &lt;/a&gt; but similar to &lt;code&gt;numpy.repeat&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="db053cd2631236023d48272222af395e026941e8" translate="yes" xml:space="preserve">
          <source>This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt;, but is more efficient in some cases.</source>
          <target state="translated">This is equivalent to &lt;code&gt;self.log_pob(input).argmax(dim=1)&lt;/code&gt; , but is more efficient in some cases.</target>
        </trans-unit>
        <trans-unit id="374b7dab14fec694b4f5430f4c9e29dd3262cee0" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt;&lt;code&gt;torch.atleast_2d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_2d#torch.atleast_2d&quot;&gt; &lt;code&gt;torch.atleast_2d()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b6ef108509ff368b6991089b8608a37aca821f0d" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</source>
          <target state="translated">This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.</target>
        </trans-unit>
        <trans-unit id="c5b9e7c481799905ba918331b66fdb7b66a8da08" translate="yes" xml:space="preserve">
          <source>This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt;&lt;code&gt;torch.atleast_3d()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by &lt;a href=&quot;torch.atleast_3d#torch.atleast_3d&quot;&gt; &lt;code&gt;torch.atleast_3d()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c60beaf37836d9a93c155f39fdfc6ecdf807a30e" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent with &lt;a href=&quot;#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="d6810c532f9495d317b56fc8164ab42f154df6d1" translate="yes" xml:space="preserve">
          <source>This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt;&lt;code&gt;self.train(False)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is equivalent with &lt;a href=&quot;torch.nn.module#torch.nn.Module.train&quot;&gt; &lt;code&gt;self.train(False)&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="18f227b9fbcb58581a48e62cc2b1e4b0a1736273" translate="yes" xml:space="preserve">
          <source>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</source>
          <target state="translated">This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model &lt;em&gt;before&lt;/em&gt; saving it ensures that the tracer has the correct device information.</target>
        </trans-unit>
        <trans-unit id="61139b0235e460eab7709588bf0db5fef3c1ed33" translate="yes" xml:space="preserve">
          <source>This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</source>
          <target state="translated">This is supported for &lt;a href=&quot;#module-attributes&quot;&gt;module attributes&lt;/a&gt; class attribute annotations but not for functions</target>
        </trans-unit>
        <trans-unit id="5d93a3f447e3870288983d617bf3d5ca0babcaca" translate="yes" xml:space="preserve">
          <source>This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt;).</source>
          <target state="translated">This is the default method, meaning that &lt;code&gt;init_method&lt;/code&gt; does not have to be specified (or can be &lt;code&gt;env://&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="b58bf6260a34528b9c54af5a74ed650426bd462d" translate="yes" xml:space="preserve">
          <source>This is the functional version of the DataParallel module.</source>
          <target state="translated">This is the functional version of the DataParallel module.</target>
        </trans-unit>
        <trans-unit id="9f2aa90c9ddd953d8134c3b2013f0dd5a5e04793" translate="yes" xml:space="preserve">
          <source>This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt;&lt;code&gt;ELU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized equivalent of &lt;a href=&quot;generated/torch.nn.elu#torch.nn.ELU&quot;&gt; &lt;code&gt;ELU&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="051cad7dbbb9111a95c9efe4f04cfeb331282a9e" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="ea4fe276ddec4ebc483841a1170deaf100cb0956" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt;&lt;code&gt;BatchNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d&quot;&gt; &lt;code&gt;BatchNorm3d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e5ce0ce0859be0cbdcd66ae4d96e1834e80fb440" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt;&lt;code&gt;GroupNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.groupnorm#torch.nn.GroupNorm&quot;&gt; &lt;code&gt;GroupNorm&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4fd9da5ac2b1219bc045ac6f1d4efc7962f29b18" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt;&lt;code&gt;Hardswish&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.hardswish#torch.nn.Hardswish&quot;&gt; &lt;code&gt;Hardswish&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="81ee4b5a559a55d776be5e5ad40fea15e08c77c3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt;&lt;code&gt;InstanceNorm1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d&quot;&gt; &lt;code&gt;InstanceNorm1d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="306614a03d85542a417f30827501dbc097b0e4e2" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt;&lt;code&gt;InstanceNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d&quot;&gt; &lt;code&gt;InstanceNorm2d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5c54c49dfa0b1be230f77c6acfc8832f75ff793b" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt;&lt;code&gt;InstanceNorm3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d&quot;&gt; &lt;code&gt;InstanceNorm3d&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="1ab22dac0481c7e9d72041caf66a8d6e6bce24a3" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt;&lt;code&gt;LayerNorm&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;generated/torch.nn.layernorm#torch.nn.LayerNorm&quot;&gt; &lt;code&gt;LayerNorm&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="1c1757bb112d3facda0c6b8d39d0c0a08d49265a" translate="yes" xml:space="preserve">
          <source>This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt;&lt;code&gt;hardswish()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the quantized version of &lt;a href=&quot;nn.functional#torch.nn.functional.hardswish&quot;&gt; &lt;code&gt;hardswish()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c6493b5bad0951f0b43e4846b14cd1a429b62cc1" translate="yes" xml:space="preserve">
          <source>This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is the reverse operation of the manner described in &lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5295faab37629f799f6e6fbb1e9825a33f998b68" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;torch.max()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;torch.max()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="19c006dca5dfcf6b24613e9eb4ec7d4564e3ffde" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;torch.min()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;torch.min()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="ea4ca9f34dc42157c431ad05a09921e13d264890" translate="yes" xml:space="preserve">
          <source>This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt;&lt;code&gt;torch.sort()&lt;/code&gt;&lt;/a&gt;. See its documentation for the exact semantics of this method.</source>
          <target state="translated">This is the second value returned by &lt;a href=&quot;torch.sort#torch.sort&quot;&gt; &lt;code&gt;torch.sort()&lt;/code&gt; &lt;/a&gt;. See its documentation for the exact semantics of this method.</target>
        </trans-unit>
        <trans-unit id="f4162fd55661c8ae3715301b7e45fb9d304f83a8" translate="yes" xml:space="preserve">
          <source>This is typically passed to an optimizer.</source>
          <target state="translated">This is typically passed to an optimizer.</target>
        </trans-unit>
        <trans-unit id="8881d58b1f068203ff9ef6d05a7b8193f5aad690" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="bc05416a2fcfa2651cf566b1804c2fef547cfa8c" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="cdf0d415e18e8e03494fb14369b3744608b2dde4" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="387b12018432b2012dd14d16de801f3f2b4ba6be" translate="yes" xml:space="preserve">
          <source>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm&amp;rsquo;s &lt;code&gt;running_mean&lt;/code&gt; is not a parameter, but is part of the module&amp;rsquo;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting &lt;code&gt;persistent&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="18ae6606a0b9fa5a1d8375a29d5adf1260f68aa6" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</source>
          <target state="translated">This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets</target>
        </trans-unit>
        <trans-unit id="44bcfdd8636d4fef63e74c75c8c5e2e8049a6723" translate="yes" xml:space="preserve">
          <source>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</source>
          <target state="translated">This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets &lt;code&gt;t[i]&lt;/code&gt; should be numbers between 0 and 1.</target>
        </trans-unit>
        <trans-unit id="82ff1110b5f2f18a81804af99bb7a1033487ea4f" translate="yes" xml:space="preserve">
          <source>This is useful for implementing efficient sub-pixel convolution with a stride of</source>
          <target state="translated">This is useful for implementing efficient sub-pixel convolution with a stride of</target>
        </trans-unit>
        <trans-unit id="0b3e0c96ed0785872533b5ba8bff1ae227ce271e" translate="yes" xml:space="preserve">
          <source>This layer uses statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">This layer uses statistics computed from input data in both training and evaluation modes.</target>
        </trans-unit>
        <trans-unit id="e61f8043e790e86849febfe55d39af2c5aac3218" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</source>
          <target state="translated">This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class.</target>
        </trans-unit>
        <trans-unit id="940b923ef28b8c58cfd52fc019408d07c1f8a33f" translate="yes" xml:space="preserve">
          <source>This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</source>
          <target state="translated">This loss combines a &lt;code&gt;Sigmoid&lt;/code&gt; layer and the &lt;code&gt;BCELoss&lt;/code&gt; in one single class. This version is more numerically stable than using a plain &lt;code&gt;Sigmoid&lt;/code&gt; followed by a &lt;code&gt;BCELoss&lt;/code&gt; as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</target>
        </trans-unit>
        <trans-unit id="b85bf08f0c04a7b3a3773bc9f4c6784cf3720ebd" translate="yes" xml:space="preserve">
          <source>This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</source>
          <target state="translated">This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &amp;lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.</target>
        </trans-unit>
        <trans-unit id="0b5f23050e1225367270ed2f80d8587f70aa2118" translate="yes" xml:space="preserve">
          <source>This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt;. Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt;, and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</source>
          <target state="translated">This message indicates to us that the computation differed between when we first traced it and when we traced it with the &lt;code&gt;check_inputs&lt;/code&gt; . Indeed, the loop within the body of &lt;code&gt;loop_in_traced_fn&lt;/code&gt; depends on the shape of the input &lt;code&gt;x&lt;/code&gt; , and thus when we try another &lt;code&gt;x&lt;/code&gt; with a different shape, the trace differs.</target>
        </trans-unit>
        <trans-unit id="6b6071e9753ecc5843a463c244f78990925791d4" translate="yes" xml:space="preserve">
          <source>This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</source>
          <target state="translated">This method assumes that the file system supports locking using &lt;code&gt;fcntl&lt;/code&gt; - most local systems and NFS support it.</target>
        </trans-unit>
        <trans-unit id="56fec11901471926f8e8051da013976d59b168bf" translate="yes" xml:space="preserve">
          <source>This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</source>
          <target state="translated">This method can only be called on a coalesced sparse tensor. See &lt;code&gt;Tensor.coalesce()&lt;/code&gt; for details.</target>
        </trans-unit>
        <trans-unit id="ab188eb8949a2274bed5b2cb6c64d4adfb8be156" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</target>
        </trans-unit>
        <trans-unit id="9b6b6de7677cc3f29ad4b4bb71322ca8119f722c" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</source>
          <target state="translated">This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:</target>
        </trans-unit>
        <trans-unit id="07d469ff88f699ae509e6d44562a14e49142eb33" translate="yes" xml:space="preserve">
          <source>This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.ifft#torch.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; with differences only in formats of the input and output.</target>
        </trans-unit>
        <trans-unit id="f0df83d3e836abc896e3d3ef88cddff7fe41858c" translate="yes" xml:space="preserve">
          <source>This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt; with differences only in formats of the input and output.</source>
          <target state="translated">This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with &lt;a href=&quot;torch.fft#torch.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; with differences only in formats of the input and output.</target>
        </trans-unit>
        <trans-unit id="cdf0174c78d429d9d5f3575a0eab21bb4e7cb40a" translate="yes" xml:space="preserve">
          <source>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</source>
          <target state="translated">This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</target>
        </trans-unit>
        <trans-unit id="ca8069362a7e2b0181c1b2a91e97eec6d51986a6" translate="yes" xml:space="preserve">
          <source>This method is implemented using the Singular Value Decomposition.</source>
          <target state="translated">This method is implemented using the Singular Value Decomposition.</target>
        </trans-unit>
        <trans-unit id="a51d9e13859426129477705a920dac6fd0c48f90" translate="yes" xml:space="preserve">
          <source>This method modifies the module in-place.</source>
          <target state="translated">This method modifies the module in-place.</target>
        </trans-unit>
        <trans-unit id="44d70c89b433cfca3df93bd2dca81609164685a8" translate="yes" xml:space="preserve">
          <source>This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</source>
          <target state="translated">This method sets the parameters&amp;rsquo; &lt;code&gt;requires_grad&lt;/code&gt; attributes in-place.</target>
        </trans-unit>
        <trans-unit id="ba184c175718a9033afc302007ca57b3b2566fe2" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least &lt;code&gt;signal_ndim + 1&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; , this normalizes the result by dividing it with</target>
        </trans-unit>
        <trans-unit id="4d0394dd315af559db4d9ad47ffb52f54e5c2212" translate="yes" xml:space="preserve">
          <source>This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, this normalizes the result by dividing it with</source>
          <target state="translated">This method supports 1D, 2D and 3D real-to-complex transforms, indicated by &lt;code&gt;signal_ndim&lt;/code&gt; . &lt;code&gt;input&lt;/code&gt; must be a tensor with at least &lt;code&gt;signal_ndim&lt;/code&gt; dimensions with optionally arbitrary number of leading batch dimensions. If &lt;code&gt;normalized&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; , this normalizes the result by dividing it with</target>
        </trans-unit>
        <trans-unit id="75583911adf2f93140c081540537180003c155f0" translate="yes" xml:space="preserve">
          <source>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; is called.</target>
        </trans-unit>
        <trans-unit id="36d7188a05dab83ac9defa225d250cd47190ba2f" translate="yes" xml:space="preserve">
          <source>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</source>
          <target state="translated">This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</target>
        </trans-unit>
        <trans-unit id="57da4e0220bd8e9709ab47673f34882d1449aae9" translate="yes" xml:space="preserve">
          <source>This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</source>
          <target state="translated">This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.</target>
        </trans-unit>
        <trans-unit id="da76922486c33377aa2791af6603164e435bb6ab" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</source>
          <target state="translated">This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.</target>
        </trans-unit>
        <trans-unit id="d9d848a6942846852bedaf5aea8e5e5bd3995b73" translate="yes" xml:space="preserve">
          <source>This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</source>
          <target state="translated">This mode is used to export all operators as regular ONNX operators. This is the default &lt;code&gt;operator_export_type&lt;/code&gt; mode.</target>
        </trans-unit>
        <trans-unit id="15b7320d744a4575a9e6699705096be13d718c1b" translate="yes" xml:space="preserve">
          <source>This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt;. However, corresponding parameters in different processes must have the same strides.</source>
          <target state="translated">This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose &lt;code&gt;torch.memory_format&lt;/code&gt; is &lt;code&gt;torch.contiguous_format&lt;/code&gt; and others whose format is &lt;code&gt;torch.channels_last&lt;/code&gt; . However, corresponding parameters in different processes must have the same strides.</target>
        </trans-unit>
        <trans-unit id="db9962266fa6ecf5ecb4337d45f21434962d46f1" translate="yes" xml:space="preserve">
          <source>This module also contains any parameters that the original module had as well.</source>
          <target state="translated">This module also contains any parameters that the original module had as well.</target>
        </trans-unit>
        <trans-unit id="5790600450f4acb47de56be8f01d6569e2e765da" translate="yes" xml:space="preserve">
          <source>This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt;, the gradient reduction on these mixed types of parameters will just work fine.</source>
          <target state="translated">This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of &lt;code&gt;fp16&lt;/code&gt; and &lt;code&gt;fp32&lt;/code&gt; , the gradient reduction on these mixed types of parameters will just work fine.</target>
        </trans-unit>
        <trans-unit id="aaad9c62985785aa5e132959be1bf299d5f15d45" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</source>
          <target state="translated">This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</target>
        </trans-unit>
        <trans-unit id="56c78b422ae4f13f13cfea0f3331ef6b5469addb" translate="yes" xml:space="preserve">
          <source>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</source>
          <target state="translated">This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient &lt;code&gt;allreduce&lt;/code&gt; following the reverse order of the registered parameters of the model. In other words, it is users&amp;rsquo; responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.</target>
        </trans-unit>
        <trans-unit id="a2fc61472d65b92ddda57c12782f0c49ee43ce07" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="51bd62541eadcc8d3cc881058d8a71b8bed71b7f" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="03c2d1d66e7773d2c26345cff96e3404e927c5ec" translate="yes" xml:space="preserve">
          <source>This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</source>
          <target state="translated">This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).</target>
        </trans-unit>
        <trans-unit id="8a37a41f99ccf18be289e494dc942de371c38958" translate="yes" xml:space="preserve">
          <source>This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">This module currently does not support custom distributed collective operations in the forward pass, such as &lt;code&gt;SyncBatchNorm&lt;/code&gt; or other custom defined collectives in the model&amp;rsquo;s forward pass.</target>
        </trans-unit>
        <trans-unit id="e4e13a3367c83e826e07b89f6231cff61b5213ab" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</source>
          <target state="translated">This module doesn&amp;rsquo;t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use &lt;code&gt;LogSoftmax&lt;/code&gt; instead (it&amp;rsquo;s faster and has better numerical properties).</target>
        </trans-unit>
        <trans-unit id="b724d7ccdd2d4a2fad1d30f042a783b90f3e1b70" translate="yes" xml:space="preserve">
          <source>This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</source>
          <target state="translated">This module doesn&amp;rsquo;t work with &lt;a href=&quot;../autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt; (i.e. it will only work if gradients are to be accumulated in &lt;code&gt;.grad&lt;/code&gt; attributes of parameters).</target>
        </trans-unit>
        <trans-unit id="b31e8020fcc76734bdb0706e9d94b6fab41f09de" translate="yes" xml:space="preserve">
          <source>This module implements the combined (fused) modules conv + relu which can be then quantized.</source>
          <target state="translated">This module implements the combined (fused) modules conv + relu which can be then quantized.</target>
        </trans-unit>
        <trans-unit id="a5f06261e4042915626c1433e6142f8f0b72218a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized implementations of fused operations like conv + relu.</source>
          <target state="translated">This module implements the quantized implementations of fused operations like conv + relu.</target>
        </trans-unit>
        <trans-unit id="fe84855e7e341d93fbb671f10f878fe740d68b6a" translate="yes" xml:space="preserve">
          <source>This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt;.</source>
          <target state="translated">This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` and &lt;code&gt;torch.nn.ReLU&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1cc16ec4495bf63da75c9357c640897793fcbd84" translate="yes" xml:space="preserve">
          <source>This module implements the versions of those fused operations needed for quantization aware training.</source>
          <target state="translated">This module implements the versions of those fused operations needed for quantization aware training.</target>
        </trans-unit>
        <trans-unit id="cd564c5f343a0b5d3e2281d9491b97ab89a390d6" translate="yes" xml:space="preserve">
          <source>This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</source>
          <target state="translated">This module implements versions of the key nn modules &lt;strong&gt;Conv2d()&lt;/strong&gt; and &lt;strong&gt;Linear()&lt;/strong&gt; which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</target>
        </trans-unit>
        <trans-unit id="2d28957208d801ad4a8275f10517753872c57b97" translate="yes" xml:space="preserve">
          <source>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</source>
          <target state="translated">This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</target>
        </trans-unit>
        <trans-unit id="a56a23f8f5587064fad32f9351698184c9934864" translate="yes" xml:space="preserve">
          <source>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</source>
          <target state="translated">이 모듈은 종종 단어 임베딩을 저장하고 색인을 사용하여 검색하는 데 사용됩니다. 모듈에 대한 입력은 인덱스 목록이고 출력은 해당 단어 임베딩입니다.</target>
        </trans-unit>
        <trans-unit id="8c517186ab821b45b2c9090ee2862c5050c7fd67" translate="yes" xml:space="preserve">
          <source>This module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design&lt;/a&gt;.</source>
          <target state="translated">이 모듈은 모델 병렬 학습과 같은 애플리케이션에 사용할 수있는 RPC 기반 분산 autograd 프레임 워크를 제공합니다. 간단히 말해, 애플리케이션은 RPC를 통해 그래디언트 기록 텐서를 보내고받을 수 있습니다. 정방향 패스에서는 기울기 기록 텐서가 RPC를 통해 전송되는시기를 기록하고 역방향 패스 중에이 정보를 사용하여 RPC를 사용하여 분산 된 역방향 패스를 수행합니다. 자세한 내용은 &lt;a href=&quot;rpc/distributed_autograd#distributed-autograd-design&quot;&gt;Distributed Autograd Design을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="87af39490036eebaa3381d1990a65c31f6408fae" translate="yes" xml:space="preserve">
          <source>This module returns a &lt;code&gt;NamedTuple&lt;/code&gt; with &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;loss&lt;/code&gt; fields. See further documentation for details.</source>
          <target state="translated">이 모듈은 &lt;code&gt;output&lt;/code&gt; 및 &lt;code&gt;loss&lt;/code&gt; 필드 가있는 &lt;code&gt;NamedTuple&lt;/code&gt; 을 반환 합니다. 자세한 내용은 추가 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="59e9905faed8adfff4eeb0ecdbe57c1cac83f3a4" translate="yes" xml:space="preserve">
          <source>This module supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32를&lt;/a&gt; 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="c9c9e1a196eef3e1f967008237f4372d9edc34d7" translate="yes" xml:space="preserve">
          <source>This module works only with the multi-process, single-device usage of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, which means that a single process works on a single GPU.</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt; 의 다중 프로세스 단일 장치 사용에서만 작동합니다. 즉, 단일 프로세스가 단일 GPU에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="31368b5a919564a12c569127df416d6acb1973be" translate="yes" xml:space="preserve">
          <source>This op should be disambiguated with &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt;&lt;code&gt;torch.logsumexp()&lt;/code&gt;&lt;/a&gt; which performs a reduction on a single tensor.</source>
          <target state="translated">이 연산은 단일 텐서에서 감소를 수행하는 &lt;a href=&quot;torch.logsumexp#torch.logsumexp&quot;&gt; &lt;code&gt;torch.logsumexp()&lt;/code&gt; &lt;/a&gt; 로 명확해야합니다 .</target>
        </trans-unit>
        <trans-unit id="3256e874629b9e3bf7609aa4774907058a898427" translate="yes" xml:space="preserve">
          <source>This operation is not differentiable.</source>
          <target state="translated">이 작업은 구별 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="f253d72c955897e96cbe7e655ade3b1e375b9508" translate="yes" xml:space="preserve">
          <source>This operation is useful for explicit broadcasting by names (see examples).</source>
          <target state="translated">이 작업은 이름 별 명시 적 브로드 캐스팅에 유용합니다 (예제 참조).</target>
        </trans-unit>
        <trans-unit id="be6c030e3433204ea07b32a8e6ce714fff330e3a" translate="yes" xml:space="preserve">
          <source>This operator supports &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32&lt;/a&gt;.</source>
          <target state="translated">이 연산자는 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat32를&lt;/a&gt; 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="836145aa5caebff4eaffcf9dc94e2f473ff1df15" translate="yes" xml:space="preserve">
          <source>This package provides a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects. Currently, the &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; type is primarily used by the &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;.</source>
          <target state="translated">이 패키지는 비동기 실행을 캡슐화하는 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 유형과 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 개체 에 대한 작업을 단순화하는 유틸리티 함수 집합을 제공 합니다. 현재 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 유형은 주로 &lt;a href=&quot;rpc#distributed-rpc-framework&quot;&gt;분산 RPC 프레임 워크&lt;/a&gt; 에서 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="510d5659f81fbb2c92add3848324ac3e769287d8" translate="yes" xml:space="preserve">
          <source>This requires &lt;code&gt;scipy&lt;/code&gt; to be installed</source>
          <target state="translated">&lt;code&gt;scipy&lt;/code&gt; 를 설치 해야합니다.</target>
        </trans-unit>
        <trans-unit id="7d1f42ea5265cdd17159cef860e2867af1a5fed2" translate="yes" xml:space="preserve">
          <source>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</source>
          <target state="translated">이 섹션에서는 PyTorch 1.2의 TorchScript 변경 사항에 대해 자세히 설명합니다. TorchScript를 처음 사용하는 경우이 섹션을 건너 뛸 수 있습니다. PyTorch 1.2를 사용하는 TorchScript API에는 두 가지 주요 변경 사항이 있습니다.</target>
        </trans-unit>
        <trans-unit id="792e85a6083fd7bff7900d359e6ad50e4d63f9a7" translate="yes" xml:space="preserve">
          <source>This subset is restricted:</source>
          <target state="translated">이 하위 집합은 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="f610c707a33d251ea07e96805fc742b6f94fb773" translate="yes" xml:space="preserve">
          <source>This will call &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt;&lt;/a&gt; on each worker containing parameters to be optimized, and will block until all workers return. The provided &lt;code&gt;context_id&lt;/code&gt; will be used to retrieve the corresponding &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;context&lt;/code&gt;&lt;/a&gt; that contains the gradients that should be applied to the parameters.</source>
          <target state="translated">이렇게하면 최적화 할 매개 변수가 포함 된 각 워커에서 &lt;a href=&quot;optim#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;torch.optim.Optimizer.step()&lt;/code&gt; &lt;/a&gt; 이 호출 되고 모든 워커가 반환 될 때까지 차단됩니다. 제공된 &lt;code&gt;context_id&lt;/code&gt; 는 매개 변수에 적용해야하는 그라디언트를 포함 하는 해당 &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;context&lt;/code&gt; &lt;/a&gt; 를 검색하는 데 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="c51f7b72279e26fd7cf66d9d61e7f7204bf3b26a" translate="yes" xml:space="preserve">
          <source>Threshold</source>
          <target state="translated">Threshold</target>
        </trans-unit>
        <trans-unit id="4c8543e85c70d9042806658de48a8eae05e8c630" translate="yes" xml:space="preserve">
          <source>Threshold is defined as:</source>
          <target state="translated">임계 값은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a2ed3cfe5a92d0e9667b7a33a4666b01b859c7a8" translate="yes" xml:space="preserve">
          <source>Thresholds each element of the input Tensor.</source>
          <target state="translated">입력 Tensor의 각 요소를 임계 값으로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="880f2f41d4783f36e126ec98db6135c252954795" translate="yes" xml:space="preserve">
          <source>To achieve this, developers need to touch the source code of PyTorch. Please follow the &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;instructions&lt;/a&gt; for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX operator list&lt;/a&gt;.</source>
          <target state="translated">이를 위해 개발자는 PyTorch의 소스 코드를 만져야합니다. 소스에서 PyTorch를 설치 하려면 &lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot;&gt;지침&lt;/a&gt; 을 따르십시오 . 원하는 연산자가 ONNX에서 표준화 된 경우 이러한 연산자를 내보내기위한 지원을 쉽게 추가 할 수 있습니다 (연산자에 대한 기호 기능 추가). 운영자가 표준화되었는지 여부를 확인하려면 &lt;a href=&quot;https://github.com/onnx/onnx/blob/master/docs/Operators.md&quot;&gt;ONNX 운영자 목록을&lt;/a&gt; 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="b5aa3a35df66cb08f05248ae2964bf5546e98afe" translate="yes" xml:space="preserve">
          <source>To align a tensor to a specific order, use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">텐서를 특정 순서로 정렬하려면 &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="42a1cb8b698578c9fe48bbb02d735034e28323f6" translate="yes" xml:space="preserve">
          <source>To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of &lt;code&gt;torch.Tensor.item()&lt;/code&gt;. Torch supports implicit cast of single-element tensors to numbers. E.g.:</source>
          <target state="translated">ONNX 모델의 일부로 고정 값 상수로 가변 스칼라 텐서를 내 보내지 않으려면 &lt;code&gt;torch.Tensor.item()&lt;/code&gt; 사용을 피하십시오 . Torch는 단일 요소 텐서의 암시 적 캐스트를 숫자로 지원합니다. 예 :</target>
        </trans-unit>
        <trans-unit id="e33cac23c601d0b5ef35b2617bd1e0b84d5ac6c7" translate="yes" xml:space="preserve">
          <source>To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; as well.</source>
          <target state="translated">모듈을 저장할 수 있으려면 네이티브 Python 함수를 호출하지 않아야합니다. 즉, 모든 하위 모듈도 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 의&lt;/a&gt; 하위 클래스 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="517547cd8d7cdf6793257301cea557fd580f4509" translate="yes" xml:space="preserve">
          <source>To change an existing tensor&amp;rsquo;s &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; and/or &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, consider using &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt; method on the tensor.</source>
          <target state="translated">기존 텐서의 변경 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 및 / 또는 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 을&lt;/a&gt; 사용하여 고려 &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 텐서에 방법.</target>
        </trans-unit>
        <trans-unit id="cccb194237ee7a6be8a8fca8580f1ea27dde7026" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; (and recursively compile anything it calls), add the &lt;a href=&quot;../jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator to the method. To opt out of compilation use &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;forward&lt;/code&gt; 이외의 메서드를 컴파일하려면 (그리고 호출하는 모든 것을 재귀 적으로 컴파일하려면) &lt;a href=&quot;../jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt; 데코레이터를 메서드에 추가합니다 . 컴파일을 거부하려면 &lt;a href=&quot;torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="13132861c718e28cce4e074280b9a030df11d21a" translate="yes" xml:space="preserve">
          <source>To compile a method other than &lt;code&gt;forward&lt;/code&gt; that is not called from &lt;code&gt;forward&lt;/code&gt;, add &lt;code&gt;@torch.jit.export&lt;/code&gt;.</source>
          <target state="translated">이외의 방법 컴파일하려면 &lt;code&gt;forward&lt;/code&gt; 에서 호출되지 않습니다 &lt;code&gt;forward&lt;/code&gt; 추가 &lt;code&gt;@torch.jit.export&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f8e9d3b1e59225987eeef2681c83b1bcdde57d0" translate="yes" xml:space="preserve">
          <source>To compile the sources, the default system compiler (&lt;code&gt;c++&lt;/code&gt;) is used, which can be overridden by setting the &lt;code&gt;CXX&lt;/code&gt; environment variable. To pass additional arguments to the compilation process, &lt;code&gt;extra_cflags&lt;/code&gt; or &lt;code&gt;extra_ldflags&lt;/code&gt; can be provided. For example, to compile your extension with optimizations, pass &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt;. You can also use &lt;code&gt;extra_cflags&lt;/code&gt; to pass further include directories.</source>
          <target state="translated">소스를 컴파일하려면 &lt;code&gt;CXX&lt;/code&gt; 환경 변수를 설정하여 재정의 할 수있는 기본 시스템 컴파일러 ( &lt;code&gt;c++&lt;/code&gt; )가 사용됩니다 . 컴파일 프로세스에 추가 인수를 전달하기 위해 &lt;code&gt;extra_cflags&lt;/code&gt; 또는 &lt;code&gt;extra_ldflags&lt;/code&gt; 를 제공 할 수 있습니다. 예를 들어 최적화를 사용하여 확장을 컴파일하려면 &lt;code&gt;extra_cflags=['-O3']&lt;/code&gt; 를 전달하십시오 . &lt;code&gt;extra_cflags&lt;/code&gt; 를 사용 하여 추가 포함 디렉토리를 전달할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e66cd2fec20ece4ce742fe906879dd31b4bc3e7a" translate="yes" xml:space="preserve">
          <source>To compute log-probabilities for all classes, the &lt;code&gt;log_prob&lt;/code&gt; method can be used.</source>
          <target state="translated">모든 클래스에 대한 로그 확률을 계산하려면 &lt;code&gt;log_prob&lt;/code&gt; 메서드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bcee88acc8b939fa9c28961779b7b64da43214ae" translate="yes" xml:space="preserve">
          <source>To create a tensor with pre-existing data, use &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">기존 데이터로 텐서를 만들려면 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; 사용 하세요 .</target>
        </trans-unit>
        <trans-unit id="427cce7803261872f64a384400396e0d1af0bda9" translate="yes" xml:space="preserve">
          <source>To create a tensor with similar type but different size as another tensor, use &lt;code&gt;tensor.new_*&lt;/code&gt; creation ops.</source>
          <target state="translated">유형은 비슷하지만 다른 텐서와 크기가 다른 텐서를 만들려면 &lt;code&gt;tensor.new_*&lt;/code&gt; 생성 작업을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="ed5a59600ac4cfd9c95f09892fb740465f6d0959" translate="yes" xml:space="preserve">
          <source>To create a tensor with specific size, use &lt;code&gt;torch.*&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">특정 크기, 용도와 텐서 만들 &lt;code&gt;torch.*&lt;/code&gt; 텐서 생성 OPS (참조 &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;작성 작전을&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="a84b4146d0aefbfce9733ab7943da8a8972fc9bd" translate="yes" xml:space="preserve">
          <source>To create a tensor with the same size (and similar types) as another tensor, use &lt;code&gt;torch.*_like&lt;/code&gt; tensor creation ops (see &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt;).</source>
          <target state="translated">다른 텐서와 동일한 크기 (및 유사한 유형)의 텐서를 생성 하려면 &lt;code&gt;torch.*_like&lt;/code&gt; 텐서 생성 작업을 사용합니다 ( &lt;a href=&quot;torch#tensor-creation-ops&quot;&gt;Creation Ops&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="e323c9fb04b24bbe21e136c6130c8b1036dfb2e1" translate="yes" xml:space="preserve">
          <source>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (&amp;ldquo;CPU total time is much greater than CUDA total time&amp;rdquo;). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.</source>
          <target state="translated">살펴볼 (CPU 전용 모드 또는 CUDA 모드) 자동 등급 프로파일 러 출력을 결정하려면 먼저 스크립트가 CPU 바운드인지 ( &quot;CPU 총 시간이 CUDA 총 시간보다 훨씬 큽니다&quot;) 확인해야합니다. CPU 바운드 인 경우 CPU 모드 autograd 프로파일 러의 결과를 보면 도움이됩니다. 반면에 스크립트가 GPU에서 실행하는 데 대부분의 시간을 소비하는 경우 CUDA 모드 autograd 프로파일 러의 출력에서 ​​책임있는 CUDA 연산자를 찾기 시작하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1d6487001340d96dfbd24e0dfe67d91d448bd378" translate="yes" xml:space="preserve">
          <source>To enable &lt;code&gt;backend == Backend.MPI&lt;/code&gt;, PyTorch needs to be built from source on a system that supports MPI.</source>
          <target state="translated">&lt;code&gt;backend == Backend.MPI&lt;/code&gt; 를 활성화하려면 MPI를 지원하는 시스템의 소스에서 PyTorch를 빌드해야합니다.</target>
        </trans-unit>
        <trans-unit id="928a8a1dba70cf38d7b64d41e9f2de79497ca1b2" translate="yes" xml:space="preserve">
          <source>To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a &lt;code&gt;Future&lt;/code&gt; object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with &lt;code&gt;@staticmethod&lt;/code&gt; or &lt;code&gt;@classmethod&lt;/code&gt;, &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt;.</source>
          <target state="translated">비동기 실행을 사용하려면 애플리케이션이이 데코레이터가 반환 한 함수 객체를 RPC API에 전달해야합니다. RPC가이 데코레이터에 의해 설치된 속성을 감지하면이 함수가 &lt;code&gt;Future&lt;/code&gt; 객체를 반환하고 그에 따라 처리 할 것임을 알고 있습니다. 그러나 이것이 함수를 정의 할 때이 데코레이터가 가장 바깥쪽에 있어야한다는 것을 의미하지는 않습니다. 예를 들면, 함께 결합 될 때 &lt;code&gt;@staticmethod&lt;/code&gt; 또는 &lt;code&gt;@classmethod&lt;/code&gt; , &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; 필요한 목표 함수는 정적 또는 클래스 함수로서 인식 될 수 있도록 내부 장식한다. 이 대상 함수는 액세스 할 때 정적 또는 클래스 메서드가 &lt;code&gt;@rpc.functions.async_execution&lt;/code&gt; 의해 설치된 속성을 유지하기 때문에 여전히 비동기 적으로 실행할 수 있습니다..</target>
        </trans-unit>
        <trans-unit id="16234789078f98ba40aad60005ffa95f2e9e8ac6" translate="yes" xml:space="preserve">
          <source>To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.</source>
          <target state="translated">올바른 수의 스레드가 사용되는지 확인하려면 eager, JIT 또는 autograd 코드를 실행하기 전에 set_num_threads를 호출해야합니다.</target>
        </trans-unit>
        <trans-unit id="2cd4366390e742467d09cd376578959df8585e90" translate="yes" xml:space="preserve">
          <source>To export a raw ir.</source>
          <target state="translated">원시 IR을 내보내려면.</target>
        </trans-unit>
        <trans-unit id="13a45cf8b5a671e88848c78052e09b8b9f6c109f" translate="yes" xml:space="preserve">
          <source>To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.</source>
          <target state="translated">ONNX에서 지원되지 않는 ATen 연산자를 대체합니다. 지원되는 연산자는 정기적으로 ONNX로 내보내집니다. 다음 예에서 aten :: triu는 ONNX에서 지원되지 않습니다. 내보내기는이 연산자를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="64ba89c98e2ef72ba75fa028267bbaa3251bdd0e" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a complex data type, the property &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt;&lt;code&gt;is_complex&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a complex data type.</source>
          <target state="translated">&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 이 복합 데이터 유형 인지 확인하기 위해 is_complex 속성을 &lt;a href=&quot;generated/torch.is_complex#torch.is_complex&quot;&gt; &lt;code&gt;is_complex&lt;/code&gt; &lt;/a&gt; 수 있습니다. 이 속성 은 데이터 유형이 복합 데이터 유형 인 경우 &lt;code&gt;True&lt;/code&gt; 를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="4bde254844ec10d73414a4f9097cbf8e8d5d1c09" translate="yes" xml:space="preserve">
          <source>To find out if a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is a floating point data type, the property &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt;&lt;code&gt;is_floating_point&lt;/code&gt;&lt;/a&gt; can be used, which returns &lt;code&gt;True&lt;/code&gt; if the data type is a floating point data type.</source>
          <target state="translated">있는지 확인하려면 &lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 은&lt;/a&gt; 부동 소수점 데이터 유형, 속성 &lt;a href=&quot;generated/torch.is_floating_point#torch.is_floating_point&quot;&gt; &lt;code&gt;is_floating_point&lt;/code&gt; 은&lt;/a&gt; 반환, 사용할 수있는 &lt;code&gt;True&lt;/code&gt; 데이터 유형이 부동 소수점 데이터 유형 인 경우.</target>
        </trans-unit>
        <trans-unit id="48e44e1c5bae8e339aa29cc00850c62cff2a0ea7" translate="yes" xml:space="preserve">
          <source>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It&amp;rsquo;s also helpful to include a minimal working example.</source>
          <target state="translated">사용자가 문서를 앞뒤로 참조하지 않고 탐색 할 수 있도록 repo 소유자가 기능 도움말 메시지를 명확하고 간결하게 만드는 것이 좋습니다. 최소한의 작업 예제를 포함하는 것도 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="6e370e4d333aa52e5158136385c8ce683abfc7e1" translate="yes" xml:space="preserve">
          <source>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</source>
          <target state="translated">확장을로드하기 위해 주어진 소스를 동적 라이브러리로 컴파일하는 데 사용되는 Ninja 빌드 파일이 생성됩니다. 이 라이브러리는 이후에 현재 Python 프로세스에 모듈로로드되고 사용할 준비가 된이 함수에서 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="0f1500bbb514e4eeb3d360549b7085361d0903f1" translate="yes" xml:space="preserve">
          <source>To look up what optional arguments this module offers:</source>
          <target state="translated">이 모듈이 제공하는 선택적 인수를 찾으려면 다음을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="efe5a1ff1c27c51217ed9e88d6d14c167e44ae98" translate="yes" xml:space="preserve">
          <source>To make it easier to understand, here is a small example:</source>
          <target state="translated">이해하기 쉽도록 다음은 작은 예입니다.</target>
        </trans-unit>
        <trans-unit id="4861c71c5547ba6da8095cee191825540e015436" translate="yes" xml:space="preserve">
          <source>To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to &lt;code&gt;torch&lt;/code&gt;, the TorchScript compiler is actually resolving it to the &lt;code&gt;torch&lt;/code&gt; Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are de-sugared at compile-time into the primitive types that TorchScript supports. This depends on the dynamic type of the Python valued referenced when compilation occurs. This section describes the rules that are used when accessing Python values in TorchScript.</source>
          <target state="translated">TorchScript 작성을보다 편리하게하기 위해 스크립트 코드가 주변 범위의 Python 값을 참조하도록 허용합니다. 예를 들어, &lt;code&gt;torch&lt;/code&gt; 에 대한 참조가있을 때마다 TorchScript 컴파일러는 함수가 선언 될 때 실제로이를 &lt;code&gt;torch&lt;/code&gt; Python 모듈로 해석합니다 . 이러한 Python 값은 TorchScript의 첫 번째 클래스 부분이 아닙니다. 대신 컴파일 타임에 TorchScript가 지원하는 기본 유형으로 de-sugared됩니다. 이것은 컴파일이 발생할 때 참조되는 Python 값의 동적 유형에 따라 다릅니다. 이 섹션에서는 TorchScript에서 Python 값에 액세스 할 때 사용되는 규칙에 대해 설명합니다.</target>
        </trans-unit>
        <trans-unit id="56b4b291b8193f650ab440b4d0108041657026e3" translate="yes" xml:space="preserve">
          <source>To obtain repeatable results, reset the seed for the pseudorandom number generator</source>
          <target state="translated">반복 가능한 결과를 얻으려면 의사 난수 생성기의 시드를 재설정하십시오.</target>
        </trans-unit>
        <trans-unit id="cdaaaf6ab8e173c9d7ceb41b8dea6cb46e09c870" translate="yes" xml:space="preserve">
          <source>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</source>
          <target state="translated">사용자 정의 된 추가 정보를 인쇄하려면 자체 모듈에서이 방법을 다시 구현해야합니다. 한 줄 및 여러 줄 문자열이 모두 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="ff43c0b33f5574b95fded09d54e409788745cf85" translate="yes" xml:space="preserve">
          <source>To run the exported script with &lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2&lt;/a&gt;, you will need to install &lt;code&gt;caffe2&lt;/code&gt;: If you don&amp;rsquo;t have one already, Please &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;follow the install instructions&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://caffe2.ai/&quot;&gt;caffe2로&lt;/a&gt; 내 보낸 스크립트를 실행 하려면 &lt;code&gt;caffe2&lt;/code&gt; 를 설치해야합니다. 아직 설치 하지 않은 경우 &lt;a href=&quot;https://caffe2.ai/docs/getting-started.html&quot;&gt;설치 지침을 따르십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c62b3cc0b531750df7ee34e20beaf808665f78f8" translate="yes" xml:space="preserve">
          <source>To specify the scale, it takes either the &lt;code&gt;size&lt;/code&gt; or the &lt;code&gt;scale_factor&lt;/code&gt; as it&amp;rsquo;s constructor argument.</source>
          <target state="translated">스케일을 지정하려면 생성자 인수로 &lt;code&gt;size&lt;/code&gt; 또는 &lt;code&gt;scale_factor&lt;/code&gt; 를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="8bc8fb9e3000235214e485d36fb9186012476b04" translate="yes" xml:space="preserve">
          <source>To stop the compiler from compiling a method, add &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;@torch.jit.unused&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;@ignore&lt;/code&gt; leaves the</source>
          <target state="translated">컴파일러가 메소드 컴파일을 중지하려면 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;@torch.jit.unused&lt;/code&gt; &lt;/a&gt; 추가하십시오 . &lt;code&gt;@ignore&lt;/code&gt; 잎은</target>
        </trans-unit>
        <trans-unit id="83e075942db53d26dad4b0cea801b6df02d892d5" translate="yes" xml:space="preserve">
          <source>To take a batch diagonal, pass in dim1=-2, dim2=-1.</source>
          <target state="translated">배치 대각선을 취하려면 dim1 = -2, dim2 = -1을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="06d7e723aad8726de16a32b689408fd5f6a39bff" translate="yes" xml:space="preserve">
          <source>To trace a specific method on a module, see &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt;&lt;code&gt;torch.jit.trace_module&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">모듈에서 특정 메소드를 추적하려면 &lt;a href=&quot;generated/torch.jit.trace_module#torch.jit.trace_module&quot;&gt; &lt;code&gt;torch.jit.trace_module&lt;/code&gt; 을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7cbbf69a127d5627b468752b233e45c932bef649" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;DistributedDataParallel&lt;/code&gt; on a host with N GPUs, you should spawn up &lt;code&gt;N&lt;/code&gt; processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; for every process or by calling:</source>
          <target state="translated">N 개의 GPU가있는 호스트에서 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 사용하려면 &lt;code&gt;N&lt;/code&gt; 개의 프로세스를 생성하여 각 프로세스가 0에서 N-1까지 단일 GPU에서 독점적으로 작동하도록해야합니다. 이는 모든 프로세스에 대해 &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; 를 설정 하거나 다음을 호출 하여 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="55e1a9559362bb4b36c706d26afd5c7744bfa4b4" translate="yes" xml:space="preserve">
          <source>To use a &lt;code&gt;nn.ModuleList&lt;/code&gt; inside a compiled method, it must be marked constant by adding the name of the attribute to the &lt;code&gt;__constants__&lt;/code&gt; list for the type. For loops over a &lt;code&gt;nn.ModuleList&lt;/code&gt; will unroll the body of the loop at compile time, with each member of the constant module list.</source>
          <target state="translated">컴파일 된 메서드 내 에서 &lt;code&gt;nn.ModuleList&lt;/code&gt; 를 사용하려면 해당 유형 의 &lt;code&gt;__constants__&lt;/code&gt; 목록에 속성 이름을 추가하여 상수로 표시해야합니다 . &lt;code&gt;nn.ModuleList&lt;/code&gt; 에 대한 For 루프 는 컴파일 타임에 상수 모듈 목록의 각 멤버와 함께 루프 본문을 펼 칩니다 .</target>
        </trans-unit>
        <trans-unit id="527375a7ef1d0e262542dedd5c0253dc05250f0d" translate="yes" xml:space="preserve">
          <source>To use these functions the torch.fft module must be imported since its name conflicts with the &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">이러한 함수를 사용하려면 이름이 &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt; 함수 와 충돌하므로 torch.fft 모듈을 가져와야 합니다.</target>
        </trans-unit>
        <trans-unit id="43427ba5c0a00d9ec53c4149e3f5de362381fec9" translate="yes" xml:space="preserve">
          <source>To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.</source>
          <target state="translated">이를 사용하여 프로세스 전반에 걸쳐 입력이 고르지 않은 학습을 가능하게하려면이 컨텍스트 관리자를 학습 루프에 감싸면됩니다. 모델이나 데이터로드에 대한 추가 수정이 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6239832323d9bb66e30c43f099010c4cf674fbce" translate="yes" xml:space="preserve">
          <source>To utilize &lt;em&gt;script-based&lt;/em&gt; exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:</source>
          <target state="translated">동적 루프를 캡처 하기 위해 &lt;em&gt;스크립트 기반&lt;/em&gt; 내보내기 를 사용하려면 &lt;em&gt;스크립트&lt;/em&gt; 에 루프를 작성하고 일반 nn.Module에서 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="111ccaae56ca75ec4b6793420aa1331efeccd7da" translate="yes" xml:space="preserve">
          <source>Top-1 error</source>
          <target state="translated">Top-1 오류</target>
        </trans-unit>
        <trans-unit id="f1240994c38def01302ee353c1f3e6ab783c6922" translate="yes" xml:space="preserve">
          <source>Top-5 error</source>
          <target state="translated">Top-5 오류</target>
        </trans-unit>
        <trans-unit id="ecebe7e89f3537ec9d48a23fefb140aef1ac7e73" translate="yes" xml:space="preserve">
          <source>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</source>
          <target state="translated">Torch는 다음과 같은 CPU 및 GPU 변형으로 10 개의 텐서 유형을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="2270c240add89c3f53429a527826adf7f8c68848" translate="yes" xml:space="preserve">
          <source>Torch hub works by importing the package as if it was installed. There&amp;rsquo;re some side effects introduced by importing in Python. For example, you can see new items in Python caches &lt;code&gt;sys.modules&lt;/code&gt; and &lt;code&gt;sys.path_importer_cache&lt;/code&gt; which is normal Python behavior.</source>
          <target state="translated">토치 허브는 설치된 것처럼 패키지를 가져 와서 작동합니다. Python으로 가져 오면 몇 가지 부작용이 발생합니다. 예를 들어, Python에서 정상적인 Python 동작 인 &lt;code&gt;sys.modules&lt;/code&gt; 및 &lt;code&gt;sys.path_importer_cache&lt;/code&gt; 를 캐시하는 새 항목을 볼 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e678d1ba0a8c71917974dd6b809d475b07d566ed" translate="yes" xml:space="preserve">
          <source>Torch mobile supports &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blacklisting optimization set and a preserved method list</source>
          <target state="translated">Torch 모바일은 평가 모드에서 모듈로 최적화 패스 목록을 실행하기 위해 &lt;code&gt;torch.mobile_optimizer.optimize_for_mobile&lt;/code&gt; 유틸리티를 지원 합니다. 이 메서드는 torch.jit.ScriptModule 객체, 블랙리스트 최적화 세트 및 보존 된 메서드 목록과 같은 매개 변수를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="03340697e5da4f35bf9335b934c01e6bb269d07d" translate="yes" xml:space="preserve">
          <source>Torch supports sparse tensors in COO(rdinate) format, which can efficiently store and process tensors for which the majority of elements are zeros.</source>
          <target state="translated">Torch는 COO (rdinate) 형식의 희소 텐서를 지원하므로 대부분의 요소가 0 인 텐서를 효율적으로 저장하고 처리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="efa8e2bf58145beaa70cc1139b800b0cc73441ae" translate="yes" xml:space="preserve">
          <source>TorchElastic</source>
          <target state="translated">TorchElastic</target>
        </trans-unit>
        <trans-unit id="3b90040dd3c7ea550e3ae7ebb31cbbf38c50d775" translate="yes" xml:space="preserve">
          <source>TorchScript</source>
          <target state="translated">TorchScript</target>
        </trans-unit>
        <trans-unit id="604c8e5563e16356b9e1a864727a3495aaa4b8f6" translate="yes" xml:space="preserve">
          <source>TorchScript Classes</source>
          <target state="translated">TorchScript 클래스</target>
        </trans-unit>
        <trans-unit id="5d73d728706f6b5bfb8bc6488f8b896532317b47" translate="yes" xml:space="preserve">
          <source>TorchScript Enums</source>
          <target state="translated">TorchScript 열거 형</target>
        </trans-unit>
        <trans-unit id="2d3d498afcd6dbfb2767a5e73bfdf3c641826801" translate="yes" xml:space="preserve">
          <source>TorchScript Language</source>
          <target state="translated">TorchScript 언어</target>
        </trans-unit>
        <trans-unit id="655dca0ad6a50c42b950785617112aa26c2b7ea8" translate="yes" xml:space="preserve">
          <source>TorchScript Language Reference</source>
          <target state="translated">TorchScript 언어 참조</target>
        </trans-unit>
        <trans-unit id="4ca55dbce27b090f25e1beef5f42bc0b74a3e196" translate="yes" xml:space="preserve">
          <source>TorchScript Unsupported Pytorch Constructs</source>
          <target state="translated">TorchScript 지원되지 않는 Pytorch 구성</target>
        </trans-unit>
        <trans-unit id="f37751da08a275fc84f6719af858bb6fca4efcaa" translate="yes" xml:space="preserve">
          <source>TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</source>
          <target state="translated">TorchScript는 또한 IR 그래프의 형태로 코드 프리티 프린터보다 낮은 수준의 표현을 가지고 있습니다.</target>
        </trans-unit>
        <trans-unit id="1f6a6dd855f78002eaa186408d37b5a14d38fb6e" translate="yes" xml:space="preserve">
          <source>TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.</source>
          <target state="translated">TorchScript는 또한 Python에 정의 된 상수를 사용하는 방법을 제공합니다. 이는 하이퍼 매개 변수를 함수에 하드 코딩하거나 범용 상수를 정의하는 데 사용할 수 있습니다. Python 값을 상수로 처리하도록 지정하는 방법에는 두 가지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="252b708f15ebb50ba55ac0b5f321607abb84f702" translate="yes" xml:space="preserve">
          <source>TorchScript can call Python functions. This functionality is very useful when incrementally converting a model to TorchScript. The model can be moved function-by-function to TorchScript, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go.</source>
          <target state="translated">TorchScript는 Python 함수를 호출 할 수 있습니다. 이 기능은 모델을 TorchScript로 점진적으로 변환 할 때 매우 유용합니다. 모델은 기능별로 TorchScript로 이동하여 Python 함수에 대한 호출을 그대로 둘 수 있습니다. 이렇게하면 진행하면서 모델의 정확성을 점진적으로 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e634a60c8e32dbe73734d8f74e5fc62a8bbd892" translate="yes" xml:space="preserve">
          <source>TorchScript can lookup attributes on modules. &lt;code&gt;Builtin functions&lt;/code&gt; like &lt;code&gt;torch.add&lt;/code&gt; are accessed this way. This allows TorchScript to call functions defined in other modules.</source>
          <target state="translated">TorchScript는 모듈의 속성을 조회 할 수 있습니다. &lt;code&gt;torch.add&lt;/code&gt; 와 같은 &lt;code&gt;Builtin functions&lt;/code&gt; 는 이런 방식으로 액세스됩니다. 이를 통해 TorchScript는 다른 모듈에 정의 된 함수를 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="05a14206c4e373e5e12c57e024877cf642631ecf" translate="yes" xml:space="preserve">
          <source>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a &lt;code&gt;NamedTuple&lt;/code&gt; with methods attached).</source>
          <target state="translated">TorchScript 클래스 지원은 실험적입니다. 현재는 간단한 레코드와 유사한 유형에 가장 적합합니다 ( 메서드가 첨부 된 &lt;code&gt;NamedTuple&lt;/code&gt; 을 생각해 보십시오 ).</target>
        </trans-unit>
        <trans-unit id="40d12a53eedc122f7c1c26cc26562fe87abc6002" translate="yes" xml:space="preserve">
          <source>TorchScript classes are statically typed. Members can only be declared by assigning to self in the &lt;code&gt;__init__()&lt;/code&gt; method.</source>
          <target state="translated">TorchScript 클래스는 정적으로 형식화됩니다. 멤버는 &lt;code&gt;__init__()&lt;/code&gt; 메서드 에서 self에 할당해야만 선언 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6fd4fa394dbd8f52548eb83a0ab865caf3d46d44" translate="yes" xml:space="preserve">
          <source>TorchScript does not support &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt; so this type is not used</source>
          <target state="translated">TorchScript는 &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#bytes&quot;&gt; &lt;code&gt;bytes&lt;/code&gt; &lt;/a&gt; 지원하지 않으므로이 유형은 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="95d83676fd42ab283c45a040ec05adf9e17c4b2a" translate="yes" xml:space="preserve">
          <source>TorchScript does not support all features and types of the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module. Some of these are more fundamental things that are unlikely to be added in the future while others may be added if there is enough user demand to make it a priority.</source>
          <target state="translated">TorchScript는 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; 모듈 의 모든 기능과 유형을 지원하지 않습니다 . 이들 중 일부는 미래에 추가 될 가능성이 낮은보다 근본적인 것들이며, 우선 순위로 할 충분한 사용자 요구가있는 경우 다른 것들이 추가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="155bc1507dbbed5e44c922d48bd933f969683779" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python that can either be written directly (using the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; decorator) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code.</source>
          <target state="translated">TorchScript는 직접 작성 ( &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 데코레이터 사용)하거나 추적을 통해 Python 코드에서 자동으로 생성 할 수있는 정적으로 형식화 된 Python의 하위 집합입니다 . 추적을 사용할 때 코드는 텐서에 실제 연산자 만 기록하고 단순히 주변의 다른 Python 코드를 실행하고 삭제하여 Python의이 하위 집합으로 자동 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="6e3be4ae1ba105244947e696b16b923455648d05" translate="yes" xml:space="preserve">
          <source>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt; for details.</source>
          <target state="translated">TorchScript는 정적으로 형식화 된 Python의 하위 집합이므로 많은 Python 기능이 TorchScript에 직접 적용됩니다. 자세한 내용은 전체 &lt;a href=&quot;jit_language_reference#language-reference&quot;&gt;TorchScript 언어 참조&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b80f73ada7844bf2ed31f70d01150d3e0f521094" translate="yes" xml:space="preserve">
          <source>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</source>
          <target state="translated">TorchScript는 PyTorch 코드에서 직렬화 및 최적화 가능한 모델을 생성하는 방법입니다. 모든 TorchScript 프로그램은 Python 프로세스에서 저장하고 Python 종속성이없는 프로세스에서로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f3730c4db0b9ee76825d3365cc7e6ee5d81c09f3" translate="yes" xml:space="preserve">
          <source>TorchScript provides a code pretty-printer for all &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; instances. This pretty-printer gives an interpretation of the script method&amp;rsquo;s code as valid Python syntax. For example:</source>
          <target state="translated">TorchScript는 모든 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 인스턴스에 대한 코드 프리티 프린터를 제공 합니다. 이 예쁜 프린터는 스크립트 메서드의 코드를 유효한 Python 구문으로 해석합니다. 예를 들면 :</target>
        </trans-unit>
        <trans-unit id="6f800d70869c5af16f03f352efad0babdc4a4cee" translate="yes" xml:space="preserve">
          <source>TorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, &lt;code&gt;torch.distributed.rpc&lt;/code&gt; supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.</source>
          <target state="translated">RPC의 TorchScript 지원은 프로토 타입 기능이며 변경 될 수 있습니다. v1.5.0부터 &lt;code&gt;torch.distributed.rpc&lt;/code&gt; 는 TorchScript 함수를 RPC 대상 함수로 호출하는 것을 지원하며 TorchScript 함수를 실행하는 데 GIL이 필요하지 않으므로 호출 수신자 측에서 병렬 처리를 개선하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="7d483bee6192edebb2157f5cffc7ebe0cf7bf9b5" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of Python&amp;rsquo;s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statement.</source>
          <target state="translated">TorchScript는 Python의 변수 확인 (예 : 범위 지정) 규칙의 하위 집합을 지원합니다. 지역 변수는 변수가 함수를 통한 모든 경로에서 동일한 유형을 가져야한다는 제한을 제외하고 Python에서와 동일하게 작동합니다. 변수가 if 문의 다른 분기에서 다른 유형을 갖는 경우 if 문의 종료 후 변수를 사용하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="439e6a570c3e92811a193d2bb2eb0c532aac85c2" translate="yes" xml:space="preserve">
          <source>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the &lt;code&gt;torch&lt;/code&gt; namespace, all functions in &lt;code&gt;torch.nn.functional&lt;/code&gt; and most modules from &lt;code&gt;torch.nn&lt;/code&gt; are supported in TorchScript.</source>
          <target state="translated">TorchScript는 PyTorch가 제공하는 텐서 및 신경망 기능의 하위 집합을 지원합니다. Tensor의 대부분의 메서드와 &lt;code&gt;torch&lt;/code&gt; 네임 스페이스의 함수, &lt;code&gt;torch.nn.functional&lt;/code&gt; 의 모든 함수 및 torch.nn의 대부분의 모듈이 &lt;code&gt;torch.nn&lt;/code&gt; 에서 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="85815975862ed9b632ab9ab2cb6516181e624194" translate="yes" xml:space="preserve">
          <source>TorchScript supports the following types of statements:</source>
          <target state="translated">TorchScript는 다음 유형의 문을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="237a371f4a7260a3005de6488e3d8470546b5ec9" translate="yes" xml:space="preserve">
          <source>TorchScript supports the use of most PyTorch functions and many Python built-ins. See &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; for a full reference of supported functions.</source>
          <target state="translated">TorchScript는 대부분의 PyTorch 함수와 많은 Python 내장 기능의 사용을 지원합니다. 지원되는 함수에 대한 전체 참조는 &lt;a href=&quot;jit_builtin_functions#builtin-functions&quot;&gt;TorchScript Builtins&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="f626090730ab6e08eee60582294fe0a9f90efe3d" translate="yes" xml:space="preserve">
          <source>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</source>
          <target state="translated">TorchScript는 계산을 나타 내기 위해 정적 단일 할당 (SSA) 중간 표현 (IR)을 사용합니다. 이 형식의 명령어는 ATen (PyTorch의 C ++ 백엔드) 연산자와 루프 및 조건에 대한 제어 흐름 연산자를 포함한 기타 기본 연산자로 구성됩니다. 예로서:</target>
        </trans-unit>
        <trans-unit id="ee07cfab05a06e606f9327817d635689c5cd9b5d" translate="yes" xml:space="preserve">
          <source>TorchScript will refine the type of a variable of type &lt;code&gt;Optional[T]&lt;/code&gt; when a comparison to &lt;code&gt;None&lt;/code&gt; is made inside the conditional of an if-statement or checked in an &lt;code&gt;assert&lt;/code&gt;. The compiler can reason about multiple &lt;code&gt;None&lt;/code&gt; checks that are combined with &lt;code&gt;and&lt;/code&gt;, &lt;code&gt;or&lt;/code&gt;, and &lt;code&gt;not&lt;/code&gt;. Refinement will also occur for else blocks of if-statements that are not explicitly written.</source>
          <target state="translated">TorchScript는 if- 문의 조건부 내에서 &lt;code&gt;None&lt;/code&gt; 에 대한 비교 가 이루어 지거나 &lt;code&gt;assert&lt;/code&gt; 에서 체크인 될 때 &lt;code&gt;Optional[T]&lt;/code&gt; 유형의 변수 유형을 구체화합니다 . 컴파일러는 &lt;code&gt;and&lt;/code&gt; , &lt;code&gt;or&lt;/code&gt; , &lt;code&gt;not&lt;/code&gt; 과 결합 된 여러 &lt;code&gt;None&lt;/code&gt; 검사 에 대해 추론 할 수 있습니다 . 명시 적으로 작성되지 않은 if 문의 else 블록에 대해서도 구체화가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="db3578de30ac281698832393e03259941ecc59a4" translate="yes" xml:space="preserve">
          <source>TorchServe</source>
          <target state="translated">TorchServe</target>
        </trans-unit>
        <trans-unit id="174786ece7d01b50c5a292bbe7c461a354fd217b" translate="yes" xml:space="preserve">
          <source>TorchVision support</source>
          <target state="translated">TorchVision 지원</target>
        </trans-unit>
        <trans-unit id="eb01e02a87150ef514202bea3337f1bbd7e6a429" translate="yes" xml:space="preserve">
          <source>Total norm of the parameters (viewed as a single vector).</source>
          <target state="translated">매개 변수의 총 노름 (단일 벡터로 표시됨).</target>
        </trans-unit>
        <trans-unit id="117b4c950645374477e1f2b2a96517111745fe19" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">함수를 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 파일 또는 &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="4cae3f60263770c81b066cebd1a77d6ae0c2a925" translate="yes" xml:space="preserve">
          <source>Trace a function and return an executable or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on &lt;code&gt;Tensor&lt;/code&gt;s and lists, dictionaries, and tuples of &lt;code&gt;Tensor&lt;/code&gt;s.</source>
          <target state="translated">함수를 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 파일 또는 &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 을 반환합니다 . 추적은 &lt;code&gt;Tensor&lt;/code&gt; 및 목록, 사전 및 &lt;code&gt;Tensor&lt;/code&gt; 튜플 에서만 작동하는 코드에 이상적입니다 .</target>
        </trans-unit>
        <trans-unit id="20f26cd87e3024b30b8eeb8b74edbe9cdd81f4b6" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation.</source>
          <target state="translated">모듈을 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 가능한 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="3bad49a27d3b91711b9d48661ccd1bb2133d17d3" translate="yes" xml:space="preserve">
          <source>Trace a module and return an executable &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that will be optimized using just-in-time compilation. When a module is passed to &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt;&lt;code&gt;torch.jit.trace&lt;/code&gt;&lt;/a&gt;, only the &lt;code&gt;forward&lt;/code&gt; method is run and traced. With &lt;code&gt;trace_module&lt;/code&gt;, you can specify a dictionary of method names to example inputs to trace (see the &lt;code&gt;inputs&lt;/code&gt;) argument below.</source>
          <target state="translated">모듈을 추적하고 Just-In-Time 컴파일을 사용하여 최적화 될 실행 가능한 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 을 반환합니다 . 모듈이 &lt;a href=&quot;torch.jit.trace#torch.jit.trace&quot;&gt; &lt;code&gt;torch.jit.trace&lt;/code&gt; 에&lt;/a&gt; 전달 되면 &lt;code&gt;forward&lt;/code&gt; 메소드 만 실행되고 추적됩니다. &lt;code&gt;trace_module&lt;/code&gt; 을 사용하면 아래의 인수 를 추적 할 ( &lt;code&gt;inputs&lt;/code&gt; 참조) 예제 입력에 메서드 이름 사전을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d93ef3f27cd83786d560cf0e7159be09c5658df2" translate="yes" xml:space="preserve">
          <source>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</source>
          <target state="translated">추적 된 함수는 스크립트 함수를 호출 할 수 있습니다. 이는 대부분의 모델이 피드 포워드 네트워크 일지라도 모델의 작은 부분에 약간의 제어 흐름이 필요할 때 유용합니다. 추적 된 함수에 의해 호출 된 스크립트 함수 내부의 제어 흐름이 올바르게 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="0fce73bbb5357ac93c61680e5f58be84db277c7b" translate="yes" xml:space="preserve">
          <source>Tracer</source>
          <target state="translated">Tracer</target>
        </trans-unit>
        <trans-unit id="d09bb53da249750b0b3b27f27eabe9dbddadc0ed" translate="yes" xml:space="preserve">
          <source>Tracer Warnings</source>
          <target state="translated">추적자 경고</target>
        </trans-unit>
        <trans-unit id="bf933c3083c5a6cb4cedac1b1e0c22d6553dab1d" translate="yes" xml:space="preserve">
          <source>Tracing Edge Cases</source>
          <target state="translated">가장자리 케이스 추적</target>
        </trans-unit>
        <trans-unit id="c5153b07a6f011eb8b3585b8b94ddfea62a7a0ce" translate="yes" xml:space="preserve">
          <source>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</source>
          <target state="translated">입력에 의존하는 제어 흐름 추적 (예 : 텐서 모양)</target>
        </trans-unit>
        <trans-unit id="656eeb39a9810745d15b70b8f4cca3cf416cf8d6" translate="yes" xml:space="preserve">
          <source>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</source>
          <target state="translated">텐서 뷰의 내부 작업 추적 (예 : 할당의 왼쪽에 인덱싱)</target>
        </trans-unit>
        <trans-unit id="e793ff03e25bf5dd4f85e642caecdeee487a3a73" translate="yes" xml:space="preserve">
          <source>Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned &lt;code&gt;ScriptModule&lt;/code&gt; will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,</source>
          <target state="translated">추적은 데이터에 종속되지 않고 (예 : 텐서의 데이터에 대한 조건이 없음) 추적되지 않은 외부 종속성 (예 : 입력 / 출력 수행 또는 전역 변수 액세스)이없는 함수 및 모듈 만 올바르게 기록합니다. 추적은 주어진 텐서에서 주어진 함수가 실행될 때 수행 된 작업 만 기록합니다. 따라서 반환 된 &lt;code&gt;ScriptModule&lt;/code&gt; 은 모든 입력에서 항상 동일한 추적 그래프를 실행합니다. 이는 모듈이 입력 및 / 또는 모듈 상태에 따라 다른 작업 집합을 실행할 것으로 예상되는 경우 몇 가지 중요한 의미를 갖습니다. 예를 들면</target>
        </trans-unit>
        <trans-unit id="1289e51c253ed91b441a90b66b33a5a7cc2af8cf" translate="yes" xml:space="preserve">
          <source>Tracing vs Scripting</source>
          <target state="translated">추적 vs 스크립팅</target>
        </trans-unit>
        <trans-unit id="4de8ec680853c82acd4d7ddcd35ef7c092a558c3" translate="yes" xml:space="preserve">
          <source>Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.</source>
          <target state="translated">추적은 if 문이나 루프와 같은 제어 흐름을 기록하지 않습니다. 이 제어 흐름이 모듈 전체에서 일정하면 문제가 없으며 제어 흐름 결정을 인라인하는 경우가 많습니다. 그러나 때때로 제어 흐름은 실제로 모델 자체의 일부입니다. 예를 들어, 순환 네트워크는 입력 시퀀스의 (동적 일 가능성이있는) 길이에 대한 루프입니다.</target>
        </trans-unit>
        <trans-unit id="b6fe7f5e79177b05f6d251ecb9c162d45455045d" translate="yes" xml:space="preserve">
          <source>Training</source>
          <target state="translated">Training</target>
        </trans-unit>
        <trans-unit id="68c170c0011cf476eed353d994b12887940cfc96" translate="yes" xml:space="preserve">
          <source>Transformer</source>
          <target state="translated">Transformer</target>
        </trans-unit>
        <trans-unit id="6e42fdb55f8e197d60cafca548c1e82579acbea4" translate="yes" xml:space="preserve">
          <source>Transformer Layers</source>
          <target state="translated">트랜스포머 레이어</target>
        </trans-unit>
        <trans-unit id="ef303bb941bf717e2006e7273f0810b07b78b045" translate="yes" xml:space="preserve">
          <source>TransformerDecoder</source>
          <target state="translated">TransformerDecoder</target>
        </trans-unit>
        <trans-unit id="39490c0e215073daec405a4b72d513bc1f4fb82e" translate="yes" xml:space="preserve">
          <source>TransformerDecoder is a stack of N decoder layers</source>
          <target state="translated">TransformerDecoder는 N 개의 디코더 레이어 스택입니다.</target>
        </trans-unit>
        <trans-unit id="92f20fa7687824fdbb370a6b475f6eeb6f79194b" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer</source>
          <target state="translated">TransformerDecoderLayer</target>
        </trans-unit>
        <trans-unit id="98ae4e9bf414c9fe8b37e6dbc99426a1c7335c2f" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</source>
          <target state="translated">TransformerDecoderLayer는 자체 참석, 다중 헤드 참석 및 피드 포워드 네트워크로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="04a4a3c2fb795f7db70756477e2d518c4a892786" translate="yes" xml:space="preserve">
          <source>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerDecoderLayer는 자체 참석, 다중 헤드 참석 및 피드 포워드 네트워크로 구성됩니다. 이 표준 디코더 레이어는 &quot;Attention Is All You Need&quot;라는 논문을 기반으로합니다. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심 만 있으면됩니다. Advances in Neural Information Processing Systems, 페이지 6000-6010. 사용자는 응용 프로그램 중에 다른 방식으로 수정하거나 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="32d7343edd3b94812b03b1cdf834b1b16cc2a3fc" translate="yes" xml:space="preserve">
          <source>TransformerEncoder</source>
          <target state="translated">TransformerEncoder</target>
        </trans-unit>
        <trans-unit id="933db464a961834a0fb72e3b98e0d537c401c215" translate="yes" xml:space="preserve">
          <source>TransformerEncoder is a stack of N encoder layers</source>
          <target state="translated">TransformerEncoder는 N 인코더 계층의 스택입니다.</target>
        </trans-unit>
        <trans-unit id="2a732f60462f98f99de0f8f387f8c71e6c32aba7" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer</source>
          <target state="translated">TransformerEncoderLayer</target>
        </trans-unit>
        <trans-unit id="7b3ae5e9124dac923ef7ce7bbf1287aa019083f2" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network.</source>
          <target state="translated">TransformerEncoderLayer는 자체 참석 및 피드 포워드 네트워크로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="0c8dccb3d09a0f9d0d9b17ea9825251d663f04d4" translate="yes" xml:space="preserve">
          <source>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</source>
          <target state="translated">TransformerEncoderLayer는 자체 참석 및 피드 포워드 네트워크로 구성됩니다. 이 표준 인코더 계층은 &quot;주의가 필요한 모든 것&quot;이라는 논문을 기반으로합니다. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심 만 있으면됩니다. Advances in Neural Information Processing Systems, 페이지 6000-6010. 사용자는 응용 프로그램 중에 다른 방식으로 수정하거나 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a9cb51a530db6f55c97c4e0720722c1b583dafb7" translate="yes" xml:space="preserve">
          <source>TripletMarginLoss</source>
          <target state="translated">TripletMarginLoss</target>
        </trans-unit>
        <trans-unit id="858db382b48e5cc5988f431f28f1130330c1eaf1" translate="yes" xml:space="preserve">
          <source>TripletMarginWithDistanceLoss</source>
          <target state="translated">TripletMarginWithDistanceLoss</target>
        </trans-unit>
        <trans-unit id="88b33e4e12f75ac8bf792aebde41f1a090f3a612" translate="yes" xml:space="preserve">
          <source>True</source>
          <target state="translated">True</target>
        </trans-unit>
        <trans-unit id="6692b9e66830b65bbf15ff9de3c8b1af8bfbc5eb" translate="yes" xml:space="preserve">
          <source>Tuple Construction</source>
          <target state="translated">튜플 구성</target>
        </trans-unit>
        <trans-unit id="2c809de45543797accbb7ba7433a5877747ac289" translate="yes" xml:space="preserve">
          <source>Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to &lt;code&gt;pack_padded_sequence&lt;/code&gt; or &lt;code&gt;pack_sequence&lt;/code&gt;.</source>
          <target state="translated">패딩 된 시퀀스를 포함하는 Tensor의 튜플과 배치에있는 각 시퀀스의 길이 목록을 포함하는 Tensor. 배치 요소는 배치가 &lt;code&gt;pack_padded_sequence&lt;/code&gt; 또는 &lt;code&gt;pack_sequence&lt;/code&gt; 에 전달 될 때 원래 순서대로 다시 정렬됩니다 .</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="19fd25590b1f3de52164e96d6a585a4ae3f02132" translate="yes" xml:space="preserve">
          <source>Two names &lt;em&gt;match&lt;/em&gt; if they are equal (string equality) or if at least one is &lt;code&gt;None&lt;/code&gt;. Nones are essentially a special &amp;ldquo;wildcard&amp;rdquo; name.</source>
          <target state="translated">두 이름 이 같거나 (문자열 같음) 적어도 하나가 &lt;code&gt;None&lt;/code&gt; 인 경우 &lt;em&gt;일치&lt;/em&gt; 합니다 . None은 본질적으로 특별한 &quot;와일드 카드&quot;이름입니다.</target>
        </trans-unit>
        <trans-unit id="3deb7456519697ecf4eefc455516c969a3681bae" translate="yes" xml:space="preserve">
          <source>Type</source>
          <target state="translated">Type</target>
        </trans-unit>
        <trans-unit id="74a99ad458c9adf9d415d4bdb125da11688a37f7" translate="yes" xml:space="preserve">
          <source>Type Info</source>
          <target state="translated">유형 정보</target>
        </trans-unit>
        <trans-unit id="58cf557846d7f65d34e8a92739eef2665164fad4" translate="yes" xml:space="preserve">
          <source>Type aliases</source>
          <target state="translated">타입 별칭</target>
        </trans-unit>
        <trans-unit id="93b9e289e2842469d001eccf7ad5d79f3c302dc9" translate="yes" xml:space="preserve">
          <source>Types</source>
          <target state="translated">Types</target>
        </trans-unit>
        <trans-unit id="c0d285234eb927c53c2fc1dda528e04061e0d90d" translate="yes" xml:space="preserve">
          <source>Types produced by &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; can be used in TorchScript.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt; &lt;code&gt;collections.namedtuple&lt;/code&gt; 에&lt;/a&gt; 의해 생성 된 유형 은 TorchScript에서 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b2c7c0caa10a0cca5ea7d69e54018ae0c0389dd6" translate="yes" xml:space="preserve">
          <source>U</source>
          <target state="translated">U</target>
        </trans-unit>
        <trans-unit id="4e5f249d2049283bfab86474c53e19434fabe07e" translate="yes" xml:space="preserve">
          <source>URL specifying how to initialize the process group. Default is &lt;code&gt;env://&lt;/code&gt;</source>
          <target state="translated">프로세스 그룹을 초기화하는 방법을 지정하는 URL입니다. 기본값은 &lt;code&gt;env://&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="9016840b6ab501762ae42c3bc2d77284127ecd9d" translate="yes" xml:space="preserve">
          <source>Unflatten</source>
          <target state="translated">Unflatten</target>
        </trans-unit>
        <trans-unit id="22753787538a4df3368517dc3a28771d6a6bd1c2" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape.</source>
          <target state="translated">원하는 모양으로 확장하는 텐서 희미한 평면을 해제합니다.</target>
        </trans-unit>
        <trans-unit id="ffd6987181a5e9a56284c36d90107b63ec0ad279" translate="yes" xml:space="preserve">
          <source>Unflattens a tensor dim expanding it to a desired shape. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">원하는 모양으로 확장하는 텐서 희미한 평면을 해제합니다. &lt;code&gt;Sequential&lt;/code&gt; 과 함께 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="02d85ea4efaa0d1ca099f19843e9be07e28b954d" translate="yes" xml:space="preserve">
          <source>Unfold</source>
          <target state="translated">Unfold</target>
        </trans-unit>
        <trans-unit id="27c8f884a26740cbb923c350e9fa436fb8314b34" translate="yes" xml:space="preserve">
          <source>Unfortunately, the concrete &lt;code&gt;subset&lt;/code&gt; that was used is lost. For more information see &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;this discussion&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;these experiments&lt;/a&gt;.</source>
          <target state="translated">불행히도 사용 된 구체적인 &lt;code&gt;subset&lt;/code&gt; 이 손실됩니다. 자세한 내용은 &lt;a href=&quot;https://github.com/pytorch/vision/issues/1439&quot;&gt;이 토론&lt;/a&gt; 또는 &lt;a href=&quot;https://github.com/pytorch/vision/pull/1965&quot;&gt;이 실험을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b69bc99e876e93b97ea0728c1acca97b14bbffef" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt;, this function copies the tensor&amp;rsquo;s data.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt; 와 달리이 함수는 텐서의 데이터를 복사합니다.</target>
        </trans-unit>
        <trans-unit id="ef5392b685d4622304e5dfc150347ec19f0ee0af" translate="yes" xml:space="preserve">
          <source>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the &lt;code&gt;affine&lt;/code&gt; option, Layer Normalization applies per-element scale and bias with &lt;code&gt;elementwise_affine&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;affine&lt;/code&gt; 옵션을 사용하여 각 전체 채널 / 평면에 스칼라 스케일 및 바이어스를 적용하는 Batch Normalization 및 Instance Normalization과 달리 Layer Normalization은 &lt;code&gt;elementwise_affine&lt;/code&gt; 을 사용 하여 요소 별 스케일 및 바이어스를 적용 합니다.</target>
        </trans-unit>
        <trans-unit id="2498c971c849991894b17969e87a3329f48f1d2d" translate="yes" xml:space="preserve">
          <source>Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions.</source>
          <target state="translated">Python과 달리 TorchScript 함수의 각 변수에는 단일 정적 유형이 있어야합니다. 이렇게하면 TorchScript 기능을보다 쉽게 ​​최적화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3d7bdd35d7e8961eff68645ed326f054b5482c52" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented</source>
          <target state="translated">구현 가능성이 낮음</target>
        </trans-unit>
        <trans-unit id="a5b6f222f736f9cef541160a005848fe4ef77888" translate="yes" xml:space="preserve">
          <source>Unlikely to be implemented (however &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt;&lt;code&gt;typing.Optional&lt;/code&gt;&lt;/a&gt; is supported)</source>
          <target state="translated">구현 가능성이 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#typing.Optional&quot;&gt; &lt;code&gt;typing.Optional&lt;/code&gt; &lt;/a&gt; (단, 입력 옵션 은 지원됨)</target>
        </trans-unit>
        <trans-unit id="01773bdc2e10d2279820d7115b2522610a09e4f3" translate="yes" xml:space="preserve">
          <source>Unpacks the data and pivots from a LU factorization of a tensor.</source>
          <target state="translated">텐서의 LU 분해에서 데이터 및 피벗을 압축 해제합니다.</target>
        </trans-unit>
        <trans-unit id="a7afe79ac1105fe133bbed5120636913c111a929" translate="yes" xml:space="preserve">
          <source>Unsupported Typing Constructs</source>
          <target state="translated">지원되지 않는 타이핑 구문</target>
        </trans-unit>
        <trans-unit id="b2d1fe571301d72d9191816bf953a47985e426ec" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">매핑 또는 반복 가능한 키-값 쌍으로 &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt; 를 업데이트하여 기존 키를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="4eca5c6565cbf1cfd8391f002e24b24767bdf27d" translate="yes" xml:space="preserve">
          <source>Update the &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt; with the key-value pairs from a mapping or an iterable, overwriting existing keys.</source>
          <target state="translated">매핑 또는 반복 가능한 키-값 쌍으로 &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt; 를 업데이트하여 기존 키를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="e9f4cec65260954e7a90830aa6f73b90e5c8817f" translate="yes" xml:space="preserve">
          <source>Upsample</source>
          <target state="translated">Upsample</target>
        </trans-unit>
        <trans-unit id="0b472c3013e4cd26ac1f5f84d9b57c5b6b4ca455" translate="yes" xml:space="preserve">
          <source>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</source>
          <target state="translated">주어진 다중 채널 1D (시간), 2D (공간) 또는 3D (체적) 데이터를 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="55dfe2d567c845d264928977c08cd8d377265025" translate="yes" xml:space="preserve">
          <source>Upsamples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">입력을 주어진 &lt;code&gt;size&lt;/code&gt; 또는 주어진 &lt;code&gt;scale_factor&lt;/code&gt; 로 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="c9b84912595a004dd92417c63046087231bc7d36" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using bilinear upsampling.</source>
          <target state="translated">쌍 선형 업 샘플링을 사용하여 입력을 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="a31dc0017d79484c9069d06f3ac78a206e41dbfa" translate="yes" xml:space="preserve">
          <source>Upsamples the input, using nearest neighbours&amp;rsquo; pixel values.</source>
          <target state="translated">가장 가까운 이웃의 픽셀 값을 사용하여 입력을 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="fef8a1cac9f9d013e57f5acb6da8a9f36cede8d1" translate="yes" xml:space="preserve">
          <source>UpsamplingBilinear2d</source>
          <target state="translated">UpsamplingBilinear2d</target>
        </trans-unit>
        <trans-unit id="2a7960d23688a71e6707ef6d16f626ed000e779c" translate="yes" xml:space="preserve">
          <source>UpsamplingNearest2d</source>
          <target state="translated">UpsamplingNearest2d</target>
        </trans-unit>
        <trans-unit id="680c6c9f314b68f86ff46d8fdf9a289c7fb0c343" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt;&lt;code&gt;align_as()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to align tensor dimensions by name to a specified ordering. This is useful for performing &amp;ldquo;broadcasting by names&amp;rdquo;.</source>
          <target state="translated">사용 &lt;a href=&quot;#torch.Tensor.align_as&quot;&gt; &lt;code&gt;align_as()&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 지정된 순서에 이름으로 정렬 텐서 치수. 이것은 &quot;이름으로 방송&quot;을 수행 할 때 유용합니다.</target>
        </trans-unit>
        <trans-unit id="66804d0e4e83078e14bfdbaa37e395675103792c" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.align_to&quot;&gt;&lt;code&gt;align_to()&lt;/code&gt;&lt;/a&gt; to permute large amounts of dimensions without mentioning all of them as in required by &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt;&lt;code&gt;permute()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.align_to&quot;&gt; &lt;code&gt;align_to()&lt;/code&gt; &lt;/a&gt; 를 사용 하여 &lt;a href=&quot;tensors#torch.Tensor.permute&quot;&gt; &lt;code&gt;permute()&lt;/code&gt; &lt;/a&gt; 에서 요구하는대로 모든 차원을 언급하지 않고 다량의 차원을 permute 합니다.</target>
        </trans-unit>
        <trans-unit id="ede9bff88acacf3e23cc669406c39e0d8aba75b8" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;torch.Tensor.item()&lt;/code&gt;&lt;/a&gt; to get a Python number from a tensor containing a single value:</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;torch.Tensor.item()&lt;/code&gt; &lt;/a&gt; 을 사용 하여 단일 값을 포함하는 텐서에서 Python 번호를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="e75b6839717cab84ecfa02664778f32a2c198780" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; to access the dimension names of a tensor and &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt; to rename named dimensions.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 을 사용 하여 텐서의 차원 이름에 액세스하고 &lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt; 을 사용하여 명명 된 차원의 이름을 변경하십시오.</target>
        </trans-unit>
        <trans-unit id="b5d22900f5f0642196ebec69d6c5572874ee7c1b" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt;&lt;code&gt;flatten()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt;&lt;code&gt;unflatten()&lt;/code&gt;&lt;/a&gt; to flatten and unflatten dimensions, respectively. These methods are more verbose than &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, but have more semantic meaning to someone reading the code.</source>
          <target state="translated">사용 &lt;a href=&quot;tensors#torch.Tensor.flatten&quot;&gt; &lt;code&gt;flatten()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.Tensor.unflatten&quot;&gt; &lt;code&gt;unflatten()&lt;/code&gt; &lt;/a&gt; 각각 평평하고 패턴 화 해제 치수합니다. 이러한 메서드는 &lt;a href=&quot;tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensors#torch.Tensor.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 보다 더 장황 하지만 코드를 읽는 사람에게 더 의미가 있습니다.</target>
        </trans-unit>
        <trans-unit id="641cc18d21edd9671570b1c6c598d26a3e5396f4" translate="yes" xml:space="preserve">
          <source>Use Gloo, unless you have specific reasons to use MPI.</source>
          <target state="translated">MPI를 사용해야하는 특별한 이유가 없다면 Gloo를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="4b4d173cba748d683c0210fec722f5787173d83a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</source>
          <target state="translated">특히 다중 프로세스 단일 노드 또는 다중 노드 분산 학습에 대해 현재 최상의 분산 GPU 학습 성능을 제공하므로 NCCL을 사용하십시오. NCCL에 문제가 발생하면 Gloo를 대체 옵션으로 사용하십시오. (현재 Gloo는 GPU의 경우 NCCL보다 느리게 실행됩니다.)</target>
        </trans-unit>
        <trans-unit id="9b1bf082bc9bf3ac7914e6187c00d474a91fb73a" translate="yes" xml:space="preserve">
          <source>Use NCCL, since it&amp;rsquo;s the only backend that currently supports InfiniBand and GPUDirect.</source>
          <target state="translated">NCCL은 현재 InfiniBand 및 GPUDirect를 지원하는 유일한 백엔드이므로 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5e1700010ee503b77904278f8d396bfe42da63c7" translate="yes" xml:space="preserve">
          <source>Use external data format</source>
          <target state="translated">외부 데이터 형식 사용</target>
        </trans-unit>
        <trans-unit id="9d0232b01e54b3a2e55b622100adbb68b4edb340" translate="yes" xml:space="preserve">
          <source>Use of Python Values</source>
          <target state="translated">Python 값 사용</target>
        </trans-unit>
        <trans-unit id="73922dca1fc1eb81dc09b31de16429c2bad893e0" translate="yes" xml:space="preserve">
          <source>Use the Gloo backend for distributed &lt;strong&gt;CPU&lt;/strong&gt; training.</source>
          <target state="translated">분산 &lt;strong&gt;CPU&lt;/strong&gt; 훈련을 위해 Gloo 백엔드를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="fc3648b93e2309e15d382b23a53006e5ccb31b40" translate="yes" xml:space="preserve">
          <source>Use the NCCL backend for distributed &lt;strong&gt;GPU&lt;/strong&gt; training</source>
          <target state="translated">분산 &lt;strong&gt;GPU&lt;/strong&gt; 학습에 NCCL 백엔드 사용</target>
        </trans-unit>
        <trans-unit id="9ca66ca0ae6872cefb7cdaba9a1b1d7c4fb946f1" translate="yes" xml:space="preserve">
          <source>Used to infer dtype for python complex numbers. The default complex dtype is set to &lt;code&gt;torch.complex128&lt;/code&gt; if default floating point dtype is &lt;code&gt;torch.float64&lt;/code&gt;, otherwise it&amp;rsquo;s set to &lt;code&gt;torch.complex64&lt;/code&gt;</source>
          <target state="translated">파이썬 복소수에 대한 dtype을 추론하는 데 사용됩니다. 기본 복잡한 DTYPE로 설정되어 &lt;code&gt;torch.complex128&lt;/code&gt; 기본 부동 소수점 DTYPE 경우 &lt;code&gt;torch.float64&lt;/code&gt; 에 그렇지 않은 경우의 설정 &lt;code&gt;torch.complex64&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fac6c061fc6d273aebc0e9ab0c99f04ae81e1097" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Argument</source>
          <target state="translated">소유자가 인수 인 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="a20f912ad88b795f826c479ed261f676c30da2f7" translate="yes" xml:space="preserve">
          <source>User Share RRef with Owner as Return Value</source>
          <target state="translated">반환 값으로 소유자와 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="a7e43a6c0dede8cc47d9e15900c78d9da28530c0" translate="yes" xml:space="preserve">
          <source>User Share RRef with User</source>
          <target state="translated">사용자와 사용자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="78d0e495aa5431e6e0a8139c0666804c17a54940" translate="yes" xml:space="preserve">
          <source>User extensions can register their own location tags and tagging and deserialization methods using &lt;code&gt;torch.serialization.register_package()&lt;/code&gt;.</source>
          <target state="translated">사용자 확장은 &lt;code&gt;torch.serialization.register_package()&lt;/code&gt; 사용하여 자체 위치 태그와 태그 지정 및 역 직렬화 방법을 등록 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a2628232b96e27d1cfe74641b5e5eef1e4a68d67" translate="yes" xml:space="preserve">
          <source>Users can force a reload by calling &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt;. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</source>
          <target state="translated">사용자는 &lt;code&gt;hub.load(..., force_reload=True)&lt;/code&gt; 를 호출하여 강제로 다시로드 할 수 있습니다 . 이렇게하면 기존 github 폴더와 다운로드 된 가중치가 삭제되고 새로운 다운로드가 다시 초기화됩니다. 이것은 업데이트가 동일한 브랜치에 게시 될 때 유용하며 사용자는 최신 릴리스를 따라갈 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="df3d36e2101db7508ee9c65bba6c79f4162eb560" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;DistributedDataParallel&lt;/code&gt; in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; is experimental and subject to change.</source>
          <target state="translated">&lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; 와 함께 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 사용하는 것은 실험적이며 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6f8c8c4de2dead884cf1cd1b1efe492df15bdbf3" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;torch.jit.trace&lt;/code&gt; and &lt;code&gt;torch.jit.trace_module&lt;/code&gt;, you can turn an existing module or Python function into a TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.</source>
          <target state="translated">&lt;code&gt;torch.jit.trace&lt;/code&gt; 및 &lt;code&gt;torch.jit.trace_module&lt;/code&gt; 을 사용 하면 기존 모듈 또는 Python 함수를 TorchScript &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 로 바꿀 수 있습니다 . 예제 입력을 제공하고 함수를 실행하여 모든 텐서에서 수행 된 작업을 기록해야합니다.</target>
        </trans-unit>
        <trans-unit id="016141762b3eda50d55b6434765c656b21e2c21e" translate="yes" xml:space="preserve">
          <source>Using GPU tensors as arguments or return values of &lt;code&gt;func&lt;/code&gt; is not supported since we don&amp;rsquo;t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="translated">GPU 텐서를 인수로 사용하거나 &lt;code&gt;func&lt;/code&gt; 의 반환 값을 사용 하는 것은 유선을 통한 GPU 텐서 전송을 지원하지 않기 때문에 지원되지 않습니다. GPU 텐서를 인수로 사용하거나 &lt;code&gt;func&lt;/code&gt; 의 값을 반환하기 전에 CPU 텐서를 명시 적으로 복사해야합니다 .</target>
        </trans-unit>
        <trans-unit id="40f8083d709ef1dfeba3266d73cd40bc2120c4c0" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute matrix norms:</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 인수를 사용하여 행렬 규범 계산 :</target>
        </trans-unit>
        <trans-unit id="34eeb9d75aa34a25cf342ae4a398eead839c1594" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;dim&lt;/code&gt; argument to compute vector norms:</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 인수를 사용하여 벡터 노름 계산 :</target>
        </trans-unit>
        <trans-unit id="892e242741e1c766ef8440ce7e813adfc429718b" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv2d&lt;/code&gt; modules.</source>
          <target state="translated">일반적으로 입력은 &lt;code&gt;nn.Conv2d&lt;/code&gt; 모듈 에서 제공됩니다 .</target>
        </trans-unit>
        <trans-unit id="62c50ebd5d624de117b146ade6f9d5575d770fc3" translate="yes" xml:space="preserve">
          <source>Usually the input comes from &lt;code&gt;nn.Conv3d&lt;/code&gt; modules.</source>
          <target state="translated">일반적으로 입력은 &lt;code&gt;nn.Conv3d&lt;/code&gt; 모듈 에서 제공됩니다 .</target>
        </trans-unit>
        <trans-unit id="18fdc5ee8b1f8fba8dabaa933373c0483ab7fad7" translate="yes" xml:space="preserve">
          <source>Utilities</source>
          <target state="translated">Utilities</target>
        </trans-unit>
        <trans-unit id="ad96fcc3041d4053b9449ab4a648c6341e2217d7" translate="yes" xml:space="preserve">
          <source>Utility functions in other modules</source>
          <target state="translated">다른 모듈의 유틸리티 기능</target>
        </trans-unit>
        <trans-unit id="1c41457151932f1c1abddf366a843f91dd5ea098" translate="yes" xml:space="preserve">
          <source>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</source>
          <target state="translated">단위를 가지 치기하지 않고 하나의 마스크로 가지 치기 매개 변수를 생성하는 유틸리티 가지 치기 방법입니다.</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="1eafbd543ea2d8baa3df59414ec62e236836b78c" translate="yes" xml:space="preserve">
          <source>V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</source>
          <target state="translated">V. Balntas, et al .: 삼중 항 손실이있는 얕은 컨볼 루션 기능 설명자 학습 : &lt;a href=&quot;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&quot;&gt;http://www.bmva.org/bmvc/2016/papers/paper119/index.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2badedb167ecf8f97a1ffcdd2817401074fa728e" translate="yes" xml:space="preserve">
          <source>VGG</source>
          <target state="translated">VGG</target>
        </trans-unit>
        <trans-unit id="76dc2de0cc7c95272fc67f826f2dc5fa09c8ded2" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) from &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot; &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&lt;/a&gt; &quot;의 VGG 11 계층 모델 (구성 &quot;A&quot;)</target>
        </trans-unit>
        <trans-unit id="efac66ad38738b85f5ef46a914f1b2bede87a54e" translate="yes" xml:space="preserve">
          <source>VGG 11-layer model (configuration &amp;ldquo;A&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&lt;/a&gt; &quot;를 사용하는 VGG 11 계층 모델 (구성 &quot;A&quot;)</target>
        </trans-unit>
        <trans-unit id="8ab10add11dcdf5fb182aad6d9f35aa59134c466" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 13 계층 모델 (구성&amp;ldquo;B&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c68abe1f69acd7242c26b04b450c20879de247dc" translate="yes" xml:space="preserve">
          <source>VGG 13-layer model (configuration &amp;ldquo;B&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;를 사용하는&lt;/a&gt; VGG 13- 레이어 모델 (구성 &quot;B&quot;)</target>
        </trans-unit>
        <trans-unit id="7df2c09a84b7ee83b6d8493aa3dccf2e12cc714a" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 16 계층 모델 (구성&amp;ldquo;D&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4979b51da2376b295925fa358ed83201a60846eb" translate="yes" xml:space="preserve">
          <source>VGG 16-layer model (configuration &amp;ldquo;D&amp;rdquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화 &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;를 사용하는&lt;/a&gt; VGG 16 레이어 모델 (구성 &quot;D&quot;)</target>
        </trans-unit>
        <trans-unit id="734385d469c0b7b16d39310aab3a223999973e9e" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">VGG 19 레이어 모델 (구성&amp;ldquo;E&amp;rdquo;) &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b468de2d615302723a63a19cf8ef07ba2f87d240" translate="yes" xml:space="preserve">
          <source>VGG 19-layer model (configuration &amp;lsquo;E&amp;rsquo;) with batch normalization &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&amp;ldquo;Very Deep Convolutional Networks For Large-Scale Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">배치 정규화를 사용하는 VGG 19 레이어 모델 (구성 'E') &lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;&quot;대규모 이미지 인식을위한 매우 깊은 컨볼 루션 네트워크&quot;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22dca5ed30facb9e021b2a416131b71d9addd5df" translate="yes" xml:space="preserve">
          <source>VGG-11</source>
          <target state="translated">VGG-11</target>
        </trans-unit>
        <trans-unit id="59c5eb8db8735266b63e11d68abfe0513f53e937" translate="yes" xml:space="preserve">
          <source>VGG-11 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-11</target>
        </trans-unit>
        <trans-unit id="275a0abbe0e323332b36f91fd1e0bbabaa98823b" translate="yes" xml:space="preserve">
          <source>VGG-13</source>
          <target state="translated">VGG-13</target>
        </trans-unit>
        <trans-unit id="00e8568346d410021e9a56f3b1b1f1983ac75021" translate="yes" xml:space="preserve">
          <source>VGG-13 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-13</target>
        </trans-unit>
        <trans-unit id="3fa5851c47708aca43af1f031237ec76652d5fb8" translate="yes" xml:space="preserve">
          <source>VGG-16</source>
          <target state="translated">VGG-16</target>
        </trans-unit>
        <trans-unit id="6794221fe94aa99e89cd4a3fc469cd560c5b1798" translate="yes" xml:space="preserve">
          <source>VGG-16 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-16</target>
        </trans-unit>
        <trans-unit id="e33f8119a3d18ad4bc21aa5913ffff22adbc62fa" translate="yes" xml:space="preserve">
          <source>VGG-19</source>
          <target state="translated">VGG-19</target>
        </trans-unit>
        <trans-unit id="6a06101d542844efc9851734dd33c0a3fcfb9071" translate="yes" xml:space="preserve">
          <source>VGG-19 with batch normalization</source>
          <target state="translated">배치 정규화를 사용하는 VGG-19</target>
        </trans-unit>
        <trans-unit id="08ab4ecc000363002865da057bb07b708354e689" translate="yes" xml:space="preserve">
          <source>Valid operation names:</source>
          <target state="translated">유효한 작업 이름 :</target>
        </trans-unit>
        <trans-unit id="8ab2f6ea14647497320511c9699ad1fa98390d6e" translate="yes" xml:space="preserve">
          <source>Value associated with &lt;code&gt;key&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; is in the store.</source>
          <target state="translated">가치와 관련된 &lt;code&gt;key&lt;/code&gt; 경우 &lt;code&gt;key&lt;/code&gt; 가게에 있습니다.</target>
        </trans-unit>
        <trans-unit id="8f65e3050b4aa23d2b9ffeafc8ef420283f1bc31" translate="yes" xml:space="preserve">
          <source>Values looked up as attributes of a module are assumed to be constant:</source>
          <target state="translated">모듈의 속성으로 조회 된 값은 상수로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="766a55330fbdeceea3990ed650f19ff9d7ebc2b2" translate="yes" xml:space="preserve">
          <source>Vandermonde matrix. If increasing is False, the first column is</source>
          <target state="translated">Vandermonde 행렬. 증가가 False이면 첫 번째 열은</target>
        </trans-unit>
        <trans-unit id="8993fc586517fabcc3d0fce5c92e800dc4e3de15" translate="yes" xml:space="preserve">
          <source>Variable Resolution</source>
          <target state="translated">가변 해상도</target>
        </trans-unit>
        <trans-unit id="ac018db1f7b00972061adff843d37497d8ee153c" translate="yes" xml:space="preserve">
          <source>Variables</source>
          <target state="translated">Variables</target>
        </trans-unit>
        <trans-unit id="b1c39119660f33e5be726c1652639e668fcdfcd8" translate="yes" xml:space="preserve">
          <source>Verifies that the given compiler is ABI-compatible with PyTorch.</source>
          <target state="translated">주어진 컴파일러가 PyTorch와 ABI 호환되는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="991eeb7d6acd3a1b90daf8e907ad4a1126fbf67e" translate="yes" xml:space="preserve">
          <source>Via a string and device ordinal:</source>
          <target state="translated">문자열 및 장치 서수를 통해 :</target>
        </trans-unit>
        <trans-unit id="78f371ae51565c626e223c43a305351baadcdfcb" translate="yes" xml:space="preserve">
          <source>Via a string:</source>
          <target state="translated">문자열을 통해 :</target>
        </trans-unit>
        <trans-unit id="3eb4e2b65c3b6adcd10d477d9e1ded0579505479" translate="yes" xml:space="preserve">
          <source>Video classification</source>
          <target state="translated">비디오 분류</target>
        </trans-unit>
        <trans-unit id="61ad6c7f7397fbdc6a6513e489917c12a6953f11" translate="yes" xml:space="preserve">
          <source>View this tensor as the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.view_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.view(other.size())&lt;/code&gt;.</source>
          <target state="translated">이 텐서를 &lt;code&gt;other&lt;/code&gt; 것과 같은 크기로 봅니다. &lt;code&gt;self.view_as(other)&lt;/code&gt; 는 &lt;code&gt;self.view(other.size())&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="68b460da28c94fa06be12e6f2242c79c2eca8bd2" translate="yes" xml:space="preserve">
          <source>Vision Layers</source>
          <target state="translated">비전 레이어</target>
        </trans-unit>
        <trans-unit id="8cd7ed1352bc6bdd613ec698de479a989aa9357b" translate="yes" xml:space="preserve">
          <source>Vision functions</source>
          <target state="translated">비전 기능</target>
        </trans-unit>
        <trans-unit id="e2415cb7f63df0c9de23362326ad3c37a9adfc96" translate="yes" xml:space="preserve">
          <source>W</source>
          <target state="translated">W</target>
        </trans-unit>
        <trans-unit id="46965f2770b3f862f5982cb5b877918de164026c" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride [1]}-2 \ times \ text {padding [1]} + \ text {kernel \ _size [1]}</target>
        </trans-unit>
        <trans-unit id="5b7347a76af1465d2b26baeed335756321992f36" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride [2]}-2 \ times \ text {padding [2]} + \ text {kernel \ _size [2]}</target>
        </trans-unit>
        <trans-unit id="06cfb3f342da0b25161bf11f15bf2d81bfb676a7" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride} [1]-2 \ times \ text {padding} [1] + \ text {dilation} [1] \ times (\ text { 커널 \ _size} [1]-1) + \ text {output \ _padding} [1] + 1</target>
        </trans-unit>
        <trans-unit id="144094cbaa937780b12d5cb8356e1fc4a6c692aa" translate="yes" xml:space="preserve">
          <source>W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</source>
          <target state="translated">W_ {out} = (W_ {in}-1) \ times \ text {stride} [2]-2 \ times \ text {padding} [2] + \ text {dilation} [2] \ times (\ text { 커널 \ _size} [2]-1) + \ text {output \ _padding} [2] + 1</target>
        </trans-unit>
        <trans-unit id="23da97705d0e8d5a571139859c1eefc7a25b59f7" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}</source>
          <target state="translated">W_ {out} = W_ {in} + \ text {padding \ _left} + \ text {padding \ _right}</target>
        </trans-unit>
        <trans-unit id="bf2c9a7a90367be7032bd4f58614c0c9ad3779f8" translate="yes" xml:space="preserve">
          <source>W_{out} = W_{in} \times \text{upscale\_factor}</source>
          <target state="translated">W_ {out} = W_ {in} \ times \ text {upscale \ _factor}</target>
        </trans-unit>
        <trans-unit id="c5a97d192d1067a74706f036b87ed2b96b8d5895" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor W_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="dd2204cd0212ab9c7330f7c2e6a0b40cae1ae693" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]} \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 * \ text {padding [1]}-\ text {dilation [1]} \ times (\ text {kernel \ _size [1]}- 1)-1} {\ text {stride [1]}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="876c8e0e601e7178678d337712548c5cc7269f61" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [1]-\ text {dilation} [1] \ times (\ text {kernel \ _size} [1] -1)-1} {\ text {stride} [1]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="6e0657f1536a7a37c826480069677e9010835607" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [1]-\ text {kernel \ _size} [1]} {\ text {stride} [1]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="a11c0191402c7be2c31a4ea7dcec53e255776ef6" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [2]-\ text {dilation} [2] \ times (\ text {kernel \ _size} [2] -1)-1} {\ text {stride} [2]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="b4191f6bab2de7c1a0f2413fa95cad94e0003ac5" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in} + 2 \ times \ text {padding} [2]-\ text {kernel \ _size} [2]} {\ text {stride} [2]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="90b3ee06862d0af7ed835a4f57d3b9839c234435" translate="yes" xml:space="preserve">
          <source>W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">W_ {out} = \ left \ lfloor \ frac {W_ {in}-\ text {kernel \ _size} [1]} {\ text {stride} [1]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="02d96942e51e668861c82b6ad6a1888bbaffdfc3" translate="yes" xml:space="preserve">
          <source>Waits for all provided futures to be complete, and returns the list of completed values.</source>
          <target state="translated">제공된 모든 Future가 완료 될 때까지 기다린 후 완료된 값 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0a15d8c4d33065db8154fb49b0e2d2a98624af19" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store, and throws an exception if the keys have not been set by the supplied &lt;code&gt;timeout&lt;/code&gt;.</source>
          <target state="translated">각 키 대기 &lt;code&gt;keys&lt;/code&gt; 저장소에 추가하고, 키가 제공된가 설정되지 않은 경우 예외가 발생 될 &lt;code&gt;timeout&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="24a824e1b1fcd89978ae586133c59626ca034951" translate="yes" xml:space="preserve">
          <source>Waits for each key in &lt;code&gt;keys&lt;/code&gt; to be added to the store. If not all keys are set before the &lt;code&gt;timeout&lt;/code&gt; (set during store initialization), then &lt;code&gt;wait&lt;/code&gt; will throw an exception.</source>
          <target state="translated">각 키에 대한 대기 &lt;code&gt;keys&lt;/code&gt; 저장소에 추가 할 수 있습니다. &lt;code&gt;timeout&lt;/code&gt; 이전에 모든 키가 설정되지 않은 경우 (저장소 초기화 중에 설정 됨) &lt;code&gt;wait&lt;/code&gt; 는 예외를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="322f5a9cd5158cc9dec2b3e7da850004249c9e4e" translate="yes" xml:space="preserve">
          <source>We accumulate the gradients in the appropriate &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt;&lt;code&gt;torch.distributed.autograd.context&lt;/code&gt;&lt;/a&gt; on each of the nodes. The autograd context to be used is looked up given the &lt;code&gt;context_id&lt;/code&gt; that is passed in when &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt;&lt;code&gt;get_gradients()&lt;/code&gt;&lt;/a&gt; API.</source>
          <target state="translated">각 노드 의 적절한 &lt;a href=&quot;#torch.distributed.autograd.context&quot;&gt; &lt;code&gt;torch.distributed.autograd.context&lt;/code&gt; &lt;/a&gt; 에 그라디언트를 누적합니다 . 사용할 autograd 컨텍스트 는 &lt;a href=&quot;#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt; 가 호출 될 때 전달 되는 &lt;code&gt;context_id&lt;/code&gt; 가 주어 지면 조회 됩니다. 주어진 ID에 해당하는 유효한 autograd 컨텍스트가 없으면 오류가 발생합니다. &lt;a href=&quot;#torch.distributed.autograd.get_gradients&quot;&gt; &lt;code&gt;get_gradients()&lt;/code&gt; &lt;/a&gt; API를 사용하여 누적 된 그라디언트를 검색 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="db56b28279fe3aea1467a7671b69e8c448f2218c" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt;&lt;code&gt;torch.nn.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.linear#torch.nn.Linear&quot;&gt; &lt;code&gt;torch.nn.Linear&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="3f8d51566ad98edb597de3ba663338465830f744" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv2d&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="2e48e5f00566ef4a4b3bcb10eab8d225acffc2e4" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;torch.nn.quantized.Conv3d&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="a3fb7d904d476590d98359b7241880b1be2f9a76" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.quantized#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;torch.nn.quantized.Linear&lt;/code&gt; &lt;/a&gt; 와 동일한 인터페이스를 채택합니다 .</target>
        </trans-unit>
        <trans-unit id="9fb0293b1874dcfe3feb591ccd49a1edaf2d5f3f" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Conv2d&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Conv2d&lt;/code&gt; 와 동일한 인터페이스를 채택합니다 . 문서 는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&quot;&gt;https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="27cfadeddc9160f1498530cc3eb1f719bfd8434a" translate="yes" xml:space="preserve">
          <source>We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">&lt;code&gt;torch.nn.Linear&lt;/code&gt; 와 동일한 인터페이스를 채택합니다 . 문서 는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="5105f862983064b0cf7594afba8d668c8f56e9ff" translate="yes" xml:space="preserve">
          <source>We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:</source>
          <target state="translated">추적과 스크립팅을 혼합 할 수 있습니다. 모델 일부의 특정 요구 사항에 맞게 추적 및 스크립팅을 구성 할 수 있습니다. 이 예를 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="6defebafebb2e25f76bcf411ab1ec455c032f365" translate="yes" xml:space="preserve">
          <source>We also do not support the following subsystems, though some may work out of the box:</source>
          <target state="translated">다음 하위 시스템도 지원하지 않지만 일부 하위 시스템은 즉시 작동 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3fa9a188f6de578d3bee11e42ad3ca8321060293" translate="yes" xml:space="preserve">
          <source>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with &lt;code&gt;torch.cat&lt;/code&gt;:</source>
          <target state="translated">내부 업데이트를 사용하지 않도록 코드를 수정하여이 문제를 해결할 수 있습니다. 대신 &lt;code&gt;torch.cat&lt;/code&gt; 을 사용 하여 결과 텐서를 외부에 빌드합니다 .</target>
        </trans-unit>
        <trans-unit id="7aac442ad82fddd2074ccf19e4043ee60a7cdfb0" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt; 의 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="4304f8e1f80a53c4c4788f88c99ef4dcb6d0bc02" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;torch.nn.ReLU&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;torch.nn.ReLU&lt;/code&gt; &lt;/a&gt; 의 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="94fba3bcda49d77ec84730c68d223d594ab23cb3" translate="yes" xml:space="preserve">
          <source>We combined the interface of &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;torch.nn.Conv2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;torch.nn.Conv2d&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;torch.nn.BatchNorm2d&lt;/code&gt; &lt;/a&gt; 인터페이스를 결합했습니다 .</target>
        </trans-unit>
        <trans-unit id="a068c518b2d5e22e14c479569a01c9c9d06bceec" translate="yes" xml:space="preserve">
          <source>We highly recommend taking a look at the original paper for more details.</source>
          <target state="translated">자세한 내용은 원본 논문을 참조하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="86ad4838215218ac000a7d87b4b48e01c6961c5f" translate="yes" xml:space="preserve">
          <source>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in &lt;code&gt;references/video_classification&lt;/code&gt;.</source>
          <target state="translated">Kinetics-400에 대해 사전 훈련 된 동작 인식 모델을 제공합니다. 그들은 모두 &lt;code&gt;references/video_classification&lt;/code&gt; 에 제공된 스크립트로 훈련되었습니다 .</target>
        </trans-unit>
        <trans-unit id="e921c07692e5a21fc62c5f94070c73cf433f0bb7" translate="yes" xml:space="preserve">
          <source>We provide pre-trained models, using the PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt;&lt;code&gt;torch.utils.model_zoo&lt;/code&gt;&lt;/a&gt;. These can be constructed by passing &lt;code&gt;pretrained=True&lt;/code&gt;:</source>
          <target state="translated">PyTorch &lt;a href=&quot;../model_zoo#module-torch.utils.model_zoo&quot;&gt; &lt;code&gt;torch.utils.model_zoo&lt;/code&gt; 를&lt;/a&gt; 사용하여 사전 학습 된 모델을 제공합니다 . &lt;code&gt;pretrained=True&lt;/code&gt; 전달하여 구성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="62c9fd377e2aad0f66ac5c26c69e0a5867347f36" translate="yes" xml:space="preserve">
          <source>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</source>
          <target state="translated">순수 Python 프로그램에서 독립형 C ++ 프로그램과 같이 Python과 독립적으로 실행할 수있는 TorchScript 프로그램으로 모델을 점진적으로 전환하는 도구를 제공합니다. 이를 통해 Python의 익숙한 도구를 사용하여 PyTorch에서 모델을 훈련 한 다음 TorchScript를 통해 Python 프로그램이 성능 및 멀티 스레딩 이유로 불리 할 수있는 프로덕션 환경으로 모델을 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bd73422aa80da87d8ae75185414e27781227d850" translate="yes" xml:space="preserve">
          <source>We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.</source>
          <target state="translated">제공된 루트를 사용하여 autograd 그래프를 찾고 적절한 종속성을 계산합니다. 이 메서드는 전체 autograd 계산이 완료 될 때까지 차단됩니다.</target>
        </trans-unit>
        <trans-unit id="48996658259127412cd98e4a4e7b4a204e6d1b0a" translate="yes" xml:space="preserve">
          <source>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by &lt;code&gt;name&lt;/code&gt; (e.g. &lt;code&gt;'weight'&lt;/code&gt;) with two parameters: one specifying the magnitude (e.g. &lt;code&gt;'weight_g'&lt;/code&gt;) and one specifying the direction (e.g. &lt;code&gt;'weight_v'&lt;/code&gt;). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every &lt;code&gt;forward()&lt;/code&gt; call.</source>
          <target state="translated">가중치 정규화는 가중치 텐서의 크기를 방향에서 분리하는 재 매개 변수화입니다. 이는 &lt;code&gt;name&lt;/code&gt; 지정된 매개 변수 (예 : &lt;code&gt;'weight'&lt;/code&gt; )를 두 개의 매개 변수로 대체합니다 . 하나는 크기를 지정하는 매개 변수 (예 : &lt;code&gt;'weight_g'&lt;/code&gt; )와 다른 하나는 방향을 지정하는 매개 변수 (예 : &lt;code&gt;'weight_v'&lt;/code&gt; )입니다. 가중치 정규화는 모든 &lt;code&gt;forward()&lt;/code&gt; 호출 전에 크기와 방향에서 가중치 텐서를 다시 계산하는 후크를 통해 구현됩니다 .</target>
        </trans-unit>
        <trans-unit id="d53879f401fe4a3643952a897298dc95fdaf8d7e" translate="yes" xml:space="preserve">
          <source>Weight:</source>
          <target state="translated">Weight:</target>
        </trans-unit>
        <trans-unit id="d4a914f8dcec6b172849d1c8fb16401fbdbb7604" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when &lt;code&gt;align_corners = False&lt;/code&gt;. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at &lt;code&gt;-1&lt;/code&gt;. From version 1.3.0, under &lt;code&gt;align_corners = True&lt;/code&gt; all grid points along a unit dimension are considered to be at &lt;code&gt;`0&lt;/code&gt; (the center of the input image).</source>
          <target state="translated">경우 &lt;code&gt;align_corners = True&lt;/code&gt; , 2 차원 데이터를 1 차원 데이터, 3 차원 아핀 변환에 2 차원 아핀 변환 (즉, 공간 차원의 하나의 단위 크기를 갖는 경우) 의도 된 사용 사례 불명확하고, 없다. &lt;code&gt;align_corners = False&lt;/code&gt; 문제가되지 않습니다 . 버전 1.2.0까지는 단위 치수를 따르는 모든 그리드 점이 임의로 &lt;code&gt;-1&lt;/code&gt; 로 간주되었습니다 . 버전 1.3.0부터 &lt;code&gt;align_corners = True&lt;/code&gt; 아래 에서 단위 치수를 따르는 모든 그리드 포인트는 &lt;code&gt;`0&lt;/code&gt; (입력 이미지의 중심)에있는 것으로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="71f1f4c1d858ca7758a6c256f77db87995af50dc" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;align_corners = True&lt;/code&gt;, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt;&lt;code&gt;grid_sample()&lt;/code&gt;&lt;/a&gt; will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was &lt;code&gt;align_corners = True&lt;/code&gt;. Since then, the default behavior has been changed to &lt;code&gt;align_corners = False&lt;/code&gt;, in order to bring it in line with the default for &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt;&lt;code&gt;interpolate()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">경우 &lt;code&gt;align_corners = True&lt;/code&gt; 그리드 위치가 입력 화상의 사이즈, 화소 사이즈에 의존하고 의해 샘플링 위치되도록 &lt;a href=&quot;#torch.nn.functional.grid_sample&quot;&gt; &lt;code&gt;grid_sample()&lt;/code&gt; &lt;/a&gt; 다른 해상도에서 주어진 동일한 입력 다르다 (즉, 업 샘플링 또는 다운 샘플링 된 후). 버전 1.2.0까지의 기본 동작은 &lt;code&gt;align_corners = True&lt;/code&gt; 입니다. 그 이후로 기본 동작은 &lt;a href=&quot;#torch.nn.functional.interpolate&quot;&gt; &lt;code&gt;interpolate()&lt;/code&gt; &lt;/a&gt; 의 기본값에 &lt;code&gt;align_corners = False&lt;/code&gt; 위해 align_corners = False 로 변경되었습니다 .</target>
        </trans-unit>
        <trans-unit id="19d86416250dfee41099689ccdc474523b9de05a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;compute_uv&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, backward cannot be performed since &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; from the forward pass is required for the backward operation.</source>
          <target state="translated">경우 &lt;code&gt;compute_uv&lt;/code&gt; 가 = &lt;code&gt;False&lt;/code&gt; , 후방 캔 이후 수행 될 &lt;code&gt;U&lt;/code&gt; 및 &lt;code&gt;V&lt;/code&gt; 순방향 패스에서는 역방향 동작에 요구된다.</target>
        </trans-unit>
        <trans-unit id="2e150d7511a776c744a811c73ef08944e9bb6434" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;dim&lt;/code&gt; is given, a squeeze operation is done only in the given dimension. If &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">때 &lt;code&gt;dim&lt;/code&gt; 주어집니다, 스퀴즈 작업은 주어진 차원에서 이루어집니다. 경우 &lt;code&gt;input&lt;/code&gt; 형상이다 :</target>
        </trans-unit>
        <trans-unit id="b1120f7dfd992f23bbea9418ec57a1af2fe9af61" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a scalar value, the operation applied is:</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 스칼라 값이고, 동작은 적용 :</target>
        </trans-unit>
        <trans-unit id="66481b64a8fde71d4312aa50639d9ce101db880a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the operation applied is:</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 텐서이고, 동작은 적용 :</target>
        </trans-unit>
        <trans-unit id="25f39b6b9d40581f69f611ee72f68c18106be3d1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;exponent&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;exponent&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">경우 &lt;code&gt;exponent&lt;/code&gt; 텐서는,의 형태 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;exponent&lt;/code&gt; 있어야 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2903dec45ead9638a2d9f01c40302928711917b1" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;groups == in_channels&lt;/code&gt; and &lt;code&gt;out_channels == K * in_channels&lt;/code&gt;, where &lt;code&gt;K&lt;/code&gt; is a positive integer, this operation is also termed in literature as depthwise convolution.</source>
          <target state="translated">경우 &lt;code&gt;groups == in_channels&lt;/code&gt; 및 &lt;code&gt;out_channels == K * in_channels&lt;/code&gt; , &lt;code&gt;K&lt;/code&gt; 는 양의 정수이고,이 동작은 또한 깊이 방향으로 콘볼 루션 문헌 불린다.</target>
        </trans-unit>
        <trans-unit id="c1b8e1328795e35aa33cb3682bf9e0a128163cf6" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;input&lt;/code&gt; is on CUDA, &lt;a href=&quot;#torch.nonzero&quot;&gt;&lt;code&gt;torch.nonzero()&lt;/code&gt;&lt;/a&gt; causes host-device synchronization.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 이 CUDA에 있을 때 &lt;a href=&quot;#torch.nonzero&quot;&gt; &lt;code&gt;torch.nonzero()&lt;/code&gt; &lt;/a&gt; 는 호스트-장치 동기화를 유발합니다.</target>
        </trans-unit>
        <trans-unit id="9fd8de791261b8fe5e3fd820c55e05ef945d4e88" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;module&lt;/code&gt; returns a scalar (i.e., 0-dimensional tensor) in &lt;code&gt;forward()&lt;/code&gt;, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</source>
          <target state="translated">되면 &lt;code&gt;module&lt;/code&gt; 의 스칼라 (즉, 0 차원 텐서)를 리턴 &lt;code&gt;forward()&lt;/code&gt; ,이 랩퍼는 각 장치의 결과를 포함하는, 데이터를 병렬로 사용되는 장치들의 수와 동일한 길이의 벡터를 리턴한다.</target>
        </trans-unit>
        <trans-unit id="14811e48d579473d679dcda9d3180b5365c2423a" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shape of &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor</source>
          <target state="translated">때 &lt;code&gt;other&lt;/code&gt; 텐서이며, 모양 &lt;code&gt;other&lt;/code&gt; 있어야합니다 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; 기본 텐서의 모양</target>
        </trans-unit>
        <trans-unit id="6d6234d9ff0cd3ff058351e570bff6a3e90e8d1e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;other&lt;/code&gt; is a tensor, the shapes of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;other&lt;/code&gt; 텐서의 모양입니다 &lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; 해야합니다 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b23daf961076c6f771f3b98153804afb65cab2e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;size&lt;/code&gt; is given, it is the output size of the image &lt;code&gt;(h, w)&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;size&lt;/code&gt; 주어, 영상의 출력 크기 &lt;code&gt;(h, w)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d240a83215b54cebb671916345cdadc2427a5a89" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt;, the gradients on &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; and &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</source>
          <target state="translated">경우 &lt;code&gt;some&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 에서 그라디언트 &lt;code&gt;U[..., :, min(m, n):]&lt;/code&gt; 및 &lt;code&gt;V[..., :, min(m, n):]&lt;/code&gt; 이들 벡터로 후방에서 무시 될 것이다 수 부분 공간의 임의의 기준입니다.</target>
        </trans-unit>
        <trans-unit id="85caf16edf789cb76e0cd678a0b93ec725f0105b" translate="yes" xml:space="preserve">
          <source>When a model is trained on &lt;code&gt;M&lt;/code&gt; nodes with &lt;code&gt;batch=N&lt;/code&gt;, the gradient will be &lt;code&gt;M&lt;/code&gt; times smaller when compared to the same model trained on a single node with &lt;code&gt;batch=M*N&lt;/code&gt; (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart.</source>
          <target state="translated">모델이 &lt;code&gt;batch=N&lt;/code&gt; 인 &lt;code&gt;M&lt;/code&gt; 노드에서 학습 된 경우, 단일 노드에서 &lt;code&gt;batch=M*N&lt;/code&gt; 으로 학습 된 동일한 모델과 비교할 때 기울기가 &lt;code&gt;M&lt;/code&gt; 배 더 작아집니다 (다른 노드 간의 기울기가 평균화되기 때문). 로컬 교육과 비교하여 수학적으로 동등한 교육 과정을 얻으려면이 점을 고려해야합니다.</target>
        </trans-unit>
        <trans-unit id="bc06fbb059ebdabdac77c068a81b8512376f9ddd" translate="yes" xml:space="preserve">
          <source>When called with &lt;code&gt;dims&lt;/code&gt; of the list form, the given dimensions will be contracted in place of the last</source>
          <target state="translated">목록 양식의 &lt;code&gt;dims&lt;/code&gt; 부분으로 호출되면 주어진 크기가 마지막 크기 대신 축소됩니다.</target>
        </trans-unit>
        <trans-unit id="8f0991d5226cadceec16a5e2277a3d90d9a71503" translate="yes" xml:space="preserve">
          <source>When called with a non-negative integer argument &lt;code&gt;dims&lt;/code&gt; =</source>
          <target state="translated">음이 아닌 정수 인수로 호출 할 때 &lt;code&gt;dims&lt;/code&gt; =</target>
        </trans-unit>
        <trans-unit id="6abce014373fbb8ba1a8a6700a958f442096752d" translate="yes" xml:space="preserve">
          <source>When combined with TorchScript decorators, this decorator must be the outmost one.</source>
          <target state="translated">TorchScript 데코레이터와 결합 할 때이 데코레이터는 가장 바깥쪽에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="01692078c2de1ab235032349659ea1bad48180a7" translate="yes" xml:space="preserve">
          <source>When combined with static or class method, this decorator must be the inner one.</source>
          <target state="translated">정적 또는 클래스 메소드와 결합 할 때이 데코레이터는 내부 데코레이터 여야합니다.</target>
        </trans-unit>
        <trans-unit id="24460dec44dc8e232523a0b3b6cffc083f18ab5b" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt;&lt;code&gt;new_tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">데이터가 텐서 &lt;code&gt;x&lt;/code&gt; 이면 &lt;a href=&quot;#torch.Tensor.new_tensor&quot;&gt; &lt;code&gt;new_tensor()&lt;/code&gt; &lt;/a&gt; 는 전달 된 데이터에서 '데이터'를 읽고 리프 변수를 생성합니다. 따라서 &lt;code&gt;tensor.new_tensor(x)&lt;/code&gt; 는 &lt;code&gt;x.clone().detach()&lt;/code&gt; 와 &lt;code&gt;tensor.new_tensor(x, requires_grad=True)&lt;/code&gt; 하고 tensor.new_tensor (x, requires_grad = True) 는 &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; 합니다. &lt;code&gt;clone()&lt;/code&gt; 및 &lt;code&gt;detach()&lt;/code&gt; 사용하는 동일한 방법 이 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="44313813a50d98d5898f4bddc561c8a3109b1400" translate="yes" xml:space="preserve">
          <source>When data is a tensor &lt;code&gt;x&lt;/code&gt;, &lt;a href=&quot;#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; reads out &amp;lsquo;the data&amp;rsquo; from whatever it is passed, and constructs a leaf variable. Therefore &lt;code&gt;torch.tensor(x)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach()&lt;/code&gt; and &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; is equivalent to &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt;. The equivalents using &lt;code&gt;clone()&lt;/code&gt; and &lt;code&gt;detach()&lt;/code&gt; are recommended.</source>
          <target state="translated">데이터가 텐서 &lt;code&gt;x&lt;/code&gt; 인 경우 &lt;a href=&quot;#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; 는 전달 된 데이터에서 '데이터'를 읽고 리프 변수를 생성합니다. 따라서 &lt;code&gt;torch.tensor(x)&lt;/code&gt; 는 &lt;code&gt;x.clone().detach()&lt;/code&gt; 와 &lt;code&gt;torch.tensor(x, requires_grad=True)&lt;/code&gt; 하고 torch.tensor (x, requires_grad = True) 는 &lt;code&gt;x.clone().detach().requires_grad_(True)&lt;/code&gt; 합니다. &lt;code&gt;clone()&lt;/code&gt; 및 &lt;code&gt;detach()&lt;/code&gt; 사용하는 동일한 방법 이 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="4d87a694539e44d9bb0922817749f14565db02a2" translate="yes" xml:space="preserve">
          <source>When drawn without replacement, &lt;code&gt;num_samples&lt;/code&gt; must be lower than number of non-zero elements in &lt;code&gt;input&lt;/code&gt; (or the min number of non-zero elements in each row of &lt;code&gt;input&lt;/code&gt; if it is a matrix).</source>
          <target state="translated">대체하지 않고 그릴 때 &lt;code&gt;num_samples&lt;/code&gt; 는 &lt;code&gt;input&lt;/code&gt; 에서 0이 아닌 요소의 수 (또는 행렬 인 경우 &lt;code&gt;input&lt;/code&gt; 각 행에있는 0이 아닌 요소의 최소 수) 보다 작아야 합니다 .</target>
        </trans-unit>
        <trans-unit id="aeab232209859fdb56ecfdd5e27289109b3779cd" translate="yes" xml:space="preserve">
          <source>When given an image of &lt;code&gt;Channels x Height x Width&lt;/code&gt;, it will apply &lt;code&gt;Softmax&lt;/code&gt; to each location</source>
          <target state="translated">&lt;code&gt;Channels x Height x Width&lt;/code&gt; 의 이미지가 주어지면 각 위치에 &lt;code&gt;Softmax&lt;/code&gt; 가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="9635ed5ff58b5f10f0a861bfe4da315b19a3d9ec" translate="yes" xml:space="preserve">
          <source>When manually importing this backend and invoking &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt; with the corresponding backend name, the &lt;code&gt;torch.distributed&lt;/code&gt; package runs on the new backend.</source>
          <target state="translated">이 백엔드를 수동으로 가져오고 해당 백엔드 이름으로 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; 을 호출 하면 &lt;code&gt;torch.distributed&lt;/code&gt; 패키지가 새 백엔드에서 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="6c58952c2db520eb2d0c000e5b5179ffccb22124" translate="yes" xml:space="preserve">
          <source>When passed to the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script&lt;/code&gt;&lt;/a&gt; function, a &lt;code&gt;torch.nn.Module&lt;/code&gt;&amp;rsquo;s data is copied to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; and the TorchScript compiler compiles the module. The module&amp;rsquo;s &lt;code&gt;forward&lt;/code&gt; is compiled by default. Methods called from &lt;code&gt;forward&lt;/code&gt; are lazily compiled in the order they are used in &lt;code&gt;forward&lt;/code&gt;, as well as any &lt;code&gt;@torch.jit.export&lt;/code&gt; methods.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수에 전달되면 &lt;code&gt;torch.nn.Module&lt;/code&gt; 의 데이터가 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 에&lt;/a&gt; 복사되고 TorchScript 컴파일러가 모듈을 컴파일합니다. 모듈의 &lt;code&gt;forward&lt;/code&gt; 는 기본적으로 컴파일됩니다. &lt;code&gt;forward&lt;/code&gt; 에서 호출 된 메서드 는 &lt;code&gt;forward&lt;/code&gt; 및 &lt;code&gt;@torch.jit.export&lt;/code&gt; 메서드 에서 사용되는 순서대로 느리게 컴파일됩니다 .</target>
        </trans-unit>
        <trans-unit id="98975218294f0d2cf1d9e027d0c9f18c335eccc0" translate="yes" xml:space="preserve">
          <source>When running on CUDA, &lt;code&gt;row * col&lt;/code&gt; must be less than</source>
          <target state="translated">CUDA에서 실행할 때 &lt;code&gt;row * col&lt;/code&gt; 은 다음보다 작아야합니다.</target>
        </trans-unit>
        <trans-unit id="6442e147505eb082b7d915b41a9300ebc0cdaab5" translate="yes" xml:space="preserve">
          <source>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</source>
          <target state="translated">scale_factor가 지정되면 recompute_scale_factor = True이면 scale_factor를 사용하여 output_size를 계산 한 다음 보간을위한 새 스케일을 추론하는 데 사용됩니다. recompute_scale_factor의 기본 동작은 1.6.0에서 False로 변경되었으며, 보간 계산에 scale_factor가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6134dc991dfe53224afc4b25750c288358d06db2" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;divisor&lt;/code&gt; tensor contains no zero elements, then &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;unfold&lt;/code&gt; operations are inverses of each other (up to constant divisor).</source>
          <target state="translated">때 &lt;code&gt;divisor&lt;/code&gt; 텐서 더 제로 요소가없는 다음 &lt;code&gt;fold&lt;/code&gt; 와 &lt;code&gt;unfold&lt;/code&gt; 작업 (정수 제수까지) 서로의 역수이다.</target>
        </trans-unit>
        <trans-unit id="ea57553addc94dbd66c0aab4dca41d371d835f2e" translate="yes" xml:space="preserve">
          <source>When the dtypes of inputs to an arithmetic operation (&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;) differ, we promote by finding the minimum dtype that satisfies the following rules:</source>
          <target state="translated">산술 연산 ( &lt;code&gt;add&lt;/code&gt; , &lt;code&gt;sub&lt;/code&gt; , &lt;code&gt;div&lt;/code&gt; , &lt;code&gt;mul&lt;/code&gt; ) 에 대한 입력의 dtype이 다를 경우 다음 규칙을 충족하는 최소 dtype을 찾아 승격합니다.</target>
        </trans-unit>
        <trans-unit id="b36d5c15d9cebdd9841a3cbecf930ef5be3e8fb3" translate="yes" xml:space="preserve">
          <source>When the input Tensor is a sparse tensor then the unspecifed values are treated as &lt;code&gt;-inf&lt;/code&gt;.</source>
          <target state="translated">입력 텐서가 희소 텐서이면 지정되지 않은 값은 &lt;code&gt;-inf&lt;/code&gt; 로 처리됩니다 .</target>
        </trans-unit>
        <trans-unit id="adc8c2e11d80dad257b6a86b35591121148d959c" translate="yes" xml:space="preserve">
          <source>When the shapes do not match, the shape of &lt;a href=&quot;torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; is used as the shape for the returned output tensor</source>
          <target state="translated">모양이 일치하지 않으면 반환 된 출력 텐서 의 모양으로 &lt;a href=&quot;torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt; 의 모양 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7a635c724cadbffe9cd8e217f6606360c059e6c0" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt;&lt;code&gt;BuildExtension&lt;/code&gt;&lt;/a&gt;, it is allowed to supply a dictionary for &lt;code&gt;extra_compile_args&lt;/code&gt; (rather than the usual list) that maps from languages (&lt;code&gt;cxx&lt;/code&gt; or &lt;code&gt;nvcc&lt;/code&gt;) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.cpp_extension.BuildExtension&quot;&gt; &lt;code&gt;BuildExtension&lt;/code&gt; 을&lt;/a&gt; 사용할 때 언어 ( &lt;code&gt;cxx&lt;/code&gt; 또는 &lt;code&gt;nvcc&lt;/code&gt; )에서 컴파일러에 제공 할 추가 컴파일러 플래그 목록으로 매핑되는 &lt;code&gt;extra_compile_args&lt;/code&gt; (일반 목록이 아닌)에 대한 사전 을 제공 할 수 있습니다. 이렇게하면 혼합 컴파일 중에 C ++ 및 CUDA 컴파일러에 다른 플래그를 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7635dd8102bfabbafa2a3b5c97436e5cb54d9ba1" translate="yes" xml:space="preserve">
          <source>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">CUDA 백엔드를 사용할 때이 작업은 쉽게 꺼지지 않는 역방향 패스에서 비 결정적 동작을 유발할 수 있습니다. 배경 은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;재현성에&lt;/a&gt; 대한 참고 사항을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="79561f3cf985a5e4719b8247ff531adef7e29706" translate="yes" xml:space="preserve">
          <source>When writing TorchScript directly using &lt;code&gt;@torch.jit.script&lt;/code&gt; decorator, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. See &lt;code&gt;Builtin Functions&lt;/code&gt; for a complete reference of available Pytorch tensor methods, modules, and functions.</source>
          <target state="translated">&lt;code&gt;@torch.jit.script&lt;/code&gt; 데코레이터를 사용하여 직접 TorchScript를 작성할 때 프로그래머는 TorchScript에서 지원되는 Python의 하위 집합 만 사용해야합니다. 이 섹션에서는 독립 실행 형 언어에 대한 언어 참조 인 것처럼 TorchScript에서 지원되는 내용을 문서화합니다. 이 참조에서 언급되지 않은 Python의 모든 기능은 TorchScript의 일부가 아닙니다. 사용 가능한 Pytorch 텐서 메서드, 모듈 및 함수에 대한 전체 참조는 &lt;code&gt;Builtin Functions&lt;/code&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="50bf66ea03e8d3f7a8a4f3f33c0d423a99cbb251" translate="yes" xml:space="preserve">
          <source>When you call &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; and then &lt;code&gt;load_state_dict()&lt;/code&gt; to avoid GPU RAM surge when loading a model checkpoint.</source>
          <target state="translated">GPU 텐서를 포함하는 파일에서 &lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; 를 호출하면 해당 텐서가 기본적으로 GPU에로드됩니다. 당신은 호출 할 수 있습니다 &lt;code&gt;torch.load(.., map_location='cpu')&lt;/code&gt; 다음 &lt;code&gt;load_state_dict()&lt;/code&gt; 모델의 체크 포인트를로드 할 때 피하기 GPU의 RAM의 급증을.</target>
        </trans-unit>
        <trans-unit id="d4b9a73359398be175a62d4f2cbf23eca1eb7394" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 FFT 크기입니다. 동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환 사이 에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="540833bd1b024f45b94c5fe10bdcce398da41e2a" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical FFT size. Calling the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 FFT 크기입니다. 동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="4fa609678777cb0f1d2bea5c36804f3e7f699ab3" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 IFFT 크기입니다. 동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="983301015460563605882b2d43f60b6f31ca13f7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;n = prod(s)&lt;/code&gt; is the logical IFFT size. Calling the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">여기서 &lt;code&gt;n = prod(s)&lt;/code&gt; 는 논리적 IFFT 크기입니다. 동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환 사이 에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; 을 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="dcabf659c01386b96c4252ad5247bb9f50b66574" translate="yes" xml:space="preserve">
          <source>Where are my downloaded models saved?</source>
          <target state="translated">다운로드 한 모델은 어디에 저장됩니까?</target>
        </trans-unit>
        <trans-unit id="017d4a3464bb8236ca4dfe2a53c2f262d76de8ab" translate="yes" xml:space="preserve">
          <source>Which backend to use?</source>
          <target state="translated">사용할 백엔드는 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="3631991286d3139c1de9a41ef4982f8e5d886ce2" translate="yes" xml:space="preserve">
          <source>Which produces:</source>
          <target state="translated">어느 생산 :</target>
        </trans-unit>
        <trans-unit id="a68a67a970d91d390715c4a5723442211582200e" translate="yes" xml:space="preserve">
          <source>While Loops</source>
          <target state="translated">While 루프</target>
        </trans-unit>
        <trans-unit id="48f3d5bde3297b3b923d0f7759d754056d7f6c40" translate="yes" xml:space="preserve">
          <source>While it is assumed that &lt;code&gt;A&lt;/code&gt; is symmetric, &lt;code&gt;A.grad&lt;/code&gt; is not. To make sure that &lt;code&gt;A.grad&lt;/code&gt; is symmetric, so that &lt;code&gt;A - t * A.grad&lt;/code&gt; is symmetric in first-order optimization routines, prior to running &lt;code&gt;lobpcg&lt;/code&gt; we do the following symmetrization map: &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt;. The map is performed only when the &lt;code&gt;A&lt;/code&gt; requires gradients.</source>
          <target state="translated">&lt;code&gt;A&lt;/code&gt; 가 대칭 이라고 가정하지만 &lt;code&gt;A.grad&lt;/code&gt; 는 그렇지 않습니다. &lt;code&gt;A.grad&lt;/code&gt; 가 대칭 인지 확인하기 위해 &lt;code&gt;A - t * A.grad&lt;/code&gt; 가 1 차 최적화 루틴에서 대칭이되도록 &lt;code&gt;lobpcg&lt;/code&gt; 를 실행하기 전에 다음 대칭 맵을 수행합니다. &lt;code&gt;A -&amp;gt; (A + A.t()) / 2&lt;/code&gt; . 맵은 &lt;code&gt;A&lt;/code&gt; 에 그라디언트가 필요할 때만 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="f69597c76f979c6f47613f7588234a4093ae500a" translate="yes" xml:space="preserve">
          <source>While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.</source>
          <target state="translated">항상 유효한 분해를 제공해야하지만 플랫폼간에 동일한 분해를 제공하지 않을 수 있습니다. 이는 LAPACK 구현에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="90f928cf1ec6eb30bffa5d56a4ee00f33a26ccf6" translate="yes" xml:space="preserve">
          <source>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</source>
          <target state="translated">수학적으로 log (softmax (x))와 동일하지만이 두 작업을 개별적으로 수행하는 것은 더 느리고 수치 적으로 불안정합니다. 이 함수는 대체 공식을 사용하여 출력과 기울기를 올바르게 계산합니다.</target>
        </trans-unit>
        <trans-unit id="adfc4c1cf043279d74d69b96592d62572f9a8648" translate="yes" xml:space="preserve">
          <source>Wide ResNet</source>
          <target state="translated">넓은 ResNet</target>
        </trans-unit>
        <trans-unit id="eec7e605ef42b4a1da3f9a4494e1414462efb313" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2</source>
          <target state="translated">넓은 ResNet-101-2</target>
        </trans-unit>
        <trans-unit id="ac86db1353b797ed1c0d73605a56146ab6cb1914" translate="yes" xml:space="preserve">
          <source>Wide ResNet-101-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&quot;Wide Residual Networks&quot;의&lt;/a&gt; Wide ResNet-101-2 모델</target>
        </trans-unit>
        <trans-unit id="b631c349f7132ef6c2a8879b09c961b30fce8aba" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2</source>
          <target state="translated">넓은 ResNet-50-2</target>
        </trans-unit>
        <trans-unit id="726f31b3e4c15ecbbc898cae3f9e8f73a3460020" translate="yes" xml:space="preserve">
          <source>Wide ResNet-50-2 model from &lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&amp;ldquo;Wide Residual Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/pdf/1605.07146.pdf&quot;&gt;&quot;Wide Residual Networks&quot;의&lt;/a&gt; Wide ResNet-50-2 모델</target>
        </trans-unit>
        <trans-unit id="d2b826d3f7d8e2201135c671569ea283afb245af" translate="yes" xml:space="preserve">
          <source>Will result in:</source>
          <target state="translated">결과는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8fa871c4385dea4733bc80aea73b5f5364e4ef06" translate="yes" xml:space="preserve">
          <source>Windows FAQ</source>
          <target state="translated">Windows FAQ</target>
        </trans-unit>
        <trans-unit id="8ac66d0e68a85c8d292fec91f6cbcf0bd809b4e9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;bilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;bilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt; 샘플을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="05c8bfe5d3e24898d75d1b114e7f0550313f9961" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See below for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;linear&lt;/code&gt; , &lt;code&gt;bilinear&lt;/code&gt; , &lt;code&gt;bicubic&lt;/code&gt; , 및 &lt;code&gt;trilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 아래를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="9b1242167a707b16c47db2fe5f3c0130b4361833" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;align_corners = True&lt;/code&gt;, the linearly interpolating modes (&lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;bilinear&lt;/code&gt;, and &lt;code&gt;trilinear&lt;/code&gt;) don&amp;rsquo;t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is &lt;code&gt;align_corners = False&lt;/code&gt;. See &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt;&lt;code&gt;Upsample&lt;/code&gt;&lt;/a&gt; for concrete examples on how this affects the outputs.</source>
          <target state="translated">함께 &lt;code&gt;align_corners = True&lt;/code&gt; , 직선 보간 모드 ( &lt;code&gt;linear&lt;/code&gt; , &lt;code&gt;bilinear&lt;/code&gt; 및 &lt;code&gt;trilinear&lt;/code&gt; ) 비례 출력 및 입력 픽셀 정렬되지 않으며, 따라서 출력 값은 입력 크기에 의존 할 수있다. 이것은 버전 0.3.1까지 이러한 모드의 기본 동작이었습니다. 그 이후로 기본 동작은 &lt;code&gt;align_corners = False&lt;/code&gt; 입니다. 이것이 출력에 미치는 영향에 대한 구체적인 예는 &lt;a href=&quot;generated/torch.nn.upsample#torch.nn.Upsample&quot;&gt; &lt;code&gt;Upsample&lt;/code&gt; &lt;/a&gt; 샘플을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="343e3c563441d98e78b2af38a8ee16b2107d4b5b" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;mode='bicubic'&lt;/code&gt;, it&amp;rsquo;s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; if you want to reduce the overshoot when displaying the image.</source>
          <target state="translated">로 &lt;code&gt;mode='bicubic'&lt;/code&gt; , 이것은 오버 슈트의 원인 말하면 음수 값 또는 값보다 큰 이미지를 255 생성 할 수있다. 이미지를 표시 할 때 오버 슈트를 줄이려면 명시 적으로 &lt;code&gt;result.clamp(min=0, max=255)&lt;/code&gt; 호출 하십시오.</target>
        </trans-unit>
        <trans-unit id="670f4308b71dfa44f0ce3d603252c9a80431b7a1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;padding_idx&lt;/code&gt; set, the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from &lt;a href=&quot;#torch.nn.Embedding&quot;&gt;&lt;code&gt;Embedding&lt;/code&gt;&lt;/a&gt; is always zero.</source>
          <target state="translated">&lt;code&gt;padding_idx&lt;/code&gt; 를 설정 하면 padding_idx 의 임베딩 벡터 &lt;code&gt;padding_idx&lt;/code&gt; 모두 0으로 초기화됩니다. 그러나이 벡터는 사용자 정의 된 초기화 방법을 사용하여 나중에 수정할 수 있으므로 출력을 채우는 데 사용되는 벡터를 변경할 수 있습니다. &lt;a href=&quot;#torch.nn.Embedding&quot;&gt; &lt;code&gt;Embedding&lt;/code&gt; &lt;/a&gt; 의이 벡터에 대한 기울기 는 항상 0입니다.</target>
        </trans-unit>
        <trans-unit id="245747fecda85d5b71d626b9e2eb04627690bcb2" translate="yes" xml:space="preserve">
          <source>With &lt;em&gt;trace-based&lt;/em&gt; exporter, we get the result ONNX graph which unrolls the for loop:</source>
          <target state="translated">&lt;em&gt;추적 기반&lt;/em&gt; 내보내기를 사용 하면 for 루프를 펼치는 결과 ONNX 그래프를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="19fffb59deac1debbb0ed0b605bbe04aebbddb29" translate="yes" xml:space="preserve">
          <source>With the default arguments it uses the Euclidean norm over vectors along dimension</source>
          <target state="translated">기본 인수를 사용하면 차원을 따라 벡터에 대한 유클리드 노름을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="67793521614b6b44f958c708ad8988aeadfd2309" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length in the last dimension:</source>
          <target state="translated">출력 길이를 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 지정하지 않으면 입력이 마지막 차원에서 홀수 길이이기 때문에 출력이 제대로 왕복되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4263d953681d9ae30565f30202d1492e42e9e8ac" translate="yes" xml:space="preserve">
          <source>Without specifying the output length to &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output will not round-trip properly because the input is odd-length:</source>
          <target state="translated">출력 길이를 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 지정하지 않으면 입력이 홀수 길이이기 때문에 출력이 제대로 왕복되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="5375f310fdbbbaceba22e923b5eb13a5f74fab7f" translate="yes" xml:space="preserve">
          <source>Wrapper around a &lt;code&gt;torch._C.Future&lt;/code&gt; which encapsulates an asynchronous execution of a callable, e.g. &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;. It also exposes a set of APIs to add callback functions and set results.</source>
          <target state="translated">호출 가능의 비동기 실행을 캡슐화하는 &lt;code&gt;torch._C.Future&lt;/code&gt; 주위의 래퍼 , 예 : &lt;a href=&quot;rpc#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt; . 또한 콜백 함수를 추가하고 결과를 설정하는 API 세트를 노출합니다.</target>
        </trans-unit>
        <trans-unit id="65acb3d4bdbad8b7e153886b6b2199ad76a756c8" translate="yes" xml:space="preserve">
          <source>Wrapper class for quantized operations.</source>
          <target state="translated">양자화 된 작업을위한 래퍼 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="2a31ffa99fd6d35b029d1f439707a5bd7c9b4da8" translate="yes" xml:space="preserve">
          <source>Writes all values from the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor. For each value in &lt;code&gt;src&lt;/code&gt;, its output index is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">텐서에서 모든 값을 기록 &lt;code&gt;src&lt;/code&gt; 에 &lt;code&gt;self&lt;/code&gt; 에 지정된 인덱스에서 &lt;code&gt;index&lt;/code&gt; 텐서. 의 각 값에 대해 &lt;code&gt;src&lt;/code&gt; , 출력 인덱스는 인덱스에 의해 지정되는 &lt;code&gt;src&lt;/code&gt; 을위한 &lt;code&gt;dimension != dim&lt;/code&gt; 과의 대응 값 &lt;code&gt;index&lt;/code&gt; 에 대한 &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5adf3c6baa6091132b20eef0ff93f0984499afa5" translate="yes" xml:space="preserve">
          <source>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</source>
          <target state="translated">텐서 보드가 사용할 log_dir의 이벤트 파일에 직접 항목을 씁니다.</target>
        </trans-unit>
        <trans-unit id="c032adc1ff629c9b66f22749ad667e6beadf144b" translate="yes" xml:space="preserve">
          <source>X</source>
          <target state="translated">X</target>
        </trans-unit>
        <trans-unit id="29822901b44f81c6464586704f28915142f932bc" translate="yes" xml:space="preserve">
          <source>X (Tensor): tensor of eigenvectors of size</source>
          <target state="translated">X (텐서) : 크기의 고유 벡터의 텐서</target>
        </trans-unit>
        <trans-unit id="030b21a9a5ef307806a06b78be434c1972173393" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = X ^ * [N_1-\ omega_1, \ dots, N_d-\ omega_d],</target>
        </trans-unit>
        <trans-unit id="ed1d6f505827ea7067cb8c300c0904dbfa5fbd7a" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = \ frac {1} {\ prod_ {i = 1} ^ d N_i} \ sum_ {n_1 = 0} ^ {N_1-1} \ dots \ sum_ {n_d = 0 } ^ {N_d-1} x [n_1, \ dots, n_d] e ^ {\ j \ 2 \ pi \ sum_ {i = 0} ^ d \ frac {\ omega_i n_i} {N_i}},</target>
        </trans-unit>
        <trans-unit id="746d94247ac643f86d1ee17dab642515e2ff9085" translate="yes" xml:space="preserve">
          <source>X[\omega_1, \dots, \omega_d] = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d] e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</source>
          <target state="translated">X [\ omega_1, \ dots, \ omega_d] = \ sum_ {n_1 = 0} ^ {N_1-1} \ dots \ sum_ {n_d = 0} ^ {N_d-1} x [n_1, \ dots, n_d] e ^ {-j \ 2 \ pi \ sum_ {i = 0} ^ d \ frac {\ omega_i n_i} {N_i}},</target>
        </trans-unit>
        <trans-unit id="a14f6ead4d7bc2b1f74caf9a0cc9b1f3d45fbb66" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = X[m, \text{n\_fft} - \omega]^*</source>
          <target state="translated">X [m, \ omega] = X [m, \ text {n \ _fft}-\ omega] ^ *</target>
        </trans-unit>
        <trans-unit id="4e6854a6a1b525f353a271b3c9b646d1b5c28d3f" translate="yes" xml:space="preserve">
          <source>X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}% \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ % \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</source>
          <target state="translated">X [m, \ omega] = \ sum_ {k = 0} ^ {\ text {win \ _length-1}} % \ text {window} [k] \ \ text {input} [m \ times \ text {hop \ _length} + k] \ % \ exp \ left (-j \ frac {2 \ pi \ cdot \ omega k} {\ text {win \ _length}} \ right),</target>
        </trans-unit>
        <trans-unit id="14ebad2cda6dcca683250b526859348822d9d5c8" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. E.g.:</source>
          <target state="translated">예, 현재 ONNX opset 버전&amp;gt; = 11에 대해 지원됩니다. 예 :</target>
        </trans-unit>
        <trans-unit id="5684cedf8d1212771db72a004af08859ca152372" translate="yes" xml:space="preserve">
          <source>Yes, this is supported now for ONNX opset version &amp;gt;= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:</source>
          <target state="translated">예, 현재 ONNX opset 버전&amp;gt; = 11에 대해 지원됩니다. ONNX는 opset 11에 시퀀스 개념을 도입했습니다. 목록과 유사하게 시퀀스는 임의 개수의 Tensor를 포함하는 데이터 유형입니다. SequenceInsert, SequenceAt 등과 같은 관련 연산자도 ONNX에 도입되었습니다. 그러나 루프 내 내부 목록 추가는 ONNX로 내보낼 수 없습니다. 이를 구현하려면 inplace add 연산자를 사용하십시오. 예 :</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="7c19fb2e314a3137e93fb758f531465aacc3456b" translate="yes" xml:space="preserve">
          <source>You can also construct hybrid sparse tensors, where only the first n dimensions are sparse, and the rest of the dimensions are dense.</source>
          <target state="translated">또한 처음 n 개의 차원 만 희소하고 나머지 차원은 조밀 한 하이브리드 희소 텐서를 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="d617602dcc62272743d7a795be368e5ffbe5433d" translate="yes" xml:space="preserve">
          <source>You can also run the exported model with &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX Runtime&lt;/a&gt;, you will need to install &lt;code&gt;ONNX Runtime&lt;/code&gt;: please &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;follow these instructions&lt;/a&gt;.</source>
          <target state="translated">당신은 또한에 내 보낸 모델을 실행할 수 있습니다 &lt;a href=&quot;https://github.com/microsoft/onnxruntime&quot;&gt;ONNX 런타임을&lt;/a&gt; , 당신은 설치해야합니다 &lt;code&gt;ONNX Runtime&lt;/code&gt; : 제발 &lt;a href=&quot;https://github.com/microsoft/onnxruntime#installation&quot;&gt;다음 지침을 따르십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c8c75fa187b181f202978d8a86ab28cca3e9eb36" translate="yes" xml:space="preserve">
          <source>You can also verify the protobuf using the &lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; library. You can install &lt;code&gt;ONNX&lt;/code&gt; with conda:</source>
          <target state="translated">&lt;a href=&quot;https://github.com/onnx/onnx/&quot;&gt;ONNX&lt;/a&gt; 라이브러리를 사용하여 protobuf를 확인할 수도 있습니다. &lt;code&gt;ONNX&lt;/code&gt; 하여 ONNX를 설치할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ec539497ce517be7d2c9354eae2288ab7aed0f7d" translate="yes" xml:space="preserve">
          <source>You can construct a model with random weights by calling its constructor:</source>
          <target state="translated">생성자를 호출하여 무작위 가중치로 모델을 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="762890e45c5b225da275ae73984ed756702605c8" translate="yes" xml:space="preserve">
          <source>You should never try to change your model&amp;rsquo;s parameters after wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Because, when wrapping up your model with &lt;code&gt;DistributedDataParallel&lt;/code&gt;, the constructor of &lt;code&gt;DistributedDataParallel&lt;/code&gt; will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model&amp;rsquo;s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.</source>
          <target state="translated">&lt;code&gt;DistributedDataParallel&lt;/code&gt; 로 모델을 래핑 한 후에는 모델의 매개 변수를 변경해서는 안됩니다 . 와 모델 마무리 할 때, 때문에 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 을 ,의 생성자 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 은 건축 당시의 모델 자체의 모든 매개 변수에 추가 기울기 감소 기능을 등록합니다. 나중에 모델의 매개 변수를 변경하면 그래디언트 중복 함수가 더 이상 올바른 매개 변수 세트와 일치하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ccd4aee0db1253438f0c3a9d3c74816b28340f08" translate="yes" xml:space="preserve">
          <source>You&amp;rsquo;ll generally want to use &lt;a href=&quot;torch.qr#torch.qr&quot;&gt;&lt;code&gt;torch.qr()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">일반적으로 대신 &lt;a href=&quot;torch.qr#torch.qr&quot;&gt; &lt;code&gt;torch.qr()&lt;/code&gt; &lt;/a&gt; 을 사용하고 싶을 것입니다.</target>
        </trans-unit>
        <trans-unit id="f259809289921b4be91b7ff0e29bbdb10551660f" translate="yes" xml:space="preserve">
          <source>Your models should also subclass this class.</source>
          <target state="translated">모델도이 클래스를 하위 클래스로 분류해야합니다.</target>
        </trans-unit>
        <trans-unit id="93d450611dc79948223d0cdb9f4a99610848c9d6" translate="yes" xml:space="preserve">
          <source>ZeroPad2d</source>
          <target state="translated">ZeroPad2d</target>
        </trans-unit>
        <trans-unit id="90bc5acc0a3b091bfe56eb668e9c84ac53428130" translate="yes" xml:space="preserve">
          <source>[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]]</source>
          <target state="translated">[* \ times \ text {normalized \ _shape} [0] \ times \ text {normalized \ _shape} [1] \ times \ ldots \ times \ text {normalized \ _shape} [-1]]</target>
        </trans-unit>
        <trans-unit id="b61210c8825bb88613c060ba48aae051333bd30a" translate="yes" xml:space="preserve">
          <source>[-1, 1]</source>
          <target state="translated">[-1, 1]</target>
        </trans-unit>
        <trans-unit id="ae17aa1eaf46c89eecaf929c74d8fda9a55db49b" translate="yes" xml:space="preserve">
          <source>[0, 1)</source>
          <target state="translated">[0, 1)</target>
        </trans-unit>
        <trans-unit id="c49d95c97e6b97b46f0fa87c4c3d5517b2dd2ac1" translate="yes" xml:space="preserve">
          <source>[0, C-1]</source>
          <target state="translated">[0, C-1]</target>
        </trans-unit>
        <trans-unit id="c00ab2adc7412d84a0750550969f7439fdbed02f" translate="yes" xml:space="preserve">
          <source>[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;code&gt;code&lt;/code&gt;. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant&amp;rsquo;s values.</source>
          <target state="translated">[0] &lt;code&gt;forward&lt;/code&gt; 메서드에 대한 내부 그래프의 예쁘게 인쇄 된 표현 (유효한 Python 구문)입니다 . &lt;code&gt;code&lt;/code&gt; 참조하십시오 . [1] [0] 출력의 CONSTANT.cN 형식을 따르는 ConstMap. [0] 출력의 인덱스는 기본 상수 값의 키입니다.</target>
        </trans-unit>
        <trans-unit id="bc47f02dcecd076fd210f14fb47e8772c796bc8d" translate="yes" xml:space="preserve">
          <source>[1] D. W. Griffin and J. S. Lim, &amp;ldquo;Signal estimation from modified short-time Fourier transform,&amp;rdquo; IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.</source>
          <target state="translated">[1] DW Griffin과 JS Lim,&amp;ldquo;수정 된 단시간 푸리에 변환으로부터 신호 추정&amp;rdquo;, IEEE Trans. ASSP, vol.32, no.2, pp.236-243, 1984 년 4 월.</target>
        </trans-unit>
        <trans-unit id="9390898997c65a41ca007de79555abfa5f0ba5df" translate="yes" xml:space="preserve">
          <source>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</source>
          <target state="translated">[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) LOBPCG의 강력하고 효율적인 구현. SIAM J. Sci. 계산, 40 (5), C655-C676. (22 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/17M1129830&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/17M1129830&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08253d232026d76249c2db42c2940e24ba3bacf" translate="yes" xml:space="preserve">
          <source>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</source>
          <target state="translated">[Knyazev2001] Andrew V. Knyazev. (2001) 최적의 사전 조건화 된 고유 솔버를 향해 : 국부적으로 최적의 블록 사전 조건화 된 켤레 기울기 방법. SIAM J. Sci. 계산, 23 (2), 517-541. (25 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&quot;&gt;https://epubs.siam.org/doi/abs/10.1137/S1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ff64b4c2d43518023be089f31430ef90034f605e" translate="yes" xml:space="preserve">
          <source>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</source>
          <target state="translated">[StathopoulosEtal2002] Andreas Stathopoulos와 Kesheng Wu. (2002) 일정한 동기화 요구 사항이있는 블록 직교 화 절차. SIAM J. Sci. Comput., 23 (6), 2165-2182. (18 페이지) &lt;a href=&quot;https://epubs.siam.org/doi/10.1137/S1064827500370883&quot;&gt;https://epubs.siam.org/doi/10.1137/S1064827500370883&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1e9b6692d359e3ebb28163afd3f06ce34d6b2df" translate="yes" xml:space="preserve">
          <source>[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(&amp;lsquo;A&amp;rsquo;, &amp;lsquo;B1&amp;rsquo;, &amp;lsquo;B2&amp;rsquo;))</source>
          <target state="translated">[[-1.1819, -0.8899], [1.5813, 0.2274]]], 이름 = ( 'A', 'B1', 'B2'))</target>
        </trans-unit>
        <trans-unit id="38ac8c676253501b3fb6819ff9373ce6cfe5b055" translate="yes" xml:space="preserve">
          <source>\ ^*</source>
          <target state="translated">\ ^ *</target>
        </trans-unit>
        <trans-unit id="80a1cbf9743b017c73b23f243284395f0c518670" translate="yes" xml:space="preserve">
          <source>\Gamma(\cdot)</source>
          <target state="translated">\Gamma(\cdot)</target>
        </trans-unit>
        <trans-unit id="15e33ea71862e7b6632cf90aebb8ab938c698f9d" translate="yes" xml:space="preserve">
          <source>\Phi(x)</source>
          <target state="translated">\Phi(x)</target>
        </trans-unit>
        <trans-unit id="2a6c118642891e41137cb68e1346d34dabbd128c" translate="yes" xml:space="preserve">
          <source>\Vert x \Vert _p = \left( \sum_{i=1}^n \vert x_i \vert ^ p \right) ^ {1/p}.</source>
          <target state="translated">\ Vert x \ Vert _p = \ left (\ sum_ {i = 1} ^ n \ vert x_i \ vert ^ p \ right) ^ {1 / p}.</target>
        </trans-unit>
        <trans-unit id="f7c665b45932a814215e979bc2611080b4948e68" translate="yes" xml:space="preserve">
          <source>\alpha</source>
          <target state="translated">\alpha</target>
        </trans-unit>
        <trans-unit id="1eafe1d2e67a6ccfd6c43fcc6a7aba05feb684c2" translate="yes" xml:space="preserve">
          <source>\alpha = 1.6732632423543772848170429916717</source>
          <target state="translated">\ alpha = 1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="cbe1c5b26860bdd639e58977c2072d9b65aece25" translate="yes" xml:space="preserve">
          <source>\alpha=1.6732632423543772848170429916717</source>
          <target state="translated">\alpha=1.6732632423543772848170429916717</target>
        </trans-unit>
        <trans-unit id="eef3f6c4a900c50c6ccae139e6900e18ddc9222e" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times d + k, \text{stride[1]} \times h + m, \text{stride[2]} \times w + n) \end{aligned}</source>
          <target state="translated">\ begin {정렬} \ text {out} (N_i, C_j, d, h, w) = {} &amp;amp; \ max_ {k = 0, \ ldots, kD-1} \ max_ {m = 0, \ ldots, kH -1} \ max_ {n = 0, \ ldots, kW-1} \\ &amp;amp; \ text {input} (N_i, C_j, \ text {stride [0]} \ times d + k, \ text {stride [1 ]} \ times h + m, \ text {stride [2]} \ times w + n) \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="3444423e4aa6a27dc2d4e17fde867139b4f23a7b" translate="yes" xml:space="preserve">
          <source>\begin{aligned} \text{out}(N_i, C_j, d, h, w) ={} &amp;amp; \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\ &amp;amp; \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k, \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)} {kD \times kH \times kW} \end{aligned}</source>
          <target state="translated">\ begin {정렬} \ text {out} (N_i, C_j, d, h, w) = {} &amp;amp; \ sum_ {k = 0} ^ {kD-1} \ sum_ {m = 0} ^ {kH-1 } \ sum_ {n = 0} ^ {kW-1} \\ &amp;amp; \ frac {\ text {input} (N_i, C_j, \ text {stride} [0] \ times d + k, \ text {stride} [ 1] \ times h + m, \ text {stride} [2] \ times w + n)} {kD \ times kH \ times kW} \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="9b5fbddfbbd75f7006abb89ad6964ec0f13538c7" translate="yes" xml:space="preserve">
          <source>\begin{aligned} out(N_i, C_j, h, w) ={} &amp;amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\ &amp;amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m, \text{stride[1]} \times w + n) \end{aligned}</source>
          <target state="translated">\ begin {aligned} out (N_i, C_j, h, w) = {} &amp;amp; \ max_ {m = 0, \ ldots, kH-1} \ max_ {n = 0, \ ldots, kW-1} \\ &amp;amp; \ text {input} (N_i, C_j, \ text {stride [0]} \ times h + m, \ text {stride [1]} \ times w + n) \ end {aligned}</target>
        </trans-unit>
        <trans-unit id="4ea19e5cef880f790f54322d49c339a67ef7eba6" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \\ i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\ f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\ g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\ o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\ c_t = f_t \odot c_{t-1} + i_t \odot g_t \\ h_t = o_t \odot \tanh(c_t) \\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} \\ i_t = \ sigma (W_ {ii} x_t + b_ {ii} + W_ {hi} h_ {t-1} + b_ {hi}) \\ f_t = \ sigma (W_ {if} x_t + b_ {if} + W_ {hf} h_ {t-1} + b_ {hf}) \\ g_t = \ tanh (W_ {ig} x_t + b_ {ig} + W_ {hg} h_ { t-1} + b_ {hg}) \\ o_t = \ sigma (W_ {io} x_t + b_ {io} + W_ {ho} h_ {t-1} + b_ {ho}) \\ c_t = f_t \ odot c_ {t-1} + i_t \ odot g_t \\ h_t = o_t \ odot \ tanh (c_t) \\ \ end {array}</target>
        </trans-unit>
        <trans-unit id="6b6cb34a158b8201fbf48f50465b1ece00a7eb5f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|AX-B\|_2. \end{array}</source>
          <target state="translated">\ begin {array} {ll} \ min_X &amp;amp; \ | AX-B \ | _2. \ end {배열}</target>
        </trans-unit>
        <trans-unit id="9312cff9d66b9a6753766d98374f8fbc6cf98e2f" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} \min_X &amp;amp; \|X\|_2 &amp;amp; \text{subject to} &amp;amp; AX = B. \end{array}</source>
          <target state="translated">\ begin {array} {ll} \ min_X &amp;amp; \ | X \ | _2 &amp;amp; \ text {subject to} &amp;amp; AX = B. \ end {array}</target>
        </trans-unit>
        <trans-unit id="930f69e97734f3784bc6b17a4919f47bb6fa5660" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\ f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\ g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\ o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\ c' = f * c + i * g \\ h' = o * \tanh(c') \\ \end{array}</source>
          <target state="translated">\ begin {array} {ll} i = \ sigma (W_ {ii} x + b_ {ii} + W_ {hi} h + b_ {hi}) \\ f = \ sigma (W_ {if} x + b_ { if} + W_ {hf} h + b_ {hf}) \\ g = \ tanh (W_ {ig} x + b_ {ig} + W_ {hg} h + b_ {hg}) \\ o = \ sigma ( W_ {io} x + b_ {io} + W_ {ho} h + b_ {ho}) \\ c '= f * c + i * g \\ h'= o * \ tanh (c ') \\ \ end {array}</target>
        </trans-unit>
        <trans-unit id="e6bea3b60c648ba0107ad5a273553cd4d585a9b3" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\ z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\ n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\ h' = (1 - z) * n + z * h \end{array}</source>
          <target state="translated">\ begin {array} {ll} r = \ sigma (W_ {ir} x + b_ {ir} + W_ {hr} h + b_ {hr}) \\ z = \ sigma (W_ {iz} x + b_ { iz} + W_ {hz} h + b_ {hz}) \\ n = \ tanh (W_ {in} x + b_ {in} + r * (W_ {hn} h + b_ {hn})) \\ h '= (1-z) * n + z * h \ end {array}</target>
        </trans-unit>
        <trans-unit id="e41fd097ab91cc853c05e5d0d9e0d37ff3dd7662" translate="yes" xml:space="preserve">
          <source>\begin{array}{ll} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \end{array}</source>
          <target state="translated">\ begin {array} {ll} r_t = \ sigma (W_ {ir} x_t + b_ {ir} + W_ {hr} h _ {(t-1)} + b_ {hr}) \\ z_t = \ sigma (W_ {iz} x_t + b_ {iz} + W_ {hz} h _ {(t-1)} + b_ {hz}) \\ n_t = \ tanh (W_ {in} x_t + b_ {in} + r_t * (W_ {hn} h _ {(t-1)} + b_ {hn})) \\ h_t = (1-z_t) * n_t + z_t * h _ {(t-1)} \ end {array}</target>
        </trans-unit>
        <trans-unit id="6499d503bfc00cadae1440b191c52a8632e2f8c4" translate="yes" xml:space="preserve">
          <source>\beta</source>
          <target state="translated">\beta</target>
        </trans-unit>
        <trans-unit id="6ceee6706c4f97ef457cd65c6ed19732a4a4c7e0" translate="yes" xml:space="preserve">
          <source>\delta^{(l-1)}_t</source>
          <target state="translated">\delta^{(l-1)}_t</target>
        </trans-unit>
        <trans-unit id="986673ab936df8ce1454d7dc93297bd7bbc51c4b" translate="yes" xml:space="preserve">
          <source>\ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">\ ell (a, p, n) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_i = \ max \ {d (a_i, p_i)-d (a_i, n_i) + {\ rm 마진}, 0 \}</target>
        </trans-unit>
        <trans-unit id="4d38ace96b9720e15bef838db0ce85ef5b456092" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_n \ left [y_n \ cdot \ log \ sigma (x_n) + (1-y_n) \ cdot \ log (1-\ sigma (x_n)) \ right],</target>
        </trans-unit>
        <trans-unit id="ec2ad56a41af8d7cfc9a26f61f048cf6cbd7e78d" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_n \ left [y_n \ cdot \ log x_n + (1-y_n) \ cdot \ log ( 1-x_n) \ 오른쪽],</target>
        </trans-unit>
        <trans-unit id="de7b2379237fe388c3459990f0a508ab1bd86588" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n =-w_ {y_n} x_ {n, y_n}, \ quad w_ {c} = \ text { weight} [c] \ cdot \ mathbb {1} \ {c \ not = \ text {ignore \ _index} \},</target>
        </trans-unit>
        <trans-unit id="f6a58890a57b18854cfa1707f659a4fd6ff6e4cb" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2,</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n = \ left (x_n-y_n \ right) ^ 2,</target>
        </trans-unit>
        <trans-unit id="bbe073a5e1335cf9e8392124ee3bc939a82d8704" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right|,</source>
          <target state="translated">\ ell (x, y) = L = \ {l_1, \ dots, l_N \} ^ \ top, \ quad l_n = \ left | x_n-y_n \ 오른쪽 |,</target>
        </trans-unit>
        <trans-unit id="295ee7beff5d61b430ac876642e47ec1ac559795" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text { 'mean';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="4d5ca23779a198f768fff42eaa5060e03279277c" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text { 'mean';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="3c1589dd2fb19090831cbb7f520baedd8d65a3e4" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \operatorname{mean}(L), &amp;amp; \text{if reduction} = \text{`mean';}\\ \operatorname{sum}(L), &amp;amp; \text{if reduction} = \text{`sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ operatorname {mean} (L), &amp;amp; \ text {if reduction} = \ text {`mean ';} \\ \ operatorname {sum} (L), &amp;amp; \ text {if reduction} = \ text {`sum '.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="8ba1942b6f9513bd39cc8cb1f1c52037ed6d49d3" translate="yes" xml:space="preserve">
          <source>\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;amp; \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, &amp;amp; \text{if reduction} = \text{'sum'.} \end{cases}</source>
          <target state="translated">\ ell (x, y) = \ begin {cases} \ sum_ {n = 1} ^ N \ frac {1} {\ sum_ {n = 1} ^ N w_ {y_n}} l_n 및 \ text {축소 인 경우 } = \ text { 'mean';} \\ \ sum_ {n = 1} ^ N l_n, &amp;amp; \ text {if reduction} = \ text { 'sum'.} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="5a3a211a3bdfba5d642037c3946f715f2ce73187" translate="yes" xml:space="preserve">
          <source>\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c}) + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</source>
          <target state="translated">\ ell_c (x, y) = L_c = \ {l_ {1, c}, \ dots, l_ {N, c} \} ^ \ top, \ quad l_ {n, c} =-w_ {n, c} \ left [p_c y_ {n, c} \ cdot \ log \ sigma (x_ {n, c}) + (1-y_ {n, c}) \ cdot \ log (1-\ sigma (x_ {n, c })) \권리],</target>
        </trans-unit>
        <trans-unit id="c04fbcdd9308d49d4c2300f2ebbb1e83c44e8b83" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target} * \text{input}</source>
          <target state="translated">\ exp (\ text {input})-\ text {target} * \ text {input}</target>
        </trans-unit>
        <trans-unit id="cd47433a58e73b2601de69c1a9744b288212fb5f" translate="yes" xml:space="preserve">
          <source>\exp(\text{input}) - \text{target}*\text{input}</source>
          <target state="translated">\ exp (\ text {input})-\ text {target} * \ text {input}</target>
        </trans-unit>
        <trans-unit id="fe43eb66c8bdd27010ca6db1c98281c7d4c4731f" translate="yes" xml:space="preserve">
          <source>\exp^A = \sum_{k=0}^\infty A^k / k!.</source>
          <target state="translated">\ exp ^ A = \ sum_ {k = 0} ^ \ infty A ^ k / k !.</target>
        </trans-unit>
        <trans-unit id="14f6d7074093dd4d9ef81de502fd654605e4bb67" translate="yes" xml:space="preserve">
          <source>\forall i = d, \dots, d+k-1</source>
          <target state="translated">\ forall i = d, \ dots, d + k-1</target>
        </trans-unit>
        <trans-unit id="6aed51b4bd21bf9aaad2932aa936dab4bdd62611" translate="yes" xml:space="preserve">
          <source>\frac{1}{1-p}</source>
          <target state="translated">\frac{1}{1-p}</target>
        </trans-unit>
        <trans-unit id="037205eaa442691656469a316bf2dff320c2f572" translate="yes" xml:space="preserve">
          <source>\frac{1}{2} N (N - 1)</source>
          <target state="translated">\ frac {1} {2} N (N-1)</target>
        </trans-unit>
        <trans-unit id="64c94d13eeb330b494061e86538db66574ad0f7d" translate="yes" xml:space="preserve">
          <source>\frac{1}{3}</source>
          <target state="translated">\frac{1}{3}</target>
        </trans-unit>
        <trans-unit id="5947a169159fe867f85f3fd8b9690019b48152f5" translate="yes" xml:space="preserve">
          <source>\frac{1}{8}</source>
          <target state="translated">\frac{1}{8}</target>
        </trans-unit>
        <trans-unit id="f81c864ea46e4c42b16663b744993f9011be95f2" translate="yes" xml:space="preserve">
          <source>\frac{300}{100}=3</source>
          <target state="translated">\frac{300}{100}=3</target>
        </trans-unit>
        <trans-unit id="436bcf2181eea8fe79cdb015a5567ca1b8d69652" translate="yes" xml:space="preserve">
          <source>\frac{5}{3}</source>
          <target state="translated">\frac{5}{3}</target>
        </trans-unit>
        <trans-unit id="a9c2bfb5b8138830fd93a3a13faef54bc4dc94a2" translate="yes" xml:space="preserve">
          <source>\frac{m}{2} \leq</source>
          <target state="translated">\ frac {m} {2} \ leq</target>
        </trans-unit>
        <trans-unit id="b8fb95020958e9f0b58822f35b35fb490054c02e" translate="yes" xml:space="preserve">
          <source>\frac{p - 1}{2}</source>
          <target state="translated">\ frac {p-1} {2}</target>
        </trans-unit>
        <trans-unit id="67833ee2012ec1c6254b6c009dc72bf0dc48aa6d" translate="yes" xml:space="preserve">
          <source>\gamma</source>
          <target state="translated">\gamma</target>
        </trans-unit>
        <trans-unit id="27634ea2c473bc36e59149a7f0c457ffb292325a" translate="yes" xml:space="preserve">
          <source>\hat{x}</source>
          <target state="translated">\hat{x}</target>
        </trans-unit>
        <trans-unit id="b9e47e1f86f68634e0d0a997d4ea3952fae1e892" translate="yes" xml:space="preserve">
          <source>\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</source>
          <target state="translated">\ hat {x} _ \ text {new} = (1-\ text {momentum}) \ times \ hat {x} + \ text {momentum} \ times x_t</target>
        </trans-unit>
        <trans-unit id="770b7843ffee64a984041915fe8461fd98a76060" translate="yes" xml:space="preserve">
          <source>\in [0, \infty]</source>
          <target state="translated">\ in [0, \ infty]</target>
        </trans-unit>
        <trans-unit id="9b97f26fbeb1b84327c736de515f10980d9c7d68" translate="yes" xml:space="preserve">
          <source>\infty</source>
          <target state="translated">\infty</target>
        </trans-unit>
        <trans-unit id="b237071f96360004cf37b06340000864b59f54c3" translate="yes" xml:space="preserve">
          <source>\int y\,dx</source>
          <target state="translated">\ int y \, dx</target>
        </trans-unit>
        <trans-unit id="a10251c74fceb1b1b9e9c45471b613f216beb4a9" translate="yes" xml:space="preserve">
          <source>\int_a^b f = -\int_b^a f</source>
          <target state="translated">\ int_a ^ bf =-\ int_b ^ af</target>
        </trans-unit>
        <trans-unit id="b3931f1ce298c536432fd324b3a1ab4337120689" translate="yes" xml:space="preserve">
          <source>\lambda</source>
          <target state="translated">\lambda</target>
        </trans-unit>
        <trans-unit id="1b47c3b18de49455a713efe91ac03ec27aad59a5" translate="yes" xml:space="preserve">
          <source>\lbrace (i, i) \rbrace</source>
          <target state="translated">\ lbrace (i, i) \ rbrace</target>
        </trans-unit>
        <trans-unit id="0fa91d11567564172cce99f01e48d1e73a28bc31" translate="yes" xml:space="preserve">
          <source>\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]</source>
          <target state="translated">\ left [0, 1, 2, \ dots, \ left \ lfloor \ frac {\ text {n \ _fft}} {2} \ right \ rfloor + 1 \ right]</target>
        </trans-unit>
        <trans-unit id="89ca5f286a6835c142dd1390cd67d2698dca7bcb" translate="yes" xml:space="preserve">
          <source>\left[\text{-clip\_value}, \text{clip\_value}\right]</source>
          <target state="translated">\ left [\ text {-clip \ _value}, \ text {clip \ _value} \ right]</target>
        </trans-unit>
        <trans-unit id="6ceb7ce51c6d9da5e34a800614788f55dff712c0" translate="yes" xml:space="preserve">
          <source>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</source>
          <target state="translated">\ left \ lceil \ frac {\ text {end}-\ text {start}} {\ text {step}} \ right \ rceil</target>
        </trans-unit>
        <trans-unit id="a74c86baea5a7bb180947e759ccafb91eee50e79" translate="yes" xml:space="preserve">
          <source>\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</source>
          <target state="translated">\ left \ lfloor \ frac {\ text {end}-\ text {start}} {\ text {step}} \ right \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="ed56196dc362da2ecbc9244bc832a7311739bb3d" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="de0088faf4547f5a22dd4cd77b23209a7dd8113e" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="88061aa72baf92fe1d129c82b73d16c37a6af8f0" translate="yes" xml:space="preserve">
          <source>\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</source>
          <target state="translated">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</target>
        </trans-unit>
        <trans-unit id="f0a9328764eddcc1a62bf8fbb7c249505ed2c539" translate="yes" xml:space="preserve">
          <source>\leq</source>
          <target state="translated">\leq</target>
        </trans-unit>
        <trans-unit id="49abff7af14753dc3cdab27a1bf7f8ed5f2c2fa1" translate="yes" xml:space="preserve">
          <source>\leq 256</source>
          <target state="translated">\ leq 256</target>
        </trans-unit>
        <trans-unit id="a719b606b8fa813d6dc2397930551b66016965ba" translate="yes" xml:space="preserve">
          <source>\leq S</source>
          <target state="translated">\ leq S</target>
        </trans-unit>
        <trans-unit id="f7d8821758716417ac6285ab84b0661734c3439c" translate="yes" xml:space="preserve">
          <source>\leq T</source>
          <target state="translated">\ leq T</target>
        </trans-unit>
        <trans-unit id="9ca5d36772ef139985921c4d545788d48fddcf0c" translate="yes" xml:space="preserve">
          <source>\lfloor \frac{N_d}{2} \rfloor + 1</source>
          <target state="translated">\ lfloor \ frac {N_d} {2} \ rfloor + 1</target>
        </trans-unit>
        <trans-unit id="bc0c94864255e5978415bf4d290dee47e2677194" translate="yes" xml:space="preserve">
          <source>\lfloor\frac{\text{input planes}}{sT}\rfloor</source>
          <target state="translated">\ lfloor \ frac {\ text {input planes}} {sT} \ rfloor</target>
        </trans-unit>
        <trans-unit id="9878e94e87e5bd7098242aa36f29f409bc60ed2b" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty</source>
          <target state="translated">\ lim_ {x \ to 0} \ frac {d} {dx} \ log (x) = \ infty</target>
        </trans-unit>
        <trans-unit id="26c67f72ffeaf83ec8c07ff1c00c57b7b4ce05e6" translate="yes" xml:space="preserve">
          <source>\lim_{x\to 0} \log (x) = -\infty</source>
          <target state="translated">\ lim_ {x \ to 0} \ log (x) =-\ infty</target>
        </trans-unit>
        <trans-unit id="d92b1f0baf6824880fb434c0077a618ba265ea33" translate="yes" xml:space="preserve">
          <source>\log (0) = -\infty</source>
          <target state="translated">\ log (0) =-\ infty</target>
        </trans-unit>
        <trans-unit id="99e33ed0cf12197cde63048ced916ea73cbc9c3e" translate="yes" xml:space="preserve">
          <source>\log(0)</source>
          <target state="translated">\log(0)</target>
        </trans-unit>
        <trans-unit id="9f7859c28e08a9c15994c8896e613dd27080b6a5" translate="yes" xml:space="preserve">
          <source>\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)</source>
          <target state="translated">\ log (\ Gamma_ {p} (a)) = C + \ displaystyle \ sum_ {i = 1} ^ {p} \ log \ left (\ Gamma \ left (a-\ frac {i-1} {2} \맞아 맞아)</target>
        </trans-unit>
        <trans-unit id="c1e3a3daacbe46fc4916de547be4049f250f3288" translate="yes" xml:space="preserve">
          <source>\log(\text{Softmax}(x))</source>
          <target state="translated">\log(\text{Softmax}(x))</target>
        </trans-unit>
        <trans-unit id="322a0b1861bac0e7bc727021b82a0313c2a2b303" translate="yes" xml:space="preserve">
          <source>\log\left(e^x + e^y\right)</source>
          <target state="translated">\ log \ left (e ^ x + e ^ y \ right)</target>
        </trans-unit>
        <trans-unit id="8783c75c38eec2ddd68cac88f7dc5ac00e806274" translate="yes" xml:space="preserve">
          <source>\log_2\left(2^x + 2^y\right)</source>
          <target state="translated">\ log_2 \ left (2 ^ x + 2 ^ y \ right)</target>
        </trans-unit>
        <trans-unit id="8935b97a0f600209c234abf4d224c5fe967c9fa1" translate="yes" xml:space="preserve">
          <source>\lvert \text{input} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert</source>
          <target state="translated">\ lvert \ text {input}-\ text {other} \ rvert \ leq \ texttt {atol} + \ texttt {rtol} \ times \ lvert \ text {other} \ rvert</target>
        </trans-unit>
        <trans-unit id="eafd018e3a8aa85682e4f0b33612ce6efed642ca" translate="yes" xml:space="preserve">
          <source>\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})}, \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}</source>
          <target state="translated">\ mathbf {W} _ {SN} = \ dfrac {\ mathbf {W}} {\ sigma (\ mathbf {W})}, \ sigma (\ mathbf {W}) = \ max _ {\ mathbf {h} : \ mathbf {h} \ ne 0} \ dfrac {\ | \ mathbf {W} \ mathbf {h} \ | _2} {\ | \ mathbf {h} \ | _2}</target>
        </trans-unit>
        <trans-unit id="201a130976ca261c5bc7ef67511ced9f15d7653a" translate="yes" xml:space="preserve">
          <source>\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</source>
          <target state="translated">\ mathbf {w} = g \ dfrac {\ mathbf {v}} {\ | \ mathbf {v} \ |}</target>
        </trans-unit>
        <trans-unit id="df511dffcdad49a67cd5945ee0ab7aef8bb4f9fc" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 0.01)</source>
          <target state="translated">\ mathcal {N} (0, 0.01)</target>
        </trans-unit>
        <trans-unit id="8f3ba18912099017c093331676f6188e744efdac" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, 1)</source>
          <target state="translated">\ mathcal {N} (0, 1)</target>
        </trans-unit>
        <trans-unit id="50c7a47dd01b89919d5422d0295475337aab6ace" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(0, \text{std}^2)</source>
          <target state="translated">\ mathcal {N} (0, \ text {std} ^ 2)</target>
        </trans-unit>
        <trans-unit id="92c992916044275832483fbb98179a2d00ca4c1e" translate="yes" xml:space="preserve">
          <source>\mathcal{N}(\text{mean}, \text{std}^2)</source>
          <target state="translated">\ mathcal {N} (\ text {평균}, \ text {std} ^ 2)</target>
        </trans-unit>
        <trans-unit id="87b1ef0bb4d4a0924cc95ef538f83b2bb3e53e42" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\sqrt{k}, \sqrt{k})</source>
          <target state="translated">\ mathcal {U} (-\ sqrt {k}, \ sqrt {k})</target>
        </trans-unit>
        <trans-unit id="d7f6ed3e182dc030701ad2deb57bf7caa5b88d6f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-\text{bound}, \text{bound})</source>
          <target state="translated">\ mathcal {U} (-\ text {바운드}, \ text {바운드})</target>
        </trans-unit>
        <trans-unit id="325a9ed78efc110eaab4160ef2c97f9ac713df84" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(-a, a)</source>
          <target state="translated">\ mathcal {U} (-a, a)</target>
        </trans-unit>
        <trans-unit id="b28ca9812620dfef2c686761b7aa8334acb0312f" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(0, 1)</source>
          <target state="translated">\ mathcal {U} (0, 1)</target>
        </trans-unit>
        <trans-unit id="b9712b8b025515dc1dea5f46b97d3c4fe634d954" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(\text{lower}, \text{upper})</source>
          <target state="translated">\ mathcal {U} (\ text {lower}, \ text {upper})</target>
        </trans-unit>
        <trans-unit id="e216b98083013f9a980a8176c0057ec95ff5e691" translate="yes" xml:space="preserve">
          <source>\mathcal{U}(a, b)</source>
          <target state="translated">\ mathcal {U} (a, b)</target>
        </trans-unit>
        <trans-unit id="2df7cbd2cee6b04f561cf080750aba56226af4cb" translate="yes" xml:space="preserve">
          <source>\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\ mathrm {erfc} (x) = 1-\ frac {2} {\ sqrt {\ pi}} \ int_ {0} ^ {x} e ^ {-t ^ 2} dt</target>
        </trans-unit>
        <trans-unit id="20a69b6b57674088939e407b3b5098441f752171" translate="yes" xml:space="preserve">
          <source>\mathrm{erfinv}(\mathrm{erf}(x)) = x</source>
          <target state="translated">\ mathrm {erfinv} (\ mathrm {erf} (x)) = x</target>
        </trans-unit>
        <trans-unit id="56d825970c3fb1fe7543e6c61686e2830fac7d9f" translate="yes" xml:space="preserve">
          <source>\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt</source>
          <target state="translated">\ mathrm {erf} (x) = \ frac {2} {\ sqrt {\ pi}} \ int_ {0} ^ {x} e ^ {-t ^ 2} dt</target>
        </trans-unit>
        <trans-unit id="3e9c0d2c84e11d8338d1dac942d5dede72bd1acf" translate="yes" xml:space="preserve">
          <source>\min(input.size(-1), input.size(-2))</source>
          <target state="translated">\ min (input.size (-1), input.size (-2))</target>
        </trans-unit>
        <trans-unit id="3a4e56595df1d02f21e5c3ff7b0e9d80921858ff" translate="yes" xml:space="preserve">
          <source>\mu</source>
          <target state="translated">\mu</target>
        </trans-unit>
        <trans-unit id="c8e2d1a0bf50a27d43ade30cfb048d99feb31ad1" translate="yes" xml:space="preserve">
          <source>\odot</source>
          <target state="translated">\odot</target>
        </trans-unit>
        <trans-unit id="73b077a63e22815fe5c8ee82dab9894be842b19c" translate="yes" xml:space="preserve">
          <source>\omega</source>
          <target state="translated">\omega</target>
        </trans-unit>
        <trans-unit id="72166555a6db55785fc6fbe9a7c5bbe72be28db8" translate="yes" xml:space="preserve">
          <source>\otimes</source>
          <target state="translated">\otimes</target>
        </trans-unit>
        <trans-unit id="6dc2ada78a76bad95d3b921558b1195549985eab" translate="yes" xml:space="preserve">
          <source>\prod(\text{kernel\_size})</source>
          <target state="translated">\prod(\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="02cea321dbc3a3edfc9cc1780dfe84f694c585f0" translate="yes" xml:space="preserve">
          <source>\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}</source>
          <target state="translated">\ psi (x) = \ frac {d} {dx} \ ln \ left (\ Gamma \ left (x \ right) \ right) = \ frac {\ Gamma '(x)} {\ Gamma (x)}</target>
        </trans-unit>
        <trans-unit id="b30c154a70c78f1c5dcecce58dde2e9ff4ec56aa" translate="yes" xml:space="preserve">
          <source>\psi^{(n)}(x) = \frac{d^{(n)}}{dx^{(n)}} \psi(x)</source>
          <target state="translated">\ psi ^ {(n)} (x) = \ frac {d ^ {(n)}} {dx ^ {(n)}} \ psi (x)</target>
        </trans-unit>
        <trans-unit id="69c15416b63fd933850978302bcda59da879774e" translate="yes" xml:space="preserve">
          <source>\sigma</source>
          <target state="translated">\sigma</target>
        </trans-unit>
        <trans-unit id="bfe16f27ebc966df6f10ba356a1547b6e7242dd7" translate="yes" xml:space="preserve">
          <source>\sqrt{2}</source>
          <target state="translated">\sqrt{2}</target>
        </trans-unit>
        <trans-unit id="358161536f25000be6a773604fcf4a28afd7b7bd" translate="yes" xml:space="preserve">
          <source>\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</source>
          <target state="translated">\ sqrt {\ frac {2} {1 + \ text {negative \ _slope} ^ 2}}</target>
        </trans-unit>
        <trans-unit id="64126f7d8d3d661d4e29a191268f98bf759903a3" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^K N_i}</source>
          <target state="translated">\ sqrt {\ prod_ {i = 1} ^ K N_i}</target>
        </trans-unit>
        <trans-unit id="976e9a0789eb95323325b92e6edc3b432c6754b8" translate="yes" xml:space="preserve">
          <source>\sqrt{\prod_{i=1}^d N_i}</source>
          <target state="translated">\ sqrt {\ prod_ {i = 1} ^ d N_i}</target>
        </trans-unit>
        <trans-unit id="9312c2748ccad7c25f8d92bc855a5a0b38989a51" translate="yes" xml:space="preserve">
          <source>\star</source>
          <target state="translated">\star</target>
        </trans-unit>
        <trans-unit id="3df82d7a797b96797b79d72f235bc018cb8155c9" translate="yes" xml:space="preserve">
          <source>\sum_{t=-\infty}^{\infty} |w|^2[n-t\times hop\_length] \cancel{=} 0</source>
          <target state="translated">\ sum_ {t =-\ infty} ^ {\ infty} | w | ^ 2 [nt \ times hop \ _length] \ cancel {=} 0</target>
        </trans-unit>
        <trans-unit id="a12be8b8923c0e92cce3f35968e39960bc665833" translate="yes" xml:space="preserve">
          <source>\tanh</source>
          <target state="translated">\tanh</target>
        </trans-unit>
        <trans-unit id="053658991aeb9a94a57c16cbe979538b3eca3b46" translate="yes" xml:space="preserve">
          <source>\texttt{n\_classes}</source>
          <target state="translated">\texttt{n\_classes}</target>
        </trans-unit>
        <trans-unit id="79df4825cae48291f72d91cc9840f2adcb2716c4" translate="yes" xml:space="preserve">
          <source>\texttt{result[i]}</source>
          <target state="translated">\texttt{result[i]}</target>
        </trans-unit>
        <trans-unit id="8a01c6904694a6040cbbe4d31b35e67c5fb8c5bc" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p\_tensor[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p\_tensor[i]})</target>
        </trans-unit>
        <trans-unit id="248f05fb27cede89d993bc3bb9750c4fde2467c2" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{p})</source>
          <target state="translated">\text{Bernoulli}(\texttt{p})</target>
        </trans-unit>
        <trans-unit id="15ca5fa99d2411cc7a336f5df78382fd090360c9" translate="yes" xml:space="preserve">
          <source>\text{Bernoulli}(\texttt{self[i]})</source>
          <target state="translated">\text{Bernoulli}(\texttt{self[i]})</target>
        </trans-unit>
        <trans-unit id="f4308a3c6285dab040bdf907914e0906e5918d43" translate="yes" xml:space="preserve">
          <source>\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</source>
          <target state="translated">\ text {CELU} (x) = \ max (0, x) + \ min (0, \ alpha * (\ exp (x / \ alpha)-1))</target>
        </trans-unit>
        <trans-unit id="1c5fbaf107a1bd80dcb32b25718623eef6cabd9f" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; 0\\ \alpha * (\exp(x) - 1), &amp;amp; \text{ if } x \leq 0 \end{cases}</source>
          <target state="translated">\ text {ELU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x&amp;gt; 0 \\ \ alpha * (\ exp (x)-1), &amp;amp; \ text {if} x \ leq 0 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="ee2c000c326328286929b5abe78d5f28ecc355ca" translate="yes" xml:space="preserve">
          <source>\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))</source>
          <target state="translated">\ text {ELU} (x) = \ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1))</target>
        </trans-unit>
        <trans-unit id="1ae2cb648d379e90a17c82c0bca8393180166a13" translate="yes" xml:space="preserve">
          <source>\text{GELU}(x) = x * \Phi(x)</source>
          <target state="translated">\ text {GELU} (x) = x * \ Phi (x)</target>
        </trans-unit>
        <trans-unit id="98d15f980c451db5bf1f8450a31ea12ac80b261b" translate="yes" xml:space="preserve">
          <source>\text{GLU}(a, b) = a \otimes \sigma(b)</source>
          <target state="translated">\ text {GLU} (a, b) = a \ otimes \ sigma (b)</target>
        </trans-unit>
        <trans-unit id="2b0fd43cdc620c274316d22c100c5a1cedf8758f" translate="yes" xml:space="preserve">
          <source>\text{HardShrink}(x) = \begin{cases} x, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {HardShrink} (x) = \ begin {cases} x, &amp;amp; \ text {if} x&amp;gt; \ lambda \\ x, &amp;amp; \ text {if} x &amp;lt;-\ lambda \\ 0, &amp;amp; \ text {그렇지 않으면 } \ end {cases}</target>
        </trans-unit>
        <trans-unit id="905f89b6d5fe72acb1c8befdbd1fbd394ccc9c79" translate="yes" xml:space="preserve">
          <source>\text{HardTanh}(x) = \begin{cases} 1 &amp;amp; \text{ if } x &amp;gt; 1 \\ -1 &amp;amp; \text{ if } x &amp;lt; -1 \\ x &amp;amp; \text{ otherwise } \\ \end{cases}</source>
          <target state="translated">\ text {HardTanh} (x) = \ begin {cases} 1 &amp;amp; \ text {if} x&amp;gt; 1 \\ -1 &amp;amp; \ text {if} x &amp;lt;-1 \\ x &amp;amp; \ text {그렇지 않으면} \\ \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c0b2885acfe7c1d78f13cbb375808a8a70622e9c" translate="yes" xml:space="preserve">
          <source>\text{Hardsigmoid}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ 1 &amp;amp; \text{if~} x \ge +3, \\ x / 6 + 1 / 2 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\ text {Hardsigmoid} (x) = \ begin {cases} 0 &amp;amp; \ text {if ~} x \ le -3, \\ 1 &amp;amp; \ text {if ~} x \ ge +3, \\ x / 6 + 1/2 &amp;amp; \ text {otherwise} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="3707baa731a2a1d93fe314a962f04a7440710fa1" translate="yes" xml:space="preserve">
          <source>\text{Hardswish}(x) = \begin{cases} 0 &amp;amp; \text{if~} x \le -3, \\ x &amp;amp; \text{if~} x \ge +3, \\ x \cdot (x + 3) /6 &amp;amp; \text{otherwise} \end{cases}</source>
          <target state="translated">\ text {Hardswish} (x) = \ begin {cases} 0 &amp;amp; \ text {if ~} x \ le -3, \\ x &amp;amp; \ text {if ~} x \ ge +3, \\ x \ cdot ( x + 3) / 6 &amp;amp; \ text {otherwise} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="f4d15ebefe7344cee7608a90993d80b6ed04f631" translate="yes" xml:space="preserve">
          <source>\text{LeakyRELU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ \text{negative\_slope} \times x, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {LeakyRELU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x \ geq 0 \\ \ text {negative \ _slope} \ times x, &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="68988527b0b6237352fc1f66e3f8116ea81dd84a" translate="yes" xml:space="preserve">
          <source>\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</source>
          <target state="translated">\ text {LeakyReLU} (x) = \ max (0, x) + \ text {negative \ _slope} * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="51a86607b1de32af3fd6faa8dfe21281f15bc36b" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</source>
          <target state="translated">\ text {LogSigmoid} (x) = \ log \ left (\ frac {1} {1 + \ exp (-x)} \ right)</target>
        </trans-unit>
        <trans-unit id="a4939da8f3a870de43d45395bdbb72f415fd6a5c" translate="yes" xml:space="preserve">
          <source>\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</source>
          <target state="translated">\ text {LogSigmoid} (x_i) = \ log \ left (\ frac {1} {1 + \ exp (-x_i)} \ right)</target>
        </trans-unit>
        <trans-unit id="a58af29fea78e99ffe2b4d0392c259d952558379" translate="yes" xml:space="preserve">
          <source>\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</source>
          <target state="translated">\ text {LogSoftmax} (x_ {i}) = \ log \ left (\ frac {\ exp (x_i)} {\ sum_j \ exp (x_j)} \ right)</target>
        </trans-unit>
        <trans-unit id="96135669cb29b74a14e76462df05bf97eef0ca84" translate="yes" xml:space="preserve">
          <source>\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</source>
          <target state="translated">\ text {MultiHead} (Q, K, V) = \ text {Concat} (head_1, \ dots, head_h) W ^ O \ text {where} head_i = \ text {Attention} (QW_i ^ Q, KW_i ^ K, VW_i ^ V)</target>
        </trans-unit>
        <trans-unit id="ccbab99b824f6a91f1e1bf82a1a0fdaf575b1bb6" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \begin{cases} x, &amp;amp; \text{ if } x \geq 0 \\ ax, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {PReLU} (x) = \ begin {cases} x, &amp;amp; \ text {if} x \ geq 0 \\ ax, &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="caa4e9686c6c337eb30002e3bdcb6a130c86d148" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</source>
          <target state="translated">\ text {PReLU} (x) = \ max (0, x) + \ text {weight} * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="5b347087764b6a93bb6f3ca448de0809ea32466b" translate="yes" xml:space="preserve">
          <source>\text{PReLU}(x) = \max(0,x) + a * \min(0,x)</source>
          <target state="translated">\ text {PReLU} (x) = \ max (0, x) + a * \ min (0, x)</target>
        </trans-unit>
        <trans-unit id="8ab73665e19333454b7dcdadd14407c6f4eaf162" translate="yes" xml:space="preserve">
          <source>\text{RReLU}(x) = \begin{cases} x &amp;amp; \text{if } x \geq 0 \\ ax &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {RReLU} (x) = \ begin {cases} x &amp;amp; \ text {if} x \ geq 0 \\ ax &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="ca059eb696d6eaad86cfafe12af59ddef8aeb677" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(0,x), 6)</source>
          <target state="translated">\ text {ReLU6} (x) = \ min (\ max (0, x), 6)</target>
        </trans-unit>
        <trans-unit id="be84129fb353e250c9169d66a9b03062f62f4151" translate="yes" xml:space="preserve">
          <source>\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</source>
          <target state="translated">\ text {ReLU6} (x) = \ min (\ max (x_0, x), q (6))</target>
        </trans-unit>
        <trans-unit id="3bfab1dc2c67446ab1cfb128a9c2bc89afb1f336" translate="yes" xml:space="preserve">
          <source>\text{ReLU}</source>
          <target state="translated">\text{ReLU}</target>
        </trans-unit>
        <trans-unit id="08bf65cb7cffc00d2f4597ea4d30a22a63976e74" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x) = (x)^+ = \max(0, x)</source>
          <target state="translated">\ text {ReLU} (x) = (x) ^ + = \ max (0, x)</target>
        </trans-unit>
        <trans-unit id="836fcdcc8abf1c798414718b995cf2a357c6c5a8" translate="yes" xml:space="preserve">
          <source>\text{ReLU}(x)= \max(x_0, x)</source>
          <target state="translated">\ text {ReLU} (x) = \ max (x_0, x)</target>
        </trans-unit>
        <trans-unit id="cab6677fddc14bf3ddd26b8fe4fe9e7d772b3259" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\ text {SELU} (x) = \ text {scale} * (\ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1)))</target>
        </trans-unit>
        <trans-unit id="0eacb9a4c9b22d795a671d9085d3576f65f88226" translate="yes" xml:space="preserve">
          <source>\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</source>
          <target state="translated">\ text {SELU} (x) = 배율 * (\ max (0, x) + \ min (0, \ alpha * (\ exp (x)-1)))</target>
        </trans-unit>
        <trans-unit id="14b5832965c3a723de26aea464be1a7e1207b0e7" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {시그 모이 드} (x) = \ frac {1} {1 + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="dd6633f7f328cd88d5fded389f4a69b2a11f8bed" translate="yes" xml:space="preserve">
          <source>\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</source>
          <target state="translated">\ text {시그 모이 드} (x) = \ sigma (x) = \ frac {1} {1 + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="fa68ac444b17ef606bcb6fc0587fbb0c08130721" translate="yes" xml:space="preserve">
          <source>\text{SoftShrinkage}(x) = \begin{cases} x - \lambda, &amp;amp; \text{ if } x &amp;gt; \lambda \\ x + \lambda, &amp;amp; \text{ if } x &amp;lt; -\lambda \\ 0, &amp;amp; \text{ otherwise } \end{cases}</source>
          <target state="translated">\ text {SoftShrinkage} (x) = \ begin {cases} x-\ lambda, &amp;amp; \ text {if} x&amp;gt; \ lambda \\ x + \ lambda, &amp;amp; \ text {if} x &amp;lt;-\ lambda \\ 0 , &amp;amp; \ text {그렇지 않으면} \ end {cases}</target>
        </trans-unit>
        <trans-unit id="16ab4540ec544f6557be83896c80fd5ae2348d4a" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{ 1 + |x|}</source>
          <target state="translated">\ text {SoftSign} (x) = \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="b4e8d4178c0a5a0115f54178dd9083d36fddd624" translate="yes" xml:space="preserve">
          <source>\text{SoftSign}(x) = \frac{x}{1 + |x|}</source>
          <target state="translated">\ text {SoftSign} (x) = \ frac {x} {1 + | x |}</target>
        </trans-unit>
        <trans-unit id="a2f536bf3a8bd1033dd978de393c848edde7120d" translate="yes" xml:space="preserve">
          <source>\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</source>
          <target state="translated">\ text {Softmax} (x_ {i}) = \ frac {\ exp (x_i)} {\ sum_j \ exp (x_j)}</target>
        </trans-unit>
        <trans-unit id="5b81ade15ec338b468467f739fb0a7a9ffbb69c9" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x) = \text{Softmax}(-x)</source>
          <target state="translated">\ text {Softmin} (x) = \ text {Softmax} (-x)</target>
        </trans-unit>
        <trans-unit id="f07f1bdd8c02f9e56e3ec0dac4ae00ebee846aae" translate="yes" xml:space="preserve">
          <source>\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</source>
          <target state="translated">\ text {Softmin} (x_ {i}) = \ frac {\ exp (-x_i)} {\ sum_j \ exp (-x_j)}</target>
        </trans-unit>
        <trans-unit id="2b624f5a535d4c37714f3d81447e39c785f806f7" translate="yes" xml:space="preserve">
          <source>\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</source>
          <target state="translated">\ text {Softplus} (x) = \ frac {1} {\ beta} * \ log (1 + \ exp (\ beta * x))</target>
        </trans-unit>
        <trans-unit id="c0fd3a2a1e0641ec623557f2f43fda5f7c89dc6c" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \tanh(x)</source>
          <target state="translated">\ text {Tanhshrink} (x) = x-\ tanh (x)</target>
        </trans-unit>
        <trans-unit id="995fcea13ed41fcda67e8f62153baff1abf82a33" translate="yes" xml:space="preserve">
          <source>\text{Tanhshrink}(x) = x - \text{Tanh}(x)</source>
          <target state="translated">\ text {Tanhshrink} (x) = x-\ text {Tanh} (x)</target>
        </trans-unit>
        <trans-unit id="d0334b6a3fe1d679df6ff4bfb438db0b64230682" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh} (x) = \ tanh (x) = \ frac {\ exp (x)-\ exp (-x)} {\ exp (x) + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="75a10715f4ce8ac09932b779e24b1a0a5468348f" translate="yes" xml:space="preserve">
          <source>\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</source>
          <target state="translated">\ text {Tanh} (x) = \ tanh (x) = \ frac {\ exp (x)-\ exp (-x)} {\ exp (x) + \ exp (-x)}</target>
        </trans-unit>
        <trans-unit id="f169080e8d1eb07929eead4219c50bde74fb23da" translate="yes" xml:space="preserve">
          <source>\text{batch1} \mathbin{@} \text{batch2}</source>
          <target state="translated">\ text {batch1} \ mathbin {@} \ text {batch2}</target>
        </trans-unit>
        <trans-unit id="229245bb08fb0569d7993b5cea15398607e69900" translate="yes" xml:space="preserve">
          <source>\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}</source>
          <target state="translated">\ text {bound} = \ text {gain} \ times \ sqrt {\ frac {3} {\ text {fan \ _mode}}}</target>
        </trans-unit>
        <trans-unit id="3e2f5fb306be83947e6aad79d83c918b084bbee4" translate="yes" xml:space="preserve">
          <source>\text{in\_channels}</source>
          <target state="translated">\text{in\_channels}</target>
        </trans-unit>
        <trans-unit id="26c469932ffffddf5378f023f97a627052bcde45" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;gt; \text{other}</source>
          <target state="translated">\ text {입력}&amp;gt; \ text {기타}</target>
        </trans-unit>
        <trans-unit id="c15cd245b2e6c1904e5b0a29dc06bb37cb23bb5e" translate="yes" xml:space="preserve">
          <source>\text{input} &amp;lt; \text{other}</source>
          <target state="translated">\ text {입력} &amp;lt;\ text {기타}</target>
        </trans-unit>
        <trans-unit id="f395bedc77d1db475982b0b075906a247b84c2eb" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target} * \log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log (\ text {input} + \ text {eps})</target>
        </trans-unit>
        <trans-unit id="686d55b3f5e0392f4e8fcfd75b77c03037718ecf" translate="yes" xml:space="preserve">
          <source>\text{input} - \text{target}*\log(\text{input}+\text{eps})</source>
          <target state="translated">\ text {input}-\ text {target} * \ log (\ text {input} + \ text {eps})</target>
        </trans-unit>
        <trans-unit id="6da1a408fc1384eb95e3208a559d3a3e0f03e99f" translate="yes" xml:space="preserve">
          <source>\text{input} = Q R</source>
          <target state="translated">\ text {input} = QR</target>
        </trans-unit>
        <trans-unit id="c4dd474d77793c7a92038a0755b05fe0ba4adf17" translate="yes" xml:space="preserve">
          <source>\text{input} = V \text{diag}(e) V^T</source>
          <target state="translated">\ text {input} = V \ text {diag} (e) V ^ T</target>
        </trans-unit>
        <trans-unit id="e721787652f4fc0b6f66950d386917c38d8957f8" translate="yes" xml:space="preserve">
          <source>\text{input} \geq \text{other}</source>
          <target state="translated">\ text {입력} \ geq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="3cbe8a936fc7cb3a9fdf09ad329713fbe61317e4" translate="yes" xml:space="preserve">
          <source>\text{input} \leq \text{other}</source>
          <target state="translated">\ text {입력} \ leq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="3901704e8ee508e9522d87ef1f88b52ef027532b" translate="yes" xml:space="preserve">
          <source>\text{input} \neq \text{other}</source>
          <target state="translated">\ text {입력} \ neq \ text {기타}</target>
        </trans-unit>
        <trans-unit id="ccf6648a02673a0bbbd4f96889ba48888a0e40f4" translate="yes" xml:space="preserve">
          <source>\text{input}[i, j]</source>
          <target state="translated">\ text {입력} [i, j]</target>
        </trans-unit>
        <trans-unit id="47eeab2292dd874946bd7e66f27cc66b11eafc5e" translate="yes" xml:space="preserve">
          <source>\text{input}_{i}</source>
          <target state="translated">\text{input}_{i}</target>
        </trans-unit>
        <trans-unit id="dd2a901c455f604c6af98c920667ce74d801ef8c" translate="yes" xml:space="preserve">
          <source>\text{input}_{i} / \text{other}_{i}</source>
          <target state="translated">\ text {input} _ {i} / \ text {기타} _ {i}</target>
        </trans-unit>
        <trans-unit id="c920fc92a2f279db08ab80d5217f4a1714bd40d6" translate="yes" xml:space="preserve">
          <source>\text{i}^{th}</source>
          <target state="translated">\text{i}^{th}</target>
        </trans-unit>
        <trans-unit id="6ca0545c1ccedc4ff77a3d4acb74e3770c0fbbdd" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]})</source>
          <target state="translated">\ text {커널 \ _size [0]}, \ text {커널 \ _size [1]})</target>
        </trans-unit>
        <trans-unit id="9229970eeb833df18b71461a132cd3b0ce98ef3a" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</source>
          <target state="translated">\ text {kernel \ _size [0]}, \ text {kernel \ _size [1]}, \ text {kernel \ _size [2]})</target>
        </trans-unit>
        <trans-unit id="15869cb2da85e3f8dc3a924d02c0d13f080d42d1" translate="yes" xml:space="preserve">
          <source>\text{kernel\_size})</source>
          <target state="translated">\text{kernel\_size})</target>
        </trans-unit>
        <trans-unit id="80b2d9d0a1aff139a212f622b578c27a2cb6c126" translate="yes" xml:space="preserve">
          <source>\text{k}^{th}</source>
          <target state="translated">\text{k}^{th}</target>
        </trans-unit>
        <trans-unit id="0f84b2b9c4507f4c38935799e7145beab872e3ff" translate="yes" xml:space="preserve">
          <source>\text{logcumsumexp}(x)_{ij} = \log \sum\limits_{j=0}^{i} \exp(x_{ij})</source>
          <target state="translated">\ text {logcumsumexp} (x) _ {ij} = \ log \ sum \ limits_ {j = 0} ^ {i} \ exp (x_ {ij})</target>
        </trans-unit>
        <trans-unit id="e4fff2f19ba639fc2ee13a3cda790993ad87fdfe" translate="yes" xml:space="preserve">
          <source>\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})</source>
          <target state="translated">\ text {logsumexp} (x) _ {i} = \ log \ sum_j \ exp (x_ {ij})</target>
        </trans-unit>
        <trans-unit id="e990f63d798552bb3d1da254e1d0c3c11424c266" translate="yes" xml:space="preserve">
          <source>\text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</source>
          <target state="translated">\ text {loss} = \ frac {\ sum ^ {N} _ {i = 1} loss (i, class [i])} {\ sum ^ {N} _ {i = 1} weight [class [i] ]}</target>
        </trans-unit>
        <trans-unit id="65ca80a02b47d73c67e5a3e63c9b314787018d8d" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) = -x[class] + \log\left(\sum_j \exp(x[j])\right)</source>
          <target state="translated">\ text {손실} (x, 클래스) =-\ log \ left (\ frac {\ exp (x [클래스])} {\ sum_j \ exp (x [j])} \ right) = -x [클래스] + \ log \ left (\ sum_j \ exp (x [j]) \ right)</target>
        </trans-unit>
        <trans-unit id="019a711532dd32707cd8b1b52a85f9dcf92bf993" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</source>
          <target state="translated">\ text {손실} (x, 클래스) = 가중치 [클래스] \ left (-x [클래스] + \ log \ left (\ sum_j \ exp (x [j]) \ right) \ right)</target>
        </trans-unit>
        <trans-unit id="ffd0b22341cc6c018e0b5728d553ab798985dbfc" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \begin{cases} 1 - \cos(x_1, x_2), &amp;amp; \text{if } y = 1 \\ \max(0, \cos(x_1, x_2) - \text{margin}), &amp;amp; \text{if } y = -1 \end{cases}</source>
          <target state="translated">\ text {loss} (x, y) = \ begin {cases} 1-\ cos (x_1, x_2), &amp;amp; \ text {if} y = 1 \\ \ max (0, \ cos (x_1, x_2)- \ text {margin}), &amp;amp; \ text {if} y = -1 \ end {cases}</target>
        </trans-unit>
        <trans-unit id="c59e75cdc56fc3f83b35cdb9f628a1f6dc39d344" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {1} {n} \ sum_ {i} z_ {i}</target>
        </trans-unit>
        <trans-unit id="b06e3acda76e096ac3266b690a1f15f3c86a036c" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {\ sum_i \ max (0, \ text {margin}-x [y] + x [i])) ^ p} {\ text {x.size} ( 0)}</target>
        </trans-unit>
        <trans-unit id="932b5db2aa0a39e3cde91ee0926767654acd252f" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</source>
          <target state="translated">\ text {손실} (x, y) = \ frac {\ sum_i \ max (0, w [y] * (\ text {margin}-x [y] + x [i])) ^ p)} {\ 텍스트 {x.size} (0)}</target>
        </trans-unit>
        <trans-unit id="1be1a15f43a5fe29e30cefc8a77697b52665cc11" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</source>
          <target state="translated">\ text {loss} (x, y) = \ sum_i \ frac {\ log (1 + \ exp (-y [i] * x [i]))} {\ text {x.nelement} ()}</target>
        </trans-unit>
        <trans-unit id="be8b3997d164f84206bbdb59fc30b16e4df28e7b" translate="yes" xml:space="preserve">
          <source>\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</source>
          <target state="translated">\ text {loss} (x, y) = \ sum_ {ij} \ frac {\ max (0, 1-(x [y [j]]-x [i]))} {\ text {x.size} (0)}</target>
        </trans-unit>
        <trans-unit id="ed773e5223ed4e8e7a2085c415f1e22caa9ee61a" translate="yes" xml:space="preserve">
          <source>\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</source>
          <target state="translated">\ text {손실} (x1, x2, y) = \ max (0, -y * (x1-x2) + \ text {margin})</target>
        </trans-unit>
        <trans-unit id="1a70593a5153f0c40ba1299b72f0b57903179f3e" translate="yes" xml:space="preserve">
          <source>\text{other}_{i}</source>
          <target state="translated">\text{other}_{i}</target>
        </trans-unit>
        <trans-unit id="17f7519844253d6013b5ffbe16cec0f08dcc6ff0" translate="yes" xml:space="preserve">
          <source>\text{out} = -1 \times \text{input}</source>
          <target state="translated">\ text {out} = -1 \ times \ text {input}</target>
        </trans-unit>
        <trans-unit id="90a41651feeb08c3eedd09a310ec016d8b14ad0c" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {mat1} _i \ mathbin {@} \ text {mat2} _i)</target>
        </trans-unit>
        <trans-unit id="bb3213012476cae8440fbe19a6e07cd71a7793bd" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{mat} \mathbin{@} \text{vec})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {mat} \ mathbin {@} \ text {vec})</target>
        </trans-unit>
        <trans-unit id="e949532f9deb6bd6d752ab7f572576835541d193" translate="yes" xml:space="preserve">
          <source>\text{out} = \beta\ \text{input} + \alpha\ (\text{vec1} \otimes \text{vec2})</source>
          <target state="translated">\ text {out} = \ beta \ \ text {input} + \ alpha \ (\ text {vec1} \ otimes \ text {vec2})</target>
        </trans-unit>
        <trans-unit id="0358e27d2d15a38fee22f3930f0704830355a8e2" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{abs} \cdot \cos(\text{angle}) + \text{abs} \cdot \sin(\text{angle}) \cdot j</source>
          <target state="translated">\ text {out} = \ text {abs} \ cdot \ cos (\ text {angle}) + \ text {abs} \ cdot \ sin (\ text {angle}) \ cdot j</target>
        </trans-unit>
        <trans-unit id="36eb11cd26daa728869e9dc0ff4a77afceb36308" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{alpha} \times \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {alpha} \ times \ text {other}</target>
        </trans-unit>
        <trans-unit id="256d0e45774d81cdb6896827af46bd880155747c" translate="yes" xml:space="preserve">
          <source>\text{out} = \text{input} + \text{other}</source>
          <target state="translated">\ text {out} = \ text {input} + \ text {other}</target>
        </trans-unit>
        <trans-unit id="b32a02ba7ef69d1c7496b06c7c87e64e5599ad7d" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1} \text{input}(N_i, C_j, \text{stride} \times l + m)</source>
          <target state="translated">\ text {out} (N_i, C_j, l) = \ frac {1} {k} \ sum_ {m = 0} ^ {k-1} \ text {input} (N_i, C_j, \ text {stride} \ 곱하기 l + m)</target>
        </trans-unit>
        <trans-unit id="6ffc018e43b40a0d8b659cfa4eb86a17bb9a021a" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out} (N_i, C _ {\ text {out} _j}) = \ text {bias} (C _ {\ text {out} _j}) + \ sum_ {k = 0} ^ {C _ {\ text { in}}-1} \ text {weight} (C _ {\ text {out} _j}, k) \ star \ text {input} (N_i, k)</target>
        </trans-unit>
        <trans-unit id="1329f06a718ae43bb8cf775c59ab239d1fb3b8cd" translate="yes" xml:space="preserve">
          <source>\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</source>
          <target state="translated">\ text {out} (N_i, C _ {\ text {out} _j}) = \ text {bias} (C _ {\ text {out} _j}) + \ sum_ {k = 0} ^ {C_ {in}- 1} \ text {weight} (C _ {\ text {out} _j}, k) \ star \ text {input} (N_i, k)</target>
        </trans-unit>
        <trans-unit id="43e8084d30aefee2bbf06206dd0a8ab14834163f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \begin{cases} \text{x}_i &amp;amp; \text{if } \text{condition}_i \\ \text{y}_i &amp;amp; \text{otherwise} \\ \end{cases}</source>
          <target state="translated">\ text {out} _i = \ begin {cases} \ text {x} _i &amp;amp; \ text {if} \ text {condition} _i \\ \ text {y} _i &amp;amp; \ text {otherwise} \\ \ end {cases }</target>
        </trans-unit>
        <trans-unit id="887bfc6ee868fe88195f2b1a697c2be49d6ee9a3" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \beta\ \text{input}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)</source>
          <target state="translated">\ text {out} _i = \ beta \ \ text {input} _i + \ alpha \ (\ text {batch1} _i \ mathbin {@} \ text {batch2} _i)</target>
        </trans-unit>
        <trans-unit id="65617250468b820b83705d0c5a443bdbe573e367" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \frac{\text{input}_i}{\text{other}_i}</source>
          <target state="translated">\ text {out} _i = \ frac {\ text {input} _i} {\ text {other} _i}</target>
        </trans-unit>
        <trans-unit id="07bf2f98a46000c9d46f464978022925a2bd7e3f" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ frac {\ text {tensor1} _i} {\ text {tensor2} _i}</target>
        </trans-unit>
        <trans-unit id="3da52d8260a7cb2e43645767f57dc80dcb7dfe10" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i + \ text {value} \ times \ text {tensor1} _i \ times \ text {tensor2} _i</target>
        </trans-unit>
        <trans-unit id="62f87f35ab6f8ad57797570d5573c2164ad3b265" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ mathbin {@} \ text {mat2} _i</target>
        </trans-unit>
        <trans-unit id="7a9589802a74e5f4ebdc7b3bf9edf66a572b8b2d" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{input}_i \times \text{other}_i</source>
          <target state="translated">\ text {out} _i = \ text {input} _i \ times \ text {other} _i</target>
        </trans-unit>
        <trans-unit id="598abd50041125894fc9d56c763e555dc53626ff" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{other} \times \text{input}_i</source>
          <target state="translated">\ text {out} _i = \ text {other} \ times \ text {input} _i</target>
        </trans-unit>
        <trans-unit id="f19bf9db2f65318c8d3279e96d9b9001602fb09a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{self} ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = \ text {self} ^ {\ text {exponent} _i}</target>
        </trans-unit>
        <trans-unit id="a7a2515cc1e93998b3a18636546dddfa38daeec0" translate="yes" xml:space="preserve">
          <source>\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)</source>
          <target state="translated">\ text {out} _i = \ text {start} _i + \ text {weight} _i \ times (\ text {end} _i-\ text {start} _i)</target>
        </trans-unit>
        <trans-unit id="0cb57186f10cc53449c5a3667b3e9e3171f06840" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ \text{exponent}</source>
          <target state="translated">\ text {out} _i = x_i ^ \ text {exponent}</target>
        </trans-unit>
        <trans-unit id="fc2ce21ca05c23e8481f3ba74137e106e861116a" translate="yes" xml:space="preserve">
          <source>\text{out}_i = x_i ^ {\text{exponent}_i}</source>
          <target state="translated">\ text {out} _i = x_i ^ {\ text {지수} _i}</target>
        </trans-unit>
        <trans-unit id="199c9bafa2de3cddd25186555e3341b49eafa3be" translate="yes" xml:space="preserve">
          <source>\text{out}_i \sim \text{Poisson}(\text{input}_i)</source>
          <target state="translated">\ text {out} _i \ sim \ text {Poisson} (\ text {input} _i)</target>
        </trans-unit>
        <trans-unit id="ec5b93008f32dc9277b22a2479b439d557449a63" translate="yes" xml:space="preserve">
          <source>\text{out}_{i+1} = \text{out}_i + \text{step}.</source>
          <target state="translated">\ text {out} _ {i + 1} = \ text {out} _i + \ text {step}.</target>
        </trans-unit>
        <trans-unit id="ebe2d70cc579db08dc5e67ffe414031300d09f4c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = I_0(\text{input}_{i}) = \sum_{k=0}^{\infty} \frac{(\text{input}_{i}^2/4)^k}{(k!)^2}</source>
          <target state="translated">\ text {out} _ {i} = I_0 (\ text {input} _ {i}) = \ sum_ {k = 0} ^ {\ infty} \ frac {(\ text {input} _ {i} ^ 2 / 4) ^ k} {(k!) ^ 2}</target>
        </trans-unit>
        <trans-unit id="6b1003c5523ecc63d8ba6aacefacf06606afa128" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="f24a159b70e10e5357a57a49e58cd32d187c432a" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cos^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cos ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="21103602f770a5f7ee7fc1da1e89140f56599227" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="84def1c39988847c83528f8fb8f04adcd967d458" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \cosh^{-1}(\text{input}_{i})</source>
          <target state="translated">\ text {out} _ {i} = \ cosh ^ {-1} (\ text {input} _ {i})</target>
        </trans-unit>
        <trans-unit id="0f2819fdd8d29dbcba0965986f63a77a316be87c" translate="yes" xml:space="preserve">
          <source>\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}</source>
          <target state="translated">\ text {out} _ {i} = \ frac {1} {1 + e ^ {-\ text {input} _ {i}}}</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
