<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="360796ce4b88f8a72813e93a376d34c0228713ee" translate="yes" xml:space="preserve">
          <source>Add histogram to summary.</source>
          <target state="translated">Add histogram to summary.</target>
        </trans-unit>
        <trans-unit id="1b6b7d6099858bcb255707e61a495522a0414994" translate="yes" xml:space="preserve">
          <source>Add image data to summary.</source>
          <target state="translated">Add image data to summary.</target>
        </trans-unit>
        <trans-unit id="a392c90962df4acfb05e6d0ec46fd5a47980ff9d" translate="yes" xml:space="preserve">
          <source>Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</source>
          <target state="translated">Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</target>
        </trans-unit>
        <trans-unit id="1a9584fc26e356ef39ba9497c010651d8eae52d2" translate="yes" xml:space="preserve">
          <source>Add observer for the leaf child of the module.</source>
          <target state="translated">모듈의 리프 자식에 대한 관찰자를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="701a19440eea643652da7e69f517b8c3d995eacc" translate="yes" xml:space="preserve">
          <source>Add scalar data to summary.</source>
          <target state="translated">Add scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="7a54b0b6b6b02f66bd6beaecffa1d99a5f7eb192" translate="yes" xml:space="preserve">
          <source>Add text data to summary.</source>
          <target state="translated">Add text data to summary.</target>
        </trans-unit>
        <trans-unit id="c77f8dadb528de130d984e34a450fb3582dda9aa" translate="yes" xml:space="preserve">
          <source>Add video data to summary.</source>
          <target state="translated">Add video data to summary.</target>
        </trans-unit>
        <trans-unit id="8afe6f3185ed76e023f80393638df4f157d48f52" translate="yes" xml:space="preserve">
          <source>Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</source>
          <target state="translated">Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</target>
        </trans-unit>
        <trans-unit id="4c9c6ec239352be639e5bda7f0d0b398caeb9237" translate="yes" xml:space="preserve">
          <source>Adding support for operators</source>
          <target state="translated">Adding support for operators</target>
        </trans-unit>
        <trans-unit id="4e032cc35826dbb2a4a560c47d8b0573e59463ab" translate="yes" xml:space="preserve">
          <source>Additional args:</source>
          <target state="translated">Additional args:</target>
        </trans-unit>
        <trans-unit id="b02695e3612016d107e21b0824bdec94963550bb" translate="yes" xml:space="preserve">
          <source>Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt; . For each value in &lt;code&gt;src&lt;/code&gt; , the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d9baa44eaf376e383c39686ddc11f7bd15a1ea88" translate="yes" xml:space="preserve">
          <source>Adds a buffer to the module.</source>
          <target state="translated">Adds a buffer to the module.</target>
        </trans-unit>
        <trans-unit id="677c1d2632a65843e9a01046b892b48b9f5176ac" translate="yes" xml:space="preserve">
          <source>Adds a child module to the current module.</source>
          <target state="translated">Adds a child module to the current module.</target>
        </trans-unit>
        <trans-unit id="96b77db46874289e2dd12b6bc9489ce00f46440a" translate="yes" xml:space="preserve">
          <source>Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</source>
          <target state="translated">Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</target>
        </trans-unit>
        <trans-unit id="f9bccaca3fe9322a4122d15d5e4fdb25202c0320" translate="yes" xml:space="preserve">
          <source>Adds a parameter to the module.</source>
          <target state="translated">Adds a parameter to the module.</target>
        </trans-unit>
        <trans-unit id="7fc7b19e96d1e48b76f041d88e6e8c8791a06fa6" translate="yes" xml:space="preserve">
          <source>Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;scatter_()&lt;/code&gt;&lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;scatter_()&lt;/code&gt; &lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt; , it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8562c008b76c53309d3a0973a409f19d43babb11" translate="yes" xml:space="preserve">
          <source>Adds many scalar data to summary.</source>
          <target state="translated">Adds many scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="8058a9f23e00e9bb14b6abf55b56e8a8543f9c29" translate="yes" xml:space="preserve">
          <source>Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</source>
          <target state="translated">Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</target>
        </trans-unit>
        <trans-unit id="fd54948ee53264250bb1b351262b4ccb8986d94e" translate="yes" xml:space="preserve">
          <source>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</source>
          <target state="translated">Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</target>
        </trans-unit>
        <trans-unit id="ab685d43fc5e49c46e526f76f07f3d59e811972e" translate="yes" xml:space="preserve">
          <source>Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</target>
        </trans-unit>
        <trans-unit id="b2b204082818c243f404c5bfdc3dc16bbc32640c" translate="yes" xml:space="preserve">
          <source>After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</source>
          <target state="translated">After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</target>
        </trans-unit>
        <trans-unit id="cd74631d206c5c267345dcc56490ab6ea1c83f20" translate="yes" xml:space="preserve">
          <source>After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, or &lt;code&gt;str&lt;/code&gt;. All values must be of the same type; heterogenous types for enum values are not supported.</source>
          <target state="translated">After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt; , &lt;code&gt;float&lt;/code&gt; , or &lt;code&gt;str&lt;/code&gt; . All values must be of the same type; heterogenous types for enum values are not supported.</target>
        </trans-unit>
        <trans-unit id="587806098e59106c8c783b918f92e7fc29e4d2ab" translate="yes" xml:space="preserve">
          <source>After fetching a list of samples using the indices from sampler, the function passed as the &lt;code&gt;collate_fn&lt;/code&gt; argument is used to collate lists of samples into batches.</source>
          <target state="translated">샘플러에서 인덱스를 사용하여 샘플 목록을 가져온 후 &lt;code&gt;collate_fn&lt;/code&gt; 인수 로 전달 된 함수를 사용하여 샘플 목록을 배치로 대조합니다.</target>
        </trans-unit>
        <trans-unit id="c52a739b9d98878e44d92e91256235a6d294431c" translate="yes" xml:space="preserve">
          <source>After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="f3dd860a8580060d82d526ad04c5badbd57df94f" translate="yes" xml:space="preserve">
          <source>After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</source>
          <target state="translated">After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</target>
        </trans-unit>
        <trans-unit id="e03f01ec1cb89a34bde4816ebeb0fe756b9b7364" translate="yes" xml:space="preserve">
          <source>After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="9ae05a9ce240652b1911853b65ddd62f9764383c" translate="yes" xml:space="preserve">
          <source>AlexNet</source>
          <target state="translated">AlexNet</target>
        </trans-unit>
        <trans-unit id="ca5a1956913984160d31d9bf92ec542323ba8065" translate="yes" xml:space="preserve">
          <source>AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="translated">AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</target>
        </trans-unit>
        <trans-unit id="52fc4196dd42d00268d82ad7f69953e231b88729" translate="yes" xml:space="preserve">
          <source>Alexnet</source>
          <target state="translated">Alexnet</target>
        </trans-unit>
        <trans-unit id="67e15eb99dc0e473c962d6a494d00e048e9f6fc6" translate="yes" xml:space="preserve">
          <source>Algorithms</source>
          <target state="translated">Algorithms</target>
        </trans-unit>
        <trans-unit id="c4836f5ef10696c1523f7a542472a979fa86940f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="251f8be41a3289fe0ff13ca86e83184aaa6d1685" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt;&lt;code&gt;clamp_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt; &lt;code&gt;clamp_()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8b1674d7e68b7ce19ae1eac32e5f2d61ba39a3fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b806d3eb44bb5995d21b017162c333c30909bfd1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt;&lt;code&gt;numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt; &lt;code&gt;numel()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8003ce6f6a4824ac7e86312aee5a809819964394" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae0811ab2275dcf323f302291b8578862167855b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dff47f32b9bc388203bf7e97dfa096239e9aa4fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="27f71b5c7ea505c6db6e7c3f268907cac51983c8" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8915eae6d7b9ee1bf0b6f73f5bcf701208d83a87" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="12a1bcfe361d7554bc235392cdf072ae77235de7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21de09801d295b9e55c5ca95a97e6b707bdd91c4" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42b48abe60c1061b77c3d37b15698be8fee9c90" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b913a66219f4d1a88ba91e403c72efb7f3ef3abb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4ba0bdfda926bfb5d618f4ee65334d6fb8eb1eee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="346a30cebac1d8f7c67c1c3a7f4f3bc3613f3f76" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b313b53bc520608d37513795d2fb79da1dd596f1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fd6c12495884b27e735c199934fdc7b613fa577d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="982d40969f8de3053b6a7ba2029aed4446d9284b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="2d9a228af380b06ff04ec79aacc440723157779f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="93282bf10a2917c15fe551a51f5161e90b61877f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5c53bd5a9189ae9c36a9718c39849a71cf4795d9" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="84a5bea4d6c5059fe64ceca353dc77a1a8d6f0c7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="07f60a052509a04e4581fb6abfc90937c2c71d1e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="d26757f4ad01130675ba8ea2672c12a2db5b8aab" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="f1445190c40314f7662b67191fe5caef59adeec0" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="b7d58a750debce958ceee5c25ad01617693d3ea2" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="3aec840f94f18b783a10543e6249d4a95dd33cf1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="606d0a5ac071a495928d8903629e0d3c2c53055d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="dee63cafa3f7f5b85ddbc8fc58438f4ee2396691" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="8c0014cc8ab298138389abb57c1b96765b473b42" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="23c4daa625a3c02d837a48a43fcee80a5cf020be" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="2cc182b7bdf6fcacb622e0e829ca673368ffde7e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="caa996e0ac1f8de7d67f7455447758713ba8feee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="392a801bbfecc8b02cebb4cb713410918e491a2f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="9b13ac85fa2f8f4adaad80131c925608d0b684eb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="6b6841b18d2b1421ff7068e9ab0cf0387e0beeda" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="4bb2a8621505158fea7a3e7e83a0736217506d1f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="3eaccfa1051feb57b0d0c06ee1f7f0ca18ed2f48" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="80bfac967c271e4287120446d7e735504a25deee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="63adb787982c4f010005f5042a7176ac57a5c3dd" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="004a1c4ca4c726826c71b8d3f3c668a30c19eca5" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">필드 번호 0의 ​​별명</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">필드 번호 1의 별명</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">필드 번호 2의 별명</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">필드 번호 3의 별명</target>
        </trans-unit>
        <trans-unit id="c8d021883b0a18d7d212863e386fe9e4590e884d" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="50e05f8a73a607a4d8a0fae45b49062e73a2e308" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="47e1617de19cb0d4a15c8e0fcabf777055dd3bb7" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="9e010f721821118294a0c5b2474248233557907a" translate="yes" xml:space="preserve">
          <source>All &lt;code&gt;Tensor&lt;/code&gt; s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you&amp;rsquo;re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.</source>
          <target state="translated">모든 &lt;code&gt;Tensor&lt;/code&gt; 는 적용된 내부 작업을 추적하고 구현에서 텐서가 함수 중 하나에서 역방향으로 저장되었지만 나중에 내부에서 수정되었음을 감지하면 역방향 전달이 완료되면 오류가 발생합니다. 시작되었습니다. 이렇게하면 내부 함수를 사용 중이고 오류가 표시되지 않는 경우 계산 된 그래디언트가 올바른지 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="199b1c80b5c9c67ba6a90eb5f4a37f1d39d57d4e" translate="yes" xml:space="preserve">
          <source>All CUDA kernels queued within its context will be enqueued on a selected stream.</source>
          <target state="translated">컨텍스트 내 대기열에있는 모든 CUDA 커널은 선택한 스트림의 대기열에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="7d50263b9330ad372189eff67c5c8da79c613de2" translate="yes" xml:space="preserve">
          <source>All RNN modules accept packed sequences as inputs.</source>
          <target state="translated">모든 RNN 모듈은 패킹 된 시퀀스를 입력으로 받아들입니다.</target>
        </trans-unit>
        <trans-unit id="0378c5b2171bc1b88405ce471d2eb658bdba38cd" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">이 모든 텐서 &lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 입니다 &lt;code&gt;False&lt;/code&gt; 관례 잎 텐서됩니다.</target>
        </trans-unit>
        <trans-unit id="bcd80e614fd3639e154329146ec9d72471b9fa7b" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">이 모든 텐서 &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 입니다 &lt;code&gt;False&lt;/code&gt; 관례 잎 텐서됩니다.</target>
        </trans-unit>
        <trans-unit id="fae256c49cab7960ddc648f6bae5910cd2d3503e" translate="yes" xml:space="preserve">
          <source>All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt;.</source>
          <target state="translated">양자화 된 버전을 제외한 모든 TorchVision 모델은 ONNX로 내보낼 수 있습니다. 자세한 내용은 &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5dce89529d95dc69f4c1f1ca05bfc2111d81383b" translate="yes" xml:space="preserve">
          <source>All arguments are forwarded to the &lt;code&gt;setuptools.Extension&lt;/code&gt; constructor.</source>
          <target state="translated">모든 인수는 &lt;code&gt;setuptools.Extension&lt;/code&gt; 생성자 로 전달됩니다 .</target>
        </trans-unit>
        <trans-unit id="11ff807dac1aacbdf6348e8f37bb396417fac326" translate="yes" xml:space="preserve">
          <source>All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite &lt;code&gt;__getitem__()&lt;/code&gt;, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite &lt;code&gt;__len__()&lt;/code&gt;, which is expected to return the size of the dataset by many &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; implementations and the default options of &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">키에서 데이터 샘플로의 맵을 나타내는 모든 데이터 세트는이를 하위 클래스로 분류해야합니다. 모든 서브 클래스는 &lt;code&gt;__getitem__()&lt;/code&gt; 덮어 써서 주어진 키에 대한 데이터 샘플 가져 오기를 지원 해야 합니다. 서브 클래스는 또한 선택적으로 &lt;code&gt;__len__()&lt;/code&gt; 덮어 쓸 수 있으며 , 이는 많은 &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;Sampler&lt;/code&gt; &lt;/a&gt; 구현 및 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt; 의 기본 옵션에 의해 데이터 세트의 크기를 반환 할 것으로 예상됩니다 .</target>
        </trans-unit>
        <trans-unit id="82dec6743296a8c3a95df2231d58c66c783ae2bd" translate="yes" xml:space="preserve">
          <source>All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.</source>
          <target state="translated">반복 가능한 데이터 샘플을 나타내는 모든 데이터 세트는이를 하위 클래스로 분류해야합니다. 이러한 형태의 데이터 세트는 데이터가 스트림에서 나올 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="7c7fdd1a2c6981ce3b8e5aa9cb3e2ddd838235f0" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; may contain additional names that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">모든 차원 이름 &lt;code&gt;self&lt;/code&gt; 에 있어야합니다 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 에는 &lt;code&gt;self.names&lt;/code&gt; 에 없는 추가 이름 이 포함될 수 있습니다 . 출력 텐서는 각각의 새 이름에 대해 크기가 1 차원입니다.</target>
        </trans-unit>
        <trans-unit id="1576ac96be05f9aa6b0251d2e542e4b0f488bd8b" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;code&gt;other.names&lt;/code&gt;. &lt;code&gt;other&lt;/code&gt; may contain named dimensions that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 모든 차원 이름은 &lt;code&gt;other.names&lt;/code&gt; 에 있어야합니다 . &lt;code&gt;other&lt;/code&gt; 는 &lt;code&gt;self.names&lt;/code&gt; 에 없는 명명 된 차원을 포함 할 수 있습니다 . 출력 텐서는 각각의 새 이름에 대해 크기가 1 차원입니다.</target>
        </trans-unit>
        <trans-unit id="91188b536d6c07689225e8b91d26aa659eef6d49" translate="yes" xml:space="preserve">
          <source>All elements must be greater than</source>
          <target state="translated">모든 요소는 다음보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="fa3a1109ed903de03a2630b0856e9124664ffc41" translate="yes" xml:space="preserve">
          <source>All functions must be valid TorchScript functions (including &lt;code&gt;__init__()&lt;/code&gt;).</source>
          <target state="translated">모든 함수는 유효한 TorchScript 함수 여야합니다 ( &lt;code&gt;__init__()&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e460db881180d9a3276146d738dac2c41672c148" translate="yes" xml:space="preserve">
          <source>All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.</source>
          <target state="translated">이 스트림에 제출 된 모든 향후 작업은 호출 시점에 지정된 스트림에 제출 된 모든 커널이 완료 될 때까지 대기합니다.</target>
        </trans-unit>
        <trans-unit id="d5ff4adfb04564464a64a43c245685fb2b5d3919" translate="yes" xml:space="preserve">
          <source>All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.</source>
          <target state="translated">모든 입력은 모양, dtype 및 레이아웃이 일치해야합니다. 출력 텐서는 모양, dtype 및 레이아웃이 동일합니다.</target>
        </trans-unit>
        <trans-unit id="8102d7ae18041c98a7f876840a19500755dde277" translate="yes" xml:space="preserve">
          <source>All modules, no matter their device, are always loaded onto the CPU during loading. This is different from &lt;a href=&quot;torch.load#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s semantics and may change in the future.</source>
          <target state="translated">장치에 관계없이 모든 모듈은로드하는 동안 항상 CPU에로드됩니다. 이것은 &lt;a href=&quot;torch.load#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; 의 의미와 다르며 향후 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="391810af4c5adb65a3bbe96665345a54a154a4ff" translate="yes" xml:space="preserve">
          <source>All of &lt;code&gt;dims&lt;/code&gt; must be consecutive in order in the &lt;code&gt;self&lt;/code&gt; tensor, but not necessary contiguous in memory.</source>
          <target state="translated">모든 &lt;code&gt;dims&lt;/code&gt; 은 &lt;code&gt;self&lt;/code&gt; 텐서 에서 순서대로 연속되어야 하지만 메모리에서 연속적 일 필요는 없습니다.</target>
        </trans-unit>
        <trans-unit id="6c24f14499bb0bd88c18f05a4fe7875666f28f6d" translate="yes" xml:space="preserve">
          <source>All of the dims of &lt;code&gt;self&lt;/code&gt; must be named in order to use this method. The resulting tensor is a view on the original tensor.</source>
          <target state="translated">이 방법을 사용하려면 &lt;code&gt;self&lt;/code&gt; 의 모든 희미한 이름을 지정해야합니다. 결과 텐서는 원래 텐서의 뷰입니다.</target>
        </trans-unit>
        <trans-unit id="8d9325f5f71bde68b50e39d98dd2b2fc4b9b3f89" translate="yes" xml:space="preserve">
          <source>All operations that support named tensors propagate names.</source>
          <target state="translated">명명 된 텐서를 지원하는 모든 작업은 이름을 전파합니다.</target>
        </trans-unit>
        <trans-unit id="b60bcfd2d0b2dce7638e7cd3bf4d7879477608de" translate="yes" xml:space="preserve">
          <source>All optimizers implement a &lt;a href=&quot;#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt; method, that updates the parameters. It can be used in two ways:</source>
          <target state="translated">모든 최적화 프로그램 은 매개 변수를 업데이트 하는 &lt;a href=&quot;#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; &lt;/a&gt; 메서드를 구현합니다 . 두 가지 방법으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c773fd48da7cc8c1f4621a11671c67f1f6303a0c" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. You can use the following transform to normalize:</source>
          <target state="translated">모든 사전 훈련 된 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. [0, 1] 범위로로드 된 다음 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 및 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; 사용하여 정규화됩니다 . 다음 변환을 사용하여 정규화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec2e3e9eabe115169e1deab63808ca48a84df052" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; and &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt;.</source>
          <target state="translated">사전 훈련 된 모든 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. 즉, H와 W는 112가 될 것으로 예상되는 형태 (3 x T x H x W)의 3 채널 RGB 비디오 미니 배치, T는 클립의 비디오 프레임 수. 이미지는 [0, 1] 범위로로드 한 다음 &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; 및 &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt; 사용하여 정규화해야 합니다.</target>
        </trans-unit>
        <trans-unit id="7463404aa8619413edebf6ff4257cb99add42cba" translate="yes" xml:space="preserve">
          <source>All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn&amp;rsquo;t have certain devices), an exception is raised.</source>
          <target state="translated">이전에 저장 한 모든 모듈은 장치에 관계없이 먼저 CPU에로드 된 다음 저장된 장치로 이동됩니다. 이것이 실패하면 (예 : 런타임 시스템에 특정 장치가 없기 때문에) 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="1b8a41c0d78fe02aeede8e4c39fabb850893cedd" translate="yes" xml:space="preserve">
          <source>All subclasses should overwrite &lt;code&gt;__iter__()&lt;/code&gt;, which would return an iterator of samples in this dataset.</source>
          <target state="translated">모든 서브 클래스는 &lt;code&gt;__iter__()&lt;/code&gt; 덮어 써야 하며 ,이 데이터 세트에서 샘플의 반복자를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="f4fb70a59932061a3d80fc551911560fa69384f6" translate="yes" xml:space="preserve">
          <source>All summed &lt;code&gt;dim&lt;/code&gt; are squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting an output tensor having &lt;code&gt;dim&lt;/code&gt; fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">모든 합산 된 &lt;code&gt;dim&lt;/code&gt; 이 압축되어 ( &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt; 참조 ) 출력 텐서 가 &lt;code&gt;input&lt;/code&gt; 보다 &lt;code&gt;dim&lt;/code&gt; 더 적은 차원을 갖게 됩니다.</target>
        </trans-unit>
        <trans-unit id="6bd30243e5faf7879695a6c25ee7d928afafccc6" translate="yes" xml:space="preserve">
          <source>All tensors need to be of the same size.</source>
          <target state="translated">모든 텐서는 크기가 같아야합니다.</target>
        </trans-unit>
        <trans-unit id="647c455337c3b30ca33d7810c4e9c4f5a9641226" translate="yes" xml:space="preserve">
          <source>All the weights and biases are initialized from</source>
          <target state="translated">모든 가중치와 편향은 다음에서 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="970c0ae90b9eb34e0cb6a356cf47964705868146" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces.</source>
          <target state="translated">모델이 다른 표현 부분 공간의 정보에 공동으로 참여할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5702cc92d2bcf7e780dcdf99f53eee4b5e88684e" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need</source>
          <target state="translated">모델이 다른 표현 부분 공간의 정보에 공동으로 참여할 수 있습니다. 참조 참조 :주의가 필요한 모든 것</target>
        </trans-unit>
        <trans-unit id="bcb917dff27ba02ff781ce514dcf9c4e2df73ba5" translate="yes" xml:space="preserve">
          <source>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</source>
          <target state="translated">Alpha Dropout은 자체 정규화 속성을 유지하는 Dropout 유형입니다. 평균이 0이고 단위 표준 편차가있는 입력의 경우 알파 드롭 아웃의 출력은 입력의 원래 평균과 표준 편차를 유지합니다. Alpha Dropout은 SELU 활성화 기능과 함께 사용되어 출력의 평균 및 단위 표준 편차가 0이되도록합니다.</target>
        </trans-unit>
        <trans-unit id="468dc142cbb278c344ed57a0c2341d7dabf6044d" translate="yes" xml:space="preserve">
          <source>AlphaDropout</source>
          <target state="translated">AlphaDropout</target>
        </trans-unit>
        <trans-unit id="697ae46b13c17cf9f965bf8eeb7388f2f2599380" translate="yes" xml:space="preserve">
          <source>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">또한 기본적으로 훈련 중에이 계층은 계산 된 평균 및 분산의 추정치를 계속 실행하며, 평가 중에 정규화에 사용됩니다. 실행 추정치는 기본 &lt;code&gt;momentum&lt;/code&gt; 0.1 로 유지됩니다 .</target>
        </trans-unit>
        <trans-unit id="8bf20b1915a4ba9029b2263ed8aac08cdcf55753" translate="yes" xml:space="preserve">
          <source>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</source>
          <target state="translated">장식 자 역할도합니다. (괄호로 인스턴스화해야합니다.)</target>
        </trans-unit>
        <trans-unit id="51924beeca0c98eba05c19c63804e09424b9e355" translate="yes" xml:space="preserve">
          <source>Also known as Glorot initialization.</source>
          <target state="translated">Glorot 초기화라고도합니다.</target>
        </trans-unit>
        <trans-unit id="99d08174dc1487c12f409bf77bbf4a6a16dc84ad" translate="yes" xml:space="preserve">
          <source>Also known as He initialization.</source>
          <target state="translated">He 초기화라고도합니다.</target>
        </trans-unit>
        <trans-unit id="9319f57915da9aa68da62d35edfd64135450d0d5" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(input_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;input_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">또한 유의 &lt;code&gt;len(input_tensor_lists)&lt;/code&gt; 와 각 요소의 크기 &lt;code&gt;input_tensor_lists&lt;/code&gt; (각 요소 목록이며, 따라서 &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt; )이 함수를 호출 모든 분산 프로세스에 대해 동일해야한다.</target>
        </trans-unit>
        <trans-unit id="3a0c3f7b9176b92a14d4b6b5cc0f2c401a31947c" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(output_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;output_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">또한 유의 &lt;code&gt;len(output_tensor_lists)&lt;/code&gt; 와 각 요소의 크기 &lt;code&gt;output_tensor_lists&lt;/code&gt; (각 요소 목록이며, 따라서 &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt; )이 함수를 호출 모든 분산 프로세스에 대해 동일해야한다.</target>
        </trans-unit>
        <trans-unit id="0c8280a06795f9ac5918243b5469526506807f31" translate="yes" xml:space="preserve">
          <source>Although CUDA versions &amp;gt;= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.</source>
          <target state="translated">CUDA 버전&amp;gt; = 11은 두 가지 이상의 우선 순위 수준을 지원하지만 PyTorch에서는 두 가지 수준의 우선 순위 만 지원합니다.</target>
        </trans-unit>
        <trans-unit id="043751522414e7c9aaee8aec5948d07968c78350" translate="yes" xml:space="preserve">
          <source>Although the recipe for forward pass needs to be defined within this function, one should call the &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</source>
          <target state="translated">포워드 패스를위한 레시피는이 함수 내에서 정의되어야 하지만, 전자는 등록 된 후크를 실행하고 후자는 자동으로 무시하므로 나중에 &lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 인스턴스를 호출해야 합니다.</target>
        </trans-unit>
        <trans-unit id="a16ed36f1048b1b7e767b2e1655943a9aa7dfbb6" translate="yes" xml:space="preserve">
          <source>An &lt;code&gt;RRef&lt;/code&gt; (Remote REFerence) is a reference to a value of some type &lt;code&gt;T&lt;/code&gt; (e.g. &lt;code&gt;Tensor&lt;/code&gt;) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules&lt;/a&gt; that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;Remote Reference Protocol&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;RRef&lt;/code&gt; (원격 참조)는 어떤 유형의 값에 대한 참조 인 &lt;code&gt;T&lt;/code&gt; (예 &lt;code&gt;Tensor&lt;/code&gt; 원격 작업자). 이 핸들은 참조 된 원격 값을 소유자에게 유지하지만 나중에 값이 로컬 작업자에게 전송된다는 의미는 없습니다. RRef는 다른 워커에 존재 하는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules에&lt;/a&gt; 대한 참조를 보유 하고 적절한 함수를 호출하여 훈련 중에 매개 변수를 검색하거나 수정 함으로써 다중 머신 훈련에 사용할 수 있습니다 . 자세한 내용은 &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;원격 참조 프로토콜&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="366e57ddf274ce33b825366c8f324bc7720a8851" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity.</source>
          <target state="translated">tanh 또는 ReLU 비선형 성이있는 Elman RNN 셀.</target>
        </trans-unit>
        <trans-unit id="0a352f479f8560e5a2c03ce7b4e08fa3e11c8ded" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as &lt;code&gt;torch.nn.RNNCell&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&lt;/a&gt; for documentation.</source>
          <target state="translated">tanh 또는 ReLU 비선형 성이있는 Elman RNN 셀. 입력 및 출력으로 부동 소수점 텐서를 사용하는 동적 양자화 된 RNNCell 모듈. 가중치는 8 비트로 양자화됩니다. &lt;code&gt;torch.nn.RNNCell&lt;/code&gt; 과 동일한 인터페이스를 채택합니다 . 문서 는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&lt;/a&gt; 을 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="ecda50e6fe26ce22d999e9c65fbfaac432da6710" translate="yes" xml:space="preserve">
          <source>An EventList containing FunctionEventAvg objects.</source>
          <target state="translated">FunctionEventAvg 개체를 포함하는 EventList입니다.</target>
        </trans-unit>
        <trans-unit id="89afc3eab800778902526ed4786c65d7f4ca6494" translate="yes" xml:space="preserve">
          <source>An abstract class representing a &lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt; &lt;code&gt;Dataset&lt;/code&gt; 을&lt;/a&gt; 나타내는 추상 클래스 입니다.</target>
        </trans-unit>
        <trans-unit id="cda80fd86e845c48c7faf0e81a98aca3de57ab49" translate="yes" xml:space="preserve">
          <source>An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; in order to initialize RPC with specific configurations, such as the RPC timeout and &lt;code&gt;init_method&lt;/code&gt; to be used.</source>
          <target state="translated">RPC 백엔드로 전달 된 옵션을 캡슐화하는 추상 구조입니다. 이 클래스의 인스턴스는 사용할 RPC 시간 제한 및 &lt;code&gt;init_method&lt;/code&gt; 와 같은 특정 구성으로 RPC를 초기화하기 위해 &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; 에 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="34f08a969a81be9fad81ba44cfa1144a898067e4" translate="yes" xml:space="preserve">
          <source>An additional dimension of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; is appended in the returned tensor.</source>
          <target state="translated">크기 &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 의 추가 차원이 반환 된 텐서에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="619ad847534358910127729afc029b7953abcecf" translate="yes" xml:space="preserve">
          <source>An empty dict is assumed have type &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. The types of other dict literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">빈 dict는 &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 유형을 갖는다 고 가정 합니다. 다른 dict 리터럴의 유형은 멤버 유형에서 파생됩니다. 자세한 내용은 &lt;a href=&quot;#default-types&quot;&gt;기본 유형&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c1a0eecf8a5a6cdc72f4e0da994b510481da5a4d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed have type &lt;code&gt;List[Tensor]&lt;/code&gt;. The types of other list literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">빈 목록은 &lt;code&gt;List[Tensor]&lt;/code&gt; 유형이 있다고 가정 합니다. 다른 목록 리터럴의 유형은 멤버 유형에서 파생됩니다. 자세한 내용은 &lt;a href=&quot;#default-types&quot;&gt;기본 유형&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="d84c564cad7cf9fb4e3caa453c3a49e8beeae54d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed to be &lt;code&gt;List[Tensor]&lt;/code&gt; and empty dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. To instantiate an empty list or dict of other types, use &lt;code&gt;Python 3 type hints&lt;/code&gt;.</source>
          <target state="translated">빈 목록은 &lt;code&gt;List[Tensor]&lt;/code&gt; 이고 빈 dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 라고 가정합니다 . 빈 목록 또는 다른 유형의 사전을 인스턴스화하려면 &lt;code&gt;Python 3 type hints&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc24c0a7e765c257ec2c1deb32cf9b9b1bc99fd5" translate="yes" xml:space="preserve">
          <source>An empty sparse tensor can be constructed by specifying its size:</source>
          <target state="translated">빈 희소 텐서는 크기를 지정하여 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="17f91bb7684b57d410e71c667849b986b1e265c6" translate="yes" xml:space="preserve">
          <source>An enum class of available backends.</source>
          <target state="translated">사용 가능한 백엔드의 열거 형 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="a60c11218c3c0643831cc9fba424f6f68723b504" translate="yes" xml:space="preserve">
          <source>An enum-like class for available reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt;.</source>
          <target state="translated">사용 가능한 축소 작업에 대한 열거 형 클래스 : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; , &lt;code&gt;MAX&lt;/code&gt; , &lt;code&gt;BAND&lt;/code&gt; , &lt;code&gt;BOR&lt;/code&gt; 및 &lt;code&gt;BXOR&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c5d6691fbcb10d6ab33410657c5bcc031006a7f4" translate="yes" xml:space="preserve">
          <source>An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.</source>
          <target state="translated">사용 가능한 백엔드의 열거 형 클래스 : GLOO, NCCL, MPI 및 기타 등록 된 백엔드.</target>
        </trans-unit>
        <trans-unit id="c399f6bcf61d242f357b89e04dfba538f2cb230b" translate="yes" xml:space="preserve">
          <source>An example for the usage of &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;TransformedDistribution&lt;/code&gt;&lt;/a&gt; would be:</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;TransformedDistribution&lt;/code&gt; &lt;/a&gt; 의 사용 예 는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="519ac03d4f41ca6837697ea6c98bf8c921545893" translate="yes" xml:space="preserve">
          <source>An example of such normalization can be found in the imagenet example &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;here&lt;/a&gt;</source>
          <target state="translated">이러한 정규화의 예는 &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;여기&lt;/a&gt; 이미지 넷 예 에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3c1ceedb1dedcde4e24919af77558ac6f1600e4d" translate="yes" xml:space="preserve">
          <source>An example where &lt;code&gt;transform_to&lt;/code&gt; and &lt;code&gt;biject_to&lt;/code&gt; differ is &lt;code&gt;constraints.simplex&lt;/code&gt;: &lt;code&gt;transform_to(constraints.simplex)&lt;/code&gt; returns a &lt;a href=&quot;#torch.distributions.transforms.SoftmaxTransform&quot;&gt;&lt;code&gt;SoftmaxTransform&lt;/code&gt;&lt;/a&gt; that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, &lt;code&gt;biject_to(constraints.simplex)&lt;/code&gt; returns a &lt;a href=&quot;#torch.distributions.transforms.StickBreakingTransform&quot;&gt;&lt;code&gt;StickBreakingTransform&lt;/code&gt;&lt;/a&gt; that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.</source>
          <target state="translated">예 &lt;code&gt;transform_to&lt;/code&gt; 및 &lt;code&gt;biject_to&lt;/code&gt; 가 다를이다 &lt;code&gt;constraints.simplex&lt;/code&gt; : &lt;code&gt;transform_to(constraints.simplex)&lt;/code&gt; 반환 &lt;a href=&quot;#torch.distributions.transforms.SoftmaxTransform&quot;&gt; &lt;code&gt;SoftmaxTransform&lt;/code&gt; 를&lt;/a&gt; 단순히 exponentiates 및 그 입력을 정규화하고; 이것은 SVI와 같은 알고리즘에 적합한 저렴하고 대체로 좌표에 맞는 연산입니다. 대조적으로, &lt;code&gt;biject_to(constraints.simplex)&lt;/code&gt; 는 입력을 1 차원 이하의 공간으로 biject 하는 &lt;a href=&quot;#torch.distributions.transforms.StickBreakingTransform&quot;&gt; &lt;code&gt;StickBreakingTransform&lt;/code&gt; &lt;/a&gt; 을 반환합니다 . 이것은 더 비싸고 덜 수치 적으로 안정적인 변환이지만 HMC와 같은 알고리즘에 필요합니다.</target>
        </trans-unit>
        <trans-unit id="44802d0d5256af16bcf81baadaa0893f0644e7bc" translate="yes" xml:space="preserve">
          <source>An integral output tensor cannot accept a floating point tensor.</source>
          <target state="translated">적분 출력 텐서는 부동 소수점 텐서를 받아 들일 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="2d42c640da9c089ad44c78deb2cd826d6b0df3f8" translate="yes" xml:space="preserve">
          <source>An iterable Dataset.</source>
          <target state="translated">반복 가능한 데이터 세트.</target>
        </trans-unit>
        <trans-unit id="498090423a5e476802043e5808dfa44461e23184" translate="yes" xml:space="preserve">
          <source>An iterable-style dataset is an instance of a subclass of &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;__iter__()&lt;/code&gt; protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.</source>
          <target state="translated">반복 가능한 스타일 데이터 세트는 &lt;code&gt;__iter__()&lt;/code&gt; 프로토콜 을 구현하는 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 하위 클래스의 인스턴스이며 데이터 샘플에 대한 반복 가능을 나타냅니다. 이러한 유형의 데이터 세트는 무작위 읽기가 비싸거나 가능성이 거의없고 배치 크기가 가져온 데이터에 따라 달라지는 경우에 특히 적합합니다.</target>
        </trans-unit>
        <trans-unit id="ada1c75faa68087ab2e8ec52c71f9a5f98fa8946" translate="yes" xml:space="preserve">
          <source>An torch.Generator object.</source>
          <target state="translated">torch.Generator 개체입니다.</target>
        </trans-unit>
        <trans-unit id="042787a30d514e644feadd2d89eacccefc21c5e7" translate="yes" xml:space="preserve">
          <source>Anomaly detection</source>
          <target state="translated">이상 탐지</target>
        </trans-unit>
        <trans-unit id="ef94106a43404d867533b831248eaa8963699ea0" translate="yes" xml:space="preserve">
          <source>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired &lt;code&gt;world_size&lt;/code&gt;. The URL should start with &lt;code&gt;file://&lt;/code&gt; and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn&amp;rsquo;t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; call on the same file path/name.</source>
          <target state="translated">또 다른 초기화 방법은 원하는 &lt;code&gt;world_size&lt;/code&gt; 와 함께 그룹의 모든 시스템에서 공유되고 볼 수있는 파일 시스템을 사용합니다 . URL은 &lt;code&gt;file://&lt;/code&gt; 로 시작해야 하며 공유 파일 시스템에 존재하지 않는 파일 (기존 디렉토리에 있음)에 대한 경로를 포함 해야 합니다. 파일 시스템 초기화는 파일이 존재하지 않는 경우 자동으로 생성되지만 파일을 삭제하지는 않습니다. 따라서 동일한 파일 경로 / 이름에 대한 다음 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; 호출 전에 파일이 정리되었는지 확인하는 것은 사용자의 책임 입니다.</target>
        </trans-unit>
        <trans-unit id="f5194d861e34e463cbb383549f2be7e4afdfdb33" translate="yes" xml:space="preserve">
          <source>Any functions executed during the backward pass are also decorated with &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. During default backward (with &lt;code&gt;create_graph=False&lt;/code&gt;) this information is irrelevant, and in fact, &lt;code&gt;N&lt;/code&gt; may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects&amp;rsquo; &lt;code&gt;apply()&lt;/code&gt; methods are useful, as a way to correlate these Function objects with the earlier forward pass.</source>
          <target state="translated">역방향 패스 중에 실행되는 모든 함수도 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 으로 장식됩니다 . 기본 역방향 ( &lt;code&gt;create_graph=False&lt;/code&gt; 사용 ) 동안 이 정보는 관련이 없으며 실제로 &lt;code&gt;N&lt;/code&gt; 은 이러한 모든 함수에 대해 단순히 0 일 수 있습니다. 역방향 Function 개체의 &lt;code&gt;apply()&lt;/code&gt; 메서드 와 관련된 최상위 범위 만 이러한 Function 개체를 이전 순방향 패스와 연관시키는 방법으로 유용합니다.</target>
        </trans-unit>
        <trans-unit id="1eec009a2337c1a488dd1b74c1aff7cc1d139bf1" translate="yes" xml:space="preserve">
          <source>Any other functionality from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module not explitily listed in this documentation is unsupported.</source>
          <target state="translated">이 문서에 명시 적으로 나열되지 않은 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; 모듈의 다른 기능 은 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9267fbcc5ac03c112f491ee9bc7bbb21bb5ae1c5" translate="yes" xml:space="preserve">
          <source>Append the given callback function to this &lt;code&gt;Future&lt;/code&gt;, which will be run when the &lt;code&gt;Future&lt;/code&gt; is completed. Multiple callbacks can be added to the same &lt;code&gt;Future&lt;/code&gt;, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this &lt;code&gt;Future&lt;/code&gt;. The callback function can use the &lt;code&gt;Future.wait()&lt;/code&gt; API to get the value.</source>
          <target state="translated">&lt;code&gt;Future&lt;/code&gt; 가 완료되면 실행될 이 &lt;code&gt;Future&lt;/code&gt; 에 주어진 콜백 함수를 추가합니다 . 동일한 &lt;code&gt;Future&lt;/code&gt; 에 여러 콜백을 추가 할 수 있으며 추가 된 것과 동일한 순서로 호출됩니다. 콜백은이 &lt;code&gt;Future&lt;/code&gt; 에 대한 참조 인 하나의 인수를 가져야합니다 . 콜백 함수는 &lt;code&gt;Future.wait()&lt;/code&gt; API를 사용하여 값을 가져올 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f3f60044b06335eb6c4230e3c39f5e0218445878" translate="yes" xml:space="preserve">
          <source>Appendix</source>
          <target state="translated">Appendix</target>
        </trans-unit>
        <trans-unit id="c87f37c9f3693b628e726dc6f101341b952d29db" translate="yes" xml:space="preserve">
          <source>Appends a given module to the end of the list.</source>
          <target state="translated">목록 끝에 주어진 모듈을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="379011982516fbdbffaea6138d56d674a94f5e18" translate="yes" xml:space="preserve">
          <source>Appends a given parameter at the end of the list.</source>
          <target state="translated">목록 끝에 주어진 매개 변수를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="9b249ec6f74bf6c17017438ecc23770483a688cc" translate="yes" xml:space="preserve">
          <source>Appends modules from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterable의 모듈을 목록 끝에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="0e68b1f2c86ef40318501ff9b078b446cee7c813" translate="yes" xml:space="preserve">
          <source>Appends parameters from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterable의 매개 변수를 목록 끝에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="378aa8a7eda72c52086938707b31878d8a635ee4" translate="yes" xml:space="preserve">
          <source>Applied element-wise, as:</source>
          <target state="translated">다음과 같이 요소별로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="47ea67db7d24ea56bb6efc03577734231d9fa58e" translate="yes" xml:space="preserve">
          <source>Applies 2D average-pooling operation in</source>
          <target state="translated">2D 평균 풀링 작업을</target>
        </trans-unit>
        <trans-unit id="26a3eb8c50568931de48e1e7fa354baa87e37c44" translate="yes" xml:space="preserve">
          <source>Applies 3D average-pooling operation in</source>
          <target state="translated">3D 평균 풀링 작업을</target>
        </trans-unit>
        <trans-unit id="897491bacd3642b7742fd85de111604a2b1e2139" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;callable&lt;/code&gt; for each element in &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; and stores the results in &lt;code&gt;self&lt;/code&gt; tensor. &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 및 주어진 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; 각 요소에 대해 &lt;code&gt;callable&lt;/code&gt; 을 적용 하고 결과를 &lt;code&gt;self&lt;/code&gt; 텐서 에 저장합니다 . &lt;code&gt;self&lt;/code&gt; 텐서 및 주어진 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; 는 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅 가능&lt;/a&gt; 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="854151a01bba98f938fa936327548dc77fb47b3f" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;fn&lt;/code&gt; recursively to every submodule (as returned by &lt;code&gt;.children()&lt;/code&gt;) as well as self. Typical use includes initializing the parameters of a model (see also &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt;).</source>
          <target state="translated">self뿐만 아니라 모든 하위 모듈 ( &lt;code&gt;.children()&lt;/code&gt; 의해 반환 됨 )에 &lt;code&gt;fn&lt;/code&gt; 을 재귀 적으로 적용 합니다. 일반적인 사용에는 모델의 매개 변수 초기화가 포함됩니다 ( &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="f3ce1d7a64849414112d779b273ff91d45596516" translate="yes" xml:space="preserve">
          <source>Applies Alpha Dropout over the input.</source>
          <target state="translated">입력에 알파 드롭 아웃을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="13e7aada5fd7c3d78fcf6b9c0d54d77028725d8c" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization for each channel across a batch of data.</source>
          <target state="translated">데이터 배치에서 각 채널에 대해 배치 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="29f9114f73a8742a5df31f7a4732b3ef2d2fa5ed" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 2D 또는 3D 입력 (선택 사항 인 추가 채널 차원이있는 1D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="77df134b3c1dec023ae7933dc984fdf0b9a37d0b" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 4D 입력 (추가 채널 차원이있는 2D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="d5008d35280faa701906a5c7bb40531dff590ddb" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 5D 입력 (추가 채널 차원이있는 3D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="ebba8e6ee8c679b795f34839517e1330fe0f6cd0" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">문서 Batch Normalization &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 문서에 설명 된대로 N 차원 입력 (추가 채널 차원이있는 [N-2] D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="4c0b063a3d01c57a39497f55e2c1c93c68f9edde" translate="yes" xml:space="preserve">
          <source>Applies Group Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;Group Normalization&lt;/a&gt;</source>
          <target state="translated">논문 &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;그룹 정규화에&lt;/a&gt; 설명 된대로 입력의 미니 배치에 대해 그룹 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6ddcab60a314dad6ac4e8205f6d0bbe42d633f04" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization for each channel in each data sample in a batch.</source>
          <target state="translated">일괄 적으로 각 데이터 샘플의 각 채널에 대해 인스턴스 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8eba97c1edce80ebe5a8f2d8c4ea438567d2707c" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 3D 입력 (선택 사항 인 추가 채널 차원이있는 1D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="1c1378db79ff8ef5bf285d891bdde624c5ed8210" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 4D 입력 (추가 채널 차원이있는 2D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="331da363e42c71d65e839afe96ad5dce770a5331" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 5D 입력 (추가 채널 차원이있는 3D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="b97189b49bbea1acf1c1f0194b9ab27dea9973b9" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization for last certain number of dimensions.</source>
          <target state="translated">마지막 특정 수의 차원에 대해 레이어 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="835e00e544806b9f13082cf66ead874e6019f00b" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt;</source>
          <target state="translated">문서 레이어 정규화에 설명 된대로 입력의 미니 배치에 대해 &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;레이어 정규화를 적용합니다.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aeb29e205e308a59093be0d4988fa5cd74bac086" translate="yes" xml:space="preserve">
          <source>Applies SoftMax over features to each spatial location.</source>
          <target state="translated">기능에 SoftMax를 각 공간 위치에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="68727ddc338094e31b998534ceb0a7cd1e1f8321" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="0d6edcf464a5fa2406d025e84b2ed153de9c75ee" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b8f3289d5daff81c1311cb6f2ca1629e42cafa4f" translate="yes" xml:space="preserve">
          <source>Applies a 1D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3c27388a69cba70f821eaba1e90c8524f7a152d3" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized 1D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 1D 입력에 1D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="322ebd47033ac7c9c030a4056a89e60da48e7a7e" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 1D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="dcfb7d54383b0ff98c06cfb1f40c5d85baf1b8d7" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 회선을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ba598dc972597d5f42702090e4175518b59bd582" translate="yes" xml:space="preserve">
          <source>Applies a 1D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6e7aac7c6517399757e4cbb358ae6fceeed56898" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 전력 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8bfc842196843fc0db2f2d55709b870df2c8219f" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 전력 평균 풀링을 적용합니다. &lt;code&gt;p&lt;/code&gt; 의 거듭 제곱에 대한 모든 입력의 합 이 0이면 기울기도 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="96ca07ea2e411b1ba9e96092e82956074c9632be" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 1D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4b460f2239dd693e787bc347edd22db345631a88" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">&quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 신호에 대해 1D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="aa89da022829347e9eb20aeec1fec7371b510238" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 2D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="2990176eebf3f395560769ef6ba618123ba3cbdc" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8f9f8906e50a1a40da69b6542725773d5f97b476" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d576b7f4a24ec8fd4a5244faa85b001a714e4b2b" translate="yes" xml:space="preserve">
          <source>Applies a 2D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4eef15eebd1e3c58ec128ae4ac04428c46e00ff5" translate="yes" xml:space="preserve">
          <source>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</source>
          <target state="translated">여러 입력 채널로 구성된 입력 신호에 2D 이중 선형 업 샘플링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="47468ad50007cdb83f448361561cc024fef32584" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized 2D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 2D 입력에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3f8605d5715138e74b1fe2f63f6110cb337b8e9e" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="01f53bdcdfe76a89f9da101eca78388d0e3db80b" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4d7022a0fdb1e618b639247856e6903aab3855e6" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d5648bdd82e29b0ec766dab1d7ea5bd15fb57316" translate="yes" xml:space="preserve">
          <source>Applies a 2D fractional max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 2D 부분 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4dcf07079bf5dd6c28cbfd49dde4f39b4bf6b35b" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 대해 2D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a476032b7da8421b8000aa56e627562cba0b8e78" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="16be7a5fb7dca84029d9fea2edb8b4f2f0ee57f8" translate="yes" xml:space="preserve">
          <source>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</source>
          <target state="translated">여러 입력 채널로 구성된 입력 신호에 2D 최근 접 이웃 업 샘플링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="41e06a057945b6ec3fbb4344ac26327e886f585c" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 전력 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d69ec22c35e226c6aa2b1af68d48e3fa60401801" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 전력 평균 풀링을 적용합니다. &lt;code&gt;p&lt;/code&gt; 의 거듭 제곱에 대한 모든 입력의 합 이 0이면 기울기도 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="aa4b7ef410c54190539113762f6eeb2a66bf418c" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">때때로 &quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 이미지에 2D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b43e206b10ee89d84a5a812651aa838b543004e7" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 2D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="e5d643fce9e39e07cff6793421bc79eff19c89aa" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ae56276da0a49f342cfd4b640808d55b08117aa7" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="37570017fc80764b6133f223b871fd527ef1696c" translate="yes" xml:space="preserve">
          <source>Applies a 3D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="af8bbac2d382374f13d6575cf2d20a9ea3e1524d" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized 3D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 3D 입력에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="c2ec5baf08b4d035b1c172c872a14f1141d860e8" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="f6f649a7867c44dcfc76acf042b1d45c5f149502" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 회선을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="51acb800dbe635481a7193d943f2c4fdf6f9b633" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a5c65698b496aec798c667f51e238522d8ca0853" translate="yes" xml:space="preserve">
          <source>Applies a 3D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="330c7c56dc877c7532afe521797eaa31d71f0cd0" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;</source>
          <target state="translated">&quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8ceaf63b96a331eabb115beb8c3e00540f6c5ffc" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="65ce4bde72c1f78502a6398b7865adc72e50ac02" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다. 전치 컨볼 루션 연산자는 각 입력 값에 학습 가능한 커널을 요소별로 곱하고 모든 입력 특성 평면의 출력을 합산합니다.</target>
        </trans-unit>
        <trans-unit id="66c150b86ae084c85a0e9289376927637ec64e05" translate="yes" xml:space="preserve">
          <source>Applies a bilinear transformation to the incoming data:</source>
          <target state="translated">수신 데이터에 쌍 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6ff756a7c7cfe8cec72e517aee9af4136e1371be" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming data:</source>
          <target state="translated">수신 데이터에 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8ff337dc0db982c6291a92a0d7e5b13fd2dbe747" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming quantized data:</source>
          <target state="translated">들어오는 양자화 된 데이터에 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a2436507aea812ee14f1556396ee7b6b4cbdbe1d" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer Elman RNN with</source>
          <target state="translated">다층 Elman RNN을 다음과 같이 적용합니다.</target>
        </trans-unit>
        <trans-unit id="bc4223d58e32410e3d29e09ef9a1db009d298cd1" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</source>
          <target state="translated">입력 시퀀스에 다 계층 게이트 반복 장치 (GRU) RNN을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="02da21c27e34ce09b75309eefe898e142b0a5cb5" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</source>
          <target state="translated">입력 시퀀스에 다층 장단기 기억 (LSTM) RNN을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="7660fbeb32f57e21a2dce8c33a454664d3adf995" translate="yes" xml:space="preserve">
          <source>Applies a softmax followed by a logarithm.</source>
          <target state="translated">소프트 맥스 뒤에 로그를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="16d6a0fbab19784fd6044224005fc862384c1a6c" translate="yes" xml:space="preserve">
          <source>Applies a softmax function.</source>
          <target state="translated">소프트 맥스 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3511c68e1b0c335caf44167160bac43a1d7fe51a" translate="yes" xml:space="preserve">
          <source>Applies a softmin function.</source>
          <target state="translated">softmin 기능을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="cfc3d07eb38a8451b3b1a0481d05e3ce6d35b93e" translate="yes" xml:space="preserve">
          <source>Applies alpha dropout to the input.</source>
          <target state="translated">입력에 알파 드롭 아웃을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d7a232c41e3f6477e97dc6f5c3a6a2571c32c1b2" translate="yes" xml:space="preserve">
          <source>Applies element-wise</source>
          <target state="translated">요소별로 적용</target>
        </trans-unit>
        <trans-unit id="113342e492fe7e12d769f5fd51f90c782c13aba3" translate="yes" xml:space="preserve">
          <source>Applies element-wise the function</source>
          <target state="translated">요소 별 함수 적용</target>
        </trans-unit>
        <trans-unit id="f558b6f91b1db6ba6cd7e44d550b3b8856e30ab7" translate="yes" xml:space="preserve">
          <source>Applies element-wise,</source>
          <target state="translated">요소별로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="bdcea2507c8f3d67607511bd4053b83de34dceec" translate="yes" xml:space="preserve">
          <source>Applies element-wise, the function</source>
          <target state="translated">요소 별, 함수 적용</target>
        </trans-unit>
        <trans-unit id="e6f0771cea3151fdc337ea7709543dfa37058eea" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</source>
          <target state="translated">채널이 두 번째 차원을 차지하는 여러 입력 평면으로 구성된 입력 신호에 로컬 응답 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="5dfd4a4a091440af428452b6ccfc1c2f2eb1887c" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</source>
          <target state="translated">채널이 두 번째 차원을 차지하는 여러 입력 평면으로 구성된 입력 신호에 로컬 응답 정규화를 적용합니다. 채널 전체에 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4ed90484b4c8624d14fcf12a0e8edf257e35f7b9" translate="yes" xml:space="preserve">
          <source>Applies pruning reparametrization to the tensor corresponding to the parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; without actually pruning any units.</source>
          <target state="translated">매개 변수 호출에 해당하는 텐서에 가지 치기 reparametrization을 적용 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 실제로 단위를 치기없이.</target>
        </trans-unit>
        <trans-unit id="459126de8057e720ba86989e3d80f97bc3223f0e" translate="yes" xml:space="preserve">
          <source>Applies quantized rectified linear unit function element-wise:</source>
          <target state="translated">양자화 된 정류 선형 단위 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="fdc304cf2ec7d610cd24075ed2e43e1a79d41759" translate="yes" xml:space="preserve">
          <source>Applies spectral normalization to a parameter in the given module.</source>
          <target state="translated">주어진 모듈의 매개 변수에 스펙트럼 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="2134b29fbca7ca9ac873ada1558b714c901876b0" translate="yes" xml:space="preserve">
          <source>Applies the</source>
          <target state="translated">적용</target>
        </trans-unit>
        <trans-unit id="a78621f5f34b400e9542d53a0ce88e233a0027bb" translate="yes" xml:space="preserve">
          <source>Applies the Gaussian Error Linear Units function:</source>
          <target state="translated">Gaussian Error Linear Units 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="26cda85cebacaa2f55fa0de1b622a487073b568a" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise</source>
          <target state="translated">HardTanh 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="12f634982a2632889413c5e4a92650f28c9bdade" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise. See &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt;&lt;code&gt;Hardtanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">HardTanh 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt; &lt;code&gt;Hardtanh&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="95b3d284066a600a911ef55c57442d7f4b86507f" translate="yes" xml:space="preserve">
          <source>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</source>
          <target state="translated">Softmax 함수를 n 차원 입력 Tensor에 적용하여 n 차원 출력 Tensor의 요소가 [0,1] 범위에 있고 합계가 1이되도록 조정합니다.</target>
        </trans-unit>
        <trans-unit id="01834f69d8e2aaf053d85470251564527cc332f1" translate="yes" xml:space="preserve">
          <source>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">Softmin 함수를 n 차원 입력 Tensor에 적용하여 n 차원 출력 Tensor의 요소가 &lt;code&gt;[0, 1]&lt;/code&gt; 범위에 있고 합계가 1이되도록 조정합니다.</target>
        </trans-unit>
        <trans-unit id="e581807715bdab40e3c6fa2d722c3f49a2fef542" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function</source>
          <target state="translated">요소 별 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="c91f96c06647ec0aa46ecdb47707667cb0cb30c3" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function:</source>
          <target state="translated">요소 별 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="20e17ca8da87d2e89b6ddbe8ec999295cc6de103" translate="yes" xml:space="preserve">
          <source>Applies the function &lt;code&gt;callable&lt;/code&gt; to each element in the tensor, replacing each element with the value returned by &lt;code&gt;callable&lt;/code&gt;.</source>
          <target state="translated">텐서의 각 요소에 &lt;code&gt;callable&lt;/code&gt; 함수를 적용 하여 각 요소를 &lt;code&gt;callable&lt;/code&gt; 이 반환 한 값으로 바꿉니다 .</target>
        </trans-unit>
        <trans-unit id="4f92865eb288d8379f17f76a886fec1e420e13a3" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise</source>
          <target state="translated">하드 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="56fce9ee97b2fb62286ad99c1ca046ac1d4d2b2c" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise:</source>
          <target state="translated">하드 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ec160fe4b0b150b142f63859285de9f24ee55b53" translate="yes" xml:space="preserve">
          <source>Applies the hardswish function, element-wise, as described in the paper:</source>
          <target state="translated">문서에 설명 된대로 요소별로 hardswish 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="936f4f75c2b9f6e3826efbb88a4e51ae2ab84f70" translate="yes" xml:space="preserve">
          <source>Applies the latest &lt;code&gt;method&lt;/code&gt; by computing the new partial masks and returning its combination with the &lt;code&gt;default_mask&lt;/code&gt;. The new partial mask should be computed on the entries or channels that were not zeroed out by the &lt;code&gt;default_mask&lt;/code&gt;. Which portions of the tensor &lt;code&gt;t&lt;/code&gt; the new mask will be calculated from depends on the &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (handled by the type handler):</source>
          <target state="translated">새로운 부분 마스크를 계산하고 &lt;code&gt;default_mask&lt;/code&gt; 와의 조합을 반환 하여 최신 &lt;code&gt;method&lt;/code&gt; 를 적용합니다 . 새 부분 마스크는 &lt;code&gt;default_mask&lt;/code&gt; 에 의해 제로화되지 않은 항목 또는 채널에서 계산되어야합니다 . 텐서의 어느 부분 &lt;code&gt;t&lt;/code&gt; 온 의존에서 새로운 마스크가 계산 될 &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (타입 핸들러에 의해 처리)</target>
        </trans-unit>
        <trans-unit id="7d91ea3d6a32f01c8496b5becb21a5b21016d757" translate="yes" xml:space="preserve">
          <source>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</source>
          <target state="translated">논문에 설명 된대로, 요소별로 무작위 누출 정류 라이너 유닛 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="82d1c6b2a1ee5d5cde8c89bdcc2746329c59171c" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bbd19df946d0f4bb7fc4682300e73774ba71feab" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="78f0af302507bc331ec744dca018a734a0627b7b" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise:</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="43edbfe54e1d38b1a754c8ddb026e3655a93da7b" translate="yes" xml:space="preserve">
          <source>Applies the silu function, element-wise.</source>
          <target state="translated">silu 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="f0e7dde2fc1c92ff1943988e21d9ce05bb8c8f04" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise</source>
          <target state="translated">소프트 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="7e5fad00eee592f1c4a5342fdc927299a5993172" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise:</source>
          <target state="translated">소프트 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4f3fa5f7feb89ef4634370ab877a48de7ffe1ed7" translate="yes" xml:space="preserve">
          <source>Applies weight normalization to a parameter in the given module.</source>
          <target state="translated">주어진 모듈의 매개 변수에 가중치 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="adb3943666d494e8bd63152770abcc75d5223bd5" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">동일한 인수를 사용하여이 함수의 출력에 &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; 를 적용 하면 입력의 대각선 항목이있는 대각 행렬이 생성됩니다. 그러나 &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; 는 기본 크기가 다르므로 명시 적으로 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="09ebc50e43a0da169dba1646717349a905de80e9" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a matrix identical to input. However, &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">동일한 인수를 사용하여이 함수의 출력에 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; 을 적용하면 입력과 동일한 행렬이 생성됩니다. 그러나 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; 은 기본 크기가 다르므로 명시 적으로 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="d22103e4c9027e5c172cbf43f3dd7371a41d32a7" translate="yes" xml:space="preserve">
          <source>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be &lt;strong&gt;scattered&lt;/strong&gt; on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">임의의 위치 및 키워드 입력이 DataParallel로 전달 될 수 있지만 일부 유형은 특별히 처리됩니다. 텐서는 지정된 희미한 &lt;strong&gt;곳에 흩어져&lt;/strong&gt; 있습니다 (기본값 0). 튜플, 목록 및 dict 유형은 얕은 복사됩니다. 다른 유형은 다른 스레드간에 공유되며 모델의 순방향 패스에 기록되면 손상 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6075aed00d09ffa3057c99d13847e6be9606a66" translate="yes" xml:space="preserve">
          <source>Args:</source>
          <target state="translated">Args:</target>
        </trans-unit>
        <trans-unit id="511f2c74f69da56453fcefe6e26731eae720fb15" translate="yes" xml:space="preserve">
          <source>Args: &lt;code&gt;mod&lt;/code&gt; a float module, either produced by torch.quantization utilities or directly from user</source>
          <target state="translated">Args : torch.quantization 유틸리티에서 생성하거나 사용자로부터 직접 생성 한 float 모듈을 &lt;code&gt;mod&lt;/code&gt; 합니다 .</target>
        </trans-unit>
        <trans-unit id="155bff5864cb1f24be13ad5ed169f780a903d97c" translate="yes" xml:space="preserve">
          <source>Arguments can also be &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">인수는 &lt;code&gt;None&lt;/code&gt; 일 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a0478ca5f4c068ca3ac12f9474f6a15865ce9053" translate="yes" xml:space="preserve">
          <source>Arguments:</source>
          <target state="translated">Arguments:</target>
        </trans-unit>
        <trans-unit id="3839352701cd3d321f8924d7921c49d951abf8f7" translate="yes" xml:space="preserve">
          <source>Arguments::</source>
          <target state="translated">Arguments::</target>
        </trans-unit>
        <trans-unit id="6104f39ed22a2cd32e98536a3447a01c4b9f4781" translate="yes" xml:space="preserve">
          <source>Arithmetic Operators</source>
          <target state="translated">산술 연산자</target>
        </trans-unit>
        <trans-unit id="c0e74bbb76aa26e443aa08680a9aad05da89267b" translate="yes" xml:space="preserve">
          <source>Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998.</source>
          <target state="translated">Art B. Owen. Sobol 및 Niederreiter-Xing 포인트를 스크램블합니다. Journal of Complexity, 14 (4) : 466-489, 1998 년 12 월.</target>
        </trans-unit>
        <trans-unit id="44e38ee54f654b08e43c3403586961f50c567585" translate="yes" xml:space="preserve">
          <source>As a result of these changes, the following items are considered deprecated and should not appear in new code:</source>
          <target state="translated">이러한 변경으로 인해 다음 항목은 더 이상 사용되지 않는 것으로 간주되며 새 코드에 나타나지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="9f1d874b6f85d0af8d36d263ad9bfb71343abc57" translate="yes" xml:space="preserve">
          <source>As a special case, when &lt;code&gt;input&lt;/code&gt; has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.</source>
          <target state="translated">특별한 경우로, &lt;code&gt;input&lt;/code&gt; 에 차원이 0이고 스칼라 값이 0이 아닌 경우 요소가 1 개인 1 차원 텐서로 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="50ba4ee86165b9263342121969588460e8e3d7d0" translate="yes" xml:space="preserve">
          <source>As a subset of Python, any valid TorchScript function is also a valid Python function. This makes it possible to &lt;code&gt;disable TorchScript&lt;/code&gt; and debug the function using standard Python tools like &lt;code&gt;pdb&lt;/code&gt;. The reverse is not true: there are many valid Python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.</source>
          <target state="translated">Python의 하위 집합 인 유효한 TorchScript 함수도 유효한 Python 함수입니다. 이를 통해 &lt;code&gt;disable TorchScript&lt;/code&gt; 를 비활성화 하고 &lt;code&gt;pdb&lt;/code&gt; 와 같은 표준 Python 도구를 사용하여 함수를 디버그 할 수 있습니다 . 그 반대는 사실이 아닙니다. 유효한 TorchScript 프로그램이 아닌 유효한 Python 프로그램이 많이 있습니다. 대신 TorchScript는 PyTorch에서 신경망 모델을 나타내는 데 필요한 Python의 기능에 특히 중점을 둡니다.</target>
        </trans-unit>
        <trans-unit id="b6cd63f503882dea2536baff15dc121bd94a50a6" translate="yes" xml:space="preserve">
          <source>As above, but the sample points are spaced uniformly at a distance of &lt;code&gt;dx&lt;/code&gt;.</source>
          <target state="translated">위와 같지만 샘플 포인트는 &lt;code&gt;dx&lt;/code&gt; 거리에서 균일 한 간격으로 배치 됩니다.</target>
        </trans-unit>
        <trans-unit id="4514250ef4bd2637a52c6977efbe9cd977631d0f" translate="yes" xml:space="preserve">
          <source>As described in the paper &lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Efficient Object Localization Using Convolutional Networks&lt;/a&gt; , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Convolutional Networks를 사용한 Efficient Object Localization&lt;/a&gt; 논문에 설명 된대로 피처 맵 내의 인접 픽셀이 강한 상관 관계가있는 경우 (일반적으로 초기 컨볼 루션 레이어의 경우) iid 드롭 아웃이 활성화를 정규화하지 않고 그렇지 않으면 효과적인 학습률을 초래합니다. 감소.</target>
        </trans-unit>
        <trans-unit id="99099b94ea4fc98fc5c41ec54e22d892cee1ab67" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4부터이 함수는 &lt;code&gt;out&lt;/code&gt; 키워드를 지원하지 않습니다 . 대안으로, 이전 &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; 은 &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="3fb950c139664617530977c05541f892b77d98f6" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4부터이 함수는 &lt;code&gt;out&lt;/code&gt; 키워드를 지원하지 않습니다 . 대안으로, 이전 &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; 은 &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="5d301eba7fb8fca354fa028e6aad9db923bee3a2" translate="yes" xml:space="preserve">
          <source>As of PyTorch v1.7, Windows support for the distributed package only covers collective communications with Gloo backend, &lt;code&gt;FileStore&lt;/code&gt;, and &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Therefore, the &lt;code&gt;init_method&lt;/code&gt; argument in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; must point to a file. This works for both local and shared file systems:</source>
          <target state="translated">PyTorch v1.7부터 분산 패키지에 대한 Windows 지원은 Gloo 백엔드, &lt;code&gt;FileStore&lt;/code&gt; 및 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 과의 집합 적 통신 만 포함 합니다. 따라서 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;init_method&lt;/code&gt; 인수 는 파일을 가리켜 야합니다. 이것은 로컬 및 공유 파일 시스템 모두에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="80184e7d134febd3ee09d8016449ea572c67e780" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;input&lt;/code&gt; given is expected to contain &lt;em&gt;log-probabilities&lt;/em&gt; and is not restricted to a 2D Tensor. The targets are interpreted as &lt;em&gt;probabilities&lt;/em&gt; by default, but could be considered as &lt;em&gt;log-probabilities&lt;/em&gt; with &lt;code&gt;log_target&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">와 마찬가지로 &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;NLLLoss&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;input&lt;/code&gt; 주어진을 포함 할 것으로 예상된다 &lt;em&gt;로그 확률&lt;/em&gt; 및 2 차원 텐서에 제한되지 않는다. 대상은로 해석됩니다 &lt;em&gt;확률&lt;/em&gt; 기본적으로, 그러나로 간주 될 수 &lt;em&gt;로그 확률&lt;/em&gt; 로 &lt;code&gt;log_target&lt;/code&gt; 의 로 설정 &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="99266bbca47ea9ff5e29020debec6a9680d5bf7a" translate="yes" xml:space="preserve">
          <source>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a range of &lt;code&gt;[0, 1]&lt;/code&gt; and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. They have been trained on images resized such that their minimum size is 520.</source>
          <target state="translated">이미지 분류 모델과 마찬가지로 모든 사전 학습 된 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. 이미지는 &lt;code&gt;[0, 1]&lt;/code&gt; 범위로로드 한 다음 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 및 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; 사용하여 정규화해야 합니다. 최소 크기가 520이되도록 크기가 조정 된 이미지에 대해 교육을 받았습니다.</target>
        </trans-unit>
        <trans-unit id="8b593995c88f61a609044a01e37e8e4ccc22e065" translate="yes" xml:space="preserve">
          <source>Assumptions</source>
          <target state="translated">Assumptions</target>
        </trans-unit>
        <trans-unit id="855f8e40c74ae8a51a99a06c8a2a032f398b5ed4" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. 없음 (async_op이 아니거나 그룹의 일부가 아닌 경우)</target>
        </trans-unit>
        <trans-unit id="a78338cab2cb5c76c6ab7df23e81a35449f6df4f" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. async_op이 아니거나 그룹의 일부가 아닌 경우 없음.</target>
        </trans-unit>
        <trans-unit id="4042af2d84df423621c1b7387da3334c7f85d192" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, otherwise</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. 없음, 그렇지 않으면</target>
        </trans-unit>
        <trans-unit id="1889aec46759ffe70c5c9ddb3fee3b5ee5713f7b" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters (of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 자체 필터 세트 (크기</target>
        </trans-unit>
        <trans-unit id="6510deaa2781bdfd17b677974666275b9af08554" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 크기의 자체 필터 세트와 컨볼 루션됩니다.</target>
        </trans-unit>
        <trans-unit id="9d3f19004934696d7b2d6e8d06f0e3a963b23772" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size:</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 다음과 같은 크기의 자체 필터 세트와 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="bcb1bde7e11ed9170bfe340f3dbc5867346695da" translate="yes" xml:space="preserve">
          <source>At groups=1, all inputs are convolved to all outputs.</source>
          <target state="translated">그룹 = 1에서 모든 입력은 모든 출력으로 컨볼 루션됩니다.</target>
        </trans-unit>
        <trans-unit id="f9d6240328fee5db3b7a1921b6420e8a8acf15ff" translate="yes" xml:space="preserve">
          <source>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</source>
          <target state="translated">groups = 2에서 작업은 두 개의 conv 레이어를 나란히두고 각각 입력 채널의 절반을보고 출력 채널의 절반을 생성 한 다음 둘 다 연결하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da760faa855b591ad5bebacae023465206f25e52" translate="yes" xml:space="preserve">
          <source>At p =</source>
          <target state="translated">p =에서</target>
        </trans-unit>
        <trans-unit id="ac85559223d37e290d1c1660fd99f2a4cf5904d9" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</source>
          <target state="translated">p = 1에서 합계 풀링 (평균 풀링에 비례)을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="9dbb94515aba090ad21fcfa70bd96411862efa24" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</source>
          <target state="translated">p = 1에서 Sum Pooling (평균 풀링에 비례)을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="ba60e6b895acf11e04420dfecb408950d41fdcb0" translate="yes" xml:space="preserve">
          <source>At the heart of PyTorch data loading utility is the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt;&lt;/a&gt; class. It represents a Python iterable over a dataset, with support for</source>
          <target state="translated">PyTorch 데이터 로딩 유틸리티의 핵심은 &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt; &lt;/a&gt; 클래스입니다. 데이터 세트에 대해 반복 가능한 Python을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="74e0b9c80dca267a78a89ad68a2f6c73241b5973" translate="yes" xml:space="preserve">
          <source>Attention</source>
          <target state="translated">Attention</target>
        </trans-unit>
        <trans-unit id="9b0aef7ef75761256026f8850aa6308137966a97" translate="yes" xml:space="preserve">
          <source>Attribute Lookup On Python Modules</source>
          <target state="translated">Python 모듈의 속성 조회</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="cde0319b4a4a6d08df1fd51fb49e58f04a817a7e" translate="yes" xml:space="preserve">
          <source>Attributes of a ScriptModule can be marked constant by annotating them with &lt;code&gt;Final[T]&lt;/code&gt;</source>
          <target state="translated">ScriptModule의 속성은 &lt;code&gt;Final[T]&lt;/code&gt; 로 주석을 달아 상수로 표시 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="effc3797a92fafe90a0f719052ebdd67ab00baf2" translate="yes" xml:space="preserve">
          <source>Attributes: Same as torch.nn.quantized.Conv3d</source>
          <target state="translated">속성 : torch.nn.quantized.Conv3d와 동일</target>
        </trans-unit>
        <trans-unit id="e326babbae1150bc97146fd07419454c68ba8c11" translate="yes" xml:space="preserve">
          <source>Autocast Op Reference</source>
          <target state="translated">Autocast Op 참조</target>
        </trans-unit>
        <trans-unit id="9b3dcde733ca512dd4a6b9b1a49fa5f26623a298" translate="yes" xml:space="preserve">
          <source>Autocasting</source>
          <target state="translated">Autocasting</target>
        </trans-unit>
        <trans-unit id="78965d92bd603a457514f5c8bd35fc06957dafd3" translate="yes" xml:space="preserve">
          <source>Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.</source>
          <target state="translated">Autograd는 현재 제한된 방식으로 명명 된 텐서를 지원합니다. autograd는 모든 텐서의 이름을 무시합니다. 그래디언트 계산은 여전히 ​​옳지 만 이름이주는 안전성을 잃습니다.</target>
        </trans-unit>
        <trans-unit id="03567b4fb2f83cc2c2baad9124a8e9bc1b24aeed" translate="yes" xml:space="preserve">
          <source>Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using &lt;a href=&quot;#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;profile&lt;/code&gt;&lt;/a&gt;. and nvprof based (registers both CPU and GPU activity) using &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Autograd에는 모델 내에서 CPU와 GPU 모두에서 여러 연산자의 비용을 검사 할 수있는 프로파일 러가 포함되어 있습니다. ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ 현재 두 가지 모드가 구현되어 있습니다. &lt;a href=&quot;#torch.autograd.profiler.profile&quot;&gt; &lt;code&gt;profile&lt;/code&gt; 을&lt;/a&gt; 사용하는 CPU 전용 입니다. 기초 nvprof 및 사용 (두 CPU 및 GPU 활성 레지스터) &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; 를&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1cead4310cfb62879040be96cb0226da15307414" translate="yes" xml:space="preserve">
          <source>Autograd is supported, see &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd support&lt;/a&gt;. Because gradients are currently unnamed, optimizers may work but are untested.</source>
          <target state="translated">Autograd가 지원됩니다 . &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd 지원을&lt;/a&gt; 참조 하세요 . 그래디언트는 현재 이름이 지정되지 않았기 때문에 최적화 프로그램은 작동 할 수 있지만 테스트되지 않았습니다.</target>
        </trans-unit>
        <trans-unit id="1fc85b24b64820e5c1a91ec86a7fdeb96b65e6ed" translate="yes" xml:space="preserve">
          <source>Autograd mechanics</source>
          <target state="translated">Autograd 역학</target>
        </trans-unit>
        <trans-unit id="a6a465a4bfe05284d5084187d5372a6d8da9b829" translate="yes" xml:space="preserve">
          <source>Autograd recording during the forward pass</source>
          <target state="translated">정방향 패스 중 Autograd 기록</target>
        </trans-unit>
        <trans-unit id="7807d41b89e768682ae049c6166592980a550e48" translate="yes" xml:space="preserve">
          <source>Autograd support</source>
          <target state="translated">Autograd 지원</target>
        </trans-unit>
        <trans-unit id="9b8005aa270f8147e0440468e241bd00f96800bc" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision examples</source>
          <target state="translated">자동 혼합 정밀도 예</target>
        </trans-unit>
        <trans-unit id="56280d27e59e4f9a79ed74f7e14cd8c0e581e855" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision package - torch.cuda.amp</source>
          <target state="translated">자동 혼합 정밀 패키지-torch.cuda.amp</target>
        </trans-unit>
        <trans-unit id="0f3dc77bfd956e56fcd5b8e898b3fabd32b08034" translate="yes" xml:space="preserve">
          <source>Automatic Trace Checking</source>
          <target state="translated">자동 추적 검사</target>
        </trans-unit>
        <trans-unit id="16c312b664ed41d040921c5ac59416706fca528b" translate="yes" xml:space="preserve">
          <source>Automatic batching (default)</source>
          <target state="translated">자동 일괄 처리 (기본값)</target>
        </trans-unit>
        <trans-unit id="6b27a774aecaec8b1c217a82557d2af8c10933cc" translate="yes" xml:space="preserve">
          <source>Automatic differentiation package - torch.autograd</source>
          <target state="translated">자동 차별화 패키지-torch.autograd</target>
        </trans-unit>
        <trans-unit id="b29f65429a1d343b4fb7640c7ef2b8fc40202a32" translate="yes" xml:space="preserve">
          <source>Available for Python &amp;gt;= 3.4.</source>
          <target state="translated">Python&amp;gt; = 3.4에서 사용 가능합니다.</target>
        </trans-unit>
        <trans-unit id="feed24a7c6df28036011e42f91dd99e7f26aa307" translate="yes" xml:space="preserve">
          <source>Averages all events.</source>
          <target state="translated">모든 이벤트의 평균을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="85ece30f6d8af988432cf40fa8753c609179aa8c" translate="yes" xml:space="preserve">
          <source>Averages all function events over their keys.</source>
          <target state="translated">키에 대한 모든 기능 이벤트의 평균을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1def506ac3e846cb4c938843152cd0b9bba71135" translate="yes" xml:space="preserve">
          <source>AvgPool1d</source>
          <target state="translated">AvgPool1d</target>
        </trans-unit>
        <trans-unit id="c673cdd08db86374f9ff97d13da9945ad47b1180" translate="yes" xml:space="preserve">
          <source>AvgPool2d</source>
          <target state="translated">AvgPool2d</target>
        </trans-unit>
        <trans-unit id="fc7fc296c7e1f04112232fe8a9e0e82d5c7f190f" translate="yes" xml:space="preserve">
          <source>AvgPool3d</source>
          <target state="translated">AvgPool3d</target>
        </trans-unit>
        <trans-unit id="6da0341102c44a220c312de3110fd1209e71eaf5" translate="yes" xml:space="preserve">
          <source>Ax = b</source>
          <target state="translated">도끼 = b</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="a9d087c32e7ca877fdba43990aa26038f88f00de" translate="yes" xml:space="preserve">
          <source>B \times P \times M</source>
          <target state="translated">B \ x P \ x M</target>
        </trans-unit>
        <trans-unit id="db6d439058d0e22e68b29c790c70d29e6828be28" translate="yes" xml:space="preserve">
          <source>B \times P \times R</source>
          <target state="translated">B \ x P \ x R</target>
        </trans-unit>
        <trans-unit id="2dfbff36e0eaece5eff122157b4277846a87f5f4" translate="yes" xml:space="preserve">
          <source>B \times R \times M</source>
          <target state="translated">B \ times R \ times M</target>
        </trans-unit>
        <trans-unit id="45bdfdeb5092f65ada4a9a1c95c08cee2ffa142d" translate="yes" xml:space="preserve">
          <source>BAND</source>
          <target state="translated">BAND</target>
        </trans-unit>
        <trans-unit id="0ef786803f54f9a9e02024a654989e60c7f87b66" translate="yes" xml:space="preserve">
          <source>BCELoss</source>
          <target state="translated">BCELoss</target>
        </trans-unit>
        <trans-unit id="c174768efb833ed7a3edebffb9950a4f61c4c940" translate="yes" xml:space="preserve">
          <source>BCEWithLogitsLoss</source>
          <target state="translated">BCEWithLogitsLoss</target>
        </trans-unit>
        <trans-unit id="2c89bbb2577ddfe977123efaba3737f659629fff" translate="yes" xml:space="preserve">
          <source>BLAS and LAPACK Operations</source>
          <target state="translated">BLAS 및 LAPACK 작업</target>
        </trans-unit>
        <trans-unit id="4aefcf5c188e5bba658f180d7b47a0379fa5e70c" translate="yes" xml:space="preserve">
          <source>BOR</source>
          <target state="translated">BOR</target>
        </trans-unit>
        <trans-unit id="bd606f52a5ad5bc6c6cc894b3accba4125210f66" translate="yes" xml:space="preserve">
          <source>BXOR</source>
          <target state="translated">BXOR</target>
        </trans-unit>
        <trans-unit id="e758ca64563fdd62965a2b97b76d555b1a70d938" translate="yes" xml:space="preserve">
          <source>Backend</source>
          <target state="translated">Backend</target>
        </trans-unit>
        <trans-unit id="b3776d63ad7b7a84bfe20d9c6d4a53a2b25d0e43" translate="yes" xml:space="preserve">
          <source>Backends</source>
          <target state="translated">Backends</target>
        </trans-unit>
        <trans-unit id="82921ea0c75d2b9e5a6aaafaf19c2a51a8ee3c26" translate="yes" xml:space="preserve">
          <source>Backends that come with PyTorch</source>
          <target state="translated">PyTorch와 함께 제공되는 백엔드</target>
        </trans-unit>
        <trans-unit id="64dd60fe1a049fe6db3eb1369dec2e42bf428e21" translate="yes" xml:space="preserve">
          <source>Background</source>
          <target state="translated">Background</target>
        </trans-unit>
        <trans-unit id="4c7660dd341e52619271ac35d19b4aa963dcdc89" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; 를 통한 뒤로는 &lt;code&gt;input&lt;/code&gt; 이 반전되지 않을 때 내부적으로 SVD 결과를 사용합니다 . 이 경우 &lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; 를 통한 이중 역방향 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c28a70ba3b665ff380843011cb9bf4004fe6ce27" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; 를 통한 역방향 은 &lt;code&gt;input&lt;/code&gt; 이 반전되지 않을 때 내부적으로 SVD 결과를 사용합니다 . 이 경우, &lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; 를 통한 double back 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="28943ec2b2ba443d9dbd186c1f96eafcfe632579" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; 를 통한 역방향 은 &lt;code&gt;input&lt;/code&gt; 반전 할 수 없는 경우 내부적으로 SVD 결과를 사용합니다 . 이 경우 &lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; 를 통한 이중 역방향 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bb44593f48eb0328822a985f5aa285528658fb23" translate="yes" xml:space="preserve">
          <source>Bartlett window function.</source>
          <target state="translated">Bartlett 창 기능.</target>
        </trans-unit>
        <trans-unit id="919b23578508277c7be92184e7175d68ef74ae7e" translate="yes" xml:space="preserve">
          <source>Base class for all Samplers.</source>
          <target state="translated">모든 샘플러의 기본 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="8f6ce4f7d8508e7be7e4cda0e650d850c9905e8a" translate="yes" xml:space="preserve">
          <source>Base class for all neural network modules.</source>
          <target state="translated">모든 신경망 모듈의 기본 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="aa2e98de019e1d429d4b048d107e89660df3590b" translate="yes" xml:space="preserve">
          <source>Base class for all optimizers.</source>
          <target state="translated">모든 옵티마이 저의 기본 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="3756aaad75fa5f15b1e3570c38539a1b4fa27093" translate="yes" xml:space="preserve">
          <source>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">배포 된 PyTorch에서 제공하는 3 개 ( &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt; ) 와 같은 모든 저장소 구현의 기본 클래스입니다 .</target>
        </trans-unit>
        <trans-unit id="7fa48cad7c036a5d05a3d1ee3ac94a5b6b79fa80" translate="yes" xml:space="preserve">
          <source>Base observer Module. Any observer implementation should derive from this class.</source>
          <target state="translated">기본 관찰자 모듈. 관찰자 구현은이 클래스에서 파생되어야합니다.</target>
        </trans-unit>
        <trans-unit id="fb7f5bac791ae3c9dcca099683932046b2428b2e" translate="yes" xml:space="preserve">
          <source>BasePruningMethod</source>
          <target state="translated">BasePruningMethod</target>
        </trans-unit>
        <trans-unit id="173d42bdad4a08f4dc4a342a93191085b664cf61" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;&lt;code&gt;torch.distributions.distribution.Distribution&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">기지 : &lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt; &lt;code&gt;torch.distributions.distribution.Distribution&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f6b8b8e70456b968792e96e115e64852d9f96501" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.exp_family.ExponentialFamily&quot;&gt;&lt;code&gt;torch.distributions.exp_family.ExponentialFamily&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">기지 : &lt;a href=&quot;#torch.distributions.exp_family.ExponentialFamily&quot;&gt; &lt;code&gt;torch.distributions.exp_family.ExponentialFamily&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a86b5e13aaeb2e61798bec3653ac860d7b6fac71" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.gamma.Gamma&quot;&gt;&lt;code&gt;torch.distributions.gamma.Gamma&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">베이스 : &lt;a href=&quot;#torch.distributions.gamma.Gamma&quot;&gt; &lt;code&gt;torch.distributions.gamma.Gamma&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f37aa8ac66020a1c6c2b5aa83188590e63bd8613" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;torch.distributions.transformed_distribution.TransformedDistribution&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">기지 : &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;torch.distributions.transformed_distribution.TransformedDistribution&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="68ca77158d1716ec91be76868bd9ad42dd2154e8" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt;&lt;code&gt;object&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">기지 : &lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt; &lt;code&gt;object&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7070665795a3b04470b95d5347795e00aee639f7" translate="yes" xml:space="preserve">
          <source>Basic name inference rules</source>
          <target state="translated">기본 이름 추론 규칙</target>
        </trans-unit>
        <trans-unit id="5fcebeefad3cdbbf8733aa928160dec7dc90c1a1" translate="yes" xml:space="preserve">
          <source>Basics</source>
          <target state="translated">Basics</target>
        </trans-unit>
        <trans-unit id="16f46715e1716fa628885ccd95ddf80183d01012" translate="yes" xml:space="preserve">
          <source>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;. For instance, given data &lt;code&gt;abc&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; the &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; would contain data &lt;code&gt;axbc&lt;/code&gt; with &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt;.</source>
          <target state="translated">배치 크기는 &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt; 전달 된 다양한 시퀀스 길이가 아니라 배치의 각 시퀀스 단계에서 요소 수를 나타냅니다 . 예를 들어, 주어진 데이터 &lt;code&gt;abc&lt;/code&gt; 및 &lt;code&gt;x&lt;/code&gt; &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; 는&lt;/a&gt; 데이터 포함될 것이다 &lt;code&gt;axbc&lt;/code&gt; 와 &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="640ed9b96cb507644eb24c180e159849e7025eaa" translate="yes" xml:space="preserve">
          <source>BatchNorm</source>
          <target state="translated">BatchNorm</target>
        </trans-unit>
        <trans-unit id="c7c537e5e7d31a4a94ee4f8e5ebc01c365e0771a" translate="yes" xml:space="preserve">
          <source>BatchNorm1d</source>
          <target state="translated">BatchNorm1d</target>
        </trans-unit>
        <trans-unit id="539a37ce419a6050de06321e52ac8941c6a520dd" translate="yes" xml:space="preserve">
          <source>BatchNorm2d</source>
          <target state="translated">BatchNorm2d</target>
        </trans-unit>
        <trans-unit id="4036bb17e086d06a41c9d03825932c8325ec759f" translate="yes" xml:space="preserve">
          <source>BatchNorm3d</source>
          <target state="translated">BatchNorm3d</target>
        </trans-unit>
        <trans-unit id="58a700d3fc7db1c024d5e6e31878faaaa556887d" translate="yes" xml:space="preserve">
          <source>Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.</source>
          <target state="translated">명명 된 텐서는 명명되지 않은 텐서와 공존 할 수 있기 때문에 이름을 정제하면 명명 된 텐서와 명명되지 않은 텐서 모두에서 작동하는 명명 된 텐서 인식 코드를 작성할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0a3a2aefc4920597d8297a1ff6ee9586d1d76c12" translate="yes" xml:space="preserve">
          <source>Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.</source>
          <target state="translated">API의 유사성으로 인해이 패키지 내용의 대부분을 문서화하지 않으며 원래 모듈의 아주 좋은 문서를 참조하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="85ad6aa0a04066bce002e177e5b44f0e8ada2c9c" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done for each channel in the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, +)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;C&lt;/code&gt; 차원의 각 채널에 대해 수행 되어 &lt;code&gt;(N, +)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하므로이 볼륨 배치 정규화 또는 시공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="fd602c285d639d22638df01e41486fc6be835af0" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, D, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, D, H, W)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 볼륨 배치 정규화 또는 시공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="e3a6337703c27cf2e6ddd618d4baf0da4204696b" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Spatial Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, H, W)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="288d84de2a904a705b5e7ec68955d71169bacf13" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, L)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, L)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 임시 배치 정규화를 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="0da3b2b883ef3b23c37366196cd69afd8c187cdd" translate="yes" xml:space="preserve">
          <source>Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in &lt;code&gt;input[0]&lt;/code&gt; would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.</source>
          <target state="translated">신호가 시간 영역에서 Hermitian이기 때문에 결과는 주파수 영역에서 실제입니다. 일부 입력 주파수는 Hermitian 속성을 충족하기 위해 실수 값이어야합니다. 이러한 경우 가상 구성 요소는 무시됩니다. 예를 들어, &lt;code&gt;input[0]&lt;/code&gt; 허수 성분은 실제 출력으로 표현할 수없는 하나 이상의 복잡한 주파수 항을 생성하므로 항상 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="2e29f922f6839735f3a19bbe923a21ab693afacc" translate="yes" xml:space="preserve">
          <source>Because your script will be profiled, please ensure that it exits in a finite amount of time.</source>
          <target state="translated">스크립트가 프로파일 링되므로 제한된 시간 내에 종료되는지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="956a144b3d9996462b7b18cbedf9997fc2011650" translate="yes" xml:space="preserve">
          <source>Before going further, more details on TensorBoard can be found at &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt;</source>
          <target state="translated">계속 진행하기 전에 TensorBoard에 대한 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt; 에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8b23ce245f346d21fc21404045956dd5e7ec60de" translate="yes" xml:space="preserve">
          <source>Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; which would initialize the RPC framework, RRef framework and distributed autograd.</source>
          <target state="translated">RPC 및 분산 autograd 프리미티브를 사용하기 전에 초기화가 수행되어야합니다. RPC 프레임 워크를 초기화하려면 RPC 프레임 워크, RRef 프레임 워크 및 분산 autograd를 초기화하는 &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; 를 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="3e921d24df994f442d57e133126d10fb2ac6e163" translate="yes" xml:space="preserve">
          <source>Below is an example of running a TorchScript function using RPC.</source>
          <target state="translated">다음은 RPC를 사용하여 TorchScript 함수를 실행하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="dc8bffef30a767db701efa64dcad8d3b22a3cf6c" translate="yes" xml:space="preserve">
          <source>Bernoulli</source>
          <target state="translated">Bernoulli</target>
        </trans-unit>
        <trans-unit id="ad9db732260b1d62e536980667c87db0f86fe3e4" translate="yes" xml:space="preserve">
          <source>Bernoulli trials, the first</source>
          <target state="translated">베르누이 재판, 첫 번째</target>
        </trans-unit>
        <trans-unit id="01f8c251cc6790b547148a802d152b484b1bf377" translate="yes" xml:space="preserve">
          <source>Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials - Custom C++ and CUDA Extensions&lt;/a&gt; and &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt;. The capability of third-party backends are decided by their own implementations.</source>
          <target state="translated">GLOO / MPI / NCCL 백엔드 외에도 PyTorch 분산은 런타임 레지스터 메커니즘을 통해 타사 백엔드를 지원합니다. C ++ Extension을 통해 타사 백엔드를 개발하는 방법에 대한 참조는 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials-Custom C ++ 및 CUDA Extensions&lt;/a&gt; 및 &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt; 를 참조하십시오 . 타사 백엔드의 기능은 자체 구현에 의해 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="f03b60f7e52b7ce49ed1e4f9fa511c452a2185bb" translate="yes" xml:space="preserve">
          <source>Beta</source>
          <target state="translated">Beta</target>
        </trans-unit>
        <trans-unit id="7190fb74ee81a83c8faf5a8c92b9ef9ebbf6afa2" translate="yes" xml:space="preserve">
          <source>Beta distribution parameterized by &lt;a href=&quot;#torch.distributions.beta.Beta.concentration1&quot;&gt;&lt;code&gt;concentration1&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.beta.Beta.concentration0&quot;&gt;&lt;code&gt;concentration0&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.beta.Beta.concentration1&quot;&gt; &lt;code&gt;concentration1&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributions.beta.Beta.concentration0&quot;&gt; &lt;code&gt;concentration0&lt;/code&gt; &lt;/a&gt; 의해 매개 변수화 된 베타 분포 .</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="a8b51aa01c82ba019a69245f557ba6ce284edd3e" translate="yes" xml:space="preserve">
          <source>Bilinear</source>
          <target state="translated">Bilinear</target>
        </trans-unit>
        <trans-unit id="054debc367aa35026cc5f23e63b04983a105cd4a" translate="yes" xml:space="preserve">
          <source>Binary arithmetic ops: &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;Unifies names from inputs&lt;/a&gt;</source>
          <target state="translated">이진 산술 연산 : &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;입력의 이름 통합&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f22fc461ba15be26425f3495add8c55aef51c91e" translate="yes" xml:space="preserve">
          <source>Binomial</source>
          <target state="translated">Binomial</target>
        </trans-unit>
        <trans-unit id="5200f1acde5b24e6b432770b7da7700cd60a0c9b" translate="yes" xml:space="preserve">
          <source>Blackman window function.</source>
          <target state="translated">블랙맨 창 기능.</target>
        </trans-unit>
        <trans-unit id="0f52a562e394a39a60e84e9b3ac335fc3bd698d8" translate="yes" xml:space="preserve">
          <source>Block until the value of this &lt;code&gt;Future&lt;/code&gt; is ready.</source>
          <target state="translated">이 &lt;code&gt;Future&lt;/code&gt; 의 가치 가 준비 될 때까지 차단하십시오 .</target>
        </trans-unit>
        <trans-unit id="c2959d258f8603010c5e64b30d6b389ff4f7e543" translate="yes" xml:space="preserve">
          <source>Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.</source>
          <target state="translated">소유자에서 로컬 노드로 RRef의 값을 복사하여 반환하는 차단 호출. 현재 노드가 소유자 인 경우 로컬 값에 대한 참조를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b76ff4906f33c2dd97ddd929b9662ba8cac6174c" translate="yes" xml:space="preserve">
          <source>Boolean</source>
          <target state="translated">Boolean</target>
        </trans-unit>
        <trans-unit id="8bdc4a42a8aebafd81e1c9ebefc552e2af4c86e0" translate="yes" xml:space="preserve">
          <source>Both &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must have integer types.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 모두 정수 유형이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="1f1c331482420b1dcc09832ffc768fcc272a654b" translate="yes" xml:space="preserve">
          <source>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</source>
          <target state="translated">매개 변수와 영구 버퍼 (예 : 평균 실행)가 모두 포함됩니다. 키는 해당 매개 변수 및 버퍼 이름입니다.</target>
        </trans-unit>
        <trans-unit id="fbb16819919dac69a8f12c76f83bc60666b08a4a" translate="yes" xml:space="preserve">
          <source>Break and Continue</source>
          <target state="translated">중단하고 계속</target>
        </trans-unit>
        <trans-unit id="7263f9de457f4107fd587921961d9e2a1123f9c7" translate="yes" xml:space="preserve">
          <source>Broadcasting semantics</source>
          <target state="translated">방송 의미론</target>
        </trans-unit>
        <trans-unit id="7fa441232eb833209908a5491f939a27366f7f1a" translate="yes" xml:space="preserve">
          <source>Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.</source>
          <target state="translated">시퀀스 텐서를 지정된 GPU로 브로드 캐스트합니다. 작은 텐서는 동기화 수를 줄이기 위해 먼저 버퍼로 통합됩니다.</target>
        </trans-unit>
        <trans-unit id="1e2b086a05d88dd9ed966250318444cafc6508f3" translate="yes" xml:space="preserve">
          <source>Broadcasts a tensor to specified GPU devices.</source>
          <target state="translated">지정된 GPU 장치에 텐서를 브로드 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="c0e07d9c87aab6412703456e09b86450b239d9d4" translate="yes" xml:space="preserve">
          <source>Broadcasts the given tensors according to &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Broadcasting semantics&lt;/a&gt;.</source>
          <target state="translated">브로드 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스팅 의미론&lt;/a&gt; 에 따라 주어진 텐서를 브로드 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="3ef2cb9c843900d814506b00e103fcfa6e5a1790" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group with multiple GPU tensors per node.</source>
          <target state="translated">노드 당 여러 GPU 텐서를 사용하여 전체 그룹에 텐서를 브로드 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="8cd98b60116d9de3db535a744915ab8ea6c752e2" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group.</source>
          <target state="translated">텐서를 전체 그룹에 브로드 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="9d0d383693b784a550085e06cd3563d333282d5c" translate="yes" xml:space="preserve">
          <source>Buffers can be accessed as attributes using given names.</source>
          <target state="translated">버퍼는 주어진 이름을 사용하여 속성으로 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="072f59c8f878e682443386c6e7570c90195dbc31" translate="yes" xml:space="preserve">
          <source>Built-in Functions and Modules</source>
          <target state="translated">내장 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="2faa9e18e8bd0f32f4bea2db29afed6e0ae12775" translate="yes" xml:space="preserve">
          <source>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt;&lt;code&gt;new_group()&lt;/code&gt;&lt;/a&gt; function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a &lt;code&gt;group&lt;/code&gt; argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</source>
          <target state="translated">기본적으로 집합체는 기본 그룹 (월드라고도 함)에서 작동하며 모든 프로세스가 분산 함수 호출을 입력해야합니다. 그러나 일부 워크로드는보다 세분화 된 통신의 이점을 누릴 수 있습니다. 이것은 분산 그룹이 작동하는 곳입니다. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt; &lt;code&gt;new_group()&lt;/code&gt; &lt;/a&gt; 함수는 모든 프로세스의 임의의 하위 집합으로 새 그룹을 만드는 데 사용할 수 있습니다. 모든 집합체에 대한 &lt;code&gt;group&lt;/code&gt; 인수로 제공 될 수있는 불투명 한 그룹 핸들을 반환합니다 (집합은 잘 알려진 특정 프로그래밍 패턴에서 정보를 교환하는 분산 함수입니다).</target>
        </trans-unit>
        <trans-unit id="aa6f8066d9ec2654575e0a2dde8cde52f9a324b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;dim&lt;/code&gt; is the last dimension of the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">기본적으로 &lt;code&gt;dim&lt;/code&gt; 은 &lt;code&gt;input&lt;/code&gt; 텐서 의 마지막 차원입니다 .</target>
        </trans-unit>
        <trans-unit id="06e2dff3c96eaf47ae009ba6e54dc225888930ea" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;torch.optim.swa_utils.AveragedModel&lt;/code&gt; computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the &lt;code&gt;avg_fn&lt;/code&gt; parameter. In the following example &lt;code&gt;ema_model&lt;/code&gt; computes an exponential moving average.</source>
          <target state="translated">기본적으로 &lt;code&gt;torch.optim.swa_utils.AveragedModel&lt;/code&gt; 은 사용자가 제공하는 매개 변수의 실행 평균을 계산하지만 &lt;code&gt;avg_fn&lt;/code&gt; 매개 변수 와 함께 사용자 지정 평균화 함수를 사용할 수도 있습니다 . 다음 예제에서 &lt;code&gt;ema_model&lt;/code&gt; 은 지수 이동 평균을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="fbd6c7669b0b5fa4b44c256ffe1ad2c51d96bac8" translate="yes" xml:space="preserve">
          <source>By default, all parameters to a TorchScript function are assumed to be Tensor. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above.</source>
          <target state="translated">기본적으로 TorchScript 함수에 대한 모든 매개 변수는 Tensor로 간주됩니다. TorchScript 함수에 대한 인수가 다른 유형임을 지정하려면 위에 나열된 유형을 사용하여 MyPy 스타일 유형 주석을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3269c80ab5ee7ca74422e9e598672c0d5563eb7c" translate="yes" xml:space="preserve">
          <source>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</source>
          <target state="translated">기본적으로 NCCL 및 Gloo 백엔드는 사용할 올바른 네트워크 인터페이스를 찾으려고합니다. 자동으로 감지 된 인터페이스가 올바르지 않은 경우 다음 환경 변수를 사용하여 재정의 할 수 있습니다 (각 백엔드에 적용 가능).</target>
        </trans-unit>
        <trans-unit id="c4858611fd19a2d108e1171bb6e36531524a98b7" translate="yes" xml:space="preserve">
          <source>By default, each worker will have its PyTorch seed set to &lt;code&gt;base_seed + worker_id&lt;/code&gt;, where &lt;code&gt;base_seed&lt;/code&gt; is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily). However, seeds for other libraries may be duplicated upon initializing workers (e.g., NumPy), causing each worker to return identical random numbers. (See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#dataloader-workers-random-seed&quot;&gt;this section&lt;/a&gt; in FAQ.).</source>
          <target state="translated">기본적으로 각 워커는 PyTorch 시드를 &lt;code&gt;base_seed + worker_id&lt;/code&gt; 설정합니다 . 여기서 &lt;code&gt;base_seed&lt;/code&gt; 는 RNG를 사용하여 메인 프로세스에서 생성 한 long입니다 (따라서 RNG 상태를 의무적으로 사용함). 그러나 다른 라이브러리의 시드는 작업자 (예 : NumPy)를 초기화 할 때 복제되어 각 작업자가 동일한 난수를 반환하게 할 수 있습니다. ( FAQ &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#dataloader-workers-random-seed&quot;&gt;의이 섹션&lt;/a&gt; 을 참조하십시오 .)</target>
        </trans-unit>
        <trans-unit id="a47868a7f38c8661f0bf65ea44d4c34b8a4bf540" translate="yes" xml:space="preserve">
          <source>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the &lt;code&gt;MAX_JOBS&lt;/code&gt; environment variable to a non-negative number.</source>
          <target state="translated">기본적으로 Ninja 백엔드는 #CPUS + 2 명의 작업자를 사용하여 확장 프로그램을 빌드합니다. 이것은 일부 시스템에서 너무 많은 자원을 사용할 수 있습니다. &lt;code&gt;MAX_JOBS&lt;/code&gt; 환경 변수를 음수가 아닌 숫자 로 설정하여 작업자 수를 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0d1c0495670db55861d9901a2985f3df3c09c53d" translate="yes" xml:space="preserve">
          <source>By default, the directory to which the build file is emitted and the resulting library compiled to is &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; is the temporary folder on the current platform and &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; the name of the extension. This location can be overridden in two ways. First, if the &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; environment variable is set, it replaces &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; and all extensions will be compiled into subfolders of this directory. Second, if the &lt;code&gt;build_directory&lt;/code&gt; argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</source>
          <target state="translated">기본적으로 빌드 파일이 생성되고 결과 라이브러리가 컴파일되는 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt; 는 &amp;lt;tmp&amp;gt; / torch_extensions / &amp;lt;name&amp;gt;입니다 . 여기서 &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; 는 현재 플랫폼의 임시 폴더이고 &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; 은 확장의 이름입니다. . 이 위치는 두 가지 방법으로 재정의 할 수 있습니다. 먼저 &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; 환경 변수가 설정되면 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; 를 대체 하고 모든 확장이이 디렉토리의 하위 폴더로 컴파일됩니다. 둘째, 이 함수에 대한 &lt;code&gt;build_directory&lt;/code&gt; 인수가 제공되면 전체 경로를 덮어 씁니다. 즉, 라이브러리가 해당 폴더로 직접 컴파일됩니다.</target>
        </trans-unit>
        <trans-unit id="836a1bf0b7e01bddc495fceffa973ccc50f2cdd2" translate="yes" xml:space="preserve">
          <source>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">기본적으로이 계층은 훈련 및 평가 모드 모두에서 입력 데이터에서 계산 된 인스턴스 통계를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="e0d34afff9ec6083c3b3dad548be8255d68d92ca" translate="yes" xml:space="preserve">
          <source>By default, this returns the peak allocated memory since the beginning of this program. &lt;code&gt;reset_peak_stats()&lt;/code&gt; can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.</source>
          <target state="translated">기본적으로이 프로그램이 시작된 이후 할당 된 최대 메모리를 반환합니다. &lt;code&gt;reset_peak_stats()&lt;/code&gt; 는이 측정 항목을 추적 할 때 시작점을 재설정하는 데 사용할 수 있습니다. 예를 들어,이 두 함수는 훈련 루프에서 각 반복의 최대 할당 메모리 사용량을 측정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="27248360b06b99273d17a1100cf62c5ed9fc9c4e" translate="yes" xml:space="preserve">
          <source>By default, this returns the peak cached memory since the beginning of this program. &lt;code&gt;reset_peak_stats()&lt;/code&gt; can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.</source>
          <target state="translated">기본적으로이 프로그램이 시작된 이후 캐시 된 최대 메모리를 반환합니다. &lt;code&gt;reset_peak_stats()&lt;/code&gt; 는이 측정 항목을 추적 할 때 시작점을 재설정하는 데 사용할 수 있습니다. 예를 들어,이 두 함수는 훈련 루프에서 각 반복의 최대 캐시 메모리 양을 측정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="260db48b6c19b12ad8aaa2c223fbe53f044c91eb" translate="yes" xml:space="preserve">
          <source>By default, we decode byte strings as &lt;code&gt;utf-8&lt;/code&gt;. This is to avoid a common error case &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra &lt;code&gt;encoding&lt;/code&gt; keyword argument to specify how these objects should be loaded, e.g., &lt;code&gt;encoding='latin1'&lt;/code&gt; decodes them to strings using &lt;code&gt;latin1&lt;/code&gt; encoding, and &lt;code&gt;encoding='bytes'&lt;/code&gt; keeps them as byte arrays which can be decoded later with &lt;code&gt;byte_array.decode(...)&lt;/code&gt;.</source>
          <target state="translated">기본적으로 바이트 문자열을 &lt;code&gt;utf-8&lt;/code&gt; 로 디코딩 합니다. 이것은 일반적인 오류 사례를 방지하기위한 것입니다. &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; Python 2에서 Python 3에서 저장 한 파일을로드 할 때 ... 이 기본값이 올바르지 않으면 추가 &lt;code&gt;encoding&lt;/code&gt; 키워드 인수를 사용하여 지정할 수 있습니다. 이러한 객체를로드하는 방법, 예를 들어 &lt;code&gt;encoding='latin1'&lt;/code&gt; 은 &lt;code&gt;latin1&lt;/code&gt; 인코딩을 사용하여 문자열로 디코딩 하고 &lt;code&gt;encoding='bytes'&lt;/code&gt; 는 나중에 &lt;code&gt;byte_array.decode(...)&lt;/code&gt; 로 디코딩 할 수있는 바이트 배열로 유지합니다 .</target>
        </trans-unit>
        <trans-unit id="5b3e53bd55890d668be711af0b27d6b4e4085bb4" translate="yes" xml:space="preserve">
          <source>By default, we don&amp;rsquo;t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">기본적으로 파일을로드 한 후에는 정리하지 않습니다. Hub는 &lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 디렉토리에 이미있는 경우 기본적으로 캐시를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="4d31b2b62d5a7017f58599071410bf4ed9eb8d68" translate="yes" xml:space="preserve">
          <source>By default, with &lt;code&gt;dim=0&lt;/code&gt;, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use &lt;code&gt;dim=None&lt;/code&gt;.</source>
          <target state="translated">기본적으로 &lt;code&gt;dim=0&lt;/code&gt; 을 사용하면 표준이 출력 채널 / 평면별로 독립적으로 계산됩니다. 전체 가중치 텐서에 대한 노름을 계산하려면 &lt;code&gt;dim=None&lt;/code&gt; 을 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="32096c2e0eff33d844ee6d675407ace18289357d" translate="yes" xml:space="preserve">
          <source>C</source>
          <target state="translated">C</target>
        </trans-unit>
        <trans-unit id="eaef64b3192bf8d5502eee309124b228be2379ec" translate="yes" xml:space="preserve">
          <source>C = \log(\pi) \times \frac{p (p - 1)}{4}</source>
          <target state="translated">C = \ log (\ pi) \ times \ frac {p (p-1)} {4}</target>
        </trans-unit>
        <trans-unit id="4ee9f8327170df1bedff8fda035491204285f3c3" translate="yes" xml:space="preserve">
          <source>C = \text{number of classes (including blank)}</source>
          <target state="translated">C = \ text {클래스 수 (공백 포함)}</target>
        </trans-unit>
        <trans-unit id="f46c4b42af8b2196cc344f07319e812268c3c0dd" translate="yes" xml:space="preserve">
          <source>C \times \prod(\text{kernel\_size})</source>
          <target state="translated">C \ times \ prod (\ text {커널 \ _size})</target>
        </trans-unit>
        <trans-unit id="fc2b4216164cfb01ac45112054b3fedda8b56c86" translate="yes" xml:space="preserve">
          <source>C++</source>
          <target state="translated">C++</target>
        </trans-unit>
        <trans-unit id="a3d883aa22c9b3cbe1562cc3d62c063468b9a196" translate="yes" xml:space="preserve">
          <source>C=\text{num\_channels}</source>
          <target state="translated">C=\text{num\_channels}</target>
        </trans-unit>
        <trans-unit id="798a57343d5fc6a0cf122924f6ca62852b64f4a6" translate="yes" xml:space="preserve">
          <source>CELU</source>
          <target state="translated">CELU</target>
        </trans-unit>
        <trans-unit id="ff221d4752ce05f5a91bbf1d28b78a7bf7e2ddaa" translate="yes" xml:space="preserve">
          <source>CPU</source>
          <target state="translated">CPU</target>
        </trans-unit>
        <trans-unit id="d2b3baf18b41b52a701d4019f7ffaa284e02f53a" translate="yes" xml:space="preserve">
          <source>CPU hosts with Ethernet interconnect</source>
          <target state="translated">이더넷 상호 연결이있는 CPU 호스트</target>
        </trans-unit>
        <trans-unit id="bfea39d4ef79843c8c035774adc370d11a93b4e8" translate="yes" xml:space="preserve">
          <source>CPU hosts with InfiniBand interconnect</source>
          <target state="translated">InfiniBand 상호 연결이있는 CPU 호스트</target>
        </trans-unit>
        <trans-unit id="aca0030d1b8e86f8e968a622d4b61c5e238ad1fa" translate="yes" xml:space="preserve">
          <source>CPU tensor</source>
          <target state="translated">CPU 텐서</target>
        </trans-unit>
        <trans-unit id="1adcc1c5aae9ba0bc68cde14e7017456258e8921" translate="yes" xml:space="preserve">
          <source>CPU threading and TorchScript inference</source>
          <target state="translated">CPU 스레딩 및 TorchScript 추론</target>
        </trans-unit>
        <trans-unit id="5c435b722f2bc9152d11b1225cc3099efa162504" translate="yes" xml:space="preserve">
          <source>CTCLoss</source>
          <target state="translated">CTCLoss</target>
        </trans-unit>
        <trans-unit id="b793fde9f05fa7373de2fa5fbd4dcb147eafd14e" translate="yes" xml:space="preserve">
          <source>CUDA events are synchronization markers that can be used to monitor the device&amp;rsquo;s progress, to accurately measure timing, and to synchronize CUDA streams.</source>
          <target state="translated">CUDA 이벤트는 장치의 진행 상황을 모니터링하고 타이밍을 정확하게 측정하며 CUDA 스트림을 동기화하는 데 사용할 수있는 동기화 마커입니다.</target>
        </trans-unit>
        <trans-unit id="1ab6d957380a70ab72c7926a9d4fae8cf48ecf35" translate="yes" xml:space="preserve">
          <source>CUDA semantics</source>
          <target state="translated">CUDA 의미론</target>
        </trans-unit>
        <trans-unit id="3f5da429aac783fd0a3a0da898da1913e428656f" translate="yes" xml:space="preserve">
          <source>CUDA support with mixed compilation is provided. Simply pass CUDA source files (&lt;code&gt;.cu&lt;/code&gt; or &lt;code&gt;.cuh&lt;/code&gt;) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking &lt;code&gt;cudart&lt;/code&gt;. You can pass additional flags to nvcc via &lt;code&gt;extra_cuda_cflags&lt;/code&gt;, just like with &lt;code&gt;extra_cflags&lt;/code&gt; for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the &lt;code&gt;CUDA_HOME&lt;/code&gt; environment variable is the safest option.</source>
          <target state="translated">혼합 컴파일을 통한 CUDA 지원이 제공됩니다. CUDA 소스 파일 ( &lt;code&gt;.cu&lt;/code&gt; 또는 &lt;code&gt;.cuh&lt;/code&gt; )을 다른 소스와 함께 전달하기 만하면 됩니다. 이러한 파일은 C ++ 컴파일러가 아닌 nvcc로 감지되고 컴파일됩니다. 여기에는 CUDA lib64 디렉토리를 라이브러리 디렉토리로 전달하고 &lt;code&gt;cudart&lt;/code&gt; 연결이 포함 됩니다 . 당신은을 통해 NVCC에 추가 플래그를 전달할 수 있습니다 &lt;code&gt;extra_cuda_cflags&lt;/code&gt; 단지와 마찬가지로, &lt;code&gt;extra_cflags&lt;/code&gt; C ++합니다. CUDA 설치 디렉토리를 찾기위한 다양한 휴리스틱이 사용되며 일반적으로 잘 작동합니다. 그렇지 않은 경우 &lt;code&gt;CUDA_HOME&lt;/code&gt; 환경 변수를 설정하는 것이 가장 안전한 옵션입니다.</target>
        </trans-unit>
        <trans-unit id="56a1c421d119a488bb14d44dd6891b51aea087b0" translate="yes" xml:space="preserve">
          <source>Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:</source>
          <target state="translated">캐싱은 역이 비싸거나 수치 적으로 불안정한 변환에 유용합니다. autograd 그래프가 반전 될 수 있으므로 메모 된 값에주의해야합니다. 예를 들어 다음은 캐싱을 사용하거나 사용하지 않고 작동합니다.</target>
        </trans-unit>
        <trans-unit id="5abf962ed164e31df7bbd04bfe694578c153b51d" translate="yes" xml:space="preserve">
          <source>Caching logic</source>
          <target state="translated">캐싱 로직</target>
        </trans-unit>
        <trans-unit id="e8a73f7e1b281ff72f0cd1f11baf202568648a12" translate="yes" xml:space="preserve">
          <source>Calculates determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬의 행렬식 또는 정사각형 행렬의 배치를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b469ab85777119e331c0d2b40b68cd19ac71da49" translate="yes" xml:space="preserve">
          <source>Calculates log determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬 또는 정사각형 행렬의 배치의 로그 행렬식을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="dbc9fd03c3e295de099741d45013bec1bd5703f7" translate="yes" xml:space="preserve">
          <source>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be &amp;ldquo;many-to-one&amp;rdquo;, which limits the length of the target sequence such that it must be</source>
          <target state="translated">연속 (분할되지 않은) 시계열과 대상 시퀀스 간의 손실을 계산합니다. CTCLoss는 대상에 대한 입력 정렬 가능성을 합산하여 각 입력 노드에 대해 미분 할 수있는 손실 값을 생성합니다. 타겟에 대한 입력의 정렬은&amp;ldquo;다 대일&amp;rdquo;로 가정되며, 이는 타겟 시퀀스의 길이를 제한하여</target>
        </trans-unit>
        <trans-unit id="8b95dc9e720785341df91a709dd18edb57e422e8" translate="yes" xml:space="preserve">
          <source>Calculates pointwise</source>
          <target state="translated">포인트 단위로 계산</target>
        </trans-unit>
        <trans-unit id="19a076a6c7df9171545ad04fdc2cc5f305e06b38" translate="yes" xml:space="preserve">
          <source>Calculates the learning rate at batch index. This function treats &lt;code&gt;self.last_epoch&lt;/code&gt; as the last batch index.</source>
          <target state="translated">배치 인덱스에서 학습률을 계산합니다. 이 함수는 &lt;code&gt;self.last_epoch&lt;/code&gt; 를 마지막 배치 인덱스로 취급합니다 .</target>
        </trans-unit>
        <trans-unit id="3f73e5660ccb505df5953e1fbed92e6c8a064be1" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</source>
          <target state="translated">2D 텐서의 의사 역 (무어-펜로즈 역이라고도 함)을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="afbe6768e28de20bd0669bfb42db4b0e5335e016" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; for more details</source>
          <target state="translated">2D 텐서의 의사 역 (무어-펜로즈 역이라고도 함)을 계산합니다. 자세한 내용 은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bf3fa1ed9287f3a31249968ae80488bbbc90709d" translate="yes" xml:space="preserve">
          <source>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬 또는 정사각형 행렬 배치의 행렬식의 부호 및 로그 절대 값을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="bc5cae9078b78a08c10f30abb3ba9b5b22cfef54" translate="yes" xml:space="preserve">
          <source>Callables prefixed with underscore are considered as helper functions which won&amp;rsquo;t show up in &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">밑줄로 시작하는 &lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt; 은 torch.hub.list () 에 표시되지 않는 도우미 함수로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="9c42beef7db382c9ae158419c3ebc5c8310023f1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt; 호출</target>
        </trans-unit>
        <trans-unit id="265655dd73dcd39520c5cd9a954e45b04d35e8a1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; is equivalent to calling &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt;. The &lt;code&gt;periodic&lt;/code&gt; argument is intended as a helpful shorthand to produce a periodic window as input to functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">호출 &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; 호출하는 것과 &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt; . &lt;code&gt;periodic&lt;/code&gt; 인수 같은 함수에 대한 입력으로 주기적 윈도우 제조하는 유용한 속기 마련된다 &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0d622de7b9cec6af9b50c591c0762f95f6cf23f1" translate="yes" xml:space="preserve">
          <source>Calling a submodule directly (e.g. &lt;code&gt;self.resnet(input)&lt;/code&gt;) is equivalent to calling its &lt;code&gt;forward&lt;/code&gt; method (e.g. &lt;code&gt;self.resnet.forward(input)&lt;/code&gt;).</source>
          <target state="translated">서브 모듈을 직접 호출하는 것은 (예 : &lt;code&gt;self.resnet(input)&lt;/code&gt; ) &lt;code&gt;forward&lt;/code&gt; 메소드 를 호출하는 것과 같습니다 (예 : &lt;code&gt;self.resnet.forward(input)&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1b7d84e5b4532eb537ef53414226540774bd94f3" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="57076e248ecf298e1219a0fa6ecf299084bfedd8" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="6550668f3134865d4fd7ecb672c8b13821a60616" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="393935e7b8f54e6c891a458a9a0a5b90620581df" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="3c4696c3c77b6fbb0767ee074a0b08401ad5fd2b" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="a0f807127c82b5c379fa18d9be9a714272f46e17" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="76582af9585743776e20d4bdf66734ecbe7e7ff9" translate="yes" xml:space="preserve">
          <source>Calls to &lt;code&gt;builtin functions&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;builtin functions&lt;/code&gt; 호출</target>
        </trans-unit>
        <trans-unit id="eb07741ad617617e9abda7e3d52fee63a305a121" translate="yes" xml:space="preserve">
          <source>Calls to methods of builtin types like tensor: &lt;code&gt;x.mm(y)&lt;/code&gt;</source>
          <target state="translated">텐서와 같은 내장 유형의 메서드 호출 : &lt;code&gt;x.mm(y)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fc7e70542d9259610f72ca817bd3eb6584edd2c8" translate="yes" xml:space="preserve">
          <source>Calls to other script functions:</source>
          <target state="translated">다른 스크립트 함수 호출 :</target>
        </trans-unit>
        <trans-unit id="b6867b70db2065294481ad42aed53ba49e6355b8" translate="yes" xml:space="preserve">
          <source>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size</source>
          <target state="translated">크기 입력을 제공하여 2D 이미지와 같은 고차원 입력에도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="221c9205ab30df3bd1004805355ca477c455a15b" translate="yes" xml:space="preserve">
          <source>Can only be called once and before any inter-op parallel work is started (e.g. JIT execution).</source>
          <target state="translated">inter-op 병렬 작업이 시작되기 전에 한 번만 호출 할 수 있습니다 (예 : JIT 실행).</target>
        </trans-unit>
        <trans-unit id="3805fd56c2af8071d51bc53ae0a92fdcb58807db" translate="yes" xml:space="preserve">
          <source>Casting Examples:</source>
          <target state="translated">캐스팅 예 :</target>
        </trans-unit>
        <trans-unit id="e7500c883cdd17fa4172ea83911ebf91a32625de" translate="yes" xml:space="preserve">
          <source>Casts</source>
          <target state="translated">Casts</target>
        </trans-unit>
        <trans-unit id="26538798d0973ae83ca7715b676794ed3b172f33" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;bfloat16&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;bfloat16&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="b6c764881902912eb6ba271fb55ce5179af34673" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;double&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;double&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="78461ae0a43c2d54b7c9efa684628bba9ad1c1ed" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;half&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;half&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="661d47897b8e53cbd52a45424e86044636652304" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to float datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수 및 버퍼를 부동 데이터 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="4724f395a64ded08f676d2e9e0393dcb0f712246" translate="yes" xml:space="preserve">
          <source>Casts all parameters and buffers to &lt;code&gt;dst_type&lt;/code&gt;.</source>
          <target state="translated">모든 매개 변수와 버퍼를 &lt;code&gt;dst_type&lt;/code&gt; 으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="79f02a31265abcb1bb988f26d0c498d397fdd789" translate="yes" xml:space="preserve">
          <source>Casts this storage to bfloat16 type</source>
          <target state="translated">이 스토리지를 bfloat16 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="bd487dabbd2c7187cd079ec504def8aaf3a7b571" translate="yes" xml:space="preserve">
          <source>Casts this storage to bool type</source>
          <target state="translated">이 저장소를 bool 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="6694cf255316b894b3ad2697cc3b38ec9452ca42" translate="yes" xml:space="preserve">
          <source>Casts this storage to byte type</source>
          <target state="translated">이 스토리지를 바이트 유형으로 캐스트</target>
        </trans-unit>
        <trans-unit id="151ff3570c6883e5b0af6409a5ae071d0e154dbc" translate="yes" xml:space="preserve">
          <source>Casts this storage to char type</source>
          <target state="translated">이 스토리지를 char 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="d3e9e6a46615473165fa7e0ec3e61fb30d1e8866" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex double type</source>
          <target state="translated">이 저장소를 복잡한 이중 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="510b5fa8a83cea216c27cd3b246f5ff67c971a64" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex float type</source>
          <target state="translated">이 저장소를 복잡한 플로트 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="2e339cd7fb3f62430cb2f45b27e9e325c9fd432f" translate="yes" xml:space="preserve">
          <source>Casts this storage to double type</source>
          <target state="translated">이 저장소를 더블 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="46bf8db49512235eab20518ddd006bc28b925a2c" translate="yes" xml:space="preserve">
          <source>Casts this storage to float type</source>
          <target state="translated">이 저장소를 float 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="63e25d02d152bfc3b6809c0d00d52a9786a45699" translate="yes" xml:space="preserve">
          <source>Casts this storage to half type</source>
          <target state="translated">이 스토리지를 하프 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="8dd4507b920a808cec77926ae75ebeb4c2c63230" translate="yes" xml:space="preserve">
          <source>Casts this storage to int type</source>
          <target state="translated">이 저장소를 int 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="afab2e0bade340b8cecdf6e13b386611e22e2730" translate="yes" xml:space="preserve">
          <source>Casts this storage to long type</source>
          <target state="translated">이 저장소를 긴 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="d4ce7dd1afcf534c203c4afd8fc54fe512dfe75d" translate="yes" xml:space="preserve">
          <source>Casts this storage to short type</source>
          <target state="translated">이 저장소를 짧은 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="8528ae94e2e148afb1dc7ce9973ecc62392671c1" translate="yes" xml:space="preserve">
          <source>Categorical</source>
          <target state="translated">Categorical</target>
        </trans-unit>
        <trans-unit id="0b7b85a2968dc04c734a5094885d58f3c06a57c0" translate="yes" xml:space="preserve">
          <source>Cauchy</source>
          <target state="translated">Cauchy</target>
        </trans-unit>
        <trans-unit id="795a0c324ff1f130803359d377842d67e3339ceb" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on parameters in this module.</source>
          <target state="translated">autograd에서이 모듈의 매개 변수에 대한 작업을 기록해야하는지 변경합니다.</target>
        </trans-unit>
        <trans-unit id="771633aa9e4ffd6ed00dfc406fa17e09d36a9380" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on this tensor: sets this tensor&amp;rsquo;s &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; attribute in-place. Returns this tensor.</source>
          <target state="translated">autograd가이 텐서에서 작업을 기록해야하는지 변경 :이 텐서의 &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 속성을 제자리에 설정합니다. 이 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="df974d2905d5cd175e8ce062cca16506fc69a1da" translate="yes" xml:space="preserve">
          <source>Channel dim is the 2nd dim of input. When input has dims &amp;lt; 2, then there is no channel dim and the number of channels = 1.</source>
          <target state="translated">Channel dim은 입력의 두 번째 dim입니다. 입력이 희미한 2 미만이면 채널이 희미 해지고 채널 수가 1입니다.</target>
        </trans-unit>
        <trans-unit id="239e4420c7d4084b23404d4d0c56239ffc4eca05" translate="yes" xml:space="preserve">
          <source>Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in &lt;code&gt;inputs&lt;/code&gt; that are of floating point or complex type and with &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">부동 소수점 또는 복합 유형이고 &lt;code&gt;requires_grad=True&lt;/code&gt; 인 &lt;code&gt;inputs&lt;/code&gt; 에서 텐서와 비교하여 작은 유한 차이를 통해 계산 된 기울기를 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="82aa07bbbf68e1780a5f4ead49b824c7cc4f5648" translate="yes" xml:space="preserve">
          <source>Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; that are of floating point or complex type and with &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">부동 소수점 또는 복합 유형이고 &lt;code&gt;requires_grad=True&lt;/code&gt; 인 &lt;code&gt;inputs&lt;/code&gt; 및 &lt;code&gt;grad_outputs&lt;/code&gt; 의 분석 그라디언트 wrt 텐서에 대해 작은 유한 차이를 통해 계산 된 그라디언트의 그라디언트를 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="56ff021269456a017773120e4ed3d4af95c8ae59" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt; 에서 상속 된 모듈에서 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 를 찾아 &lt;code&gt;module&lt;/code&gt; 이 정리 되었는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="e4c8690ed4ddc9de7d901e88e48d1d54b0026ac9" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;code&gt;BasePruningMethod&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;BasePruningMethod&lt;/code&gt; 에서 상속 된 모듈에서 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 를 찾아 &lt;code&gt;module&lt;/code&gt; 이 정리 되었는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="b3a48ae8d9eebbbcfe68f114073dd271c96bf314" translate="yes" xml:space="preserve">
          <source>Check whether it&amp;rsquo;s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread.</source>
          <target state="translated">ONNX 내보내기 중간에 있는지 확인하십시오. 이 함수는 torch.onnx.export () 중간에 True를 반환합니다. torch.onnx.export는 단일 스레드로 실행해야합니다.</target>
        </trans-unit>
        <trans-unit id="e151686a765ce214247b375cc4d7aa1c97e4e14f" translate="yes" xml:space="preserve">
          <source>Checking if the default process group has been initialized</source>
          <target state="translated">기본 프로세스 그룹이 초기화되었는지 확인</target>
        </trans-unit>
        <trans-unit id="b37b3cf9158406adf7b90792ec9193ec0e0c1df6" translate="yes" xml:space="preserve">
          <source>Checkpoint a model or part of the model</source>
          <target state="translated">모델 또는 모델의 일부를 체크 포인트</target>
        </trans-unit>
        <trans-unit id="95fac7ec70656fe8c623fe8fbf3c5c04c9b16115" translate="yes" xml:space="preserve">
          <source>Checkpointing doesn&amp;rsquo;t work with &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt;, but only with &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">검사 점은 작동하지 않습니다 &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt; , 만에 &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="f51f71651c6e78eb334ceb82bf5f372ed3b9e12e" translate="yes" xml:space="preserve">
          <source>Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply &lt;code&gt;preserve_rng_state=False&lt;/code&gt; to &lt;code&gt;checkpoint&lt;/code&gt; or &lt;code&gt;checkpoint_sequential&lt;/code&gt; to omit stashing and restoring the RNG state during each checkpoint.</source>
          <target state="translated">체크 포인트는 역방향 동안 각 체크 포인트 세그먼트에 대해 순방향 통과 세그먼트를 다시 실행하여 구현됩니다. 이로 인해 RNG 상태와 같은 지속적인 상태가 체크 포인트없이 진행될 수 있습니다. 기본적으로 체크 포인팅에는 RNG를 사용하는 체크 포인트 패스 (예 : 드롭 아웃을 통해)가 비 체크 포인트 패스와 비교하여 결정적 출력을 갖도록 RNG 상태를 저글링하는 로직이 포함됩니다. RNG 상태를 숨기고 복원하는 논리는 검사 점 작업의 런타임에 따라 중간 성능 저하를 초래할 수 있습니다. 체크 포인트되지 않은 패스와 비교 한 결정적 출력이 필요하지 않은 경우 &lt;code&gt;preserve_rng_state=False&lt;/code&gt; 를 &lt;code&gt;checkpoint&lt;/code&gt; 또는 &lt;code&gt;checkpoint_sequential&lt;/code&gt; 에 제공하십시오. 각 체크 포인트 동안 은닉 및 RNG 상태 복원을 생략합니다.</target>
        </trans-unit>
        <trans-unit id="332b6971e8e3a29969ff874b4073e4272d2a843f" translate="yes" xml:space="preserve">
          <source>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does &lt;strong&gt;not&lt;/strong&gt; save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.</source>
          <target state="translated">체크 포인트는 컴퓨팅을 메모리와 거래하여 작동합니다. 역방향 계산을 위해 전체 계산 그래프의 모든 중간 활성화를 저장하는 대신 체크 포인트 부분은 중간 활성화를 저장 하지 &lt;strong&gt;않고&lt;/strong&gt; 대신 역방향 패스에서 다시 계산합니다. 모델의 모든 부분에 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="893f0f558202bc0518489f883166ffe50bb8bd4d" translate="yes" xml:space="preserve">
          <source>Checks if all the work submitted has been completed.</source>
          <target state="translated">제출 된 모든 작업이 완료되었는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="945f252428f14d3ff3238e766e3368cedac5589e" translate="yes" xml:space="preserve">
          <source>Checks if all work currently captured by event has completed.</source>
          <target state="translated">현재 이벤트에서 캡처 한 모든 작업이 완료되었는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="635284a98c9a3f2f5e6091ab87ff16c33d35a6cc" translate="yes" xml:space="preserve">
          <source>Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.</source>
          <target state="translated">전송 된 CUDA 텐서가 메모리에서 정리 될 수 있는지 확인합니다. 활성 카운터가없는 경우 참조 카운트에 사용되는 공유 메모리 파일을 강제로 닫습니다. 생산자 프로세스가 텐서 전송을 중단하고 사용하지 않는 메모리를 해제하려는 경우 유용합니다.</target>
        </trans-unit>
        <trans-unit id="39f6e80dd8c8ac31e71117491def641d00a288d1" translate="yes" xml:space="preserve">
          <source>Checks if tensor is in shared memory.</source>
          <target state="translated">텐서가 공유 메모리에 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="19004c9ecdb08358c5474afc574d3f10f8cf3c71" translate="yes" xml:space="preserve">
          <source>Checks if the MPI backend is available.</source>
          <target state="translated">MPI 백엔드를 사용할 수 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="f2d9da2e3f6bd1dd91d87846cd5b419112d24146" translate="yes" xml:space="preserve">
          <source>Checks if the NCCL backend is available.</source>
          <target state="translated">NCCL 백엔드를 사용할 수 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="1f4ac26af6f73e2c8bc1f384efba8f7bd4839e87" translate="yes" xml:space="preserve">
          <source>Chi2</source>
          <target state="translated">Chi2</target>
        </trans-unit>
        <trans-unit id="56268a6e36c10e47601c6c33d1ee9618154a6c67" translate="yes" xml:space="preserve">
          <source>Choosing the network interface to use</source>
          <target state="translated">사용할 네트워크 인터페이스 선택</target>
        </trans-unit>
        <trans-unit id="d9eaceff91c313f7cab180625e8b4dd09baebd23" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 범위 로 고정하고 결과 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="378e42a306697a4dd63aa3dd31b3c55c600db5d2" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 범위 로 고정하고 결과 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="cfe592b543afcbd470b6fcfa84bfb4061fbeb434" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be larger or equal &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; 보다 크거나 같도록 클램프합니다 .</target>
        </trans-unit>
        <trans-unit id="e7bab3a95a3c2fe6a5a776d67024bda46c058f02" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be smaller or equal &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; 보다 작거나 같게 고정 합니다.</target>
        </trans-unit>
        <trans-unit id="a8943ee6c6160fec8979faa9096fac2e19ce7bfd" translate="yes" xml:space="preserve">
          <source>Classes must be new-style classes, as we use &lt;code&gt;__new__()&lt;/code&gt; to construct them with pybind11.</source>
          <target state="translated">&lt;code&gt;__new__()&lt;/code&gt; 를 사용하여 pybind11 로 구성하므로 클래스는 새로운 스타일의 클래스 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="9b6b05a782b6d150aaf1e98e0fdbbcbc865ca9be" translate="yes" xml:space="preserve">
          <source>Classes that inherit from &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.ScriptModule&lt;/code&gt; 에서 상속되는 클래스</target>
        </trans-unit>
        <trans-unit id="94c2a3189e7f7885455350c4c7a8df2d0d6ad1d1" translate="yes" xml:space="preserve">
          <source>Classification</source>
          <target state="translated">Classification</target>
        </trans-unit>
        <trans-unit id="140c2943a67faf9ddd529fa9ef19fca05ebb9209" translate="yes" xml:space="preserve">
          <source>Clears the cuFFT plan cache.</source>
          <target state="translated">cuFFT 계획 캐시를 지 웁니다.</target>
        </trans-unit>
        <trans-unit id="026a5307ef4b381234e9524521b9341950a6f9cf" translate="yes" xml:space="preserve">
          <source>Clip acc@1</source>
          <target state="translated">클립 acc @ 1</target>
        </trans-unit>
        <trans-unit id="a7ba9ed0f0f5ac74d1bfd4abe70aaeb95921f947" translate="yes" xml:space="preserve">
          <source>Clip acc@5</source>
          <target state="translated">클립 acc @ 5</target>
        </trans-unit>
        <trans-unit id="75ccb1e878d6b2fcd6f1473f8a7c353d6b2e4806" translate="yes" xml:space="preserve">
          <source>Clips gradient norm of an iterable of parameters.</source>
          <target state="translated">반복 가능한 매개 변수의 그라디언트 표준을 자릅니다.</target>
        </trans-unit>
        <trans-unit id="990a2e75895b3d0ccfe4b20a82d20496b3e8ee0f" translate="yes" xml:space="preserve">
          <source>Clips gradient of an iterable of parameters at specified value.</source>
          <target state="translated">지정된 값에서 반복 가능한 매개 변수의 그라디언트를 자릅니다.</target>
        </trans-unit>
        <trans-unit id="d4abb632898cd5f62fe263afaf95d739031ecab2" translate="yes" xml:space="preserve">
          <source>Closure use is not currently supported.</source>
          <target state="translated">폐쇄 사용은 현재 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2b8f8ed2e1db909d591f68c446a99d83144eb890" translate="yes" xml:space="preserve">
          <source>Code running on Node 0</source>
          <target state="translated">노드 0에서 실행되는 코드</target>
        </trans-unit>
        <trans-unit id="4a4d07ff7beaaa363e77cbea9319ab7a99ad01d0" translate="yes" xml:space="preserve">
          <source>Code running on Node 1</source>
          <target state="translated">노드 1에서 실행되는 코드</target>
        </trans-unit>
        <trans-unit id="bfa7c14251111856333ef843853b74b24bdf052b" translate="yes" xml:space="preserve">
          <source>Collective functions</source>
          <target state="translated">집단 기능</target>
        </trans-unit>
        <trans-unit id="43092611315cdcde3137662c72b15d7a3a889e75" translate="yes" xml:space="preserve">
          <source>Collects the provided &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects into a single combined &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that is completed when all of the sub-futures are completed.</source>
          <target state="translated">제공된 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 객체를 모든 하위 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 가 완료 될 때 완료 되는 단일 결합 Future 로 수집합니다 .</target>
        </trans-unit>
        <trans-unit id="ce079595058d15f2351c808e0ae1b284bea0c578" translate="yes" xml:space="preserve">
          <source>Combines an array of sliding local blocks into a large containing tensor.</source>
          <target state="translated">슬라이딩 로컬 블록 배열을 큰 포함 텐서로 결합합니다.</target>
        </trans-unit>
        <trans-unit id="46488d0a373ab0b6babd0f93cccfb474321ae981" translate="yes" xml:space="preserve">
          <source>Combining Distributed DataParallel with Distributed RPC Framework</source>
          <target state="translated">Distributed DataParallel과 Distributed RPC 프레임 워크 결합</target>
        </trans-unit>
        <trans-unit id="c170217f39333026f4f3aa0323fa905552560ec9" translate="yes" xml:space="preserve">
          <source>Common environment variables</source>
          <target state="translated">공통 환경 변수</target>
        </trans-unit>
        <trans-unit id="086cb07d3ecde2b840e1f458bfa9fda481174a5f" translate="yes" xml:space="preserve">
          <source>Common linear algebra operations.</source>
          <target state="translated">일반적인 선형 대수 연산.</target>
        </trans-unit>
        <trans-unit id="b4f955c58ceb08791953e5d03b53e986ad69907d" translate="yes" xml:space="preserve">
          <source>Commonly used ones include the following for debugging purposes:</source>
          <target state="translated">일반적으로 사용되는 것은 디버깅 목적으로 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4beeb1b54292a1c6c080f3d414574b83eca3b2f1" translate="yes" xml:space="preserve">
          <source>Communication collectives</source>
          <target state="translated">커뮤니케이션 집단</target>
        </trans-unit>
        <trans-unit id="bfd58ee3a270f3a931009900e1008d549bbd7453" translate="yes" xml:space="preserve">
          <source>Community</source>
          <target state="translated">Community</target>
        </trans-unit>
        <trans-unit id="e0c1faa1db1f49518cfef07ea955debfa4db607f" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교하십시오 .</target>
        </trans-unit>
        <trans-unit id="0ccc8d4186f8b576455c6bf986c275fa4e1ca508" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교하십시오 .</target>
        </trans-unit>
        <trans-unit id="d9194e4e84d8475d85c26bc6d3d0aabdb9a3cfba" translate="yes" xml:space="preserve">
          <source>Compared against the full output from &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;, we have all elements up to the Nyquist frequency.</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교 하면 Nyquist 주파수까지 모든 요소가 있습니다.</target>
        </trans-unit>
        <trans-unit id="25acda77b0bc6b058bd349dd6f4a271e2a967bfe" translate="yes" xml:space="preserve">
          <source>Comparison Operators</source>
          <target state="translated">비교 연산자</target>
        </trans-unit>
        <trans-unit id="64e945410bd38eb8bc541fa4fe0d1a5601a0a7ba" translate="yes" xml:space="preserve">
          <source>Comparison Ops</source>
          <target state="translated">비교 운영</target>
        </trans-unit>
        <trans-unit id="c73def212afdc811169afd7e77aebfbeecb5facc" translate="yes" xml:space="preserve">
          <source>Complex Numbers</source>
          <target state="translated">복소수</target>
        </trans-unit>
        <trans-unit id="258ac8f61c6506a35c0dfc4029a4818ba9555f2a" translate="yes" xml:space="preserve">
          <source>Complex values are infinite when their real or imaginary part is infinite.</source>
          <target state="translated">실수 또는 허수 부분이 무한 할 때 복잡한 값은 무한합니다.</target>
        </trans-unit>
        <trans-unit id="20602cb78c6ade6e08a8e69fafec22e07a7a725f" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Discrete Fourier Transform.</source>
          <target state="translated">복합에서 복합 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="a7075fa71ba5aa25f75795b49f1a3a1c5779af40" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Inverse Discrete Fourier Transform.</source>
          <target state="translated">복소수에서 복소수 역 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="cfb4fe1390cd2aa35fb926daaca343f331617fd7" translate="yes" xml:space="preserve">
          <source>Complex-to-real Inverse Discrete Fourier Transform.</source>
          <target state="translated">복소수에서 실제로의 역 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="5269e04efa4004aaf8887ad9e476bf6a875cd697" translate="yes" xml:space="preserve">
          <source>Composes multiple transforms in a chain. The transforms being composed are responsible for caching.</source>
          <target state="translated">체인에서 여러 변환을 작성합니다. 구성되는 변환은 캐싱을 담당합니다.</target>
        </trans-unit>
        <trans-unit id="27aacbfb281b15263d3c0a945d248f0f178b79f9" translate="yes" xml:space="preserve">
          <source>Compute Kullback-Leibler divergence</source>
          <target state="translated">Kullback-Leibler 발산 계산</target>
        </trans-unit>
        <trans-unit id="49a3dcb8dae28aaaa8933d10f9d1fc99995fdb01" translate="yes" xml:space="preserve">
          <source>Compute combinations of length</source>
          <target state="translated">길이 조합 계산</target>
        </trans-unit>
        <trans-unit id="1c47a0f87804b817e8011c46890b198b5bc4d108" translate="yes" xml:space="preserve">
          <source>Compute the scale and zero point the same way as in the</source>
          <target state="translated">에서와 같은 방식으로 스케일과 영점을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="ad88b5c709893b3c20a622a8af743326fc15e581" translate="yes" xml:space="preserve">
          <source>Computes</source>
          <target state="translated">Computes</target>
        </trans-unit>
        <trans-unit id="b29ddfa9b93c0c21e2cd9673bf929a7688cb085a" translate="yes" xml:space="preserve">
          <source>Computes &lt;code&gt;input&lt;/code&gt; divided by &lt;code&gt;other&lt;/code&gt;, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 값을 &lt;code&gt;other&lt;/code&gt; 로 나눈 값을 요소 별로 계산 하고 각 몫을 0으로 반올림합니다. 동등하게, 몫을 자릅니다.</target>
        </trans-unit>
        <trans-unit id="d3181f8cc8e8fa272eef8d4d3ff7058798e00f63" translate="yes" xml:space="preserve">
          <source>Computes a QR decomposition of &lt;code&gt;input&lt;/code&gt;, but without constructing</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 QR 분해를 계산 하지만 구성하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f69a85669ba4f546c3f6c75d8d8a225191c83e6a" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="bf04c1ac66a51bee90d0426e3c8106a0e4185416" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="f360cd6902e1a0579d80b170c5497449c1afe016" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="26e6d25fb90ed6ad09263d6546b96e18a94d0c3e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool1d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool1d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="18c31c026887e211a21e3dc7927a9d793872976e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool2d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool2d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="85ab6bb517a0364e7f136e092fa61b3c016286ff" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool3d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool3d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="b7cc19759037b27ca2d7fb79591e62353c0937a7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by zeroing out the channels along the specified dim with the lowest Ln-norm.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 정리되지 않은 경우 1의 마스크 여야 함) 에서 시작 하여 &lt;code&gt;default_mask&lt;/code&gt; 노름이 가장 낮은 지정된 dim을 따라 채널을 0으로 제거하여 default_mask 위에 적용 할 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="40d80ee71e511c62ded894cfa5f39bc5b20875f0" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; according to the specific pruning method recipe.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 프 루닝되지 않은 경우 마스크 여야 함)에서 시작 하여 특정 프 루닝 방법 레시피에 따라 &lt;code&gt;default_mask&lt;/code&gt; 위에 적용 할 임의 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="65434598b41da121b29ca65831d67382801ae0c7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by randomly zeroing out channels along the specified dim of the tensor.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 정리되지 않은 경우 1의 마스크 여야 함) 에서 시작 하여 텐서 의 지정된 dim을 따라 채널을 무작위로 제로화 하여 &lt;code&gt;default_mask&lt;/code&gt; 위에 적용 할 임의 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="180bc7b0d8de7293424fe248fc89cd2c995f979c" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7a3fb50e3449e8908990f0c00a969ef20d83c411" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="9efff4b9b21781ca7329ebc9ab81c9ec882d85a1" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="f99800562bd522721e9ea1fa3102c89a24cca75f" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="28c95a2d472ee12c714c4dae6bc27475d57d2a6e" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;code&gt;compute_mask()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;compute_mask()&lt;/code&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7251032ff2b6d46923b585e413559af8bf29d269" translate="yes" xml:space="preserve">
          <source>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</source>
          <target state="translated">입력 값에 대한 출력 기울기의 합을 계산하고 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2ad7888f31836fca25fd59e8bf8deaf0775bd8bb" translate="yes" xml:space="preserve">
          <source>Computes batched the p-norm distance between each pair of the two collections of row vectors.</source>
          <target state="translated">두 행 벡터 모음의 각 쌍 사이에 배치 된 p- 노름 거리를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="f2e6dd93c4230ac985c6eeef22bb5e42799d351c" translate="yes" xml:space="preserve">
          <source>Computes element-wise equality</source>
          <target state="translated">요소 별 동등성을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="23c55c6419438b7d4003d50aab0fa866dc6b754c" translate="yes" xml:space="preserve">
          <source>Computes log probabilities for all</source>
          <target state="translated">모두에 대한 로그 확률을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="bffd347218e0b14edb14dc921db35164acedc246" translate="yes" xml:space="preserve">
          <source>Computes sums or means of &amp;lsquo;bags&amp;rsquo; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">중간 임베딩을 인스턴스화하지 않고 임베딩 '백'의 합계 또는 수단을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="a7d9c1eb8347c9d1cc0af2b2b967716bd8c4ae77" translate="yes" xml:space="preserve">
          <source>Computes sums, means or maxes of &lt;code&gt;bags&lt;/code&gt; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">중간 임베딩을 인스턴스화하지 않고 임베딩 &lt;code&gt;bags&lt;/code&gt; 합계, 평균 또는 최대 값을 계산 합니다.</target>
        </trans-unit>
        <trans-unit id="b71f4915a8b78a90bf71f4feb80ae5bf3e75096c" translate="yes" xml:space="preserve">
          <source>Computes the</source>
          <target state="translated">계산</target>
        </trans-unit>
        <trans-unit id="ff498bc657a9c74488608c0ce73fd38063feeafe" translate="yes" xml:space="preserve">
          <source>Computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;multivariate log-gamma function&lt;/a&gt;) with dimension</source>
          <target state="translated">차원을 사용 하여 &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;다변량 로그 감마 함수&lt;/a&gt; )를 계산 합니다.</target>
        </trans-unit>
        <trans-unit id="76557cba93a25c6c86c33cc0d82d4b6080c0ca6f" translate="yes" xml:space="preserve">
          <source>Computes the Cholesky decomposition of a symmetric positive-definite matrix</source>
          <target state="translated">양의 정의 대칭 행렬의 촐레 스키 분해를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="81b53a3d90c63815ad24683a8c1c69a90d47d01e" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 헤비 사이드 스텝 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="5d07d50a47d9ca8fe0d585f7aa291a49089343e4" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;. The Heaviside step function is defined as:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 헤비 사이드 스텝 함수를 계산합니다 . 헤비 사이드 스텝 함수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="275e74bec95b0ab85103e19b6d6479c703706e12" translate="yes" xml:space="preserve">
          <source>Computes the Kaiser window with window length &lt;code&gt;window_length&lt;/code&gt; and shape parameter &lt;code&gt;beta&lt;/code&gt;.</source>
          <target state="translated">창 길이 &lt;code&gt;window_length&lt;/code&gt; 및 모양 매개 변수 &lt;code&gt;beta&lt;/code&gt; 로 카이저 창을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="622fda98680f4721949b3e2bf9b78a14c34deb65" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;.</source>
          <target state="translated">행렬의 LU 분해 또는 행렬 &lt;code&gt;A&lt;/code&gt; 의 배치를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="8ec07270d301f5370c046b7066eb8663a8f402a1" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;. Returns a tuple containing the LU factorization and pivots of &lt;code&gt;A&lt;/code&gt;. Pivoting is done if &lt;code&gt;pivot&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">행렬의 LU 분해 또는 행렬 &lt;code&gt;A&lt;/code&gt; 의 배치를 계산합니다 . &lt;code&gt;A&lt;/code&gt; 의 LU 분해 및 피벗을 포함하는 튜플을 반환합니다 . 피벗이 &lt;code&gt;True&lt;/code&gt; 로 설정 되면 &lt;code&gt;pivot&lt;/code&gt; 이 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="0c3fba3f6161937ff4ae86bdb0e983d0c0fdda0b" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 N 차원 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="5852120d66c32f7c318de497ae26eeb74f1de763" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 N 차원 역 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="efd57d1e36575205366c8a3b4753ee576bd1a231" translate="yes" xml:space="preserve">
          <source>Computes the N-dimensional discrete Fourier transform of real &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">실수 &lt;code&gt;input&lt;/code&gt; 의 N 차원 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="a21392e07d8e832fdaaf1fc616e555a310a70752" translate="yes" xml:space="preserve">
          <source>Computes the QR decomposition of a matrix or a batch of matrices &lt;code&gt;input&lt;/code&gt;, and returns a namedtuple (Q, R) of tensors such that</source>
          <target state="translated">행렬의 QR 분해 또는 행렬 &lt;code&gt;input&lt;/code&gt; 의 배치를 계산하고 다음 과 같은 텐서의 명명 된 튜플 (Q, R)을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d476f44296e977553528b2efe494b3789b4d6cc8" translate="yes" xml:space="preserve">
          <source>Computes the absolute value of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 에있는 각 요소의 절대 값을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="da0e0333ca7d69a3bf51d86eddafcf5537f32ee1" translate="yes" xml:space="preserve">
          <source>Computes the base two exponential function of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 밑이 2 인 지수 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="833d30840f6f79f8ac884e63624d72a29fe2aedb" translate="yes" xml:space="preserve">
          <source>Computes the batchwise pairwise distance between vectors</source>
          <target state="translated">벡터 사이의 배치 별 쌍별 거리를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b81b9d06afde312e3c1de6fa3003c1826f20a144" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 AND를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="724c0661558717da97da9f13301714db62123db8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 AND를 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 AND를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="4c385fff8361fd76237dd3d0d310601e4ded675d" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor.</source>
          <target state="translated">주어진 입력 텐서의 비트 단위 NOT을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d34002186774eda6ff1544d9bb235cb5b5e464f1" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.</source>
          <target state="translated">주어진 입력 텐서의 비트 단위 NOT을 계산합니다. 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 NOT을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1676d4cae5deb51721032154a09c9a5c26471fe8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 OR을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="db7eb5bd0ae610915cc8f5aaa985b610903472f9" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 OR을 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 OR을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="2a7aa82e188d57e1be91dd1e59e101918adf2efe" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 XOR을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="88579c44bdc54edf5679b9534b4247fa5a02a043" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 XOR을 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 XOR을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="8a3e45b925774a34d85da6c0c7b80c7c01e6ba8d" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 보완 오차 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="159a12e467808284be4f45386c2f19ec971818e9" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;. The complementary error function is defined as follows:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 보완 오차 함수를 계산합니다 . 보완 오류 함수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="e8d67586d06368678f0e647fdffb8c00b8bde78b" translate="yes" xml:space="preserve">
          <source>Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.</source>
          <target state="translated">변환을 반전하고 기본 분포의 점수를 계산하여 누적 분포 함수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1983c92beb407d56255dcc00b5fe7b66872fc6d6" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors.</source>
          <target state="translated">두 텐서의 내적 (내적)을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="9a5b212f2d4dd5a8260512297de68c3095e68cab" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.</source>
          <target state="translated">두 텐서의 내적 (내적)을 계산합니다. vdot (a, b) 함수는 dot (a, b)와는 다르게 복소수를 처리합니다. 첫 번째 인수가 복소수이면 첫 번째 인수의 켤레 복소수가 내적 계산에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="e16561fbcf4a6c2f2dd9afb54001a93a98731fdc" translate="yes" xml:space="preserve">
          <source>Computes the eigenvalues and eigenvectors of a real square matrix.</source>
          <target state="translated">실수 제곱 행렬의 고유 값과 고유 벡터를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="52f9bd141b84eda92242b06f7f6d290a253966b4" translate="yes" xml:space="preserve">
          <source>Computes the element-wise angle (in radians) of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 각도 (라디안 단위)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="47dc21f5f4986e27abb977788d16b590a07a71d6" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 켤레를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="9a964097a08f63a42878b22585751fd92a8682df" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor. If :attr`input` has a non-complex dtype, this function just returns &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 켤레를 계산합니다 . : attr`input`에 복잡하지 않은 dtype이 있으면이 함수는 &lt;code&gt;input&lt;/code&gt; 을 반환 합니다 .</target>
        </trans-unit>
        <trans-unit id="8e315d5a6fd3a46bc729f0eda3f3fb92b601c50b" translate="yes" xml:space="preserve">
          <source>Computes the element-wise greatest common divisor (GCD) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 요소 별 최대 공약수 (GCD)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="91ee032a4ff000ec84d35b04317442f4984e6937" translate="yes" xml:space="preserve">
          <source>Computes the element-wise least common multiple (LCM) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 요소 별 최소 공배수 (LCM)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="8e7b021553644e46362536880a5431b72a0587b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="68b2bf2409a8b5d2be8906244e0a49294d2e9679" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a94b7ab9f10124804e19d264d7207840dc6728d2" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor.</target>
        </trans-unit>
        <trans-unit id="117610e848f679548feba89108c832e6e9a20124" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9572b2dd91b0d03ca02c1394c350df1174a646b0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="256665a803b61ee6a430a86e174d4bca0ee69abe" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c172a20dd69ad22bb7b0a4de9500f73f3c3f98b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="c8837d2e60640bc8199344497981535b4ce62699" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b73dd1c77581f85619be2c0a672fa472582b7ef0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c821156c5be7a7c6ec010c82496a8862539fc670" translate="yes" xml:space="preserve">
          <source>Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6b3693910cf57f242c4267e044d56aef8263e01c" translate="yes" xml:space="preserve">
          <source>Computes the element-wise remainder of division.</source>
          <target state="translated">Computes the element-wise remainder of division.</target>
        </trans-unit>
        <trans-unit id="9003dd6b7c6711ac836cacfed24f4e0435a56aec" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element.</source>
          <target state="translated">Computes the error function of each element.</target>
        </trans-unit>
        <trans-unit id="29774d569920339acd042c100fd75031d60d53ac" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element. The error function is defined as follows:</source>
          <target state="translated">Computes the error function of each element. The error function is defined as follows:</target>
        </trans-unit>
        <trans-unit id="2434be214f6e5985b87d4c052539ff329a7f0314" translate="yes" xml:space="preserve">
          <source>Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cb83638df4a6cb7d822fc5fb54ba573a3cd29240" translate="yes" xml:space="preserve">
          <source>Computes the gradient of current tensor w.r.t. graph leaves.</source>
          <target state="translated">Computes the gradient of current tensor w.r.t. graph leaves.</target>
        </trans-unit>
        <trans-unit id="ce4470aa5bf338d4ad1d1c606fbb7a81df3b9429" translate="yes" xml:space="preserve">
          <source>Computes the histogram of a tensor.</source>
          <target state="translated">Computes the histogram of a tensor.</target>
        </trans-unit>
        <trans-unit id="28063e348dc010ab497d4b723f1ef5afa57d8d1a" translate="yes" xml:space="preserve">
          <source>Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e87d84d689f558075500c51085bf75e2883ed115" translate="yes" xml:space="preserve">
          <source>Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.</source>
          <target state="translated">변환을 사용하고 기본 분포의 점수를 계산하여 역 누적 분포 함수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="a8f61c7d805f4a069d9c46e57aa8a4eda42a714f" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ac7da6502603da8eebb2a12646d0b0cbe100d072" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;. The inverse error function is defined in the range</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; . The inverse error function is defined in the range</target>
        </trans-unit>
        <trans-unit id="e0fc6a3f3385283ab40e97e09d370d8da0903b9c" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42895543a1b54cd19094b1bf3f3033fd72a6293" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8157bbb847ce3db3aa43a43004aeed9105153b04" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="751c20273a8826ac2930fa4c66f27dae7e42581e" translate="yes" xml:space="preserve">
          <source>Computes the inverse of a symmetric positive-definite matrix</source>
          <target state="translated">Computes the inverse of a symmetric positive-definite matrix</target>
        </trans-unit>
        <trans-unit id="fa6955d637d20e846385cffc851f7eba38902835" translate="yes" xml:space="preserve">
          <source>Computes the log det jacobian &lt;code&gt;log |dy/dx|&lt;/code&gt; given input and output.</source>
          <target state="translated">로그 det jacobian &lt;code&gt;log |dy/dx|&lt;/code&gt; 계산합니다. 주어진 입력과 출력.</target>
        </trans-unit>
        <trans-unit id="5ad2395a52ab42ac3844b6eab85a745737496c2b" translate="yes" xml:space="preserve">
          <source>Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0ab463de8fae2192880ba9572b3595b33254233d" translate="yes" xml:space="preserve">
          <source>Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="53d62470ba220fb7afc0e820b9e1d137675f35a7" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="df866b02f2fffb4ec82fb366d91cc209748d5c59" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e2bd9739508dd66799ff1e51b1381c47e1c1143b" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</target>
        </trans-unit>
        <trans-unit id="1c827fe5b069aea41e126efde791986643cae21d" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b12e2ad844db4fa1d8dc61bc76b5cc7fede08ebb" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="9a50e0c97fa82cb476bece9bdcb5b778a06bbe5b" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c4aeca69a4a2b0bea72290f1b36d08fba25dfffc" translate="yes" xml:space="preserve">
          <source>Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt;. This function will be faster if the rows are contiguous.</source>
          <target state="translated">Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt; . This function will be faster if the rows are contiguous.</target>
        </trans-unit>
        <trans-unit id="52fb62cb631b335790a2488186d42d7739c05544" translate="yes" xml:space="preserve">
          <source>Computes the solution to the least squares and least norm problems for a full rank matrix</source>
          <target state="translated">Computes the solution to the least squares and least norm problems for a full rank matrix</target>
        </trans-unit>
        <trans-unit id="cc93b1c0376d6ee7ab14503b3b1f5781ffb751f1" translate="yes" xml:space="preserve">
          <source>Computes the sum of gradients of given tensors w.r.t. graph leaves.</source>
          <target state="translated">주어진 텐서 wrt 그래프 잎의 기울기 합계를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="cf99caa72d6917de8396ddac94469751042f2b03" translate="yes" xml:space="preserve">
          <source>Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cbcbb3162b2e52eeda82ef5d679b6c19cdfaf1b5" translate="yes" xml:space="preserve">
          <source>Computing dependencies</source>
          <target state="translated">Computing dependencies</target>
        </trans-unit>
        <trans-unit id="6ad79ab6353b1eee8ebbc085e10d17c4fcfb024f" translate="yes" xml:space="preserve">
          <source>Concat</source>
          <target state="translated">Concat</target>
        </trans-unit>
        <trans-unit id="4ddddf59160aed751a5f07007a03c044d9753ba8" translate="yes" xml:space="preserve">
          <source>Concatenates a sequence of tensors along a new dimension.</source>
          <target state="translated">Concatenates a sequence of tensors along a new dimension.</target>
        </trans-unit>
        <trans-unit id="1b32473fe5755da0d831ffcfc10b7cdd982c9092" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</target>
        </trans-unit>
        <trans-unit id="a9ec200f382eb7e2b6f899132d934e4d406c3bcb" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</target>
        </trans-unit>
        <trans-unit id="c06ff30c05c7117f8cd03376fe398c72a9caaba3" translate="yes" xml:space="preserve">
          <source>Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a &lt;code&gt;calculate_qparams&lt;/code&gt; function that computes the quantization parameters given the collected statistics.</source>
          <target state="translated">구체적인 관찰자는 동일한 API를 따라야합니다. 앞으로 그들은 관찰 된 Tensor의 통계를 업데이트 할 것입니다. 그리고 수집 된 통계에 따라 양자화 매개 변수를 계산 하는 &lt;code&gt;calculate_qparams&lt;/code&gt; 함수를 제공해야합니다 .</target>
        </trans-unit>
        <trans-unit id="1f8d364de32d6d9b96e9b370a225d6dab6d594c7" translate="yes" xml:space="preserve">
          <source>Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</source>
          <target state="translated">Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; &lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</target>
        </trans-unit>
        <trans-unit id="30e59e381077a379cb9607bde3a4a42eb3f45ab9" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</target>
        </trans-unit>
        <trans-unit id="7f6c99c148a673c2e06016f795e6fb113b6d25b3" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</target>
        </trans-unit>
        <trans-unit id="41c82c00063d59cec7ac15ce301400bc6cba5a1e" translate="yes" xml:space="preserve">
          <source>Considering the specific case of Momentum, the update can be written as</source>
          <target state="translated">Momentum의 특정 사례를 고려할 때 업데이트는 다음과 같이 작성할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a3d8f578f82ef706b4638b32cc8ac9d832c7f7c7" translate="yes" xml:space="preserve">
          <source>ConstantPad1d</source>
          <target state="translated">ConstantPad1d</target>
        </trans-unit>
        <trans-unit id="998c295145e82549d2f17c1a6ba6c23bef09837b" translate="yes" xml:space="preserve">
          <source>ConstantPad2d</source>
          <target state="translated">ConstantPad2d</target>
        </trans-unit>
        <trans-unit id="4a15d68828e68d36dcfe86ffe64b4e4f826c7f8a" translate="yes" xml:space="preserve">
          <source>ConstantPad3d</source>
          <target state="translated">ConstantPad3d</target>
        </trans-unit>
        <trans-unit id="0a41b38808acdf43af00c5eb932dd87575946224" translate="yes" xml:space="preserve">
          <source>ConstantPadNd</source>
          <target state="translated">ConstantPadNd</target>
        </trans-unit>
        <trans-unit id="0f386d7e7881b32fa39cb7b62bdb15c0f3a4c0e1" translate="yes" xml:space="preserve">
          <source>Constants</source>
          <target state="translated">Constants</target>
        </trans-unit>
        <trans-unit id="cba7185d08d214544898ab239c21fb5414c4fc69" translate="yes" xml:space="preserve">
          <source>Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt;.</source>
          <target state="translated">Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3e93ee8b7e4549e7e136ce0f0147ddef90f905dc" translate="yes" xml:space="preserve">
          <source>Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58c6f5a95316e94c0311dfa74a05b94384191ff6" translate="yes" xml:space="preserve">
          <source>Constructing averaged models</source>
          <target state="translated">평균 모델 구성</target>
        </trans-unit>
        <trans-unit id="33ba182b915c42fbc2d5adcf6ef39b2978c20cad" translate="yes" xml:space="preserve">
          <source>Constructing it</source>
          <target state="translated">그것을 구성</target>
        </trans-unit>
        <trans-unit id="14faa516e57e5e69781808c4221cf6c62dc83c10" translate="yes" xml:space="preserve">
          <source>Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="243abd8ffcbf23837c06014609b82a14c78192f1" translate="yes" xml:space="preserve">
          <source>Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="25ef738c9d86e2fc8c4436c90a8dc11de37e7492" translate="yes" xml:space="preserve">
          <source>Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</source>
          <target state="translated">Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</target>
        </trans-unit>
        <trans-unit id="e3591588e6daf80e91783781c9de4873426a6c50" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="0b9d4f7f98afc347ec7829b73a659d20e47f4656" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="a8efeef3cc601befcf295dbb51e707b10b91e402" translate="yes" xml:space="preserve">
          <source>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="63a7411b909e98e9e117a9221e16319404fd9e0b" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="40a8ca8c7bf720c8a0f63021a990b502e7b406b0" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="c44ff1a6ea4f78ec6e55eb5a4dd0b0381511fd66" translate="yes" xml:space="preserve">
          <source>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="0bac8f59a0445e0c89aaa35b2ecaf7957f6cbc76" translate="yes" xml:space="preserve">
          <source>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="ed68461c0a66e54bb8577296b733b09ee8628e1f" translate="yes" xml:space="preserve">
          <source>Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fa58d20a53a578ca582a51e6082babe2633522d9" translate="yes" xml:space="preserve">
          <source>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a RetinaNet model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="c060b450d2f139169d01201742e2de6bd6813c33" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c1b1411f7ad7f2a9785e3f797d1ed1dca55c0522" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="946788267a37fd704ae9ab9be9bb919471421254" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c3079515cd84378d124e9d68610779cdd6cd48ea" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="17f894712d4813d2bb2bfed6f1251b3bc7ad7f28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21791c63e7cf531ba91fa49b8e8790ebd26c8e28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="3f22df20de166bd700f665def840237abc20c0b1" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="7cbc2c2e6f7677b79f4e49073177abc0eaf5c360" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f93f080d3b4b5a3e3dc0939299e73f9681c6cfa7" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="be72a7a131ec1a761dd654077e838e6f0e7ea737" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;. A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt;, in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; . A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt; , in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="0e69f365d19e40e598e945db7b7eaf81602d0597" translate="yes" xml:space="preserve">
          <source>Constructs a tensor with &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">Constructs a tensor with &lt;code&gt;data&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="26d1a380017a1ee5db1ae9f2be051aa31298d8a4" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning.</target>
        </trans-unit>
        <trans-unit id="ecedca4e6711cc4546829104ac643201429f438a" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</target>
        </trans-unit>
        <trans-unit id="e040a458f46532a90ec69fa0b4bfc33ba151c98b" translate="yes" xml:space="preserve">
          <source>Containers</source>
          <target state="translated">Containers</target>
        </trans-unit>
        <trans-unit id="a5f7ef3dcfb494670b1f12c053712004a30a5030" translate="yes" xml:space="preserve">
          <source>Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</source>
          <target state="translated">Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</target>
        </trans-unit>
        <trans-unit id="c965d220340a70f3a925cd031cea31e23574727b" translate="yes" xml:space="preserve">
          <source>Context manager that makes every autograd operation emit an NVTX range.</source>
          <target state="translated">모든 autograd 작업이 NVTX 범위를 내보내도록하는 컨텍스트 관리자.</target>
        </trans-unit>
        <trans-unit id="67a3ce3e7cefceac3d4c664550d90fc678be467d" translate="yes" xml:space="preserve">
          <source>Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks</source>
          <target state="translated">autograd 프로파일 러 상태를 관리하고 결과 요약을 보유하는 컨텍스트 관리자. 내부적으로는 C ++에서 실행되는 함수의 이벤트를 기록하고 해당 이벤트를 Python에 노출합니다. 모든 코드를 래핑 할 수 있으며 PyTorch 함수의 런타임 만보고합니다. 참고 : 프로파일 러는 스레드 로컬이며 비동기 작업에 자동으로 전파됩니다.</target>
        </trans-unit>
        <trans-unit id="1b40e97ec64197b261ed2c352a763baa4c6268be" translate="yes" xml:space="preserve">
          <source>Context method mixins</source>
          <target state="translated">컨텍스트 메서드 믹스 인</target>
        </trans-unit>
        <trans-unit id="69cef769a134b0d4a56c19cba61f8a8ed316d60c" translate="yes" xml:space="preserve">
          <source>Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt;, which is required to correctly execute a distributed autograd pass.</source>
          <target state="translated">Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt; , which is required to correctly execute a distributed autograd pass.</target>
        </trans-unit>
        <trans-unit id="34579070906cfe3afb9639eadd970c899029c8ca" translate="yes" xml:space="preserve">
          <source>Context-manager that changes the current device to that of given object.</source>
          <target state="translated">현재 장치를 주어진 개체의 장치로 변경하는 컨텍스트 관리자입니다.</target>
        </trans-unit>
        <trans-unit id="4802d234013bb6f1339b5d9be21cf85859570968" translate="yes" xml:space="preserve">
          <source>Context-manager that changes the selected device.</source>
          <target state="translated">선택한 장치를 변경하는 컨텍스트 관리자입니다.</target>
        </trans-unit>
        <trans-unit id="05fa9ad5bb0946657dbbab246393aea9c7883c54" translate="yes" xml:space="preserve">
          <source>Context-manager that disabled gradient calculation.</source>
          <target state="translated">Context-manager that disabled gradient calculation.</target>
        </trans-unit>
        <trans-unit id="10cdc0b206fae92706049769655993622c2c5fd2" translate="yes" xml:space="preserve">
          <source>Context-manager that enable anomaly detection for the autograd engine.</source>
          <target state="translated">autograd 엔진에 대한 이상 감지를 사용하는 컨텍스트 관리자.</target>
        </trans-unit>
        <trans-unit id="98ceadd98235bb51cb8307cf83dd7c6d79e59720" translate="yes" xml:space="preserve">
          <source>Context-manager that enables gradient calculation.</source>
          <target state="translated">Context-manager that enables gradient calculation.</target>
        </trans-unit>
        <trans-unit id="da467a4306f8a33fc27d4f7cbea22fc9876c9e5b" translate="yes" xml:space="preserve">
          <source>Context-manager that selects a given stream.</source>
          <target state="translated">주어진 스트림을 선택하는 컨텍스트 관리자.</target>
        </trans-unit>
        <trans-unit id="8479c988b868dfa3b4c887d081e83a92acf0bb43" translate="yes" xml:space="preserve">
          <source>Context-manager that sets gradient calculation to on or off.</source>
          <target state="translated">Context-manager that sets gradient calculation to on or off.</target>
        </trans-unit>
        <trans-unit id="dc43ee1ff31732f8dd825fad1558c5ab44a47fa2" translate="yes" xml:space="preserve">
          <source>Context-manager that sets the anomaly detection for the autograd engine on or off.</source>
          <target state="translated">autograd 엔진에 대한 이상 감지를 설정하거나 해제하는 컨텍스트 관리자입니다.</target>
        </trans-unit>
        <trans-unit id="2fee61d743d1ceafef226afc45bfff28d600d1ec" translate="yes" xml:space="preserve">
          <source>ContinuousBernoulli</source>
          <target state="translated">ContinuousBernoulli</target>
        </trans-unit>
        <trans-unit id="4fbe3836d5db9ff249ce993c595d3f0f979273c3" translate="yes" xml:space="preserve">
          <source>Conv</source>
          <target state="translated">Conv</target>
        </trans-unit>
        <trans-unit id="0f579280c2328a913d6d725180603d671f003e1c" translate="yes" xml:space="preserve">
          <source>Conv1d</source>
          <target state="translated">Conv1d</target>
        </trans-unit>
        <trans-unit id="40b3c3b8c860add6637a732716bb59fe0a472372" translate="yes" xml:space="preserve">
          <source>Conv2d</source>
          <target state="translated">Conv2d</target>
        </trans-unit>
        <trans-unit id="11775e3bcd7c158060f3c37b634846211f59b0a8" translate="yes" xml:space="preserve">
          <source>Conv3d</source>
          <target state="translated">Conv3d</target>
        </trans-unit>
        <trans-unit id="7bd21924a004fee82a9b59ee3da28ba6cf10e0cd" translate="yes" xml:space="preserve">
          <source>ConvBn1d</source>
          <target state="translated">ConvBn1d</target>
        </trans-unit>
        <trans-unit id="b49eb764a64698e64c0e874c6a40f0bd980f5854" translate="yes" xml:space="preserve">
          <source>ConvBn2d</source>
          <target state="translated">ConvBn2d</target>
        </trans-unit>
        <trans-unit id="f2a4cf1ba0285a54525c0f0bced1d81a716af040" translate="yes" xml:space="preserve">
          <source>ConvBnReLU1d</source>
          <target state="translated">ConvBnReLU1d</target>
        </trans-unit>
        <trans-unit id="9a62f54f222bf04d2c6e425b49c602b198a2f54e" translate="yes" xml:space="preserve">
          <source>ConvBnReLU2d</source>
          <target state="translated">ConvBnReLU2d</target>
        </trans-unit>
        <trans-unit id="02be55e7a33c6df1aae2e7f342fb637e9ac77c6b" translate="yes" xml:space="preserve">
          <source>ConvReLU1d</source>
          <target state="translated">ConvReLU1d</target>
        </trans-unit>
        <trans-unit id="ee43a211652ce7b140d6ac846ff3539f57bfe4ce" translate="yes" xml:space="preserve">
          <source>ConvReLU2d</source>
          <target state="translated">ConvReLU2d</target>
        </trans-unit>
        <trans-unit id="9c9f7992fc4e6c43d3ef3ba68725dbe4ae51d5e7" translate="yes" xml:space="preserve">
          <source>ConvReLU3d</source>
          <target state="translated">ConvReLU3d</target>
        </trans-unit>
        <trans-unit id="afca9fffa388b0f4407644f89fc3d2572519c33f" translate="yes" xml:space="preserve">
          <source>ConvTranspose1d</source>
          <target state="translated">ConvTranspose1d</target>
        </trans-unit>
        <trans-unit id="fc42d54df990ea8e2fd96576309fbe506686556e" translate="yes" xml:space="preserve">
          <source>ConvTranspose2d</source>
          <target state="translated">ConvTranspose2d</target>
        </trans-unit>
        <trans-unit id="aa70f25fef0f0928f03585abe233d18e1b7a43bf" translate="yes" xml:space="preserve">
          <source>ConvTranspose3d</source>
          <target state="translated">ConvTranspose3d</target>
        </trans-unit>
        <trans-unit id="bf6b6d744534af31ad6085d41b063c70edf02466" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</target>
        </trans-unit>
        <trans-unit id="6e35177ef47547c230ba4dcc90eda6d927bb9e5a" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</target>
        </trans-unit>
        <trans-unit id="5a0770a286742ba2629162d473ed7de8b93bc405" translate="yes" xml:space="preserve">
          <source>Convert one vector to the parameters</source>
          <target state="translated">Convert one vector to the parameters</target>
        </trans-unit>
        <trans-unit id="be41e2cca6d10c1b2760ba9429806df5479848bc" translate="yes" xml:space="preserve">
          <source>Convert parameters to one vector</source>
          <target state="translated">Convert parameters to one vector</target>
        </trans-unit>
        <trans-unit id="01733ec2af89cb05f3440f813cca55cd11cb2b18" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6906a54aac4ab7e793a9985a2b6c90ca5f574d9a" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;. If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt;, no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt;. Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; . If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt; , no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt; . Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</target>
        </trans-unit>
        <trans-unit id="294b48683ec8d036ef223113fe45f4c4409c00f1" translate="yes" xml:space="preserve">
          <source>Converts a float model to dynamic (i.e. weights-only) quantized model.</source>
          <target state="translated">부동 모델을 동적 (예 : 가중치 전용) 양자화 된 모델로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="5490b3bea11fc41ac534c3f667531da495228f6a" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</source>
          <target state="translated">Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</target>
        </trans-unit>
        <trans-unit id="a8473a264e544b1bee1239b9dbf9ad1a93b701b5" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a quantized tensor with given scale and zero point.</source>
          <target state="translated">Converts a float tensor to a quantized tensor with given scale and zero point.</target>
        </trans-unit>
        <trans-unit id="7e8ec30d2295e5cc0c009208c53acb2a3f05bb56" translate="yes" xml:space="preserve">
          <source>Converts submodules in input module to a different module according to &lt;code&gt;mapping&lt;/code&gt; by calling &lt;code&gt;from_float&lt;/code&gt; method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.</source>
          <target state="translated">대상 모듈 클래스에서 &lt;code&gt;from_float&lt;/code&gt; 메서드를 호출 하여 &lt;code&gt;mapping&lt;/code&gt; 에 따라 입력 모듈의 서브 모듈을 다른 모듈로 변환 합니다. 그리고 remove_qconfig가 True로 설정된 경우 끝에 qconfig를 제거하십시오.</target>
        </trans-unit>
        <trans-unit id="7757dbf24e068e638ad997ae9c4d2e07adceb8c8" translate="yes" xml:space="preserve">
          <source>Convolution Layers</source>
          <target state="translated">Convolution Layers</target>
        </trans-unit>
        <trans-unit id="be084db8daa3740ea18a2c2e59749d4c0010cf94" translate="yes" xml:space="preserve">
          <source>Convolution functions</source>
          <target state="translated">Convolution functions</target>
        </trans-unit>
        <trans-unit id="989f24495d4e5092a0b9ea78596dc97ffbe13f9f" translate="yes" xml:space="preserve">
          <source>Conv{1,2,3}D</source>
          <target state="translated">Conv{1,2,3}D</target>
        </trans-unit>
        <trans-unit id="1737f5913c8956ac817ace6ab2658a389afc44d8" translate="yes" xml:space="preserve">
          <source>Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</source>
          <target state="translated">Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ffea1905ca53aaa0c2e1dd04c44cfd1d1d610e9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="b20573e90f9dd7fefb1d1ba5fccbebf327c4350e" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="2aa7f8afed131bd721ebda279fdf8b30aaa83be9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="3fbe05d7c19e34a099d5b27aecd7e50cc995055f" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="c58511363c704a11c0530ab36e516cead0a26399" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</source>
          <target state="translated">Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</target>
        </trans-unit>
        <trans-unit id="31514897cedc5e7c028b472351d6c9c6b226eacf" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b302a58928c86985fd5405fb9b12e6f3b385a56d" translate="yes" xml:space="preserve">
          <source>Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt; . For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt; , then the &lt;code&gt;i&lt;/code&gt; th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt; th row of &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b91c805903c6cb0b3510a18d72ca193bc741d244" translate="yes" xml:space="preserve">
          <source>Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="278e4ebde33e7e777180bcca75800abdc23470b8" translate="yes" xml:space="preserve">
          <source>Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="361dead9429b7e599e4930ab20d7e7621cc46eae" translate="yes" xml:space="preserve">
          <source>Core statistics:</source>
          <target state="translated">핵심 통계 :</target>
        </trans-unit>
        <trans-unit id="bbf794aba20b276c8548169896a15590fe109e14" translate="yes" xml:space="preserve">
          <source>CosineEmbeddingLoss</source>
          <target state="translated">CosineEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="38519b5b3e654f4e1b794c35fd7670bb326f9738" translate="yes" xml:space="preserve">
          <source>CosineSimilarity</source>
          <target state="translated">CosineSimilarity</target>
        </trans-unit>
        <trans-unit id="09bf9d0f08c3cee1a60852d335fb4cf2f77d1281" translate="yes" xml:space="preserve">
          <source>Count the frequency of each value in an array of non-negative ints.</source>
          <target state="translated">Count the frequency of each value in an array of non-negative ints.</target>
        </trans-unit>
        <trans-unit id="fa674450dd813dec8a084d834fc97c369ca9a763" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0da99e01ab08d505f8e3aacba31f722f60ab5af" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;. If no dim is specified then all non-zeros in the tensor are counted.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; . If no dim is specified then all non-zeros in the tensor are counted.</target>
        </trans-unit>
        <trans-unit id="8bd108ea71b4b5faec418d13c1eaaf112af7d5fc" translate="yes" xml:space="preserve">
          <source>Create a block diagonal matrix from provided tensors.</source>
          <target state="translated">Create a block diagonal matrix from provided tensors.</target>
        </trans-unit>
        <trans-unit id="c794a7007c1ced08b83ca3f0fb4e8db7a3266ce0" translate="yes" xml:space="preserve">
          <source>Create a dynamic quantized module from a float module or qparams_dict</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 동적 양자화 모듈 만들기</target>
        </trans-unit>
        <trans-unit id="661a797fdd966423deb47c56a841ce6d565626a3" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="b873bdb21fd500fbd503a0fa2ae31f161f0a0486" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8b2c6a8315871636ff748acb6e7ca0ce33cac24a" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8c6bb577e26b7162d7d6c68e8046a3fec50e1ed2" translate="yes" xml:space="preserve">
          <source>Create a qat module from a float module or qparams_dict</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 qat 모듈 만들기</target>
        </trans-unit>
        <trans-unit id="608ebd71f80f5c1a22f515709f259847e91c3798" translate="yes" xml:space="preserve">
          <source>Create a quantized module from a float module or qparams_dict</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 양자화 된 모듈 만들기</target>
        </trans-unit>
        <trans-unit id="25224bf405304e1ab57eac3f4ba2017fad81ac0a" translate="yes" xml:space="preserve">
          <source>Create a symbolic function named &lt;code&gt;symbolic&lt;/code&gt; in the corresponding Function class.</source>
          <target state="translated">해당 Function 클래스에 &lt;code&gt;symbolic&lt;/code&gt; 이라는 기호 함수를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="05e63a314a968dccb93df86ac820277d35c3b513" translate="yes" xml:space="preserve">
          <source>Create a view of an existing &lt;code&gt;torch.Tensor&lt;/code&gt;&lt;code&gt;input&lt;/code&gt; with specified &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;storage_offset&lt;/code&gt;.</source>
          <target state="translated">지정된 &lt;code&gt;size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; 및 &lt;code&gt;storage_offset&lt;/code&gt; 로 기존 &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;code&gt;input&lt;/code&gt; 의보기를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="b77a1bd9ce30851f58a1b21fe4ac8853dde6d2a8" translate="yes" xml:space="preserve">
          <source>Create special chart by collecting charts tags in &amp;lsquo;scalars&amp;rsquo;. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.</source>
          <target state="translated">'스칼라'에서 차트 태그를 수집하여 특별한 차트를 만듭니다. 이 함수는 각 SummaryWriter () 개체에 대해 한 번만 호출 할 수 있습니다. 텐서 보드에 메타 데이터 만 제공하므로 훈련 루프 전후에 함수를 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ad7ae06b82cbf41685a6d0a8499d7c2be99cffa5" translate="yes" xml:space="preserve">
          <source>Create the histogram of the incoming inputs.</source>
          <target state="translated">들어오는 입력의 히스토그램을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="a4594b741f646db69f78e37c46423c0ff117e55e" translate="yes" xml:space="preserve">
          <source>Creates Embedding instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">지정된 2 차원 FloatTensor에서 Embedding 인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="a6e25ee28f09c0ec7810655de85d0843eaddac29" translate="yes" xml:space="preserve">
          <source>Creates EmbeddingBag instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">지정된 2 차원 FloatTensor에서 EmbeddingBag 인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7ab70bdf8d978808ae2341155ab591efacf83cbe" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; 에서 &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="380a5b530b50dcf602f5ae20b20833a1876b83b8" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; 에서 &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="8ff7109193ed0423f307bf24dbf6b88c15e440c1" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;SummaryWriter&lt;/code&gt; that will write out events and summaries to the event file.</source>
          <target state="translated">이벤트 및 요약을 이벤트 파일에 기록 하는 &lt;code&gt;SummaryWriter&lt;/code&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="ba6567ea492c95adff11560e78e65d6f2fa6dfe4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for C++.</source>
          <target state="translated">C ++ 용 &lt;code&gt;setuptools.Extension&lt;/code&gt; 을 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="b766aaed745dd6ce4f2529c6188fca66de086dd4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for CUDA/C++.</source>
          <target state="translated">CUDA / C ++ 용 &lt;code&gt;setuptools.Extension&lt;/code&gt; 을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="f3354489ddc8254e60320b0bc37db8d7b82c9d9c" translate="yes" xml:space="preserve">
          <source>Creates a Bernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">에 의해 파라미터 베르누이 분포를 작성 &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; 을&lt;/a&gt; (하지만 모두).</target>
        </trans-unit>
        <trans-unit id="58f0d733886ad515c8d42aceb08240a1f3daf7d2" translate="yes" xml:space="preserve">
          <source>Creates a Binomial distribution parameterized by &lt;code&gt;total_count&lt;/code&gt; and either &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). &lt;code&gt;total_count&lt;/code&gt; must be broadcastable with &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;/&lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;total_count&lt;/code&gt; 와 &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt; (둘다는 아님)로 매개 변수화 된 이항 분포를 생성합니다 . &lt;code&gt;total_count&lt;/code&gt; 는 &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; / &lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt; 로 브로드 캐스팅 가능해야합니다 .</target>
        </trans-unit>
        <trans-unit id="719b609f965a9077ede6ab6190b76d01527929f0" translate="yes" xml:space="preserve">
          <source>Creates a Chi2 distribution parameterized by shape parameter &lt;a href=&quot;#torch.distributions.chi2.Chi2.df&quot;&gt;&lt;code&gt;df&lt;/code&gt;&lt;/a&gt;. This is exactly equivalent to &lt;code&gt;Gamma(alpha=0.5*df, beta=0.5)&lt;/code&gt;</source>
          <target state="translated">모양 모수 &lt;a href=&quot;#torch.distributions.chi2.Chi2.df&quot;&gt; &lt;code&gt;df&lt;/code&gt; &lt;/a&gt; 로 모수화 된 Chi2 분포를 생성합니다 . 이것은 &lt;code&gt;Gamma(alpha=0.5*df, beta=0.5)&lt;/code&gt; 와 정확히 동일합니다.</target>
        </trans-unit>
        <trans-unit id="1aa6f05fdf77d536ae15b48713329d9043bf42ff" translate="yes" xml:space="preserve">
          <source>Creates a Dirichlet distribution parameterized by concentration &lt;code&gt;concentration&lt;/code&gt;.</source>
          <target state="translated">농도 &lt;code&gt;concentration&lt;/code&gt; 매개 변수화 된 Dirichlet 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="9d283176d9c3cbc5838d89065f1a04eb5cd11ce7" translate="yes" xml:space="preserve">
          <source>Creates a Exponential distribution parameterized by &lt;code&gt;rate&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;rate&lt;/code&gt; 로 매개 변수화 된 지수 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="99602cffe1f166e20c0a8e58347d9e329ce816ad" translate="yes" xml:space="preserve">
          <source>Creates a Fisher-Snedecor distribution parameterized by &lt;code&gt;df1&lt;/code&gt; and &lt;code&gt;df2&lt;/code&gt;.</source>
          <target state="translated">에 의해 파라미터 피셔 - Snedecor 분포를 작성 &lt;code&gt;df1&lt;/code&gt; 및 &lt;code&gt;df2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d042259ebf83e1f39612d0d46ecb014811b94bfd" translate="yes" xml:space="preserve">
          <source>Creates a Gamma distribution parameterized by shape &lt;code&gt;concentration&lt;/code&gt; and &lt;code&gt;rate&lt;/code&gt;.</source>
          <target state="translated">모양 &lt;code&gt;concentration&lt;/code&gt; 및 &lt;code&gt;rate&lt;/code&gt; 매개 변수화 된 감마 분포를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="9fcc365d0f990b9958aa880a339a219e17e17273" translate="yes" xml:space="preserve">
          <source>Creates a Geometric distribution parameterized by &lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;, where &lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is the probability of success of Bernoulli trials. It represents the probability that in</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 로 모수화 된 기하 분포를 생성합니다 . 여기서 &lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 는 Bernoulli 시행의 성공 확률입니다. 그것은 확률을 나타냅니다</target>
        </trans-unit>
        <trans-unit id="f986bba11240572671f561e08bcb8f571d37c585" translate="yes" xml:space="preserve">
          <source>Creates a Laplace distribution parameterized by &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;loc&lt;/code&gt; 및 &lt;code&gt;scale&lt;/code&gt; 로 매개 변수화 된 Laplace 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="4adb5e5fdc76653b710744984dd086c968ee39fd" translate="yes" xml:space="preserve">
          <source>Creates a LogitRelaxedBernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both), which is the logit of a RelaxedBernoulli distribution.</source>
          <target state="translated">매개 변수화 LogitRelaxedBernoulli 분포 생성 &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt; RelaxedBernoulli 분포의 로짓이다 (그러나 모두).</target>
        </trans-unit>
        <trans-unit id="15a0c995e1894ed35a375aec8d89951825601652" translate="yes" xml:space="preserve">
          <source>Creates a Multinomial distribution parameterized by &lt;code&gt;total_count&lt;/code&gt; and either &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). The innermost dimension of &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; indexes over categories. All other dimensions index over batches.</source>
          <target state="translated">&lt;code&gt;total_count&lt;/code&gt; 및 &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt; (둘다는 아님)로 매개 변수화 된 다항 분포를 생성합니다 . &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 의 가장 안쪽 차원은 범주에 대한 색인을 생성합니다. 다른 모든 차원은 배치에 대해 인덱싱됩니다.</target>
        </trans-unit>
        <trans-unit id="e3924f533e522b7cb260639c31a007a0e461a496" translate="yes" xml:space="preserve">
          <source>Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before &lt;code&gt;total_count&lt;/code&gt; failures are achieved. The probability of success of each Bernoulli trial is &lt;a href=&quot;#torch.distributions.negative_binomial.NegativeBinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">음 이항 분포, 즉 &lt;code&gt;total_count&lt;/code&gt; 실패가 달성 되기 전에 성공한 독립적이고 동일한 Bernoulli 시행 횟수의 분포를 생성합니다 . 각 Bernoulli 시행의 성공 확률은 &lt;a href=&quot;#torch.distributions.negative_binomial.NegativeBinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="500755127f28d6a568f776f9aef009f0a35b4dde" translate="yes" xml:space="preserve">
          <source>Creates a Poisson distribution parameterized by &lt;code&gt;rate&lt;/code&gt;, the rate parameter.</source>
          <target state="translated">rate 모수 인 &lt;code&gt;rate&lt;/code&gt; 로 모수화 된 포아송 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="a83a7cc2ac442a11cbf1dc1e134a50d3c260f42e" translate="yes" xml:space="preserve">
          <source>Creates a RelaxedBernoulli distribution, parametrized by &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature&quot;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/a&gt;, and either &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). This is a relaxed version of the &lt;code&gt;Bernoulli&lt;/code&gt; distribution, so the values are in (0, 1), and has reparametrizable samples.</source>
          <target state="translated">에 의해 매개 변수화 RelaxedBernoulli 분포 생성 &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature&quot;&gt; &lt;code&gt;temperature&lt;/code&gt; &lt;/a&gt; 및 하나 &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt; (그러나 모두). 이것은 &lt;code&gt;Bernoulli&lt;/code&gt; 분포 의 완화 된 버전 이므로 값은 (0, 1)에 있으며 다시 매개 변수화 할 수있는 샘플을가집니다.</target>
        </trans-unit>
        <trans-unit id="f1a742732148726c845a736e718e0b42ae326d5a" translate="yes" xml:space="preserve">
          <source>Creates a RelaxedOneHotCategorical distribution parametrized by &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature&quot;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/a&gt;, and either &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;. This is a relaxed version of the &lt;code&gt;OneHotCategorical&lt;/code&gt; distribution, so its samples are on simplex, and are reparametrizable.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature&quot;&gt; &lt;code&gt;temperature&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; 로&lt;/a&gt; 매개 변수화 된 RelaxedOneHotCategorical 분포를 만듭니다 . 이것은 &lt;code&gt;OneHotCategorical&lt;/code&gt; 분포 의 완화 된 버전 이므로 해당 샘플은 단순하고 다시 매개 변수화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7c20c969a8d2e6e99e36b8641b0c9149ce07f5be" translate="yes" xml:space="preserve">
          <source>Creates a Student&amp;rsquo;s t-distribution parameterized by degree of freedom &lt;code&gt;df&lt;/code&gt;, mean &lt;code&gt;loc&lt;/code&gt; and scale &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">학생의 t 분포는 자유의 정도에 의해 파라미터 작성 &lt;code&gt;df&lt;/code&gt; , 평균 &lt;code&gt;loc&lt;/code&gt; 규모의 &lt;code&gt;scale&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="956c67cb0f1c70fdb976dabf9f583b2950425b18" translate="yes" xml:space="preserve">
          <source>Creates a categorical distribution parameterized by either &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.categorical.Categorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">중 하나에 의해 파라미터 범주 형 분포를 작성 &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.categorical.Categorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; 을&lt;/a&gt; (하지만 모두).</target>
        </trans-unit>
        <trans-unit id="20a8f67a767e2c70110f0eacd788bdca79dafc4f" translate="yes" xml:space="preserve">
          <source>Creates a continuous Bernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">에 의해 파라미터 연속 베르누이 분포를 작성 &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; 을&lt;/a&gt; (하지만 모두).</target>
        </trans-unit>
        <trans-unit id="0ccb6b54dc9399c2c85ab56d56ee522676eba886" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</source>
          <target state="translated">대상과 출력 사이의 이진 교차 엔트로피를 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="3d34756a1f2c85c7f54f427b38b59a7f138b4091" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given input tensors</source>
          <target state="translated">주어진 입력 텐서의 손실을 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="b30adb8efde8fc483debf3f678fe0a50c0f3a700" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given inputs</source>
          <target state="translated">주어진 입력 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="db68393122ce4917cd40e5341fb476ef56ce47be" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean absolute error (MAE) between each element in the input</source>
          <target state="translated">입력의 각 요소 간의 평균 절대 오차 (MAE)를 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="dc0bceb74569c8f166528464397135db651de1e9" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input</source>
          <target state="translated">입력의 각 요소 사이의 평균 제곱 오차 (제곱 L2 표준)를 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c567813437a56bca1c0ee05ac4bbbc7a9a0f0b86" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given an input tensors</source>
          <target state="translated">입력 텐서가 주어지면 삼중 선 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="54c0c219984f4a50b8cfc1212d879c396ddab80a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given input tensors</source>
          <target state="translated">주어진 입력 텐서의 삼중 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="3ab7e1fd5db78a2e9eadeb7c1fe0ccb243aebcd4" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input</source>
          <target state="translated">입력 간 다중 클래스 분류 힌지 손실 (마진 기반 손실)을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="0f6e83829c9a9db048daa12ae2ef6905382cf541" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input</source>
          <target state="translated">입력 간 다중 클래스 다중 분류 힌지 손실 (마진 기반 손실)을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="74a30cc1dfda2e2ca6b72fcb42e123c4afa11917" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input</source>
          <target state="translated">입력 간 최대 엔트로피를 기반으로 다중 레이블 일대일 손실을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="38862788d574ffd77ba587c5570ecdb676e04e5f" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a two-class classification logistic loss between input tensor</source>
          <target state="translated">입력 텐서 간의 2 클래스 분류 로지스틱 손실을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d4196579876a20368dab571bd9df29f488cfb987" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">요소 별 절대 오차가 베타 이하로 떨어지면 제곱항을 사용하고 그렇지 않으면 L1 항을 사용하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="416d9c5bdd3061ae8e8917cecda26e9fe1f6412a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the &lt;code&gt;MSELoss&lt;/code&gt; and in some cases prevents exploding gradients (e.g. see &lt;code&gt;Fast R-CNN&lt;/code&gt; paper by Ross Girshick). Also known as the Huber loss:</source>
          <target state="translated">요소 별 절대 오차가 베타 이하로 떨어지면 제곱항을 사용하고 그렇지 않으면 L1 항을 사용하는 기준을 만듭니다. &lt;code&gt;MSELoss&lt;/code&gt; 보다 특이 치에 덜 민감하며 경우에 따라 급격한 기울기를 방지합니다 (예 : Ross Girshick의 &lt;code&gt;Fast R-CNN&lt;/code&gt; 논문 참조 ). Huber 손실이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="1f5f49121121fa1e6abb3edd3d6d453965003b41" translate="yes" xml:space="preserve">
          <source>Creates a half-Cauchy distribution parameterized by &lt;code&gt;scale&lt;/code&gt; where:</source>
          <target state="translated">다음과 같은 경우 &lt;code&gt;scale&lt;/code&gt; 별로 모수화 된 반 조심스러운 분포를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="4deb8f0f5867e5b62231478b068c257636c7c149" translate="yes" xml:space="preserve">
          <source>Creates a half-normal distribution parameterized by &lt;code&gt;scale&lt;/code&gt; where:</source>
          <target state="translated">&lt;code&gt;scale&lt;/code&gt; 로 모수화 된 반 정규 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="7674fe00de59675dc7589881d3e4f40957fcf6b1" translate="yes" xml:space="preserve">
          <source>Creates a log-normal distribution parameterized by &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.loc&quot;&gt;&lt;code&gt;loc&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; where:</source>
          <target state="translated">에 의해 파라미터 로그 - 정규 분포 작성 &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.loc&quot;&gt; &lt;code&gt;loc&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="abf49ecc79a512b11706abef3bf65667a788d341" translate="yes" xml:space="preserve">
          <source>Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.</source>
          <target state="translated">평균 벡터와 공분산 행렬로 매개 변수화 된 다변량 정규 분포 (가우스라고도 함)를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="3e6e658a2c1f4f9bb64225bec1cd132cf3dc92f9" translate="yes" xml:space="preserve">
          <source>Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by &lt;code&gt;cov_factor&lt;/code&gt; and &lt;code&gt;cov_diag&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;cov_factor&lt;/code&gt; 및 &lt;code&gt;cov_diag&lt;/code&gt; 로 매개 변수화 된 낮은 순위 형식의 공분산 행렬을 사용하여 다변량 정규 분포를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="1ebf98c1eaeed448f01f072825db935178bbb990" translate="yes" xml:space="preserve">
          <source>Creates a new distributed group.</source>
          <target state="translated">새 분산 그룹을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="cb830598128131448f4401b8c4fa79f95497434a" translate="yes" xml:space="preserve">
          <source>Creates a normal (also called Gaussian) distribution parameterized by &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;loc&lt;/code&gt; 및 &lt;code&gt;scale&lt;/code&gt; 로 매개 변수화 된 정규 분포 (가우스라고도 함)를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="073fb44c832f84ccfa42ff94accd75b4dd72e7da" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from</source>
          <target state="translated">값의 간격이 균등 한 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="350152525d5dacc5decf599c92018f7fa5823d28" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive.</source>
          <target state="translated">값이 &lt;code&gt;start&lt;/code&gt; 부터 &lt;code&gt;end&lt;/code&gt; 균등 한 간격 ( 포함) 인 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="9f41c69b98f8c4bce082aab055bcac8455947a3e" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive. That is, the value are:</source>
          <target state="translated">값이 &lt;code&gt;start&lt;/code&gt; 부터 &lt;code&gt;end&lt;/code&gt; 균등 한 간격 ( 포함) 인 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다 . 즉, 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c05b957d45d9e4908a05a11e87943ceb8347e92e" translate="yes" xml:space="preserve">
          <source>Creates a one-hot categorical distribution parameterized by &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; 로&lt;/a&gt; 매개 변수화 된 원-핫 범주 형 분포를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="9962e59b2ab133d8c3b2267ae38553f28f8a79bd" translate="yes" xml:space="preserve">
          <source>Creates a quantized module from a float module or qparams_dict.</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 양자화 된 모듈을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="d2147eb4da2cad834986bf746aeef9a006067d0f" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; 로 채워진 크기 &lt;code&gt;size&lt;/code&gt; 의 텐서를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="ed2b3c6c1dcd8ed54d69945a6e4acc25f855c267" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. The tensor&amp;rsquo;s dtype is inferred from &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; 로 채워진 크기 &lt;code&gt;size&lt;/code&gt; 의 텐서를 만듭니다 . 텐서의 dtype은 &lt;code&gt;fill_value&lt;/code&gt; 에서 유추됩니다 .</target>
        </trans-unit>
        <trans-unit id="12f064b3e975b58c920e1056343b0cd452f5ecab" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">누구 (의해 지정된 특정 2D 평면의 대각선 텐서 작성 &lt;code&gt;dim1&lt;/code&gt; 및 &lt;code&gt;dim2&lt;/code&gt; )로 채워진다 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a4e06d7745c58e373370f1d2833461757426df92" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.</source>
          <target state="translated">누구 (의해 지정된 특정 2D 평면의 대각선 텐서 작성 &lt;code&gt;dim1&lt;/code&gt; 및 &lt;code&gt;dim2&lt;/code&gt; )로 채워진다 &lt;code&gt;input&lt;/code&gt; . 배치 된 대각 행렬을 쉽게 생성하기 위해 반환 된 텐서의 마지막 2 차원으로 형성된 2D 평면이 기본적으로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="54d06cd8eb770a4665e06b480e41c18b35882c33" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution.</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 실행하고이 실행 결과 값에 대한 참조를 실행하는 비동기 작업을 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="a528098d6266957ce08cc5436471c4e687e167c2" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution. &lt;code&gt;fork&lt;/code&gt; will return immediately, so the return value of &lt;code&gt;func&lt;/code&gt; may not have been computed yet. To force completion of the task and access the return value invoke &lt;code&gt;torch.jit.wait&lt;/code&gt; on the Future. &lt;code&gt;fork&lt;/code&gt; invoked with a &lt;code&gt;func&lt;/code&gt; which returns &lt;code&gt;T&lt;/code&gt; is typed as &lt;code&gt;torch.jit.Future[T]&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, &lt;code&gt;fork&lt;/code&gt; will not execute in parallel. &lt;code&gt;fork&lt;/code&gt; will also not execute in parallel when invoked while tracing, however the &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;wait&lt;/code&gt; calls will be captured in the exported IR Graph. .. warning:</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 실행하고이 실행 결과 값에 대한 참조를 실행하는 비동기 작업을 만듭니다 . &lt;code&gt;fork&lt;/code&gt; 는 즉시 반환되므로 &lt;code&gt;func&lt;/code&gt; 의 반환 값은 아직 계산되지 않았을 수 있습니다. 작업을 강제로 완료하고 반환 값에 액세스 하려면 Future에서 &lt;code&gt;torch.jit.wait&lt;/code&gt; 를 호출 합니다. &lt;code&gt;T&lt;/code&gt; 를 반환 하는 &lt;code&gt;func&lt;/code&gt; 로 호출 된 &lt;code&gt;fork&lt;/code&gt; 는 &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 형식으로 지정됩니다 . &lt;code&gt;fork&lt;/code&gt; 호출은 임의로 중첩 될 수 있으며 위치 및 키워드 인수로 호출 될 수 있습니다. 비동기 실행은 TorchScript에서 실행될 때만 발생합니다. 순수 파이썬에서 실행하면 &lt;code&gt;fork&lt;/code&gt; 가 병렬로 실행되지 않습니다. &lt;code&gt;fork&lt;/code&gt; 는 추적하는 동안 호출 될 때 병렬로 실행되지 않지만 &lt;code&gt;fork&lt;/code&gt; 및 &lt;code&gt;wait&lt;/code&gt; 호출은 내 보낸 IR 그래프에 캡처됩니다. .. 경고 :</target>
        </trans-unit>
        <trans-unit id="5c73fcf2c3b7a88652c30df05d4a49be7a9a3a82" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</source>
          <target state="translated">의사 난수를 생성하는 알고리즘의 상태를 관리하는 생성기 객체를 만들고 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7bf69af350942e6f19285c4071e0d9bc1d0390fb" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; functions.</source>
          <target state="translated">의사 난수를 생성하는 알고리즘의 상태를 관리하는 생성기 객체를 만들고 반환합니다. 많은 &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;내부 임의 샘플링&lt;/a&gt; 함수 에서 키워드 인수로 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="6db27514dc4a87b77455deb1f68ac2812f51084a" translate="yes" xml:space="preserve">
          <source>Creating TorchScript Code</source>
          <target state="translated">TorchScript 코드 생성</target>
        </trans-unit>
        <trans-unit id="14dc9f6dca53e0df6aaf4e5eb53a7efa6593aab3" translate="yes" xml:space="preserve">
          <source>Creating named tensors</source>
          <target state="translated">명명 된 텐서 만들기</target>
        </trans-unit>
        <trans-unit id="733cbb78a0d286a0935c6c571466400a11828907" translate="yes" xml:space="preserve">
          <source>Creation Ops</source>
          <target state="translated">창조 작전</target>
        </trans-unit>
        <trans-unit id="d66fef2dd7f330ad8c7b2de8229080af12ec8719" translate="yes" xml:space="preserve">
          <source>Creation of this class requires that &lt;code&gt;torch.distributed&lt;/code&gt; to be already initialized, by calling &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 클래스의 창조해야 &lt;code&gt;torch.distributed&lt;/code&gt; 호출하여 이미 초기화 할 &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711f625ceb8a65f44028c4e7b7e4e818fc7445a7" translate="yes" xml:space="preserve">
          <source>CrossEntropyLoss</source>
          <target state="translated">CrossEntropyLoss</target>
        </trans-unit>
        <trans-unit id="5eec0b5e11526f784758a0afbe48baade8e71e9e" translate="yes" xml:space="preserve">
          <source>Current implementation of &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</source>
          <target state="translated">현재 &lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 구현은 메모리 오버 헤드를 도입하므로 작은 텐서가 많은 애플리케이션에서 예기치 않게 높은 메모리 사용량이 발생할 수 있습니다. 이 경우 하나의 큰 구조를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="96c1a2df528a935c7c808490ba7a2921d055775f" translate="yes" xml:space="preserve">
          <source>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">현재 구현은 모든 호출에 가중치를 부여하므로 성능이 저하됩니다. 오버 헤드를 피하려면 &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;Linear&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="392192a4ae05351b1f7b21e538ff3e8ca534e67d" translate="yes" xml:space="preserve">
          <source>Currently &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; only supports &lt;code&gt;DistributedDataParallel&lt;/code&gt; (DDP) with single GPU per process. Use &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt;&lt;/a&gt; to convert &lt;code&gt;BatchNorm*D&lt;/code&gt; layer to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; before wrapping Network with DDP.</source>
          <target state="translated">현재 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 은 프로세스 당 단일 GPU가있는 DDP ( &lt;code&gt;DistributedDataParallel&lt;/code&gt; ) 만 지원합니다 . 네트워크를 DDP로 래핑하기 전에 &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt; &lt;/a&gt; 을 사용 하여 &lt;code&gt;BatchNorm*D&lt;/code&gt; 레이어를 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 으로 변환 합니다.</target>
        </trans-unit>
        <trans-unit id="a78ac4f002658abaaa21040baac2a600bf544edc" translate="yes" xml:space="preserve">
          <source>Currently in the CUDA implementation and the CPU implementation when dim is specified, &lt;code&gt;torch.unique&lt;/code&gt; always sort the tensor at the beginning regardless of the &lt;code&gt;sort&lt;/code&gt; argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; which avoids the sorting.</source>
          <target state="translated">현재 CUDA 구현 및 dim이 지정된 CPU 구현에서 &lt;code&gt;torch.unique&lt;/code&gt; 는 &lt;code&gt;sort&lt;/code&gt; 인수에 관계없이 항상 처음에 텐서를 정렬합니다 . 정렬이 느릴 수 있으므로 입력 텐서가 이미 정렬 된 경우 정렬을 피하는 &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt; 를 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="c8a61a0d816f8b21bf7f4b46f888c832b03676e2" translate="yes" xml:space="preserve">
          <source>Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).</source>
          <target state="translated">현재 공간 및 체적 업 샘플링이 지원됩니다 (예 : 예상 입력은 4 차원 또는 5 차원).</target>
        </trans-unit>
        <trans-unit id="8a9d7a3e06793c263711ce33ba0ffdc49d409027" translate="yes" xml:space="preserve">
          <source>Currently supported operations and subsystems</source>
          <target state="translated">현재 지원되는 작업 및 하위 시스템</target>
        </trans-unit>
        <trans-unit id="3b05bf74a2646cf7882607d1b6e8f3921b3f2124" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">현재 시간적, 공간적 및 체적 샘플링이 지원됩니다. 즉, 예상 입력은 모양이 3D, 4D 또는 5D입니다.</target>
        </trans-unit>
        <trans-unit id="af454dd72bdbaa9839806374d040bd77db038329" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">현재 시간적, 공간적 및 체적 업 샘플링이 지원됩니다. 즉, 예상 입력은 모양이 3D, 4D 또는 5D입니다.</target>
        </trans-unit>
        <trans-unit id="d49c5e3f020c5093dc8a1df7be3b5947455c7de6" translate="yes" xml:space="preserve">
          <source>Currently three initialization methods are supported:</source>
          <target state="translated">현재 세 가지 초기화 방법이 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="ecab6b4c3bf958fdadbd30836155137da6ef98e2" translate="yes" xml:space="preserve">
          <source>Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128</source>
          <target state="translated">현재 유효한 스칼라 및 텐서 조합은 1. 부동 dtype 및 torch.double의 스칼라 2. 정수 dtype 및 torch.long의 스칼라 3. 복합 dtype 및 torch.complex128의 스칼라</target>
        </trans-unit>
        <trans-unit id="0cf55afa31790d6a153e9124662c9ad715c264be" translate="yes" xml:space="preserve">
          <source>Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.</source>
          <target state="translated">현재 3D 출력 텐서 (펼쳐진 배치 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="cf7422cb9205f4ada47421ece107faeeaf3ef476" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D input tensors (batched image-like tensors) are supported.</source>
          <target state="translated">현재는 4 차원 입력 텐서 (일괄 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="804337292e10b528ce9da39728466ea21dd2d93d" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D output tensors (batched image-like tensors) are supported.</source>
          <target state="translated">현재는 4D 출력 텐서 (일괄 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="55e4dc48ed51c13b29c9e279e5505c0a895f1cb1" translate="yes" xml:space="preserve">
          <source>Currently, only spatial (4-D) and volumetric (5-D) &lt;code&gt;input&lt;/code&gt; are supported.</source>
          <target state="translated">현재 공간 (4-D) 및 체적 (5-D) &lt;code&gt;input&lt;/code&gt; 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="16e0def02ff49d292a9c6c0ee9ab56bbab51c7e1" translate="yes" xml:space="preserve">
          <source>Custom averaging strategies</source>
          <target state="translated">맞춤형 평균화 전략</target>
        </trans-unit>
        <trans-unit id="b674ceb20a7250912a1c130e2165e5889d62f4ab" translate="yes" xml:space="preserve">
          <source>Custom operators</source>
          <target state="translated">맞춤 연산자</target>
        </trans-unit>
        <trans-unit id="3461dd173d30fce828765c425b88ef7516427e68" translate="yes" xml:space="preserve">
          <source>CustomFromMask</source>
          <target state="translated">CustomFromMask</target>
        </trans-unit>
        <trans-unit id="e93172f8decb4286fa3d5ed1a6b28a8e5bb1dd99" translate="yes" xml:space="preserve">
          <source>Cyclical learning rate policy changes the learning rate after every batch. &lt;code&gt;step&lt;/code&gt; should be called after a batch has been used for training.</source>
          <target state="translated">주기적 학습률 정책은 매 배치 후 학습률을 변경합니다. 학습에 배치를 사용한 후에 &lt;code&gt;step&lt;/code&gt; 를 호출해야합니다.</target>
        </trans-unit>
        <trans-unit id="50c9e8d5fc98727b4bbc93cf5d64a68db647f04f" translate="yes" xml:space="preserve">
          <source>D</source>
          <target state="translated">D</target>
        </trans-unit>
        <trans-unit id="eb3635e47f534f46cf856ff7cd245f64d37e6d18" translate="yes" xml:space="preserve">
          <source>DCGAN</source>
          <target state="translated">DCGAN</target>
        </trans-unit>
        <trans-unit id="17ba820b5bcd55a78cfd53d227f66dc3adcb9245" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride [0]}-2 \ times \ text {padding [0]} + \ text {kernel \ _size [0]}</target>
        </trans-unit>
        <trans-unit id="11e96ca0ef0a7d1c0968f6c0aa661473e65963b1" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride} [0]-2 \ times \ text {padding} [0] + \ text {dilation} [0] \ times (\ text { 커널 \ _size} [0]-1) + \ text {output \ _padding} [0] + 1</target>
        </trans-unit>
        <trans-unit id="c8db0974bd119b8e244f7c9a0fbb5cd60340ca9b" translate="yes" xml:space="preserve">
          <source>D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}</source>
          <target state="translated">D_ {out} = D_ {in} + \ text {padding \ _front} + \ text {padding \ _back}</target>
        </trans-unit>
        <trans-unit id="343f5280293229b193ab7d6cc2f6fecb4a2a501b" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor D_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="5daaf33316a5a1e996b64f9e865fa2c06a216412" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {dilation} [0] \ times (\ text {kernel \ _size} [0] -1)-1} {\ text {stride} [0]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="a58eeddbe8ce176e982a4bd5546958165921baf4" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {kernel \ _size} [0]} {\ text {stride} [0]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="58d17582a05d81dbc970a2fb5dde4c86c385cf6c" translate="yes" xml:space="preserve">
          <source>Danger</source>
          <target state="translated">Danger</target>
        </trans-unit>
        <trans-unit id="543cf269e3398d54432632170f9d8a44ec8a82d9" translate="yes" xml:space="preserve">
          <source>Data Loading Order and Sampler</source>
          <target state="translated">데이터로드 순서 및 샘플러</target>
        </trans-unit>
        <trans-unit id="faa811b7a6757fc89dec83a4445037d0b6d29fe4" translate="yes" xml:space="preserve">
          <source>Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.</source>
          <target state="translated">데이터 로더. 데이터 세트와 샘플러를 결합하고 주어진 데이터 세트에 대한 반복 가능 항목을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ee503fe5765b8d9456bf21def7f9e06d2b12ebb6" translate="yes" xml:space="preserve">
          <source>Data type</source>
          <target state="translated">데이터 형식</target>
        </trans-unit>
        <trans-unit id="5e3a2e3c18839bb47fb3084db81d051ea2d9e572" translate="yes" xml:space="preserve">
          <source>DataParallel</source>
          <target state="translated">DataParallel</target>
        </trans-unit>
        <trans-unit id="f41afa14a2956bd6dee2529d81450b9fde84c7bb" translate="yes" xml:space="preserve">
          <source>DataParallel Layers (multi-GPU, distributed)</source>
          <target state="translated">DataParallel 레이어 (다중 GPU, 분산)</target>
        </trans-unit>
        <trans-unit id="2c2866ebc153b9ba5ee12510871be4ea8e2d66ad" translate="yes" xml:space="preserve">
          <source>DataParallel functions (multi-GPU, distributed)</source>
          <target state="translated">DataParallel 함수 (다중 GPU, 분산)</target>
        </trans-unit>
        <trans-unit id="6f795ae6a697849179693a4d0d952f51dfe274a9" translate="yes" xml:space="preserve">
          <source>Dataset Types</source>
          <target state="translated">데이터 세트 유형</target>
        </trans-unit>
        <trans-unit id="a759fa5cf358fa4e839fa020614e6a71debce953" translate="yes" xml:space="preserve">
          <source>Dataset as a concatenation of multiple datasets.</source>
          <target state="translated">여러 데이터 세트를 연결 한 데이터 세트입니다.</target>
        </trans-unit>
        <trans-unit id="3eac6174468e8a6ce464f86a0d629040e97fe409" translate="yes" xml:space="preserve">
          <source>Dataset for chainning multiple &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; s.</source>
          <target state="translated">여러 &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt; 을 연결하기위한 데이터 세트입니다 .</target>
        </trans-unit>
        <trans-unit id="63231ef80d6022f27be0ddfb59b1c8c16d68db48" translate="yes" xml:space="preserve">
          <source>Dataset is assumed to be of constant size.</source>
          <target state="translated">데이터 세트는 일정한 크기로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="6935d46588944d8c25616ca6972db5257d686341" translate="yes" xml:space="preserve">
          <source>Dataset wrapping tensors.</source>
          <target state="translated">텐서를 래핑하는 데이터 세트.</target>
        </trans-unit>
        <trans-unit id="75c05e642a4a82474f0ded1d73a7d77a7cab17de" translate="yes" xml:space="preserve">
          <source>DeQuantize</source>
          <target state="translated">DeQuantize</target>
        </trans-unit>
        <trans-unit id="895b27c88016513d278a0ce3dc0663fae3829d58" translate="yes" xml:space="preserve">
          <source>Debugging</source>
          <target state="translated">Debugging</target>
        </trans-unit>
        <trans-unit id="2d9ba523f6bd6f1366ef31447c6896de739c5ebc" translate="yes" xml:space="preserve">
          <source>Debugging this script with &lt;code&gt;pdb&lt;/code&gt; works except for when we invoke the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function. We can globally disable JIT, so that we can call the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function and not compile it. If the above script is called &lt;code&gt;disable_jit_example.py&lt;/code&gt;, we can invoke it like so:</source>
          <target state="translated">&lt;code&gt;pdb&lt;/code&gt; 로이 스크립트를 디버깅 하는 것은 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수를 호출하는 경우를 제외하고 작동 합니다. JIT를 전역 적으로 비활성화하여 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수를 일반적인 Python 함수로 호출하고 컴파일하지 않을 수 있습니다. 위의 스크립트가 &lt;code&gt;disable_jit_example.py&lt;/code&gt; 인 경우 다음과 같이 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="20487077b8cc9537e212490b180a9c77a92fedaf" translate="yes" xml:space="preserve">
          <source>Debugging utilities</source>
          <target state="translated">디버깅 유틸리티</target>
        </trans-unit>
        <trans-unit id="cdeed230ed1d975e42532ba4eb68e61538b14c37" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">Epoch마다 감마로 각 매개 변수 그룹의 학습률을 감소시킵니다. last_epoch = -1이면 초기 lr을 lr로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="68df60584336645f5d49c32e82cf477c01c234ca" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">step_size epoch마다 감마로 각 파라미터 그룹의 학습률을 감소시킵니다. 이러한 감쇠는이 스케줄러 외부에서 학습률에 대한 다른 변경과 동시에 발생할 수 있습니다. last_epoch = -1이면 초기 lr을 lr로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="65627d1ff3ccfff17f931ea2b13d4278cbbe2474" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">에포크 수가 마일스톤 중 하나에 도달하면 감마에 의해 각 매개 변수 그룹의 학습률이 감소합니다. 이러한 감쇠는이 스케줄러 외부에서 학습률에 대한 다른 변경과 동시에 발생할 수 있습니다. last_epoch = -1이면 초기 lr을 lr로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="9f7b680b643589b9ed8668bf524f99287df6399c" translate="yes" xml:space="preserve">
          <source>Decodes a DLPack to a tensor.</source>
          <target state="translated">DLPack을 텐서로 디코딩합니다.</target>
        </trans-unit>
        <trans-unit id="dceee5b011b77632a7ffb29ac384f66f1112b455" translate="yes" xml:space="preserve">
          <source>Decorator to register a pairwise function with &lt;a href=&quot;#torch.distributions.kl.kl_divergence&quot;&gt;&lt;code&gt;kl_divergence()&lt;/code&gt;&lt;/a&gt;. Usage:</source>
          <target state="translated">&lt;a href=&quot;#torch.distributions.kl.kl_divergence&quot;&gt; &lt;code&gt;kl_divergence()&lt;/code&gt; &lt;/a&gt; 로 쌍별 함수를 등록하는 데코레이터 . 용법:</target>
        </trans-unit>
        <trans-unit id="bbe3208131f8562d117c292d268d2f8db716693e" translate="yes" xml:space="preserve">
          <source>DeepLabV3</source>
          <target state="translated">DeepLabV3</target>
        </trans-unit>
        <trans-unit id="67529023df524563878c2ac2647a317ad8500070" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet101</source>
          <target state="translated">DeepLabV3 ResNet101</target>
        </trans-unit>
        <trans-unit id="617f04b5a03ab100dd0d3e0e5532b5eb50903269" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50</source>
          <target state="translated">DeepLabV3 ResNet50</target>
        </trans-unit>
        <trans-unit id="d2b8f3a731cc6350b5d6b3c7afddb1acf6006314" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50, ResNet101</source>
          <target state="translated">DeepLabV3 ResNet50, ResNet101</target>
        </trans-unit>
        <trans-unit id="8c8189f5b90e5b1272f55bc887fde7008a4bafdf" translate="yes" xml:space="preserve">
          <source>Default Types</source>
          <target state="translated">기본 유형</target>
        </trans-unit>
        <trans-unit id="2d116b23402cf7d0664bd12bb48874727780a4ac" translate="yes" xml:space="preserve">
          <source>Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset</source>
          <target state="translated">기본 평가 함수는 torch.utils.data.Dataset 또는 입력 Tensor 목록을 사용하고 데이터 세트에서 모델을 실행합니다.</target>
        </trans-unit>
        <trans-unit id="f410bf7d7a6034879436d17d9eeb5c8f8ee302f3" translate="yes" xml:space="preserve">
          <source>Default gradient layouts</source>
          <target state="translated">기본 그라디언트 레이아웃</target>
        </trans-unit>
        <trans-unit id="e51881f10b051af0af29bc3e379261ed831a6098" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (no normalization).</source>
          <target state="translated">기본값은 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (정규화 없음)입니다.</target>
        </trans-unit>
        <trans-unit id="0e0fda91e6a2713611da6dc3cb2473efcd25b651" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (normalize by &lt;code&gt;1/n&lt;/code&gt;).</source>
          <target state="translated">기본값은 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; ( &lt;code&gt;1/n&lt;/code&gt; 으로 정규화 ).</target>
        </trans-unit>
        <trans-unit id="8e4f64360906e7f1452e75f1085dc507b2bff801" translate="yes" xml:space="preserve">
          <source>Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">기본값 : &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be8bdbcf95f1b2c543f6b5f2c72d836f7c5b5704" translate="yes" xml:space="preserve">
          <source>Defaults to zero if not provided. where</source>
          <target state="translated">제공되지 않은 경우 기본값은 0입니다. 어디</target>
        </trans-unit>
        <trans-unit id="0c57e9446082424cd876b20c90b07486b438b9b9" translate="yes" xml:space="preserve">
          <source>Define the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt;, for example &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch/onnx/symbolic_opset9.py&lt;/a&gt;. Make sure the function has the same name as the ATen operator/function defined in &lt;code&gt;VariableType.h&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; 기호 함수를 정의합니다 ( 예 : &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch / onnx / symbolic_opset9.py)&lt;/a&gt; . 함수의 이름이 &lt;code&gt;VariableType.h&lt;/code&gt; 에 정의 된 ATen 연산자 / 함수와 동일한 지 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="4cec63288c88c513e15dd89936be8dc37543d14b" translate="yes" xml:space="preserve">
          <source>Defines a formula for differentiating the operation.</source>
          <target state="translated">작업을 차별화하기위한 공식을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="7773324d0fbb19d2b097125eeec23bba37fd9fae" translate="yes" xml:space="preserve">
          <source>Defines the computation performed at every call.</source>
          <target state="translated">모든 호출에서 수행되는 계산을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="9fdfda877e82e2aeb2ae17e08e28fcbd6d18a121" translate="yes" xml:space="preserve">
          <source>Deletes the key-value pair associated with &lt;code&gt;key&lt;/code&gt; from the store. Returns &lt;code&gt;true&lt;/code&gt; if the key was successfully deleted, and &lt;code&gt;false&lt;/code&gt; if it was not.</source>
          <target state="translated">저장소에서 &lt;code&gt;key&lt;/code&gt; 와 연결된 키-값 쌍을 삭제합니다 . 키가 성공적으로 삭제 된 경우 &lt;code&gt;true&lt;/code&gt; 를 반환하고 그렇지 않은 경우 &lt;code&gt;false&lt;/code&gt; 를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="fa7b4a50c0e0f2cffa01451426972ab2f487a9dd" translate="yes" xml:space="preserve">
          <source>DenseNet</source>
          <target state="translated">DenseNet</target>
        </trans-unit>
        <trans-unit id="8b2ff8d2942b70f38d615ed1e52ba52e2285edab" translate="yes" xml:space="preserve">
          <source>Densenet-121</source>
          <target state="translated">Densenet-121</target>
        </trans-unit>
        <trans-unit id="d26e787bd56b9c7aab1c534a7f7ead9ab4b12d73" translate="yes" xml:space="preserve">
          <source>Densenet-121 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-121 모델</target>
        </trans-unit>
        <trans-unit id="dc79947b23872b66c941b9c7a01bd0904908a8d9" translate="yes" xml:space="preserve">
          <source>Densenet-161</source>
          <target state="translated">Densenet-161</target>
        </trans-unit>
        <trans-unit id="8079b2939275d34b5b7b870ad8e0a1f7fe8edd0d" translate="yes" xml:space="preserve">
          <source>Densenet-161 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-161 모델</target>
        </trans-unit>
        <trans-unit id="f8e6fd5af55a790890421b436f68cbc8406a0b19" translate="yes" xml:space="preserve">
          <source>Densenet-169</source>
          <target state="translated">Densenet-169</target>
        </trans-unit>
        <trans-unit id="7480a230cf176343f7e3077f6904ede5855a19d8" translate="yes" xml:space="preserve">
          <source>Densenet-169 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-169 모델</target>
        </trans-unit>
        <trans-unit id="fc80ee1a1bf823e3bf3be2c3a3194f577392f377" translate="yes" xml:space="preserve">
          <source>Densenet-201</source>
          <target state="translated">Densenet-201</target>
        </trans-unit>
        <trans-unit id="7124ecd6d83a80564dc1ba72e45a576f118ac991" translate="yes" xml:space="preserve">
          <source>Densenet-201 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-201 모델</target>
        </trans-unit>
        <trans-unit id="7a89d9b0077197cdfbba3a66cddb06012c599011" translate="yes" xml:space="preserve">
          <source>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;, and not a full &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;. It is up to the user to add proper padding.</source>
          <target state="translated">커널의 크기에 따라 입력의 여러 열 (마지막)이 손실 될 수 있습니다. 이는 전체 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;상호 &lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;상관&lt;/a&gt; 이 아니라 유효한 상호 상관 이기 때문입니다 . 적절한 패딩을 추가하는 것은 사용자에게 달려 있습니다.</target>
        </trans-unit>
        <trans-unit id="7a0685deb3b8e4521a47904d7970aabb4610a92a" translate="yes" xml:space="preserve">
          <source>Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the &lt;code&gt;custom_opsets&lt;/code&gt; dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 custom ops&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX Runtime custom ops&lt;/a&gt;.</source>
          <target state="translated">커스텀 연산자에 따라 하나 또는 기존 ONNX 작업의 조합으로 내보낼 수 있습니다. ONNX에서도 사용자 지정 작업으로 내보낼 수도 있습니다. 이 경우 내보내기시 &lt;code&gt;custom_opsets&lt;/code&gt; 사전을 사용하여 사용자 지정 도메인 및 버전 (사용자 지정 opset)을 지정할 수 있습니다 . 명시 적으로 지정되지 않은 경우 사용자 지정 opset 버전은 기본적으로 1로 설정됩니다. 사용자 지정 ONNX 작업을 사용하면 일치하는 사용자 지정 작업 구현 (예 : &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 사용자 지정 작업&lt;/a&gt; , &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX 런타임 사용자 지정 작업)으로&lt;/a&gt; 선택한 백엔드를 확장해야합니다 .</target>
        </trans-unit>
        <trans-unit id="0ab688461ca113091fd169e55e7c9e66deb886f0" translate="yes" xml:space="preserve">
          <source>Deprecated enum-like class for reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, and &lt;code&gt;MAX&lt;/code&gt;.</source>
          <target state="translated">축소 작업에 대해 사용되지 않는 열거 형 클래스 : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; 및 &lt;code&gt;MAX&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4995eb29b7f6d3dc7cac42d9f08dad19e95a9dfa" translate="yes" xml:space="preserve">
          <source>Deprecated; see &lt;a href=&quot;#torch.cuda.max_memory_reserved&quot;&gt;&lt;code&gt;max_memory_reserved()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">더 이상 사용되지 않습니다. &lt;a href=&quot;#torch.cuda.max_memory_reserved&quot;&gt; &lt;code&gt;max_memory_reserved()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e046da60da747b4d87f8d67259afaf6b9fe07bf5" translate="yes" xml:space="preserve">
          <source>Deprecated; see &lt;a href=&quot;#torch.cuda.memory_reserved&quot;&gt;&lt;code&gt;memory_reserved()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">더 이상 사용되지 않습니다. &lt;a href=&quot;#torch.cuda.memory_reserved&quot;&gt; &lt;code&gt;memory_reserved()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="15cdb030ac617cabaf72e8f3099d5d7b07f0d492" translate="yes" xml:space="preserve">
          <source>Dequantize stub module, before calibration, this is same as identity, this will be swapped as &lt;code&gt;nnq.DeQuantize&lt;/code&gt; in &lt;code&gt;convert&lt;/code&gt;.</source>
          <target state="translated">스터브 역 양자화 모듈은 교정 전, 이것은이 같이 바꾼 것, 신원과 동일 &lt;code&gt;nnq.DeQuantize&lt;/code&gt; 으로 &lt;code&gt;convert&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="44fd5a225ad67410d4630aa610f55ced5a7a7c71" translate="yes" xml:space="preserve">
          <source>Dequantizes an incoming tensor</source>
          <target state="translated">들어오는 텐서를 역 양자화합니다.</target>
        </trans-unit>
        <trans-unit id="2a6b986891e70c721e528fb3141e3feb4bc76505" translate="yes" xml:space="preserve">
          <source>Derived classes should implement one or both of &lt;code&gt;_call()&lt;/code&gt; or &lt;code&gt;_inverse()&lt;/code&gt;. Derived classes that set &lt;code&gt;bijective=True&lt;/code&gt; should also implement &lt;a href=&quot;#torch.distributions.transforms.Transform.log_abs_det_jacobian&quot;&gt;&lt;code&gt;log_abs_det_jacobian()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">파생 클래스는 &lt;code&gt;_call()&lt;/code&gt; 또는 &lt;code&gt;_inverse()&lt;/code&gt; 중 하나 또는 둘 다를 구현해야합니다 . &lt;code&gt;bijective=True&lt;/code&gt; 를 설정하는 파생 클래스는 &lt;a href=&quot;#torch.distributions.transforms.Transform.log_abs_det_jacobian&quot;&gt; &lt;code&gt;log_abs_det_jacobian()&lt;/code&gt; &lt;/a&gt; 도 구현해야합니다 .</target>
        </trans-unit>
        <trans-unit id="516c22b47b1fe91311a530710a5d3d144b2ada06" translate="yes" xml:space="preserve">
          <source>Describe an instantaneous event that occurred at some point.</source>
          <target state="translated">어느 시점에서 발생한 즉각적인 사건을 설명하십시오.</target>
        </trans-unit>
        <trans-unit id="8e0a2537499a2a7aaa95e9b6ce35838ddd92db58" translate="yes" xml:space="preserve">
          <source>Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights.</source>
          <target state="translated">가중치에 대한 설정 (관찰자 클래스)을 제공하여 계층 또는 네트워크의 일부를 동적으로 양자화하는 방법을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="fd0e609a210b25ce30494d69de9681e62976242f" translate="yes" xml:space="preserve">
          <source>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</source>
          <target state="translated">활성화 및 가중치에 대한 설정 (관찰자 클래스)을 각각 제공하여 계층 또는 네트워크의 일부를 양자화하는 방법을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="55f8ebc805e65b5b71ddafdae390e3be2bcd69af" translate="yes" xml:space="preserve">
          <source>Description</source>
          <target state="translated">Description</target>
        </trans-unit>
        <trans-unit id="5966dbc5c4d197355b50f3dc66783c2fa78a5226" translate="yes" xml:space="preserve">
          <source>Design Notes</source>
          <target state="translated">디자인 노트</target>
        </trans-unit>
        <trans-unit id="fbe406308e1df41bfda74c701d4a66a5af72c8b6" translate="yes" xml:space="preserve">
          <source>Design Reasoning</source>
          <target state="translated">디자인 추론</target>
        </trans-unit>
        <trans-unit id="91441a05e70cda570458aad59617766bb3ed7236" translate="yes" xml:space="preserve">
          <source>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</source>
          <target state="translated">Tensor를 생성 한 그래프에서 분리하여 리프로 만듭니다. 뷰는 제자리에서 분리 할 수 ​​없습니다.</target>
        </trans-unit>
        <trans-unit id="0e01337242e0297823f889a52bc9dad47f00c9ad" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">유형 승격 &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;문서에&lt;/a&gt; 설명 된 PyTorch 캐스팅 규칙에서 유형 변환이 허용되는지 여부를 결정합니다 .</target>
        </trans-unit>
        <trans-unit id="d2244fb2618c9d05384c145a55a7a5a37bf99078" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">유형 승격 &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;문서에&lt;/a&gt; 설명 된 PyTorch 캐스팅 규칙에서 유형 변환이 허용되는지 여부를 결정합니다 .</target>
        </trans-unit>
        <trans-unit id="a5a74a6df09278b88cb6ea23b7d7f2570c33babf" translate="yes" xml:space="preserve">
          <source>Device</source>
          <target state="translated">Device</target>
        </trans-unit>
        <trans-unit id="1c1f67e2f072a5af5bf17679821b62f151e69437" translate="yes" xml:space="preserve">
          <source>Dict Construction</source>
          <target state="translated">Dict 건설</target>
        </trans-unit>
        <trans-unit id="4461566599c5b88c3897f351e4ae4b0ddc655116" translate="yes" xml:space="preserve">
          <source>Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:</source>
          <target state="translated">표준 SVD와 달리 반환 된 행렬의 크기는 다음과 같이 지정된 순위 및 q 값에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="abe2291b9a77a8ae7585fd7e4b1df0a06016b998" translate="yes" xml:space="preserve">
          <source>Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).</source>
          <target state="translated">차원 이름에는 문자 또는 밑줄이 포함될 수 있습니다. 또한 차원 이름은 유효한 Python 변수 이름이어야합니다 (즉, 밑줄로 시작하지 않음).</target>
        </trans-unit>
        <trans-unit id="a6f1bd13e9cbff9087ce39dcba569ea9ce48c700" translate="yes" xml:space="preserve">
          <source>Dirichlet</source>
          <target state="translated">Dirichlet</target>
        </trans-unit>
        <trans-unit id="e1cf09d41f0116a87479fbe7f5ef39aeac24297b" translate="yes" xml:space="preserve">
          <source>Disable JIT for Debugging</source>
          <target state="translated">디버깅을 위해 JIT 비활성화</target>
        </trans-unit>
        <trans-unit id="35a71645b1a4bfec2d328e91748bd5d190545dea" translate="yes" xml:space="preserve">
          <source>Disable automatic batching</source>
          <target state="translated">자동 일괄 처리 비활성화</target>
        </trans-unit>
        <trans-unit id="dae5511126cad0d55eede647161bef3c8ecf5358" translate="yes" xml:space="preserve">
          <source>Disables denormal floating numbers on CPU.</source>
          <target state="translated">CPU에서 비정규 부동 숫자를 비활성화합니다.</target>
        </trans-unit>
        <trans-unit id="c513ea041a153d26658bb6cf95f8c77cf39ac2f0" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;Tensor.backward()&lt;/code&gt;&lt;/a&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">그라디언트 계산을 비활성화하면 &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;Tensor.backward()&lt;/code&gt; &lt;/a&gt; 호출하지 않을 것이 확실 할 때 추론에 유용합니다 . 그렇지 않으면 &lt;code&gt;requires_grad=True&lt;/code&gt; 가있는 계산에 대한 메모리 소비를 줄일 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d2f39154ff5c9f1589f29bf3daf21678e81210ff" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;code&gt;Tensor.backward()&lt;/code&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">그라디언트 계산을 비활성화하면 &lt;code&gt;Tensor.backward()&lt;/code&gt; 호출하지 않을 것이 확실 할 때 추론에 유용합니다 . 그렇지 않으면 &lt;code&gt;requires_grad=True&lt;/code&gt; 가있는 계산에 대한 메모리 소비를 줄일 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f9c83b5259f966caf231a71b3ad8e69e880665ed" translate="yes" xml:space="preserve">
          <source>Discrete Fourier transforms and related functions.</source>
          <target state="translated">이산 푸리에 변환 및 관련 함수.</target>
        </trans-unit>
        <trans-unit id="938004e21f3f3e82099132266fc1891a7f13de20" translate="yes" xml:space="preserve">
          <source>Distance Functions</source>
          <target state="translated">거리 함수</target>
        </trans-unit>
        <trans-unit id="9cde45ececcca1e1adb41659d3d28a0420087495" translate="yes" xml:space="preserve">
          <source>Distance functions</source>
          <target state="translated">거리 기능</target>
        </trans-unit>
        <trans-unit id="303a9c010db654973c68f36cc8c79b5ce73fe308" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Context</source>
          <target state="translated">분산 된 Autograd 컨텍스트</target>
        </trans-unit>
        <trans-unit id="310713a581eec3788cf900c68063e38885f001bd" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Design</source>
          <target state="translated">분산 형 Autograd 디자인</target>
        </trans-unit>
        <trans-unit id="fa35316634dcb2108118798dd9f61f3d3f7fbb66" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Framework</source>
          <target state="translated">분산 Autograd 프레임 워크</target>
        </trans-unit>
        <trans-unit id="ec9dd95759950a78e8ade825f089a889c2eb7fab" translate="yes" xml:space="preserve">
          <source>Distributed Backward Pass</source>
          <target state="translated">분산 된 역방향 패스</target>
        </trans-unit>
        <trans-unit id="aba915801825e438dbb9e75721a94e96b234ba7d" translate="yes" xml:space="preserve">
          <source>Distributed Data Parallel</source>
          <target state="translated">분산 데이터 병렬</target>
        </trans-unit>
        <trans-unit id="ef10fda1ae05d95a37d1371a4a918aad1148a2da" translate="yes" xml:space="preserve">
          <source>Distributed Key-Value Store</source>
          <target state="translated">분산 키-값 저장소</target>
        </trans-unit>
        <trans-unit id="34eb6d25d260c57a8faa37ee402539b363513315" translate="yes" xml:space="preserve">
          <source>Distributed Optimizer</source>
          <target state="translated">분산 최적화 도구</target>
        </trans-unit>
        <trans-unit id="75f572c75699c1008b8a3d33982f73a57479f298" translate="yes" xml:space="preserve">
          <source>Distributed Pipeline Parallel</source>
          <target state="translated">분산 파이프 라인 병렬</target>
        </trans-unit>
        <trans-unit id="5dfffd4675ffa868d517cf4ad2d7b981131e1dbd" translate="yes" xml:space="preserve">
          <source>Distributed RPC Framework</source>
          <target state="translated">분산 RPC 프레임 워크</target>
        </trans-unit>
        <trans-unit id="e3de08dea5da9f0cffd62432ca7da9726c39e218" translate="yes" xml:space="preserve">
          <source>Distributed communication package - torch.distributed</source>
          <target state="translated">분산 통신 패키지-torch.distributed</target>
        </trans-unit>
        <trans-unit id="67b250f513d5c0c4bc692b9447f357fc7ef369ea" translate="yes" xml:space="preserve">
          <source>DistributedDataParallel</source>
          <target state="translated">DistributedDataParallel</target>
        </trans-unit>
        <trans-unit id="ec463333beac30f92377ae176f21cc4eb4b46520" translate="yes" xml:space="preserve">
          <source>DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.</source>
          <target state="translated">DistributedOptimizer는 작업자에 흩어져있는 매개 변수에 대한 원격 참조를 가져와 각 매개 변수에 대해 로컬로 지정된 최적화 프로그램을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="1d3c457cbe3e35739086346e0a4048653efa42e7" translate="yes" xml:space="preserve">
          <source>Distribution</source>
          <target state="translated">Distribution</target>
        </trans-unit>
        <trans-unit id="c0fd2f6b48b7ad3974231443cb830eddeebb455e" translate="yes" xml:space="preserve">
          <source>Distribution is the abstract base class for probability distributions.</source>
          <target state="translated">분포는 확률 분포의 추상 기본 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="56af6c440851565952f28972260908c667af4940" translate="yes" xml:space="preserve">
          <source>Divides (&amp;ldquo;unscales&amp;rdquo;) the optimizer&amp;rsquo;s gradient tensors by the scale factor.</source>
          <target state="translated">옵티마이 저의 그래디언트 텐서를 스케일 팩터로 나눕니다 ( &quot;스케일 해제&quot;).</target>
        </trans-unit>
        <trans-unit id="e4468b3425489b998cab9185522d9f737628cd1d" translate="yes" xml:space="preserve">
          <source>Divides each element of the input &lt;code&gt;input&lt;/code&gt; by the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">입력 &lt;code&gt;input&lt;/code&gt; 의 각 요소 를 &lt;code&gt;other&lt;/code&gt; 의 해당 요소로 나눕니다 .</target>
        </trans-unit>
        <trans-unit id="33968e4e64500a95f8035e27727c823cadd48d34" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors.</source>
          <target state="translated">주어진 텐서 시퀀스의 데카르트 곱을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="721f0d8eda69e29b574fd64fe0f6c7674a1a6f89" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors. The behavior is similar to python&amp;rsquo;s &lt;code&gt;itertools.product&lt;/code&gt;.</source>
          <target state="translated">주어진 텐서 시퀀스의 데카르트 곱을 수행하십시오. 동작은 python의 &lt;code&gt;itertools.product&lt;/code&gt; 와 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="c4d30aee7a62cbfdaa3d84814f5fc6534da714e2" translate="yes" xml:space="preserve">
          <source>Do quantization aware training and output a quantized model</source>
          <target state="translated">양자화 인식 훈련을 수행하고 양자화 된 모델을 출력합니다.</target>
        </trans-unit>
        <trans-unit id="fcb94671187d8e10ddb359c9decbb94311e20300" translate="yes" xml:space="preserve">
          <source>Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It&amp;rsquo;s highly recommended to add a few examples here.</source>
          <target state="translated">함수의 독 스트링은 도움말 메시지로 작동합니다. 모델이 수행하는 작업과 허용되는 위치 / 키워드 인수가 무엇인지 설명합니다. 여기에 몇 가지 예를 추가하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="96f57a29074dff5e315a68cd23e9cc22e3995e9f" translate="yes" xml:space="preserve">
          <source>Does a linear interpolation of two tensors &lt;code&gt;start&lt;/code&gt; (given by &lt;code&gt;input&lt;/code&gt;) and &lt;code&gt;end&lt;/code&gt; based on a scalar or tensor &lt;code&gt;weight&lt;/code&gt; and returns the resulting &lt;code&gt;out&lt;/code&gt; tensor.</source>
          <target state="translated">두 텐서의 선형 보간 않음 &lt;code&gt;start&lt;/code&gt; (주어진 &lt;code&gt;input&lt;/code&gt; ) 및 &lt;code&gt;end&lt;/code&gt; 스칼라 또는 텐서에 기초 &lt;code&gt;weight&lt;/code&gt; 복귀 결과 &lt;code&gt;out&lt;/code&gt; 텐서.</target>
        </trans-unit>
        <trans-unit id="8782c3b2687a1a4eba3fb6aace40c73bfe021afc" translate="yes" xml:space="preserve">
          <source>Does nothing if the CUDA state is already initialized.</source>
          <target state="translated">CUDA 상태가 이미 초기화 된 경우 아무 작업도 수행하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="525c9a2be6af3422648e9f02a6746e9df12f182d" translate="yes" xml:space="preserve">
          <source>Don&amp;rsquo;t pass received tensors.</source>
          <target state="translated">받은 텐서를 전달하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="e06ffaf662b8ea46a067dea9fb380262a87db6a2" translate="yes" xml:space="preserve">
          <source>Down/up samples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">입력을 주어진 &lt;code&gt;size&lt;/code&gt; 또는 주어진 &lt;code&gt;scale_factor&lt;/code&gt; 로 다운 / 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="e61af53453cf013b5be43bae97307de95bad250e" translate="yes" xml:space="preserve">
          <source>Download object at the given URL to a local path.</source>
          <target state="translated">주어진 URL의 개체를 로컬 경로로 다운로드합니다.</target>
        </trans-unit>
        <trans-unit id="361a0dc66d29b16c754d93724204f2ac9044c37d" translate="yes" xml:space="preserve">
          <source>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</source>
          <target state="translated">Bernoulli 분포에서 이진 난수 (0 또는 1)를 그립니다.</target>
        </trans-unit>
        <trans-unit id="99a0d2b3ff3d729e26380413abca3d7df775cdde" translate="yes" xml:space="preserve">
          <source>Dropout</source>
          <target state="translated">Dropout</target>
        </trans-unit>
        <trans-unit id="132aa5f9595506ef0ca296d6be570b1e4aa7acb2" translate="yes" xml:space="preserve">
          <source>Dropout Layers</source>
          <target state="translated">드롭 아웃 레이어</target>
        </trans-unit>
        <trans-unit id="15a66e933b5615cb5433ce9102df020458b34bcb" translate="yes" xml:space="preserve">
          <source>Dropout functions</source>
          <target state="translated">드롭 아웃 기능</target>
        </trans-unit>
        <trans-unit id="cfe20a4021b31e2ea913c19efd2987a230ac723c" translate="yes" xml:space="preserve">
          <source>Dropout2d</source>
          <target state="translated">Dropout2d</target>
        </trans-unit>
        <trans-unit id="830971c9b2da3b705ee96a71f9d0d4e5128e9bb7" translate="yes" xml:space="preserve">
          <source>Dropout3d</source>
          <target state="translated">Dropout3d</target>
        </trans-unit>
        <trans-unit id="a7c8e330062301237ad2aac63813f57bd9bb1e14" translate="yes" xml:space="preserve">
          <source>Due to limited dynamic range of half datatype, performing this operation in half precision may cause the first element of result to overflow for certain inputs.</source>
          <target state="translated">half 데이터 유형의 제한된 동적 범위로 인해이 작업을 반 정밀도로 수행하면 특정 입력에 대해 결과의 첫 번째 요소가 오버플로 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36b5672884c133b8c278456fa603d4322ce8f54a" translate="yes" xml:space="preserve">
          <source>Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.</source>
          <target state="translated">CUDA 커널의 비동기 특성으로 인해 CUDA 코드에 대해 실행할 때 cProfile 출력 및 CPU 모드 autograd 프로파일 러가 올바른 타이밍을 표시하지 않을 수 있습니다.보고 된 CPU 시간은 커널을 시작하는 데 사용 된 시간을보고하지만 시간은 포함하지 않습니다. 작업이 동기화를 수행하지 않는 한 커널은 GPU에서 실행하는 데 소비했습니다. 동기화를 수행하는 작업은 일반 CPU 모드 프로파일 러에서 매우 많은 비용이 드는 것으로 보입니다. 타이밍이 올바르지 않은 경우 CUDA 모드 autograd 프로파일 러가 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa6e674ff3a01dbf141329517674a02a51f4e055" translate="yes" xml:space="preserve">
          <source>Due to the conjugate symmetry, &lt;code&gt;input&lt;/code&gt; do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when &lt;code&gt;input&lt;/code&gt; is given by &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt;. In such case, set the &lt;code&gt;onesided&lt;/code&gt; argument of this method to &lt;code&gt;True&lt;/code&gt;. Moreover, the original signal shape information can sometimes be lost, optionally set &lt;code&gt;signal_sizes&lt;/code&gt; to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.</source>
          <target state="translated">켤레 대칭으로 인해 &lt;code&gt;input&lt;/code&gt; 은 전체 복소 주파수 값을 포함 할 필요가 없습니다. 시의 경우와 거의 절반의 값은 충분하다 &lt;code&gt;input&lt;/code&gt; 주어진다 &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt; . 이 경우이 메서드 의 &lt;code&gt;onesided&lt;/code&gt; 인수를 &lt;code&gt;True&lt;/code&gt; 로 설정 합니다. 또한 원래 신호 모양 정보가 손실 될 수 있습니다. 선택적으로 &lt;code&gt;signal_sizes&lt;/code&gt; 를 원래 신호의 크기로 설정하여 (배치 모드 인 경우 배치 차원없이) 올바른 모양으로 복구합니다.</target>
        </trans-unit>
        <trans-unit id="3f628f956c51a607f601ca3a9c65aebac60f6d34" translate="yes" xml:space="preserve">
          <source>Duplicate modules are returned only once. In the following example, &lt;code&gt;l&lt;/code&gt; will be returned only once.</source>
          <target state="translated">중복 모듈은 한 번만 반환됩니다. 다음 예에서 &lt;code&gt;l&lt;/code&gt; 은 한 번만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="50fa35c3d84d578bb945e995330eef7c63dcf91c" translate="yes" xml:space="preserve">
          <source>During backward, only gradients at &lt;code&gt;nnz&lt;/code&gt; locations of &lt;code&gt;input&lt;/code&gt; will propagate back. Note that the gradients of &lt;code&gt;input&lt;/code&gt; is coalesced.</source>
          <target state="translated">역방향 동안에는 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;nnz&lt;/code&gt; 위치에있는 그래디언트 만 다시 전파됩니다. &lt;code&gt;input&lt;/code&gt; 의 기울기 가 합쳐집니다.</target>
        </trans-unit>
        <trans-unit id="8e08ad58135bf66f71d67cdf9e3eb42be9cdd416" translate="yes" xml:space="preserve">
          <source>During evaluation the module simply computes an identity function.</source>
          <target state="translated">평가하는 동안 모듈은 단순히 식별 함수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="c42cc58336b9bff21da008fbb38ac485f6938f2c" translate="yes" xml:space="preserve">
          <source>During inference, the model requires only the input tensors, and returns the post-processed predictions as a &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt;, one for each input image. The fields of the &lt;code&gt;Dict&lt;/code&gt; are as follows:</source>
          <target state="translated">추론하는 동안 모델은 입력 텐서 만 필요하고 후 처리 된 예측 을 각 입력 이미지에 대해 하나씩 &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt; 로 반환합니다 . &lt;code&gt;Dict&lt;/code&gt; 의 필드는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8d654e950cd9f5e68da4c08caa15327df6055306" translate="yes" xml:space="preserve">
          <source>During the forward pass, each function range is decorated with &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. &lt;code&gt;seq&lt;/code&gt; is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function&amp;rsquo;s &lt;code&gt;apply()&lt;/code&gt; call is decorated with &lt;code&gt;stashed seq=&amp;lt;M&amp;gt;&lt;/code&gt;. &lt;code&gt;M&lt;/code&gt; is the sequence number that the backward object was created with. By comparing &lt;code&gt;stashed seq&lt;/code&gt; numbers in backward with &lt;code&gt;seq&lt;/code&gt; numbers in forward, you can track down which forward op created each backward Function.</source>
          <target state="translated">순방향 패스 동안 각 기능 범위는 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 으로 장식됩니다 . &lt;code&gt;seq&lt;/code&gt; 는 새로운 역방향 Function 개체가 생성되고 역방향으로 숨겨 질 때마다 증가하는 실행 카운터입니다. 따라서 각 전방 함수 범위와 연관된 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 주석은이 전방 함수에 의해 후방 함수 객체가 생성 된 경우 후방 객체가 시퀀스 번호 N을 받게된다는 것을 알려줍니다. 후방 패스 중에 최상위 범위 래핑 각 C ++ 역방향 함수의 &lt;code&gt;apply()&lt;/code&gt; 호출은 &lt;code&gt;stashed seq=&amp;lt;M&amp;gt;&lt;/code&gt; 장식됩니다 . &lt;code&gt;M&lt;/code&gt; 은 역방향 개체가 생성 된 시퀀스 번호입니다. &lt;code&gt;stashed seq&lt;/code&gt; 번호를 &lt;code&gt;seq&lt;/code&gt; 와 거꾸로 비교하여 앞으로의 숫자, 당신은 각 뒤로 함수를 만든 앞으로 작업을 추적 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="899a3b765d6ee8d823d5852dbea555348e22fa78" translate="yes" xml:space="preserve">
          <source>During training, it randomly masks some of the elements of the input tensor with probability &lt;em&gt;p&lt;/em&gt; using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</source>
          <target state="translated">훈련 중에 베르누이 분포의 샘플을 사용하여 입력 텐서의 일부 요소를 확률 &lt;em&gt;p&lt;/em&gt; 로 무작위로 마스킹 합니다. 마스킹 할 요소는 모든 포워드 호출에서 무작위 화되고 평균 0과 단위 표준 편차를 유지하기 위해 크기가 조정되고 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="1ddb72e8fb2bca4bbdd9cee1bd0f4124ba674fe1" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution.</source>
          <target state="translated">훈련 중에 Bernoulli 분포의 표본을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 입력 텐서의 일부 요소를 무작위로 0으로 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7e07f3de383a6b909b32c2c8420ab783da80d74b" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</source>
          <target state="translated">훈련 중에 Bernoulli 분포의 표본을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 입력 텐서의 일부 요소를 무작위로 0으로 만듭니다. 각 채널은 모든 착신 전환에서 독립적으로 0이됩니다.</target>
        </trans-unit>
        <trans-unit id="e72931d7ae530922c7186be0d18873ff29d42369" translate="yes" xml:space="preserve">
          <source>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</source>
          <target state="translated">학습 중에 모델은 입력 텐서와 다음을 포함하는 대상 (사전 목록)을 모두 예상합니다.</target>
        </trans-unit>
        <trans-unit id="e0184adedf913b076626646d3f52c3b49c39ad6d" translate="yes" xml:space="preserve">
          <source>E</source>
          <target state="translated">E</target>
        </trans-unit>
        <trans-unit id="a466f9bd6da8a16be528d1b97b22f8b80c50fc61" translate="yes" xml:space="preserve">
          <source>E (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;)</source>
          <target state="translated">E ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;텐서&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="db547439274fed58f2b9ee57841dc8931b457c07" translate="yes" xml:space="preserve">
          <source>ELU</source>
          <target state="translated">ELU</target>
        </trans-unit>
        <trans-unit id="c4cf619f3157e4a678ac53e564134cffe6210613" translate="yes" xml:space="preserve">
          <source>Each &lt;code&gt;torch.Tensor&lt;/code&gt; has a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">각 &lt;code&gt;torch.Tensor&lt;/code&gt; 에는 &lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; 이&lt;/a&gt; 있습니다.</target>
        </trans-unit>
        <trans-unit id="1c0427e3aafaddc6d9583e9cefbad6140b6d07cf" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;input&lt;/code&gt; is multiplied by the corresponding element of the Tensor &lt;code&gt;other&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">텐서 &lt;code&gt;input&lt;/code&gt; 의 각 요소에 Tensor &lt;code&gt;other&lt;/code&gt; 의 해당 요소가 곱해집니다 . 결과 텐서가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="728738a25fa4324927c05f66f2cfc30782cc84df" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;other&lt;/code&gt; is multiplied by the scalar &lt;code&gt;alpha&lt;/code&gt; and added to each element of the tensor &lt;code&gt;input&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; 텐서의 각 요소에 스칼라 &lt;code&gt;alpha&lt;/code&gt; 곱하고 텐서 &lt;code&gt;input&lt;/code&gt; 의 각 요소에 더 합니다 . 결과 텐서가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6ea1bfa1407efaeb9ea4478b91f07efe596058d3" translate="yes" xml:space="preserve">
          <source>Each element will be masked independently on every forward call with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.</source>
          <target state="translated">각 요소는 Bernoulli 분포의 샘플을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 모든 착신 전환에서 독립적으로 마스킹됩니다 . 마스킹 할 요소는 모든 포워드 호출에서 무작위 화되고 평균 및 단위 분산을 0으로 유지하기 위해 크기가 조정되고 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="8e969c5beedf78b6dd5c2c5d126246b66ca00cf2" translate="yes" xml:space="preserve">
          <source>Each parameter&amp;rsquo;s gradient (&lt;code&gt;.grad&lt;/code&gt; attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.</source>
          <target state="translated">각 매개 변수의 기울기 ( &lt;code&gt;.grad&lt;/code&gt; 속성)는 최적화 프로그램이 매개 변수를 업데이트하기 전에 스케일을 해제해야하므로 스케일 팩터가 학습률을 방해하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="04229e567ec03d81b5712793e639928468412520" translate="yes" xml:space="preserve">
          <source>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and &amp;ldquo;GIL-thrashing&amp;rdquo; that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</source>
          <target state="translated">각 프로세스에는 독립적 인 Python 인터프리터가 포함되어있어 단일 Python 프로세스에서 여러 실행 스레드, 모델 복제본 또는 GPU를 구동 할 때 발생하는 추가 인터프리터 오버 헤드와 &quot;GIL 스 래싱&quot;을 제거합니다. 이는 반복 레이어 또는 많은 작은 구성 요소가있는 모델을 포함하여 Python 런타임을 많이 사용하는 모델에 특히 중요합니다.</target>
        </trans-unit>
        <trans-unit id="3c11c2aae826827df9743912100e743d6a724614" translate="yes" xml:space="preserve">
          <source>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</source>
          <target state="translated">각 프로세스는 자체 최적화 프로그램을 유지하고 각 반복마다 완전한 최적화 단계를 수행합니다. 중복 된 것처럼 보일 수 있지만 그래디언트가 이미 함께 수집되고 프로세스간에 평균화되어 모든 프로세스에 대해 동일하므로 매개 변수 브로드 캐스트 단계가 필요하지 않으므로 노드간에 텐서를 전송하는 데 소요되는 시간이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="25ddc73f3c685d39e655eadbdd24861cccbd855c" translate="yes" xml:space="preserve">
          <source>Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.</source>
          <target state="translated">각 프로세스는 입력 텐서 목록을 그룹의 모든 프로세스에 분산시키고 수집 된 텐서 목록을 출력 목록에 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e01a4afade34e5c383597dd828cf5ce98ca5f574" translate="yes" xml:space="preserve">
          <source>Each process will receive exactly one tensor and store its data in the &lt;code&gt;tensor&lt;/code&gt; argument.</source>
          <target state="translated">각 프로세스는 정확히 하나의 텐서를 수신하고 해당 데이터를 &lt;code&gt;tensor&lt;/code&gt; 인수 에 저장합니다 .</target>
        </trans-unit>
        <trans-unit id="14f825720650e629375c55e6473f7e4c1fd8e6fb" translate="yes" xml:space="preserve">
          <source>Each sample will be retrieved by indexing tensors along the first dimension.</source>
          <target state="translated">각 샘플은 첫 번째 차원을 따라 텐서를 인덱싱하여 검색됩니다.</target>
        </trans-unit>
        <trans-unit id="a930fc3f8ffe54e36efcacdd772fc5e7328c868c" translate="yes" xml:space="preserve">
          <source>Each tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. The tensor class also provides multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage and defines numeric operations on it.</source>
          <target state="translated">각 텐서에는 데이터를 보유 하는 연결된 &lt;code&gt;torch.Storage&lt;/code&gt; 가 있습니다. 텐서 클래스는 또한 스토리지에 대한 다차원의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;스트라이드&lt;/a&gt; 뷰를 제공하고 이에 대한 숫자 연산을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="0df7dd6db96e14d83140e8da4324d11663fe359e" translate="yes" xml:space="preserve">
          <source>Each tensor in &lt;code&gt;output_tensor_list&lt;/code&gt; should reside on a separate GPU, as should each list of tensors in &lt;code&gt;input_tensor_lists&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_tensor_list&lt;/code&gt; 의 각 텐서 는 &lt;code&gt;input_tensor_lists&lt;/code&gt; 의 각 텐서 목록과 마찬가지로 별도의 GPU에 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="96aad5e1712347cedb5784e57172eb768752a304" translate="yes" xml:space="preserve">
          <source>Efficient softmax approximation as described in &lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier, and Herv&amp;eacute; J&amp;eacute;gou&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier 및 Herv&amp;eacute; J&amp;eacute;gou의 GPU에 대한 효율적인 소프트 맥스&lt;/a&gt; 근사에 설명 된대로 효율적인 소프트 맥스 근사 .</target>
        </trans-unit>
        <trans-unit id="f39713bdb9925bfb39803109379a2aa9f81732f8" translate="yes" xml:space="preserve">
          <source>Either the inplace modified module with submodules wrapped in &lt;code&gt;QuantWrapper&lt;/code&gt; based on qconfig or a new &lt;code&gt;QuantWrapper&lt;/code&gt; module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.</source>
          <target state="translated">qconfig를 기반으로 &lt;code&gt;QuantWrapper&lt;/code&gt; 로 래핑 된 하위 모듈이있는 inplace 수정 된 모듈 또는 입력 모듈을 래핑 하는 새로운 &lt;code&gt;QuantWrapper&lt;/code&gt; 모듈, 후자의 경우는 입력 모듈이 리프 모듈이고이를 양자화하려는 경우에만 발생합니다.</target>
        </trans-unit>
        <trans-unit id="c8078d924b19f64b223a6df8142a489104063237" translate="yes" xml:space="preserve">
          <source>Either:</source>
          <target state="translated">Either:</target>
        </trans-unit>
        <trans-unit id="cba1a5641f37d31826ff43a90ac27303cd2de000" translate="yes" xml:space="preserve">
          <source>Element-wise arctangent of</source>
          <target state="translated">요소 별 아크 탄젠트</target>
        </trans-unit>
        <trans-unit id="c0ce0d75b4a69f8d83fed56dda584bc104824e33" translate="yes" xml:space="preserve">
          <source>Elements lower than min and higher than max are ignored.</source>
          <target state="translated">최소보다 낮고 최대보다 높은 요소는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="279868256307295ed763af9335611205d2a9e0e9" translate="yes" xml:space="preserve">
          <source>Eliminates all but the first element from every consecutive group of equivalent elements.</source>
          <target state="translated">동일한 요소의 모든 연속 그룹에서 첫 번째 요소를 제외한 모든 요소를 ​​제거합니다.</target>
        </trans-unit>
        <trans-unit id="65e872502ba89d481aa8b75804d63958258d47d9" translate="yes" xml:space="preserve">
          <source>Embedding</source>
          <target state="translated">Embedding</target>
        </trans-unit>
        <trans-unit id="fb52acf7809d84b49f20c1e0c2aeb4d9421e883d" translate="yes" xml:space="preserve">
          <source>Embedding (no optional arguments supported)</source>
          <target state="translated">임베딩 (지원되는 선택적 인수 없음)</target>
        </trans-unit>
        <trans-unit id="ad4c45718740fb7fd09a983448014fcde632287c" translate="yes" xml:space="preserve">
          <source>EmbeddingBag</source>
          <target state="translated">EmbeddingBag</target>
        </trans-unit>
        <trans-unit id="f25f65444d58a4000e445a3586674f2f0a1c519b" translate="yes" xml:space="preserve">
          <source>EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by &lt;code&gt;mode&lt;/code&gt;. If &lt;code&gt;per_sample_weights`&lt;/code&gt; is passed, the only supported &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, which computes a weighted sum according to &lt;code&gt;per_sample_weights&lt;/code&gt;.</source>
          <target state="translated">EmbeddingBag는 또한 순방향 패스에 대한 인수로 샘플 당 가중치를 지원합니다. 이것은 &lt;code&gt;mode&lt;/code&gt; 에 의해 지정된 가중치 감소를 수행하기 전에 Embedding의 출력을 조정 합니다 . 경우 &lt;code&gt;per_sample_weights`&lt;/code&gt; 가 전달되면, 지원되는 &lt;code&gt;mode&lt;/code&gt; 이다 &lt;code&gt;&quot;sum&quot;&lt;/code&gt; 에 따른 가중 된 합을 계산하고, &lt;code&gt;per_sample_weights&lt;/code&gt; 이 .</target>
        </trans-unit>
        <trans-unit id="7a184f78683775965d53e63c45e77c29a82cb431" translate="yes" xml:space="preserve">
          <source>Enables .grad attribute for non-leaf Tensors.</source>
          <target state="translated">리프가 아닌 Tensor에 .grad 속성을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="97e857cbb6b3eaaffa99455ab0a6561f4d2333be" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;#torch.autograd.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.autograd.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.autograd.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;#torch.autograd.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt; 를 통해 비활성화 된 경우 그래디언트 계산을 활성화합니다 .</target>
        </trans-unit>
        <trans-unit id="4ba6db1c18e6b89f8ae8769b517165aebdeeb5e7" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt; 를 통해 비활성화 된 경우 그래디언트 계산을 활성화합니다 .</target>
        </trans-unit>
        <trans-unit id="129b2e35235ca3917459fbda73dccc7c7ba4bb9f" translate="yes" xml:space="preserve">
          <source>Ensures that the tensor memory is not reused for another tensor until all current work queued on &lt;code&gt;stream&lt;/code&gt; are complete.</source>
          <target state="translated">&lt;code&gt;stream&lt;/code&gt; 에 대기중인 모든 현재 작업 이 완료 될 때까지 텐서 메모리가 다른 텐서에 재사용되지 않도록 합니다.</target>
        </trans-unit>
        <trans-unit id="02f525cd5d7591c6c78960a007006cf272db0f53" translate="yes" xml:space="preserve">
          <source>Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.</source>
          <target state="translated">진입 점 함수는 모델 (nn.module) 또는 사용자 워크 플로를 더 원활하게 만드는 보조 도구 (예 : 토크 나이저)를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b0dd14c2fc526f82985ca8c8a3a6e1c3b85b6488" translate="yes" xml:space="preserve">
          <source>Environment variable initialization</source>
          <target state="translated">환경 변수 초기화</target>
        </trans-unit>
        <trans-unit id="089481a55a3bf2c4a369d848036bccebf8abe33b" translate="yes" xml:space="preserve">
          <source>Equivalent to input[:,::-1]. Requires the array to be at least 2-D.</source>
          <target state="translated">input [:, ::-1]과 같습니다. 배열이 2 차원 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="3b4933d2950181258ba9867fa0e789a6fac6c0e2" translate="yes" xml:space="preserve">
          <source>Equivalent to input[::-1,&amp;hellip;]. Requires the array to be at least 1-D.</source>
          <target state="translated">input [::-1,&amp;hellip;]과 같습니다. 배열이 1 차원 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="9c1eedde4e72567b3c48e7393929af1241088b7e" translate="yes" xml:space="preserve">
          <source>Errors such as timeouts for the &lt;code&gt;remote&lt;/code&gt; API are handled on a best-effort basis. This means that when remote calls initiated by &lt;code&gt;remote&lt;/code&gt; fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as &lt;code&gt;to_here&lt;/code&gt; or fork call), then future uses of the &lt;code&gt;RRef&lt;/code&gt; will appropriately raise errors. However, it is possible that the user application will use the &lt;code&gt;RRef&lt;/code&gt; before the errors are handled. In this case, errors may not be raised as they have not yet been handled.</source>
          <target state="translated">&lt;code&gt;remote&lt;/code&gt; API에 대한 시간 초과와 같은 오류 는 최선을 다해 처리됩니다. 즉 , 시간 초과 오류와 같이 &lt;code&gt;remote&lt;/code&gt; 시작된 원격 호출이 실패 할 때 오류 처리에 최선의 접근 방식을 취합니다. 이는 오류가 비동기식으로 결과 RRef에서 처리되고 설정됨을 의미합니다. 이 처리 전에 응용 프로그램에서 RRef를 사용하지 않은 경우 (예 : &lt;code&gt;to_here&lt;/code&gt; 또는 fork 호출) 나중에 &lt;code&gt;RRef&lt;/code&gt; 를 사용 하면 오류가 발생합니다. 그러나 오류가 처리되기 전에 사용자 응용 프로그램이 &lt;code&gt;RRef&lt;/code&gt; 를 사용할 수 있습니다 . 이 경우 아직 처리되지 않았으므로 오류가 발생하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2352230ae4369b2400cd9a8b59625450e193a06a" translate="yes" xml:space="preserve">
          <source>Estimate</source>
          <target state="translated">Estimate</target>
        </trans-unit>
        <trans-unit id="9f5d3b9ce4cb19a213d647b445b4900659d33a6d" translate="yes" xml:space="preserve">
          <source>Evaluates module(input) in parallel across the GPUs given in device_ids.</source>
          <target state="translated">device_ids에 지정된 GPU에서 병렬로 모듈 (입력)을 평가합니다.</target>
        </trans-unit>
        <trans-unit id="5b74f76cd70090d77d5d968780f896243c537f02" translate="yes" xml:space="preserve">
          <source>Every &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; has a corresponding storage of the same data type.</source>
          <target state="translated">모든 &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 에는 동일한 데이터 유형의 해당 스토리지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="6c95c4e1c7a1060f504484945d50cad58c391f52" translate="yes" xml:space="preserve">
          <source>Every Sampler subclass has to provide an &lt;code&gt;__iter__()&lt;/code&gt; method, providing a way to iterate over indices of dataset elements, and a &lt;code&gt;__len__()&lt;/code&gt; method that returns the length of the returned iterators.</source>
          <target state="translated">모든 Sampler 하위 클래스는 &lt;code&gt;__iter__()&lt;/code&gt; 메서드를 제공하여 데이터 집합 요소의 인덱스를 반복하는 방법과 반환 된 반복기의 길이를 반환 하는 &lt;code&gt;__len__()&lt;/code&gt; 메서드를 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="88cdbf2ac191cecaae2230a964d90524ead7b613" translate="yes" xml:space="preserve">
          <source>Every collective operation function supports the following two kinds of operations:</source>
          <target state="translated">모든 집합 연산 기능은 다음 두 종류의 연산을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="25ee794140785458589b1f0dd87ed4991a86d45f" translate="yes" xml:space="preserve">
          <source>Every operation performed on &lt;code&gt;Tensor&lt;/code&gt; s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (&lt;code&gt;input &amp;lt;- output&lt;/code&gt;). Then, when backward is called, the graph is processed in the topological ordering, by calling &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; methods of each &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt; object, and passing returned gradients on to next &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt; s.</source>
          <target state="translated">&lt;code&gt;Tensor&lt;/code&gt; 에서 수행되는 모든 작업 은 계산을 수행하고 발생한 것을 기록하는 새로운 함수 객체를 생성합니다. 히스토리는 데이터 종속성을 나타내는 에지 ( &lt;code&gt;input &amp;lt;- output&lt;/code&gt; -output)와 함께 함수 DAG 형식으로 유지됩니다 . 그런 다음 backward이 호출되면 각 &lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt; 객체 의 &lt;a href=&quot;#torch.autograd.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; 메서드를 호출 하고 반환 된 그래디언트를 다음 &lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt; 에 전달 하여 토폴로지 순서대로 그래프를 처리 합니다 .</target>
        </trans-unit>
        <trans-unit id="8d3ee82a911f61025ee027bbb14528d9be81be1b" translate="yes" xml:space="preserve">
          <source>Every tensor that&amp;rsquo;s been modified in-place in a call to &lt;code&gt;forward()&lt;/code&gt; should be given to this function, to ensure correctness of our checks. It doesn&amp;rsquo;t matter whether the function is called before or after modification.</source>
          <target state="translated">검사의 정확성을 보장하기 위해 &lt;code&gt;forward()&lt;/code&gt; 호출에서 제자리에서 수정 된 모든 텐서 가이 함수에 제공되어야합니다. 함수가 수정 전후에 호출되는지 여부는 중요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="cac312ad37989d865266909c5b6681a3176edc55" translate="yes" xml:space="preserve">
          <source>Everything in a user defined &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript Class&lt;/a&gt; is exported by default, functions can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">사용자 정의 &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript 클래스의&lt;/a&gt; 모든 것은 기본적으로 내보내지며 필요한 경우 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 함수를 꾸밀 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e1753c80d644f128cefa2284b014d868b991b9d9" translate="yes" xml:space="preserve">
          <source>Exactly one of &lt;code&gt;devices&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; must be specified.</source>
          <target state="translated">정확히 하나의 &lt;code&gt;devices&lt;/code&gt; 와 &lt;code&gt;out&lt;/code&gt; 을 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="502159c89a8ff9154570557bd71a328efcce5727" translate="yes" xml:space="preserve">
          <source>Exactly one of &lt;code&gt;devices&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; must be specified. When &lt;code&gt;out&lt;/code&gt; is specified, &lt;code&gt;chunk_sizes&lt;/code&gt; must not be specified and will be inferred from sizes of &lt;code&gt;out&lt;/code&gt;.</source>
          <target state="translated">정확히 하나의 &lt;code&gt;devices&lt;/code&gt; 와 &lt;code&gt;out&lt;/code&gt; 을 지정해야합니다. 때 &lt;code&gt;out&lt;/code&gt; 지정되어, &lt;code&gt;chunk_sizes&lt;/code&gt; 은 지정하지 않아야 및 크기에서 유추됩니다 &lt;code&gt;out&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0f01ed56a1e32a05e5ef96e4d779f34784af9a96" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="translated">Example</target>
        </trans-unit>
        <trans-unit id="b3c7ccbaed13d7cf475ac0a8540d9af04a1abfba" translate="yes" xml:space="preserve">
          <source>Example (a type mismatch)</source>
          <target state="translated">예 (유형 불일치)</target>
        </trans-unit>
        <trans-unit id="fb18992fbb684cb9203c712439d9448a7d50b7c9" translate="yes" xml:space="preserve">
          <source>Example (an exported and ignored method in a module):</source>
          <target state="translated">예 (모듈에서 내보내고 무시 된 메서드) :</target>
        </trans-unit>
        <trans-unit id="3fb80256910d817ce9141fdd4451ee10894ec35c" translate="yes" xml:space="preserve">
          <source>Example (calling a script function in a traced function):</source>
          <target state="translated">예 (추적 된 함수에서 스크립트 함수 호출) :</target>
        </trans-unit>
        <trans-unit id="b5bcda1a3bfb1af96df6dd020d0588805d88feba" translate="yes" xml:space="preserve">
          <source>Example (calling a traced function in script):</source>
          <target state="translated">예 (스크립트에서 추적 된 함수 호출) :</target>
        </trans-unit>
        <trans-unit id="9488ed97f58e814938180bd3de0fc03a37f8d8c1" translate="yes" xml:space="preserve">
          <source>Example (fork a free function):</source>
          <target state="translated">예 (프리 함수 포크) :</target>
        </trans-unit>
        <trans-unit id="8d40b7fb2ebedc8ff715bb44861bcfbdc43fa817" translate="yes" xml:space="preserve">
          <source>Example (fork a module method):</source>
          <target state="translated">예 (모듈 메서드 분기) :</target>
        </trans-unit>
        <trans-unit id="c61fd1aa74a0dc25c5318d75c242307960c7041c" translate="yes" xml:space="preserve">
          <source>Example (refining types on parameters and locals):</source>
          <target state="translated">예 (매개 변수 및 지역에 대한 유형 구체화) :</target>
        </trans-unit>
        <trans-unit id="9f3b97fb80872ca69d304fba5bcdeedcdbea8e85" translate="yes" xml:space="preserve">
          <source>Example (scripting a function):</source>
          <target state="translated">예 (함수 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="de4e7f540085eb6296ec8260ef53372b15064ca1" translate="yes" xml:space="preserve">
          <source>Example (scripting a module with traced submodules):</source>
          <target state="translated">예 (추적 된 하위 모듈로 모듈 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="c5b0acb6ce867883478ae6d5579cfda80cbb9404" translate="yes" xml:space="preserve">
          <source>Example (scripting a simple module with a Parameter):</source>
          <target state="translated">예 (매개 변수로 간단한 모듈 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="98fb0963dfe59db2efb10e03278c046397333fb9" translate="yes" xml:space="preserve">
          <source>Example (tracing a function):</source>
          <target state="translated">예 (함수 추적) :</target>
        </trans-unit>
        <trans-unit id="0dad6a76da0fc25a19704212a257d6379a12034b" translate="yes" xml:space="preserve">
          <source>Example (tracing a module with multiple methods):</source>
          <target state="translated">예 (여러 메서드로 모듈 추적) :</target>
        </trans-unit>
        <trans-unit id="07f0e6f11243b3ae74d74b1d28cef8c785cb16fa" translate="yes" xml:space="preserve">
          <source>Example (tracing an existing module):</source>
          <target state="translated">예 (기존 모듈 추적) :</target>
        </trans-unit>
        <trans-unit id="68ede4837476671e511c48828e0de48f7b06d426" translate="yes" xml:space="preserve">
          <source>Example (type annotations for Python 3):</source>
          <target state="translated">예 (Python 3의 유형 주석) :</target>
        </trans-unit>
        <trans-unit id="87a104ad2463311906e75683079bbad94a4fd5c2" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.export&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.export&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="d1c7a00dbf049f876c9759d8441a029ed8e2f4ac" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; on a method):</source>
          <target state="translated">예제 ( 메서드에 &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; 사용) :</target>
        </trans-unit>
        <trans-unit id="f45fe2e88f095f6547360c930d8119bf8c200149" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.ignore&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="ee3db4aefed96eb801a5613278d0970a19c709cc" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.unused&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.unused&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="15234a3935f9814be4e208815ad355afc291f504" translate="yes" xml:space="preserve">
          <source>Example (using a traced module):</source>
          <target state="translated">예 (추적 된 모듈 사용) :</target>
        </trans-unit>
        <trans-unit id="73cb14b980b72004cbdbfb86fac47cf40133628b" translate="yes" xml:space="preserve">
          <source>Example 1: splitting workload across all workers in &lt;code&gt;__iter__()&lt;/code&gt;:</source>
          <target state="translated">예제 1 : &lt;code&gt;__iter__()&lt;/code&gt; 모든 작업자에 걸쳐 워크로드 분할 :</target>
        </trans-unit>
        <trans-unit id="877513517da6d71b899179494c7d8cfcf56d5921" translate="yes" xml:space="preserve">
          <source>Example 2: splitting workload across all workers using &lt;code&gt;worker_init_fn&lt;/code&gt;:</source>
          <target state="translated">예 2 : &lt;code&gt;worker_init_fn&lt;/code&gt; 을 사용하여 모든 작업자에 워크로드 분할 :</target>
        </trans-unit>
        <trans-unit id="49895a4623d5b4a9344031509b184da5a2c67818" translate="yes" xml:space="preserve">
          <source>Example. if we have the following shape for inputs and outputs:</source>
          <target state="translated">예. 입력 및 출력에 대해 다음과 같은 모양이있는 경우 :</target>
        </trans-unit>
        <trans-unit id="c63737abd7347a7ae582cb9fbdf37d6c0e5b251e" translate="yes" xml:space="preserve">
          <source>Example:</source>
          <target state="translated">Example:</target>
        </trans-unit>
        <trans-unit id="8ec20a84a991e14aeedf10c8e4e9247bf2c9315c" translate="yes" xml:space="preserve">
          <source>Example: End-to-end AlexNet from PyTorch to ONNX</source>
          <target state="translated">예 : PyTorch에서 ONNX 로의 종단 간 AlexNet</target>
        </trans-unit>
        <trans-unit id="038d2f8486ff39be2d765514d254dcc770c02da8" translate="yes" xml:space="preserve">
          <source>Example: Suppose the last window is: &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; vs &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</source>
          <target state="translated">예 : 마지막 창이 다음과 같다고 가정합니다. &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; 대 &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6104a08ed7eb335488d3ea3f93c6210a1cc4746c" translate="yes" xml:space="preserve">
          <source>Example::</source>
          <target state="translated">Example::</target>
        </trans-unit>
        <trans-unit id="eb01bf04c9a0e8a71c45816513df424f1c7ffedb" translate="yes" xml:space="preserve">
          <source>Examples</source>
          <target state="translated">Examples</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="de3e24010b1050a8265462ab7e8f3a95c00717ea" translate="yes" xml:space="preserve">
          <source>Examples::</source>
          <target state="translated">Examples::</target>
        </trans-unit>
        <trans-unit id="8cc1b49841e2eac10bc692d1dee6f79c0d6dc0f7" translate="yes" xml:space="preserve">
          <source>Expand this tensor to the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.expand_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.expand(other.size())&lt;/code&gt;.</source>
          <target state="translated">이 텐서를 &lt;code&gt;other&lt;/code&gt; 와 같은 크기로 확장합니다 . &lt;code&gt;self.expand_as(other)&lt;/code&gt; 는 &lt;code&gt;self.expand(other.size())&lt;/code&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="7381344dd49d389b0f60204fb4dbd8a7ee76371e" translate="yes" xml:space="preserve">
          <source>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the &lt;code&gt;stride&lt;/code&gt; to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</source>
          <target state="translated">텐서를 확장하면 새 메모리가 할당되지 않고, &lt;code&gt;stride&lt;/code&gt; 를 0 으로 설정하여 크기 1의 차원이 더 큰 크기로 확장되는 기존 텐서에 새 뷰만 생성 됩니다. 크기 1의 모든 차원은 임의의 값으로 확장 할 수 있습니다. 새로운 메모리를 할당하지 않고.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
