<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="0d60fa4ee1b04194485d841887bd2a9720912ce7" translate="yes" xml:space="preserve">
          <source>For one, if either</source>
          <target state="translated">하나의 경우</target>
        </trans-unit>
        <trans-unit id="388c4450cdbff00300f1e66a7fe9bdbd7ad63df6" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the accuracies for the pre-trained models are as follows</source>
          <target state="translated">사람 키포인트 감지의 경우 사전 훈련 된 모델의 정확도는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="50930fbd40ff369c0fa47ef8013fdffeda924fcf" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the pre-trained model return the keypoints in the following order:</source>
          <target state="translated">사람 키포인트 감지의 경우 사전 학습 된 모델은 다음 순서로 키포인트를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="1547488bbec68c16b5473f238ae370eab49bdca4" translate="yes" xml:space="preserve">
          <source>For references on how to use it, please refer to &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/imagenet&quot;&gt;PyTorch example - ImageNet implementation&lt;/a&gt;</source>
          <target state="translated">사용 방법에 대한 참조는 &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/imagenet&quot;&gt;PyTorch 예제-ImageNet 구현을&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5b0de76315890608ccfec148ffc9eb96ab3d19d0" translate="yes" xml:space="preserve">
          <source>For summation index</source>
          <target state="translated">합산 지수 용</target>
        </trans-unit>
        <trans-unit id="ae21afa5351a91a1f97daa42d064cca4e245e30f" translate="yes" xml:space="preserve">
          <source>For test time, we report the time for the model evaluation and postprocessing (including mask pasting in image), but not the time for computing the precision-recall.</source>
          <target state="translated">테스트 시간의 경우 모델 평가 및 후 처리 (이미지에 마스크 붙여 넣기 포함) 시간을보고하지만 정밀도 재현율 계산 시간은보고하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="294fafc65e707c55d30f92f6e6fa7597c4416793" translate="yes" xml:space="preserve">
          <source>For the case of two input spatial dimensions this operation is sometimes called &lt;code&gt;im2col&lt;/code&gt;.</source>
          <target state="translated">두 개의 입력 공간 차원의 경우이 작업을 &lt;code&gt;im2col&lt;/code&gt; 이라고도 합니다.</target>
        </trans-unit>
        <trans-unit id="4294b91105e53a9501af3f666602dc49b4fae690" translate="yes" xml:space="preserve">
          <source>For the case of two output spatial dimensions this operation is sometimes called &lt;code&gt;col2im&lt;/code&gt;.</source>
          <target state="translated">두 개의 출력 공간 차원의 경우이 작업을 &lt;code&gt;col2im&lt;/code&gt; 이라고 합니다 .</target>
        </trans-unit>
        <trans-unit id="63464d394b40abaadcd5cd07925b101f22c85d5d" translate="yes" xml:space="preserve">
          <source>For the following examples:</source>
          <target state="translated">다음 예의 경우 :</target>
        </trans-unit>
        <trans-unit id="250e5319dc4351da49602e421d48ab9ddd21e40e" translate="yes" xml:space="preserve">
          <source>For the full list of NCCL environment variables, please refer to &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html&quot;&gt;NVIDIA NCCL&amp;rsquo;s official documentation&lt;/a&gt;</source>
          <target state="translated">NCCL 환경 변수의 전체 목록은 &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html&quot;&gt;NVIDIA NCCL의 공식 문서&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="fbb3381cc7af3b7907b168edfc7c7438b66cd477" translate="yes" xml:space="preserve">
          <source>For the most part, you shouldn&amp;rsquo;t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, there are two cases in which you may need to care.</source>
          <target state="translated">대부분의 경우, 희소 텐서가 합쳐 졌는지 여부는 신경 쓰지 않아도됩니다. 그러나 당신이 돌봐야 할 두 가지 경우가 있습니다.</target>
        </trans-unit>
        <trans-unit id="b5ee688f7705f701edf82664a012c925ae4f0c34" translate="yes" xml:space="preserve">
          <source>For the unpacked case, the directions can be separated using &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt;, with forward and backward being direction &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; respectively. Similarly, the directions can be separated in the packed case.</source>
          <target state="translated">압축이 풀린 경우 방향은 &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt; 사용하여 분리 할 수 ​​있으며 , 앞으로 및 뒤로는 각각 방향 &lt;code&gt;0&lt;/code&gt; 과 &lt;code&gt;1&lt;/code&gt; 이 됩니다. 마찬가지로 포장 된 케이스에서 방향을 분리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="0b3352e82009813f0901947623d7383dc915b831" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted by length in a decreasing order, i.e. &lt;code&gt;input[:,0]&lt;/code&gt; should be the longest sequence, and &lt;code&gt;input[:,B-1]&lt;/code&gt; the shortest one. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="translated">정렬되지 않은 시퀀스의 경우 &lt;code&gt;enforce_sorted = False&lt;/code&gt; . 경우 &lt;code&gt;enforce_sorted&lt;/code&gt; 가 있다 &lt;code&gt;True&lt;/code&gt; 상기 시퀀스 즉, 내림차순으로 정렬 길이한다 &lt;code&gt;input[:,0]&lt;/code&gt; 긴 서열, 그리고해야 &lt;code&gt;input[:,B-1]&lt;/code&gt; 최단 하나. force_sorted &lt;code&gt;enforce_sorted = True&lt;/code&gt; 는 ONNX 내보내기에만 필요합니다.</target>
        </trans-unit>
        <trans-unit id="74aa708d44a325bbc3a2350d27d2ddf6e3e6b36a" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted in the order of decreasing length. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="translated">정렬되지 않은 시퀀스의 경우 &lt;code&gt;enforce_sorted = False&lt;/code&gt; . 경우 &lt;code&gt;enforce_sorted&lt;/code&gt; 가 있다 &lt;code&gt;True&lt;/code&gt; , 시퀀스의 길이는 감소하는 순서로 정렬되어야한다. force_sorted &lt;code&gt;enforce_sorted = True&lt;/code&gt; 는 ONNX 내보내기에만 필요합니다.</target>
        </trans-unit>
        <trans-unit id="65edc3b96f1e2f54f14a87eb92d4dea02b1b6e9b" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task.</source>
          <target state="translated">&lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 비동기 작업을 강제로 완료 하고 작업 결과를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="dc619e95fe61cfbb5a61b8e23e153aa71a53f467" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task. See &lt;a href=&quot;torch.jit.fork#torch.jit.fork&quot;&gt;&lt;code&gt;fork()&lt;/code&gt;&lt;/a&gt; for docs and examples. :param func: an asynchronous task reference, created through &lt;code&gt;torch.jit.fork&lt;/code&gt; :type func: torch.jit.Future[T]</source>
          <target state="translated">&lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 비동기 작업을 강제로 완료 하고 작업 결과를 반환합니다. 문서 및 예제는 &lt;a href=&quot;torch.jit.fork#torch.jit.fork&quot;&gt; &lt;code&gt;fork()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 . : param func : &lt;code&gt;torch.jit.fork&lt;/code&gt; 를 통해 생성 된 비동기 작업 참조 : type func : torch.jit.Future [T]</target>
        </trans-unit>
        <trans-unit id="e0b8013b454609aa16cd91b06faebccde13182b1" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules will be invoked &lt;code&gt;len(device_ids)&lt;/code&gt; times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via &lt;a href=&quot;torch.nn.module#torch.nn.Module.register_forward_pre_hook&quot;&gt;&lt;code&gt;register_forward_pre_hook()&lt;/code&gt;&lt;/a&gt; be executed before &lt;code&gt;all&lt;/code&gt;&lt;code&gt;len(device_ids)&lt;/code&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; calls, but that each such hook be executed before the corresponding &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; call of that device.</source>
          <target state="translated">&lt;code&gt;module&lt;/code&gt; 및 하위 모듈에 정의 된 정방향 및 역방향 후크 는 각각 특정 장치에있는 입력과 함께 &lt;code&gt;len(device_ids)&lt;/code&gt; 번 호출됩니다 . 특히, 후크는 해당 장치의 작동과 관련하여 올바른 순서로만 실행되도록 보장됩니다. 예를 들어, &lt;a href=&quot;torch.nn.module#torch.nn.Module.register_forward_pre_hook&quot;&gt; &lt;code&gt;register_forward_pre_hook()&lt;/code&gt; &lt;/a&gt; 통해 설정된 후크가 &lt;code&gt;all&lt;/code&gt; &lt;code&gt;len(device_ids)&lt;/code&gt; &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt; 호출 전에 실행 된다는 보장은 없지만 이러한 후크 는 해당 장치 의 해당 &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt; 호출 전에 실행 됩니다.</target>
        </trans-unit>
        <trans-unit id="6acf90721bbeda5f77e7073663e225cf7715008a" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules won&amp;rsquo;t be invoked anymore, unless the hooks are initialized in the &lt;code&gt;forward()&lt;/code&gt; method.</source>
          <target state="translated">&lt;code&gt;module&lt;/code&gt; 및 하위 모듈에 정의 된 정방향 및 역방향 후크 는 후크가 &lt;code&gt;forward()&lt;/code&gt; 메서드 에서 초기화되지 않는 한 더 이상 호출되지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="988135b5646708fe12c52e4f90092901a6ed112d" translate="yes" xml:space="preserve">
          <source>Fractional MaxPooling is described in detail in the paper &lt;a href=&quot;https://arxiv.org/abs/1412.6071&quot;&gt;Fractional MaxPooling&lt;/a&gt; by Ben Graham</source>
          <target state="translated">Fractional MaxPooling은 Ben Graham의 &lt;a href=&quot;https://arxiv.org/abs/1412.6071&quot;&gt;Fractional MaxPooling&lt;/a&gt; 논문에 자세히 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="2297da08c1909f5dca5e7ab8bf486b563fbe806e" translate="yes" xml:space="preserve">
          <source>FractionalMaxPool2d</source>
          <target state="translated">FractionalMaxPool2d</target>
        </trans-unit>
        <trans-unit id="d790b402d79ac1a723c790313bcd679999474630" translate="yes" xml:space="preserve">
          <source>Frequently Asked Questions</source>
          <target state="translated">자주 묻는 질문</target>
        </trans-unit>
        <trans-unit id="8c7beab7a6d84d3a1004bcc75ccd72648a4866a0" translate="yes" xml:space="preserve">
          <source>Frobenius norm</source>
          <target state="translated">프로 베니 우스 표준</target>
        </trans-unit>
        <trans-unit id="e553dabf708d383e58b9442ad6ace6c5038afc7a" translate="yes" xml:space="preserve">
          <source>From the &lt;code&gt;torch.nn.utils&lt;/code&gt; module</source>
          <target state="translated">로부터 &lt;code&gt;torch.nn.utils&lt;/code&gt; 의 모듈</target>
        </trans-unit>
        <trans-unit id="f92451195f62b1d7cd9731015aa1fa56d4159f9d" translate="yes" xml:space="preserve">
          <source>Fully Convolutional Networks</source>
          <target state="translated">완전 컨볼 루션 네트워크</target>
        </trans-unit>
        <trans-unit id="d8cdf10face49f05a0d7bce562c1cbcff9eeec04" translate="yes" xml:space="preserve">
          <source>Function Calls</source>
          <target state="translated">함수 호출</target>
        </trans-unit>
        <trans-unit id="6eb78f68900c241e14e0ca55cb63779b55624f2b" translate="yes" xml:space="preserve">
          <source>Function that measures Binary Cross Entropy between target and output logits.</source>
          <target state="translated">목표 로짓과 출력 로짓 사이의 이진 교차 엔트로피를 측정하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="7648ef7c5f8c3df8793ee3a78cea755f5210492d" translate="yes" xml:space="preserve">
          <source>Function that measures the Binary Cross Entropy between the target and the output.</source>
          <target state="translated">타겟과 출력 사이의 이진 교차 엔트로피를 측정하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="9ffd252ff27f5e82a15818c0d16b964ae35b7608" translate="yes" xml:space="preserve">
          <source>Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode:</source>
          <target state="translated">컴파일시 True를 반환하고 그렇지 않으면 False를 반환하는 함수입니다. 이것은 아직 TorchScript와 호환되지 않는 코드를 모델에 남겨두기 위해 @unused 데코레이터와 함께 특히 유용합니다. .. 테스트 코드 :</target>
        </trans-unit>
        <trans-unit id="4f7128796586951f3b8d4c12bad19a0f376bcf68" translate="yes" xml:space="preserve">
          <source>Function that takes the mean element-wise absolute value difference.</source>
          <target state="translated">평균 요소 별 절대 값 차이를 취하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="092441d339641c913a709b12bd8927bee99f6a19" translate="yes" xml:space="preserve">
          <source>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">요소 별 절대 오차가 베타 미만이면 제곱항을 사용하고 그렇지 않으면 L1 항을 사용하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="e285c73ee5b2c775ccf90093dc413f8d49580952" translate="yes" xml:space="preserve">
          <source>Function to draw a sequence of &lt;code&gt;n&lt;/code&gt; points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is</source>
          <target state="translated">Sobol 시퀀스에서 &lt;code&gt;n&lt;/code&gt; 개의 포인트 시퀀스를 그리는 기능 입니다. 샘플은 이전 샘플에 따라 다릅니다. 결과의 크기는</target>
        </trans-unit>
        <trans-unit id="54711e443d02794b60e7868b9b77738a8238e18e" translate="yes" xml:space="preserve">
          <source>Function to fast-forward the state of the &lt;code&gt;SobolEngine&lt;/code&gt; by &lt;code&gt;n&lt;/code&gt; steps. This is equivalent to drawing &lt;code&gt;n&lt;/code&gt; samples without using the samples.</source>
          <target state="translated">의 빨리 감기 상태로 기능 &lt;code&gt;SobolEngine&lt;/code&gt; 의해 &lt;code&gt;n&lt;/code&gt; 단계. 이는 샘플을 사용하지 않고 &lt;code&gt;n&lt;/code&gt; 개의 샘플 을 그리는 것과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="ef0c07b5cc0a1093381723476ea2c34106682639" translate="yes" xml:space="preserve">
          <source>Function to reset the &lt;code&gt;SobolEngine&lt;/code&gt; to base state.</source>
          <target state="translated">&lt;code&gt;SobolEngine&lt;/code&gt; 을 기본 상태 로 재설정하는 기능 입니다.</target>
        </trans-unit>
        <trans-unit id="f4400d33370b62a4424c92ac993690b6bf723355" translate="yes" xml:space="preserve">
          <source>Functional interface</source>
          <target state="translated">기능적 인터페이스</target>
        </trans-unit>
        <trans-unit id="c75f6c5a3a8ea37a3aa2ef8f8e4a53197661b6a7" translate="yes" xml:space="preserve">
          <source>Functional interface (quantized).</source>
          <target state="translated">기능적 인터페이스 (양자화).</target>
        </trans-unit>
        <trans-unit id="c6f96173e459065a4f35775c868cf69352b58eaa" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 과 기능적으로 동일 하지만 단일 함수를 나타내며 속성이나 매개 변수가 없습니다.</target>
        </trans-unit>
        <trans-unit id="1f72e9d093d3406e36decddb7c64c8207dc32272" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="translated">&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 과 기능적으로 동일 하지만 단일 함수를 나타내며 속성이나 매개 변수가 없습니다.</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="5bab26ebb87fa09b2599a41edb42c2be419cbfdb" translate="yes" xml:space="preserve">
          <source>Functions don&amp;rsquo;t change much, they can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;torch.jit.unused&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">함수는 많이 변경되지 않으며 필요한 경우 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;torch.jit.unused&lt;/code&gt; &lt;/a&gt; 로 장식 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f08615c088136953c00202fd041b9b4df5e1dfea" translate="yes" xml:space="preserve">
          <source>Furthermore, if the &lt;code&gt;functions&lt;/code&gt; argument is supplied, bindings will be automatically generated for each function specified. &lt;code&gt;functions&lt;/code&gt; can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.</source>
          <target state="translated">또한 &lt;code&gt;functions&lt;/code&gt; 인수가 제공되면 지정된 각 함수에 대해 바인딩이 자동으로 생성됩니다. &lt;code&gt;functions&lt;/code&gt; 는 함수 이름 목록이거나 함수 이름에서 독 스트링으로의 사전 매핑 일 수 있습니다. 목록이 주어지면 각 함수의 이름이 독 스트링으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="5bace8a23b9deebac72cffe4881dc6eed9968755" translate="yes" xml:space="preserve">
          <source>Furthermore, the outputs are scaled by a factor of</source>
          <target state="translated">또한 출력은</target>
        </trans-unit>
        <trans-unit id="5f71e96ba4fcb977ab1e675f428bf9d148fa7baf" translate="yes" xml:space="preserve">
          <source>GELU</source>
          <target state="translated">GELU</target>
        </trans-unit>
        <trans-unit id="1f9ad9bc561d09f2f445363cf93033cdd6037e9c" translate="yes" xml:space="preserve">
          <source>GLU</source>
          <target state="translated">GLU</target>
        </trans-unit>
        <trans-unit id="a6a6318544c9b361fc08c6ed94696c4d207d2748" translate="yes" xml:space="preserve">
          <source>GPU</source>
          <target state="translated">GPU</target>
        </trans-unit>
        <trans-unit id="954c88d52c93dd19e22542fcb20c7d583f7b8447" translate="yes" xml:space="preserve">
          <source>GPU hosts with Ethernet interconnect</source>
          <target state="translated">이더넷 상호 연결이있는 GPU 호스트</target>
        </trans-unit>
        <trans-unit id="9238bee4212524d967f1550988f8d590152434a1" translate="yes" xml:space="preserve">
          <source>GPU hosts with InfiniBand interconnect</source>
          <target state="translated">InfiniBand 상호 연결이있는 GPU 호스트</target>
        </trans-unit>
        <trans-unit id="2b56fffb35167d283f21ccca2d0bc8fbd0a6fe43" translate="yes" xml:space="preserve">
          <source>GPU tensor</source>
          <target state="translated">GPU 텐서</target>
        </trans-unit>
        <trans-unit id="51c6274e38d61ebd2c2aa1fabf4fb11564b93c03" translate="yes" xml:space="preserve">
          <source>GRU</source>
          <target state="translated">GRU</target>
        </trans-unit>
        <trans-unit id="1e29d48c3333cba171b9e878119cbab38e34e4a1" translate="yes" xml:space="preserve">
          <source>GRUCell</source>
          <target state="translated">GRUCell</target>
        </trans-unit>
        <trans-unit id="473631bf9e98f3b45650e597635bf741c36747b6" translate="yes" xml:space="preserve">
          <source>Gathers a list of tensors in a single process.</source>
          <target state="translated">단일 프로세스에서 텐서 목록을 수집합니다.</target>
        </trans-unit>
        <trans-unit id="dfd7c23593ebd41b22c549217d736ad6cbc14502" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list.</source>
          <target state="translated">목록의 전체 그룹에서 텐서를 수집합니다.</target>
        </trans-unit>
        <trans-unit id="a033b46254c69591de655bd57283ab1b1f241ecf" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list. Each tensor in &lt;code&gt;tensor_list&lt;/code&gt; should reside on a separate GPU</source>
          <target state="translated">목록의 전체 그룹에서 텐서를 수집합니다. &lt;code&gt;tensor_list&lt;/code&gt; 의 각 텐서 는 별도의 GPU에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="d1ed345e7cd127b4f17c3621e8381fb806d9f267" translate="yes" xml:space="preserve">
          <source>Gathers values along an axis specified by &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 으로 지정된 축을 따라 값을 수집합니다 .</target>
        </trans-unit>
        <trans-unit id="83e2e53da9f325087f4bce1c008f1d1192e058ed" translate="yes" xml:space="preserve">
          <source>Generally speaking, input to this function should contain values following conjugate symmetry. Note that even if &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, often symmetry on some part is still needed. When this requirement is not satisfied, the behavior of &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; is undefined. Since &lt;a href=&quot;../autograd#torch.autograd.gradcheck&quot;&gt;&lt;code&gt;torch.autograd.gradcheck()&lt;/code&gt;&lt;/a&gt; estimates numerical Jacobian with point perturbations, &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; will almost certainly fail the check.</source>
          <target state="translated">일반적으로이 함수에 대한 입력은 켤레 대칭을 따르는 값을 포함해야합니다. &lt;code&gt;onesided&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; 라도 일부 부분의 대칭이 여전히 필요한 경우가 많습니다. 이 요구 사항이 충족되지 않으면 &lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 의 동작 이 정의되지 않습니다. 이후 &lt;a href=&quot;../autograd#torch.autograd.gradcheck&quot;&gt; &lt;code&gt;torch.autograd.gradcheck()&lt;/code&gt; &lt;/a&gt; 포인트 동요와 수치 자 코비안을 추정 &lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 거의 확실하게 체크를 실패합니다.</target>
        </trans-unit>
        <trans-unit id="925a95fee15d092ef688c243751df4f4117845d7" translate="yes" xml:space="preserve">
          <source>Generate a square mask for the sequence. The masked positions are filled with float(&amp;lsquo;-inf&amp;rsquo;). Unmasked positions are filled with float(0.0).</source>
          <target state="translated">시퀀스에 대한 정사각형 마스크를 생성합니다. 마스크 된 위치는 float ( '-inf')로 채워집니다. 마스크되지 않은 위치는 float (0.0)으로 채워집니다.</target>
        </trans-unit>
        <trans-unit id="05e4b9ca24a152cc1be23c09ff6368077d47b2b3" translate="yes" xml:space="preserve">
          <source>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices &lt;code&gt;theta&lt;/code&gt;.</source>
          <target state="translated">affine 행렬 &lt;code&gt;theta&lt;/code&gt; 의 배치가 주어지면 2D 또는 3D 유동장 (샘플링 그리드)을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="535a8bdc4a8b5e249e51bb72f3ff867e269f10d8" translate="yes" xml:space="preserve">
          <source>Generates a Vandermonde matrix.</source>
          <target state="translated">Vandermonde 행렬을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="1d20de03126b297e05c13a7d280f33e24c72c537" translate="yes" xml:space="preserve">
          <source>Generator</source>
          <target state="translated">Generator</target>
        </trans-unit>
        <trans-unit id="009c08ccefb98d44d31c65421a4320ecc2bb6f65" translate="yes" xml:space="preserve">
          <source>Generator.device -&amp;gt; device</source>
          <target state="translated">Generator.device-&amp;gt; 장치</target>
        </trans-unit>
        <trans-unit id="a3e705cc61a19f33d7c9c030f107a70569966485" translate="yes" xml:space="preserve">
          <source>Generators</source>
          <target state="translated">Generators</target>
        </trans-unit>
        <trans-unit id="80dadd86173d0ff3979257793d4e45beb238b6a2" translate="yes" xml:space="preserve">
          <source>Generics</source>
          <target state="translated">Generics</target>
        </trans-unit>
        <trans-unit id="43ac4a1c9cd4df862c76bc8567278e105813907d" translate="yes" xml:space="preserve">
          <source>Get &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; of a given worker name. Use this &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; to avoid passing an expensive string on every invocation.</source>
          <target state="translated">가져 &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt; &lt;code&gt;WorkerInfo&lt;/code&gt; 을&lt;/a&gt; 주어진 작업자 이름으로. 모든 호출에서 값 비싼 문자열을 전달하지 &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt; &lt;code&gt;WorkerInfo&lt;/code&gt; &lt;/a&gt; 려면 이 WorkerInfo 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="41265238a92192e4ad5f6debaeb489d9d1f1c823" translate="yes" xml:space="preserve">
          <source>Get the Torch Hub cache directory used for storing downloaded models &amp;amp; weights.</source>
          <target state="translated">다운로드 한 모델 및 무게를 저장하는 데 사용되는 Torch Hub 캐시 디렉토리를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="31e03005a5c777fa579a5d2e380424ed9ca0b4c8" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">현재 기본 부동 소수점 &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 을 가져옵니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b8610c06d92827723a75fd0f1cce98077b3a70ce" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">현재 기본 부동 소수점 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; 을 가져옵니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5aca0eb2fa5dfc1cb36d36748698ea752a002636" translate="yes" xml:space="preserve">
          <source>Get the include paths required to build a C++ or CUDA extension.</source>
          <target state="translated">C ++ 또는 CUDA 확장을 빌드하는 데 필요한 포함 경로를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="8c5303d4517efbd9f11eb88c60066d56bc4690f4" translate="yes" xml:space="preserve">
          <source>Get the k-th diagonal of a given matrix:</source>
          <target state="translated">주어진 행렬의 k 번째 대각선을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="d8e48ff0538ba6cbffe42300fa9158799c05f336" translate="yes" xml:space="preserve">
          <source>Get the square matrix where the input vector is the diagonal:</source>
          <target state="translated">입력 벡터가 대각선 인 정사각형 행렬을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="c3d9b9988129bd858a97b5e5dca9086f55e31cf1" translate="yes" xml:space="preserve">
          <source>Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.</source>
          <target state="translated">std :: random_device 또는 현재 시간에서 비 결정적 난수를 가져 와서 Generator를 시드하는 데 사용합니다.</target>
        </trans-unit>
        <trans-unit id="8c370c58908a07f2882a626fa97d9bcfd7bd61c5" translate="yes" xml:space="preserve">
          <source>Gets the current device of the generator.</source>
          <target state="translated">생성기의 현재 장치를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="6a2e241a18985fac466002399c13c581b746dc24" translate="yes" xml:space="preserve">
          <source>Getting started with Distributed RPC Framework</source>
          <target state="translated">분산 RPC 프레임 워크 시작하기</target>
        </trans-unit>
        <trans-unit id="c7e10d3aed3a471005914064e3e561dd9fce7d89" translate="yes" xml:space="preserve">
          <source>Given a 3-D tensor and reduction using the multiplication operation, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="translated">곱하기 연산을 사용하는 3 차원 텐서와 축소가 주어지면 &lt;code&gt;self&lt;/code&gt; 는 다음과 같이 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="cd7589a87267ad50da5e4440a08034e70102a4b2" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="translated">채널당 선형 (아핀) 양자화에 의해 양자화 된 Tensor가 주어지면 기본 양자화 기의 Tensor 스케일 스케일을 반환합니다. 텐서의 해당 차원 (q_per_channel_axis에서)과 일치하는 요소의 수가 있습니다.</target>
        </trans-unit>
        <trans-unit id="9a7e24af56b75cfda37479a07ba5602027c1c762" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="translated">채널당 선형 (아핀) 양자화에 의해 양자화 된 Tensor가 주어지면 기본 양자화 기의 zero_points의 텐서를 반환합니다. 텐서의 해당 차원 (q_per_channel_axis에서)과 일치하는 요소의 수가 있습니다.</target>
        </trans-unit>
        <trans-unit id="2ca023eeaa09e79566843e7d6c6a44e41c5d8038" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</source>
          <target state="translated">채널당 선형 (아핀) 양자화에 의해 양자화 된 Tensor가 주어지면 채널당 양자화가 적용되는 차원의 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="488568401d49cd08f3be08869dd3fbd14d9e5113" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</source>
          <target state="translated">선형 (아핀) 양자화에 의해 양자화 된 Tensor가 주어지면 기본 quantizer ()의 스케일을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="70ceca1da9acd2d4b46a6674c2ea2f823b1ff770" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</source>
          <target state="translated">선형 (아핀) 양자화에 의해 양자화 된 Tensor가 주어지면 기본 quantizer ()의 zero_point를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="90c7e80a6051c57ef51f03a4a57294f2d936927c" translate="yes" xml:space="preserve">
          <source>Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors</source>
          <target state="translated">양자화 된 텐서 목록이 주어지면이를 역 양자화하고 fp32 텐서 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="275d4783ccc8a12c168de20cf782230c04ee1b4f" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, &lt;code&gt;self.int_repr()&lt;/code&gt; returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</source>
          <target state="translated">양자화 된 Tensor가 주어지면 &lt;code&gt;self.int_repr()&lt;/code&gt; 은 주어진 Tensor의 기본 uint8_t 값을 저장하는 데이터 유형으로 uint8_t가있는 CPU Tensor를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a07592b05f2ffe420d78857907dc0a868ad8e9ba" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</source>
          <target state="translated">양자화 된 Tensor가 주어지면 역 양자화하고 역 양자화 된 float Tensor를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="df67c816ade50221be71f968c55c25fc9c22abad" translate="yes" xml:space="preserve">
          <source>Given an &lt;code&gt;input&lt;/code&gt; and a flow-field &lt;code&gt;grid&lt;/code&gt;, computes the &lt;code&gt;output&lt;/code&gt; using &lt;code&gt;input&lt;/code&gt; values and pixel locations from &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">주어 &lt;code&gt;input&lt;/code&gt; 및 흐름 필드 &lt;code&gt;grid&lt;/code&gt; 의 연산 &lt;code&gt;output&lt;/code&gt; 이용하여 &lt;code&gt;input&lt;/code&gt; 의 값과 화소의 위치를 &lt;code&gt;grid&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1edda6e755754d8aedb4972c3cf98f079bb5938f" translate="yes" xml:space="preserve">
          <source>Given the legs of a right triangle, return its hypotenuse.</source>
          <target state="translated">직각 삼각형의 다리가 주어지면 빗변을 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="2507bf0b60a0a2f4a957fa33007787504bb4d58f" translate="yes" xml:space="preserve">
          <source>Gives us the following diagnostic information:</source>
          <target state="translated">다음 진단 정보를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="d4b28b75d3d71965ffab4c160557a8512917b298" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;.</source>
          <target state="translated">지정된 &lt;code&gt;pruning_method&lt;/code&gt; 를 적용하여 &lt;code&gt;parameters&lt;/code&gt; 모든 매개 변수에 해당하는 텐서를 전역 적으로 제거 합니다.</target>
        </trans-unit>
        <trans-unit id="5b2d596c770a42eb120b6d78a8d729e895cf8f96" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;. Modifies modules in place by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">지정된 &lt;code&gt;pruning_method&lt;/code&gt; 를 적용하여 &lt;code&gt;parameters&lt;/code&gt; 모든 매개 변수에 해당하는 텐서를 전역 적으로 제거 합니다. 1) pruning 메소드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가하여 모듈을 제자리에 수정합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="1c964494fd83a4fa2353af497be43d7d3c6b7430" translate="yes" xml:space="preserve">
          <source>Globally unique id to identify the worker.</source>
          <target state="translated">작업자를 식별하기위한 전역 적으로 고유 한 ID입니다.</target>
        </trans-unit>
        <trans-unit id="2dd8834e4e85debb567290745d3b3e1efdcd0e5d" translate="yes" xml:space="preserve">
          <source>Gloo has been hardened by years of extensive use in PyTorch and is thus very reliable. However, as it was designed to perform collective communication, it may not always be the best fit for RPC. For example, each networking operation is synchronous and blocking, which means that it cannot be run in parallel with others. Moreover, it opens a connection between all pairs of nodes, and brings down all of them when one fails, thus reducing the resiliency and the elasticity of the system.</source>
          <target state="translated">Gloo는 PyTorch에서 수년간 광범위하게 사용되어 강화되었으며 따라서 매우 신뢰할 수 있습니다. 그러나 집단 통신을 수행하도록 설계되었으므로 항상 RPC에 가장 적합한 것은 아닙니다. 예를 들어 각 네트워킹 작업은 동기식이며 차단되므로 다른 작업과 병렬로 실행할 수 없습니다. 또한 모든 노드 쌍 사이의 연결을 열고 하나가 실패하면 모든 노드를 중단하여 시스템의 탄력성과 탄력성을 감소시킵니다.</target>
        </trans-unit>
        <trans-unit id="0493f8c5c9a4c1e6227e147f0649196be2a0314a" translate="yes" xml:space="preserve">
          <source>GoogLeNet</source>
          <target state="translated">GoogLeNet</target>
        </trans-unit>
        <trans-unit id="07ad0c4f37a421022704ed36cdcbf7e981dfe5e6" translate="yes" xml:space="preserve">
          <source>GoogLeNet (Inception v1) model architecture from &lt;a href=&quot;http://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;Going Deeper with Convolutions&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">GoogLeNet (Inception v1) &lt;a href=&quot;http://arxiv.org/abs/1409.4842&quot;&gt;&quot;Going Deeper with Convolutions&quot;의&lt;/a&gt; 모델 아키텍처 .</target>
        </trans-unit>
        <trans-unit id="a46ca2c8e8067233adba7afe56ba3ef36ef0ac6c" translate="yes" xml:space="preserve">
          <source>GoogleNet</source>
          <target state="translated">GoogleNet</target>
        </trans-unit>
        <trans-unit id="6ff674c26d399b6074c452be450688d5b72da6db" translate="yes" xml:space="preserve">
          <source>Gradients are modified in-place.</source>
          <target state="translated">그라디언트는 제자리에서 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="51f814ea6f476134875113c78aa4479584c4db98" translate="yes" xml:space="preserve">
          <source>Graphs can be inspected as shown to confirm that the computation described by a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; is correct, in both automated and manual fashion, as described below.</source>
          <target state="translated">아래에 설명 된대로 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 이 설명하는 계산 이 자동 및 수동 방식 모두에서 올바른지 확인하기 위해 표시된대로 그래프를 검사 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="db56247cfffad115bd03a34895c4144a2f602e2a" translate="yes" xml:space="preserve">
          <source>GroupNorm</source>
          <target state="translated">GroupNorm</target>
        </trans-unit>
        <trans-unit id="ae9629f4ebb82c6331c0809fa9a0e54b00e578e6" translate="yes" xml:space="preserve">
          <source>Groups</source>
          <target state="translated">Groups</target>
        </trans-unit>
        <trans-unit id="7cf184f4c67ad58283ecb19349720b0cae756829" translate="yes" xml:space="preserve">
          <source>H</source>
          <target state="translated">H</target>
        </trans-unit>
        <trans-unit id="e19beadee3375715c817358699b6b6b7c3b1c276" translate="yes" xml:space="preserve">
          <source>H=\text{embedding\_dim}</source>
          <target state="translated">H=\text{embedding\_dim}</target>
        </trans-unit>
        <trans-unit id="0daac27dde551de614ce1f8b22990869e521a767" translate="yes" xml:space="preserve">
          <source>H_{all}=\text{num\_directions} * \text{hidden\_size}</source>
          <target state="translated">H_ {all} = \ text {num \ _directions} * \ text {hidden \ _size}</target>
        </trans-unit>
        <trans-unit id="5af2c1c42a9e7cbeea8d0ef376277c241943a820" translate="yes" xml:space="preserve">
          <source>H_{in1}=\text{in1\_features}</source>
          <target state="translated">H_{in1}=\text{in1\_features}</target>
        </trans-unit>
        <trans-unit id="553b3def323fb898bbf5a634729bc342796f0364" translate="yes" xml:space="preserve">
          <source>H_{in2}=\text{in2\_features}</source>
          <target state="translated">H_{in2}=\text{in2\_features}</target>
        </trans-unit>
        <trans-unit id="3bcb97c0c827228e3e76ce7ff65ced7d17f29099" translate="yes" xml:space="preserve">
          <source>H_{in}</source>
          <target state="translated">H_{in}</target>
        </trans-unit>
        <trans-unit id="208ac84ce5aa93dcc05f3ab66b20dc1d3775c246" translate="yes" xml:space="preserve">
          <source>H_{in} = \text{in\_features}</source>
          <target state="translated">H_ {in} = \ text {in \ _features}</target>
        </trans-unit>
        <trans-unit id="699846905d49c5bcf7b21460adb78513ef80b2fb" translate="yes" xml:space="preserve">
          <source>H_{in}=\text{input\_size}</source>
          <target state="translated">H_{in}=\text{input\_size}</target>
        </trans-unit>
        <trans-unit id="7530e9ad4c2fb5dd91aaec26c4ecbcf1a3a5ae05" translate="yes" xml:space="preserve">
          <source>H_{out}</source>
          <target state="translated">H_{out}</target>
        </trans-unit>
        <trans-unit id="29b7895de890ff23af55e3b18083accf3b3c9504" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">H_ {out} = (H_ {in}-1) \ times \ text {stride [0]}-2 \ times \ text {padding [0]} + \ text {kernel \ _size [0]}</target>
        </trans-unit>
        <trans-unit id="2ab9363dd5f0a667cae76fffcb24249e4641e2c1" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">H_ {out} = (H_ {in}-1) \ times \ text {stride [1]}-2 \ times \ text {padding [1]} + \ text {kernel \ _size [1]}</target>
        </trans-unit>
        <trans-unit id="eb1e8745c8404aa0257819793650f55ce47e4a32" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">H_ {out} = (H_ {in}-1) \ times \ text {stride} [0]-2 \ times \ text {padding} [0] + \ text {dilation} [0] \ times (\ text { 커널 \ _size} [0]-1) + \ text {output \ _padding} [0] + 1</target>
        </trans-unit>
        <trans-unit id="18b0ad397dc5e56655f52d98246617aead364a80" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]</source>
          <target state="translated">H_ {out} = (H_ {in}-1) \ times \ text {stride} [0]-2 \ times \ text {padding} [0] + \ text {kernel \ _size} [0]</target>
        </trans-unit>
        <trans-unit id="79c01a18bf062cd324b81ee63feda955b8a67b5e" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="translated">H_ {out} = (H_ {in}-1) \ times \ text {stride} [1]-2 \ times \ text {padding} [1] + \ text {dilation} [1] \ times (\ text { 커널 \ _size} [1]-1) + \ text {output \ _padding} [1] + 1</target>
        </trans-unit>
        <trans-unit id="092f027e0b0b6c6ccdb35d8eefd3fbd19f119f52" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}</source>
          <target state="translated">H_ {out} = H_ {in} + \ text {padding \ _top} + \ text {padding \ _bottom}</target>
        </trans-unit>
        <trans-unit id="42cf8f4ab52d42aa9e4d4c9b75634fb9ef976615" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} \times \text{upscale\_factor}</source>
          <target state="translated">H_ {out} = H_ {in} \ times \ text {upscale \ _factor}</target>
        </trans-unit>
        <trans-unit id="be48d8fe2e8d0b5e7eb05cfe3a720b22ad1fddb5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor H_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="2df9cd2b82eb2979d700977c589d82b892fd325a" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]} \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in} + 2 * \ text {padding [0]}-\ text {dilation [0]} \ times (\ text {kernel \ _size [0]}- 1)-1} {\ text {stride [0]}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="475e8a255144af15d34931776b25e492a4505dc7" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in} + 2 \ times \ text {padding} [0]-\ text {dilation} [0] \ times (\ text {kernel \ _size} [0] -1)-1} {\ text {stride} [0]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="017fc26768fffbdc1fef6a9416fcdd78e1cd6d94" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in} + 2 \ times \ text {padding} [0]-\ text {kernel \ _size} [0]} {\ text {stride} [0]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="d6788aaadc5cc718ef1733396e3d48f7aa8bc1e1" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in} + 2 \ times \ text {padding} [1]-\ text {확장} [1] \ times (\ text {kernel \ _size} [1] -1)-1} {\ text {stride} [1]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="7eddc94bad71df818c92fb147993424e6e6351b5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in} + 2 \ times \ text {padding} [1]-\ text {kernel \ _size} [1]} {\ text {stride} [1]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="fea56e0b8cb35cbb479b336782329a86326003b9" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_ {out} = \ left \ lfloor \ frac {H_ {in}-\ text {kernel \ _size} [0]} {\ text {stride} [0]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="563db076821ec71bea9cecf69cd0383ccd26729e" translate="yes" xml:space="preserve">
          <source>H_{out} = \text{out\_features}</source>
          <target state="translated">H_ {out} = \ text {out \ _features}</target>
        </trans-unit>
        <trans-unit id="4a917c5666fd0c869393621972c028e97e193eb0" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{hidden\_size}</source>
          <target state="translated">H_{out}=\text{hidden\_size}</target>
        </trans-unit>
        <trans-unit id="baeaafcb89405e03dc701698426b69d67220a62d" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{out\_features}</source>
          <target state="translated">H_{out}=\text{out\_features}</target>
        </trans-unit>
        <trans-unit id="593e90ae43cc7b6082dfac4b6d583f5426501c8b" translate="yes" xml:space="preserve">
          <source>Hamming window function.</source>
          <target state="translated">해밍 창 기능.</target>
        </trans-unit>
        <trans-unit id="69d7b15573065426b9ad5d17655afb90ef85c68f" translate="yes" xml:space="preserve">
          <source>Hann window function.</source>
          <target state="translated">Hann 창 기능.</target>
        </trans-unit>
        <trans-unit id="9deb0b051207a937d2c2524ee2d17196f4c7e2ae" translate="yes" xml:space="preserve">
          <source>HardShrink</source>
          <target state="translated">HardShrink</target>
        </trans-unit>
        <trans-unit id="a1d971d31da1fbc25ba44171ebf839ffcca3d2d9" translate="yes" xml:space="preserve">
          <source>HardTanh</source>
          <target state="translated">HardTanh</target>
        </trans-unit>
        <trans-unit id="c9c68b0024efc57340d60b6a9fea397c5c6ea7cf" translate="yes" xml:space="preserve">
          <source>HardTanh is defined as:</source>
          <target state="translated">HardTanh은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b2d44614503eb1489dff12f9f7cd6893be6f2de6" translate="yes" xml:space="preserve">
          <source>Hardshrink</source>
          <target state="translated">Hardshrink</target>
        </trans-unit>
        <trans-unit id="6da2cabc8d832449dfd76781dd5cab3e0527d812" translate="yes" xml:space="preserve">
          <source>Hardsigmoid</source>
          <target state="translated">Hardsigmoid</target>
        </trans-unit>
        <trans-unit id="34e8d8510ac49404bdd474758f835c671f823a90" translate="yes" xml:space="preserve">
          <source>Hardswish</source>
          <target state="translated">Hardswish</target>
        </trans-unit>
        <trans-unit id="c5343dec88b0ad5e872efa7466247b124ae44911" translate="yes" xml:space="preserve">
          <source>Hardtanh</source>
          <target state="translated">Hardtanh</target>
        </trans-unit>
        <trans-unit id="e2828b0734fec8e5d21a67190b498444f3d1d6ac" translate="yes" xml:space="preserve">
          <source>Helper function to convert all &lt;code&gt;BatchNorm*D&lt;/code&gt; layers in the model to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers.</source>
          <target state="translated">모델의 모든 &lt;code&gt;BatchNorm*D&lt;/code&gt; 레이어를 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 레이어 로 변환하는 도우미 함수 입니다.</target>
        </trans-unit>
        <trans-unit id="ecbcb0e424c40d9931c2c7c8b44c33c5f4e12465" translate="yes" xml:space="preserve">
          <source>Here</source>
          <target state="translated">Here</target>
        </trans-unit>
        <trans-unit id="2bcc88cec50d653f641638296cdbe26d969f54b5" translate="yes" xml:space="preserve">
          <source>Here are the summary of the accuracies for the models trained on the instances set of COCO train2017 and evaluated on COCO val2017.</source>
          <target state="translated">다음은 COCO train2017의 인스턴스 세트에서 훈련되고 COCO val2017에서 평가 된 모델의 정확도 요약입니다.</target>
        </trans-unit>
        <trans-unit id="dab4abc36186b02c3f92677e0eb34f60ea19ee65" translate="yes" xml:space="preserve">
          <source>Here are the ways to call &lt;code&gt;to&lt;/code&gt;:</source>
          <target state="translated">여기에 호출하는 방법입니다 &lt;code&gt;to&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="445a6f9d83de6be3e9c47cd5d94f26ed51643163" translate="yes" xml:space="preserve">
          <source>Here is a code snippet specifies an entrypoint for &lt;code&gt;resnet18&lt;/code&gt; model if we expand the implementation in &lt;code&gt;pytorch/vision/hubconf.py&lt;/code&gt;. In most case importing the right function in &lt;code&gt;hubconf.py&lt;/code&gt; is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;pytorch/vision repo&lt;/a&gt;</source>
          <target state="translated">다음은 &lt;code&gt;pytorch/vision/hubconf.py&lt;/code&gt; 에서 구현을 확장하는 경우 &lt;code&gt;resnet18&lt;/code&gt; 모델 의 진입 점을 지정하는 코드 스 니펫 입니다. 대부분의 경우 &lt;code&gt;hubconf.py&lt;/code&gt; 에서 올바른 기능을 가져 오는 것으로 충분합니다. 여기서는 확장 된 버전을 예제로 사용하여 작동 방식을 보여 드리고자합니다. &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;pytorch / vision repo&lt;/a&gt; 에서 전체 스크립트를 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ae7bbf97b180351dca19b9329b791cc507f2b072" translate="yes" xml:space="preserve">
          <source>Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX. It runs a single round of inference and then saves the resulting traced model to &lt;code&gt;alexnet.onnx&lt;/code&gt;:</source>
          <target state="translated">다음은 torchvision에 정의 된대로 사전 훈련 된 AlexNet을 ONNX로 내보내는 간단한 스크립트입니다. 단일 추론을 실행 한 다음 결과 추적 모델을 &lt;code&gt;alexnet.onnx&lt;/code&gt; 에 저장합니다 .</target>
        </trans-unit>
        <trans-unit id="92287b24e3387d7af034be3a29ae6b7bb430d9be" translate="yes" xml:space="preserve">
          <source>Here is an example of handling missing symbolic function for &lt;code&gt;elu&lt;/code&gt; operator. We try to export the model and see the error message as below:</source>
          <target state="translated">다음은 &lt;code&gt;elu&lt;/code&gt; 연산자에 대한 누락 된 기호 함수를 처리하는 예입니다 . 모델 내보내기를 시도하면 아래와 같은 오류 메시지가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="e2d985f7b3554227167f11444cb5f1733937d03c" translate="yes" xml:space="preserve">
          <source>Here is another &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;tutorial of exporting the SuperResolution model to ONNX.&lt;/a&gt;.</source>
          <target state="translated">다음은 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;SuperResolution 모델을 ONNX로 내보내는&lt;/a&gt; 또 다른 튜토리얼입니다. .</target>
        </trans-unit>
        <trans-unit id="3d3237992ce2b8eaf6bc76469f46fd9b7ad4ba34" translate="yes" xml:space="preserve">
          <source>HingeEmbeddingLoss</source>
          <target state="translated">HingeEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="144dc3699fae1cc6bf638916541c8cf0248b5ad0" translate="yes" xml:space="preserve">
          <source>Histogram represented as a tensor</source>
          <target state="translated">텐서로 표현 된 히스토그램</target>
        </trans-unit>
        <trans-unit id="6abad81420de9ddfe06289f2d7475110d726e0f7" translate="yes" xml:space="preserve">
          <source>Holds parameters in a dictionary.</source>
          <target state="translated">사전에 매개 변수를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="5b433389b7ec9270f0a561a21e07b6fafe96e6b4" translate="yes" xml:space="preserve">
          <source>Holds parameters in a list.</source>
          <target state="translated">목록에 매개 변수를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="57c99ed1c82d3f1f9d82887040191b4b4abeeaa4" translate="yes" xml:space="preserve">
          <source>Holds submodules in a dictionary.</source>
          <target state="translated">사전에 하위 모듈을 보관합니다.</target>
        </trans-unit>
        <trans-unit id="26eb2ae7fc8977e64bf6bd3cc41f59552193cc67" translate="yes" xml:space="preserve">
          <source>Holds submodules in a list.</source>
          <target state="translated">목록에 하위 모듈을 보관합니다.</target>
        </trans-unit>
        <trans-unit id="e7b0c97307f7d5f337291cd7806994ff8fc4ef93" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; of a packed sequence.</source>
          <target state="translated">패킹 된 시퀀스 의 &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt; &lt;code&gt;batch_sizes&lt;/code&gt; &lt;/a&gt; 목록과 데이터를 보유합니다 .</target>
        </trans-unit>
        <trans-unit id="8419c96872dc0338c3c6bf4230e67b362cd3efd6" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;code&gt;batch_sizes&lt;/code&gt; of a packed sequence.</source>
          <target state="translated">패킹 된 시퀀스 의 &lt;code&gt;batch_sizes&lt;/code&gt; 목록과 데이터를 보유합니다 .</target>
        </trans-unit>
        <trans-unit id="94d972a8a1e2dd3bfc19dfa88cfbe55c7c0c3689" translate="yes" xml:space="preserve">
          <source>How to implement an entrypoint?</source>
          <target state="translated">진입 점을 구현하는 방법은 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="25a9b274bc53945b3703ea4d8441695a628a5f50" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.EmbeddingBag&quot;&gt;&lt;code&gt;EmbeddingBag&lt;/code&gt;&lt;/a&gt; is much more time and memory efficient than using a chain of these operations.</source>
          <target state="translated">However, &lt;a href=&quot;#torch.nn.EmbeddingBag&quot;&gt; &lt;code&gt;EmbeddingBag&lt;/code&gt; &lt;/a&gt; is much more time and memory efficient than using a chain of these operations.</target>
        </trans-unit>
        <trans-unit id="0911b09b4e58b715dde3450d0531ebe9129437bf" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; should always be a CPU &lt;code&gt;torch.int64&lt;/code&gt; tensor.</source>
          <target state="translated">However, &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt; &lt;code&gt;batch_sizes&lt;/code&gt; &lt;/a&gt; should always be a CPU &lt;code&gt;torch.int64&lt;/code&gt; tensor.</target>
        </trans-unit>
        <trans-unit id="ca73ab65568cd125c2d27a22bbd9e863c10b675d" translate="yes" xml:space="preserve">
          <source>I</source>
          <target state="translated">I</target>
        </trans-unit>
        <trans-unit id="17be3fac04b9eb29173ba1517552ac97aa85e862" translate="yes" xml:space="preserve">
          <source>I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.</source>
          <target state="translated">I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.</target>
        </trans-unit>
        <trans-unit id="c3eeaff2d8f8be416c4b7ba0cc9972581ba73c12" translate="yes" xml:space="preserve">
          <source>INDICES WITH CORRESPONDING NAMES:</source>
          <target state="translated">INDICES WITH CORRESPONDING NAMES:</target>
        </trans-unit>
        <trans-unit id="7e5a975b6add84fd53e3710a9ceac15eb06663b7" translate="yes" xml:space="preserve">
          <source>Identity</source>
          <target state="translated">Identity</target>
        </trans-unit>
        <trans-unit id="751c68a3471b1c791efaee0a8e7c24ea0c266efd" translate="yes" xml:space="preserve">
          <source>If</source>
          <target state="translated">If</target>
        </trans-unit>
        <trans-unit id="e876238e2103f6c99bd92bbd23268dffc89d5e98" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.hub.set_dir&quot;&gt;&lt;code&gt;set_dir()&lt;/code&gt;&lt;/a&gt; is not called, default path is &lt;code&gt;$TORCH_HOME/hub&lt;/code&gt; where environment variable &lt;code&gt;$TORCH_HOME&lt;/code&gt; defaults to &lt;code&gt;$XDG_CACHE_HOME/torch&lt;/code&gt;. &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; follows the X Design Group specification of the Linux filesystem layout, with a default value &lt;code&gt;~/.cache&lt;/code&gt; if the environment variable is not set.</source>
          <target state="translated">If &lt;a href=&quot;#torch.hub.set_dir&quot;&gt; &lt;code&gt;set_dir()&lt;/code&gt; &lt;/a&gt; is not called, default path is &lt;code&gt;$TORCH_HOME/hub&lt;/code&gt; where environment variable &lt;code&gt;$TORCH_HOME&lt;/code&gt; defaults to &lt;code&gt;$XDG_CACHE_HOME/torch&lt;/code&gt; . &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; follows the X Design Group specification of the Linux filesystem layout, with a default value &lt;code&gt;~/.cache&lt;/code&gt; if the environment variable is not set.</target>
        </trans-unit>
        <trans-unit id="2365a78f9c70fd188f67372dcb4ead5e46155be1" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="translated">If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; &amp;gt; 0, it is above the main diagonal.</target>
        </trans-unit>
        <trans-unit id="3b581ac7d1874578962f7ebaf7dd7355265bfcb4" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="translated">If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; &amp;lt; 0, it is below the main diagonal.</target>
        </trans-unit>
        <trans-unit id="d93aaba0d391e746ad390a6373a6f9bdbe62d27c" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, it is the main diagonal.</source>
          <target state="translated">If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt; = 0, it is the main diagonal.</target>
        </trans-unit>
        <trans-unit id="a1496c02b09e4951bc82bcd52564a0a26f1a0976" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;(h_0, c_0)&lt;/code&gt; is not provided, both &lt;strong&gt;h_0&lt;/strong&gt; and &lt;strong&gt;c_0&lt;/strong&gt; default to zero.</source>
          <target state="translated">If &lt;code&gt;(h_0, c_0)&lt;/code&gt; is not provided, both &lt;strong&gt;h_0&lt;/strong&gt; and &lt;strong&gt;c_0&lt;/strong&gt; default to zero.</target>
        </trans-unit>
        <trans-unit id="db361da536d20a314407fbca9d9b4b9534c18d89" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;Model&lt;/code&gt; is instantiated it will result in a compilation error since the compiler doesn&amp;rsquo;t know about &lt;code&gt;x&lt;/code&gt;. There are 4 ways to inform the compiler of attributes on &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">If &lt;code&gt;Model&lt;/code&gt; is instantiated it will result in a compilation error since the compiler doesn&amp;rsquo;t know about &lt;code&gt;x&lt;/code&gt; . There are 4 ways to inform the compiler of attributes on &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;:</target>
        </trans-unit>
        <trans-unit id="5fde3f46b9d751a46e85953fec7714aa67e2342e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="translated">If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the elements in &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; are added to &lt;code&gt;self&lt;/code&gt; . If accumulate is &lt;code&gt;False&lt;/code&gt; , the behavior is undefined if indices contain duplicate elements.</target>
        </trans-unit>
        <trans-unit id="2201c7c6a621b0e5c67fe66d2f987b652400f785" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;code&gt;value&lt;/code&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="translated">If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the elements in &lt;code&gt;value&lt;/code&gt; are added to &lt;code&gt;self&lt;/code&gt; . If accumulate is &lt;code&gt;False&lt;/code&gt; , the behavior is undefined if indices contain duplicate elements.</target>
        </trans-unit>
        <trans-unit id="989ae399047ba12cf5c5c1ad5cbe5e9842000a8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the output tensor containing indices. If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.</source>
          <target state="translated">If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , the output tensor containing indices. If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.</target>
        </trans-unit>
        <trans-unit id="42d4ccb0a610720c59f54cf25fbe1940e041af3d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;batch1&lt;/code&gt; is a</source>
          <target state="translated">If &lt;code&gt;batch1&lt;/code&gt; is a</target>
        </trans-unit>
        <trans-unit id="f6f5f33da7723c89d0774922372d006d60d5610a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;beta&lt;/code&gt; is 0, then &lt;code&gt;input&lt;/code&gt; will be ignored, and &lt;code&gt;nan&lt;/code&gt; and &lt;code&gt;inf&lt;/code&gt; in it will not be propagated.</source>
          <target state="translated">If &lt;code&gt;beta&lt;/code&gt; is 0, then &lt;code&gt;input&lt;/code&gt; will be ignored, and &lt;code&gt;nan&lt;/code&gt; and &lt;code&gt;inf&lt;/code&gt; in it will not be propagated.</target>
        </trans-unit>
        <trans-unit id="dc51e352774290d1926f402524b258eb4db4275e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), &lt;code&gt;input&lt;/code&gt; will be padded on both sides so that the</source>
          <target state="translated">If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), &lt;code&gt;input&lt;/code&gt; will be padded on both sides so that the</target>
        </trans-unit>
        <trans-unit id="965719376b6d5fe07ada3da22176f7f46b7ed555" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then there will be padding e.g. &lt;code&gt;'constant'&lt;/code&gt;, &lt;code&gt;'reflect'&lt;/code&gt;, etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.</source>
          <target state="translated">If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then there will be padding e.g. &lt;code&gt;'constant'&lt;/code&gt; , &lt;code&gt;'reflect'&lt;/code&gt; , etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.</target>
        </trans-unit>
        <trans-unit id="fc14ae1693b0f07d578cbfe76b09c86688e0df82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;compute_uv&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will be zero matrices of shape</source>
          <target state="translated">If &lt;code&gt;compute_uv&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will be zero matrices of shape</target>
        </trans-unit>
        <trans-unit id="65b33918f4984ada2bac2f4ee54a2728e16bb872" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;descending&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the elements are sorted in descending order by value.</source>
          <target state="translated">If &lt;code&gt;descending&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the elements are sorted in descending order by value.</target>
        </trans-unit>
        <trans-unit id="f30e67b5addf429de66048f84b97eb2d56624113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.</source>
          <target state="translated">If &lt;code&gt;dim&lt;/code&gt; is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.</target>
        </trans-unit>
        <trans-unit id="e52a79087756b4493bd3fe7986958f5739aaef82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, the last dimension of the &lt;code&gt;input&lt;/code&gt; is chosen.</source>
          <target state="translated">If &lt;code&gt;dim&lt;/code&gt; is not given, the last dimension of the &lt;code&gt;input&lt;/code&gt; is chosen.</target>
        </trans-unit>
        <trans-unit id="ffe3df3fa3b821e3480cebbf887d0692dbdcfbb2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt; or &lt;code&gt;forward&lt;/code&gt; of &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object with a single &lt;code&gt;forward&lt;/code&gt; method containing the traced code. The returned &lt;code&gt;ScriptModule&lt;/code&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;func&lt;/code&gt; is a standalone function, &lt;code&gt;trace&lt;/code&gt; returns &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt; or &lt;code&gt;forward&lt;/code&gt; of &lt;code&gt;nn.Module&lt;/code&gt; , &lt;code&gt;trace&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; object with a single &lt;code&gt;forward&lt;/code&gt; method containing the traced code. The returned &lt;code&gt;ScriptModule&lt;/code&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt; . If &lt;code&gt;func&lt;/code&gt; is a standalone function, &lt;code&gt;trace&lt;/code&gt; returns &lt;code&gt;ScriptFunction&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="12f27a06f15ef646d5a97006314eddc5168ac16a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;function&lt;/code&gt; invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won&amp;rsquo;t be equivalent, and unfortunately it can&amp;rsquo;t be detected.</source>
          <target state="translated">If &lt;code&gt;function&lt;/code&gt; invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won&amp;rsquo;t be equivalent, and unfortunately it can&amp;rsquo;t be detected.</target>
        </trans-unit>
        <trans-unit id="7fc30718c6826ba4f06aa25c6aae50a9ac61ea27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;grid&lt;/code&gt; has values outside the range of &lt;code&gt;[-1, 1]&lt;/code&gt;, the corresponding outputs are handled as defined by &lt;code&gt;padding_mode&lt;/code&gt;. Options are</source>
          <target state="translated">If &lt;code&gt;grid&lt;/code&gt; has values outside the range of &lt;code&gt;[-1, 1]&lt;/code&gt; , the corresponding outputs are handled as defined by &lt;code&gt;padding_mode&lt;/code&gt; . Options are</target>
        </trans-unit>
        <trans-unit id="56cdf52769f875d95bfc8ef6e8310ccf2d0ab6ef" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;hop_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;hop_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9a42f5f55bf046653a704f6db77ba0e3241e7e2f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; has</target>
        </trans-unit>
        <trans-unit id="b713735765f53d252a6982eaad4ca993659b98bf" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has zero determinant, this returns &lt;code&gt;(0, -inf)&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; has zero determinant, this returns &lt;code&gt;(0, -inf)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e500b7aaaa41be4614fc8206c1a0e4e07b1a2886" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 1D of shape &lt;code&gt;(N)&lt;/code&gt;,</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is 1D of shape &lt;code&gt;(N)&lt;/code&gt; ,</target>
        </trans-unit>
        <trans-unit id="06376d4d271f3b9dcf5c2b5b33c61831ac5086a5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 2D of shape &lt;code&gt;(B, N)&lt;/code&gt;,</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is 2D of shape &lt;code&gt;(B, N)&lt;/code&gt; ,</target>
        </trans-unit>
        <trans-unit id="d721dee0fdf912ec6fe02d3c521f5fb3cf097977" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a</target>
        </trans-unit>
        <trans-unit id="333f1219fe75814d72dd2dbd93db4f7d9396ef8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="63f74a4f9eae20a71d7f307933ef2802201be33d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix with &lt;code&gt;m&lt;/code&gt; rows, &lt;code&gt;out&lt;/code&gt; is an matrix of shape</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a matrix with &lt;code&gt;m&lt;/code&gt; rows, &lt;code&gt;out&lt;/code&gt; is an matrix of shape</target>
        </trans-unit>
        <trans-unit id="27b6ec31f1f65b3c25640d5370bad846221e8b75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="40b5681668244b650aeff8654a98b80d07f3df6a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor</target>
        </trans-unit>
        <trans-unit id="35915a3eb4d935ad3d429e632e78a9274b0c797a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor with the elements of &lt;code&gt;input&lt;/code&gt; as the diagonal.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor with the elements of &lt;code&gt;input&lt;/code&gt; as the diagonal.</target>
        </trans-unit>
        <trans-unit id="ad879c1e17f47008543d08a003ad2cbc09fcfa1b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector, &lt;code&gt;out&lt;/code&gt; is a vector of size &lt;code&gt;num_samples&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is a vector, &lt;code&gt;out&lt;/code&gt; is a vector of size &lt;code&gt;num_samples&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5b37263956a979da36ed328c86d1f8d5bafac1ea" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is an n-dimensional tensor with size</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is an n-dimensional tensor with size</target>
        </trans-unit>
        <trans-unit id="f77fab43f2855d468661a78ecf375280629b91c9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;other&lt;/code&gt; should be a real number, otherwise it should be an integer</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt; , &lt;code&gt;other&lt;/code&gt; should be a real number, otherwise it should be an integer</target>
        </trans-unit>
        <trans-unit id="59214511303529b73d11d23823eff1c49a8f295c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; should be a real number, otherwise it should be an integer.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt; , &lt;code&gt;value&lt;/code&gt; should be a real number, otherwise it should be an integer.</target>
        </trans-unit>
        <trans-unit id="7935f4c92d76a69f5c3018eeffd91bc6bda0cd48" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, args &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt; , args &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; must be real numbers, otherwise they should be integers.</target>
        </trans-unit>
        <trans-unit id="74011e63fd6b8669cda7c1001070f1e13f3cf86f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;other&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="translated">If &lt;code&gt;input&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;other&lt;/code&gt; must be a real number, otherwise it should be an integer.</target>
        </trans-unit>
        <trans-unit id="fd07eaa51527d175519a582a0f6f85c473a1308f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, returns the loaded PyTorch extension as a Python module. If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; returns nothing (the shared library is loaded into the process as a side effect).</source>
          <target state="translated">If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , returns the loaded PyTorch extension as a Python module. If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; returns nothing (the shared library is loaded into the process as a side effect).</target>
        </trans-unit>
        <trans-unit id="150898b85fa40ffdbf63d6a8c5f8e7d9b416ba17" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim is ``True`&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim is ``True`&lt;/code&gt; , the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt; ), resulting in the output tensors having fewer dimension than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8dfb26e902cefe829de7730f64927f442131d727" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors are the same size as &lt;code&gt;input&lt;/code&gt;, except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors having 1 fewer dimension than the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors are the same size as &lt;code&gt;input&lt;/code&gt; , except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors having 1 fewer dimension than the &lt;code&gt;input&lt;/code&gt; tensor.</target>
        </trans-unit>
        <trans-unit id="4eef57842bb23530ec99ef24c142564e33a57efb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output dimensions are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimensions being reduced (&lt;code&gt;dim&lt;/code&gt; or all if &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;). If &lt;code&gt;q&lt;/code&gt; is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as &lt;code&gt;q&lt;/code&gt; which represents the quantiles.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output dimensions are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimensions being reduced ( &lt;code&gt;dim&lt;/code&gt; or all if &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; ) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;). If &lt;code&gt;q&lt;/code&gt; is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as &lt;code&gt;q&lt;/code&gt; which represents the quantiles.</target>
        </trans-unit>
        <trans-unit id="8af1f9e4f1fca763580351e3fbd0001f54ae795e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="71fae5aef8701d9d46f893cd636d6b67ec5e6aab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5a9d71d5cbb1ac9891b1a744240b0f98bb2945c1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 (or &lt;code&gt;len(dim)&lt;/code&gt;) fewer dimension(s).</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in the output tensor having 1 (or &lt;code&gt;len(dim)&lt;/code&gt; ) fewer dimension(s).</target>
        </trans-unit>
        <trans-unit id="7c2f11e03ffcd1caa4f43b25af60e0e5bd33899d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensors having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in the output tensors having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="21c9748f09f2547a22882ea8c179fcab7d38a94c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the outputs tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;), resulting in the outputs tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fd1cd26edf38a8fdee9967389f0e6dac29ec3c27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt; ), resulting in the output tensors having fewer dimensions than &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a5f40df338bf2f9f730257d36666627ccd6548fc" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.</source>
          <target state="translated">If &lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;dilation&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.</target>
        </trans-unit>
        <trans-unit id="55b84dbfa8c62b3f0cac777d20c854c0409394ae" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;largest&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the &lt;code&gt;k&lt;/code&gt; smallest elements are returned.</source>
          <target state="translated">If &lt;code&gt;largest&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the &lt;code&gt;k&lt;/code&gt; smallest elements are returned.</target>
        </trans-unit>
        <trans-unit id="fa798f0a6b2f55ce7a633ab807b16e5f24185685" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a &lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; object or a string containing a device tag, it indicates the location where all tensors should be loaded.</source>
          <target state="translated">If &lt;code&gt;map_location&lt;/code&gt; is a &lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; object or a string containing a device tag, it indicates the location where all tensors should be loaded.</target>
        </trans-unit>
        <trans-unit id="048f95b6581ac17eb4c969ed9d078d249d78f4f2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to &lt;code&gt;map_location&lt;/code&gt;. The builtin location tags are &lt;code&gt;'cpu'&lt;/code&gt; for CPU tensors and &lt;code&gt;'cuda:device_id'&lt;/code&gt; (e.g. &lt;code&gt;'cuda:2'&lt;/code&gt;) for CUDA tensors. &lt;code&gt;map_location&lt;/code&gt; should return either &lt;code&gt;None&lt;/code&gt; or a storage. If &lt;code&gt;map_location&lt;/code&gt; returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; will fall back to the default behavior, as if &lt;code&gt;map_location&lt;/code&gt; wasn&amp;rsquo;t specified.</source>
          <target state="translated">If &lt;code&gt;map_location&lt;/code&gt; is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to &lt;code&gt;map_location&lt;/code&gt; . The builtin location tags are &lt;code&gt;'cpu'&lt;/code&gt; for CPU tensors and &lt;code&gt;'cuda:device_id'&lt;/code&gt; (e.g. &lt;code&gt;'cuda:2'&lt;/code&gt; ) for CUDA tensors. &lt;code&gt;map_location&lt;/code&gt; should return either &lt;code&gt;None&lt;/code&gt; or a storage. If &lt;code&gt;map_location&lt;/code&gt; returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, &lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; will fall back to the default behavior, as if &lt;code&gt;map_location&lt;/code&gt; wasn&amp;rsquo;t specified.</target>
        </trans-unit>
        <trans-unit id="0e6bc544f9847053786078b7293a8d56bcd4734d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="translated">If &lt;code&gt;mat1&lt;/code&gt; is a</target>
        </trans-unit>
        <trans-unit id="42dc8bdc5d9263d6983656748538a841cde8189e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat&lt;/code&gt; is a</source>
          <target state="translated">If &lt;code&gt;mat&lt;/code&gt; is a</target>
        </trans-unit>
        <trans-unit id="ebf39946f09dcba3bcd6988a72fb252101844e11" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;modules&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="translated">If &lt;code&gt;modules&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt; , a &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</target>
        </trans-unit>
        <trans-unit id="9da2da81b60822905c169e5c07b984ccee82a21a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is negative, then the inverse of the matrix (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. For a batch of matrices, the batched inverse (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. If &lt;code&gt;n&lt;/code&gt; is 0, then an identity matrix is returned.</source>
          <target state="translated">If &lt;code&gt;n&lt;/code&gt; is negative, then the inverse of the matrix (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt; . For a batch of matrices, the batched inverse (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt; . If &lt;code&gt;n&lt;/code&gt; is 0, then an identity matrix is returned.</target>
        </trans-unit>
        <trans-unit id="7a5b0d2161214cfbfdadaec7bb8185cf215fb328" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is the number of dimensions in &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;x.T&lt;/code&gt; is equivalent to &lt;code&gt;x.permute(n-1, n-2, ..., 0)&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;n&lt;/code&gt; is the number of dimensions in &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;x.T&lt;/code&gt; is equivalent to &lt;code&gt;x.permute(n-1, n-2, ..., 0)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="64021abf601b2ef80dc11cb82f4cc12a6574725a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;nonlinearity&lt;/code&gt; is &lt;code&gt;&amp;lsquo;relu&amp;rsquo;&lt;/code&gt;, then ReLU is used in place of tanh.</source>
          <target state="translated">If &lt;code&gt;nonlinearity&lt;/code&gt; is &lt;code&gt;&amp;lsquo;relu&amp;rsquo;&lt;/code&gt; , then ReLU is used in place of tanh.</target>
        </trans-unit>
        <trans-unit id="14d766061a3de2fff996b1f5b1787217fd8c9c07" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalized&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default is &lt;code&gt;False&lt;/code&gt;), the function returns the normalized STFT results, i.e., multiplied by</source>
          <target state="translated">If &lt;code&gt;normalized&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default is &lt;code&gt;False&lt;/code&gt; ), the function returns the normalized STFT results, i.e., multiplied by</target>
        </trans-unit>
        <trans-unit id="00b342cfeb1bc29dc26ae492fe4fb871890e749d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;obj&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;script&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object. The returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;obj&lt;/code&gt; is a standalone function, a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; will be returned.</source>
          <target state="translated">If &lt;code&gt;obj&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt; , &lt;code&gt;script&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; object. The returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt; . If &lt;code&gt;obj&lt;/code&gt; is a standalone function, a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; will be returned.</target>
        </trans-unit>
        <trans-unit id="a63f6cedcd0f8aaa3b09629c749bc8a65b8e3242" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="translated">If &lt;code&gt;offset&lt;/code&gt; &amp;gt; 0, it is above the main diagonal.</target>
        </trans-unit>
        <trans-unit id="71789ffff3401f164bb9ff6ccd173e3d93708807" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="translated">If &lt;code&gt;offset&lt;/code&gt; &amp;lt; 0, it is below the main diagonal.</target>
        </trans-unit>
        <trans-unit id="68479729055a8e84ce055c5fd8fcdf5fc489269d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; = 0, it is the main diagonal.</source>
          <target state="translated">If &lt;code&gt;offset&lt;/code&gt; = 0, it is the main diagonal.</target>
        </trans-unit>
        <trans-unit id="4a6e80bcb7f10675f2ca57bde51217a2a4acefff" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default for real input), only values for</source>
          <target state="translated">If &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default for real input), only values for</target>
        </trans-unit>
        <trans-unit id="d55a2e244595c07c3e007024e25f845d2373549a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;other&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;alpha&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="translated">If &lt;code&gt;other&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;alpha&lt;/code&gt; must be a real number, otherwise it should be an integer.</target>
        </trans-unit>
        <trans-unit id="d24ea9f71cf8ff7e189e7b233b073ed7e74bfb09" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;output_size&lt;/code&gt;, &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.</source>
          <target state="translated">If &lt;code&gt;output_size&lt;/code&gt; , &lt;code&gt;kernel_size&lt;/code&gt; , &lt;code&gt;dilation&lt;/code&gt; , &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.</target>
        </trans-unit>
        <trans-unit id="bcbbd502f1fedcc0223634c589add98b568f5531" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly padded with negative infinity on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; is the stride between the elements within the sliding window. This &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of the pooling parameters.</source>
          <target state="translated">If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly padded with negative infinity on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; is the stride between the elements within the sliding window. This &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of the pooling parameters.</target>
        </trans-unit>
        <trans-unit id="a2d29b6dc8926940187a1fc2a87fbebaeec5e10f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on all three sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on all three sides for &lt;code&gt;padding&lt;/code&gt; number of points.</target>
        </trans-unit>
        <trans-unit id="a34bc02f3af80858de013a28d3f5075d29b733ee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</target>
        </trans-unit>
        <trans-unit id="0789404550f41a9218f1f8f6d2d8802206a399d3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; controls the spacing between the kernel points. It is harder to describe, but this &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of what &lt;code&gt;dilation&lt;/code&gt; does.</source>
          <target state="translated">If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; controls the spacing between the kernel points. It is harder to describe, but this &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of what &lt;code&gt;dilation&lt;/code&gt; does.</target>
        </trans-unit>
        <trans-unit id="5c75f70ae17fab5880e35d5beec80b653312b036" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;parameters&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="translated">If &lt;code&gt;parameters&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt; , a &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</target>
        </trans-unit>
        <trans-unit id="fa4de2a8b20f68604caa5fc5c42d01d653f7619e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;reduction&lt;/code&gt; is not &lt;code&gt;'none'&lt;/code&gt; (default &lt;code&gt;'mean'&lt;/code&gt;), then:</source>
          <target state="translated">If &lt;code&gt;reduction&lt;/code&gt; is not &lt;code&gt;'none'&lt;/code&gt; (default &lt;code&gt;'mean'&lt;/code&gt; ), then:</target>
        </trans-unit>
        <trans-unit id="e3f6cc94ec59a0b57a35c18da0e38bada6245d56" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_complex&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default if input is complex), the return is a &lt;code&gt;input.dim() + 1&lt;/code&gt; dimensional complex tensor. If &lt;code&gt;False&lt;/code&gt;, the output is a &lt;code&gt;input.dim() + 2&lt;/code&gt; dimensional real tensor where the last dimension represents the real and imaginary components.</source>
          <target state="translated">If &lt;code&gt;return_complex&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default if input is complex), the return is a &lt;code&gt;input.dim() + 1&lt;/code&gt; dimensional complex tensor. If &lt;code&gt;False&lt;/code&gt; , the output is a &lt;code&gt;input.dim() + 2&lt;/code&gt; dimensional real tensor where the last dimension represents the real and imaginary components.</target>
        </trans-unit>
        <trans-unit id="86cbdc2f08a338f85c6f8af902eddf736f286a0f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained indices tensor. Otherwise, this throws an error.</source>
          <target state="translated">If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained indices tensor. Otherwise, this throws an error.</target>
        </trans-unit>
        <trans-unit id="1a38a9c9f965f62380fce875a97d72dc47659bfb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained values tensor. Otherwise, this throws an error.</source>
          <target state="translated">If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained values tensor. Otherwise, this throws an error.</target>
        </trans-unit>
        <trans-unit id="95842db2195edf44c14e3fcd797605bf3a42bb61" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of dense dimensions. Otherwise, this throws an error.</source>
          <target state="translated">If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of dense dimensions. Otherwise, this throws an error.</target>
        </trans-unit>
        <trans-unit id="0bcf04d576bbe19078f6b28ab84fc731b5408840" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of sparse dimensions. Otherwise, this throws an error.</source>
          <target state="translated">If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of sparse dimensions. Otherwise, this throws an error.</target>
        </trans-unit>
        <trans-unit id="ca7f952167ad5a99c7190c4496d9c80f7e0a4624" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then memory is shared between all processes. All changes are written to the file. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the changes on the storage do not affect the file.</source>
          <target state="translated">If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then memory is shared between all processes. All changes are written to the file. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , then the changes on the storage do not affect the file.</target>
        </trans-unit>
        <trans-unit id="015e3e2de9fc9120162aae17d21cac0467229fd9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;sizedim&lt;/code&gt; is the size of dimension &lt;code&gt;dimension&lt;/code&gt; for &lt;code&gt;self&lt;/code&gt;, the size of dimension &lt;code&gt;dimension&lt;/code&gt; in the returned tensor will be &lt;code&gt;(sizedim - size) / step + 1&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;sizedim&lt;/code&gt; is the size of dimension &lt;code&gt;dimension&lt;/code&gt; for &lt;code&gt;self&lt;/code&gt; , the size of dimension &lt;code&gt;dimension&lt;/code&gt; in the returned tensor will be &lt;code&gt;(sizedim - size) / step + 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e63fb825587df089e2f9c8a5d5a13bc6f9e0bcfa" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of &lt;code&gt;input&lt;/code&gt; are &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;, then the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will contain only</source>
          <target state="translated">If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of &lt;code&gt;input&lt;/code&gt; are &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; , then the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will contain only</target>
        </trans-unit>
        <trans-unit id="7e496e3c20e80698b4576d97cd438ec7eb85163b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then this function returns the thin (reduced) QR factorization. Otherwise, if &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, this function returns the complete QR factorization.</source>
          <target state="translated">If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then this function returns the thin (reduced) QR factorization. Otherwise, if &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , this function returns the complete QR factorization.</target>
        </trans-unit>
        <trans-unit id="1ff1fe08ba98d042300c393b324450ec3743e612" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'github'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be of the form &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; with an optional tag/branch.</source>
          <target state="translated">If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'github'&lt;/code&gt; , &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be of the form &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; with an optional tag/branch.</target>
        </trans-unit>
        <trans-unit id="7ddb98294e27497103be69303e2d273125226255" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'local'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be a path to a local directory.</source>
          <target state="translated">If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'local'&lt;/code&gt; , &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be a path to a local directory.</target>
        </trans-unit>
        <trans-unit id="8e46fd64e2d63e21f45b5d33db7806f0957cdf7b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is a &lt;code&gt;Storage&lt;/code&gt;, the method sets the underlying storage, offset, size, and stride.</source>
          <target state="translated">If &lt;code&gt;source&lt;/code&gt; is a &lt;code&gt;Storage&lt;/code&gt; , the method sets the underlying storage, offset, size, and stride.</target>
        </trans-unit>
        <trans-unit id="cde3a8bb0ace5c85908b26369901162f2c89a71e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is a list, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into &lt;code&gt;len(split_size_or_sections)&lt;/code&gt; chunks with sizes in &lt;code&gt;dim&lt;/code&gt; according to &lt;code&gt;split_size_or_sections&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;split_size_or_sections&lt;/code&gt; is a list, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; will be split into &lt;code&gt;len(split_size_or_sections)&lt;/code&gt; chunks with sizes in &lt;code&gt;dim&lt;/code&gt; according to &lt;code&gt;split_size_or_sections&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a130a738825270ea8d38803841375a69a70f4847" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is an integer type, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;split_size&lt;/code&gt;.</source>
          <target state="translated">If &lt;code&gt;split_size_or_sections&lt;/code&gt; is an integer type, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;split_size&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cfc288bc3d95c2b0c35b45e151245ce8a50c4585" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</source>
          <target state="translated">If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; , this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</target>
        </trans-unit>
        <trans-unit id="12927d84c9ce948d1b220f95a6bfac95c27d8d68" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; , during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</target>
        </trans-unit>
        <trans-unit id="07322ca8d48ec2924a49a2e5be17cc42cbbed920" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;tracker&lt;/code&gt; sets &lt;code&gt;bvars[&amp;ldquo;force_stop&amp;rdquo;] = True&lt;/code&gt;, the iteration process will be hard-stopped.</source>
          <target state="translated">If &lt;code&gt;tracker&lt;/code&gt; sets &lt;code&gt;bvars[&amp;ldquo;force_stop&amp;rdquo;] = True&lt;/code&gt; , the iteration process will be hard-stopped.</target>
        </trans-unit>
        <trans-unit id="69139fd88bd2f170758129dcebe723655bc85791" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="translated">If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</target>
        </trans-unit>
        <trans-unit id="0054a72bc16c599350023586c7a4c1a0ef3cdabb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the variance will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="translated">If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , then the variance will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</target>
        </trans-unit>
        <trans-unit id="64c7abce64d159f698a9f17e0c36ec9abe8f3696" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;,</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; ,</target>
        </trans-unit>
        <trans-unit id="ea8581ad2e15c4757c07b341a196b7e3beb08404" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned matrix &lt;code&gt;L&lt;/code&gt; is lower-triangular, and the decomposition has the form:</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , the returned matrix &lt;code&gt;L&lt;/code&gt; is lower-triangular, and the decomposition has the form:</target>
        </trans-unit>
        <trans-unit id="64a08919bf7ea0ada24f0d42a92ac1bba95e2ff7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then lower triangular portion is used.</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; , then lower triangular portion is used.</target>
        </trans-unit>
        <trans-unit id="16f312504df9d82f1149f9aeb9d6c3a7744b7863" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; or not provided,</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; or not provided,</target>
        </trans-unit>
        <trans-unit id="0de37aa5dde3c55904d77c2e915431e5555caf52" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, and</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , and</target>
        </trans-unit>
        <trans-unit id="d54780fe04a147f268bb2ed2fa8ab8ca14bbc641" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the returned matrix &lt;code&gt;U&lt;/code&gt; is upper-triangular, and the decomposition has the form:</source>
          <target state="translated">If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , the returned matrix &lt;code&gt;U&lt;/code&gt; is upper-triangular, and the decomposition has the form:</target>
        </trans-unit>
        <trans-unit id="f0b760bb14c0f3a83b45512f0cb97f37a555bad1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;vec1&lt;/code&gt; is a vector of size &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; is a vector of size &lt;code&gt;m&lt;/code&gt;, then &lt;code&gt;input&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with a matrix of size</source>
          <target state="translated">경우 &lt;code&gt;vec1&lt;/code&gt; 이 크기의 벡터이고, &lt;code&gt;n&lt;/code&gt; 및 &lt;code&gt;vec2&lt;/code&gt; 크기의 벡터이고, &lt;code&gt;m&lt;/code&gt; 은 다음 &lt;code&gt;input&lt;/code&gt; 이어야 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; 크기의 행렬</target>
        </trans-unit>
        <trans-unit id="3a620298db8d1a210d3df0072bbfa22f1c6b3dc1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;win_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;n_fft&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;win_length&lt;/code&gt; 가 없습니다 &lt;code&gt;None&lt;/code&gt; (기본값), 그것은 동일로 처리됩니다 &lt;code&gt;n_fft&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e41ff352c71d6d3681ae49796d3eaff91163d87a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;window_length&lt;/code&gt; 인 경우</target>
        </trans-unit>
        <trans-unit id="5ba4029b8d23bd264ef58b8c6c6f2f2dbcc50773" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt; is one, then the returned window is a single element tensor containing a one.</source>
          <target state="translated">경우 &lt;code&gt;window_length&lt;/code&gt; 는 하나, 다음 반환 된 창은 하나를 포함하는 하나의 요소 텐서이다.</target>
        </trans-unit>
        <trans-unit id="36700c2c10655c29379465dfe1fd6e89caba46f9" translate="yes" xml:space="preserve">
          <source>If Statements</source>
          <target state="translated">if 문</target>
        </trans-unit>
        <trans-unit id="37f71555dd25e508495629f5b40797b269bbe017" translate="yes" xml:space="preserve">
          <source>If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs.</source>
          <target state="translated">True이면 내 보낸 그래프의 모든 이니셜 라이저 (일반적으로 매개 변수에 해당)도 그래프에 입력으로 추가됩니다. False이면 이니셜 라이저가 그래프에 입력으로 추가되지 않고 매개 변수가 아닌 입력 만 입력으로 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="a286c308254a9cdf1bc57c2c79179a955b02516d" translate="yes" xml:space="preserve">
          <source>If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</source>
          <target state="translated">단일 정수가 사용되는 경우 단일 목록으로 처리되며이 모듈은 해당 특정 크기로 예상되는 마지막 차원에 대해 정규화됩니다.</target>
        </trans-unit>
        <trans-unit id="6cc6559de482ee416e97f4d6264cba5f02719534" translate="yes" xml:space="preserve">
          <source>If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.</source>
          <target state="translated">차원이 0 인 텐서 피연산자가 차원이 지정된 피연산자보다 높은 범주를 갖는 경우 해당 범주의 모든 0 차원 텐서 피연산자를 보유하기에 충분한 크기와 범주를 가진 유형으로 승격합니다.</target>
        </trans-unit>
        <trans-unit id="3e818bb7352c628c52d3f8d4ba3508811d8bc58f" translate="yes" xml:space="preserve">
          <source>If any of these would help your use case, please &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22&quot;&gt;search if an issue has already been filed&lt;/a&gt; and if not, &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;file one&lt;/a&gt;.</source>
          <target state="translated">이 중 하나라도 사용 사례에 도움이되는 경우 &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22&quot;&gt;문제가 이미 제출되었는지 검색&lt;/a&gt; 하고 그렇지 않은 경우 제출 &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1080160de99f6c4b9ac7827f3fbead61bb81527f" translate="yes" xml:space="preserve">
          <source>If both arguments are 2-dimensional, the matrix-matrix product is returned.</source>
          <target state="translated">두 인수가 모두 2 차원이면 행렬-행렬 곱이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="d6fa8132ad71df4e876fdeaeb999a43daf3f4278" translate="yes" xml:space="preserve">
          <source>If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N &amp;gt; 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasted&lt;/a&gt; (and thus must be broadcastable). For example, if &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="translated">두 인수가 모두 1 차원 이상이고 하나 이상의 인수가 N 차원 (여기서 N&amp;gt; 2)이면 일괄 행렬 곱셈이 반환됩니다. 첫 번째 인수가 1 차원이면 배치 행렬 곱셈을 위해 차원 앞에 1이 추가되고 그 후에 제거됩니다. 두 번째 인수가 1 차원이면 배치 행렬 배수를 위해 차원에 1이 추가되고 이후에 제거됩니다. 매트릭스가 아닌 (예 : 배치) 차원이 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스트&lt;/a&gt; 되므로 브로드 캐스트 가능해야합니다. 예를 들어 &lt;code&gt;input&lt;/code&gt; 이</target>
        </trans-unit>
        <trans-unit id="d89fe56ed688f10e95f166f512f223aa5ad0f1fe" translate="yes" xml:space="preserve">
          <source>If both tensors are 1-dimensional, the dot product (scalar) is returned.</source>
          <target state="translated">두 텐서가 모두 1 차원이면 내적 (스칼라)이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="b74b95d5cdc716438937d19d93c000b85e6ccb8b" translate="yes" xml:space="preserve">
          <source>If checkpointed segment contains tensors detached from the computational graph by &lt;code&gt;detach()&lt;/code&gt; or &lt;code&gt;torch.no_grad()&lt;/code&gt;, the backward pass will raise an error. This is because &lt;code&gt;checkpoint&lt;/code&gt; makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the &lt;code&gt;checkpoint&lt;/code&gt; function.</source>
          <target state="translated">체크 포인트 세그먼트에 &lt;code&gt;detach()&lt;/code&gt; 또는 &lt;code&gt;torch.no_grad()&lt;/code&gt; 의해 계산 그래프에서 분리 된 텐서가 포함되어 있으면 역방향 패스에서 오류가 발생합니다. 이것은 &lt;code&gt;checkpoint&lt;/code&gt; 가 모든 출력에 기울기가 필요하게하여 텐서가 모델에 기울기가없는 것으로 정의 될 때 문제를 일으키는 원인 이되기 때문 입니다. 이를 피하려면 &lt;code&gt;checkpoint&lt;/code&gt; 함수 외부에서 텐서를 분리하십시오 .</target>
        </trans-unit>
        <trans-unit id="6148ee076450f28f4a8ddad929803f8c02667e14" translate="yes" xml:space="preserve">
          <source>If downloaded file is a zip file, it will be automatically decompressed.</source>
          <target state="translated">다운로드 한 파일이 zip 파일이면 자동으로 압축이 풀립니다.</target>
        </trans-unit>
        <trans-unit id="3632499e9221774229ac5b4849f51f9b7ea5baa6" translate="yes" xml:space="preserve">
          <source>If input has shape</source>
          <target state="translated">입력에 모양이있는 경우</target>
        </trans-unit>
        <trans-unit id="6054f9e157471bc8a7a27391680f78509c758f80" translate="yes" xml:space="preserve">
          <source>If it is &lt;code&gt;False&lt;/code&gt;, only eigenvalues are computed. If it is &lt;code&gt;True&lt;/code&gt;, both eigenvalues and eigenvectors are computed.</source>
          <target state="translated">이 경우 &lt;code&gt;False&lt;/code&gt; 만 고유 값이 계산된다. 이 경우 &lt;code&gt;True&lt;/code&gt; , 고유 값과 고유 벡터 모두 계산된다.</target>
        </trans-unit>
        <trans-unit id="c2b14334c440f3e0a03846297c6b858c19d59ebd" translate="yes" xml:space="preserve">
          <source>If neither is specified, &lt;code&gt;init_method&lt;/code&gt; is assumed to be &amp;ldquo;env://&amp;rdquo;.</source>
          <target state="translated">둘 다 지정되지 않은 경우 &lt;code&gt;init_method&lt;/code&gt; 는 &quot;env : //&quot;로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="8af7fa7360ca7c6df177260376e8748ca36e7b2e" translate="yes" xml:space="preserve">
          <source>If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module&amp;rsquo;s &lt;code&gt;_load_from_state_dict&lt;/code&gt; method can compare the version number and do appropriate changes if the state dict is from before the change.</source>
          <target state="translated">모듈에서 새 매개 변수 / 버퍼가 추가 / 제거되면이 번호가 범프되고 모듈의 &lt;code&gt;_load_from_state_dict&lt;/code&gt; 메서드는 버전 번호를 비교할 수 있으며 상태 dict가 변경 전의 것이라면 적절한 변경을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="35eb8efeed66172f49e69d3035c935e69cd09a9d" translate="yes" xml:space="preserve">
          <source>If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</source>
          <target state="translated">그렇지 않은 경우 대체없이 그려집니다. 즉, 행에 대해 샘플 인덱스를 그릴 때 해당 행에 대해 다시 그릴 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="8bad8088b028a7abefb044cb50129322a5504452" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.maximum&quot;&gt;&lt;code&gt;maximum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="translated">비교되는 요소 중 하나가 NaN이면 해당 요소가 반환됩니다. 복잡한 dtype이있는 텐서에는 &lt;a href=&quot;#torch.maximum&quot;&gt; &lt;code&gt;maximum()&lt;/code&gt; &lt;/a&gt; 이 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4d2edfb80af8b4c10b0daabd533575291d8f3436" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.minimum&quot;&gt;&lt;code&gt;minimum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="translated">비교되는 요소 중 하나가 NaN이면 해당 요소가 반환됩니다. &lt;a href=&quot;#torch.minimum&quot;&gt; &lt;code&gt;minimum()&lt;/code&gt; &lt;/a&gt; 는 복잡한 dtype이있는 텐서에 대해 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7af111fbf2a8bf97e37886e5888c11a588605b8f" translate="yes" xml:space="preserve">
          <source>If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="translated">제공되는 경우 선택적 인수 &lt;code&gt;weight&lt;/code&gt; 는 각 클래스에 가중치를 할당하는 1D Tensor 여야합니다. 이것은 균형이 맞지 않는 훈련 세트가있을 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="969ca4bbce09cb3097e3db4dcee3ccac4fa12d57" translate="yes" xml:space="preserve">
          <source>If replacement is &lt;code&gt;True&lt;/code&gt;, samples are drawn with replacement.</source>
          <target state="translated">replacement가 &lt;code&gt;True&lt;/code&gt; 이면 샘플이 교체되어 그려집니다.</target>
        </trans-unit>
        <trans-unit id="484e62a9e6c54b0aaa3a5007d8a946f20da7704f" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;repeats&lt;/code&gt; is &lt;code&gt;tensor([n1, n2, n3, &amp;hellip;])&lt;/code&gt;, then the output will be &lt;code&gt;tensor([0, 0, &amp;hellip;, 1, 1, &amp;hellip;, 2, 2, &amp;hellip;, &amp;hellip;])&lt;/code&gt; where &lt;code&gt;0&lt;/code&gt; appears &lt;code&gt;n1&lt;/code&gt; times, &lt;code&gt;1&lt;/code&gt; appears &lt;code&gt;n2&lt;/code&gt; times, &lt;code&gt;2&lt;/code&gt; appears &lt;code&gt;n3&lt;/code&gt; times, etc.</source>
          <target state="translated">경우] &lt;code&gt;repeats&lt;/code&gt; 있다 &lt;code&gt;tensor([n1, n2, n3, &amp;hellip;])&lt;/code&gt; , 출력 될 것이다 &lt;code&gt;tensor([0, 0, &amp;hellip;, 1, 1, &amp;hellip;, 2, 2, &amp;hellip;, &amp;hellip;])&lt;/code&gt; &lt;code&gt;0&lt;/code&gt; 나타날의 &lt;code&gt;n1&lt;/code&gt; 시간, &lt;code&gt;1&lt;/code&gt; 개 나타납니다 &lt;code&gt;n2&lt;/code&gt; 번, &lt;code&gt;2&lt;/code&gt; 가 나타납니다 &lt;code&gt;n3&lt;/code&gt; 등 시간을,</target>
        </trans-unit>
        <trans-unit id="5a469887feb69df132460464b530071b29b07521" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self.data&lt;/code&gt; Tensor already has the correct &lt;code&gt;torch.dtype&lt;/code&gt; and &lt;code&gt;torch.device&lt;/code&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, returns a copy with the desired configuration.</source>
          <target state="translated">는 IF &lt;code&gt;self.data&lt;/code&gt; 의 텐서 이미 올바른 가지고 &lt;code&gt;torch.dtype&lt;/code&gt; 및 &lt;code&gt;torch.device&lt;/code&gt; 을 다음 &lt;code&gt;self&lt;/code&gt; 반환됩니다. 그렇지 않으면 원하는 구성으로 복사본을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5d823313d98ebbfc4cb390dd8145252b2ceb1eaf" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self&lt;/code&gt; Tensor already has the correct &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, the returned tensor is a copy of &lt;code&gt;self&lt;/code&gt; with the desired &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">는 IF &lt;code&gt;self&lt;/code&gt; 텐서 이미 올바른 가지고 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; 을&lt;/a&gt; 다음 &lt;code&gt;self&lt;/code&gt; 반환됩니다. 그렇지 않으면 반환 된 텐서는 원하는 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 가있는 &lt;code&gt;self&lt;/code&gt; 의 복사본입니다 .</target>
        </trans-unit>
        <trans-unit id="6b8c8166f7ac4def7f8a41f990435d7b737da1e1" translate="yes" xml:space="preserve">
          <source>If the RNN is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="translated">RNN이 양방향이면 num_directions는 2 여야하고 그렇지 않으면 1이어야합니다.</target>
        </trans-unit>
        <trans-unit id="191802e65e024d3ca3eb1bca5bd908ca2613d6eb" translate="yes" xml:space="preserve">
          <source>If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.</source>
          <target state="translated">현재 노드가 소유자 인 경우 로컬 값에 대한 참조를 반환합니다. 그렇지 않으면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="9bc28fe59df6be023786ded1b0d7a664101bb1a6" translate="yes" xml:space="preserve">
          <source>If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.</source>
          <target state="translated">첫 번째 인수가 1 차원이고 두 번째 인수가 2 차원 인 경우 행렬 곱셈을 위해 차원 앞에 1이 추가됩니다. 행렬이 곱해진 후 앞에 추가 된 차원이 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="805768ae7e7883adbd4eb9cfd347ff7553b939ce" translate="yes" xml:space="preserve">
          <source>If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.</source>
          <target state="translated">첫 번째 인수가 2 차원이고 두 번째 인수가 1 차원이면 행렬-벡터 곱이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="603aa447ecbcf1ce44b939cf59f78b72d62d8d4f" translate="yes" xml:space="preserve">
          <source>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype &lt;code&gt;torch.float16&lt;/code&gt; 4) V100 GPU is used, 5) input data is not in &lt;code&gt;PackedSequence&lt;/code&gt; format persistent algorithm can be selected to improve performance.</source>
          <target state="translated">다음 조건이 충족되는 경우 : 1) cudnn이 활성화 됨, 2) 입력 데이터가 GPU에 있음 3) 입력 데이터가 dtype &lt;code&gt;torch.float16&lt;/code&gt; 에 있음 4) V100 GPU가 사용됨, 5) 입력 데이터가 &lt;code&gt;PackedSequence&lt;/code&gt; 형식 이 아님 지속적 알고리즘이 가능합니다. 성능 향상을 위해 선택되었습니다.</target>
        </trans-unit>
        <trans-unit id="787caec1c53ac744f6704113ef9317970b83ba1d" translate="yes" xml:space="preserve">
          <source>If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function &lt;code&gt;_scalar&lt;/code&gt; can convert a scalar tensor into a python scalar, and &lt;code&gt;_if_scalar_type_as&lt;/code&gt; can turn a Python scalar into a PyTorch tensor.</source>
          <target state="translated">입력 인수가 텐서이지만 ONNX가 스칼라를 요청하는 경우 명시 적으로 변환을 수행해야합니다. 도우미 함수 &lt;code&gt;_scalar&lt;/code&gt; 는 스칼라 텐서를 python 스칼라로 변환 할 수 있으며 &lt;code&gt;_if_scalar_type_as&lt;/code&gt; 는 Python 스칼라를 PyTorch 텐서로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f988049101ff18d4e7918e66cf5ad5885aec7070" translate="yes" xml:space="preserve">
          <source>If the norm of a row is lower than &lt;code&gt;maxnorm&lt;/code&gt;, the row is unchanged</source>
          <target state="translated">행의 노름이 &lt;code&gt;maxnorm&lt;/code&gt; 보다 낮 으면 행은 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d44fada1fe05f824e96651b70e1c4a292ba8a891" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">객체가 이미 &lt;code&gt;model_dir&lt;/code&gt; 에있는 경우 역 직렬화되고 반환됩니다. &lt;code&gt;model_dir&lt;/code&gt; 의 기본값 은 &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; 입니다. 여기서 &lt;code&gt;hub_dir&lt;/code&gt; 은 &lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 디렉토리 입니다.</target>
        </trans-unit>
        <trans-unit id="572431e231db065e224abcc587fb2e502c2edb94" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;hub#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">객체가 이미 &lt;code&gt;model_dir&lt;/code&gt; 에있는 경우 역 직렬화되고 반환됩니다. &lt;code&gt;model_dir&lt;/code&gt; 의 기본값 은 &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; 입니다. 여기서 &lt;code&gt;hub_dir&lt;/code&gt; 은 &lt;a href=&quot;hub#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 디렉토리 입니다.</target>
        </trans-unit>
        <trans-unit id="e07c64dd92353460c82bc7c43c0298ff2498e3ef" translate="yes" xml:space="preserve">
          <source>If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions:</source>
          <target state="translated">연산자가 비 ATEN 연산자 인 경우 해당 PyTorch 함수 클래스에 기호 함수를 추가해야합니다. 다음 지침을 읽으십시오.</target>
        </trans-unit>
        <trans-unit id="8de0216c16416c65797272e400b10794b950ac17" translate="yes" xml:space="preserve">
          <source>If the operator is an ATen operator, which means you can find the declaration of the function in &lt;code&gt;torch/csrc/autograd/generated/VariableType.h&lt;/code&gt; (available in generated code in PyTorch install dir), you should add the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; and follow the instructions listed as below:</source>
          <target state="translated">운영자는 당신이 함수의 선언을 찾을 수있는 의미 ATEN 운영자 인 경우 &lt;code&gt;torch/csrc/autograd/generated/VariableType.h&lt;/code&gt; (PyTorch에서 생성 된 코드에서 사용할 수있는 디렉토리 설치), 당신이 상징적 기능을 추가해야합니다 &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; 하고 아래 나열된 지침을 따릅니다.</target>
        </trans-unit>
        <trans-unit id="08987c5581da900d9050fec1b266346dd018ed86" translate="yes" xml:space="preserve">
          <source>If the sum to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.</source>
          <target state="translated">&lt;code&gt;p&lt;/code&gt; 제곱의 합 이 0이면이 함수의 기울기가 정의되지 않습니다. 이 구현은이 경우 그라디언트를 0으로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="a6455b98cbf68564852f3d65be9e3d85724ba33b" translate="yes" xml:space="preserve">
          <source>If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.</source>
          <target state="translated">대상이 개별 대상의 연결 인 1d 텐서로 제공되는 경우 target_lengths는 텐서의 총 길이에 합산되어야합니다.</target>
        </trans-unit>
        <trans-unit id="eb673583185e53bcc86746d0d9978c1392f01670" translate="yes" xml:space="preserve">
          <source>If the tensor has a batch dimension of size 1, then &lt;code&gt;squeeze(input)&lt;/code&gt; will also remove the batch dimension, which can lead to unexpected errors.</source>
          <target state="translated">텐서의 배치 차원이 1 인 경우 &lt;code&gt;squeeze(input)&lt;/code&gt; 도 배치 차원을 제거하므로 예기치 않은 오류가 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="67a5ac190ccd238f6283765db614cb7a1d643567" translate="yes" xml:space="preserve">
          <source>If the torch.fft module is imported then &amp;ldquo;torch.fft&amp;rdquo; will refer to the module and not this function. Use &lt;a href=&quot;../tensors#torch.Tensor.fft&quot;&gt;&lt;code&gt;torch.Tensor.fft()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">torch.fft 모듈을 가져온 경우 &quot;torch.fft&quot;는이 기능이 아닌 모듈을 참조합니다. 대신 &lt;a href=&quot;../tensors#torch.Tensor.fft&quot;&gt; &lt;code&gt;torch.Tensor.fft()&lt;/code&gt; &lt;/a&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="d413b6561145ebb882d59d64e1d5bb494eb94bfd" translate="yes" xml:space="preserve">
          <source>If the type of a scalar operand is of a higher category than tensor operands (where complex &amp;gt; floating &amp;gt; integral &amp;gt; boolean), we promote to a type with sufficient size to hold all scalar operands of that category.</source>
          <target state="translated">스칼라 피연산자의 유형이 텐서 피연산자 (복합&amp;gt; 부동&amp;gt; 적분&amp;gt; 부울)보다 높은 범주이면 해당 범주의 모든 스칼라 피연산자를 보유하기에 충분한 크기를 가진 유형으로 승격합니다.</target>
        </trans-unit>
        <trans-unit id="1fdbc94ec5c6bbfaeab5c1df5a93f67f15cfdc84" translate="yes" xml:space="preserve">
          <source>If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.</source>
          <target state="translated">축소 된 행에 여러 최대 값이있는 경우 첫 번째 최대 값의 인덱스가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="dffbe07ea95c30719f442abdb52cf12ce069da58" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.</source>
          <target state="translated">축소 된 행에 최소값이 여러 개 있으면 첫 번째 최소값의 인덱스가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="c351680cf4894b25f49425f0d4d516d6d33a16ff" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values then the indices of the first minimal value are returned.</source>
          <target state="translated">최소값이 여러 개인 경우 첫 번째 최소값의 인덱스가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="7f03b96d2971ec564ed4f31e34caa43003898f1d" translate="yes" xml:space="preserve">
          <source>If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</source>
          <target state="translated">더 높은 범주의 0 차원 피연산자가없는 경우 모든 차원의 피연산자를 보유하기에 충분한 크기와 범주를 가진 유형으로 승격합니다.</target>
        </trans-unit>
        <trans-unit id="bc9e3883be4333b3f7486d77ab3655dd30a42ad7" translate="yes" xml:space="preserve">
          <source>If this is already of the correct type, no copy is performed and the original object is returned.</source>
          <target state="translated">이것이 이미 올바른 유형이면 복사가 수행되지 않고 원래 객체가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="ce68525cc568e97b080be8925035deeb766a0ee2" translate="yes" xml:space="preserve">
          <source>If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="translated">이 개체가 이미 CPU 메모리와 올바른 장치에있는 경우 복사가 수행되지 않고 원래 개체가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6788146c6f952e177f58c27fec835e0c0d256c8a" translate="yes" xml:space="preserve">
          <source>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="translated">이 개체가 이미 CUDA 메모리에 있고 올바른 장치에있는 경우 복사가 수행되지 않고 원래 개체가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="7c50d408ff8252bcf7756ca41661606361f0e20e" translate="yes" xml:space="preserve">
          <source>If x1 has shape</source>
          <target state="translated">x1에 모양이있는 경우</target>
        </trans-unit>
        <trans-unit id="c21dcebc3506f3faeef2282027b5db5536bb6293" translate="yes" xml:space="preserve">
          <source>If you are profiling CUDA code, the first profiler that &lt;code&gt;bottleneck&lt;/code&gt; runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.</source>
          <target state="translated">CUDA 코드를 프로파일 링하는 경우 &lt;code&gt;bottleneck&lt;/code&gt; 실행 되는 첫 번째 프로파일 러 (cProfile)는 시간보고에 CUDA 시작 시간 (CUDA 버퍼 할당 비용)을 포함합니다. 병목 현상으로 인해 코드가 CUDA 시작 시간보다 훨씬 느린 경우에는 문제가되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bb09e45c509c395b596fc5d452b50e8440b313fe" translate="yes" xml:space="preserve">
          <source>If you are using DistributedDataParallel in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;, you should always use &lt;a href=&quot;../rpc#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; to compute gradients and &lt;a href=&quot;../rpc#torch.distributed.optim.DistributedOptimizer&quot;&gt;&lt;code&gt;torch.distributed.optim.DistributedOptimizer&lt;/code&gt;&lt;/a&gt; for optimizing parameters.</source>
          <target state="translated">&lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt; 와 함께 DistributedDataParallel을 사용하는 경우 항상 &lt;a href=&quot;../rpc#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt; 를 사용하여 그래디언트를 계산하고 &lt;a href=&quot;../rpc#torch.distributed.optim.DistributedOptimizer&quot;&gt; &lt;code&gt;torch.distributed.optim.DistributedOptimizer&lt;/code&gt; 를 사용&lt;/a&gt; 하여 매개 변수를 최적화해야합니다.</target>
        </trans-unit>
        <trans-unit id="79878b7e0f390005d12ab18a0ff2e5e3bf2df40c" translate="yes" xml:space="preserve">
          <source>If you have more than one GPU on each node, when using the NCCL and Gloo backend, &lt;a href=&quot;#torch.distributed.broadcast_multigpu&quot;&gt;&lt;code&gt;broadcast_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.reduce_multigpu&quot;&gt;&lt;code&gt;reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_gather_multigpu&quot;&gt;&lt;code&gt;all_gather_multigpu()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.reduce_scatter_multigpu&quot;&gt;&lt;code&gt;reduce_scatter_multigpu()&lt;/code&gt;&lt;/a&gt; support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.</source>
          <target state="translated">각 노드에 GPU가 두 개 이상있는 경우 NCCL 및 Gloo 백엔드를 사용할 때 &lt;a href=&quot;#torch.distributed.broadcast_multigpu&quot;&gt; &lt;code&gt;broadcast_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt; &lt;code&gt;all_reduce_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.reduce_multigpu&quot;&gt; &lt;code&gt;reduce_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_gather_multigpu&quot;&gt; &lt;code&gt;all_gather_multigpu()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributed.reduce_scatter_multigpu&quot;&gt; &lt;code&gt;reduce_scatter_multigpu()&lt;/code&gt; &lt;/a&gt; 는 각 노드 내의 여러 GPU간에 분산 된 집합 작업을 지원합니다. 이러한 함수는 잠재적으로 전체 분산 학습 성능을 향상시킬 수 있으며 텐서 목록을 전달하여 쉽게 사용할 수 있습니다. 전달 된 텐서 목록의 각 Tensor는 함수가 호출되는 호스트의 별도 GPU 장치에 있어야합니다. 텐서 목록의 길이는 모든 분산 된 프로세스에서 동일해야합니다. 또한 현재 다중 GPU 집합 기능은 NCCL 백엔드에서만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="136147687478e135ab5df6494e85fb08cdedac29" translate="yes" xml:space="preserve">
          <source>If you plan on using this module with a &lt;code&gt;nccl&lt;/code&gt; backend or a &lt;code&gt;gloo&lt;/code&gt; backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to &lt;code&gt;forkserver&lt;/code&gt; (Python 3 only) or &lt;code&gt;spawn&lt;/code&gt;. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don&amp;rsquo;t change this setting.</source>
          <target state="translated">여러 워커를 사용하는 DataLoader와 함께 &lt;code&gt;nccl&lt;/code&gt; 백엔드 또는 &lt;code&gt;gloo&lt;/code&gt; 백엔드 (Infiniband 사용) 와 함께이 모듈을 사용하려는 경우 다중 처리 시작 방법을 &lt;code&gt;forkserver&lt;/code&gt; (Python 3 전용) 또는 &lt;code&gt;spawn&lt;/code&gt; 으로 변경하십시오 . 불행히도 Gloo (Infiniband를 사용)와 NCCL2는 포크에 안전하지 않으며이 설정을 변경하지 않으면 교착 상태가 발생할 가능성이 있습니다.</target>
        </trans-unit>
        <trans-unit id="07ab34f8fafef28d79bd246f9789bf5105b86f8b" translate="yes" xml:space="preserve">
          <source>If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first</source>
          <target state="translated">QR을 통해 역 전파하려는 경우 현재 역방향 구현은 첫 번째</target>
        </trans-unit>
        <trans-unit id="1cd7b6977f27c5a4e9182f95ff2f859a518a8917" translate="yes" xml:space="preserve">
          <source>If you use &lt;code&gt;torch.save&lt;/code&gt; on one process to checkpoint the module, and &lt;code&gt;torch.load&lt;/code&gt; on some other processes to recover it, make sure that &lt;code&gt;map_location&lt;/code&gt; is configured properly for every process. Without &lt;code&gt;map_location&lt;/code&gt;, &lt;code&gt;torch.load&lt;/code&gt; would recover the module to devices where the module was saved from.</source>
          <target state="translated">한 프로세스에서 &lt;code&gt;torch.save&lt;/code&gt; 를 사용 하여 모듈을 검사하고 다른 프로세스에서 &lt;code&gt;torch.load&lt;/code&gt; 를 사용하여 복구하는 경우 &lt;code&gt;map_location&lt;/code&gt; 이 모든 프로세스에 대해 올바르게 구성되었는지 확인하십시오 . 없이 &lt;code&gt;map_location&lt;/code&gt; , &lt;code&gt;torch.load&lt;/code&gt; 는 모듈이에서 저장 한 장치 모듈을 복구하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="74c65669b6d4e5ce564cda9931810dc0c170c246" translate="yes" xml:space="preserve">
          <source>If you want downsampling/general resizing, you should use &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="translated">다운 샘플링 / 일반 크기 조정을 원하면 &lt;code&gt;interpolate()&lt;/code&gt; 를 사용해야합니다 .</target>
        </trans-unit>
        <trans-unit id="064ec7b53383c8d17c5751c9a34c0ac9daf7e8cd" translate="yes" xml:space="preserve">
          <source>If you&amp;rsquo;re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3&lt;/code&gt;. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.</source>
          <target state="translated">Gloo 백엔드를 사용하는 경우 다음과 같이 쉼표로 구분하여 여러 인터페이스를 지정할 수 있습니다. &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3&lt;/code&gt; . 백엔드는 이러한 인터페이스에서 라운드 로빈 방식으로 작업을 디스패치합니다. 모든 프로세스는이 변수에 동일한 수의 인터페이스를 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="40a9a99bd41d3dd35eb2ee9f7f247c013023dc94" translate="yes" xml:space="preserve">
          <source>If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.</source>
          <target state="translated">InfiniBand에서 IB를 통한 IP를 활성화 한 경우 Gloo를 사용하고, 그렇지 않으면 MPI를 대신 사용하십시오. 우리는 다음 릴리스에서 Gloo에 대한 InfiniBand 지원을 추가 할 계획입니다.</target>
        </trans-unit>
        <trans-unit id="7ea2434c9b9d3a93397a8c29208e88de7aacbec3" translate="yes" xml:space="preserve">
          <source>If your use case is always 1-D sorted sequence, &lt;a href=&quot;torch.bucketize#torch.bucketize&quot;&gt;&lt;code&gt;torch.bucketize()&lt;/code&gt;&lt;/a&gt; is preferred, because it has fewer dimension checks resulting in slightly better performance.</source>
          <target state="translated">사용 사례가 항상 1D 정렬 시퀀스 인 경우 &lt;a href=&quot;torch.bucketize#torch.bucketize&quot;&gt; &lt;code&gt;torch.bucketize()&lt;/code&gt; &lt;/a&gt; 가 선호됩니다. 그 이유는 차원 검사 수가 적어 성능이 약간 향상되기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="eec64bb7b6af56628b6864bb7faf7664a4d5fe90" translate="yes" xml:space="preserve">
          <source>Ignoring the optional batch dimension, this method computes the following expression:</source>
          <target state="translated">선택적 배치 차원을 무시하고이 메서드는 다음 식을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d5e7e82cbad5c6c7282eb3f91a5d122dc9858f8e" translate="yes" xml:space="preserve">
          <source>ImageNet 1-crop error rates (224x224)</source>
          <target state="translated">ImageNet 1 자르기 오류율 (224x224)</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="7aa6ad27b14b39704b337bb5273a54caf43dd27d" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt;</source>
          <target state="translated">구현 세부 정보 : &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e2301206e5db52d9fb3a1373c4f2d128cbca7b09" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt; section 3.2.2</source>
          <target state="translated">구현 세부 정보 : &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt; 섹션 3.2.2</target>
        </trans-unit>
        <trans-unit id="123ad8e975b9151926be8910a7ca29c06e0a08b2" translate="yes" xml:space="preserve">
          <source>Implementing a Parameter Server using Distributed RPC Framework</source>
          <target state="translated">분산 RPC 프레임 워크를 사용하여 매개 변수 서버 구현</target>
        </trans-unit>
        <trans-unit id="44171a5d0f5320fd21c59c6c6516557d63659e70" translate="yes" xml:space="preserve">
          <source>Implementing batch RPC processing</source>
          <target state="translated">일괄 RPC 처리 구현</target>
        </trans-unit>
        <trans-unit id="9f29957e1ffbfbf9022787fc2c7cfe246141046c" translate="yes" xml:space="preserve">
          <source>Implements data parallelism at the module level.</source>
          <target state="translated">모듈 수준에서 데이터 병렬 처리를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="247ad11e0e8ab7f7233963e6adbe7062bce23a44" translate="yes" xml:space="preserve">
          <source>Implements distributed data parallelism that is based on &lt;code&gt;torch.distributed&lt;/code&gt; package at the module level.</source>
          <target state="translated">모듈 수준에서 &lt;code&gt;torch.distributed&lt;/code&gt; 패키지를 기반으로하는 분산 데이터 병렬 처리를 구현 합니다.</target>
        </trans-unit>
        <trans-unit id="1ac98376b8dde644a48731cdda5a6c2f0a79cf77" translate="yes" xml:space="preserve">
          <source>Important Notice</source>
          <target state="translated">중요 공지</target>
        </trans-unit>
        <trans-unit id="dbea8ebe841db621fdf156d683ca479b155b0a0f" translate="yes" xml:space="preserve">
          <source>Important consideration in the parameters &lt;code&gt;window&lt;/code&gt; and &lt;code&gt;center&lt;/code&gt; so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically,</source>
          <target state="translated">중요한 매개 변수의 고려 &lt;code&gt;window&lt;/code&gt; 및 &lt;code&gt;center&lt;/code&gt; 그래서 모든 윈도우의 요약에 의해 생성 된 봉투는 결코 제로 시간에 특정 시점에서 없다는 것을. 구체적으로 특별히,</target>
        </trans-unit>
        <trans-unit id="1546cf194023a313eda4a32e23fe8da2e85133d5" translate="yes" xml:space="preserve">
          <source>In a multilayer GRU, the input</source>
          <target state="translated">다중 계층 GRU에서 입력</target>
        </trans-unit>
        <trans-unit id="4ca4b7a5c3d05548dd2f62668a8daeab0e9ec06f" translate="yes" xml:space="preserve">
          <source>In a multilayer LSTM, the input</source>
          <target state="translated">다층 LSTM에서 입력</target>
        </trans-unit>
        <trans-unit id="e20dc20835fec339c1d3cc16c346bd578e3d90d9" translate="yes" xml:space="preserve">
          <source>In addition to bools, floats, ints, and Tensors can be used in a conditional and will be implicitly casted to a boolean.</source>
          <target state="translated">부울 외에도 부동 소수점, 정수 및 텐서는 조건부에서 사용할 수 있으며 암시 적으로 부울로 캐스팅됩니다.</target>
        </trans-unit>
        <trans-unit id="14b2f80199ae20e9c340149a30176ab138c9610a" translate="yes" xml:space="preserve">
          <source>In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (&lt;code&gt;--nproc_per_node&lt;/code&gt;). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (&lt;code&gt;nproc_per_node&lt;/code&gt;), and each process will be operating on a single GPU from &lt;em&gt;GPU 0 to GPU (nproc_per_node - 1)&lt;/em&gt;.</source>
          <target state="translated">단일 노드 분산 학습 또는 다중 노드 분산 학습의 두 경우 모두이 유틸리티는 노드 당 주어진 수의 프로세스를 시작합니다 ( &lt;code&gt;--nproc_per_node&lt;/code&gt; ). GPU 학습에 사용되는 경우이 숫자는 현재 시스템의 GPU 수 ( &lt;code&gt;nproc_per_node&lt;/code&gt; ) 보다 적거나 같아야 하며 각 프로세스는 &lt;em&gt;GPU 0부터 GPU (nproc_per_node-1)&lt;/em&gt; 까지 단일 GPU에서 작동합니다 .</target>
        </trans-unit>
        <trans-unit id="0e876cc3bbe05952d2705985bee305b58baf29ac" translate="yes" xml:space="preserve">
          <source>In cases like these, tracing would not be appropriate and &lt;a href=&quot;torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;scripting&lt;/code&gt;&lt;/a&gt; is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.</source>
          <target state="translated">이러한 경우 추적은 적절하지 않으며 &lt;a href=&quot;torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;scripting&lt;/code&gt; &lt;/a&gt; 이 더 나은 선택입니다. 이러한 모델을 추적하면 후속 모델 호출에서 자동으로 잘못된 결과를 얻을 수 있습니다. 추적 프로그램은 잘못된 추적을 생성 할 수있는 작업을 수행 할 때 경고를 내보내려고합니다.</target>
        </trans-unit>
        <trans-unit id="86e76c5e72e3158e53103708c732b0ca2fb77314" translate="yes" xml:space="preserve">
          <source>In default &lt;code&gt;reduction&lt;/code&gt; mode &lt;code&gt;'mean'&lt;/code&gt;, the losses are averaged for each minibatch over observations &lt;strong&gt;as well as&lt;/strong&gt; over dimensions. &lt;code&gt;'batchmean'&lt;/code&gt; mode gives the correct KL divergence where losses are averaged over batch dimension only. &lt;code&gt;'mean'&lt;/code&gt; mode&amp;rsquo;s behavior will be changed to the same as &lt;code&gt;'batchmean'&lt;/code&gt; in the next major release.</source>
          <target state="translated">기본 &lt;code&gt;reduction&lt;/code&gt; 모드 &lt;code&gt;'mean'&lt;/code&gt; 에서 손실은 관측치 &lt;strong&gt;및&lt;/strong&gt; 차원에 대한 각 미니 배치에 대해 평균화됩니다 . &lt;code&gt;'batchmean'&lt;/code&gt; 모드는 손실이 배치 차원에 대해서만 평균화되는 올바른 KL 차이를 제공합니다. &lt;code&gt;'mean'&lt;/code&gt; 모드의 동작은 다음 주요 릴리스에서 &lt;code&gt;'batchmean'&lt;/code&gt; 평균' 과 동일하게 변경됩니다 .</target>
        </trans-unit>
        <trans-unit id="d7469a9738cf72345e41f4a9561f6407dd3c1952" translate="yes" xml:space="preserve">
          <source>In each forward, &lt;code&gt;module&lt;/code&gt; is &lt;strong&gt;replicated&lt;/strong&gt; on each device, so any updates to the running module in &lt;code&gt;forward&lt;/code&gt; will be lost. For example, if &lt;code&gt;module&lt;/code&gt; has a counter attribute that is incremented in each &lt;code&gt;forward&lt;/code&gt;, it will always stay at the initial value because the update is done on the replicas which are destroyed after &lt;code&gt;forward&lt;/code&gt;. However, &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; guarantees that the replica on &lt;code&gt;device[0]&lt;/code&gt; will have its parameters and buffers sharing storage with the base parallelized &lt;code&gt;module&lt;/code&gt;. So &lt;strong&gt;in-place&lt;/strong&gt; updates to the parameters or buffers on &lt;code&gt;device[0]&lt;/code&gt; will be recorded. E.g., &lt;a href=&quot;torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm&quot;&gt;&lt;code&gt;spectral_norm()&lt;/code&gt;&lt;/a&gt; rely on this behavior to update the buffers.</source>
          <target state="translated">각 포워드에서 &lt;code&gt;module&lt;/code&gt; 은 각 장치에 &lt;strong&gt;복제&lt;/strong&gt; 되므로 &lt;code&gt;forward&lt;/code&gt; 실행중인 모듈에 대한 모든 업데이트 가 손실됩니다. 예를 들어 &lt;code&gt;module&lt;/code&gt; &lt;code&gt;forward&lt;/code&gt; 마다 증가하는 카운터 속성이있는 경우 forward 이후에 삭제되는 복제본에서 업데이트가 수행되기 때문에 항상 초기 값을 유지 &lt;code&gt;forward&lt;/code&gt; . 그러나 &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt; 은 &lt;code&gt;device[0]&lt;/code&gt; 의 복제본 이 기본 병렬 &lt;code&gt;module&lt;/code&gt; 과 스토리지를 공유하는 매개 변수 및 버퍼를 갖도록 보장합니다 . 따라서 &lt;code&gt;device[0]&lt;/code&gt; 의 매개 변수 또는 버퍼에 대한 &lt;strong&gt;내부&lt;/strong&gt; 업데이트 가 기록됩니다. 예 :&lt;a href=&quot;torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt; 및&lt;a href=&quot;torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm&quot;&gt; &lt;code&gt;spectral_norm()&lt;/code&gt; &lt;/a&gt; 은이 동작에 의존하여 버퍼를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="c4c7ae6e45efeeb88e3de47cc32025f3afeae3d6" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="translated">일반적으로 접기 및 펼치기 작업은 다음과 같이 관련됩니다. 동일한 매개 변수로 생성 된 &lt;a href=&quot;#torch.nn.Fold&quot;&gt; &lt;code&gt;Fold&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt; 인스턴스를 고려하십시오 .</target>
        </trans-unit>
        <trans-unit id="14a82233b9f46d949d131d9066b4b1e345f4a14b" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;torch.nn.fold#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="translated">일반적으로 접기 및 펼치기 작업은 다음과 같이 관련됩니다. 동일한 매개 변수로 생성 된 &lt;a href=&quot;torch.nn.fold#torch.nn.Fold&quot;&gt; &lt;code&gt;Fold&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt; 인스턴스를 고려하십시오 .</target>
        </trans-unit>
        <trans-unit id="1855810e3cc280e259c5b815913beac56d322de4" translate="yes" xml:space="preserve">
          <source>In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.</source>
          <target state="translated">일반적으로 기본 방법은 반복 당 최소 시간을 소비합니다. 그러나 강력한 방법은 훨씬 더 빠르게 수렴되고 더 안정적입니다. 따라서 일반적으로 기본 방법을 사용하는 것은 권장되지 않지만 기본 방법을 사용하는 것이 선호되는 경우가 있습니다.</target>
        </trans-unit>
        <trans-unit id="5a8901639688080e9ddf9086d6c38d66aa6d29ff" translate="yes" xml:space="preserve">
          <source>In general, use the full-rank SVD implementation &lt;code&gt;torch.svd&lt;/code&gt; for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that &lt;code&gt;torch.svd&lt;/code&gt; cannot handle.</source>
          <target state="translated">일반적으로 10 배 더 높은 성능 특성으로 인해 고밀도 매트릭스에 대해 전체 등급 SVD 구현 &lt;code&gt;torch.svd&lt;/code&gt; 를 사용합니다 . 낮은 순위의 SVD는 &lt;code&gt;torch.svd&lt;/code&gt; 가 처리 할 수없는 거대한 희소 행렬에 유용합니다 .</target>
        </trans-unit>
        <trans-unit id="809a8a0c78ca14c8503127d00dd103ee7f6c80b0" translate="yes" xml:space="preserve">
          <source>In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.</source>
          <target state="translated">대부분의 경우 추적 또는 스크립팅은 모델을 TorchScript로 변환하는 더 쉬운 접근 방식입니다. 모델 일부의 특정 요구 사항에 맞게 추적 및 스크립팅을 구성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6485ce8afe7564cea5c8362029901ba50e16aea8" translate="yes" xml:space="preserve">
          <source>In order to spawn up multiple processes per node, you can use either &lt;code&gt;torch.distributed.launch&lt;/code&gt; or &lt;code&gt;torch.multiprocessing.spawn&lt;/code&gt;.</source>
          <target state="translated">노드 당 여러 프로세스를 생성하려면 &lt;code&gt;torch.distributed.launch&lt;/code&gt; 또는 &lt;code&gt;torch.multiprocessing.spawn&lt;/code&gt; 을 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="40c808fe1c4d6bdfde7e41f404147333e778923e" translate="yes" xml:space="preserve">
          <source>In order to use CuDNN, the following must be satisfied: &lt;code&gt;targets&lt;/code&gt; must be in concatenated format, all &lt;code&gt;input_lengths&lt;/code&gt; must be &lt;code&gt;T&lt;/code&gt;.</source>
          <target state="translated">CuDNN을 사용하려면 다음을 충족해야합니다. &lt;code&gt;targets&lt;/code&gt; 은 연결된 형식 이어야하며 모든 &lt;code&gt;input_lengths&lt;/code&gt; 는 &lt;code&gt;T&lt;/code&gt; 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="5224913092c4a84680bc86a2c5b52f3a9390a2e0" translate="yes" xml:space="preserve">
          <source>In other words, for an input of size</source>
          <target state="translated">즉, 크기를 입력하려면</target>
        </trans-unit>
        <trans-unit id="6f63e1b44e4587b6cbe87da3c99588530141677e" translate="yes" xml:space="preserve">
          <source>In particular, solves</source>
          <target state="translated">특히</target>
        </trans-unit>
        <trans-unit id="1cb4118455a2bb7d53d0e1417d8853260cb96078" translate="yes" xml:space="preserve">
          <source>In practice, when working with named tensors, one should avoid having unnamed dimensions because their handling can be complicated. It is recommended to lift all unnamed dimensions to be named dimensions by using &lt;a href=&quot;#torch.Tensor.refine_names&quot;&gt;&lt;code&gt;refine_names()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">실제로 명명 된 텐서로 작업 할 때 처리가 복잡 할 수 있으므로 명명되지 않은 차원을 사용하지 않아야합니다. &lt;a href=&quot;#torch.Tensor.refine_names&quot;&gt; &lt;code&gt;refine_names()&lt;/code&gt; &lt;/a&gt; 를 사용하여 명명되지 않은 모든 차원을 명명 된 차원으로 들어 올리는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="df5ee95eb608e3ef9c532ed1dab9ce8f7003c283" translate="yes" xml:space="preserve">
          <source>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting &lt;code&gt;torch.backends.cudnn.deterministic =
True&lt;/code&gt;. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">CuDNN과 함께 CUDA 백엔드를 사용할 때 일부 상황에서이 연산자는 성능을 높이기 위해 비 결정적 알고리즘을 선택할 수 있습니다. 이것이 바람직하지 않은 경우 &lt;code&gt;torch.backends.cudnn.deterministic = True&lt;/code&gt; 를 설정하여 작업을 결정적 (잠재적으로 성능 비용)으로 만들 수 있습니다 . 배경 은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;재현성에&lt;/a&gt; 대한 참고 사항을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e14a819b75469318610313772eb13bce47023e07" translate="yes" xml:space="preserve">
          <source>In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph:</source>
          <target state="translated">위의 예에서 aten :: triu는 ONNX에서 지원되지 않으므로 내보내기가이 작업으로 돌아갑니다. OperatorExportTypes.RAW : 원시 ir를 내 보냅니다. OperatorExportTypes.ONNX_FALLTHROUGH : ONNX에서 작업이 지원되지 않는 경우 사용자 지정 ONNX 작업으로 연산자를 그대로 내보내고 내보내십시오. 이 모드를 사용하면 런타임 백엔드를 위해 사용자가 작업을 내보내고 구현할 수 있습니다. 그래프 예 :</target>
        </trans-unit>
        <trans-unit id="b3a5fd174642790bebe72eca626e159e3e5dfcd8" translate="yes" xml:space="preserve">
          <source>In the above example, prim::ListConstruct is not supported, hence exporter falls through.</source>
          <target state="translated">위의 예에서 prim :: ListConstruct는 지원되지 않으므로 내보내기가 실패합니다.</target>
        </trans-unit>
        <trans-unit id="7c25e2887a622adc243b3c60db24b8cbea2277b3" translate="yes" xml:space="preserve">
          <source>In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).</source>
          <target state="translated">CUDA 장치에서 크기가 32 이하인 정사각형 행렬 배치의 경우 MAGMA 라이브러리의 버그로 인해 단일 행렬에 대해 LU 분해가 반복됩니다 (마그마 문제 13 참조).</target>
        </trans-unit>
        <trans-unit id="7889605442c466f81697836a1b9ec1edd9e39e26" translate="yes" xml:space="preserve">
          <source>In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.</source>
          <target state="translated">다음 표에서는 결과를보고하기 위해 CUDA 10.0 및 CUDNN 7.4와 함께 8 개의 V100 GPU를 사용합니다. 훈련 중에 GPU 당 배치 크기 2를 사용하고 테스트 중에 배치 크기 1이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="33fa5776db9b9bbfad4e856c392d72e978768794" translate="yes" xml:space="preserve">
          <source>In the future, &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; may return a non-writeable view for an &lt;code&gt;input&lt;/code&gt; of non-complex dtype. It&amp;rsquo;s recommended that programs not modify the tensor returned by &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;input&lt;/code&gt; is of non-complex dtype to be compatible with this change.</source>
          <target state="translated">앞으로 &lt;a href=&quot;#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt; 는 복잡하지 않은 dtype 의 &lt;code&gt;input&lt;/code&gt; 에 대해 쓰기 불가능한 뷰를 반환 할 수 있습니다 . &lt;code&gt;input&lt;/code&gt; 이 복잡하지 않은 dtype 일 때 프로그램이 &lt;a href=&quot;#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 텐서를 수정하지 않는 것이이 변경 사항과 호환되도록하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="99f1ee81dc5eba42240face3c5c428f4a78edd8a" translate="yes" xml:space="preserve">
          <source>In the future, there will be backends for other frameworks as well.</source>
          <target state="translated">미래에는 다른 프레임 워크에 대한 백엔드도있을 것입니다.</target>
        </trans-unit>
        <trans-unit id="d25cb11f14e2999086551374695bf04a7bfaf842" translate="yes" xml:space="preserve">
          <source>In the past, we were often asked: &amp;ldquo;which backend should I use?&amp;rdquo;.</source>
          <target state="translated">과거에는 &quot;어떤 백엔드를 사용해야합니까?&quot;라는 질문을 자주 받았습니다.</target>
        </trans-unit>
        <trans-unit id="0b0a51e5781bdbc893d9a5aff9fe6d5b37f0885c" translate="yes" xml:space="preserve">
          <source>In the returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, operations that have different behaviors in &lt;code&gt;training&lt;/code&gt; and &lt;code&gt;eval&lt;/code&gt; modes will always behave as if it is in the mode it was in during tracing, no matter which mode the &lt;code&gt;ScriptModule&lt;/code&gt; is in.</source>
          <target state="translated">반환 된 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 에서 &lt;code&gt;training&lt;/code&gt; 및 &lt;code&gt;eval&lt;/code&gt; 모드 에서 다른 동작을 갖는 작업 은 &lt;code&gt;ScriptModule&lt;/code&gt; 이 어떤 모드에 있든 관계없이 추적 중 모드에있는 것처럼 항상 동작합니다 .</target>
        </trans-unit>
        <trans-unit id="b3daaa94feed4c186e9e9a97512618fa018aae7f" translate="yes" xml:space="preserve">
          <source>In the simplest case, the output value of the layer with input size</source>
          <target state="translated">가장 간단한 경우 입력 크기가있는 레이어의 출력 값</target>
        </trans-unit>
        <trans-unit id="239b7a59f23d668ac60e6721fee439268e244221" translate="yes" xml:space="preserve">
          <source>In the single-machine synchronous case, &lt;code&gt;torch.distributed&lt;/code&gt; or the &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt;&lt;/a&gt; wrapper may still have advantages over other approaches to data-parallelism, including &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">단일 머신 동기의 경우 &lt;code&gt;torch.distributed&lt;/code&gt; 또는 &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt; &lt;/a&gt; 래퍼는 &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;torch.nn.DataParallel()&lt;/code&gt; &lt;/a&gt; 포함한 데이터 병렬 처리에 대한 다른 접근 방식에 비해 여전히 이점이있을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1e133f6f146acb51fdcd21b510ff4101dead133b" translate="yes" xml:space="preserve">
          <source>In the spatial (4-D) case, for &lt;code&gt;input&lt;/code&gt; with shape</source>
          <target state="translated">공간 (4-D)의 경우, 모양이있는 &lt;code&gt;input&lt;/code&gt; 경우</target>
        </trans-unit>
        <trans-unit id="51ee81d82611ed15672cbe78ad3be970f77568fa" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph.</source>
          <target state="translated">기호 함수에서 연산자가 이미 ONNX에서 표준화 된 경우 그래프에서 ONNX 연산자를 나타내는 노드를 생성하기 만하면됩니다.</target>
        </trans-unit>
        <trans-unit id="05f2edf0966166823005056e0975e4e593b57b37" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph.</source>
          <target state="translated">기호 함수에서 연산자가 이미 ONNX에서 표준화 된 경우 그래프에서 ONNX 연산자를 나타내는 노드 만 생성하면됩니다.</target>
        </trans-unit>
        <trans-unit id="c41d2ba36e63a36e43970f33786546314d161009" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout2d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">이 경우 &lt;code&gt;nn.Dropout2d()&lt;/code&gt; 는 기능 맵 간의 독립성을 높이는 데 도움이되며 대신 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="df2da228c2bd90611f132d29496ecf7b2c5d7aaa" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout3d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">이 경우 &lt;code&gt;nn.Dropout3d()&lt;/code&gt; 는 기능 맵 간의 독립성을 높이는 데 도움이되며 대신 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="f8b3c9fa943c89a945d6617acdfae17b22a6204e" translate="yes" xml:space="preserve">
          <source>In this case, data-dependent control flow like this can be captured using &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script()&lt;/code&gt;&lt;/a&gt; instead:</source>
          <target state="translated">이 경우 이와 같은 데이터 종속 제어 흐름은 대신 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script()&lt;/code&gt; &lt;/a&gt; 를 사용하여 캡처 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85295238e8aebda5327b95ca72369c8df3c9eab9" translate="yes" xml:space="preserve">
          <source>In this mode, the result of every computation will have &lt;code&gt;requires_grad=False&lt;/code&gt;, even when the inputs have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">이 모드에서는 입력에 &lt;code&gt;requires_grad=True&lt;/code&gt; 가있는 경우에도 모든 계산 결과에 &lt;code&gt;requires_grad=False&lt;/code&gt; 가 있습니다 .</target>
        </trans-unit>
        <trans-unit id="91c8c0bce1c0356637c4656cf0a0cca8ed0c2a41" translate="yes" xml:space="preserve">
          <source>In this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;.</source>
          <target state="translated">이 섹션에서 명명 된 텐서 특정 API에 대한 문서를 찾으십시오. 다른 PyTorch 연산자를 통해 이름이 전파되는 방법에 대한 포괄적 인 참조는 &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors 연산자 적용 범위를&lt;/a&gt; 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="13102db919fdc00f95907c16ae125b4a1f45c313" translate="yes" xml:space="preserve">
          <source>In version 1.6 changed to this from set_training</source>
          <target state="translated">버전 1.6에서는 set_training에서 이렇게 변경되었습니다.</target>
        </trans-unit>
        <trans-unit id="90ef0f5bd340db15b402881b337b9e6fd1b54dd1" translate="yes" xml:space="preserve">
          <source>In-place random sampling</source>
          <target state="translated">내부 무작위 샘플링</target>
        </trans-unit>
        <trans-unit id="1723c07101158e28ed701a45820ffb2cbec954ae" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="24948f16983220b792808971db48b1bcdd5b3fa1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.absolute&quot;&gt;&lt;code&gt;absolute()&lt;/code&gt;&lt;/a&gt; Alias for &lt;a href=&quot;#torch.Tensor.abs_&quot;&gt;&lt;code&gt;abs_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.abs_&quot;&gt; &lt;code&gt;abs_()&lt;/code&gt; 에&lt;/a&gt; 대한 &lt;a href=&quot;#torch.Tensor.absolute&quot;&gt; &lt;code&gt;absolute()&lt;/code&gt; &lt;/a&gt; 별칭 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="dde70002d7610f16bccc15d518923eec4d107bba" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acos&quot;&gt;&lt;code&gt;acos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.acos&quot;&gt; &lt;code&gt;acos()&lt;/code&gt; &lt;/a&gt; 의 인플레 이스 버전</target>
        </trans-unit>
        <trans-unit id="2f828fea297a593b936b38df78e2d795295f80f1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acosh&quot;&gt;&lt;code&gt;acosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.acosh&quot;&gt; &lt;code&gt;acosh()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="6e2ef98fa9aa6aca8a1bdb5dad128434e35bceaa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.add&quot;&gt;&lt;code&gt;add()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.add&quot;&gt; &lt;code&gt;add()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9afdb8ac221f8f75547e87484016d557ba50f247" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addbmm&quot;&gt;&lt;code&gt;addbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addbmm&quot;&gt; &lt;code&gt;addbmm()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="dcf43a3561084dec90d7c3a2ecc1b6cc0c26f938" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcdiv&quot;&gt;&lt;code&gt;addcdiv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addcdiv&quot;&gt; &lt;code&gt;addcdiv()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="b52599a8d04e379d3f3c9ecc7af347c1222d473b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcmul&quot;&gt;&lt;code&gt;addcmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addcmul&quot;&gt; &lt;code&gt;addcmul()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="212bb18c46269c9b244d88318432c473793d6225" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmm&quot;&gt;&lt;code&gt;addmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addmm&quot;&gt; &lt;code&gt;addmm()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="7e04b090badbdcab058cb74254d5f97b547843c5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmv&quot;&gt;&lt;code&gt;addmv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addmv&quot;&gt; &lt;code&gt;addmv()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="714cae5e9e959037c53ed668822d40bf8774aa1b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addr&quot;&gt;&lt;code&gt;addr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.addr&quot;&gt; &lt;code&gt;addr()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="85808cd790e595c311410030bdf947098dfb3832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccos&quot;&gt;&lt;code&gt;arccos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arccos&quot;&gt; &lt;code&gt;arccos()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="f222472fe1a711c429f88dfa21d735fc20269d70" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccosh&quot;&gt;&lt;code&gt;arccosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arccosh&quot;&gt; &lt;code&gt;arccosh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="e5850fc97599dacb252ef7b71a0bc059e90ac832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsin&quot;&gt;&lt;code&gt;arcsin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arcsin&quot;&gt; &lt;code&gt;arcsin()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="a92421400d2c919bc99b517f289d8339a3ae03e4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsinh&quot;&gt;&lt;code&gt;arcsinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arcsinh&quot;&gt; &lt;code&gt;arcsinh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="ab6c8d62429cad7d186033198296a84d5c952598" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctan&quot;&gt;&lt;code&gt;arctan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arctan&quot;&gt; &lt;code&gt;arctan()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="e025d0bea89a2469024775370356f0fa69f44cc9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctanh&quot;&gt;&lt;code&gt;arctanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.arctanh&quot;&gt; &lt;code&gt;arctanh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="1eda7abb65e5c787ce66fb95749143a3f04888ed" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asin&quot;&gt;&lt;code&gt;asin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.asin&quot;&gt; &lt;code&gt;asin()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="0d258dbfd544c95327dd54668d3cb8823507d044" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asinh&quot;&gt;&lt;code&gt;asinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.asinh&quot;&gt; &lt;code&gt;asinh()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="0e752ea6e17fecf2b883b074dbb3aa938a6f7d1c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan&quot;&gt;&lt;code&gt;atan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.atan&quot;&gt; &lt;code&gt;atan()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="f00b83a4661950dbba55ca10e0c95538d6e98aaf" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan2&quot;&gt;&lt;code&gt;atan2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.atan2&quot;&gt; &lt;code&gt;atan2()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9a9b90fa5b79edeea0ea7f804bf39ca483463ba1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atanh&quot;&gt;&lt;code&gt;atanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.atanh&quot;&gt; &lt;code&gt;atanh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="0cf21533d1ef19709a89c1226676212cc25b35e7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.baddbmm&quot;&gt;&lt;code&gt;baddbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.baddbmm&quot;&gt; &lt;code&gt;baddbmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="723c45d84b8a47a2aac768faa389e89dbffdc312" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_and&quot;&gt;&lt;code&gt;bitwise_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.bitwise_and&quot;&gt; &lt;code&gt;bitwise_and()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="f23ac5a50549f7c4c93c7cc4af1a4a7fb14079c7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_not&quot;&gt;&lt;code&gt;bitwise_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.bitwise_not&quot;&gt; &lt;code&gt;bitwise_not()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="1643902bef600d4d0d475acf131454814b25dc26" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_or&quot;&gt;&lt;code&gt;bitwise_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.bitwise_or&quot;&gt; &lt;code&gt;bitwise_or()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="fc90a184109b29c52ddf3915b630a1a54ae9634b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_xor&quot;&gt;&lt;code&gt;bitwise_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.bitwise_xor&quot;&gt; &lt;code&gt;bitwise_xor()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9b071498cdd10ed979300e38b8823b05d84b4e9a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ceil&quot;&gt;&lt;code&gt;ceil()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.ceil&quot;&gt; &lt;code&gt;ceil()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="cd0a36194221856cef72e71a8bee0b50682fd547" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="70bc593dd1c02b1636bd32a3b21498325eaaa85c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cos&quot;&gt;&lt;code&gt;cos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.cos&quot;&gt; &lt;code&gt;cos()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="30b3045f3354737011026cef07419edca16e51b7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cosh&quot;&gt;&lt;code&gt;cosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.cosh&quot;&gt; &lt;code&gt;cosh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="85698a440de4a886c432db2c8ba93bb04b9783a5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.digamma&quot;&gt;&lt;code&gt;digamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.digamma&quot;&gt; &lt;code&gt;digamma()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="5b9d988d2d91968da3bc8e9addd60ca857a07078" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.div&quot;&gt;&lt;code&gt;div()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.div&quot;&gt; &lt;code&gt;div()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="40f813960ae0cb30555d66235642030e14081a78" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.divide&quot;&gt;&lt;code&gt;divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.divide&quot;&gt; &lt;code&gt;divide()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3593d29a80b1997092e2a7caab1ae4c89ff665ad" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.eq&quot;&gt;&lt;code&gt;eq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.eq&quot;&gt; &lt;code&gt;eq()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="a597279c8f26129b927bac941304ed95baa85e3a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erf&quot;&gt;&lt;code&gt;erf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.erf&quot;&gt; &lt;code&gt;erf()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="cbb6ddfcf060f64c41fb12dadf24c5f4104c8248" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfc&quot;&gt;&lt;code&gt;erfc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.erfc&quot;&gt; &lt;code&gt;erfc()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9d3f26f32a7d82b45bafe3bb5a09586684935c0d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfinv&quot;&gt;&lt;code&gt;erfinv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.erfinv&quot;&gt; &lt;code&gt;erfinv()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="843b7383da05ed2bdd2c3d50d63831578e83d793" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.exp&quot;&gt;&lt;code&gt;exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.exp&quot;&gt; &lt;code&gt;exp()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="592c5733a1bb63ac93435062ef6cf042a3e0764b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.expm1&quot;&gt;&lt;code&gt;expm1()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.expm1&quot;&gt; &lt;code&gt;expm1()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="d5d395d973d45d767241ba61182b45ea933b25b3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fix&quot;&gt;&lt;code&gt;fix()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.fix&quot;&gt; &lt;code&gt;fix()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0ad235d8de2027be5bcf6998afc520cb85c9fc4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor&quot;&gt;&lt;code&gt;floor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.floor&quot;&gt; &lt;code&gt;floor()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="1298ca9624e848339b14dbd7ac9e1cc72e8426d5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor_divide&quot;&gt;&lt;code&gt;floor_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.floor_divide&quot;&gt; &lt;code&gt;floor_divide()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="ba34126e0ac2c59a401a9332bbc39d964a9f1676" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fmod&quot;&gt;&lt;code&gt;fmod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.fmod&quot;&gt; &lt;code&gt;fmod()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="05577f4319cd8a9afef3327e267c8259cd201bc1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.frac&quot;&gt;&lt;code&gt;frac()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.frac&quot;&gt; &lt;code&gt;frac()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="2c9a5d890ffefe6a4582f4b8a23dc1a024530071" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gcd&quot;&gt;&lt;code&gt;gcd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.gcd&quot;&gt; &lt;code&gt;gcd()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="64f7412f381bbec829d7e064308cdeedbdb4239d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ge&quot;&gt;&lt;code&gt;ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.ge&quot;&gt; &lt;code&gt;ge()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="c4e0348ba557fd7f53f8d998e67417ad216e0ca1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater&quot;&gt;&lt;code&gt;greater()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.greater&quot;&gt; &lt;code&gt;greater()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="c8558b60a835a6d4d07c498aab1d1c2aee7927ef" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater_equal&quot;&gt;&lt;code&gt;greater_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.greater_equal&quot;&gt; &lt;code&gt;greater_equal()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="23ff3a9c46bb69e3bf80c84bd30a30c21ea9397e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gt&quot;&gt;&lt;code&gt;gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.gt&quot;&gt; &lt;code&gt;gt()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="0101731065c7560702b5f46c4e91cdc9923c8847" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.hypot&quot;&gt;&lt;code&gt;hypot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.hypot&quot;&gt; &lt;code&gt;hypot()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2ae74e50f4d266279d834bc1480fb185d3f5523f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.i0&quot;&gt;&lt;code&gt;i0()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.i0&quot;&gt; &lt;code&gt;i0()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="a9c7375b3919548ed0ffd6165fe0af4e23e758d4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lcm&quot;&gt;&lt;code&gt;lcm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.lcm&quot;&gt; &lt;code&gt;lcm()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="c39b8eebf07573ad5eac2472d11ac00d3ec49609" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.le&quot;&gt;&lt;code&gt;le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.le&quot;&gt; &lt;code&gt;le()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="de77dd1c89036ad69daa80304475b1011fb3301e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lerp&quot;&gt;&lt;code&gt;lerp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.lerp&quot;&gt; &lt;code&gt;lerp()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="c58f50cdbebdcfb188e2bdfa9fcdc829890f845d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less&quot;&gt;&lt;code&gt;less()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.less&quot;&gt; &lt;code&gt;less()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="b9423b0d6372fd7e2833e62f8095c74ec7f8c46f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less_equal&quot;&gt;&lt;code&gt;less_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.less_equal&quot;&gt; &lt;code&gt;less_equal()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="6e2671a3a2693a72154fb41d1a6e377bc20e43a8" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lgamma&quot;&gt;&lt;code&gt;lgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.lgamma&quot;&gt; &lt;code&gt;lgamma()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9af05e667e56282be6af4fe5b0246f9f857e3a1d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log&quot;&gt;&lt;code&gt;log()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.log&quot;&gt; &lt;code&gt;log()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c10939ba9c9c50755fefd56b392ef3407792f705" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log10&quot;&gt;&lt;code&gt;log10()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.log10&quot;&gt; &lt;code&gt;log10()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="1458e566ebdc0a91bfc06212d913d1e13187212c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log1p&quot;&gt;&lt;code&gt;log1p()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.log1p&quot;&gt; &lt;code&gt;log1p()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="b4555443ad8a2acf3c819dcdfbbef49a10a2e972" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log2&quot;&gt;&lt;code&gt;log2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.log2&quot;&gt; &lt;code&gt;log2()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="433b0aeae51de4808da5b68a9beefef42daf02ce" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_and&quot;&gt;&lt;code&gt;logical_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.logical_and&quot;&gt; &lt;code&gt;logical_and()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="c70b5c1f8176e07af5e81b152a7bdfca820fb3f9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_not&quot;&gt;&lt;code&gt;logical_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.logical_not&quot;&gt; &lt;code&gt;logical_not()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="d91ce8af603413f0aefc2d1b8b6eec1f114d044b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_or&quot;&gt;&lt;code&gt;logical_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.logical_or&quot;&gt; &lt;code&gt;logical_or()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="3109d25b88385322d682d606fa91db53903884b9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_xor&quot;&gt;&lt;code&gt;logical_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.logical_xor&quot;&gt; &lt;code&gt;logical_xor()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="343ccb07f42d386eca14f79b12f59de018d60e86" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logit&quot;&gt;&lt;code&gt;logit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.logit&quot;&gt; &lt;code&gt;logit()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="9d856395e389956dab5ecceb9942b3af92fe9ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lt&quot;&gt;&lt;code&gt;lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.lt&quot;&gt; &lt;code&gt;lt()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="23d64d86cc2e5f1327a6e010cdeb7c115938b177" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mul&quot;&gt;&lt;code&gt;mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.mul&quot;&gt; &lt;code&gt;mul()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="320ff1f84d1c7cfe2a1624b610ddcce8af66a764" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.multiply&quot;&gt;&lt;code&gt;multiply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.multiply&quot;&gt; &lt;code&gt;multiply()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="d10598fcd6504f5f6bb4b9ddb4d89823cb2a72a4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mvlgamma&quot;&gt;&lt;code&gt;mvlgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.mvlgamma&quot;&gt; &lt;code&gt;mvlgamma()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="b3a59563ae4042d5dfe02b4b4d28142c18b881db" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ne&quot;&gt;&lt;code&gt;ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.ne&quot;&gt; &lt;code&gt;ne()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="0933e5f6d727692e6688d039a1e1af9fcbe844b2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.neg&quot;&gt;&lt;code&gt;neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.neg&quot;&gt; &lt;code&gt;neg()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="6b0826ad65145f6f9f94c5d871b782df9fd07711" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.negative&quot;&gt;&lt;code&gt;negative()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.negative&quot;&gt; &lt;code&gt;negative()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9a9845a362319d27c686423e43aadf669c3c68aa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.nextafter&quot;&gt;&lt;code&gt;nextafter()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.nextafter&quot;&gt; &lt;code&gt;nextafter()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="28a1d8909a6e02b4e7751f03f7b62a7e62637734" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.not_equal&quot;&gt;&lt;code&gt;not_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.not_equal&quot;&gt; &lt;code&gt;not_equal()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="f16d420f4d416e8798bbbadc72fbaac73ed31ae3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.polygamma&quot;&gt;&lt;code&gt;polygamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.polygamma&quot;&gt; &lt;code&gt;polygamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59d753fe8af8e0e2fdbff27f828f4dc81a011e2f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.pow&quot;&gt;&lt;code&gt;pow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.pow&quot;&gt; &lt;code&gt;pow()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="52dd6f8c275f23765de5ee7462ecb62204684869" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.reciprocal&quot;&gt;&lt;code&gt;reciprocal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.reciprocal&quot;&gt; &lt;code&gt;reciprocal()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="ca45cdac88c7921f089b79a8f83e20d848c48d4a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.remainder&quot;&gt;&lt;code&gt;remainder()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.remainder&quot;&gt; &lt;code&gt;remainder()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="053eb7c5cd301fdd12904be2836f5fb0ec275646" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="818df20d8449f1acb432a28eac3f0b43768c2197" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.renorm&quot;&gt;&lt;code&gt;renorm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.renorm&quot;&gt; &lt;code&gt;renorm()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="53ce6f2d6ab6ae80005a428178e9b77aef382107" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.round&quot;&gt;&lt;code&gt;round()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.round&quot;&gt; &lt;code&gt;round()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="b623bd7302eab7378ff509a5b3dd4f0470b21494" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rsqrt&quot;&gt;&lt;code&gt;rsqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.rsqrt&quot;&gt; &lt;code&gt;rsqrt()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="1fe0cf7810c0bc56d540daa9ddb7cec1599e41ee" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sgn&quot;&gt;&lt;code&gt;sgn()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.sgn&quot;&gt; &lt;code&gt;sgn()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="cdcdd0daf61ad10208f07b04eb0183a9d4d2ab44" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sigmoid&quot;&gt;&lt;code&gt;sigmoid()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.sigmoid&quot;&gt; &lt;code&gt;sigmoid()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9e2cd3c0abcebc8ed45bd6570406d8c9d4c33ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sign&quot;&gt;&lt;code&gt;sign()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">현재 위치 버전의 &lt;a href=&quot;#torch.Tensor.sign&quot;&gt; &lt;code&gt;sign()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69d6918f4d893ee927f13c625b9f268b4eea1c8b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sin&quot;&gt;&lt;code&gt;sin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.sin&quot;&gt; &lt;code&gt;sin()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="7f906cf7cca8e4f9df35be1db35c6f3f24ecf45b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sinh&quot;&gt;&lt;code&gt;sinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.sinh&quot;&gt; &lt;code&gt;sinh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="cbe8e3f4ea9deb4f23b9e270df8f53c6c9f24e99" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sqrt&quot;&gt;&lt;code&gt;sqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.sqrt&quot;&gt; &lt;code&gt;sqrt()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8821b663c3d7115389c6fb460a014cd6f46b31a6" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.square&quot;&gt;&lt;code&gt;square()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.square&quot;&gt; &lt;code&gt;square()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="331d4255e6beb57fd7b54b749ce35473d9c934d2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.squeeze&quot;&gt;&lt;code&gt;squeeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.squeeze&quot;&gt; &lt;code&gt;squeeze()&lt;/code&gt; &lt;/a&gt; 내부 버전</target>
        </trans-unit>
        <trans-unit id="9c0246ff36c2a3383754e9573043424c772cead3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sub&quot;&gt;&lt;code&gt;sub()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.sub&quot;&gt; &lt;code&gt;sub()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="afadc08d16c14a6f3ab63ac2e1777d4a0c639d3d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.subtract&quot;&gt;&lt;code&gt;subtract()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.subtract&quot;&gt; &lt;code&gt;subtract()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6e9a157c014d95de0d85c9410108016dcf899ffc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.t&quot;&gt;&lt;code&gt;t()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.t&quot;&gt; &lt;code&gt;t()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="c2bca115f72f0be4030271dbbf4001e5646c23cd" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tan&quot;&gt;&lt;code&gt;tan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.tan&quot;&gt; &lt;code&gt;tan()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="abae0cb9439d45a19abc17669152c5675c6b2f91" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tanh&quot;&gt;&lt;code&gt;tanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.tanh&quot;&gt; &lt;code&gt;tanh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="e2861f743f86145802065db61cf1289846a4dadc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.transpose&quot;&gt;&lt;code&gt;transpose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.transpose&quot;&gt; &lt;code&gt;transpose()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="45499df8d625edfd4a83589eb3c8afa20bd32bb9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tril&quot;&gt;&lt;code&gt;tril()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">내부 버전의 &lt;a href=&quot;#torch.Tensor.tril&quot;&gt; &lt;code&gt;tril()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="065bb546251aa96738ef601a77cd6f37ec7d9659" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.triu&quot;&gt;&lt;code&gt;triu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.triu&quot;&gt; &lt;code&gt;triu()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="212e370e8fdc9518bb1c8dd81fce7b074d0a6998" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.true_divide_&quot;&gt;&lt;code&gt;true_divide_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.true_divide_&quot;&gt; &lt;code&gt;true_divide_()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전</target>
        </trans-unit>
        <trans-unit id="e29d58f0e79f4e7e5e919543670aacaa5b39af55" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.trunc&quot;&gt;&lt;code&gt;trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.trunc&quot;&gt; &lt;code&gt;trunc()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="1e2464f4a6dc92f29b5fe948a1cb58f6a6f8ec52" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.unsqueeze&quot;&gt;&lt;code&gt;unsqueeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.unsqueeze&quot;&gt; &lt;code&gt;unsqueeze()&lt;/code&gt; &lt;/a&gt; 의 내부 버전</target>
        </trans-unit>
        <trans-unit id="40042c187d9c08cff699d8c486b17c8fc8e0f71b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.elu&quot;&gt;&lt;code&gt;elu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.elu&quot;&gt; &lt;code&gt;elu()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="fda2e5a23576045d8c887926b8f5f495cbb00502" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.hardtanh&quot;&gt;&lt;code&gt;hardtanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.hardtanh&quot;&gt; &lt;code&gt;hardtanh()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="4e00c79886a4c5fdfdd4fc0c5274b0108988ba92" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.leaky_relu&quot;&gt;&lt;code&gt;leaky_relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.leaky_relu&quot;&gt; &lt;code&gt;leaky_relu()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="eda3fb8c00a121092ce0d25f989c8256cb992365" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.relu&quot;&gt;&lt;code&gt;relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.relu&quot;&gt; &lt;code&gt;relu()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="e3a76eb13d927dc0ac3b8f72061ef5fd006d9d68" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.rrelu&quot;&gt;&lt;code&gt;rrelu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.rrelu&quot;&gt; &lt;code&gt;rrelu()&lt;/code&gt; &lt;/a&gt; 의 내부 버전 .</target>
        </trans-unit>
        <trans-unit id="cf12b7cfa392abc73035a860cd7f9148c1c352d9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.threshold&quot;&gt;&lt;code&gt;threshold()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.functional.threshold&quot;&gt; &lt;code&gt;threshold()&lt;/code&gt; &lt;/a&gt; 의 현재 위치 버전 .</target>
        </trans-unit>
        <trans-unit id="0b7ad9a8774865b3951b77d446d4e908b37c2715" translate="yes" xml:space="preserve">
          <source>Inception (warning: this model is highly sensitive to changes in operator implementation)</source>
          <target state="translated">시작 (경고 :이 모델은 운영자 구현 변경에 매우 민감 함)</target>
        </trans-unit>
        <trans-unit id="86187099fc5837d4a16585e59250186b1bdbfd39" translate="yes" xml:space="preserve">
          <source>Inception v3</source>
          <target state="translated">Inception v3</target>
        </trans-unit>
        <trans-unit id="512c1383f2d8169cfead07824d0994b018f42ff3" translate="yes" xml:space="preserve">
          <source>Inception v3 model architecture from &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;Rethinking the Inception Architecture for Computer Vision&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">에서 처음 v3의 모델 아키텍처 &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;&quot;컴퓨터 비전의 인 셉션 아키텍처를 다시 생각&quot;&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c2df9b932637fe9d32a0f16da1c11873398f873d" translate="yes" xml:space="preserve">
          <source>Index</source>
          <target state="translated">Index</target>
        </trans-unit>
        <trans-unit id="6f4e6ca52449e10a302a5dfc3ebba098cd7e6757" translate="yes" xml:space="preserve">
          <source>Indexing, Slicing, Joining, Mutating Ops</source>
          <target state="translated">인덱싱, 슬라이싱, 조인, 변형 작업</target>
        </trans-unit>
        <trans-unit id="5e2d7833039dc978e6eb0d1055910e03a86a4609" translate="yes" xml:space="preserve">
          <source>Indices and tables</source>
          <target state="translated">지표와 표</target>
        </trans-unit>
        <trans-unit id="4eb1cf386795f140ff47153f7333aea4dda421e5" translate="yes" xml:space="preserve">
          <source>Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).</source>
          <target state="translated">인덱스는 각각 샘플링 된시기에 따라 왼쪽에서 오른쪽으로 정렬됩니다 (첫 번째 샘플은 첫 번째 열에 배치됨).</target>
        </trans-unit>
        <trans-unit id="68fa16ffd48f366e4fa8d57fea78ff03fcab0191" translate="yes" xml:space="preserve">
          <source>Initialization</source>
          <target state="translated">Initialization</target>
        </trans-unit>
        <trans-unit id="73e5b96b48f418d64840badaa42ca942b71c67be" translate="yes" xml:space="preserve">
          <source>Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.</source>
          <target state="translated">로컬 RPC 에이전트 및 분산 autograd와 같은 RPC 프리미티브를 초기화하여 현재 프로세스가 RPC를 보내고받을 준비가되도록합니다.</target>
        </trans-unit>
        <trans-unit id="bd0f315e911df950ab4124032aa329f129570289" translate="yes" xml:space="preserve">
          <source>Initializes the default distributed process group, and this will also initialize the distributed package.</source>
          <target state="translated">기본 분산 프로세스 그룹을 초기화하고 분산 패키지도 초기화합니다.</target>
        </trans-unit>
        <trans-unit id="252c7bda7950bbedbb2a4bb4550ac0f5ef1aa6a8" translate="yes" xml:space="preserve">
          <source>Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; contains the reduce_scatter input that resides on the GPU of &lt;code&gt;output_tensor_list[i]&lt;/code&gt;.</source>
          <target state="translated">입력 목록. 집합체의 입력에 사용될 각 GPU에 올바른 크기의 텐서를 포함해야합니다. 예를 들어 &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; 에는 &lt;code&gt;output_tensor_list[i]&lt;/code&gt; 의 GPU에있는 reduce_scatter 입력이 포함 됩니다 .</target>
        </trans-unit>
        <trans-unit id="5bba22431efd0a63a04193e3ddac4464b3801e67" translate="yes" xml:space="preserve">
          <source>Input1:</source>
          <target state="translated">Input1:</target>
        </trans-unit>
        <trans-unit id="feac2b2649c90f11d6d855888f274f7a0752b7d9" translate="yes" xml:space="preserve">
          <source>Input2:</source>
          <target state="translated">Input2:</target>
        </trans-unit>
        <trans-unit id="79d70dcb4f9ee8b7d94ed9539586cc73c0d399da" translate="yes" xml:space="preserve">
          <source>Input:</source>
          <target state="translated">Input:</target>
        </trans-unit>
        <trans-unit id="7b180c0fda0377ef1dc31c7cce34da732fc57c9e" translate="yes" xml:space="preserve">
          <source>Input: LongTensor of arbitrary shape containing the indices to extract</source>
          <target state="translated">입력 : 추출 할 인덱스를 포함하는 임의 모양의 LongTensor</target>
        </trans-unit>
        <trans-unit id="15c3ba090d23cb0ca95577aa6799755edeefe016" translate="yes" xml:space="preserve">
          <source>Input_lengths: Tuple or tensor of size</source>
          <target state="translated">Input_lengths : 튜플 또는 텐서 크기</target>
        </trans-unit>
        <trans-unit id="fcadc5a2f2ce33e2ebb11bf51a1dd2322a5a729a" translate="yes" xml:space="preserve">
          <source>Inputs:</source>
          <target state="translated">Inputs:</target>
        </trans-unit>
        <trans-unit id="f969dd5ae0f7325be616e39a51b06a49fc60e816" translate="yes" xml:space="preserve">
          <source>Inputs: input, (h_0, c_0)</source>
          <target state="translated">입력 : 입력, (h_0, c_0)</target>
        </trans-unit>
        <trans-unit id="1be4f879f896bd3dcbeaf30b825be75ca856fcfb" translate="yes" xml:space="preserve">
          <source>Inputs: input, h_0</source>
          <target state="translated">입력 : 입력, h_0</target>
        </trans-unit>
        <trans-unit id="71bf746d2a9f5ff1025d4c04b2cf1fcfbd657691" translate="yes" xml:space="preserve">
          <source>Inputs: input, hidden</source>
          <target state="translated">입력 : 입력, 숨김</target>
        </trans-unit>
        <trans-unit id="7f3d3cd091d557867390efc2557a3f5b19292265" translate="yes" xml:space="preserve">
          <source>Insert a given module before a given index in the list.</source>
          <target state="translated">목록에서 주어진 색인 앞에 주어진 모듈을 삽입하십시오.</target>
        </trans-unit>
        <trans-unit id="50d8055b5be81f5be9d6435d07e2a4ac2451cc98" translate="yes" xml:space="preserve">
          <source>Inserts the key-value pair into the store based on the supplied &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt;. If &lt;code&gt;key&lt;/code&gt; already exists in the store, it will overwrite the old value with the new supplied &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">제공된 &lt;code&gt;key&lt;/code&gt; 및 &lt;code&gt;value&lt;/code&gt; 기반으로 키-값 쌍을 저장소에 삽입합니다 . 경우 &lt;code&gt;key&lt;/code&gt; 이미 저장소에 존재, 그것은 새로운 제공된로 이전 값을 덮어 쓰게됩니다 &lt;code&gt;value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="52616e80a59172caf19bcc15fac4910844e40524" translate="yes" xml:space="preserve">
          <source>Inspecting Code</source>
          <target state="translated">코드 검사</target>
        </trans-unit>
        <trans-unit id="9efecc53883d2ffb75b3f04810dbaac718af41ba" translate="yes" xml:space="preserve">
          <source>InstanceNorm1d</source>
          <target state="translated">InstanceNorm1d</target>
        </trans-unit>
        <trans-unit id="a1ce20a07b568c5cbed5860b3c884130e1c4f948" translate="yes" xml:space="preserve">
          <source>InstanceNorm2d</source>
          <target state="translated">InstanceNorm2d</target>
        </trans-unit>
        <trans-unit id="f8a9e223352c23318d808b3147734411baf32655" translate="yes" xml:space="preserve">
          <source>InstanceNorm3d</source>
          <target state="translated">InstanceNorm3d</target>
        </trans-unit>
        <trans-unit id="8ab3500f668dc459ea6ccd8a8b7456bb53bb532b" translate="yes" xml:space="preserve">
          <source>Instances of this class should never be created manually. They are meant to be instantiated by functions like &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 클래스의 인스턴스는 수동으로 생성해서는 안됩니다. &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt; 와 같은 함수로 인스턴스화됩니다 .</target>
        </trans-unit>
        <trans-unit id="5a54ceeb7f75257e5faecf7dac25c3c19332eb52" translate="yes" xml:space="preserve">
          <source>Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the &lt;code&gt;TORCH_MODEL_ZOO&lt;/code&gt; environment variable. See &lt;a href=&quot;../model_zoo#torch.utils.model_zoo.load_url&quot;&gt;&lt;code&gt;torch.utils.model_zoo.load_url()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">사전 학습 된 모델을 인스턴스화하면 가중치가 캐시 디렉토리에 다운로드됩니다. 이 디렉토리는 &lt;code&gt;TORCH_MODEL_ZOO&lt;/code&gt; 환경 변수를 사용하여 설정할 수 있습니다 . 자세한 내용은 &lt;a href=&quot;../model_zoo#torch.utils.model_zoo.load_url&quot;&gt; &lt;code&gt;torch.utils.model_zoo.load_url()&lt;/code&gt; &lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4156151a1b281cbc4bcd7e0576424486039cddfe" translate="yes" xml:space="preserve">
          <source>Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.</source>
          <target state="translated">addcdiv를 사용한 정수 분할은 더 이상 지원되지 않으며 향후 릴리스에서 addcdiv는 tensor1 및 tensor2의 실제 분할을 수행합니다. 역사적인 addcdiv 동작은 정수 입력의 경우 (input + value * torch.trunc (tensor1 / tensor2)). to (input.dtype), 부동 입력의 경우 (input + value * tensor1 / tensor2)로 구현할 수 있습니다. 미래의 addcdiv 동작은 모든 dtype에 대해 (입력 + 값 * tensor1 / tensor2) 후자의 구현입니다.</target>
        </trans-unit>
        <trans-unit id="9f650ccc7d0d6be8b0fb7120cf085b2ff78498f0" translate="yes" xml:space="preserve">
          <source>Interpreting Graphs</source>
          <target state="translated">그래프 해석</target>
        </trans-unit>
        <trans-unit id="e42baee8c04a7fb48aeb23c27863b3e7f99c9a61" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform.</source>
          <target state="translated">역 단시간 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="f2d2a372360fc80cd1c1832163878137a219bb55" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform. This is expected to be the inverse of &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt;. It has the same parameters (+ additional optional parameter of &lt;code&gt;length&lt;/code&gt;) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap).</source>
          <target state="translated">역 단시간 푸리에 변환. 이것은 &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;stft()&lt;/code&gt; &lt;/a&gt; 의 역이 될 것으로 예상됩니다 . 동일한 매개 변수 (+ &lt;code&gt;length&lt;/code&gt; 추가 선택적 매개 변수)를 가지며 원래 신호의 최소 제곱 추정값을 반환해야합니다. 알고리즘은 NOLA 조건 (0이 아닌 중첩)을 사용하여 확인합니다.</target>
        </trans-unit>
        <trans-unit id="4212bbbb75b521f6e55e2e79c4b2dab9598d7c21" translate="yes" xml:space="preserve">
          <source>Invoking &lt;code&gt;trace&lt;/code&gt; with a module&amp;rsquo;s method captures module parameters (which may require gradients) as &lt;strong&gt;constants&lt;/strong&gt;.</source>
          <target state="translated">모듈의 메서드로 &lt;code&gt;trace&lt;/code&gt; 을 호출하면 모듈 매개 변수 (그래디언트가 필요할 수 있음)를 &lt;strong&gt;상수&lt;/strong&gt; 로 캡처합니다 .</target>
        </trans-unit>
        <trans-unit id="0b1154e9d45e9b9a76119f9cdf7dd156a3cc44ad" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrices &lt;code&gt;solution&lt;/code&gt; and &lt;code&gt;LU&lt;/code&gt; will be transposed, i.e. with strides like &lt;code&gt;B.contiguous().transpose(-1, -2).stride()&lt;/code&gt; and &lt;code&gt;A.contiguous().transpose(-1, -2).stride()&lt;/code&gt; respectively.</source>
          <target state="translated">원래의 스트라이드에 관계없이 반환 된 행렬 &lt;code&gt;solution&lt;/code&gt; 과 &lt;code&gt;LU&lt;/code&gt; 가 전치됩니다. 즉 &lt;code&gt;B.contiguous().transpose(-1, -2).stride()&lt;/code&gt; 및 &lt;code&gt;A.contiguous().transpose(-1, -2).stride()&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="49986ad18b9e3b5d2fddd594cfccd882001d60e0" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;U&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;U.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="translated">원래의 스트라이드에 관계없이 반환 된 행렬 &lt;code&gt;U&lt;/code&gt; 가 전치됩니다. 즉, 스트라이드 &lt;code&gt;U.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="82269a6fafdcdbb86e718604476ce9b6c8e7d2a7" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;V&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;V.contiguous().transpose(-1, -2).stride()&lt;/code&gt;.</source>
          <target state="translated">원래의 스트라이드에 관계없이 반환 된 행렬 &lt;code&gt;V&lt;/code&gt; 가 전치됩니다. 즉, 스트라이드 &lt;code&gt;V.contiguous().transpose(-1, -2).stride()&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="01899cc116f34c67486ee354a88f582f6796cc36" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like &lt;code&gt;input.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="translated">원래의 보폭에 관계없이 반환 된 텐서는 예를 들어 &lt;code&gt;input.contiguous().transpose(-2, -1).stride()&lt;/code&gt; 와 같은 보폭으로 전치됩니다 .</target>
        </trans-unit>
        <trans-unit id="2310615072fec2b4a5a875a4e0dd6ffebdaf12ad" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if gradients need to be computed for this Tensor, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">가 &lt;code&gt;True&lt;/code&gt; 그라디언트,이 텐서에 대해 계산해야하는 경우 &lt;code&gt;False&lt;/code&gt; 이 없습니다.</target>
        </trans-unit>
        <trans-unit id="aa669dbfc27ea4c75455d3b2508d4155513b175a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is a meta tensor, &lt;code&gt;False&lt;/code&gt; otherwise. Meta tensors are like normal tensors, but they carry no data.</source>
          <target state="translated">가 &lt;code&gt;True&lt;/code&gt; 텐서는 메타 텐서 인 경우 &lt;code&gt;False&lt;/code&gt; 이 없습니다. 메타 텐서는 일반 텐서와 비슷하지만 데이터가 없습니다.</target>
        </trans-unit>
        <trans-unit id="adff4bd8499f965b85193b88135bc7439c31730a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is quantized, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">가 &lt;code&gt;True&lt;/code&gt; 텐서가, 양자화 된 경우 &lt;code&gt;False&lt;/code&gt; 이 없습니다.</target>
        </trans-unit>
        <trans-unit id="3a0d8439442e2aedd6e797ff733a51980c711c86" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is stored on the GPU, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">가 &lt;code&gt;True&lt;/code&gt; 텐서는, GPU에서 저장되어있는 경우 &lt;code&gt;False&lt;/code&gt; 이 없습니다.</target>
        </trans-unit>
        <trans-unit id="e91f70a847754923183418ed052bcea694565f54" translate="yes" xml:space="preserve">
          <source>Is the &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; where this Tensor is.</source>
          <target state="translated">이 Tensor 가있는 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="4212d08cece9c425b48ec2c1e60e8be1d1e78b6c" translate="yes" xml:space="preserve">
          <source>Is this Tensor with its dimensions reversed.</source>
          <target state="translated">이 텐서의 차원이 반대입니까?</target>
        </trans-unit>
        <trans-unit id="6b5b4f414a6eb16cb4afeb48e5665cb815db584b" translate="yes" xml:space="preserve">
          <source>It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)</source>
          <target state="translated">-출력 값을 정확히 원-핫으로 만듭니다 (y_soft 값을 더한 다음 빼기 때문에)-그래디언트를 y_soft 그래디언트와 동일하게 만듭니다 (다른 모든 그래디언트를 제거하기 때문에).</target>
        </trans-unit>
        <trans-unit id="8e6db53c7cd43b88eb003448e0a5df6411ba7a87" translate="yes" xml:space="preserve">
          <source>It currently accepts &lt;code&gt;ndarray&lt;/code&gt; with dtypes of &lt;code&gt;numpy.float64&lt;/code&gt;, &lt;code&gt;numpy.float32&lt;/code&gt;, &lt;code&gt;numpy.float16&lt;/code&gt;, &lt;code&gt;numpy.complex64&lt;/code&gt;, &lt;code&gt;numpy.complex128&lt;/code&gt;, &lt;code&gt;numpy.int64&lt;/code&gt;, &lt;code&gt;numpy.int32&lt;/code&gt;, &lt;code&gt;numpy.int16&lt;/code&gt;, &lt;code&gt;numpy.int8&lt;/code&gt;, &lt;code&gt;numpy.uint8&lt;/code&gt;, and &lt;code&gt;numpy.bool&lt;/code&gt;.</source>
          <target state="translated">그것은 현재 수용 &lt;code&gt;ndarray&lt;/code&gt; 의 dtypes와 &lt;code&gt;numpy.float64&lt;/code&gt; , &lt;code&gt;numpy.float32&lt;/code&gt; , &lt;code&gt;numpy.float16&lt;/code&gt; , &lt;code&gt;numpy.complex64&lt;/code&gt; , &lt;code&gt;numpy.complex128&lt;/code&gt; , &lt;code&gt;numpy.int64&lt;/code&gt; , &lt;code&gt;numpy.int32&lt;/code&gt; , &lt;code&gt;numpy.int16&lt;/code&gt; , &lt;code&gt;numpy.int8&lt;/code&gt; , &lt;code&gt;numpy.uint8&lt;/code&gt; 및 &lt;code&gt;numpy.bool&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="401f208e4a6ef5ef6ad2bc031ef0d0f97b3ebf14" translate="yes" xml:space="preserve">
          <source>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &amp;gt;= 3.0</source>
          <target state="translated">컴퓨팅 기능&amp;gt; = 3.0으로 NVIDIA GPU에서 텐서 계산을 실행할 수있는 CUDA 대응 물이 있습니다.</target>
        </trans-unit>
        <trans-unit id="6563cef1ae60ef9f0cfc196eb441bdefa9d3c7aa" translate="yes" xml:space="preserve">
          <source>It has similar signature as &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, except optional arguments like &lt;code&gt;non_blocking&lt;/code&gt; and &lt;code&gt;copy&lt;/code&gt; should be passed as kwargs, not args, or they will not apply to the index tensors.</source>
          <target state="translated">&lt;code&gt;non_blocking&lt;/code&gt; 및 &lt;code&gt;copy&lt;/code&gt; 와 같은 선택적 인수는 args가 아닌 kwargs로 전달되어야한다는 점을 제외하고 &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt; &lt;code&gt;torch.Tensor.to()&lt;/code&gt; &lt;/a&gt; 와 유사한 서명 이 있습니다. 그렇지 않으면 인덱스 텐서에 적용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2b37442b8edf941c9b64329b2dd98dad1125f396" translate="yes" xml:space="preserve">
          <source>It is also possible to annotate types with Python 3 type hints from the &lt;code&gt;typing&lt;/code&gt; module.</source>
          <target state="translated">&lt;code&gt;typing&lt;/code&gt; 모듈의 Python 3 유형 힌트로 유형에 주석을 달 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="86993dc3f5b11fb441bb568843f0d884da0db7f4" translate="yes" xml:space="preserve">
          <source>It is an inverse operation to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt; 의 역 연산 입니다.</target>
        </trans-unit>
        <trans-unit id="50cbae8d4910e6f042bec5701d96446fe97f36c9" translate="yes" xml:space="preserve">
          <source>It is applied to all slices along dim, and will re-scale them so that the elements lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">희미하게 모든 슬라이스에 적용되며 요소가 &lt;code&gt;[0, 1]&lt;/code&gt; 범위에 있고 합이 1이 되도록 크기를 다시 조정합니다 .</target>
        </trans-unit>
        <trans-unit id="7c6829a0a0e941307482f3fc9539e6effef1fbd8" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, instead of this class, to do multi-GPU training, even if there is only a single node. See: &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/ddp.html#ddp&quot;&gt;Distributed Data Parallel&lt;/a&gt;.</source>
          <target state="translated">단일 노드 만 있더라도 다중 GPU 학습을 수행하려면이 클래스 대신 &lt;a href=&quot;torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;DistributedDataParallel&lt;/code&gt; &lt;/a&gt; 을 사용하는 것이 좋습니다 . 참조 : &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;multiprocessing 또는 nn.DataParallel&lt;/a&gt; 및 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/ddp.html#ddp&quot;&gt;Distributed Data Parallel &lt;/a&gt;대신 nn.parallel.DistributedDataParallel 사용 .</target>
        </trans-unit>
        <trans-unit id="cc50a460e1a231d6b7914ab6f80ce6645cd28d1a" translate="yes" xml:space="preserve">
          <source>It is useful when training a classification problem with &lt;code&gt;C&lt;/code&gt; classes. If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D &lt;code&gt;Tensor&lt;/code&gt; assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 클래스 로 분류 문제를 훈련 할 때 유용합니다 . 제공되는 경우 선택적 인수 &lt;code&gt;weight&lt;/code&gt; 는 각 클래스에 가중치를 할당 하는 1D &lt;code&gt;Tensor&lt;/code&gt; 여야합니다 . 이것은 균형이 맞지 않는 훈련 세트가있을 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="88103e98293e4d193ade88c8ec2a2e41a9017ce8" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:</source>
          <target state="translated">긍정적 인 예에 가중치를 추가하여 재현율과 정밀도를 절충 할 수 있습니다. 다중 레이블 분류의 경우 손실은 다음과 같이 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ecdda59aea5ee67d7d854c969ccf7f4f4b4a4c54" translate="yes" xml:space="preserve">
          <source>Item</source>
          <target state="translated">Item</target>
        </trans-unit>
        <trans-unit id="4dd80eea3f6c51bf5b9c13a8a47609bd55b30e0e" translate="yes" xml:space="preserve">
          <source>Iterables</source>
          <target state="translated">Iterables</target>
        </trans-unit>
        <trans-unit id="069e2ae56a9eacaa6614576bc398f841898b1686" translate="yes" xml:space="preserve">
          <source>Its signature is similar to &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, but only accepts floating point desired &lt;code&gt;dtype&lt;/code&gt; s. In addition, this method will only cast the floating point parameters and buffers to &lt;code&gt;dtype&lt;/code&gt; (if given). The integral parameters and buffers will be moved &lt;code&gt;device&lt;/code&gt;, if that is given, but with dtypes unchanged. When &lt;code&gt;non_blocking&lt;/code&gt; is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</source>
          <target state="translated">서명은 &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt; &lt;code&gt;torch.Tensor.to()&lt;/code&gt; &lt;/a&gt; 와 유사 하지만 부동 소수점 원하는 &lt;code&gt;dtype&lt;/code&gt; 만 허용 합니다. 또한이 메소드는 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;dtype&lt;/code&gt; (주어진 경우)으로 만 캐스트합니다 . 정수 매개 변수와 버퍼 가 주어지면 &lt;code&gt;device&lt;/code&gt; 이동 되지만 dtypes는 변경되지 않습니다. 때 &lt;code&gt;non_blocking&lt;/code&gt; 가 설정되어,이 변환을 시도 / 호스트 가능한 경우에 대하여 비동기 이동 예를 들어, CUDA 장치에 고정 된 메모리와 CPU의 텐서를 이동.</target>
        </trans-unit>
        <trans-unit id="eb5d2f4a58038c71155ddcf3cea35dc3c6d34501" translate="yes" xml:space="preserve">
          <source>JIT</source>
          <target state="translated">JIT</target>
        </trans-unit>
        <trans-unit id="a26e63e0dd630bf0aa29409ebb2007f8ed0989a9" translate="yes" xml:space="preserve">
          <source>Javadoc</source>
          <target state="translated">Javadoc</target>
        </trans-unit>
        <trans-unit id="a7ee38bb7be4fc44198cb2685d9601dcf2b9f569" translate="yes" xml:space="preserve">
          <source>K</source>
          <target state="translated">K</target>
        </trans-unit>
        <trans-unit id="d1dd1d02265a63fb16a7c02869b20a2ff09b23ec" translate="yes" xml:space="preserve">
          <source>K \geq 1</source>
          <target state="translated">K \ geq 1</target>
        </trans-unit>
        <trans-unit id="a0d38167f0e193a1ac968e4ff065116f3dd7d3e2" translate="yes" xml:space="preserve">
          <source>KLDivLoss</source>
          <target state="translated">KLDivLoss</target>
        </trans-unit>
        <trans-unit id="2dff5751295255d47b7ebb571ae0d72fa938cb72" translate="yes" xml:space="preserve">
          <source>Keep in mind that only a limited number of optimizers support sparse gradients: currently it&amp;rsquo;s &lt;code&gt;optim.SGD&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;), &lt;code&gt;optim.SparseAdam&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;) and &lt;code&gt;optim.Adagrad&lt;/code&gt; (&lt;code&gt;CPU&lt;/code&gt;)</source>
          <target state="translated">제한된 수의 옵티마이 &lt;code&gt;optim.SGD&lt;/code&gt; 희소 그래디언트를 지원합니다. 현재 optim.SGD ( &lt;code&gt;CUDA&lt;/code&gt; 및 &lt;code&gt;CPU&lt;/code&gt; ), &lt;code&gt;optim.SparseAdam&lt;/code&gt; ( &lt;code&gt;CUDA&lt;/code&gt; 및 &lt;code&gt;CPU&lt;/code&gt; ) 및 &lt;code&gt;optim.Adagrad&lt;/code&gt; ( &lt;code&gt;CPU&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="61df1ec904a19beceda75adc97e889c2fd5cb69f" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN</source>
          <target state="translated">요점 R-CNN</target>
        </trans-unit>
        <trans-unit id="4dbced9709f5236c9b69e641ffff848fd0986a34" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN ResNet-50 FPN</source>
          <target state="translated">키포인트 R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="f2815e1786777eee9275fa2f0b31c8b705f9edc6" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">Keypoint R-CNN은 고정 된 크기의 입력 이미지를 사용하여 고정 된 배치 크기로 ONNX로 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="db32172db3cd21ae36313d815fba0dd39bd7127a" translate="yes" xml:space="preserve">
          <source>Keyword Arguments</source>
          <target state="translated">키워드 인수</target>
        </trans-unit>
        <trans-unit id="1c524ac8953f745b9e5b79c69e1b4d39891c5ba1" translate="yes" xml:space="preserve">
          <source>Keyword arguments &lt;code&gt;min_value&lt;/code&gt; and &lt;code&gt;max_value&lt;/code&gt; have been deprecated in favor of &lt;code&gt;min_val&lt;/code&gt; and &lt;code&gt;max_val&lt;/code&gt;.</source>
          <target state="translated">키워드 인수 &lt;code&gt;min_value&lt;/code&gt; 및 &lt;code&gt;max_value&lt;/code&gt; 대신 &lt;code&gt;min_val&lt;/code&gt; 및 &lt;code&gt;max_val&lt;/code&gt; 이 사용 됩니다.</target>
        </trans-unit>
        <trans-unit id="e48bd0570e2bf01526ed152277d5e60977339c96" translate="yes" xml:space="preserve">
          <source>Kicks off the distributed backward pass using the provided roots. This currently implements the &lt;a href=&quot;rpc/distributed_autograd#fast-mode-algorithm&quot;&gt;FAST mode algorithm&lt;/a&gt; which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.</source>
          <target state="translated">제공된 루트를 사용하여 분산 된 역방향 패스를 시작합니다. 이는 현재 작업자간에 동일한 분산 autograd 컨텍스트에서 전송 된 모든 RPC 메시지가 역방향 패스 동안 autograd 그래프의 일부가된다고 가정 하는 &lt;a href=&quot;rpc/distributed_autograd#fast-mode-algorithm&quot;&gt;FAST 모드 알고리즘&lt;/a&gt; 을 구현합니다 .</target>
        </trans-unit>
        <trans-unit id="41bb1570ae7ccc5980e78c5f1c45cc0aed0bdbed" translate="yes" xml:space="preserve">
          <source>Kinetics 1-crop accuracies for clip length 16 (16x112x112)</source>
          <target state="translated">클립 길이 16 (16x112x112)에 대한 운동학 1- 자르기 정확도</target>
        </trans-unit>
        <trans-unit id="a5a5fafee83492d8b176cbbdd5bb0860a6fdbf28" translate="yes" xml:space="preserve">
          <source>Known limitations:</source>
          <target state="translated">알려진 제한 사항 :</target>
        </trans-unit>
        <trans-unit id="d160e0986aca4714714a16f29ec605af90be704d" translate="yes" xml:space="preserve">
          <source>L</source>
          <target state="translated">L</target>
        </trans-unit>
        <trans-unit id="80f4812ca21133a053b1e37ce0b7c69d130f72e6" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="translated">L = \ prod_d \ left \ lfloor \ frac {\ text {output \ _size} [d] + 2 \ times \ text {padding} [d] %-\ text {dilation} [d] \ times (\ text {kernel \ _size} [d]-1)-1} {\ text {stride} [d]} + 1 \ right \ rfloor,</target>
        </trans-unit>
        <trans-unit id="c67a32aa874c2b17f9de746130e43c1407789f98" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="translated">L = \ prod_d \ left \ lfloor \ frac {\ text {spatial \ _size} [d] + 2 \ times \ text {padding} [d] %-\ text {dilation} [d] \ times (\ text {kernel \ _size} [d]-1)-1} {\ text {stride} [d]} + 1 \ right \ rfloor,</target>
        </trans-unit>
        <trans-unit id="c775d48eb4f58382ff520cfe8c90ab7190041ada" translate="yes" xml:space="preserve">
          <source>L = \{l_1,\dots,l_N\}^\top</source>
          <target state="translated">L = \ {l_1, \ dots, l_N \} ^ \ top</target>
        </trans-unit>
        <trans-unit id="77bc26a3089aa1a1138008701f4240f1d24f7ec4" translate="yes" xml:space="preserve">
          <source>L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">L (a, p, n) = \ max \ {d (a_i, p_i)-d (a_i, n_i) + {\ rm margin}, 0 \}</target>
        </trans-unit>
        <trans-unit id="38b3e9960c6ed6d7fea333d02fe4a806a8e19d30" translate="yes" xml:space="preserve">
          <source>L1Loss</source>
          <target state="translated">L1Loss</target>
        </trans-unit>
        <trans-unit id="eea818c7023faaf096d89f5dc66b905458038bf3" translate="yes" xml:space="preserve">
          <source>L1Unstructured</source>
          <target state="translated">L1Unstructured</target>
        </trans-unit>
        <trans-unit id="ba6b42d0904cce73f214db608d06bfd30f824246" translate="yes" xml:space="preserve">
          <source>L=C \times \text{upscale\_factor}^2</source>
          <target state="translated">L = C \ times \ text {업 스케일 \ _ 인자} ^ 2</target>
        </trans-unit>
        <trans-unit id="e08d4eaec3e70d3b11d0930088b84b2ff38e358f" translate="yes" xml:space="preserve">
          <source>LPPool1d</source>
          <target state="translated">LPPool1d</target>
        </trans-unit>
        <trans-unit id="2ea3c697cd5d5c5710f8ec9f1853900569f0680c" translate="yes" xml:space="preserve">
          <source>LPPool2d</source>
          <target state="translated">LPPool2d</target>
        </trans-unit>
        <trans-unit id="23757b375d1fdee5bda46fff218547176aed63b2" translate="yes" xml:space="preserve">
          <source>LSTM</source>
          <target state="translated">LSTM</target>
        </trans-unit>
        <trans-unit id="2f25bc39b85fe095b000d209878e94802865e367" translate="yes" xml:space="preserve">
          <source>LSTMCell</source>
          <target state="translated">LSTMCell</target>
        </trans-unit>
        <trans-unit id="ecfecac3ee4f2cb5286bbb038509a2e5de7c4c02" translate="yes" xml:space="preserve">
          <source>LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is not available for CPU, and attempting to do so will throw an error. However, LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is available for CUDA.</source>
          <target state="translated">&lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 를 사용한 LU 분해 는 CPU에서 사용할 수 없으며 그렇게하면 오류가 발생합니다. 그러나 &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 를 사용한 LU 분해 는 CUDA에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0deac72fe13e2e0b87977edeca8e509c0bcfa09e" translate="yes" xml:space="preserve">
          <source>L_p</source>
          <target state="translated">L_p</target>
        </trans-unit>
        <trans-unit id="06c5049b8951b65d33ef4a2ef830eb1114e69b8d" translate="yes" xml:space="preserve">
          <source>L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation} \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</source>
          <target state="translated">L_ {out} = (L_ {in}-1) \ times \ text {stride}-2 \ times \ text {padding} + \ text {dilation} \ times (\ text {kernel \ _size}-1) + \ 텍스트 {출력 \ _ 패딩} + 1</target>
        </trans-unit>
        <trans-unit id="b805a9450351de83336b48fb7172bcc72a263dc2" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_ {out} = \ left \ lfloor \ frac {L_ {in} + 2 \ times \ text {padding}-\ text {dilation} \ times (\ text {kernel \ _size}-1)-1} {\ text {stride}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="ceac6a4482e16b31e7e38fe1379ffb2a0580a4c8" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_ {out} = \ left \ lfloor \ frac {L_ {in} + 2 \ times \ text {padding}-\ text {kernel \ _size}} {\ text {stride}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="87439fc8e93caa8a60f8765a05bec347953ce5dc" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_ {out} = \ left \ lfloor \ frac {L_ {in} + 2 \ times \ text {padding}-\ text {dilation} \ times (\ text {kernel \ _size}-1)-1} {\ text {stride}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="9c7d288e1c47c63e7fa88234abfbcbb629553bc4" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_ {out} = \ left \ lfloor \ frac {L_ {in}-\ text {kernel \ _size}} {\ text {stride}} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="82415f0bf390885258b6da20230789fced78fab6" translate="yes" xml:space="preserve">
          <source>Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index &lt;code&gt;0&lt;/code&gt;, and the least frequent label should be represented by the index &lt;code&gt;n_classes - 1&lt;/code&gt;.</source>
          <target state="translated">이 모듈에 입력으로 전달 된 레이블은 빈도에 따라 정렬되어야합니다. 즉, 가장 빈번한 레이블은 인덱스 &lt;code&gt;0&lt;/code&gt; 으로 나타내야하고 가장 빈번한 레이블은 인덱스 &lt;code&gt;n_classes - 1&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e52f467c6824dfb11f12944d49910287d4e8acf9" translate="yes" xml:space="preserve">
          <source>Language Bindings</source>
          <target state="translated">언어 바인딩</target>
        </trans-unit>
        <trans-unit id="1a13e309cb6089d8795a4086c303a44f1aa93eb0" translate="yes" xml:space="preserve">
          <source>Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;chunks&lt;/code&gt;.</source>
          <target state="translated">주어진 차원 &lt;code&gt;dim&lt;/code&gt; 에 따른 텐서 크기 가 &lt;code&gt;chunks&lt;/code&gt; 로 나눌 수없는 경우 마지막 청크는 더 작아집니다 .</target>
        </trans-unit>
        <trans-unit id="c11e8b0852264fedca1944abe570a02d58008f36" translate="yes" xml:space="preserve">
          <source>Launch utility</source>
          <target state="translated">유틸리티 실행</target>
        </trans-unit>
        <trans-unit id="3e7347f29c1c343ae08ddd8deb4a8c8c703257a6" translate="yes" xml:space="preserve">
          <source>LayerNorm</source>
          <target state="translated">LayerNorm</target>
        </trans-unit>
        <trans-unit id="ae8648006d573eefab4fc3a540fa844d3c24c709" translate="yes" xml:space="preserve">
          <source>Leaky Relu</source>
          <target state="translated">새는 렐루</target>
        </trans-unit>
        <trans-unit id="f3272736528de0f5cd99ceafd394209cd4f0d9d4" translate="yes" xml:space="preserve">
          <source>LeakyRELU</source>
          <target state="translated">LeakyRELU</target>
        </trans-unit>
        <trans-unit id="fedab85703859b90b0ad1f8cd46b9b7ea93c8647" translate="yes" xml:space="preserve">
          <source>LeakyReLU</source>
          <target state="translated">LeakyReLU</target>
        </trans-unit>
        <trans-unit id="7422cede6b29fe51f9334794bb938ed3c22c4e55" translate="yes" xml:space="preserve">
          <source>Least squares estimation of the original signal of size (&amp;hellip;, signal_length)</source>
          <target state="translated">크기 (&amp;hellip;, signal_length)의 원래 신호에 대한 최소 제곱 추정</target>
        </trans-unit>
        <trans-unit id="81a9dc728fc2ca28b37b2c8b862e702ce61a20d2" translate="yes" xml:space="preserve">
          <source>Legacy Constructors</source>
          <target state="translated">레거시 생성자</target>
        </trans-unit>
        <trans-unit id="e6252983d84535ed576c200c0613bfb5c53e74f6" translate="yes" xml:space="preserve">
          <source>Let I_0 be the zeroth order modified Bessel function of the first kind (see &lt;a href=&quot;torch.i0#torch.i0&quot;&gt;&lt;code&gt;torch.i0()&lt;/code&gt;&lt;/a&gt;) and &lt;code&gt;N = L - 1&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is False and &lt;code&gt;L&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is True, where &lt;code&gt;L&lt;/code&gt; is the &lt;code&gt;window_length&lt;/code&gt;. This function computes:</source>
          <target state="translated">I_0은 0 차를 제 1 종 베셀 함수를 수정하자 (참조 &lt;a href=&quot;torch.i0#torch.i0&quot;&gt; &lt;code&gt;torch.i0()&lt;/code&gt; &lt;/a&gt; )와 &lt;code&gt;N = L - 1&lt;/code&gt; 경우 &lt;code&gt;periodic&lt;/code&gt; 거짓이고 &lt;code&gt;L&lt;/code&gt; 경우 &lt;code&gt;periodic&lt;/code&gt; 여기서, True 인 &lt;code&gt;L&lt;/code&gt; 은 은 IS &lt;code&gt;window_length&lt;/code&gt; . 이 함수는 다음을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="c0f4fa4a2a001fbe73eaa9b243479e76bce8ec6b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;unify&lt;/code&gt; are used in name inference in the case of adding two one-dim tensors with no broadcasting.</source>
          <target state="translated">방송없이 2 개의 1 차원 텐서를 추가하는 경우 이름 추론에서 &lt;code&gt;match&lt;/code&gt; 및 &lt;code&gt;unify&lt;/code&gt; 이 어떻게 사용 되는지 살펴 보겠습니다 .</target>
        </trans-unit>
        <trans-unit id="27c968e6692b41bf6e1a241b870dc41ff1a51b17" translate="yes" xml:space="preserve">
          <source>Libraries</source>
          <target state="translated">Libraries</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">3 절 BSD 라이센스에 따라 라이센스가 부여됩니다.</target>
        </trans-unit>
        <trans-unit id="7f5f45ae757c1886e79dc62c9c379591270427af" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; and similarly for &lt;em&gt;c_n&lt;/em&gt;.</source>
          <target state="translated">마찬가지로 &lt;em&gt;출력&lt;/em&gt; 의 층을 사용하여 분리 될 수 &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; 비슷하게 및 &lt;em&gt;c_n&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="db10501c95280d11842c24308a3aa239dbe03702" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt;.</source>
          <target state="translated">&lt;em&gt;output&lt;/em&gt; 과 마찬가지로 레이어는 &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; 사용하여 분리 할 수 ​​있습니다 .</target>
        </trans-unit>
        <trans-unit id="e802831cc6f03739c21444458e82e5daba72cf20" translate="yes" xml:space="preserve">
          <source>Like with &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output length must be given in order to recover an even length output:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 와 마찬가지로 짝수 길이 출력을 복구하려면 출력 길이를 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">Limitations</target>
        </trans-unit>
        <trans-unit id="af502f2b37eea07ed9083c7daf40f34553f6fccd" translate="yes" xml:space="preserve">
          <source>Linear</source>
          <target state="translated">Linear</target>
        </trans-unit>
        <trans-unit id="52f5fe1a2af4bafc542342604407d9368527a7e4" translate="yes" xml:space="preserve">
          <source>Linear / Identity</source>
          <target state="translated">선형 / 동일성</target>
        </trans-unit>
        <trans-unit id="462229a1c5dbd1fa15040281e0145c1fe6a072c6" translate="yes" xml:space="preserve">
          <source>Linear Layers</source>
          <target state="translated">선형 레이어</target>
        </trans-unit>
        <trans-unit id="e0948f7097c1b1ef0dfbf5426722f24d5359dce5" translate="yes" xml:space="preserve">
          <source>Linear functions</source>
          <target state="translated">선형 함수</target>
        </trans-unit>
        <trans-unit id="034874347ef77609e931030e8022aaf9f02a5da3" translate="yes" xml:space="preserve">
          <source>LinearReLU</source>
          <target state="translated">LinearReLU</target>
        </trans-unit>
        <trans-unit id="2654ae5325afbc554a0ee441059fbb41cbaf1dbb" translate="yes" xml:space="preserve">
          <source>List Construction</source>
          <target state="translated">목록 구성</target>
        </trans-unit>
        <trans-unit id="92ca3314a6145558197b21fe52556246426b2528" translate="yes" xml:space="preserve">
          <source>List all entrypoints available in &lt;code&gt;github&lt;/code&gt; hubconf.</source>
          <target state="translated">&lt;code&gt;github&lt;/code&gt; hubconf 에서 사용 가능한 모든 진입 점을 나열합니다 .</target>
        </trans-unit>
        <trans-unit id="c7ba7dcf662374aafe652b03d75cb8f6024519d9" translate="yes" xml:space="preserve">
          <source>Literals</source>
          <target state="translated">Literals</target>
        </trans-unit>
        <trans-unit id="bd26911422165279d6cf54be6a2c3601b7d15a43" translate="yes" xml:space="preserve">
          <source>LnStructured</source>
          <target state="translated">LnStructured</target>
        </trans-unit>
        <trans-unit id="b0a48cb5954a9d7f0650f13e3620734ccd2bae05" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;generated/torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.jit.save#torch.jit.save&quot;&gt; &lt;code&gt;torch.jit.save&lt;/code&gt; 로&lt;/a&gt; 이전에 저장 한 &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 로드</target>
        </trans-unit>
        <trans-unit id="895bd8b16804ae34cfb0e2dc72b19fd2ce1623cf" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt; &lt;code&gt;torch.jit.save&lt;/code&gt; 로&lt;/a&gt; 이전에 저장 한 &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt; 로드</target>
        </trans-unit>
        <trans-unit id="6cb35151312e0bc2bc2c91990968d299f6b351f9" translate="yes" xml:space="preserve">
          <source>Load a model from a github repo or a local directory.</source>
          <target state="translated">github 리포지토리 또는 로컬 디렉터리에서 모델을로드합니다.</target>
        </trans-unit>
        <trans-unit id="08eab7f7a8aa4c317badff7188cc2c0bb5835fc2" translate="yes" xml:space="preserve">
          <source>Loading models from Hub</source>
          <target state="translated">Hub에서 모델로드</target>
        </trans-unit>
        <trans-unit id="1df3601943ceea2bf72fecc57231a5842fe092bb" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</source>
          <target state="translated">Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</target>
        </trans-unit>
        <trans-unit id="e0c28fcdf2966da4afbb699048a05d25a1e72589" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT).</source>
          <target state="translated">Loads a PyTorch C++ extension just-in-time (JIT).</target>
        </trans-unit>
        <trans-unit id="b8172851e5d518987e24368975b30a3f3621f89a" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;generated/torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="translated">Loads an object saved with &lt;a href=&quot;generated/torch.save#torch.save&quot;&gt; &lt;code&gt;torch.save()&lt;/code&gt; &lt;/a&gt; from a file.</target>
        </trans-unit>
        <trans-unit id="abae7d5b43752c21e187437593ecfd6e103a310d" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="translated">Loads an object saved with &lt;a href=&quot;torch.save#torch.save&quot;&gt; &lt;code&gt;torch.save()&lt;/code&gt; &lt;/a&gt; from a file.</target>
        </trans-unit>
        <trans-unit id="f2285e79b168630a8a2e0f5ae0c64959ab5ec6e8" translate="yes" xml:space="preserve">
          <source>Loads the Torch serialized object at the given URL.</source>
          <target state="translated">Loads the Torch serialized object at the given URL.</target>
        </trans-unit>
        <trans-unit id="bca03b832694245e7d76094e54b41aee61f32b8c" translate="yes" xml:space="preserve">
          <source>Local file system, &lt;code&gt;init_method=&quot;file:///d:/tmp/some_file&quot;&lt;/code&gt;</source>
          <target state="translated">Local file system, &lt;code&gt;init_method=&quot;file:///d:/tmp/some_file&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9e2a7f51460fa624f29eb4d9345e24061c3314f3" translate="yes" xml:space="preserve">
          <source>LocalResponseNorm</source>
          <target state="translated">LocalResponseNorm</target>
        </trans-unit>
        <trans-unit id="0ca5c78f2cf1102c66c5583023caa00fc87c10db" translate="yes" xml:space="preserve">
          <source>Locally disabling gradient computation</source>
          <target state="translated">Locally disabling gradient computation</target>
        </trans-unit>
        <trans-unit id="5f6a43bda80ee771a5be7b5a9a5b959803eff8b6" translate="yes" xml:space="preserve">
          <source>LogSigmoid</source>
          <target state="translated">LogSigmoid</target>
        </trans-unit>
        <trans-unit id="9fcfd80b59ec37c0d7c822778e2ede78ad5ef865" translate="yes" xml:space="preserve">
          <source>LogSoftmax</source>
          <target state="translated">LogSoftmax</target>
        </trans-unit>
        <trans-unit id="2be6f5f3dfe34ae207c15144d936b394d3342629" translate="yes" xml:space="preserve">
          <source>Log_probs: Tensor of size</source>
          <target state="translated">Log_probs: Tensor of size</target>
        </trans-unit>
        <trans-unit id="3a29b9b1054fef950824734dd96565e57e322c96" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs in base-2.</source>
          <target state="translated">밑이 2 인 입력의 지수 합의 로그입니다.</target>
        </trans-unit>
        <trans-unit id="273c857a62012f5708232560c688d9c8f7c28ba5" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs.</source>
          <target state="translated">입력 지수 합의 로그입니다.</target>
        </trans-unit>
        <trans-unit id="65eac6118ebdd55aa38b41f70efdf532e567ffd8" translate="yes" xml:space="preserve">
          <source>Logical Operators</source>
          <target state="translated">논리 연산자</target>
        </trans-unit>
        <trans-unit id="4e1da61dedde155219f888017b4095799b14e811" translate="yes" xml:space="preserve">
          <source>LongTensor or tuple of LongTensor</source>
          <target state="translated">LongTensor or tuple of LongTensor</target>
        </trans-unit>
        <trans-unit id="79894b78077b352e7913286a38c3c0b109363b18" translate="yes" xml:space="preserve">
          <source>LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.</source>
          <target state="translated">LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.</target>
        </trans-unit>
        <trans-unit id="5889a83849e7452eb90474433b00a93e8ed08a2f" translate="yes" xml:space="preserve">
          <source>Look at the paper: &lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&lt;/a&gt; by Shi et. al (2016) for more details.</source>
          <target state="translated">Look at the paper: &lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&lt;/a&gt; by Shi et. al (2016) for more details.</target>
        </trans-unit>
        <trans-unit id="c0224525e6e81a08ff829f4d53c8aff4b15f7024" translate="yes" xml:space="preserve">
          <source>Loss Functions</source>
          <target state="translated">Loss Functions</target>
        </trans-unit>
        <trans-unit id="bf3496193ce471c7c1c432d246ad87b6a1823e9c" translate="yes" xml:space="preserve">
          <source>Loss functions</source>
          <target state="translated">Loss functions</target>
        </trans-unit>
        <trans-unit id="85b9d89a2ed9217bacdf503e6a3c96f95e5750aa" translate="yes" xml:space="preserve">
          <source>Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, &amp;ldquo;Loss/train&amp;rdquo; and &amp;ldquo;Loss/test&amp;rdquo; will be grouped together, while &amp;ldquo;Accuracy/train&amp;rdquo; and &amp;ldquo;Accuracy/test&amp;rdquo; will be grouped separately in the TensorBoard interface.</source>
          <target state="translated">Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, &amp;ldquo;Loss/train&amp;rdquo; and &amp;ldquo;Loss/test&amp;rdquo; will be grouped together, while &amp;ldquo;Accuracy/train&amp;rdquo; and &amp;ldquo;Accuracy/test&amp;rdquo; will be grouped separately in the TensorBoard interface.</target>
        </trans-unit>
        <trans-unit id="c63ae6dd4fc9f9dda66970e827d13f7c73fe841c" translate="yes" xml:space="preserve">
          <source>M</source>
          <target state="translated">M</target>
        </trans-unit>
        <trans-unit id="a452071172e433a963ff286fe987870a980f2d28" translate="yes" xml:space="preserve">
          <source>M (Tensor, optional): the input tensor&amp;rsquo;s mean of size</source>
          <target state="translated">M (Tensor, optional): the input tensor&amp;rsquo;s mean of size</target>
        </trans-unit>
        <trans-unit id="7b186e235f284107df6b4dbe6060d2b6a5d9f1e5" translate="yes" xml:space="preserve">
          <source>MAX</source>
          <target state="translated">MAX</target>
        </trans-unit>
        <trans-unit id="0d96af233d36586b84359d8b5bd0b417787982e1" translate="yes" xml:space="preserve">
          <source>MC3 Network definition</source>
          <target state="translated">MC3 Network definition</target>
        </trans-unit>
        <trans-unit id="04e66352aa8f9c4c5f26b71bf380973ada994760" translate="yes" xml:space="preserve">
          <source>MIN</source>
          <target state="translated">MIN</target>
        </trans-unit>
        <trans-unit id="351682e4dc04204d75a824bcbf8f8aa6acc15e71" translate="yes" xml:space="preserve">
          <source>MIXED MODE OF (1) and (2):</source>
          <target state="translated">MIXED MODE OF (1) and (2):</target>
        </trans-unit>
        <trans-unit id="59ce6264cd26f13684de464b9aeb65d1d414e559" translate="yes" xml:space="preserve">
          <source>MNASNet</source>
          <target state="translated">MNASNet</target>
        </trans-unit>
        <trans-unit id="533bea2ee6c045fe09ac0124525af723616f331a" translate="yes" xml:space="preserve">
          <source>MNASNet 1.0</source>
          <target state="translated">MNASNet 1.0</target>
        </trans-unit>
        <trans-unit id="9a7425f07104beddfc0bce0274310b0a2e2ea359" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.5 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">MNASNet with depth multiplier of 0.5 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</target>
        </trans-unit>
        <trans-unit id="98e07c23513158f84ea34588dbe9369dbc7ac20e" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.75 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">MNASNet with depth multiplier of 0.75 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</target>
        </trans-unit>
        <trans-unit id="71754b43c6a0483cdc892a42e6511e7a6757f6a2" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.0 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">MNASNet with depth multiplier of 1.0 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</target>
        </trans-unit>
        <trans-unit id="6f35e9dd2bd116147e38d35cef6e82d4b56bfc05" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.3 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">MNASNet with depth multiplier of 1.3 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</target>
        </trans-unit>
        <trans-unit id="53880fd71eeaa5e41a5d925ca0243b4befcdb092" translate="yes" xml:space="preserve">
          <source>MSELoss</source>
          <target state="translated">MSELoss</target>
        </trans-unit>
        <trans-unit id="76c7044fa78115fa06a42c399d17d51a9203e830" translate="yes" xml:space="preserve">
          <source>Make a blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.</source>
          <target state="translated">Make a blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; . RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.</target>
        </trans-unit>
        <trans-unit id="0d7820212a00c819e77e602a3ef50edfe2f50d52" translate="yes" xml:space="preserve">
          <source>Make a non-blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that can be awaited on.</source>
          <target state="translated">Make a non-blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; . RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; that can be awaited on.</target>
        </trans-unit>
        <trans-unit id="f221c8d616e55bc62ec21b79af5d8282f107f2d3" translate="yes" xml:space="preserve">
          <source>Make a remote call to run &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; and return an &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; to the result value immediately. Worker &lt;code&gt;to&lt;/code&gt; will be the owner of the returned &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the worker calling &lt;code&gt;remote&lt;/code&gt; is a user. The owner manages the global reference count of its &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the owner &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; is only destructed when globally there are no living references to it.</source>
          <target state="translated">Make a remote call to run &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; and return an &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt; to the result value immediately. Worker &lt;code&gt;to&lt;/code&gt; will be the owner of the returned &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;, and the worker calling &lt;code&gt;remote&lt;/code&gt; is a user. The owner manages the global reference count of its &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;, and the owner &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt; is only destructed when globally there are no living references to it.</target>
        </trans-unit>
        <trans-unit id="f909b867fd9b369b49e7c33ae5b4db26cd626b58" translate="yes" xml:space="preserve">
          <source>Make sure that &lt;code&gt;MASTER_ADDR&lt;/code&gt; and &lt;code&gt;MASTER_PORT&lt;/code&gt; are set properly on both workers. Refer to &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; API for more details. For example,</source>
          <target state="translated">Make sure that &lt;code&gt;MASTER_ADDR&lt;/code&gt; and &lt;code&gt;MASTER_PORT&lt;/code&gt; are set properly on both workers. Refer to &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; API for more details. For example,</target>
        </trans-unit>
        <trans-unit id="3140663730440e28711adcbaaf1e5be8f31a832b" translate="yes" xml:space="preserve">
          <source>Makes a &lt;code&gt;cls&lt;/code&gt; instance with the same data pointer as &lt;code&gt;self&lt;/code&gt;. Changes in the output mirror changes in &lt;code&gt;self&lt;/code&gt;, and the output stays attached to the autograd graph. &lt;code&gt;cls&lt;/code&gt; must be a subclass of &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">Makes a &lt;code&gt;cls&lt;/code&gt; instance with the same data pointer as &lt;code&gt;self&lt;/code&gt; . Changes in the output mirror changes in &lt;code&gt;self&lt;/code&gt; , and the output stays attached to the autograd graph. &lt;code&gt;cls&lt;/code&gt; must be a subclass of &lt;code&gt;Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="49e2977180d59fe62d760df21fb871ec41183f71" translate="yes" xml:space="preserve">
          <source>Manipulating dimensions</source>
          <target state="translated">Manipulating dimensions</target>
        </trans-unit>
        <trans-unit id="6d825496aa8e7368b658938db8b142e79e1f39ee" translate="yes" xml:space="preserve">
          <source>Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like &lt;a href=&quot;../tensors#torch.Tensor.expand&quot;&gt;&lt;code&gt;torch.Tensor.expand()&lt;/code&gt;&lt;/a&gt;, are easier to read and are therefore more advisable to use.</source>
          <target state="translated">Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like &lt;a href=&quot;../tensors#torch.Tensor.expand&quot;&gt; &lt;code&gt;torch.Tensor.expand()&lt;/code&gt; &lt;/a&gt;, are easier to read and are therefore more advisable to use.</target>
        </trans-unit>
        <trans-unit id="877edbec3bdc8c92a89c77b353c1bffa8a6d3f53" translate="yes" xml:space="preserve">
          <source>Many of Python&amp;rsquo;s &lt;a href=&quot;https://docs.python.org/3/library/functions.html&quot;&gt;built-in functions&lt;/a&gt; are supported in TorchScript. The &lt;a href=&quot;https://docs.python.org/3/library/math.html#module-math&quot;&gt;&lt;code&gt;math&lt;/code&gt;&lt;/a&gt; module is also supported (see &lt;a href=&quot;jit_builtin_functions#math-module&quot;&gt;math Module&lt;/a&gt; for details), but no other Python modules (built-in or third party) are supported.</source>
          <target state="translated">Many of Python&amp;rsquo;s &lt;a href=&quot;https://docs.python.org/3/library/functions.html&quot;&gt;built-in functions&lt;/a&gt; are supported in TorchScript. The &lt;a href=&quot;https://docs.python.org/3/library/math.html#module-math&quot;&gt; &lt;code&gt;math&lt;/code&gt; &lt;/a&gt; module is also supported (see &lt;a href=&quot;jit_builtin_functions#math-module&quot;&gt;math Module&lt;/a&gt; for details), but no other Python modules (built-in or third party) are supported.</target>
        </trans-unit>
        <trans-unit id="8b9cac864fb0630d3e0375c972324ae14f0a7cbf" translate="yes" xml:space="preserve">
          <source>MarginRankingLoss</source>
          <target state="translated">MarginRankingLoss</target>
        </trans-unit>
        <trans-unit id="3b3b6161e2be4020ea8045bf777a1b7a9a9ef9d7" translate="yes" xml:space="preserve">
          <source>Mask R-CNN</source>
          <target state="translated">Mask R-CNN</target>
        </trans-unit>
        <trans-unit id="de79ab812f56430c47ab7094aa9e5f95c476c9d5" translate="yes" xml:space="preserve">
          <source>Mask R-CNN ResNet-50 FPN</source>
          <target state="translated">Mask R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="e283616925f71932f219908f30d53b3df610dd54" translate="yes" xml:space="preserve">
          <source>Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</target>
        </trans-unit>
        <trans-unit id="370101c55e76bb32e62a76c0f757b67a5a083820" translate="yes" xml:space="preserve">
          <source>Math operations</source>
          <target state="translated">수학 연산</target>
        </trans-unit>
        <trans-unit id="f266f996d2555bfc5ac129ce470aa82f278b2388" translate="yes" xml:space="preserve">
          <source>Matrix multiplication ops: &lt;a href=&quot;name_inference#contracts-away-dims-doc&quot;&gt;Contracts away dims&lt;/a&gt;</source>
          <target state="translated">Matrix multiplication ops: &lt;a href=&quot;name_inference#contracts-away-dims-doc&quot;&gt;Contracts away dims&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7a75fc5d78987f3dab3a65065c4b1d4f0692e098" translate="yes" xml:space="preserve">
          <source>Matrix product of two tensors.</source>
          <target state="translated">Matrix product of two tensors.</target>
        </trans-unit>
        <trans-unit id="eccdf32fe79dd9c576c8118d98c630238fca6bb7" translate="yes" xml:space="preserve">
          <source>MaxPool1d</source>
          <target state="translated">MaxPool1d</target>
        </trans-unit>
        <trans-unit id="3b3d541959fe8a8dd18b510e5a3f48b2498a53b4" translate="yes" xml:space="preserve">
          <source>MaxPool2d</source>
          <target state="translated">MaxPool2d</target>
        </trans-unit>
        <trans-unit id="98908ed042b1a9862ea5e9d4da6ed687e958f368" translate="yes" xml:space="preserve">
          <source>MaxPool3d</source>
          <target state="translated">MaxPool3d</target>
        </trans-unit>
        <trans-unit id="4a5419bdd0aea2636e49e9d7ca08451ce7530629" translate="yes" xml:space="preserve">
          <source>MaxUnpool1d</source>
          <target state="translated">MaxUnpool1d</target>
        </trans-unit>
        <trans-unit id="dde49d17029a94b9195f0444462e31e369394942" translate="yes" xml:space="preserve">
          <source>MaxUnpool2d</source>
          <target state="translated">MaxUnpool2d</target>
        </trans-unit>
        <trans-unit id="cbb9a5d6b569b7fb0a0f22c9d9d9d31e12466046" translate="yes" xml:space="preserve">
          <source>MaxUnpool3d</source>
          <target state="translated">MaxUnpool3d</target>
        </trans-unit>
        <trans-unit id="fe4de202eb4956062e7cfb72ed4b684ffbb5bd30" translate="yes" xml:space="preserve">
          <source>Measures the element-wise mean squared error.</source>
          <target state="translated">Measures the element-wise mean squared error.</target>
        </trans-unit>
        <trans-unit id="09b86d975dd6b1f5a73e1c3517013bb23d540761" translate="yes" xml:space="preserve">
          <source>Measures the loss given an input tensor</source>
          <target state="translated">Measures the loss given an input tensor</target>
        </trans-unit>
        <trans-unit id="1d01e0fee07072aec458beb4c034b607f10f63a9" translate="yes" xml:space="preserve">
          <source>Members:</source>
          <target state="translated">Members:</target>
        </trans-unit>
        <trans-unit id="7c5f0da2191ca27795b8631522c1ca879f1716c9" translate="yes" xml:space="preserve">
          <source>Method Calls</source>
          <target state="translated">Method Calls</target>
        </trans-unit>
        <trans-unit id="b69c14783d5324daa62920c742f93c096f643f4f" translate="yes" xml:space="preserve">
          <source>Methods which mutate a tensor are marked with an underscore suffix. For example, &lt;code&gt;torch.FloatTensor.abs_()&lt;/code&gt; computes the absolute value in-place and returns the modified tensor, while &lt;code&gt;torch.FloatTensor.abs()&lt;/code&gt; computes the result in a new tensor.</source>
          <target state="translated">Methods which mutate a tensor are marked with an underscore suffix. For example, &lt;code&gt;torch.FloatTensor.abs_()&lt;/code&gt; computes the absolute value in-place and returns the modified tensor, while &lt;code&gt;torch.FloatTensor.abs()&lt;/code&gt; computes the result in a new tensor.</target>
        </trans-unit>
        <trans-unit id="9b7c339ea87ccc784ad1deadd85bf646248e9d73" translate="yes" xml:space="preserve">
          <source>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</source>
          <target state="translated">Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</target>
        </trans-unit>
        <trans-unit id="e067f6663a0605e92a7219ee53ccfb43fa76eaf3" translate="yes" xml:space="preserve">
          <source>Migrating to PyTorch 1.2 Recursive Scripting API</source>
          <target state="translated">Migrating to PyTorch 1.2 Recursive Scripting API</target>
        </trans-unit>
        <trans-unit id="ff26bc094b922f3c06f27a4d1e69dcd28d4c0e3c" translate="yes" xml:space="preserve">
          <source>Mixing Tracing and Scripting</source>
          <target state="translated">Mixing Tracing and Scripting</target>
        </trans-unit>
        <trans-unit id="9ff690f5176e9e122a08292f5ff8b357a6c354e0" translate="yes" xml:space="preserve">
          <source>MobileNet V2</source>
          <target state="translated">MobileNet V2</target>
        </trans-unit>
        <trans-unit id="c901047dcc843db7018568e9b72b5dbf5a54540d" translate="yes" xml:space="preserve">
          <source>MobileNet v2</source>
          <target state="translated">MobileNet v2</target>
        </trans-unit>
        <trans-unit id="b8ff02892916ff59f7fbd4e617fccd01f6bca576" translate="yes" xml:space="preserve">
          <source>Module</source>
          <target state="translated">Module</target>
        </trans-unit>
        <trans-unit id="9c3f2aba6913de8da00faed8e2df2428bb1257b7" translate="yes" xml:space="preserve">
          <source>Module Attributes</source>
          <target state="translated">모듈 속성</target>
        </trans-unit>
        <trans-unit id="3cb22c5e4eabc0e7d3ccf2b60e66f53421b63149" translate="yes" xml:space="preserve">
          <source>Module Index</source>
          <target state="translated">모듈 색인</target>
        </trans-unit>
        <trans-unit id="0b2ce983642fdce8da88af1aafe61bc6a6ee4bfb" translate="yes" xml:space="preserve">
          <source>ModuleDict</source>
          <target state="translated">ModuleDict</target>
        </trans-unit>
        <trans-unit id="e12ef0c5e95f31ebc597e0b6fd821d6b604f3f95" translate="yes" xml:space="preserve">
          <source>ModuleList</source>
          <target state="translated">ModuleList</target>
        </trans-unit>
        <trans-unit id="04e9462c0ff02bb9032b92abd45881a3c7e15fb7" translate="yes" xml:space="preserve">
          <source>Modules</source>
          <target state="translated">Modules</target>
        </trans-unit>
        <trans-unit id="9dbae4788d0b2dea68c6e66c9e0ff2aed52f652e" translate="yes" xml:space="preserve">
          <source>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:</source>
          <target state="translated">Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:</target>
        </trans-unit>
        <trans-unit id="4a649c7ebd14c451b9eadae1b57b3c6722590226" translate="yes" xml:space="preserve">
          <source>More Information about RPC Autograd</source>
          <target state="translated">More Information about RPC Autograd</target>
        </trans-unit>
        <trans-unit id="9cb54dedb558ff4177e32b924117d76eb6014c23" translate="yes" xml:space="preserve">
          <source>More Information about RRef</source>
          <target state="translated">More Information about RRef</target>
        </trans-unit>
        <trans-unit id="cea97ccd7b3c8bf2304a429a1f4a38f55fbcf7bc" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1704.07483&quot;&gt;Continuously Differentiable Exponential Linear Units&lt;/a&gt; .</source>
          <target state="translated">More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1704.07483&quot;&gt;Continuously Differentiable Exponential Linear Units&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4458c625976778e29149ea1b97ed51860f9fe8ca" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1706.02515&quot;&gt;Self-Normalizing Neural Networks&lt;/a&gt; .</source>
          <target state="translated">More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1706.02515&quot;&gt;Self-Normalizing Neural Networks&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8edf225c0a72c5fe13ece57cbf2f54c6b24e17b0" translate="yes" xml:space="preserve">
          <source>More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</target>
        </trans-unit>
        <trans-unit id="1d81819b85f2cb3da872c9da3d0ec33cdc7aecdb" translate="yes" xml:space="preserve">
          <source>More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</target>
        </trans-unit>
        <trans-unit id="304f7ec57645990453077c598f4fe8c37d2314ab" translate="yes" xml:space="preserve">
          <source>More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</target>
        </trans-unit>
        <trans-unit id="f38e0558c4bf9056c21ea63c5960cbf11fd1b539" translate="yes" xml:space="preserve">
          <source>More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</target>
        </trans-unit>
        <trans-unit id="599668aa340279a3862bca37e7e4a6db462ba962" translate="yes" xml:space="preserve">
          <source>More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.</source>
          <target state="translated">More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.</target>
        </trans-unit>
        <trans-unit id="805667fbfb7f6f1745944861662036a8e58f3c82" translate="yes" xml:space="preserve">
          <source>Moreover, as for &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;, the values of &lt;code&gt;index&lt;/code&gt; must be between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;self.size(dim) - 1&lt;/code&gt; inclusive, and all values in a row along the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; must be unique.</source>
          <target state="translated">Moreover, as for &lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;, the values of &lt;code&gt;index&lt;/code&gt; must be between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;self.size(dim) - 1&lt;/code&gt; inclusive, and all values in a row along the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt; must be unique.</target>
        </trans-unit>
        <trans-unit id="dbc5f1c387e0131e10dadc48c7f1ed13f16fda56" translate="yes" xml:space="preserve">
          <source>Most attribute types can be inferred, so &lt;code&gt;torch.jit.Attribute&lt;/code&gt; is not necessary. For empty container types, annotate their types using &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations.</source>
          <target state="translated">Most attribute types can be inferred, so &lt;code&gt;torch.jit.Attribute&lt;/code&gt; is not necessary. For empty container types, annotate their types using &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations.</target>
        </trans-unit>
        <trans-unit id="ed994a7cd5d0a3520430a627f9865ebc68ca6011" translate="yes" xml:space="preserve">
          <source>Moved to &lt;code&gt;torch.hub&lt;/code&gt;.</source>
          <target state="translated">Moved to &lt;code&gt;torch.hub&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2b0f27b71a53a0f7447384ac613f55c4204b1204" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the CPU.</source>
          <target state="translated">Moves all model parameters and buffers to the CPU.</target>
        </trans-unit>
        <trans-unit id="e8a79081679af4528b212d87c458a607691d5b3e" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the GPU.</source>
          <target state="translated">Moves all model parameters and buffers to the GPU.</target>
        </trans-unit>
        <trans-unit id="e874c62b11df47c2d692fb8935adeff7a382eea3" translate="yes" xml:space="preserve">
          <source>Moves and/or casts the parameters and buffers.</source>
          <target state="translated">Moves and/or casts the parameters and buffers.</target>
        </trans-unit>
        <trans-unit id="7729e2003ebf66fb44071aeaadd11a5376f40f12" translate="yes" xml:space="preserve">
          <source>Moves the dimension(s) of &lt;code&gt;input&lt;/code&gt; at the position(s) in &lt;code&gt;source&lt;/code&gt; to the position(s) in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="translated">Moves the dimension(s) of &lt;code&gt;input&lt;/code&gt; at the position(s) in &lt;code&gt;source&lt;/code&gt; to the position(s) in &lt;code&gt;destination&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="35ac3446a4eebf31c31aeb5dad10efd9c64f3049" translate="yes" xml:space="preserve">
          <source>Moves the storage to shared memory.</source>
          <target state="translated">Moves the storage to shared memory.</target>
        </trans-unit>
        <trans-unit id="7b5555e73ea9cfb4e4bc00be9e2f28d1b533e2f3" translate="yes" xml:space="preserve">
          <source>Moves the underlying storage to shared memory.</source>
          <target state="translated">Moves the underlying storage to shared memory.</target>
        </trans-unit>
        <trans-unit id="8a37b99360165eb79c6362b8c59ca260f24fc9e9" translate="yes" xml:space="preserve">
          <source>Multi-GPU collective functions</source>
          <target state="translated">Multi-GPU collective functions</target>
        </trans-unit>
        <trans-unit id="f8c9042e53df42e885d4b0f01c778b6c31356f24" translate="yes" xml:space="preserve">
          <source>Multi-Node multi-process distributed training: (e.g. two nodes)</source>
          <target state="translated">Multi-Node multi-process distributed training: (e.g. two nodes)</target>
        </trans-unit>
        <trans-unit id="e8e85a354a72efff2947c00f8ff7c48fed5f6652" translate="yes" xml:space="preserve">
          <source>MultiHead</source>
          <target state="translated">MultiHead</target>
        </trans-unit>
        <trans-unit id="aaf9b486dc5c2543a40061716ec697ddb8e0fbae" translate="yes" xml:space="preserve">
          <source>MultiLabelMarginLoss</source>
          <target state="translated">MultiLabelMarginLoss</target>
        </trans-unit>
        <trans-unit id="0bd3257d9c850deeb628f346481873a6ea866f39" translate="yes" xml:space="preserve">
          <source>MultiLabelSoftMarginLoss</source>
          <target state="translated">MultiLabelSoftMarginLoss</target>
        </trans-unit>
        <trans-unit id="65baaa8172153705ce14a7161536175046e824b3" translate="yes" xml:space="preserve">
          <source>MultiMarginLoss</source>
          <target state="translated">MultiMarginLoss</target>
        </trans-unit>
        <trans-unit id="d53b7a9617160a0821d5bb5cb1a941ffdfcd6b08" translate="yes" xml:space="preserve">
          <source>MultiheadAttention</source>
          <target state="translated">MultiheadAttention</target>
        </trans-unit>
        <trans-unit id="7814aed0b837819b215ee4613ccc9b3a4ba6040e" translate="yes" xml:space="preserve">
          <source>Multiple Assignments</source>
          <target state="translated">Multiple Assignments</target>
        </trans-unit>
        <trans-unit id="67620498370f67e43d30e9f331640d50be1f106c" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="translated">Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt; ) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by ( &lt;code&gt;input&lt;/code&gt; , &lt;code&gt;input2&lt;/code&gt; )).</target>
        </trans-unit>
        <trans-unit id="76de5400e24dedf164da206bd33a31ebb079cf8f" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="translated">Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt; ) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by ( &lt;code&gt;input&lt;/code&gt; , &lt;code&gt;input2&lt;/code&gt; )).</target>
        </trans-unit>
        <trans-unit id="329fd1dd91c45edaf7c72f188795f5545aaec268" translate="yes" xml:space="preserve">
          <source>Multiplies each element of the input &lt;code&gt;input&lt;/code&gt; with the scalar &lt;code&gt;other&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">Multiplies each element of the input &lt;code&gt;input&lt;/code&gt; with the scalar &lt;code&gt;other&lt;/code&gt; and returns a new resulting tensor.</target>
        </trans-unit>
        <trans-unit id="4d2004ec3a414cc551a0c27958276999f72ee752" translate="yes" xml:space="preserve">
          <source>Multiprocessing best practices</source>
          <target state="translated">Multiprocessing best practices</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="129ca909e4d3ed97aba21ae73582b3bd13e114f8" translate="yes" xml:space="preserve">
          <source>N = \text{batch size}</source>
          <target state="translated">N = \text{batch size}</target>
        </trans-unit>
        <trans-unit id="9008c254267c7b8e353b34e82cc9a3ccdffad81e" translate="yes" xml:space="preserve">
          <source>N \times 2 \times 3</source>
          <target state="translated">N \times 2 \times 3</target>
        </trans-unit>
        <trans-unit id="1b8b351ea25ef1c009076f028a05723a16aa0eb6" translate="yes" xml:space="preserve">
          <source>N \times 3 \times 4</source>
          <target state="translated">N \times 3 \times 4</target>
        </trans-unit>
        <trans-unit id="004d23aba5b55baf9486f9aab3e50a94de7147b2" translate="yes" xml:space="preserve">
          <source>N \times C \times D \times H \times W</source>
          <target state="translated">N \times C \times D \times H \times W</target>
        </trans-unit>
        <trans-unit id="14734700ec8d366b9e0bb39033413068e35b488f" translate="yes" xml:space="preserve">
          <source>N \times C \times H \times W</source>
          <target state="translated">N \times C \times H \times W</target>
        </trans-unit>
        <trans-unit id="db16fdaa05353f31c7f5c303d74f40ba9971f144" translate="yes" xml:space="preserve">
          <source>N \times H \times W \times 2</source>
          <target state="translated">N \times H \times W \times 2</target>
        </trans-unit>
        <trans-unit id="7fd20744aa54e5cfb63458f679b6501918e1991e" translate="yes" xml:space="preserve">
          <source>N \times M</source>
          <target state="translated">N \times M</target>
        </trans-unit>
        <trans-unit id="c79a64aa7f7a8e49dfba455783b75f9a68074b19" translate="yes" xml:space="preserve">
          <source>N is the batch size, &lt;code&gt;*&lt;/code&gt; means any number of additional dimensions</source>
          <target state="translated">N is the batch size, &lt;code&gt;*&lt;/code&gt; means any number of additional dimensions</target>
        </trans-unit>
        <trans-unit id="72e211ac7f7abd7ef74bebbff3bdb5e0da1f0db9" translate="yes" xml:space="preserve">
          <source>N-D</source>
          <target state="translated">N-D</target>
        </trans-unit>
        <trans-unit id="7610334d74930225de51e4e4fce7e6de6c627870" translate="yes" xml:space="preserve">
          <source>NCCL has also provided a number of environment variables for fine-tuning purposes.</source>
          <target state="translated">NCCL has also provided a number of environment variables for fine-tuning purposes.</target>
        </trans-unit>
        <trans-unit id="165a578116c3590cf83add849da91470c0378dc0" translate="yes" xml:space="preserve">
          <source>NLLLoss</source>
          <target state="translated">NLLLoss</target>
        </trans-unit>
        <trans-unit id="c63922691a03fc61bc5d30e1155494b0495ebce4" translate="yes" xml:space="preserve">
          <source>NN module forward passes have code that don&amp;rsquo;t support named tensors and will error out appropriately.</source>
          <target state="translated">NN module forward passes have code that don&amp;rsquo;t support named tensors and will error out appropriately.</target>
        </trans-unit>
        <trans-unit id="fb2adaff54f80ebafd8d75fb8e20d7e6464bee58" translate="yes" xml:space="preserve">
          <source>NN module parameters are unnamed, so outputs may be partially named.</source>
          <target state="translated">NN module parameters are unnamed, so outputs may be partially named.</target>
        </trans-unit>
        <trans-unit id="fb77d2b5cb211a1bafb61c44a7f17e0c98a57348" translate="yes" xml:space="preserve">
          <source>NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:</source>
          <target state="translated">NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:</target>
        </trans-unit>
        <trans-unit id="855336587fa59262965cdb9a2a6114933586800b" translate="yes" xml:space="preserve">
          <source>N_i</source>
          <target state="translated">N_i</target>
        </trans-unit>
        <trans-unit id="c543b3c390ed1ce750f7a99ce30092a4fe11d2d6" translate="yes" xml:space="preserve">
          <source>NaN values in &lt;code&gt;grid&lt;/code&gt; would be interpreted as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">NaN values in &lt;code&gt;grid&lt;/code&gt; would be interpreted as &lt;code&gt;-1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="709a23220f2c3d64d1e1d6d18c4d5280f8d82fca" translate="yes" xml:space="preserve">
          <source>Name</source>
          <target state="translated">Name</target>
        </trans-unit>
        <trans-unit id="0585ffb115d09e675d597261b20f2d75a3eb03a7" translate="yes" xml:space="preserve">
          <source>Name propagation semantics</source>
          <target state="translated">Name propagation semantics</target>
        </trans-unit>
        <trans-unit id="817cc3c2ad5448f94a4d253bbcac8dd6cc2d1cf9" translate="yes" xml:space="preserve">
          <source>Named Tensors</source>
          <target state="translated">Named Tensors</target>
        </trans-unit>
        <trans-unit id="e90e823237a9f4d7fe13c9da511106ba9aee2f03" translate="yes" xml:space="preserve">
          <source>Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support &amp;ldquo;broadcasting by name&amp;rdquo; rather than &amp;ldquo;broadcasting by position&amp;rdquo;.</source>
          <target state="translated">Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support &amp;ldquo;broadcasting by name&amp;rdquo; rather than &amp;ldquo;broadcasting by position&amp;rdquo;.</target>
        </trans-unit>
        <trans-unit id="6e3e9a74ab4dd7c6ec7d57c873cd59674e123d29" translate="yes" xml:space="preserve">
          <source>Named Tensors operator coverage</source>
          <target state="translated">Named Tensors operator coverage</target>
        </trans-unit>
        <trans-unit id="00f19d99a1a121465f53068cd3e37c47ef1fad72" translate="yes" xml:space="preserve">
          <source>Named Tuples</source>
          <target state="translated">명명 된 튜플</target>
        </trans-unit>
        <trans-unit id="d906a9f233a4d60e7ff3266a36abd411378985f0" translate="yes" xml:space="preserve">
          <source>Named dimensions</source>
          <target state="translated">Named dimensions</target>
        </trans-unit>
        <trans-unit id="d69a85347956dd430a196f336efedfda312c73cf" translate="yes" xml:space="preserve">
          <source>Named dimensions, like regular Tensor dimensions, are ordered. &lt;code&gt;tensor.names[i]&lt;/code&gt; is the name of dimension &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">Named dimensions, like regular Tensor dimensions, are ordered. &lt;code&gt;tensor.names[i]&lt;/code&gt; is the name of dimension &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="87ade27d950d661d18340b5de9b3d2e0c2886884" translate="yes" xml:space="preserve">
          <source>Named tensor API reference</source>
          <target state="translated">Named tensor API reference</target>
        </trans-unit>
        <trans-unit id="554d6e43a251766628f65b287dfa2b6bb568c6f8" translate="yes" xml:space="preserve">
          <source>Named tensors can coexist with unnamed tensors; named tensors are instances of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. Unnamed tensors have &lt;code&gt;None&lt;/code&gt;-named dimensions. Named tensors do not require all dimensions to be named.</source>
          <target state="translated">Named tensors can coexist with unnamed tensors; named tensors are instances of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;. Unnamed tensors have &lt;code&gt;None&lt;/code&gt; -named dimensions. Named tensors do not require all dimensions to be named.</target>
        </trans-unit>
        <trans-unit id="9babc66ca4089f8e16cccf70d5e5125ac942af22" translate="yes" xml:space="preserve">
          <source>Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called &lt;em&gt;name inference&lt;/em&gt;. More formally, name inference consists of the following two steps:</source>
          <target state="translated">Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called &lt;em&gt;name inference&lt;/em&gt;. More formally, name inference consists of the following two steps:</target>
        </trans-unit>
        <trans-unit id="ce14504f9d8c7c9312b2871ff6009782be1ad451" translate="yes" xml:space="preserve">
          <source>Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv&lt;/a&gt;).</source>
          <target state="translated">Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv&lt;/a&gt;).</target>
        </trans-unit>
        <trans-unit id="1589b41367a72480492d7a5904aab37522c48d37" translate="yes" xml:space="preserve">
          <source>Negative log likelihood loss with Poisson distribution of target.</source>
          <target state="translated">Negative log likelihood loss with Poisson distribution of target.</target>
        </trans-unit>
        <trans-unit id="53ebc572b4a44802ba114729f07bdaaf5409a9d7" translate="yes" xml:space="preserve">
          <source>Network</source>
          <target state="translated">Network</target>
        </trans-unit>
        <trans-unit id="813b5b686d397a3b63ce050a22e4bcaa735af1e8" translate="yes" xml:space="preserve">
          <source>New API:</source>
          <target state="translated">New API:</target>
        </trans-unit>
        <trans-unit id="540f7186c4d530bac579f69bb157fd5e121529cc" translate="yes" xml:space="preserve">
          <source>NewType</source>
          <target state="translated">NewType</target>
        </trans-unit>
        <trans-unit id="0013bf26fef8048809165bc5fc20da2af938e96c" translate="yes" xml:space="preserve">
          <source>No expressions except method definitions are allowed in the body of the class.</source>
          <target state="translated">No expressions except method definitions are allowed in the body of the class.</target>
        </trans-unit>
        <trans-unit id="7a05537029cdbc40cf12ae1af3fd5798f2e0fb52" translate="yes" xml:space="preserve">
          <source>No support for inheritance or any other polymorphism strategy, except for inheriting from &lt;code&gt;object&lt;/code&gt; to specify a new-style class.</source>
          <target state="translated">No support for inheritance or any other polymorphism strategy, except for inheriting from &lt;code&gt;object&lt;/code&gt; to specify a new-style class.</target>
        </trans-unit>
        <trans-unit id="16585665bdbf85c2a46ca04dcdf6f3b3c87a4c04" translate="yes" xml:space="preserve">
          <source>No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.</source>
          <target state="translated">No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.</target>
        </trans-unit>
        <trans-unit id="3a122cf075b0ba371ed052ceb0f012b712c9d692" translate="yes" xml:space="preserve">
          <source>Node 1: &lt;em&gt;(IP: 192.168.1.1, and has a free port: 1234)&lt;/em&gt;</source>
          <target state="translated">Node 1: &lt;em&gt;(IP: 192.168.1.1, and has a free port: 1234)&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="d7c20c7d93b71387bce45a0f532f94aceec659b1" translate="yes" xml:space="preserve">
          <source>Node 2:</source>
          <target state="translated">Node 2:</target>
        </trans-unit>
        <trans-unit id="2200871a1ac28c0b5dd797edb44ce5069b3ba408" translate="yes" xml:space="preserve">
          <source>Nominal typing is in development, but structural typing is not</source>
          <target state="translated">공칭 타이핑은 개발 중이지만 구조 타이핑은 아닙니다.</target>
        </trans-unit>
        <trans-unit id="56c04398e9678198426e682b1f9d96427b74b23b" translate="yes" xml:space="preserve">
          <source>Nominal vs structural subtyping</source>
          <target state="translated">공칭 vs 구조적 서브 타이핑</target>
        </trans-unit>
        <trans-unit id="b89eadfaef5d8d82b020885869aa96878170cfaa" translate="yes" xml:space="preserve">
          <source>Non-ATen operators</source>
          <target state="translated">비 ATEN 연산자</target>
        </trans-unit>
        <trans-unit id="05ac3d4369dff83cd256460b1c7ab61b3d6873b9" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (other)</source>
          <target state="translated">비선형 활성화 (기타)</target>
        </trans-unit>
        <trans-unit id="37e8f09f37b4ba586b29c8d43264a706f392c534" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (weighted sum, nonlinearity)</source>
          <target state="translated">비선형 활성화 (가중 합, 비선형 성)</target>
        </trans-unit>
        <trans-unit id="90f4c5f6b363fba151454b4dc3811eace77006ea" translate="yes" xml:space="preserve">
          <source>Non-linear activation functions</source>
          <target state="translated">비선형 활성화 함수</target>
        </trans-unit>
        <trans-unit id="32cfd0f165a52f907e24f0d0d41b54665a307df6" translate="yes" xml:space="preserve">
          <source>Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in &lt;a href=&quot;#use-of-python-values&quot;&gt;Use of Python Values&lt;/a&gt;.</source>
          <target state="translated">비 지역 변수는 함수가 정의 될 때 컴파일 타임에 Python 값으로 확인됩니다. 그런 다음 이러한 값은 &lt;a href=&quot;#use-of-python-values&quot;&gt;Python 값 사용에&lt;/a&gt; 설명 된 규칙을 사용하여 TorchScript 값으로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="6eef6648406c333a4035cd5e60d0bf2ecf2606d7" translate="yes" xml:space="preserve">
          <source>None</source>
          <target state="translated">None</target>
        </trans-unit>
        <trans-unit id="46e38cc5e7e0b2b170857112460a732303fdb5e3" translate="yes" xml:space="preserve">
          <source>Normalization Layers</source>
          <target state="translated">정규화 레이어</target>
        </trans-unit>
        <trans-unit id="746e3a4c0d98d2df15064221df23fa7793e56038" translate="yes" xml:space="preserve">
          <source>Normalization functions</source>
          <target state="translated">정규화 기능</target>
        </trans-unit>
        <trans-unit id="c4639d9330cdd597089734b95310026444f4de7e" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fae17255f7a6267cdf6f973a75edad225dcf4d82" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="37141e0de089638ade05f2fc80c5c6e1a4b3e953" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d960941bb7a853f66a4f14f14d7f53d0f7022466" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역변환 ( &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="7b81cd890e0116574dbf7776b7567225bd21e39c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e18f2dcd89118b40d295cc2d0f570dae9ee63571" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="20885ecc112a0654f2fd1749899c2aade057f18f" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="36f2290f1751dd4fd40961edc0b9d58c6d126bef" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f07c249cb9e5f2751b249e4c52c89f4d38befc44" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d3864acfcc3775dda73f1b742d452bc185e4b45c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">정규화 모드. 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; )의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ac15bca5bbea06cd32f056c8cbf634e4f1faea41" translate="yes" xml:space="preserve">
          <source>Not implemented</source>
          <target state="translated">구현되지 않음</target>
        </trans-unit>
        <trans-unit id="df8f8315e0c299c571aa4226e66410f1097a424b" translate="yes" xml:space="preserve">
          <source>Not providing a value for &lt;code&gt;steps&lt;/code&gt; is deprecated. For backwards compatibility, not providing a value for &lt;code&gt;steps&lt;/code&gt; will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for &lt;code&gt;steps&lt;/code&gt; will throw a runtime error.</source>
          <target state="translated">&lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않는 것은 더 이상 사용되지 않습니다. 이전 버전과의 호환성을 위해 &lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않으면 요소가 100 개인 텐서가 생성됩니다. 이 동작은 문서화 된 함수 시그니처에 반영되지 않으며 의존해서는 안됩니다. 향후 PyTorch 릴리스에서 &lt;code&gt;steps&lt;/code&gt; 값을 제공하지 않으면 런타임 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2c924e3088204ee77ba681f72be3444357932fca" translate="yes" xml:space="preserve">
          <source>Note</source>
          <target state="translated">Note</target>
        </trans-unit>
        <trans-unit id="da7038b6159ab37164aeb7442907c881108aff7e" translate="yes" xml:space="preserve">
          <source>Note that</source>
          <target state="translated">참고</target>
        </trans-unit>
        <trans-unit id="5433b2e634d55f555a9239d7a13b88b55129e9b2" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt; before Python version 3.6) does not preserve the order of the merged mapping.</source>
          <target state="translated">다른 정렬되지 않은 매핑 유형 (예 : Python 버전 3.6 이전 의 Python의 일반 &lt;code&gt;dict&lt;/code&gt; &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt; 사용 하는 update () 는 병합 된 매핑의 순서를 유지하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a1dc440ad60db846cad7f7d0a7ce1260c129ea8c" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt;) does not preserve the order of the merged mapping.</source>
          <target state="translated">다른 정렬되지 않은 매핑 유형 (예 : Python의 일반 &lt;code&gt;dict&lt;/code&gt; ) 을 &lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt; 하는 update () 는 병합 된 매핑의 순서를 유지하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="42c994d9673f83cc70c78e02c5d9a230c6566874" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; in &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt; are used to &lt;strong&gt;instantiate&lt;/strong&gt; a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is</source>
          <target state="translated">참고 &lt;code&gt;*args&lt;/code&gt; 와 &lt;code&gt;**kwargs&lt;/code&gt; 로 의 &lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt; 사용됩니다 &lt;strong&gt;인스턴스화&lt;/strong&gt; 모델을. 모델을로드 한 후 모델로 수행 할 수있는 작업을 어떻게 알 수 있습니까? 제안 된 워크 플로는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="2d1e0d8414eaec791533b2f0922dd4ade580351c" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt; reductions are not available when using the &lt;code&gt;NCCL&lt;/code&gt; backend.</source>
          <target state="translated">참고 &lt;code&gt;BAND&lt;/code&gt; , &lt;code&gt;BOR&lt;/code&gt; 및 &lt;code&gt;BXOR&lt;/code&gt; 의 사용할 때 감소를 사용할 수없는 &lt;code&gt;NCCL&lt;/code&gt; 의 백엔드.</target>
        </trans-unit>
        <trans-unit id="0bd438191e4b3887c615246e19e8361b84bd83a7" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; and &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; is redundant. We can thus compute the forward transform without considering negative frequencies:</source>
          <target state="translated">참고 &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; 및 &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; 중복된다. 따라서 음의 주파수를 고려하지 않고 순방향 변환을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ee47ccc0fe1d0f9aca7dffbff0fc6bd81769f2e4" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;len(output_tensor_list)&lt;/code&gt; needs to be the same for all the distributed processes calling this function.</source>
          <target state="translated">참고 &lt;code&gt;len(output_tensor_list)&lt;/code&gt; 요구가이 함수를 호출하는 모든 분산 된 프로세스에 대해 동일합니다.</target>
        </trans-unit>
        <trans-unit id="d9706617db521f3c057495c1501777340ff2c154" translate="yes" xml:space="preserve">
          <source>Note that automatic rank assignment is not supported anymore in the latest distributed package and &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">자동 순위 할당은 최신 배포 패키지에서 더 이상 지원 되지 않으며 &lt;code&gt;group_name&lt;/code&gt; 도 더 이상 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d90621e782b71803aaf8f760cde86adc6ccf606c" translate="yes" xml:space="preserve">
          <source>Note that deterministic operations tend to have worse performance than non-deterministic operations.</source>
          <target state="translated">결정적 작업은 비 결정적 작업보다 성능이 떨어지는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="b7948c066b35cd06338c8b76a0036f57024ca522" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;input_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt;, since the function scatters the result from every single GPU in the group. To interpret each element of &lt;code&gt;input_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;output_tensor_list[j]&lt;/code&gt; of rank k receives the reduce-scattered result from &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">함수가 그룹의 모든 단일 GPU에서 결과를 분산 &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt; &lt;code&gt;input_tensor_lists&lt;/code&gt; 의 각 요소의 크기는 world_size * len (output_tensor_list) 입니다. 각 요소 해석하는 &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; 것을 참고 &lt;code&gt;output_tensor_list[j]&lt;/code&gt; 랭크는 K의 감소로부터 산란 결과 수신 &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f1fa921e9c60fd6eb9bf5d36ace2910b625e1764" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;output_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt;, since the function all gathers the result from every single GPU in the group. To interpret each element of &lt;code&gt;output_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;input_tensor_list[j]&lt;/code&gt; of rank k will be appear in &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;output_tensor_lists&lt;/code&gt; 의 각 요소의 크기는 &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt; . 모든 함수는 그룹의 모든 단일 GPU에서 결과를 수집하기 때문입니다. 각 요소 해석하는 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 것을 주목 &lt;code&gt;input_tensor_list[j]&lt;/code&gt; 랭크는 K의에 표시한다 &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="261fa08253a2249def1244a03fcd9b8841a48f8d" translate="yes" xml:space="preserve">
          <source>Note that multicast address is not supported anymore in the latest distributed package. &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">멀티 캐스트 주소는 최신 배포 패키지에서 더 이상 지원되지 않습니다. &lt;code&gt;group_name&lt;/code&gt; 도 더 이상 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b2830ad219b13cbec38ff7f3f532533bf250586f" translate="yes" xml:space="preserve">
          <source>Note that non-integer &lt;code&gt;step&lt;/code&gt; is subject to floating point rounding errors when comparing against &lt;code&gt;end&lt;/code&gt;; to avoid inconsistency, we advise adding a small epsilon to &lt;code&gt;end&lt;/code&gt; in such cases.</source>
          <target state="translated">정수가 아닌 &lt;code&gt;step&lt;/code&gt; 는 &lt;code&gt;end&lt;/code&gt; 와 비교할 때 부동 소수점 반올림 오류의 영향을받습니다 . 피할 불일치에, 우리가 할 수있는 작은 엡실론 추가 조언한다 &lt;code&gt;end&lt;/code&gt; 이러한 경우를.</target>
        </trans-unit>
        <trans-unit id="fe417e45eca8b56b8a391ee5d65f646c54c00fc4" translate="yes" xml:space="preserve">
          <source>Note that the input to LongTensor is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:</source>
          <target state="translated">LongTensor에 대한 입력은 인덱스 튜플 목록이 아닙니다. 이런 식으로 인덱스를 작성하려면 희소 생성자에 전달하기 전에 전치해야합니다.</target>
        </trans-unit>
        <trans-unit id="7e0f39780c8cac4e285c8ec3c405982d7cafc31f" translate="yes" xml:space="preserve">
          <source>Note that these cases may in fact be traceable in the future.</source>
          <target state="translated">이러한 경우는 실제로 향후 추적이 가능할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="752392a2ff1e2a39cc0db5f6e4585276ad451ffb" translate="yes" xml:space="preserve">
          <source>Note that this function is simply doing &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt;. Using that &lt;code&gt;isinstance&lt;/code&gt; check is better for typechecking with mypy, and more explicit - so it&amp;rsquo;s recommended to use that instead of &lt;code&gt;is_tensor&lt;/code&gt;.</source>
          <target state="translated">이 함수는 단순히 &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt; 합니다. 해당 &lt;code&gt;isinstance&lt;/code&gt; 검사를 사용하는 것이 mypy로 유형 검사를 수행하는 데 더 좋고 더 명시 &lt;code&gt;is_tensor&lt;/code&gt; 대신 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="a5b8b84abd5cbd1ff532fd7ae1e19baa9b3b1a28" translate="yes" xml:space="preserve">
          <source>Note that this function requires Python 3.4 or higher.</source>
          <target state="translated">이 함수에는 Python 3.4 이상이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="e898adc76bed905448d3327e88134b4893e4bf98" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;matplotlib&lt;/code&gt; package.</source>
          <target state="translated">이를 위해서는 &lt;code&gt;matplotlib&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="ba4e51216b174df96efb807dbfef6adf290a07f2" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;moviepy&lt;/code&gt; package.</source>
          <target state="translated">여기에는 &lt;code&gt;moviepy&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="da8c941a1c4dc21607ac70d896e4a7cb648a3554" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;pillow&lt;/code&gt; package.</source>
          <target state="translated">&lt;code&gt;pillow&lt;/code&gt; 패키지 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="38c3af3645ee3d8c1e85f997b98d93ebdc11ac44" translate="yes" xml:space="preserve">
          <source>Note that when &lt;code&gt;tracker&lt;/code&gt; stores Tensor objects from the LOBPCG instance, it must make copies of these.</source>
          <target state="translated">&lt;code&gt;tracker&lt;/code&gt; 가 LOBPCG 인스턴스에서 Tensor 개체를 저장할 때 이러한 개체의 복사본을 만들어야합니다.</target>
        </trans-unit>
        <trans-unit id="76d32111f7bcbce3e8104b2ef5376761705e1942" translate="yes" xml:space="preserve">
          <source>Note: A full example to apply nn.Transformer module for the word language model is available in &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt;</source>
          <target state="translated">참고 : 단어 언어 모델에 nn.Transformer 모듈을 적용하는 전체 예제는 &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt; 에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2acb81c5ebbb77d4e03260d44dbd15da614d8dba" translate="yes" xml:space="preserve">
          <source>Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode.</source>
          <target state="translated">참고 : 변환기 모델의 다중 헤드 어텐션 아키텍처로 인해 변환기의 출력 시퀀스 길이는 디코딩의 입력 시퀀스 (즉, 대상) 길이와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="ea3f023c9638ee2f77985080cfee0c3149276897" translate="yes" xml:space="preserve">
          <source>Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.</source>
          <target state="translated">참고 : 모델로드는 일반적인 사용 사례이지만 토크 나이저, 손실 함수 등과 같은 다른 개체를로드하는데도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fec631db96f94a854d531dcf201c32e0e5b12fec" translate="yes" xml:space="preserve">
          <source>Note: When beta is set to 0, this is equivalent to &lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt;&lt;code&gt;L1Loss&lt;/code&gt;&lt;/a&gt;. Passing a negative value in for beta will result in an exception.</source>
          <target state="translated">참고 : 베타가 0으로 설정되면 &lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt; &lt;code&gt;L1Loss&lt;/code&gt; &lt;/a&gt; 와 동일합니다 . 베타에 음수 값을 전달하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="9b7c2d4ed1b493db790041907421b97fbf38eae9" translate="yes" xml:space="preserve">
          <source>Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with &lt;code&gt;True&lt;/code&gt; are not allowed to attend while &lt;code&gt;False&lt;/code&gt; values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of &lt;code&gt;True&lt;/code&gt; will be ignored while the position with the value of &lt;code&gt;False&lt;/code&gt; will be unchanged.</source>
          <target state="translated">참고 : [src / tgt / memory] _mask는 위치 i가 마스크되지 않은 위치에 참석할 수 있도록합니다. ByteTensor가 제공되면 0이 아닌 위치는 참석할 수 없지만 0 위치는 변경되지 않습니다. BoolTensor가 제공되면 &lt;code&gt;True&lt;/code&gt; 인 위치는 참석할 수 없으며 &lt;code&gt;False&lt;/code&gt; 값은 변경되지 않습니다. FloatTensor가 제공되면주의 가중치에 추가됩니다. [src / tgt / memory] _key_padding_mask는주의에 의해 무시 될 키에 지정된 요소를 제공합니다. ByteTensor가 제공되면 0이 아닌 위치는 무시되고 0 위치는 변경되지 않습니다. BoolTensor가 제공되면 &lt;code&gt;True&lt;/code&gt; 값의 위치는 무시되고 &lt;code&gt;False&lt;/code&gt; 값의 위치는 무시됩니다. 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="70440046a3dc2e079f23ee1c57dfa76669b732aa" translate="yes" xml:space="preserve">
          <source>Notes</source>
          <target state="translated">Notes</target>
        </trans-unit>
        <trans-unit id="4425723195c4a3b16b476f69800fa3cf16abb9b4" translate="yes" xml:space="preserve">
          <source>Notice that if</source>
          <target state="translated">만약</target>
        </trans-unit>
        <trans-unit id="d1656ebe55b849d2893f4a0cf92db35aea6025bc" translate="yes" xml:space="preserve">
          <source>Notice that operators can also have associated &lt;code&gt;blocks&lt;/code&gt;, namely the &lt;code&gt;prim::Loop&lt;/code&gt; and &lt;code&gt;prim::If&lt;/code&gt; operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.</source>
          <target state="translated">연산자는 또한 연관된 &lt;code&gt;blocks&lt;/code&gt; , 즉 &lt;code&gt;prim::Loop&lt;/code&gt; 및 &lt;code&gt;prim::If&lt;/code&gt; 연산자를 가질 수 있습니다 . 그래프 출력에서 ​​이러한 연산자는 쉽게 디버깅 할 수 있도록 동등한 소스 코드 양식을 반영하도록 형식이 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="04a5b9046bd473cb428ae07e5d6a160de265bc5d" translate="yes" xml:space="preserve">
          <source>Notice that the symmetric element &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; is omitted. At the Nyquist frequency &lt;code&gt;T[-2] == T[2]&lt;/code&gt; is it&amp;rsquo;s own symmetric pair, and therefore must always be real-valued.</source>
          <target state="translated">대칭 요소 &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; 가 생략되었습니다. Nyquist 주파수에서 &lt;code&gt;T[-2] == T[2]&lt;/code&gt; 는 자체 대칭 쌍이므로 항상 실수 값이어야합니다.</target>
        </trans-unit>
        <trans-unit id="1956334a42d3e178212344356997b21e347fa273" translate="yes" xml:space="preserve">
          <source>Now PyTorch is able to export &lt;code&gt;elu&lt;/code&gt; operator.</source>
          <target state="translated">이제 PyTorch는 &lt;code&gt;elu&lt;/code&gt; 연산자 를 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a30a0ffebbaef4efa26c2a00eda09cf59167dfe6" translate="yes" xml:space="preserve">
          <source>Now the exported ONNX graph becomes:</source>
          <target state="translated">이제 내 보낸 ONNX 그래프는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="08a914cde05039694ef0194d9ee79ff9a79dde33" translate="yes" xml:space="preserve">
          <source>O</source>
          <target state="translated">O</target>
        </trans-unit>
        <trans-unit id="58e9484683a456865672c8a4517b530e0f41cd11" translate="yes" xml:space="preserve">
          <source>ONLY INDICES:</source>
          <target state="translated">유일한 지표 :</target>
        </trans-unit>
        <trans-unit id="b335243efa572b5f0beb01acb45b42a02be187b5" translate="yes" xml:space="preserve">
          <source>ONNX</source>
          <target state="translated">ONNX</target>
        </trans-unit>
        <trans-unit id="98099fd5d344320bb061ab40d4f82b1271288a2a" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN</source>
          <target state="translated">ONNX_ATEN</target>
        </trans-unit>
        <trans-unit id="7fd76316b691135ff7ad4e41541a37a2b2ed8c3d" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN_FALLBACK</source>
          <target state="translated">ONNX_ATEN_FALLBACK</target>
        </trans-unit>
        <trans-unit id="0976054bd35910dc334e574eb3e3c16bf4dc9b17" translate="yes" xml:space="preserve">
          <source>ONNX_FALLTHROUGH</source>
          <target state="translated">ONNX_FALLTHROUGH</target>
        </trans-unit>
        <trans-unit id="946938795b376f71c3a327dfe4ec5463d0103fa6" translate="yes" xml:space="preserve">
          <source>Object Detection, Instance Segmentation and Person Keypoint Detection</source>
          <target state="translated">물체 감지, 인스턴스 분할 및 사람 키포인트 감지</target>
        </trans-unit>
        <trans-unit id="8bf3249a3a8cece6d4343af20c9431018c926b24" translate="yes" xml:space="preserve">
          <source>Obtaining log-probabilities in a neural network is easily achieved by adding a &lt;code&gt;LogSoftmax&lt;/code&gt; layer in the last layer of your network. You may use &lt;code&gt;CrossEntropyLoss&lt;/code&gt; instead, if you prefer not to add an extra layer.</source>
          <target state="translated">네트워크 의 마지막 계층에 &lt;code&gt;LogSoftmax&lt;/code&gt; 계층 을 추가하면 신경망에서 로그 확률을 쉽게 얻을 수 있습니다. 추가 레이어를 추가하지 않으 &lt;code&gt;CrossEntropyLoss&lt;/code&gt; 대신 CrossEntropyLoss 를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b98c7204267732ffe5bc04ce20cbadbe8642e52b" translate="yes" xml:space="preserve">
          <source>Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you&amp;rsquo;re evaluating. If the profiler outputs don&amp;rsquo;t help, you could try looking at the result of &lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;nvprof&lt;/code&gt;. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline.</source>
          <target state="translated">물론 현실은 훨씬 더 복잡하며 평가하는 모델의 부분에 따라 스크립트가이 두 극단 중 하나가 아닐 수도 있습니다. 프로파일 러 출력이 도움이되지 않으면 &lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt; &lt;/a&gt; 하여 torch.autograd.profiler.emit_nvtx () 의 결과 를 &lt;code&gt;nvprof&lt;/code&gt; 있습니다. 그러나 NVTX 오버 헤드가 매우 높고 종종 심하게 왜곡 된 타임 라인을 제공한다는 점을 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="c37dc6bfd2614888384f13ab211221f9b141d421" translate="yes" xml:space="preserve">
          <source>Old API:</source>
          <target state="translated">이전 API :</target>
        </trans-unit>
        <trans-unit id="fe1a8467f796e691ae75cd019ca47d231225ebf5" translate="yes" xml:space="preserve">
          <source>On CUDA 10.1, set environment variable &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt;. This may affect performance.</source>
          <target state="translated">CUDA 10.1에서 환경 변수 &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt; 을 설정합니다 . 이는 성능에 영향을 미칠 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="87f5d31208b13882b13d7cef323f2a640539b60a" translate="yes" xml:space="preserve">
          <source>On CUDA 10.2 or later, set environment variable (note the leading colon symbol) &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; or &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt;.</source>
          <target state="translated">CUDA 10.2 이상에서는 환경 변수 (선행 콜론 기호 참고) &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; 또는 &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt; 를 설정 합니다.</target>
        </trans-unit>
        <trans-unit id="b433d4e11b599fd15dbc04960281f9eb287632d1" translate="yes" xml:space="preserve">
          <source>On each window, the function computed is:</source>
          <target state="translated">각 창에서 계산되는 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a84bfeb86e4aa154caeb1f582b53ed62fd133141" translate="yes" xml:space="preserve">
          <source>On modules, methods must be compiled before they can be called. The TorchScript compiler recursively compiles methods it sees when compiling other methods. By default, compilation starts on the &lt;code&gt;forward&lt;/code&gt; method. Any methods called by &lt;code&gt;forward&lt;/code&gt; will be compiled, and any methods called by those methods, and so on. To start compilation at a method other than &lt;code&gt;forward&lt;/code&gt;, use the &lt;a href=&quot;jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator (&lt;code&gt;forward&lt;/code&gt; implicitly is marked &lt;code&gt;@torch.jit.export&lt;/code&gt;).</source>
          <target state="translated">모듈에서 메서드는 호출되기 전에 컴파일되어야합니다. TorchScript 컴파일러는 다른 메서드를 컴파일 할 때 표시되는 메서드를 재귀 적으로 컴파일합니다. 기본적으로 컴파일은 &lt;code&gt;forward&lt;/code&gt; 메서드 에서 시작됩니다 . &lt;code&gt;forward&lt;/code&gt; 에 의해 호출 된 모든 메서드와 해당 메서드에 의해 호출되는 메서드 등이 컴파일됩니다. &lt;code&gt;forward&lt;/code&gt; 이외의 메서드에서 컴파일을 시작하려면 &lt;a href=&quot;jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt; 데코레이터를 사용합니다 ( &lt;code&gt;forward&lt;/code&gt; 은 암시 적으로 &lt;code&gt;@torch.jit.export&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="a488ba8f17e355abbbc16eb50cb56f628e0ef099" translate="yes" xml:space="preserve">
          <source>On the other hand, invoking &lt;code&gt;trace&lt;/code&gt; with module&amp;rsquo;s instance (e.g. &lt;code&gt;my_module&lt;/code&gt;) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.</source>
          <target state="translated">반면에 모듈의 인스턴스 (예 : &lt;code&gt;my_module&lt;/code&gt; )로 &lt;code&gt;trace&lt;/code&gt; 를 호출 하면 새 모듈이 생성되고 매개 변수가 새 모듈에 올바르게 복사되므로 필요한 경우 그래디언트를 누적 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b392f645cfdecae166313100197dfb1e06d02b4b" translate="yes" xml:space="preserve">
          <source>Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).</source>
          <target state="translated">모든 DDP 프로세스가 결합되면 컨텍스트 관리자는 마지막으로 결합 된 프로세스에 해당하는 모델을 모든 프로세스에 브로드 캐스트하여 모델이 모든 프로세스에서 동일한 지 확인합니다 (DDP에 의해 보장됨).</target>
        </trans-unit>
        <trans-unit id="3ebcd2d39bb7401d6e980dad866025a71193ae15" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for Caffe2:</source>
          <target state="translated">설치가 완료되면 Caffe2 용 백엔드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c7da4cd77b5400566e4f52a1d8995e48e44e2095" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for ONNX Runtime:</source>
          <target state="translated">설치가 완료되면 ONNX 런타임 용 백엔드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="181dde386ac4e48b01ea4c9ac2766b9fdca3442c" translate="yes" xml:space="preserve">
          <source>Once you&amp;rsquo;ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.</source>
          <target state="translated">TensorBoard를 설치하면 이러한 유틸리티를 사용하여 TensorBoard UI 내에서 시각화 할 수 있도록 PyTorch 모델 및 메트릭을 디렉터리에 로깅 할 수 있습니다. 스칼라, 이미지, 히스토그램, 그래프 및 임베딩 시각화는 모두 PyTorch 모델과 텐서, Caffe2 네트 및 블롭에 대해 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="fa0ae9a8c41213fbda341d9975cb44fc985cb402" translate="yes" xml:space="preserve">
          <source>One can either give a &lt;code&gt;scale_factor&lt;/code&gt; or the target output &lt;code&gt;size&lt;/code&gt; to calculate the output size. (You cannot give both, as it is ambiguous)</source>
          <target state="translated">하나 중 하나 줄 수 &lt;code&gt;scale_factor&lt;/code&gt; 또는 목표 출력 &lt;code&gt;size&lt;/code&gt; 출력 크기를 계산한다. (모호하기 때문에 둘 다 줄 수는 없습니다)</target>
        </trans-unit>
        <trans-unit id="f84825776234c0c4615edbbd6631eafc8684c310" translate="yes" xml:space="preserve">
          <source>One cannot specify both positional args &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; and keyword args &lt;code&gt;rename_map&lt;/code&gt;.</source>
          <target state="translated">위치 인수 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 과 키워드 args &lt;code&gt;rename_map&lt;/code&gt; 을 모두 지정할 수는 없습니다 .</target>
        </trans-unit>
        <trans-unit id="f99ff2b17e8a666214d13b388ce8036710cb6b91" translate="yes" xml:space="preserve">
          <source>One way to automatically catch many errors in traces is by using &lt;code&gt;check_inputs&lt;/code&gt; on the &lt;code&gt;torch.jit.trace()&lt;/code&gt; API. &lt;code&gt;check_inputs&lt;/code&gt; takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:</source>
          <target state="translated">자동 트레이스 많은 오류를 포착하는 한 가지 방법은 사용하는 것입니다 &lt;code&gt;check_inputs&lt;/code&gt; 을 온 &lt;code&gt;torch.jit.trace()&lt;/code&gt; API. &lt;code&gt;check_inputs&lt;/code&gt; 는 계산을 다시 추적하고 결과를 확인하는 데 사용할 입력 튜플 목록을 가져옵니다. 예를 들면 :</target>
        </trans-unit>
        <trans-unit id="e9f428e38cd970cf00e186de12fa06466a0c70e3" translate="yes" xml:space="preserve">
          <source>Only 2D input is supported for quantized inputs</source>
          <target state="translated">양자화 된 입력에는 2D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="e86d4094fa43c487d3762de38c606afefec7936e" translate="yes" xml:space="preserve">
          <source>Only 2D inputs are supported</source>
          <target state="translated">2D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="daede8c2e196c09bd4c265e562b51d50f0840292" translate="yes" xml:space="preserve">
          <source>Only 2D/3D input is supported for quantized inputs</source>
          <target state="translated">양자화 된 입력에는 2D / 3D 입력 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="27966aeec004be97d5e6f5f602ef5deb9549007c" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;torch.quint8&lt;/code&gt; is supported for the input data type.</source>
          <target state="translated">입력 데이터 유형에는 &lt;code&gt;torch.quint8&lt;/code&gt; 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="04aa0322b01e7ae421d1285e24a5ecee9dd8da63" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;zeros&lt;/code&gt; is supported for the &lt;code&gt;padding_mode&lt;/code&gt; argument.</source>
          <target state="translated">&lt;code&gt;padding_mode&lt;/code&gt; 인수 에는 &lt;code&gt;zeros&lt;/code&gt; 만 지원됩니다 .</target>
        </trans-unit>
        <trans-unit id="4e5b1e16b907585048a530098a587f0e967c17c8" translate="yes" xml:space="preserve">
          <source>Only leaf Tensors will have their &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated during a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt;. To get &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated for non-leaf Tensors, you can use &lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt;&lt;code&gt;retain_grad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">만 잎 텐서가있을 것이다 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt; 호출하는 동안 채워 &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt; . 리프가 아닌 Tensor에 대해 &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; 를&lt;/a&gt; 채우 려면 &lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt; &lt;code&gt;retain_grad()&lt;/code&gt; &lt;/a&gt; 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="214baf780986ab4b5685dfad680eb394c92a2724" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend are currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 및 gloo 백엔드 만 지원되는 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="db8dc40130a88665cd5bf8f3be6d7ea249aff179" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 및 gloo 백엔드 만 지원됩니다. 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="df8cfb4a9929de36ffa3e1c6452d25a578580d0f" translate="yes" xml:space="preserve">
          <source>Only nccl backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">현재 nccl 백엔드 만 지원됩니다. 텐서는 GPU 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="6f7a0a50221e4f4092d5689875e749bacffbc3f1" translate="yes" xml:space="preserve">
          <source>Only the GPU of &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; on the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">순위가 &lt;code&gt;dst&lt;/code&gt; 인 프로세스에서 &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; 의 GPU 만 최종 결과를받습니다.</target>
        </trans-unit>
        <trans-unit id="c024a1c0a5c1c600879124a8ffc9c018f3eeaa1c" translate="yes" xml:space="preserve">
          <source>Only the following modes are supported for the quantized inputs:</source>
          <target state="translated">양자화 된 입력에는 다음 모드 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="4abce74e0ec5249c62cdd843742dc1fcd44cf957" translate="yes" xml:space="preserve">
          <source>Only the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">순위가 &lt;code&gt;dst&lt;/code&gt; 인 프로세스 만 최종 결과를받습니다.</target>
        </trans-unit>
        <trans-unit id="465e942c4c87f62f9475a2c554cc5cf2204bbefd" translate="yes" xml:space="preserve">
          <source>Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that dynamic lookups are not available.</source>
          <target state="translated">튜플, 목록 및 변수 만 JIT 입력 / 출력으로 지원됩니다. 사전과 문자열도 허용되지만 사용하지 않는 것이 좋습니다. 사용자는 dict 입력을주의 깊게 확인해야하며 동적 조회를 사용할 수 없다는 점을 염두에 두어야합니다.</target>
        </trans-unit>
        <trans-unit id="d0f4a3ec3e4ad0aecc8e89f342dde91d8ebe6688" translate="yes" xml:space="preserve">
          <source>Operator Export Type</source>
          <target state="translated">운영자 내보내기 유형</target>
        </trans-unit>
        <trans-unit id="bb7373849dc3f047bbc13778dd3b1cb1da43b12b" translate="yes" xml:space="preserve">
          <source>OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph:</source>
          <target state="translated">OperatorExportTypes.ONNX : 모든 작업이 일반 ONNX 작업 (ONNX 네임 스페이스 포함)으로 내보내집니다. OperatorExportTypes.ONNX_ATEN : 모든 작업이 ATen 작업 (aten 네임 스페이스 포함)으로 내보내집니다. OperatorExportTypes.ONNX_ATEN_FALLBACK : ATen 작업이 ONNX에서 지원되지 않거나 해당 기호가 누락 된 경우 ATen 작업으로 폴백합니다. 등록 된 작업은 정기적으로 ONNX로 내보내집니다. 그래프 예 :</target>
        </trans-unit>
        <trans-unit id="e90414358dbfff0a68e4eb5d68a16978cf197d5a" translate="yes" xml:space="preserve">
          <source>Operators</source>
          <target state="translated">Operators</target>
        </trans-unit>
        <trans-unit id="326510735a447ec63da9abec360c7b0441bdd180" translate="yes" xml:space="preserve">
          <source>Optional Type Refinement</source>
          <target state="translated">선택적 유형 개선</target>
        </trans-unit>
        <trans-unit id="21826226f40329aa4cc3679f6cd2d7ad202e0147" translate="yes" xml:space="preserve">
          <source>Optional values &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; are scaling factors on the outer product between &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and the added matrix &lt;code&gt;input&lt;/code&gt; respectively.</source>
          <target state="translated">선택적 값 &lt;code&gt;beta&lt;/code&gt; 와 &lt;code&gt;alpha&lt;/code&gt; 는 각각 &lt;code&gt;vec1&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 사이의 외적 과 추가 된 행렬 &lt;code&gt;input&lt;/code&gt; 에 대한 배율 인수 입니다.</target>
        </trans-unit>
        <trans-unit id="14cf9a30a86cd6abc4768ba63268809b6a642a72" translate="yes" xml:space="preserve">
          <source>Optionally set the Torch Hub directory used to save downloaded models &amp;amp; weights.</source>
          <target state="translated">다운로드 한 모델 및 무게를 저장하는 데 사용되는 Torch Hub 디렉토리를 선택적으로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="3cdb2fb789b07586dcdc33ff9c3ec35d1394e04a" translate="yes" xml:space="preserve">
          <source>Optionally, you can give non-equal weighting on the classes by passing a 1D &lt;code&gt;weight&lt;/code&gt; tensor into the constructor.</source>
          <target state="translated">선택적 으로 생성자에 1D &lt;code&gt;weight&lt;/code&gt; 텐서를 전달하여 클래스에 동일하지 않은 가중치를 부여 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ae88acefda9740ca025e9b1e8d82500694df6312" translate="yes" xml:space="preserve">
          <source>Orphan</source>
          <target state="translated">Orphan</target>
        </trans-unit>
        <trans-unit id="6e6a6f2086bb5fe5dbfd17d8d5f502d48759834b" translate="yes" xml:space="preserve">
          <source>Other</source>
          <target state="translated">Other</target>
        </trans-unit>
        <trans-unit id="d51aeaedae53d1689ff4812f653d88f9f69f9530" translate="yes" xml:space="preserve">
          <source>Other NCCL environment variables</source>
          <target state="translated">기타 NCCL 환경 변수</target>
        </trans-unit>
        <trans-unit id="b41b4ea22c0549444f4374445d8b5be41ed6c7a3" translate="yes" xml:space="preserve">
          <source>Other Operations</source>
          <target state="translated">기타 작업</target>
        </trans-unit>
        <trans-unit id="b4fb6252944f2406950d09755e09e265942a94cb" translate="yes" xml:space="preserve">
          <source>Other dimensions of &lt;code&gt;input&lt;/code&gt; that are not explicitly moved remain in their original order and appear at the positions not specified in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="translated">명시 적으로 이동되지 않은 &lt;code&gt;input&lt;/code&gt; 다른 차원은 원래 순서로 유지되고 &lt;code&gt;destination&lt;/code&gt; 지정되지 않은 위치에 나타납니다 .</target>
        </trans-unit>
        <trans-unit id="9b1d762f3a800a1e6b0f118d4aa509412d043de1" translate="yes" xml:space="preserve">
          <source>Otherwise, if &lt;code&gt;map_location&lt;/code&gt; is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).</source>
          <target state="translated">그렇지 않으면 &lt;code&gt;map_location&lt;/code&gt; 이 dict이면 파일 (키)에 나타나는 위치 태그를 저장소 (값)를 넣을 위치를 지정하는 태그로 다시 매핑하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9f6ea88f9397a13956243e283a62e08c4833e59a" translate="yes" xml:space="preserve">
          <source>Otherwise, it will not be possible to view &lt;code&gt;self&lt;/code&gt; tensor as &lt;code&gt;shape&lt;/code&gt; without copying it (e.g., via &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;). When it is unclear whether a &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; can be performed, it is advisable to use &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which returns a view if the shapes are compatible, and copies (equivalent to calling &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;) otherwise.</source>
          <target state="translated">그렇지 않으면 &lt;code&gt;self&lt;/code&gt; 텐서를 복사하지 않고 &lt;code&gt;shape&lt;/code&gt; 으로 볼 수 없습니다 (예 : &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt; 를 통해 ). &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 를 수행 할 수 있는지 여부가 확실하지 않은 경우 모양이 호환되는 경우 뷰를 반환하고 그렇지 않으면 복사 ( &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt; 를 호출하는 것과 동일 &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 하는 reshape () 를 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="d386f5cc6857d8a2d7baf372e44b4d1117dfed1a" translate="yes" xml:space="preserve">
          <source>Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.</source>
          <target state="translated">우리의 해결책은 BCELoss가 로그 함수 출력을 -100보다 크거나 같도록 클램프하는 것입니다. 이런 식으로 우리는 항상 유한 손실 값과 선형 역방향 방법을 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02600672b80ab0a1c4955d9a3e0f68bbce6d3eed" translate="yes" xml:space="preserve">
          <source>Our sparse tensor format permits &lt;em&gt;uncoalesced&lt;/em&gt; sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. Uncoalesced tensors permit us to implement certain operators more efficiently.</source>
          <target state="translated">우리의 희소 텐서 형식 은 인덱스에 중복 좌표가있을 수있는 &lt;em&gt;통합되지 않은&lt;/em&gt; 희소 텐서를 허용 합니다. 이 경우 해당 인덱스의 값은 모든 중복 값 항목의 합계라는 의미입니다. 통합되지 않은 텐서는 특정 연산자를보다 효율적으로 구현할 수 있도록합니다.</target>
        </trans-unit>
        <trans-unit id="48c4c6b206793564023af6432f79ea36266139f9" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="99a0b833417509afcf74b89d81969fd3e6454e0f" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="f02f9ec70b467240dbc6d49d1fba7b03c1442801" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="5a8086fbdf63aa63eb1e982f564377b659f098f0" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt;&lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt; &lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="73076ad1c2f9be4b4d38d73b62e6c3e6c045bec2" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt;&lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt; &lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="e013688ad79fc825d3497306dc29bb552d0e34c4" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="725700a259ff1b2b017f60b4e74698aa49714d29" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt; &lt;/a&gt; 의 잘못된 버전</target>
        </trans-unit>
        <trans-unit id="6398fea054e2780d871397e980d3cf8c7e956e4c" translate="yes" xml:space="preserve">
          <source>Out-place version of &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;index_put_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;index_put_()&lt;/code&gt; &lt;/a&gt; 의 외부 버전 . &lt;code&gt;tensor1&lt;/code&gt; 은 &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt; 의&lt;/a&gt; &lt;code&gt;self&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="33c36eb13432b5622fd7de9ba502b6b4e6d48ded" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 의 외적 .</target>
        </trans-unit>
        <trans-unit id="615e0188218c976163bf2ec4e7c83bb336305665" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;. If &lt;code&gt;input&lt;/code&gt; is a vector of size</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;vec2&lt;/code&gt; 의 외적 . &lt;code&gt;input&lt;/code&gt; 이 크기 벡터 인 경우</target>
        </trans-unit>
        <trans-unit id="67a0062a0687696c335a2c7d0e459405eba90f74" translate="yes" xml:space="preserve">
          <source>Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; contains the all_gather result that resides on the GPU of &lt;code&gt;input_tensor_list[i]&lt;/code&gt;.</source>
          <target state="translated">출력 목록. 집합의 출력에 사용할 각 GPU에 올바른 크기의 텐서를 포함해야합니다. 예를 들어 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 에는 &lt;code&gt;input_tensor_list[i]&lt;/code&gt; 의 GPU에 상주하는 all_gather 결과가 포함 됩니다.</target>
        </trans-unit>
        <trans-unit id="1b572830da9a955103a98dbcea6dc96cd3c1bf56" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;function&lt;/code&gt; on &lt;code&gt;*args&lt;/code&gt;</source>
          <target state="translated">실행의 출력 &lt;code&gt;function&lt;/code&gt; 에 &lt;code&gt;*args&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90de97722219af193315e02c2b185d1c1a5a4186" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;functions&lt;/code&gt; sequentially on &lt;code&gt;*inputs&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;*inputs&lt;/code&gt; 순차적으로 실행되는 &lt;code&gt;functions&lt;/code&gt; 출력</target>
        </trans-unit>
        <trans-unit id="613a57f34002b7db7e7eb40fb879941fd5f0eb57" translate="yes" xml:space="preserve">
          <source>Output shape: &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">출력 모양 : &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f6a2e936babf4473798688e71160bef838804c5c" translate="yes" xml:space="preserve">
          <source>Output tensors (on different GPUs) to receive the result of the operation.</source>
          <target state="translated">연산 결과를 받기 위해 텐서를 출력합니다 (다른 GPU에서).</target>
        </trans-unit>
        <trans-unit id="be4dd5eb977c617cc4374f35ce4dcfe3424e6c52" translate="yes" xml:space="preserve">
          <source>Output1:</source>
          <target state="translated">Output1:</target>
        </trans-unit>
        <trans-unit id="099f2ae15e5d444369fa820ff7e04ecd1eccb38d" translate="yes" xml:space="preserve">
          <source>Output2:</source>
          <target state="translated">Output2:</target>
        </trans-unit>
        <trans-unit id="f3c8c95c5e534bcd2ea0034a0d83177efa6923f4" translate="yes" xml:space="preserve">
          <source>Output:</source>
          <target state="translated">Output:</target>
        </trans-unit>
        <trans-unit id="8063b576f684099c293104f0277e7a1383d61e53" translate="yes" xml:space="preserve">
          <source>Output: &lt;code&gt;(*, embedding_dim)&lt;/code&gt;, where &lt;code&gt;*&lt;/code&gt; is the input shape</source>
          <target state="translated">출력 : &lt;code&gt;(*, embedding_dim)&lt;/code&gt; , 여기서 &lt;code&gt;*&lt;/code&gt; 는 입력 모양입니다.</target>
        </trans-unit>
        <trans-unit id="7b14dfe86f44f01989354331a43a653a7f8d7f72" translate="yes" xml:space="preserve">
          <source>Output: A Tensor of shape</source>
          <target state="translated">출력 : 형태의 텐서</target>
        </trans-unit>
        <trans-unit id="4a4fbbd55c6c3ccbf6bf7220f45ac1790ff9bb33" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If :attr:&lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 기본적으로 스칼라. : attr : &lt;code&gt;reduction&lt;/code&gt; 이 &lt;code&gt;'none'&lt;/code&gt; 인 경우</target>
        </trans-unit>
        <trans-unit id="7166320ecb31bd6a3586040c60443c5f16f96651" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 기본적으로 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 다음,</target>
        </trans-unit>
        <trans-unit id="88a34ee124c80fdb97be4c7994066abce0b11c8e" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 다음,</target>
        </trans-unit>
        <trans-unit id="816914c299df40485ecf46a1e795a1eb404d91d8" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then same shape as the input</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; 입력으로하고 동일한 형상을</target>
        </trans-unit>
        <trans-unit id="573c4c1298793f227765c23e85deebaee6066af6" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then the same size as the target:</source>
          <target state="translated">출력 : 스칼라. 경우 &lt;code&gt;reduction&lt;/code&gt; 없다 &lt;code&gt;'none'&lt;/code&gt; , 대상으로 다음 같은 크기는 :</target>
        </trans-unit>
        <trans-unit id="6ee76fa98a639a0ac269010b7032639b0a171647" translate="yes" xml:space="preserve">
          <source>Outputs:</source>
          <target state="translated">Outputs:</target>
        </trans-unit>
        <trans-unit id="19620445bec40ebeb61b840841c0effef04f34be" translate="yes" xml:space="preserve">
          <source>Outputs: (h_1, c_1)</source>
          <target state="translated">출력 : (h_1, c_1)</target>
        </trans-unit>
        <trans-unit id="9b4d142414484eb98f10e629a2f06df85748b811" translate="yes" xml:space="preserve">
          <source>Outputs: h&amp;rsquo;</source>
          <target state="translated">출력 : h '</target>
        </trans-unit>
        <trans-unit id="511c97e64880de564b0676893af9fcc8a480a2e2" translate="yes" xml:space="preserve">
          <source>Outputs: output, (h_n, c_n)</source>
          <target state="translated">출력 : 출력, (h_n, c_n)</target>
        </trans-unit>
        <trans-unit id="a459eb30ab99f3ae5b662cb99c1f4215b146179c" translate="yes" xml:space="preserve">
          <source>Outputs: output, h_n</source>
          <target state="translated">출력 : output, h_n</target>
        </trans-unit>
        <trans-unit id="3454926b31857082d753c8156d0ffd5035b9d6b1" translate="yes" xml:space="preserve">
          <source>Overloaded function.</source>
          <target state="translated">과부하 된 기능.</target>
        </trans-unit>
        <trans-unit id="23672df91d237894c7bd8c7afdec0a1cd14fb6f6" translate="yes" xml:space="preserve">
          <source>Owner Share RRef with User</source>
          <target state="translated">사용자와 소유자 공유 RRef</target>
        </trans-unit>
        <trans-unit id="511993d3c99719e38a6779073019dacd7178ddb9" translate="yes" xml:space="preserve">
          <source>P</source>
          <target state="translated">P</target>
        </trans-unit>
        <trans-unit id="4a6bac0b7fcbb3ba201bd5676355ccb5034a278a" translate="yes" xml:space="preserve">
          <source>P(x) = \dfrac{1}{\text{to} - \text{from}}</source>
          <target state="translated">P (x) = \ dfrac {1} {\ text {to}-\ text {from}}</target>
        </trans-unit>
        <trans-unit id="964bd5656c7a016ef0c9ada7382da7ca1fee27a4" translate="yes" xml:space="preserve">
          <source>PRODUCT</source>
          <target state="translated">PRODUCT</target>
        </trans-unit>
        <trans-unit id="768bfcbbf6df966e333cf0230ff5aeca88678ff9" translate="yes" xml:space="preserve">
          <source>PReLU</source>
          <target state="translated">PReLU</target>
        </trans-unit>
        <trans-unit id="c6672b1c1e9f2629ba013dde01df96129b495f92" translate="yes" xml:space="preserve">
          <source>PackedSequence</source>
          <target state="translated">PackedSequence</target>
        </trans-unit>
        <trans-unit id="c0577f7163ed4ff21523cd27bc5236f92de452c9" translate="yes" xml:space="preserve">
          <source>Packs a Tensor containing padded sequences of variable length.</source>
          <target state="translated">패딩 처리 된 가변 길이 시퀀스를 포함하는 Tensor를 압축합니다.</target>
        </trans-unit>
        <trans-unit id="65d415f11a63c4c34ebb5de81f2e889c9328e53e" translate="yes" xml:space="preserve">
          <source>Packs a list of variable length Tensors</source>
          <target state="translated">가변 길이 Tensor 목록을 압축합니다.</target>
        </trans-unit>
        <trans-unit id="019020b0e5b242a0cc1a8528059f3ad86084afa9" translate="yes" xml:space="preserve">
          <source>Pad a list of variable length Tensors with &lt;code&gt;padding_value&lt;/code&gt;</source>
          <target state="translated">가변 길이 텐서 목록을 &lt;code&gt;padding_value&lt;/code&gt; 로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="2560c7ddae8fa20bff911361bd8035aef865bce1" translate="yes" xml:space="preserve">
          <source>Padding Layers</source>
          <target state="translated">패딩 레이어</target>
        </trans-unit>
        <trans-unit id="119765b1620180622c6f6bb53002149703be9254" translate="yes" xml:space="preserve">
          <source>Padding mode:</source>
          <target state="translated">패딩 모드 :</target>
        </trans-unit>
        <trans-unit id="c28c7b4763bce482c5c0e969cee27313a7b7a4b3" translate="yes" xml:space="preserve">
          <source>Padding size:</source>
          <target state="translated">패딩 크기 :</target>
        </trans-unit>
        <trans-unit id="01c1aebd02a3849d24632e98070e328ff731c754" translate="yes" xml:space="preserve">
          <source>Pads a packed batch of variable length sequences.</source>
          <target state="translated">가변 길이 시퀀스의 패킹 된 배치를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="5f2ca3b7c48057f8a6249de795ad4677ba74df59" translate="yes" xml:space="preserve">
          <source>Pads tensor.</source>
          <target state="translated">패드 텐서.</target>
        </trans-unit>
        <trans-unit id="340e6f3d3040c075fcd53240ac5a1540bc713e60" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with a constant value.</source>
          <target state="translated">입력 텐서 경계를 상수 값으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="66887cd786c809b7e475e385b383407a6d7e005a" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with zero.</source>
          <target state="translated">입력 텐서 경계를 0으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="15f49157ae62175a9c8bfe59b0dd171af5f13c80" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using replication of the input boundary.</source>
          <target state="translated">입력 경계의 복제를 사용하여 입력 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="2095464e9318d466281e389dabe7d857b2607c1d" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using the reflection of the input boundary.</source>
          <target state="translated">입력 경계의 반사를 사용하여 입력 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="9b861161343780fb206bb4b2ff66ca3fba1cab61" translate="yes" xml:space="preserve">
          <source>PairwiseDistance</source>
          <target state="translated">PairwiseDistance</target>
        </trans-unit>
        <trans-unit id="1ad9f67d0f855f646efb3775c7a4778a3cc5a138" translate="yes" xml:space="preserve">
          <source>Parallelism</source>
          <target state="translated">Parallelism</target>
        </trans-unit>
        <trans-unit id="f699f295e5ae4ac633cfa18437fed38d028b3fdb" translate="yes" xml:space="preserve">
          <source>Parameter</source>
          <target state="translated">Parameter</target>
        </trans-unit>
        <trans-unit id="cb5ac90fcc6e04cb5c8cae65265e51f0ab35a0b7" translate="yes" xml:space="preserve">
          <source>Parameter names except the first must EXACTLY match the names in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">첫 번째를 제외한 매개 변수 이름은 &lt;code&gt;forward&lt;/code&gt; 의 이름과 정확히 일치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="4891fc0302ab937e6c97e91f093534d4afb88b1e" translate="yes" xml:space="preserve">
          <source>Parameter ordering does NOT necessarily match what is in &lt;code&gt;VariableType.h&lt;/code&gt;, tensors (inputs) are always first, then non-tensor arguments.</source>
          <target state="translated">매개 변수 순서가 &lt;code&gt;VariableType.h&lt;/code&gt; 에 있는 것과 반드시 ​​일치하는 것은 아닙니다 . 텐서 (입력)는 항상 먼저, 그다음에는 비 텐서 인수입니다.</target>
        </trans-unit>
        <trans-unit id="88be3e0f29cc772e39dc2a268a55384b430d8f22" translate="yes" xml:space="preserve">
          <source>ParameterDict</source>
          <target state="translated">ParameterDict</target>
        </trans-unit>
        <trans-unit id="63777906465127a1bb8f4e009ca405717880743f" translate="yes" xml:space="preserve">
          <source>ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods.</source>
          <target state="translated">ParameterDict는 일반 Python 사전처럼 인덱싱 할 수 있지만 여기에 포함 된 매개 변수는 제대로 등록되어 있으며 모든 Module 메서드에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="acdf35da4c553e1dfc2210109486c37c589cbf86" translate="yes" xml:space="preserve">
          <source>ParameterList</source>
          <target state="translated">ParameterList</target>
        </trans-unit>
        <trans-unit id="a975eea30db9fa05003e3b5097688bd49ec7e01b" translate="yes" xml:space="preserve">
          <source>Parameters</source>
          <target state="translated">Parameters</target>
        </trans-unit>
        <trans-unit id="b2ef1af91a76a3fb94590270b4864284563ecce3" translate="yes" xml:space="preserve">
          <source>Parameters are &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; subclasses, that have a very special property when used with &lt;code&gt;Module&lt;/code&gt; s - when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in &lt;code&gt;parameters()&lt;/code&gt; iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as &lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt;&lt;code&gt;Parameter&lt;/code&gt;&lt;/a&gt;, these temporaries would get registered too.</source>
          <target state="translated">매개 변수는 &lt;code&gt;Module&lt;/code&gt; 과 함께 사용할 때 매우 특별한 속성을 갖는 &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 하위 클래스입니다. 모듈 속성으로 할당되면 자동으로 매개 변수 목록에 추가되고 &lt;code&gt;parameters()&lt;/code&gt; 반복자 에 나타납니다 . Tensor를 할당하는 것은 그러한 효과가 없습니다. 이는 모델에서 RNN의 마지막 숨겨진 상태와 같은 일부 임시 상태를 캐시하려고 할 수 있기 때문입니다. &lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt; &lt;code&gt;Parameter&lt;/code&gt; &lt;/a&gt; 와 같은 클래스가 없으면 이러한 임시도 등록됩니다.</target>
        </trans-unit>
        <trans-unit id="3df6923b905b81224f8a63a634516931831a9327" translate="yes" xml:space="preserve">
          <source>Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.</source>
          <target state="translated">매개 변수는 프로세스간에 브로드 캐스트되지 않습니다. 모듈은 기울기에 대해 전체 축소 단계를 수행하고 모든 프로세스에서 동일한 방식으로 최적화 프로그램에 의해 수정 될 것이라고 가정합니다. 버퍼 (예 : BatchNorm 통계)는 랭크 0 프로세스의 모듈에서 매 반복마다 시스템의 다른 모든 복제본으로 브로드 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="a70edfd33f03d14b9304a9ca1fe474eaf0817702" translate="yes" xml:space="preserve">
          <source>Parsing the local_rank argument</source>
          <target state="translated">local_rank 인수 구문 분석</target>
        </trans-unit>
        <trans-unit id="48a79b6f6f692b43d8485194d95ec3b0c6abe1d9" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layer.</source>
          <target state="translated">인코더 레이어를 통해 입력을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="74a6cfbb8f181d4db771aa9745afb6080e4ee4fe" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layers in turn.</source>
          <target state="translated">입력을 인코더 레이어를 통해 차례로 전달합니다.</target>
        </trans-unit>
        <trans-unit id="4d0cc1f642a2a736b4718ae462ea2ac809ed76b7" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer in turn.</source>
          <target state="translated">입력 (및 마스크)을 디코더 계층을 통해 차례로 전달합니다.</target>
        </trans-unit>
        <trans-unit id="8c65d8ab6bdf0e2280cb1ffcf102aefaac492303" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer.</source>
          <target state="translated">디코더 레이어를 통해 입력 (및 마스크)을 전달합니다.</target>
        </trans-unit>
        <trans-unit id="fd97c67f36d1e3c87f8ab45030bd4f215e7035cc" translate="yes" xml:space="preserve">
          <source>Passing -1 as the size for a dimension means not changing the size of that dimension.</source>
          <target state="translated">차원의 크기로 -1을 전달하면 해당 차원의 크기가 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ea3c8179bd96f54267bf75b132502b2bb1730b13" translate="yes" xml:space="preserve">
          <source>Pattern Matching Assignments</source>
          <target state="translated">패턴 매칭 할당</target>
        </trans-unit>
        <trans-unit id="bbb4559fe386a24723c5dad9bdcdef77441cfdce" translate="yes" xml:space="preserve">
          <source>Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If &lt;code&gt;graceful=True&lt;/code&gt;, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if &lt;code&gt;graceful=False&lt;/code&gt;, this is a local shutdown, and it does not wait for other RPC processes to reach this method.</source>
          <target state="translated">RPC 에이전트를 종료 한 다음 RPC 에이전트를 제거합니다. 이렇게하면 로컬 에이전트가 미해결 요청을 수락하지 않고 모든 RPC 스레드를 종료하여 RPC 프레임 워크를 종료합니다. 경우에 &lt;code&gt;graceful=True&lt;/code&gt; ,이 모든 로컬 및 원격 RPC 프로세스가이 방법을 도달 할 때까지 차단하고 완전한 모든 뛰어난 작품을 기다립니다. 그렇지 않으면 &lt;code&gt;graceful=False&lt;/code&gt; 이면 로컬 종료이며 다른 RPC 프로세스가이 메서드에 도달 할 때까지 기다리지 않습니다.</target>
        </trans-unit>
        <trans-unit id="000d790b0603ede49c70b8206287b63e59d6ce01" translate="yes" xml:space="preserve">
          <source>Performs</source>
          <target state="translated">Performs</target>
        </trans-unit>
        <trans-unit id="ab6a68d7b5ed065f1d2b1e10ab32e7ca4fa89132" translate="yes" xml:space="preserve">
          <source>Performs Tensor dtype and/or device conversion. A &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; are inferred from the arguments of &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">Tensor dtype 및 / 또는 장치 변환을 수행합니다. &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; 가&lt;/a&gt; 의 인수에서 추론 &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="148ba906f736c7be8f3690226a95e0fc3751c601" translate="yes" xml:space="preserve">
          <source>Performs a &amp;ldquo;true&amp;rdquo; division like Python 3. See &lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt;&lt;code&gt;torch.floor_divide()&lt;/code&gt;&lt;/a&gt; for floor division.</source>
          <target state="translated">Python 3과 같은 &quot;진정한&quot;분할을 수행합니다 . 바닥 분할 은 &lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt; &lt;code&gt;torch.floor_divide()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ca1d45c075edfbcf89b6d16ec3eabd49d65b9ace" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에서 행렬의 배치 행렬-행렬 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="1d3416bea00d8ee005b5f0e7621baf85cbbbc8e4" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">&lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에서 행렬의 배치 행렬-행렬 곱을 수행합니다 . &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="6629b05e96ef663c5b3cf02ed167a20014b205ec" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</source>
          <target state="translated">감소 된 추가 단계를 사용하여 &lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 (모든 행렬 곱셈이 첫 번째 차원을 따라 누적 됨).</target>
        </trans-unit>
        <trans-unit id="1710424b09b25bd3fe48dce1c221f445326275f1" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension). &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">감소 된 추가 단계를 사용하여 &lt;code&gt;batch1&lt;/code&gt; 및 &lt;code&gt;batch2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 (모든 행렬 곱셈이 첫 번째 차원을 따라 누적 됨). &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="6860592c3b78f43f0a520489d41db750bddd29e5" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mat2&lt;/code&gt; 에 저장된 행렬의 배치 행렬-행렬 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="df9a63e54344ddcdcb571d3c78f6a26c0d733eac" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="e06b6aa87fc19869b8dd410c826fba4f794df727" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;mat1&lt;/code&gt; 및 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="bc47b86c1cbb06be939cda2d15d03119065f2793" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;. The matrix &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">행렬 &lt;code&gt;mat1&lt;/code&gt; 및 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 . 행렬 &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="ec815028dc3febded0f07da8c4424505ed14884e" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the sparse matrix &lt;code&gt;mat1&lt;/code&gt; and dense matrix &lt;code&gt;mat2&lt;/code&gt;. Similar to &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt;&lt;code&gt;torch.mm()&lt;/code&gt;&lt;/a&gt;, If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="translated">희소 행렬 &lt;code&gt;mat1&lt;/code&gt; 과 조밀 행렬 &lt;code&gt;mat2&lt;/code&gt; 의 행렬 곱셈을 수행합니다 . 유사 &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt; &lt;code&gt;torch.mm()&lt;/code&gt; &lt;/a&gt; , 경우 &lt;code&gt;mat1&lt;/code&gt; A는</target>
        </trans-unit>
        <trans-unit id="0fcbec8807d0db14810ce48d50247513762338dc" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;input&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;input&lt;/code&gt; 과 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="fd688cf71edd9657ad9f3dba62a61d9bb7db410e" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">행렬 &lt;code&gt;mat&lt;/code&gt; 와 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="0d3f478da414f374637a9f8efee9102a8950a0aa" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;. The vector &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">행렬 &lt;code&gt;mat&lt;/code&gt; 와 벡터 &lt;code&gt;vec&lt;/code&gt; 의 행렬-벡터 곱을 수행합니다 . 벡터 &lt;code&gt;input&lt;/code&gt; 이 최종 결과에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="eb4301873be53ea1f5d5d9d23cf8670a4bf294c6" translate="yes" xml:space="preserve">
          <source>Performs a single optimization step.</source>
          <target state="translated">단일 최적화 단계를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="097227fe1294d426da153667fbb89e50d7a799f6" translate="yes" xml:space="preserve">
          <source>Performs dtype and/or device conversion on &lt;code&gt;self.data&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self.data&lt;/code&gt; 에 대해 dtype 및 / 또는 장치 변환을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="50008568c0d385fc9c65f3ac3f0c22f33a0fd7e8" translate="yes" xml:space="preserve">
          <source>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</source>
          <target state="translated">낮은 순위 행렬, 이러한 행렬의 배치 또는 희소 행렬에 대해 선형 PCA (주성분 분석)를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="20ee59ad32c7c62194b8cc1470dab04b5aeb1063" translate="yes" xml:space="preserve">
          <source>Performs the element-wise division of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">수행의 소자 현명한 분할 &lt;code&gt;tensor1&lt;/code&gt; 의해 &lt;code&gt;tensor2&lt;/code&gt; , 스칼라 곱에 의한 결과 &lt;code&gt;value&lt;/code&gt; 과 추가로 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="20dfc56a802418cc6cb883565e29a8149e17cc74" translate="yes" xml:space="preserve">
          <source>Performs the element-wise multiplication of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">수행의 요소 와이즈 곱 &lt;code&gt;tensor1&lt;/code&gt; 의해 &lt;code&gt;tensor2&lt;/code&gt; , 스칼라 곱에 의한 결과 &lt;code&gt;value&lt;/code&gt; 과 추가로 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8d3d151e5b759fb242c35ceb1f1a9e038806294" translate="yes" xml:space="preserve">
          <source>Performs the outer-product of vectors &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and adds it to the matrix &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">벡터 &lt;code&gt;vec1&lt;/code&gt; 및 &lt;code&gt;vec2&lt;/code&gt; 의 외적을 수행하고 이를 행렬 &lt;code&gt;input&lt;/code&gt; 추가합니다 .</target>
        </trans-unit>
        <trans-unit id="7775a934f392a3fd32435cb3afba7f94cd7e9125" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the dimension order in the &lt;code&gt;other&lt;/code&gt; tensor, adding size-one dims for any new names.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; 텐서 의 차원 순서와 일치 하도록 &lt;code&gt;self&lt;/code&gt; 텐서의 차원 을 변경하여 새 이름에 대해 크기 1을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="172d57479053b5b2d1caa6f27506a79abfa3d89b" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the order specified in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;, adding size-one dims for any new names.</source>
          <target state="translated">순서를 무작위로 바꾸어 넣은의 크기 &lt;code&gt;self&lt;/code&gt; 텐서에 지정된 순서에 맞게 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 새로운 이름을 크기 하나 희미을 추가.</target>
        </trans-unit>
        <trans-unit id="89073f3c192cb17be2601e291aa1534b084f3029" translate="yes" xml:space="preserve">
          <source>PixelShuffle</source>
          <target state="translated">PixelShuffle</target>
        </trans-unit>
        <trans-unit id="46d351806d607c75c00d9e7addd52a82379f8961" translate="yes" xml:space="preserve">
          <source>Please checkout &lt;a href=&quot;#tracing-vs-scripting&quot;&gt;Tracing vs Scripting&lt;/a&gt;.</source>
          <target state="translated">체크 아웃하십시오 &lt;a href=&quot;#tracing-vs-scripting&quot;&gt;스크립팅 대 추적을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="77243e0c917ac356b20b9bd68dd15bf839ebd531" translate="yes" xml:space="preserve">
          <source>Please ensure that &lt;code&gt;device_ids&lt;/code&gt; argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the &lt;code&gt;device_ids&lt;/code&gt; needs to be &lt;code&gt;[args.local_rank]&lt;/code&gt;, and &lt;code&gt;output_device&lt;/code&gt; needs to be &lt;code&gt;args.local_rank&lt;/code&gt; in order to use this utility</source>
          <target state="translated">&lt;code&gt;device_ids&lt;/code&gt; 인수가 코드가 작동 할 유일한 GPU 기기 ID로 설정되어 있는지 확인하세요 . 이것은 일반적으로 프로세스의 로컬 순위입니다. 즉, 이 유틸리티를 사용 하려면 &lt;code&gt;device_ids&lt;/code&gt; 는 &lt;code&gt;[args.local_rank]&lt;/code&gt; 이고 &lt;code&gt;output_device&lt;/code&gt; 는 &lt;code&gt;args.local_rank&lt;/code&gt; 여야 합니다.</target>
        </trans-unit>
        <trans-unit id="489042bcd064b62307292513642ef8adc6ce3919" translate="yes" xml:space="preserve">
          <source>Please refer to &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed Overview&lt;/a&gt; for a brief introduction to all features related to distributed training.</source>
          <target state="translated">분산 교육과 관련된 모든 기능에 대한 간략한 소개는 &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch 분산 개요&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="17832db8653d0843df90a5d67009e3924ef34997" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;expand&lt;/code&gt;.</source>
          <target state="translated">참조하십시오 &lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용은 &lt;code&gt;expand&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5ec18ad55b74e4a4762c499e962c30b802cc51eb" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;view&lt;/code&gt;.</source>
          <target state="translated">참조하십시오 &lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용 &lt;code&gt;view&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6ce3ed1431a0c41a218668fa1492f351b3c1863d" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;reshape&lt;/code&gt;.</source>
          <target state="translated">참조하시기 바랍니다 &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt; 에 대한 자세한 내용은 &lt;code&gt;reshape&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="037ad39d2570fc5f7861f69bb41699739bb0d316" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt; for more documentation on ReLU.</source>
          <target state="translated">ReLU에 대한 자세한 문서는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4353aa1de0244f4e4d17336cf0860de21b27bcfd" translate="yes" xml:space="preserve">
          <source>Point-to-point communication</source>
          <target state="translated">지점 간 통신</target>
        </trans-unit>
        <trans-unit id="f2954403f1437b0d34c50ea2ba4d35acd2ad8ef0" translate="yes" xml:space="preserve">
          <source>Pointwise Ops</source>
          <target state="translated">Pointwise Ops</target>
        </trans-unit>
        <trans-unit id="7f6c43edfe5b4aaaf229cadb3d90c7f7c42e39d6" translate="yes" xml:space="preserve">
          <source>Poisson</source>
          <target state="translated">Poisson</target>
        </trans-unit>
        <trans-unit id="fa7dfe92888800753b97007c7d56433a35b2c1ff" translate="yes" xml:space="preserve">
          <source>Poisson negative log likelihood loss.</source>
          <target state="translated">포아송 음의 로그 우도 손실입니다.</target>
        </trans-unit>
        <trans-unit id="080ef63a7d510bdbbbb0c01b547bfd9466aafda2" translate="yes" xml:space="preserve">
          <source>PoissonNLLLoss</source>
          <target state="translated">PoissonNLLLoss</target>
        </trans-unit>
        <trans-unit id="58ef016bfefe7d0fa87dbd8a9397011926b50b05" translate="yes" xml:space="preserve">
          <source>Pooling functions</source>
          <target state="translated">풀링 기능</target>
        </trans-unit>
        <trans-unit id="4f14f8bf99ac13b99d1b436dd62fcdf8537ddee0" translate="yes" xml:space="preserve">
          <source>Pooling layers</source>
          <target state="translated">풀링 레이어</target>
        </trans-unit>
        <trans-unit id="7280aa9b126e519bb5acdcab7417841a30bb930a" translate="yes" xml:space="preserve">
          <source>Possible values are:</source>
          <target state="translated">가능한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6175db74ea439762cf31ac47bf2800599471e037" translate="yes" xml:space="preserve">
          <source>Pretrained weights can either be stored locally in the github repo, or loadable by &lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt;&lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt;&lt;/a&gt;. If less than 2GB, it&amp;rsquo;s recommended to attach it to a &lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;project release&lt;/a&gt; and use the url from the release. In the example above &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; handles &lt;code&gt;pretrained&lt;/code&gt;, alternatively you can put the following logic in the entrypoint definition.</source>
          <target state="translated">사전 훈련 된 가중치는 github 저장소에 로컬로 저장하거나 &lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt; &lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt; &lt;/a&gt; 로드 할 수 있습니다 . 2GB 미만인 경우 &lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;프로젝트 릴리스&lt;/a&gt; 에 첨부하고 릴리스 의 URL을 사용하는 것이 좋습니다 . 위의 예에서 &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; 의 핸들이 &lt;code&gt;pretrained&lt;/code&gt; , 또는 당신은 엔트리 포인트 정의에 다음과 같은 논리를 넣을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fd6f86c9793df0ba1099fc411b3e7ec52a6a1cfe" translate="yes" xml:space="preserve">
          <source>Print Statements</source>
          <target state="translated">명세서 인쇄</target>
        </trans-unit>
        <trans-unit id="d36d84ef2381c76d6d53fffa7008829b5eaeb1fc" translate="yes" xml:space="preserve">
          <source>Process Group Backend</source>
          <target state="translated">프로세스 그룹 백엔드</target>
        </trans-unit>
        <trans-unit id="40e121339893a371c4a1b2c140642c9b6a34e9bd" translate="yes" xml:space="preserve">
          <source>Produces several warnings and a graph which simply returns the input:</source>
          <target state="translated">몇 가지 경고와 단순히 입력을 반환하는 그래프를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="a445fe395c6c6d32b8f6c47c9d18f8e8ae1263f5" translate="yes" xml:space="preserve">
          <source>Profiling RPC-based Workloads</source>
          <target state="translated">RPC 기반 워크로드 프로파일 링</target>
        </trans-unit>
        <trans-unit id="6f204147095385ad851c3a0b168d9ac27a7540b0" translate="yes" xml:space="preserve">
          <source>Promotion Examples:</source>
          <target state="translated">프로모션 예 :</target>
        </trans-unit>
        <trans-unit id="17a62537e52031c55b875c513ecbb2c241853032" translate="yes" xml:space="preserve">
          <source>Protocol Scenarios</source>
          <target state="translated">프로토콜 시나리오</target>
        </trans-unit>
        <trans-unit id="7a477b80f752e81ae51a71148e25e03d00004834" translate="yes" xml:space="preserve">
          <source>Provides a skeleton for customization requiring the overriding of methods such as &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt;&lt;code&gt;apply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt; &lt;code&gt;apply()&lt;/code&gt; &lt;/a&gt; 와 같은 메서드를 재정의해야하는 사용자 지정을위한 스켈레톤을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="df76ae939d5d588eb6f5766984f7e5d52eb50216" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor at random.</source>
          <target state="translated">무작위로 텐서에서 (현재 정리되지 않은) 단위를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="368202ce04421132a9166203cfc137eb74e9e21c" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</source>
          <target state="translated">L1- 노름이 가장 낮은 단위를 0으로 제거하여 텐서에서 (현재 정리되지 않은) 단위를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="5b5612215ed58f8a7a594acd4880fde95d901f0f" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor at random.</source>
          <target state="translated">무작위로 텐서에서 전체 (현재 제거되지 않은) 채널을 정리합니다.</target>
        </trans-unit>
        <trans-unit id="a802bde3db7f577abbb1b538eee51db0b51ddf30" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.</source>
          <target state="translated">Ln- 노름을 기반으로 텐서에서 전체 (현재 제거되지 않은) 채널을 정리합니다.</target>
        </trans-unit>
        <trans-unit id="2306b2b0d296e475c246073f14ea3e13b1639fc6" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 에서 사전 계산 된 마스크를 적용하여 &lt;code&gt;mask&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8222739be42aac5d12f65e68e036f42e19a41197" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 에서 사전 계산 된 마스크를 적용하여 &lt;code&gt;mask&lt;/code&gt; . 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="12f145dd28935b207164778834f46d05e62d324e" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random.</source>
          <target state="translated">무작위로 선택된 지정된 &lt;code&gt;dim&lt;/code&gt; 을 따라 지정된 &lt;code&gt;amount&lt;/code&gt; 의 (현재 제거되지 않은) 채널을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서 를 제거합니다 .</target>
        </trans-unit>
        <trans-unit id="1993775f18835672e6ea2ee6df5cc4b95f07d89b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">무작위로 선택된 지정된 &lt;code&gt;dim&lt;/code&gt; 을 따라 지정된 &lt;code&gt;amount&lt;/code&gt; 의 (현재 제거되지 않은) 채널을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서 를 제거합니다 . 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="e4cf8aaa8979161ded3d9362964d7e1da3ab5bd2" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 에 &lt;code&gt;module&lt;/code&gt; 지정된 제거하여 &lt;code&gt;amount&lt;/code&gt; 지정된 따라 (현재 unpruned) 채널 &lt;code&gt;dim&lt;/code&gt; 최저 L``n`` 규범으로한다.</target>
        </trans-unit>
        <trans-unit id="e5e671a44c6d84c3fbfdc3856808100dbb577dde" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">호출 파라미터에 대응 텐서 자두 &lt;code&gt;name&lt;/code&gt; 에 &lt;code&gt;module&lt;/code&gt; 지정된 제거하여 &lt;code&gt;amount&lt;/code&gt; 지정된 따라 (현재 unpruned) 채널 &lt;code&gt;dim&lt;/code&gt; 최저 L``n`` 규범으로한다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="e414308272708bb6595a21b037cefd9b491135bf" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random.</source>
          <target state="translated">임의로 선택된 (현재 정리되지 않은) 단위 의 지정된 &lt;code&gt;amount&lt;/code&gt; 을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="0d1f0e7025ef360a1a8db642ffde843b0c686100" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">임의로 선택된 (현재 정리되지 않은) 단위 의 지정된 &lt;code&gt;amount&lt;/code&gt; 을 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="0e2ae92cb48940de59dcf3dcc3eee3fc2761f74b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm.</source>
          <target state="translated">L1- 노름이 가장 낮은 지정된 &lt;code&gt;amount&lt;/code&gt; (현재 정리되지 않은) 단위를 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다.</target>
        </trans-unit>
        <trans-unit id="3ceb680b02bb9fc923a4db9b23afd99bcf23bd57" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">L1- 노름이 가장 낮은 지정된 &lt;code&gt;amount&lt;/code&gt; (현재 정리되지 않은) 단위를 제거하여 &lt;code&gt;module&lt;/code&gt; 에서 &lt;code&gt;name&lt;/code&gt; 이라는 매개 변수에 해당하는 텐서를 정리합니다. 다음과 같은 방법으로 모듈을 제자리에서 수정 (수정 된 모듈도 반환)합니다. 1) 프 루닝 메서드에 의해 매개 변수 &lt;code&gt;name&lt;/code&gt; 에 적용된 바이너리 마스크에 해당하는 &lt;code&gt;name+'_mask'&lt;/code&gt; 라는 명명 된 버퍼를 추가합니다 . 2) 매개 변수 &lt;code&gt;name&lt;/code&gt; 을 프 루닝 된 버전으로 대체하는 반면 원래 (프 루닝되지 않은) 매개 변수는 &lt;code&gt;name+'_orig'&lt;/code&gt; 라는 새 매개 변수에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="8ee04a977907bca0b936aebe6227e9be8b0a5084" translate="yes" xml:space="preserve">
          <source>Pruning itself is NOT undone or reversed!</source>
          <target state="translated">가지 치기 자체는 취소되거나 취소되지 않습니다!</target>
        </trans-unit>
        <trans-unit id="fae06ff3f9da62952a413ca3bf10b3455201b027" translate="yes" xml:space="preserve">
          <source>PruningContainer</source>
          <target state="translated">PruningContainer</target>
        </trans-unit>
        <trans-unit id="784e725830e98a189da61d3afccea45b6e5a9d0f" translate="yes" xml:space="preserve">
          <source>Publishing models</source>
          <target state="translated">모델 게시</target>
        </trans-unit>
        <trans-unit id="256db3aa08282f6add3cb3226d3554daf2bee937" translate="yes" xml:space="preserve">
          <source>Puts values from the tensor &lt;code&gt;value&lt;/code&gt; into the tensor &lt;code&gt;self&lt;/code&gt; using the indices specified in &lt;a href=&quot;#torch.Tensor.indices&quot;&gt;&lt;code&gt;indices&lt;/code&gt;&lt;/a&gt; (which is a tuple of Tensors). The expression &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; is equivalent to &lt;code&gt;tensor[indices] = value&lt;/code&gt;. Returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">인덱스에 지정된 &lt;a href=&quot;#torch.Tensor.indices&quot;&gt; &lt;code&gt;indices&lt;/code&gt; &lt;/a&gt; ( 텐서 의 튜플)를 사용하여 텐서 &lt;code&gt;value&lt;/code&gt; 을 텐서 &lt;code&gt;self&lt;/code&gt; 에 넣습니다. &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; 표현식 은 &lt;code&gt;tensor[indices] = value&lt;/code&gt; 와 동일 합니다 . &lt;code&gt;self&lt;/code&gt; 를 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="98a2e30eeb85238d061519d74935e170e44c72cb" translate="yes" xml:space="preserve">
          <source>PyTorch</source>
          <target state="translated">PyTorch</target>
        </trans-unit>
        <trans-unit id="2837c35a89ca2ff1375acbd6ae4015154d808597" translate="yes" xml:space="preserve">
          <source>PyTorch Contribution Guide</source>
          <target state="translated">PyTorch 기여 가이드</target>
        </trans-unit>
        <trans-unit id="0de6ec530c1988d0af9e4ce7eb73eef1189976b6" translate="yes" xml:space="preserve">
          <source>PyTorch Functions and Modules</source>
          <target state="translated">PyTorch 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="6517675a341634e77bfecd3ddf631a647d857475" translate="yes" xml:space="preserve">
          <source>PyTorch Governance</source>
          <target state="translated">PyTorch 거버넌스</target>
        </trans-unit>
        <trans-unit id="d35b21e83a6b1a4ee2adc066dff9672eedab98a1" translate="yes" xml:space="preserve">
          <source>PyTorch Governance | Persons of Interest</source>
          <target state="translated">PyTorch 거버넌스 | 관심있는 사람</target>
        </trans-unit>
        <trans-unit id="3a2bd7a8f704e6cdbe374466d0208d799f0651cc" translate="yes" xml:space="preserve">
          <source>PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some numeric differences. Depending on model structure, these differences may be negligible, but they can also cause major divergences in behavior (especially on untrained models.) We allow Caffe2 to call directly to Torch implementations of operators, to help you smooth over these differences when precision is important, and to also document these differences.</source>
          <target state="translated">PyTorch 및 ONNX 백엔드 (Caffe2, ONNX 런타임 등)에는 종종 약간의 숫자 차이가있는 연산자 구현이 있습니다. 모델 구조에 따라 이러한 차이는 무시할 수있을 수 있지만 동작에 큰 차이를 유발할 수도 있습니다 (특히 훈련되지 않은 모델에서). Caffe2가 연산자의 Torch 구현을 직접 호출하여 정밀도가 중요한 경우 이러한 차이를 부드럽게 처리 할 수 ​​있습니다. , 그리고 이러한 차이점을 문서화합니다.</target>
        </trans-unit>
        <trans-unit id="6148268ed386643e6b3a594818c58fc07ad9c665" translate="yes" xml:space="preserve">
          <source>PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)</source>
          <target state="translated">PyTorch 분산 패키지는 Linux (안정), MacOS (안정) 및 Windows (프로토 타입)를 지원합니다. Linux의 경우 기본적으로 Gloo 및 NCCL 백엔드가 빌드되고 배포 된 PyTorch에 포함됩니다 (CUDA로 빌드하는 경우에만 NCCL). MPI는 소스에서 PyTorch를 빌드하는 경우에만 포함될 수있는 선택적 백엔드입니다. (예 : MPI가 설치된 호스트에 PyTorch 빌드)</target>
        </trans-unit>
        <trans-unit id="9456a74dbb38bfce73f52a1992187228bd2d00fa" translate="yes" xml:space="preserve">
          <source>PyTorch documentation</source>
          <target state="translated">PyTorch 문서</target>
        </trans-unit>
        <trans-unit id="d19d474346c5b08d13d3339e3bf4feebcd91f549" translate="yes" xml:space="preserve">
          <source>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</source>
          <target state="translated">PyTorch는 GPU 및 CPU를 사용하는 딥 러닝에 최적화 된 텐서 라이브러리입니다.</target>
        </trans-unit>
        <trans-unit id="899246dc869a567118284070768757e2fc424c84" translate="yes" xml:space="preserve">
          <source>PyTorch on XLA Devices</source>
          <target state="translated">XLA 장치의 PyTorch</target>
        </trans-unit>
        <trans-unit id="c93898f1a89d832ed7049068dfa7cb16c03bf3ed" translate="yes" xml:space="preserve">
          <source>PyTorch preserves storage sharing across serialization. See &lt;code&gt;preserve-storage-sharing&lt;/code&gt; for more details.</source>
          <target state="translated">PyTorch는 직렬화 과정에서 스토리지 공유를 보존합니다. 자세한 내용은 &lt;code&gt;preserve-storage-sharing&lt;/code&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ccb71e60b710059778503566d9b6733540438582" translate="yes" xml:space="preserve">
          <source>PyTorch ships with two builtin backends: &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; and &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt;. Additional ones can be registered using the &lt;code&gt;register_backend()&lt;/code&gt; function.</source>
          <target state="translated">PyTorch는 &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; 및 &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt; 의 두 가지 내장 백엔드와 함께 제공 됩니다. &lt;code&gt;register_backend()&lt;/code&gt; 함수를 사용하여 추가로 등록 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2d5a1e0600b8abe6782e41ec54cae91372de9186" translate="yes" xml:space="preserve">
          <source>Python 2 does not support Ellipsis but one may use a string literal instead (&lt;code&gt;'...'&lt;/code&gt;).</source>
          <target state="translated">파이썬 2는 줄임표를 지원하지 않지만 대신 문자열 리터럴 ( &lt;code&gt;'...'&lt;/code&gt; )을 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b5ec6c002217ad80d40ef02a1f4366196fae59b7" translate="yes" xml:space="preserve">
          <source>Python 3 type hints can be used in place of &lt;code&gt;torch.jit.annotate&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.annotate&lt;/code&gt; 대신 Python 3 유형 힌트를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0711b02500c16372db7e037b0db241051fe28013" translate="yes" xml:space="preserve">
          <source>Python API</source>
          <target state="translated">파이썬 API</target>
        </trans-unit>
        <trans-unit id="cad6fbd004ed44f2a0389e7ad1a4cfa89ba1d3fc" translate="yes" xml:space="preserve">
          <source>Python Functions and Modules</source>
          <target state="translated">Python 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="c9209054e624163b69a9eb976a32495c0a33865f" translate="yes" xml:space="preserve">
          <source>Python Language Reference Comparison</source>
          <target state="translated">Python 언어 참조 비교</target>
        </trans-unit>
        <trans-unit id="e4ec2cbb65019f4837950da585dfbc7781b88d91" translate="yes" xml:space="preserve">
          <source>Python Language Reference Coverage</source>
          <target state="translated">Python 언어 참조 범위</target>
        </trans-unit>
        <trans-unit id="fd58a092a5203baf1b73956b039089eaeff86afb" translate="yes" xml:space="preserve">
          <source>Python classes can be used in TorchScript if they are annotated with &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt;, similar to how you would declare a TorchScript function:</source>
          <target state="translated">Python 클래스는 TorchScript 함수를 선언하는 방법과 유사하게 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 로 주석이 달린 경우 TorchScript에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="dcb69ff04adf584813a764fcff415e81178eed67" translate="yes" xml:space="preserve">
          <source>Python enums can be used in TorchScript without any extra annotation or code:</source>
          <target state="translated">Python 열거 형은 추가 주석이나 코드없이 TorchScript에서 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="21aa60d14a5d3bf9941c64223102e72e43cd7db3" translate="yes" xml:space="preserve">
          <source>Python-defined Constants</source>
          <target state="translated">Python 정의 상수</target>
        </trans-unit>
        <trans-unit id="bca513fe1a1ac6b508002e4d70e9b04a95755456" translate="yes" xml:space="preserve">
          <source>Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.</source>
          <target state="translated">Pytorch Hub는 연구 재현성을 촉진하도록 설계된 사전 훈련 된 모델 저장소입니다.</target>
        </trans-unit>
        <trans-unit id="4a3b71fd725a5570aa333bd5a258f51e9ad067c8" translate="yes" xml:space="preserve">
          <source>Pytorch Hub provides convenient APIs to explore all available models in hub through &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;, show docstring and examples through &lt;a href=&quot;#torch.hub.help&quot;&gt;&lt;code&gt;torch.hub.help()&lt;/code&gt;&lt;/a&gt; and load the pre-trained models using &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Pytorch Hub는 &lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt; 통해 허브에서 사용 가능한 모든 모델을 탐색하고 torch.hub.help () 를 통해 docstring 및 예제를 표시하며 &lt;a href=&quot;#torch.hub.help&quot;&gt; &lt;code&gt;torch.hub.help()&lt;/code&gt; &lt;/a&gt; 를 사용하여 사전 &lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt; 된 모델을로드 할 수있는 편리한 API를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="71ea6e31bfc2cea7da3ec5ce3389fe77a6457276" translate="yes" xml:space="preserve">
          <source>Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple &lt;code&gt;hubconf.py&lt;/code&gt; file;</source>
          <target state="translated">Pytorch Hub는 간단한 &lt;code&gt;hubconf.py&lt;/code&gt; 파일 을 추가하여 사전 훈련 된 모델 (모델 정의 및 사전 훈련 된 가중치)을 github 저장소에 게시 하는 것을 지원 합니다.</target>
        </trans-unit>
        <trans-unit id="c3156e00d3c2588c639e0d3cf6821258b05761c7" translate="yes" xml:space="preserve">
          <source>Q</source>
          <target state="translated">Q</target>
        </trans-unit>
        <trans-unit id="817b3643a8c9250c5615063227303b8c5da71cbe" translate="yes" xml:space="preserve">
          <source>Q: Does ONNX support implicit scalar datatype casting?</source>
          <target state="translated">Q : ONNX는 암시 적 스칼라 데이터 유형 캐스팅을 지원합니까?</target>
        </trans-unit>
        <trans-unit id="e999540844f438b9780d0892a8883a34284bbec7" translate="yes" xml:space="preserve">
          <source>Q: How do I store attributes on a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;?</source>
          <target state="translated">Q : &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 에&lt;/a&gt; 속성을 어떻게 저장 합니까?</target>
        </trans-unit>
        <trans-unit id="71cd3f9caa34814ac682f01c8799e67876c37c7b" translate="yes" xml:space="preserve">
          <source>Q: How to export models with loops in it?</source>
          <target state="translated">Q : 루프가있는 모델을 내보내는 방법은 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="3ab78ce25486eb77e84e3dce117cf425f112dd6c" translate="yes" xml:space="preserve">
          <source>Q: I have exported my lstm model, but its input size seems to be fixed?</source>
          <target state="translated">Q : lstm 모델을 내보냈는데 입력 크기가 고정 된 것 같습니다.</target>
        </trans-unit>
        <trans-unit id="a12de1b8a4f8718726ec01c0631665899c7a3116" translate="yes" xml:space="preserve">
          <source>Q: I would like to trace module&amp;rsquo;s method but I keep getting this error:</source>
          <target state="translated">Q : 모듈의 메서드를 추적하고 싶지만이 오류가 계속 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2b00cd8e47e4d783f5be1951f1dd20a7ce719f21" translate="yes" xml:space="preserve">
          <source>Q: I would like to train a model on GPU and do inference on CPU. What are the best practices?</source>
          <target state="translated">Q : GPU에서 모델을 훈련시키고 CPU에서 추론을하고 싶습니다. 모범 사례는 무엇입니까?</target>
        </trans-unit>
        <trans-unit id="470067c814eb1ea3d524592caed0d6e62ae92c6f" translate="yes" xml:space="preserve">
          <source>Q: Is tensor in-place indexed assignment like &lt;code&gt;data[index] = new_data&lt;/code&gt; supported?</source>
          <target state="translated">Q : &lt;code&gt;data[index] = new_data&lt;/code&gt; 와 같은 텐서 내부 색인 할당이 지원 되나요?</target>
        </trans-unit>
        <trans-unit id="e79884b3f980f1ad19cca2c799bbefd52769c3b3" translate="yes" xml:space="preserve">
          <source>Q: Is tensor list exportable to ONNX?</source>
          <target state="translated">Q : 텐서 목록을 ONNX로 내보낼 수 있습니까?</target>
        </trans-unit>
        <trans-unit id="892976819297325dacfe2454b5f3511112ebff7a" translate="yes" xml:space="preserve">
          <source>QFunctional</source>
          <target state="translated">QFunctional</target>
        </trans-unit>
        <trans-unit id="97f0eded6b0e44ee7ab93339b4fa67092a5342fa" translate="yes" xml:space="preserve">
          <source>Quantization</source>
          <target state="translated">Quantization</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
