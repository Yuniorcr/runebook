<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="948b8abe5368abe4340d2b3e569a8f9c091af83c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Tanhshrink&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#Tanhshrink&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Tanhshrink&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#Tanhshrink&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a8c72d35e1ad2a7cc97009e1130080ae4528532" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Threshold(threshold: float, value: float, inplace: bool = False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#Threshold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Threshold(threshold: float, value: float, inplace: bool = False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#Threshold&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b5a79b7ec16365d3e28bf961e840439c5707a7d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Transformer(d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = 'relu', custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Transformer(d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = 'relu', custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae5f8d8b4eebe00aaa3324fdf01f1e0b39e7c4d9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoder&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoder&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="994c7eb931eb38d4aeb16235b135fc3746ba9709" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c0a6f1ac271088b5c07e4cdcd41e385719507e9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoder&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoder&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b1cc6f94190e5f8d8f1a606f313f2c4fa9939b4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1b8d320b0ee625b96ddc21defa8a739b828ad64" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TripletMarginLoss(margin: float = 1.0, p: float = 2.0, eps: float = 1e-06, swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/loss.html#TripletMarginLoss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TripletMarginLoss(margin: float = 1.0, p: float = 2.0, eps: float = 1e-06, swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/loss.html#TripletMarginLoss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db51b3d8935ca2897c455c0858222742108e8218" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.TripletMarginWithDistanceLoss(*, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/loss.html#TripletMarginWithDistanceLoss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.TripletMarginWithDistanceLoss(*, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/loss.html#TripletMarginWithDistanceLoss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="09e6cd37ca6d2878f357b86bb94bf6120a88423b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Unflatten(dim: Union[int, str], unflattened_size: Union[torch.Size, Tuple[Tuple[str, int]]])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/flatten.html#Unflatten&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Unflatten(dim: Union[int, str], unflattened_size: Union[torch.Size, Tuple[Tuple[str, int]]])&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/flatten.html#Unflatten&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8a655cfbc808773207e5ab1b8c26e593abbbc766" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Unfold(kernel_size: Union[T, Tuple[T, ...]], dilation: Union[T, Tuple[T, ...]] = 1, padding: Union[T, Tuple[T, ...]] = 0, stride: Union[T, Tuple[T, ...]] = 1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/fold.html#Unfold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Unfold(kernel_size: Union[T, Tuple[T, ...]], dilation: Union[T, Tuple[T, ...]] = 1, padding: Union[T, Tuple[T, ...]] = 0, stride: Union[T, Tuple[T, ...]] = 1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/fold.html#Unfold&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b6a3a936aa19c1fb0ed1e12dd45156a950a00047" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.Upsample(size: Optional[Union[T, Tuple[T, ...]]] = None, scale_factor: Optional[Union[T, Tuple[T, ...]]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#Upsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.Upsample(size: Optional[Union[T, Tuple[T, ...]]] = None, scale_factor: Optional[Union[T, Tuple[T, ...]]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#Upsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="adc58c29273eb816f626d2768d0d3e57ab2a2bab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.UpsamplingBilinear2d(size: Optional[Union[T, Tuple[T, T]]] = None, scale_factor: Optional[Union[T, Tuple[T, T]]] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.UpsamplingBilinear2d(size: Optional[Union[T, Tuple[T, T]]] = None, scale_factor: Optional[Union[T, Tuple[T, T]]] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2c028f63a248e723710e606ef3eb953271fd8afd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.UpsamplingNearest2d(size: Optional[Union[T, Tuple[T, T]]] = None, scale_factor: Optional[Union[T, Tuple[T, T]]] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.UpsamplingNearest2d(size: Optional[Union[T, Tuple[T, T]]] = None, scale_factor: Optional[Union[T, Tuple[T, T]]] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8cf231cb1778d5f554a8a5c773037ea12ccac3ac" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.ZeroPad2d(padding: Union[T, Tuple[T, T, T, T]])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/padding.html#ZeroPad2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.ZeroPad2d(padding: Union[T, Tuple[T, T, T, T]])&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/padding.html#ZeroPad2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2bfe7942d0098e35201c17dc02e2c82548a33012" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvBn1d(conv, bn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBn1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvBn1d(conv, bn)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBn1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bcad145be8166f0632cda397a4bb9af7567414ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvBn2d(conv, bn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBn2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvBn2d(conv, bn)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBn2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="27e261b0579988f8bc5e36e5647844f176e47151" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvBnReLU1d(conv, bn, relu)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBnReLU1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvBnReLU1d(conv, bn, relu)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBnReLU1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75239e796bbe97d6cc035852aaf3484029a92849" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvBnReLU2d(conv, bn, relu)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBnReLU2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvBnReLU2d(conv, bn, relu)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvBnReLU2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58464d16d7c1c0fb8d17caaf3365590d2aa85bbf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvReLU1d(conv, relu)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvReLU1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvReLU1d(conv, relu)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvReLU1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e9e5bce04d014ab4f6927002338dad40762756b4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.ConvReLU2d(conv, relu)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.ConvReLU2d(conv, relu)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/modules/fused.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fa2300b563583dbf09d7d1ac49a7188c6d227f00" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvBn2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvBn2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4ed8ad9c01bda086a946ad24bd7d59f70b1f41ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvBnReLU2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvBnReLU2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="36d2fbef4c76c22cc7af6268e95bc4e37fc13ff9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/conv_fused.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f64a20d099c6958ca170048e4cddf367a097c1b1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/linear_relu.html#LinearReLU&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/qat/modules/linear_relu.html#LinearReLU&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f526dae6c63f3bc60754fa6efd47273fe6cada9a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/conv_relu.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/conv_relu.html#ConvReLU2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="62051df2367f62f68343c0aa6dce437a62dbc3b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/conv_relu.html#ConvReLU3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/conv_relu.html#ConvReLU3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2db41745b194e11db784d987a73cbc221788d037" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/linear_relu.html#LinearReLU&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/intrinsic/quantized/modules/linear_relu.html#LinearReLU&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="52522befc09b7129ebd385a51c8ab2ba9ddc798b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="717668d56f0e7d5539d5653cb51fe7cb5ce47761" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.parameter.Parameter&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parameter.html#Parameter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.parameter.Parameter&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parameter.html#Parameter&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3309146f120da1d40c46e619d9252fc25fc463a9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/conv.html#Conv2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/conv.html#Conv2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c6c20275e941f7ddd2514a12a7bf36e26c8e703" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/linear.html#Linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/linear.html#Linear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87dff6b4b7ca7461a74bd6b4236269df8d182246" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7d96f86e8a619a290bb02fb62dde609262df5f18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f8b8a25b56a478e748b3d298b23a55d9f7f6a386" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9909f56e70cc0ee3d0199cc6984de1373402267a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="53267d41d0da1a23aa14d481444bf95e0661487c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4c8f1cb09f2390af3cb7c596e316e12b4e8cc25e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.DeQuantize&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules.html#DeQuantize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.DeQuantize&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules.html#DeQuantize&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e23f304e4df27428b147aa29adccf15c694a9ed9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ELU&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ELU&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="86a4dfdb8f62ca877805631b647c9c7feb570be6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.FloatFunctional&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/functional_modules.html#FloatFunctional&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.FloatFunctional&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/functional_modules.html#FloatFunctional&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0606d689ab93b0e805f812ccd82d9d3e72531cf1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#GroupNorm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#GroupNorm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cdce2f21a16a9238f14a745a494c4d7a7fdf1ff8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Hardswish(scale, zero_point)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#Hardswish&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Hardswish(scale, zero_point)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#Hardswish&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9ec1825917bfbc481d9d6283d4e4c94240b7c02e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="262230df38a44d38d6271916d1b666a342a84a19" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a7edd021f69a72fc6b16d320a91256438893ab65" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ba805d12c73c473e4b628c92e994038f6f609636" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#LayerNorm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/normalization.html#LayerNorm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26f63e924e051c39b534fde6115ce675a8f68d44" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/linear.html#Linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/linear.html#Linear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4f90b286fddbc7d733c7f0d8165313187130968f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.QFunctional&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/functional_modules.html#QFunctional&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.QFunctional&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/functional_modules.html#QFunctional&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4f8f1fb9a523c20e595e0f5bb3d39f39085adb7c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.Quantize(scale, zero_point, dtype)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules.html#Quantize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.Quantize(scale, zero_point, dtype)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules.html#Quantize&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e31043c88ca2c519cebcc6d8c303ffc0ac4cb115" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.ReLU(inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ReLU&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.ReLU(inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ReLU&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e12a58d749f4a49b57e0fa5685a029da1f0294ee" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.ReLU6(inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ReLU6&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.quantized.ReLU6(inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/activation.html#ReLU6&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="32e6cfcd179e87d03f2c07c67baec018fde59a7a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#GRUCell&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7530f848e448cf7fed57aa0a7192628817e55a0d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.dynamic.LSTM(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#LSTM&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c52c30f36175bb105780e8cc7186ce21c89bd033" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#LSTMCell&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5742afd8eeb4ab7ac7a0d9441c0fd60fe8a5dbad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/linear.html#Linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0e823cde4ec7f0ebb744a695e0968c5de610432" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#RNNCell&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e2680452de653f4a011d0bf4a50f8536d6655c9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.BasePruningMethod&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.BasePruningMethod&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bb1d180f159eca5927d22a3d70cd3c73e43b6e7b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.CustomFromMask(mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#CustomFromMask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.CustomFromMask(mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#CustomFromMask&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10c49d4e51e12251427b97471cbe3575d500d278" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.Identity&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#Identity&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.Identity&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#Identity&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2652c65f3170b409783b63f94e03dbe2b7ff9480" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.L1Unstructured(amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#L1Unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.L1Unstructured(amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#L1Unstructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f0b879c45c3dcca3022ae945885c0cfb3fe7963e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.LnStructured(amount, n, dim=-1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.LnStructured(amount, n, dim=-1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f17c8761d708795f8f9d25a6286f33e7b85ba0df" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.PruningContainer(*args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#PruningContainer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.PruningContainer(*args)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#PruningContainer&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="42fcc61bf0ea431bc27ca523d3548b5e2c4d5996" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.RandomStructured(amount, dim=-1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.RandomStructured(amount, dim=-1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08473be2df19d3a9d739b00b2e811c5efd36a264" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.prune.RandomUnstructured(amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomUnstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.prune.RandomUnstructured(amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomUnstructured&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="46506989c3fc4a176a0e41886b9d674e47de434a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.nn.utils.rnn.PackedSequence&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.nn.utils.rnn.PackedSequence&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0fbd59e916115537b8e763a4e757e62728501699" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.no_grad&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/grad_mode.html#no_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.no_grad&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/grad_mode.html#no_grad&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2c0ac3888e44e819ad1a3b43c166ef6bea9dd2a5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/asgd.html#ASGD&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b939d44db8f20f26e011b01741d15ca49a85a871" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adadelta.html#Adadelta&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a707132ecb104ed3f8689a44700848106de9380" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adagrad.html#Adagrad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30587fa8b73b062bbcefe8eddae7e02799fa4241" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adam.html#Adam&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3ceef3353a64680037a6cb64a81ac025566d95f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamw.html#AdamW&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff3f31554f1e45bf12501a789d5a803853c7c61b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamax.html#Adamax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85ce48cf4e4d07071c0dd4c362fecc4ba49c0cf7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lbfgs.html#LBFGS&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d64916f5ee3ca3ad64819a99d8d2f8adb88e616" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Optimizer(params, defaults)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8eaa66b9e997eafa9063b6b6be425a745ac2b524" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rmsprop.html#RMSprop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8369fd2688fee909ba95fe23136c169059a33de7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rprop.html#Rprop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f35bb11b99f07cea5d9e791b90dbecad4b4670a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.SGD(params, lr=&amp;lt;required parameter&amp;gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sgd.html#SGD&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43b95ee029e07d17a473d670e4e0ab17fd8201b2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sparse_adam.html#SparseAdam&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29ddd8ca477229424c51e40adb8c8335cbeb9090" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CosineAnnealingLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2673636f7e4f17373b5032aa468786d675418b2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CosineAnnealingWarmRestarts&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b10a0115fe1c4c18afd8d84c3b00fdebba56ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CyclicLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1976e3980c45a73f25218df8c4c9d8a6572daed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#ExponentialLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fd0425bd53d0d747811828cf6abe972a0ad6b65" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#LambdaLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d20f393d2025149c86cfd0fc924f57667283545a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiStepLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2c13740308c45aeeb782d0f241a8374b4ee0a11" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiplicativeLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c79681e92182ef96b74f915d48d732463af8aafa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#OneCycleLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1242cfc1a712f3a5c3c42c479f4b11a4a7c66b44" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#ReduceLROnPlateau&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9689e1911466df445a6e93b29a02e793b575ef5d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#StepLR&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf9fe7c1c290d649c7c0a34ae465323d2bfa3af1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.DeQuantStub&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/stubs.html#DeQuantStub&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7adaff01d21f23be402cca522143998b8b6966a3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.FakeQuantize(observer=&amp;lt;class 'torch.quantization.observer.MovingAverageMinMaxObserver'&amp;gt;, quant_min=0, quant_max=255, **observer_kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/fake_quantize.html#FakeQuantize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e0b11d27546570c3b2cff1628d4493cb5883899" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#HistogramObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e291d3936e8497ed45ce0c37a2ab4907d11e4ac" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#MinMaxObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cddaf0817f9d73d79306a8108fc709d70c096de" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#MovingAverageMinMaxObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="442659b46eb41dd589c415dbfac7c718d8122af1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#MovingAveragePerChannelMinMaxObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9071440de5580cba96ba0f9bcf270f59ea5cbfa9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.NoopObserver(dtype=torch.float16, custom_op_name='')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#NoopObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2861ce98ca7ebed2657425086989eec549d1848" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.ObserverBase(dtype)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#ObserverBase&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4f30b66e45bd16daff4b6dbdee5903b38d4bdaf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#PerChannelMinMaxObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ba44238561ef9ce8b1a6826fbd576ee09f21374" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.QConfig&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/qconfig.html#QConfig&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0423cc1e223820ce1c91fe476ef529ea9ec105c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.QConfigDynamic&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/qconfig.html#QConfigDynamic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25e7dea87cabfcd1b79c2b7a719d3a40fd991f2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.QuantStub(qconfig=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/stubs.html#QuantStub&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062e969bfd8a56ed276f9e030dba2a7b76f171ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.QuantWrapper(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/stubs.html#QuantWrapper&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a493599aa7915ddca8c7fbb6430f5c6259e9c12" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quantization.RecordingObserver(**kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/observer.html#RecordingObserver&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e496f42739cbf06cb0d93f0e32ec51fa65ff828" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f936080e1508e241fd070066f0def1c64f28924e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.set_grad_enabled(mode: bool)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/grad_mode.html#set_grad_enabled&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.set_grad_enabled(mode: bool)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/grad_mode.html#set_grad_enabled&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="105b28e7c13b2c00a738583679e680c4c4f6e691" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.BatchSampler(sampler: torch.utils.data.sampler.Sampler[int], batch_size: int, drop_last: bool)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#BatchSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d90bfb0b8a28f5728f54b3a9f1ade64d3d1b971a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.ChainDataset(datasets: Iterable[torch.utils.data.dataset.Dataset])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#ChainDataset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee953870d6cbdda4784ca329b33da7f5562006a2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.ConcatDataset(datasets: Iterable[torch.utils.data.dataset.Dataset])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#ConcatDataset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3efbf6fa1a5407f4f2da17abd7b27bc928dd5123" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.DataLoader(dataset: torch.utils.data.dataset.Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[torch.utils.data.sampler.Sampler[int]] = None, batch_sampler: Optional[torch.utils.data.sampler.Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: Callable[List[T], Any] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Callable[int, None] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataloader.html#DataLoader&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f75fb909c54c30d703302dc06d7d49c6f3a7e06" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.Dataset&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#Dataset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3255c073ea812cb23bed244746d1940748d67dba" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.IterableDataset&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#IterableDataset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb76699ed58b674ae2bad056f9e95d70bcc1c4ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.RandomSampler(data_source: collections.abc.Sized, replacement: bool = False, num_samples: Optional[int] = None, generator=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#RandomSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="406c261c77e6f7ee31c0ba702ec5af8f1abf1ba0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.Sampler(data_source: Optional[collections.abc.Sized])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#Sampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f25d7912c383d4022af807d1e94cf425fec1d3dd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.SequentialSampler(data_source)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#SequentialSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5980086eba5f32f188b6f457aeca1deef62dfefd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.Subset(dataset: torch.utils.data.dataset.Dataset[T_co], indices: Sequence[int])&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#Subset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6611f238211e33a415f9644a6dcf15ce6c8cb9a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.SubsetRandomSampler(indices: Sequence[int], generator=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#SubsetRandomSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdfc2c5bc3e15852ad871b8e5466ca94e53ef379" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.TensorDataset(*tensors: torch.Tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#TensorDataset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="180f928113f3f0f60b530972de0478ca21ac58a1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.WeightedRandomSampler(weights: Sequence[float], num_samples: int, replacement: bool = True, generator=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/sampler.html#WeightedRandomSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e87b661876c3db29a5961b2d2a06a7322e54f8d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.data.distributed.DistributedSampler(dataset: torch.utils.data.dataset.Dataset, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/distributed.html#DistributedSampler&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc8a23983389d9dea43c68aaa979532e7c2762bf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;class torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;class torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="264a5192c3190298ae845d04b23f74c1dc8dcac8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#Identity.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#Identity.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e21867d312ec243014506fb6a945651e18395c44" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0a88b88f301617596d6efa53fe1428b1aebe704" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#L1Unstructured.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#L1Unstructured.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0eff5e60a1b954b8cacdd40d9319d758ac718ab7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomUnstructured.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomUnstructured.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1ed830187e9aaebd21186a27767a8c790f27cd03" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, amount, dim=-1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, amount, dim=-1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1bb8d11fdd5deac965944d59ff501c5887fa2429" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, amount, n, dim)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, amount, n, dim)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="594f342d1601ac9afe58834dc85becf3f2cf176d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod apply(module, name, mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#CustomFromMask.apply&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod apply(module, name, mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#CustomFromMask.apply&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c6506b270cefe27adb38a46bf2cdb4c58c32189" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod convert_sync_batchnorm(module, process_group=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod convert_sync_batchnorm(module, process_group=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="80230439feba946552a0704225a4a8f8057ec541" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/conv.html#Conv2d.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/conv.html#Conv2d.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="170286855d8e4df38cae65c6ede34d5412bb66e4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/linear.html#Linear.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/qat/modules/linear.html#Linear.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6a436aa475c48b4c9bf36968d61d8edfeb86568c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/dynamic/modules/linear.html#Linear.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41ebad0768275b222ee76c480956734233e55385" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7458f42568e8e0a63b8549e53d40be9ad78f923d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b849799744f9c7777dbf1af812be2d90396c8ff3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e464c0e5ebe3c51a4c995ea97d7da3253fa9dad6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_float(mod)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/linear.html#Linear.from_float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_float(mod)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/modules/linear.html#Linear.from_float&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="611c52d6294d5cea59bdcf7f05963bc7febc180c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_ipc_handle(device, handle)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.from_ipc_handle&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebd99429d45259a388e1fae78ce00db978bf076b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cf165037cb4e01eaa89321aff3a4e65c939f4e06" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;classmethod from_pretrained(embeddings: torch.Tensor, freeze: bool = True, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, include_last_offset: bool = False) &amp;rarr; torch.nn.modules.sparse.EmbeddingBag&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/sparse.html#EmbeddingBag.from_pretrained&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;classmethod from_pretrained(embeddings: torch.Tensor, freeze: bool = True, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, include_last_offset: bool = False) &amp;rarr; torch.nn.modules.sparse.EmbeddingBag&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/sparse.html#EmbeddingBag.from_pretrained&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d8ce68abf98476d8240963cd5ac4365765e53236" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;clear() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.clear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;clear() &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.clear&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4f03390480083360394d0d086a1957badb322a87" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;clear() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.clear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;clear() &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.clear&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e090da48632650c78297560fb1ecf0a79ea7b86c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;close()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.close&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;close()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.close&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="78fd7996187e3e96f0cd1f15c452731148e599a0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured.compute_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#LnStructured.compute_mask&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="51b6d4956aa5085a5cde0699ff48b593e0c41141" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#PruningContainer.compute_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#PruningContainer.compute_mask&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="687b37361094d9d7203151d6c5ae4fc4567079ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured.compute_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;compute_mask(t, default_mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#RandomStructured.compute_mask&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="30ec406ca3e1610684e1e8ccc5716e06dba6b6c0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.covariance_matrix&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.covariance_matrix&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6e62d920dcc5b8215fccea86475131a96dc55a4c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.covariance_matrix&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.covariance_matrix&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b0ebaddc088f36283d859a646cdda08fef1503f5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;cpu() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.cpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;cpu() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.cpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5f2bac0759b7374d28d965fc186e81154faf3ff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;cuda(device: Union[int, torch.device, None] = None) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.cuda&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;cuda(device: Union[int, torch.device, None] = None) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.cuda&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6bfee6fe574277e86f7a5ecb2712d5071942fd0e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;cufft_plan_cache&lt;/code&gt; caches the cuFFT plans</source>
          <target state="translated">&lt;code&gt;cufft_plan_cache&lt;/code&gt; 는 cuFFT 계획을 캐시합니다.</target>
        </trans-unit>
        <trans-unit id="4835286aa0f0db1e1e8f4adf4adbc0575d8df52f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;current&lt;/code&gt;: current value of this metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bcea0fca27a754c09c9ab1b747beafa8e581661" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;cutoffs&lt;/code&gt; should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting &lt;code&gt;cutoffs = [10, 100, 1000]&lt;/code&gt; means that first &lt;code&gt;10&lt;/code&gt; targets will be assigned to the &amp;lsquo;head&amp;rsquo; of the adaptive softmax, targets &lt;code&gt;11, 12, &amp;hellip;, 100&lt;/code&gt; will be assigned to the first cluster, and targets &lt;code&gt;101, 102, &amp;hellip;, 1000&lt;/code&gt; will be assigned to the second cluster, while targets &lt;code&gt;1001, 1002, &amp;hellip;, n_classes - 1&lt;/code&gt; will be assigned to the last, third cluster.</source>
          <target state="translated">&lt;code&gt;cutoffs&lt;/code&gt; 는 오름차순으로 정렬 된 정수 시퀀스 여야합니다. 클러스터 수와 대상을 클러스터로 분할하는 것을 제어합니다. 예를 들어 &lt;code&gt;cutoffs = [10, 100, 1000]&lt;/code&gt; 하면 처음 &lt;code&gt;10&lt;/code&gt; 개의 대상이 적응 형 소프트 맥스의 '헤드'에 할당되고 대상 &lt;code&gt;11, 12, &amp;hellip;, 100&lt;/code&gt; 이 첫 번째 클러스터에 할당되고 대상 &lt;code&gt;101, 102, &amp;hellip;, 1000&lt;/code&gt; 이 할당 됨을 의미합니다 . 102,&amp;hellip;, 1000 은 두 번째 클러스터에 할당되고 대상 &lt;code&gt;1001, 1002, &amp;hellip;, n_classes - 1&lt;/code&gt; 은 마지막 세 번째 클러스터에 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="224b63af97633d574f66a560aad6cf2022deb43d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dataset&lt;/code&gt;: the copy of the dataset object in &lt;strong&gt;this&lt;/strong&gt; process. Note that this will be a different object in a different process than the one in the main process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d790ecf58866423b36ea1badf89c0368d5fbeb3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dependencies&lt;/code&gt; variable is a &lt;strong&gt;list&lt;/strong&gt; of package names required to &lt;strong&gt;load&lt;/strong&gt; the model. Note this might be slightly different from dependencies required for training a model.</source>
          <target state="translated">&lt;code&gt;dependencies&lt;/code&gt; 변수는 모델 을 &lt;strong&gt;로드&lt;/strong&gt; 하는 데 필요한 패키지 이름 &lt;strong&gt;목록&lt;/strong&gt; 입니다 . 이것은 모델 학습에 필요한 종속성과 약간 다를 수 있습니다.&lt;strong&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="b457a09cfadbd7d8c8de6b2f25367327ad3ee5be" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;destination&lt;/code&gt; must not be specified when &lt;code&gt;out&lt;/code&gt; is specified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84ea94c5edf88a95391ce891268d667f14e189f1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dilation&lt;/code&gt; controls the spacing between the kernel points; also known as the &amp;agrave; trous algorithm. It is harder to describe, but this &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of what &lt;code&gt;dilation&lt;/code&gt; does.</source>
          <target state="translated">&lt;code&gt;dilation&lt;/code&gt; 은 커널 지점 사이의 간격을 제어합니다. &amp;agrave; trous 알고리즘이라고도합니다. 설명하기가 더 어렵지만이 &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;링크&lt;/a&gt; 는 &lt;code&gt;dilation&lt;/code&gt; 이 수행 하는 작업에 대한 멋진 시각화를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="a8be6e9ebe8c51ee5e77750be502d50ac83c19b9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dim&lt;/code&gt; specifies the dimension of the input tensor to be unflattened, and it can be either &lt;code&gt;int&lt;/code&gt; or &lt;code&gt;str&lt;/code&gt; when &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;NamedTensor&lt;/code&gt; is used, respectively.</source>
          <target state="translated">&lt;code&gt;dim&lt;/code&gt; 은 &lt;code&gt;NamedTensor&lt;/code&gt; 않을 입력 텐서의 차원을 지정하며, &lt;code&gt;Tensor&lt;/code&gt; 또는 NamedTensor 가 각각 사용될 때 &lt;code&gt;int&lt;/code&gt; 또는 &lt;code&gt;str&lt;/code&gt; 일 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c95b21ebff0e40f81abdf2eba692741100b3ca1f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dir(model)&lt;/code&gt; to see all available methods of the model.</source>
          <target state="translated">&lt;code&gt;dir(model)&lt;/code&gt; 을 사용하여 모델의 사용 가능한 모든 메서드를 확인합니다.</target>
        </trans-unit>
        <trans-unit id="a3705557bfc7ab6d51de776d39f0b0da11ebd3e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;div_value&lt;/code&gt; is used to compute the size of each additional cluster, which is given as</source>
          <target state="translated">&lt;code&gt;div_value&lt;/code&gt; 는 각 추가 클러스터의 크기를 계산하는 데 사용되며 다음과 같이 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="a4cdfadbe144d1c31b951eecdd246c0fe81013b1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;done() &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.done&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;done() &amp;rarr; bool&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.done&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d675a4f0ef09236a16643f2487767becbc130f1c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;double() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.double&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;double() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.double&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c26b2c3b5c296ea9c68dad31767f1d8dd795ee24" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;draw(n=1, out=None, dtype=torch.float32)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.draw&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;draw(n=1, out=None, dtype=torch.float32)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.draw&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="65555474802e7b4157192f606b852d31fcb368d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;dtype&lt;/code&gt; can only take &lt;code&gt;torch.qint8&lt;/code&gt; or &lt;code&gt;torch.quint8&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9814e0dab5f44a4b332550519138c9f384dc097" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;elapsed_time(end_event)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.elapsed_time&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;elapsed_time(end_event)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.elapsed_time&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2358326741092d27bab69187e41f3ff5d4867c35" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="357d5d934321da2bd4b86ba2ae850c8cd3d8d693" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9f959f4d692b77890ec647df15b48a54bb46dae1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c7d1aeeef21c1f9d5eff3d65c43063bb36dbc01c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0c74e155ca48cd30a354d6b0ec4412f2d7a7811" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="86ba039ef57b69a4d89fe676833d0a4a865ba3c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a6b461da3c1790921357adb26a5f2f615db2f7e5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d67e8d7ad9762698afe9fa72a6233ace3086f6a6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exp_family.html#ExponentialFamily.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exp_family.html#ExponentialFamily.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e4bf43b4c5cab1fd56bcd86cd286bb5106059d3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5c1e35b67f93fe0dd1817c31e56ebb657aad6ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="af12ddba053955304fbdfd4aecfe0eea8b688d2a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e05a8cb00791410b3b93e748fff74405e83d85d4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gumbel.html#Gumbel.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gumbel.html#Gumbel.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b4d4ef96174c23a989b9ce54ac66ea68cf6c58cf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="662b84b30f2415e104a87846b31a6d137aeab793" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="79781cad24b1393afc386867e216843f5cc94794" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8a20b21d22a17608be7c486db8c366c7084981a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b6e253a08a037ac782844c57ac9dee197ef626c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/log_normal.html#LogNormal.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/log_normal.html#LogNormal.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="751bcd5af29200a56631f9572b8caf9ebc4b094e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dd164a24882419d94b8824a976c638b651b01284" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b2cbd1b4499d83ed3c7137d480678489c267056d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e68534527e24fd3de0b2f28781a097fb9f13dc3e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="45b8d0c48bcc63e215dc0df76657a6846aebd975" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/pareto.html#Pareto.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/pareto.html#Pareto.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2117b9aecbe90a334721199dc7d1b7b08a3463b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8e9f77da9c1f61cfa8e0a004bd16c01311a22eb0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d3b3613269929f25544bb6dd9e3285803a7e32ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/weibull.html#Weibull.entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;entropy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/weibull.html#Weibull.entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="038a1d0277009eee67337d098930ed768ae24709" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cf737fb94529d050c7c74c9d0ddeeebb32dd7855" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="77f0bd53d550227f53485377e70a83cba2567a40" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f82e27372620cfb387803e5709633c760eb901fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5c867baed920fcb1771ad182159b52c2161c736c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c8b3f16ecd7a600890d5aeb56617692299642df4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.enumerate_support&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;enumerate_support(expand=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.enumerate_support&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b0786cd56d4376b87b23a60c74dd6f9c334fd5ac" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;eval() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.eval&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;eval() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.eval&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c55c30fd0af020a3c56aa8a9dcff1d949dc650a2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;expand(batch_shape)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.expand&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="21bbd5788b968ede6d801db6e3414a8ccce88d6d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfc98de1a53c93a4081b8a6b6bdf2418afabc65b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22d94e39024431d37f5c249440efde61e78345e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9490e23de30d4809767de6fad5b66a899460ea2a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba0dc3fd962e0760d58bee3618bca17dc23f25ee" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69808d84f4c9d1a2cedddd35e00e015d03bc2a4f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/chi2.html#Chi2.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c344e4956dbec95be1040a66c2922e0eef9d06ca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f574080b2a3b4f9c400f412f54b3ed071d8400d2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17cf6abff20467a7c2e246795c23b86f532cbab7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a19affbaa13f4931424a5e82072c521865463d5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b37519743c34c4eeb841f100a20d03894902d55b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f753d5bb6cc5f26b5b8fe95b60efeb393230f144" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab15a9fffd3eec2f9bc429cbb575bddda8c59192" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92f748a1436d8b5f0925b54faa64128fbe750786" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gumbel.html#Gumbel.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6899f0eab46af58fb8efa589181ccb3cc62378ca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c960d255d9898ee4b48f2cd25a50ec8a3a5105fe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d5ee92f076811069b6cba89675dc858c6ae0e73" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cb3bfc15d16f76d03973b843fffda0a822c8caf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b992ee2bb34978f2b82a8d8bfd24652e4faca264" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/log_normal.html#LogNormal.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc907cd2239d3d3d9f10b336de95b201580f15a6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2bb8382e1e0e4b10ec1f2e80e7b349a087161186" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03d24ff66032105ebada8191c1e2ee183ba1dd6d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multinomial.html#Multinomial.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7a888099e5425d87deaa6454998388c8a3a58bc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1d7d725d5802ed4b81517161cef76038bbd5038" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bc5828011b920a064667779d3d0237a0f2c1532" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3248620330fbc5bfb737b12e1eaaed057d034caf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="307ca350dfebd423d7668bce2d2da4c7ef4b0583" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/pareto.html#Pareto.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6dccb239c1eb95239edfa0912a862182ca5d263" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/poisson.html#Poisson.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d403395df39f6593420836e654620a81fb5818df" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e54f7d56f0b01b3fcca36a1af08c4dbeb393afa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11c9c88c6c34e4aa169af0b33948a004021adabc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_categorical.html#RelaxedOneHotCategorical.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b707612d5a72c03ec5942e87b4178701b05f9d30" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cbaeedd3f13bca99576aeb122cbb177d3958b37" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4399ff94af4b921f1a58c3b38085a15e15893a8b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d2e60351231c41bd58920417eca4f9f41bde916" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;expand(batch_shape, _instance=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/weibull.html#Weibull.expand&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="591bd8de53c612c29a7c192e4ad339b2b1cff596" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;exponent&lt;/code&gt; can be either a single &lt;code&gt;float&lt;/code&gt; number or a &lt;code&gt;Tensor&lt;/code&gt; with the same number of elements as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;exponent&lt;/code&gt; 는 단일 &lt;code&gt;float&lt;/code&gt; 수 또는 &lt;code&gt;input&lt;/code&gt; 과 동일한 수의 요소를 가진 &lt;code&gt;Tensor&lt;/code&gt; 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d88d551bc70e344bd7eef84d48d6a0ea0bfbe6c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;export_chrome_trace(path)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.export_chrome_trace&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;export_chrome_trace(path)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.export_chrome_trace&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f7637bfe2000b4628cf07a85dee5e8fa7bfbb459" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;extend(modules: Iterable[torch.nn.modules.module.Module]) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleList.extend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;extend(modules: Iterable[torch.nn.modules.module.Module]) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleList.extend&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2e030f014a3bc3a12bf7b5d5af4d0cdbf4f9a8f1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;extend(parameters: Iterable[Parameter]) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterList.extend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;extend(parameters: Iterable[Parameter]) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterList.extend&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0d3ba12245d678b14145b0a9c95dd2007964e51a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;extra_repr() &amp;rarr; str&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.extra_repr&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;extra_repr() &amp;rarr; str&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.extra_repr&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9503c815d2a6f438c1147b7bf828de6af0a524b5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;fake_quant_enable&lt;/code&gt; controls the application of fake quantization on tensors, note that statistics can still be updated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0afa81ecb8aef6cf93e1323e60555b6190e0e7ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;fast_forward(n)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.fast_forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;fast_forward(n)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.fast_forward&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="49face4c0c1c56a172c3a8d79ed0a21c22a9308c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;flatten_parameters() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/rnn.html#RNNBase.flatten_parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;flatten_parameters() &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/rnn.html#RNNBase.flatten_parameters&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="36c4a53abceb0006cfeccd0e45010b5d86dbe3eb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;float() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.float&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;float() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.float&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aa2ccb377503a34d4006ee9903f750d08d014e21" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;flush()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.flush&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;flush()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.flush&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="21116b2efde501f3227034950234c490fe2329c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#MultiheadAttention.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/activation.html#MultiheadAttention.forward&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="56532c888595a3d47b1558e64c103189b14fb3fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(src: torch.Tensor, mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoder.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(src: torch.Tensor, mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoder.forward&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5aa364c191ab4f38238eeb406a28df7200a8811" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer.forward&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ecfb8d256338556ab17fa51ea983ebee43150a63" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(src: torch.Tensor, tgt: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(src: torch.Tensor, tgt: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer.forward&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="650dda48a58896e0f599f2700f397bc69290c791" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoder.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoder.forward&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="450583179b762af7739af1fbe5bd887349f9dc48" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer.forward&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="509708444f40370aa1c4a41a73e11e654ec5e8a7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;forward&lt;/code&gt; implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from &lt;code&gt;forward&lt;/code&gt; are compiled as they are seen by the compiler, so they do not need this decorator either.</source>
          <target state="translated">&lt;code&gt;forward&lt;/code&gt; 는 암시 적으로 진입 점으로 간주되므로이 데코레이터가 필요하지 않습니다. &lt;code&gt;forward&lt;/code&gt; 에서 호출 된 함수와 메서드 는 컴파일러에서 볼 수있는대로 컴파일되므로이 데코레이터도 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="04611ee35ecf4781b58af21dd93d45aa85b98599" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;freed&lt;/code&gt;: historical total decrease in this metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc9fc06e9b6705464d96e970e04ed0615db540a9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;generate_square_subsequent_mask(sz: int) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;generate_square_subsequent_mask(sz: int) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="61eff514ca8501f3952ee36a173d0eb2f35de0e4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;get_backoff_factor()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_backoff_factor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;get_backoff_factor()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_backoff_factor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3c02f8a983de1a6565028fdca48b84cb96d69ee7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;get_growth_factor()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_factor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;get_growth_factor()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_factor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f0d7d487a3a4bf86fd2d6121ed6e1dcded9fdb91" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;get_growth_interval()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_interval&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;get_growth_interval()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_interval&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a78bb11ccd40f7194049bec07478431e1acbcc45" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;get_lr()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CyclicLR.get_lr&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;get_lr()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CyclicLR.get_lr&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01a6c455f764b5a5c1a00735cf7a9dff1d01aa3e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;get_scale()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_scale&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;get_scale()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_scale&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e9f2dcf01e0ab3ba3fdc033845ee9f72b33da7c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;grad_outputs&lt;/code&gt; should be a sequence of length matching &lt;code&gt;output&lt;/code&gt; containing the &amp;ldquo;vector&amp;rdquo; in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn&amp;rsquo;t require_grad, then the gradient can be &lt;code&gt;None&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="268a7ea013b42508b52a391876bb93db0be23180" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;graph&lt;/code&gt; follows the same rules described in the &lt;a href=&quot;#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; section with regard to &lt;code&gt;forward&lt;/code&gt; method lookup.</source>
          <target state="translated">&lt;code&gt;graph&lt;/code&gt; 는 &lt;code&gt;forward&lt;/code&gt; 메서드 조회 와 관련하여 &lt;a href=&quot;#inspecting-code&quot;&gt;코드 검사&lt;/a&gt; 섹션에 설명 된 것과 동일한 규칙을 따릅니다 .</target>
        </trans-unit>
        <trans-unit id="a7aef717594616af9f0fa240e4a90be2021f1ca5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;grid&lt;/code&gt; specifies the sampling pixel locations normalized by the &lt;code&gt;input&lt;/code&gt; spatial dimensions. Therefore, it should have most values in the range of &lt;code&gt;[-1, 1]&lt;/code&gt;. For example, values &lt;code&gt;x = -1, y = -1&lt;/code&gt; is the left-top pixel of &lt;code&gt;input&lt;/code&gt;, and values &lt;code&gt;x = 1, y = 1&lt;/code&gt; is the right-bottom pixel of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 는 &lt;code&gt;input&lt;/code&gt; 공간 차원으로 정규화 된 샘플링 픽셀 위치를 지정 합니다. 따라서 &lt;code&gt;[-1, 1]&lt;/code&gt; 범위에 대부분의 값이 있어야합니다 . 예를 들어, 값 &lt;code&gt;x = -1, y = -1&lt;/code&gt; 은 &lt;code&gt;input&lt;/code&gt; 의 왼쪽 상단 픽셀 이고 값 &lt;code&gt;x = 1, y = 1&lt;/code&gt; 은 &lt;code&gt;input&lt;/code&gt; 의 오른쪽 하단 픽셀입니다 .</target>
        </trans-unit>
        <trans-unit id="25cbc5d98eb2d7a72f4001c53072cf2ca961dc5e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;groups&lt;/code&gt; controls the connections between inputs and outputs. &lt;code&gt;in_channels&lt;/code&gt; and &lt;code&gt;out_channels&lt;/code&gt; must both be divisible by &lt;code&gt;groups&lt;/code&gt;. For example,</source>
          <target state="translated">&lt;code&gt;groups&lt;/code&gt; 은 입력과 출력 간의 연결을 제어합니다. &lt;code&gt;in_channels&lt;/code&gt; 및 &lt;code&gt;out_channels&lt;/code&gt; 는 모두 &lt;code&gt;groups&lt;/code&gt; 으로 나눌 수 있어야합니다 . 예를 들면</target>
        </trans-unit>
        <trans-unit id="944088ce37dc7728fbcea7128a5fe98ccb0d84cc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;half() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.half&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;half() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.half&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd50a8d9f1942465c13220f4cfbe858a80dde24e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;head_bias&lt;/code&gt; if set to True, adds a bias term to the &amp;lsquo;head&amp;rsquo; of the adaptive softmax. See paper for details. Set to False in the official implementation.</source>
          <target state="translated">&lt;code&gt;head_bias&lt;/code&gt; 가 True로 설정된 경우 적응 형 소프트 맥스 의 '헤드'에 바이어스 항을 추가합니다. 자세한 내용은 논문을 참조하십시오. 공식 구현에서는 False로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="e05d774391afd070d30d18ee4718f6df2013be5a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;help(model.foo)&lt;/code&gt; to check what arguments &lt;code&gt;model.foo&lt;/code&gt; takes to run</source>
          <target state="translated">&lt;code&gt;help(model.foo)&lt;/code&gt; 를 사용하여 &lt;code&gt;model.foo&lt;/code&gt; 가 실행하는 데 필요한 인수 확인</target>
        </trans-unit>
        <trans-unit id="7d5691348a6cd5c61651a332c032a975d4a2353c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;hubconf.py&lt;/code&gt; can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).</source>
          <target state="translated">&lt;code&gt;hubconf.py&lt;/code&gt; 는 여러 진입 점 을 가질 수 있습니다. 각 진입 점은 Python 함수 (예 : 게시하려는 사전 학습 된 모델)로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="599f52f770ba4d999264f8c76536cfe05c95f48f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(prob)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(prob)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e3e0426410b3f19b7cd85bb0e7dbdda376668654" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(prob)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(prob)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0fcbc9680630983abd4fb503daaaceec8319b32" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d8a7db7e04a513d2a5e702f5251fef05ac144ddb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="014c26301f1fb755658d82be1efe192ed32d5f6c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1befafd4e8a05be894e1d65d218ba64d6e3231f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50e9b93e9a111f59df23c6786e667261363829e9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0d1feabd837f2c26032d680e2424f12888c8c24" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f5e26b70c9db34fa195dfb688c1796e346636f38" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7dd350cac5b3e3ad542c98c417bc40866c32ea1f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.icdf&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;icdf(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.icdf&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1bf09631777feaadd39c173536e0f7de5e493b5a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;id&lt;/code&gt;: the current worker id.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="135edd09ac9a700e90b9519f3120eadbe02a85b2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;indices&lt;/code&gt; does not necessarily contain the first occurrence of each median value found, unless it is unique. The exact implementation details are device-specific. Do not expect the same result when run on CPU and GPU in general. For the same reason do not expect the gradients to be deterministic.</source>
          <target state="translated">&lt;code&gt;indices&lt;/code&gt; 는 고유하지 않은 경우 발견 된 각 중앙값의 첫 번째 발생을 반드시 포함하지는 않습니다. 정확한 구현 세부 사항은 기기별로 다릅니다. 일반적으로 CPU 및 GPU에서 실행할 때 동일한 결과를 기대하지 마십시오. 같은 이유로 그라디언트가 결정적 일 것이라고 기대하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="47c0b1936561bebfdf2c27a4aebb459262781078" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;indices&lt;/code&gt;: the indices given out by &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;indices&lt;/code&gt; : &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; 가&lt;/a&gt; 제공하는 인덱스</target>
        </trans-unit>
        <trans-unit id="1b55d4a5b6659a707cec670a5f11f3cc05809fb5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;indices&lt;/code&gt;: the indices given out by &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;indices&lt;/code&gt; : &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; 에서&lt;/a&gt; 제공하는 인덱스</target>
        </trans-unit>
        <trans-unit id="72f1b66936fd249c91d98b54127ebbc5af7c8129" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;indices&lt;/code&gt;: the indices given out by &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;indices&lt;/code&gt; : &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; 에서&lt;/a&gt; 제공하는 인덱스</target>
        </trans-unit>
        <trans-unit id="d390bc728939ae8ba314dfae98925168ae061258" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; (LongTensor) and &lt;code&gt;offsets&lt;/code&gt; (LongTensor, optional)</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; (LongTensor) 및 &lt;code&gt;offsets&lt;/code&gt; (LongTensor, 선택 사항)</target>
        </trans-unit>
        <trans-unit id="4827c4911c594a0f1a6e1bb1ef5c2906b0d152dd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt; must be 3-D tensors each containing the same number of matrices.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;mat2&lt;/code&gt; 는 각각 동일한 수의 행렬을 포함하는 3 차원 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="0900edd2937eb580db79d84ca120284e507ce780" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must have the same size, and the size of their &lt;code&gt;dim&lt;/code&gt; dimension should be 3.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 및 &lt;code&gt;other&lt;/code&gt; 크기는 동일해야하며 &lt;code&gt;dim&lt;/code&gt; 차원 의 크기는 3이어야합니다.</target>
        </trans-unit>
        <trans-unit id="b7d52928106b3c9c845e8c2e5ea629b637a6a370" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; can be of size &lt;code&gt;T x B x *&lt;/code&gt; where &lt;code&gt;T&lt;/code&gt; is the length of the longest sequence (equal to &lt;code&gt;lengths[0]&lt;/code&gt;), &lt;code&gt;B&lt;/code&gt; is the batch size, and &lt;code&gt;*&lt;/code&gt; is any number of dimensions (including 0). If &lt;code&gt;batch_first&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;B x T x *&lt;/code&gt;&lt;code&gt;input&lt;/code&gt; is expected.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 크기는 &lt;code&gt;T x B x *&lt;/code&gt; 수 있습니다. 여기서 &lt;code&gt;T&lt;/code&gt; 는 가장 긴 시퀀스의 길이 ( &lt;code&gt;lengths[0]&lt;/code&gt; 동일 ), &lt;code&gt;B&lt;/code&gt; 는 배치 크기, &lt;code&gt;*&lt;/code&gt; 는 임의의 차원 수 (0 포함)입니다. 경우 &lt;code&gt;batch_first&lt;/code&gt; 가 있다 &lt;code&gt;True&lt;/code&gt; , &lt;code&gt;B x T x *&lt;/code&gt; &lt;code&gt;input&lt;/code&gt; 예상된다.</target>
        </trans-unit>
        <trans-unit id="bfe63fbbe6879747b2eb29e8b72663bd7305b22b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; has to be a Tensor of size either</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 은 텐서 크기 여야합니다.</target>
        </trans-unit>
        <trans-unit id="ca90a3064ce34d809a57a4fc73c60c7adbef692c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;. By the Hermitian property, the output will be real-valued.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 은 &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 의해 생성 된 푸리에 도메인에서 단측 에르 미트 신호로 해석됩니다 . Hermitian 속성에 따라 출력은 실수가됩니다.</target>
        </trans-unit>
        <trans-unit id="155f0bff7df1281a7e83404a373b643f04ea6470" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;. By the Hermitian property, the output will be real-valued.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 은 &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt; 의해 생성 된 푸리에 도메인에서 단측 Hermitian 신호로 해석됩니다 . Hermitian 속성에 따라 출력은 실수가됩니다.</target>
        </trans-unit>
        <trans-unit id="76b08fbdcc85a8c7320f148d075d1ff1f5c6e807" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt;. &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 값은 푸리에 도메인에서 해석되는 실수 값 신호 여야합니다. 실수 신호의 IFFT는 에르 미트 대칭, &lt;code&gt;X[i] = conj(X[-i])&lt;/code&gt; 입니다. &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; 는 Nyquist 주파수 아래의 양의 주파수 만 포함되는 단측 형식으로이를 나타냅니다. 전체 출력을 계산하려면 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="244e8c78a9978d106f2250c397ae1607a59bac06" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt; must be either a 1-D time sequence or a 2-D batch of time sequences.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 은 1-D 시간 시퀀스 또는 2-D 시간 시퀀스 배치 여야합니다.</target>
        </trans-unit>
        <trans-unit id="21818f6b59dbbe274d1fab1e5d3ee4c87e1c440a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;input&lt;/code&gt;: the input Tensor to invert</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; : 반전 할 입력 Tensor</target>
        </trans-unit>
        <trans-unit id="949122073d3e59e3101b066022830d21228a8421" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;insert(index: int, module: torch.nn.modules.module.Module) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleList.insert&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;insert(index: int, module: torch.nn.modules.module.Module) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleList.insert&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d3b0bdbf637a94ae86989a63bb1df9bcbc722433" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;iparams&lt;/code&gt;, &lt;code&gt;fparams&lt;/code&gt;, &lt;code&gt;bparams&lt;/code&gt; - dictionaries of integer, float, and boolean valued input parameters, respectively</source>
          <target state="translated">&lt;code&gt;iparams&lt;/code&gt; , &lt;code&gt;fparams&lt;/code&gt; , &lt;code&gt;bparams&lt;/code&gt; - 각각 정수, 부동 및 부울 값 입력 매개 변수의 사전</target>
        </trans-unit>
        <trans-unit id="19e38ca1a592610a9a55800c7457013bf5e80d4b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ipc_handle()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.ipc_handle&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;ipc_handle()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.ipc_handle&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ebd402b1ccbd1cbb64b5d469b63e7898838ba4f2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;is_completed()&lt;/code&gt; - returns True if the operation has finished</source>
          <target state="translated">&lt;code&gt;is_completed()&lt;/code&gt; -작업이 완료되면 True를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="ff5f50502b4cac6e5817c351a182027feaeba8da" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;is_enabled()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.is_enabled&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;is_enabled()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.is_enabled&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2e956db386d2c439d1e692d208d26cbcacaa3901" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;is_pinned()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.is_pinned&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;is_pinned()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.is_pinned&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f5e2fde9dcf3b6a015e131a279398a0632b08f05" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;is_shared()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.is_shared&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;is_shared()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.is_shared&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ded65301c623068f122fb3dd63a337adbdf21a0c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;istft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.istft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;istft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.istft&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c5fff10ae7567a58711342719a8a62447b33669" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;items() &amp;rarr; Iterable[Tuple[str, Parameter]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.items&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;items() &amp;rarr; Iterable[Tuple[str, Parameter]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.items&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0785772297be0750443bcea4d0cddf6ff41a13fa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;items() &amp;rarr; Iterable[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.items&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;items() &amp;rarr; Iterable[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.items&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bf535f83492a812e81f86f74acb94528e1219558" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ivars&lt;/code&gt;, &lt;code&gt;fvars&lt;/code&gt;, &lt;code&gt;bvars&lt;/code&gt;, &lt;code&gt;tvars&lt;/code&gt; - dictionaries of integer, float, boolean, and Tensor valued iteration variables, respectively.</source>
          <target state="translated">&lt;code&gt;ivars&lt;/code&gt; , &lt;code&gt;fvars&lt;/code&gt; , &lt;code&gt;bvars&lt;/code&gt; , &lt;code&gt;tvars&lt;/code&gt; -각각 integer, float, boolean 및 Tensor 값 반복 변수의 사전.</target>
        </trans-unit>
        <trans-unit id="efb566c0d8ffd8d73f5e1db96c7e80254a05404a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ivars[&amp;ldquo;istep&amp;rdquo;]&lt;/code&gt; - the current iteration step &lt;code&gt;X&lt;/code&gt; - the current approximation of eigenvectors &lt;code&gt;E&lt;/code&gt; - the current approximation of eigenvalues &lt;code&gt;R&lt;/code&gt; - the current residual &lt;code&gt;ivars[&amp;ldquo;converged_count&amp;rdquo;]&lt;/code&gt; - the current number of converged eigenpairs &lt;code&gt;tvars[&amp;ldquo;rerr&amp;rdquo;]&lt;/code&gt; - the current state of convergence criteria</source>
          <target state="translated">&lt;code&gt;ivars[&amp;ldquo;istep&amp;rdquo;]&lt;/code&gt; -현재 반복 단계 &lt;code&gt;X&lt;/code&gt; - 고유 벡터의 현재 근사 &lt;code&gt;E&lt;/code&gt; - 고유 값 &lt;code&gt;R&lt;/code&gt; 의 현재 근사 -현재 잔차 &lt;code&gt;ivars[&amp;ldquo;converged_count&amp;rdquo;]&lt;/code&gt; -현재 수렴 된 고유 &lt;code&gt;tvars[&amp;ldquo;rerr&amp;rdquo;]&lt;/code&gt; - 수렴 기준의 현재 상태</target>
        </trans-unit>
        <trans-unit id="1589dbbc5d0b51da73aea7e024a30bccecc8b125" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;join(divide_by_initial_world_size=True, enable=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.join&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;join(divide_by_initial_world_size=True, enable=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.join&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="04ea32a2df64d9a1624ce81a7f67e6655471b509" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;key_averages(group_by_input_shape=False, group_by_stack_n=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.key_averages&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4134774161d941d362aa189292262f09e4dcf485" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;keys() &amp;rarr; Iterable[str]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.keys&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;keys() &amp;rarr; Iterable[str]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.keys&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="494c441eb59413bb478b94caa6b09c20219c37f5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;keys() &amp;rarr; Iterable[str]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.keys&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;keys() &amp;rarr; Iterable[str]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.keys&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b3d385499901b3e62df82455878cc15e3ff51b7a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;large_pool&lt;/code&gt;: statistics for the large allocation pool (as of October 2019, for size &amp;gt;= 1MB allocations).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5bb6fe461fbb5d678b3e1207477b35f8a414493" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;len(dataloader)&lt;/code&gt; heuristic is based on the length of the sampler used. When &lt;code&gt;dataset&lt;/code&gt; is an &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt;, it instead returns an estimate based on &lt;code&gt;len(dataset) / batch_size&lt;/code&gt;, with proper rounding depending on &lt;code&gt;drop_last&lt;/code&gt;, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user &lt;code&gt;dataset&lt;/code&gt; code in correctly handling multi-process loading to avoid duplicate data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd2d9cfd4ddccc44e4db2a12013f92a8e88af4fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.load_state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4d219a4f1c48e42f92e17b885f9f8e13bcb00963" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#LambdaLR.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#LambdaLR.load_state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3631cf0150b034c88d74e1330f5861571e2bf967" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiplicativeLR.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiplicativeLR.load_state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c98bed20b9389c9592d5581eedf074a721d12ea5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.load_state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1b9c74bd1fba9cae2375c562757e49b9f3f24764" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.load_state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.load_state_dict&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="62f66bbcb0b08c54e10a39245830a0c3663b3ee0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;local_rank&lt;/code&gt; is NOT globally unique: it is only unique per process on a machine. Thus, don&amp;rsquo;t use it to decide if you should, e.g., write to a networked filesystem. See &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/12042&quot;&gt;https://github.com/pytorch/pytorch/issues/12042&lt;/a&gt; for an example of how things can go wrong if you don&amp;rsquo;t do this correctly.</source>
          <target state="translated">&lt;code&gt;local_rank&lt;/code&gt; 는 전역 적으로 고유하지 않습니다. 컴퓨터의 프로세스마다 고유합니다. 따라서 예를 들어 네트워크 파일 시스템에 기록해야하는지 여부를 결정하는 데 사용하지 마십시오. 올바르게 수행하지 않으면 상황이 어떻게 잘못 될 수 있는지에 대한 예는 &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/12042&quot;&gt;https://github.com/pytorch/pytorch/issues/12042&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="6b9b33cf0f08f87f931221eeea1d2d2fdecfd881" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_abs_det_jacobian(x, y)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transforms.html#Transform.log_abs_det_jacobian&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa7c545b613756907a429504c3115615d7789a1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="567e45dbeee56209e7ee217c64e9db81c0af6795" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="29aab90171002180969851cdf06f7b4dc8be68b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a09c3ed5a8cfda0420e618b0c888e9baac075fc9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c208f8a43fa1e1bbb204c90ce870cf2bfc3a7ce" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10b00753dd47046e078547ce9c193e85e7d6fa14" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5d352c0701eaa174fbbcf12c2c124fa628e177ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="45a896ce6a6937c9b6467df7cbef21d6dc75b754" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d84e667e13a3d81a00948ccbf469d7f40d095fc3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9ed1efc80b55bd57887b5fc14dacc9f8c6dfa3e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e632767dfa7e1bbb637400a3462eaadc586ce899" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ffc5a4062b17e77c39a1936807cba05c423c51f3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="83f24fe686e5ec0f30c23abfce724b968cfa3884" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8c91967fc7e7ba296650be26efe046ea7beda5a9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gumbel.html#Gumbel.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gumbel.html#Gumbel.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c42c192d5b6d07c4b2a565315364cea2f1a7a64e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b9ad5183b6cab4db9ddda2168a5d61910710d2c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/half_normal.html#HalfNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c774d910aafc2b88fff4d4b82e0b7958899df313" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cbeee8b3aed13721dc8203805fd71e432c66ff28" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d106d4c77115c583480439c1a14af9b619e83851" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2fa7c87f15f4c4c96ea35a9ca349e62c07153139" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multinomial.html#Multinomial.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multinomial.html#Multinomial.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="600899fa5e1c71010912c7c84c68f5748eb61fbd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cf27eaf73e2d8cde3106a7a70ac80f3d7a13c124" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87f5792dba81a776b95666976bcad55b9651a019" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93553041631d1588c561df0478ddf787c3609ba1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8c70759e717d64f9ab207799ae8148961237ba2e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/poisson.html#Poisson.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/poisson.html#Poisson.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="078f965bb9dfcb839ef7191d89e872d666516027" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fefa4c7d9c97fdee18a5f13aea99f7b14f929d36" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cdf74543a2968967136b4b5b75300111e40b7a31" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a066eef46f2c6e41cf0be998046e23785c6b3d6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="72fc79afbbd3a9d281e269509f7a3206460a5f8c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="36be6cf3b5b8f7b83f7c0b2b5168dd86aaf869ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;log_prob(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.log_prob&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;log_prob(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.log_prob&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="74e3c84ad2f93ee92b9072d8933cff33835327ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ed4ab73147857b4349c39723b2fbf11dd4484af9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b92ccca51ff111a17b5e89e8c12d69dd24500616" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a7eab7cd7d33f0a39c35b5c6378f26090b62b56" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="05196aee477f055854acc1110b7cd38f1b5e8aab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="38320012aa157a532a79542f82b9c52e767f7162" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="35033a53454388776aca196c1744482ab41c92f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;logits&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc5a71e50b3f16dc1145f31275ba17755e3208f3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;lu(pivot=True, get_infos=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.lu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;lu(pivot=True, get_infos=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.lu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e4bd810a566635fe49df3e4c077ffd75aea1f0f7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;mark_dirty(*args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_dirty&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;mark_dirty(*args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_dirty&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="63200580f2d84f14369aa54c260833514c7399e2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;mark_non_differentiable(*args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_non_differentiable&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;mark_non_differentiable(*args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_non_differentiable&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd04b20a72f6e3eefecb7fc21edef53df71f8ef7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;matmul(A, V[:, :k])&lt;/code&gt; projects data to the first k principal components</source>
          <target state="translated">&lt;code&gt;matmul(A, V[:, :k])&lt;/code&gt; 는 처음 k 개의 주성분에 데이터를 투영합니다.</target>
        </trans-unit>
        <trans-unit id="7139ac5f6d0a956209f36cd92ff11f349707f99e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;method=&amp;rdquo;basic&amp;rdquo;&lt;/code&gt; - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input.</source>
          <target state="translated">&lt;code&gt;method=&amp;rdquo;basic&amp;rdquo;&lt;/code&gt; -Andrew Knyazev가 도입 한 LOBPCG 방법, [Knyazev2001] 참조. 덜 강력한 방법은 Cholesky가 단일 입력에 적용될 때 실패 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5cc2c230f41d50cec42c5a73738ea776357df1f2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt; - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method.</source>
          <target state="translated">&lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt; -직교 기반 선택이있는 LOBPCG 방법 [StathopoulosEtal2002]. 강력한 방법.</target>
        </trans-unit>
        <trans-unit id="31632cd895f74d5feae903615972b9be78a76f37" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;modules() &amp;rarr; Iterator[torch.nn.modules.module.Module]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.modules&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;modules() &amp;rarr; Iterator[torch.nn.modules.module.Module]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.modules&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="32e35227c212f090980012e3160ed30e44efafd0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_buffers(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_buffers&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_buffers(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_buffers&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="048db148a1be9becc9e39073dbeb4b604c390c68" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_children() &amp;rarr; Iterator[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_children&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_children() &amp;rarr; Iterator[Tuple[str, torch.nn.modules.module.Module]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_children&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85bacda5665b89ed5cefaace04dda54af89f48fe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_modules&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_modules&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22a7ee3e0da6af405272a0ed0646ab619f9fb34b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;named_parameters(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;named_parameters(prefix: str = '', recurse: bool = True) &amp;rarr; Iterator[Tuple[str, torch.Tensor]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.named_parameters&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dfec0664bec327f4859fdc164e8ab345ca09035a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;names[idx]&lt;/code&gt; corresponds to the name of tensor dimension &lt;code&gt;idx&lt;/code&gt;. Names are either a string if the dimension is named or &lt;code&gt;None&lt;/code&gt; if the dimension is unnamed.</source>
          <target state="translated">&lt;code&gt;names[idx]&lt;/code&gt; 는 텐서 차원 &lt;code&gt;idx&lt;/code&gt; 의 이름에 해당합니다 . 이름은 차원이 명명 된 경우 문자열이거나 차원이 명명되지 않은 경우 &lt;code&gt;None&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="64961805f0c40621fceb6e60083709ff6fd8c3f7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;nccl&lt;/code&gt; backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.</source>
          <target state="translated">&lt;code&gt;nccl&lt;/code&gt; 백엔드는 현재 GPU를 사용할 때 가장 빠르고 강력하게 권장되는 백엔드입니다. 이는 단일 노드 및 다중 노드 분산 교육 모두에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="b98aabe4badf856a18d383aa4bbcf3819012f841" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;no_sync()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;no_sync()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="243529964a8f02b21c1cfb5a2cf1416a48cdb560" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;norm(p='fro', dim=None, keepdim=False, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;norm(p='fro', dim=None, keepdim=False, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.norm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8efce297d571813505bf816440c329931f4f1ff4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;num_workers&lt;/code&gt;: the total number of workers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40d3dcf6b89879a6aa49c649f189043c87bbd1fd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;observer_enable&lt;/code&gt; controls statistics collection on tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67fdf5912494b352e2db89b729e0ae31d14aa6b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;optimize_for_mobile&lt;/code&gt; will also invoke freeze_module pass which only preserves &lt;code&gt;forward&lt;/code&gt; method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.</source>
          <target state="translated">&lt;code&gt;optimize_for_mobile&lt;/code&gt; 은 또한 &lt;code&gt;forward&lt;/code&gt; 메소드 만 보존하는 freeze_module 패스를 호출 합니다. 보존해야하는 다른 메서드가있는 경우 보존 된 메서드 목록에 추가하고 메서드에 전달합니다.</target>
        </trans-unit>
        <trans-unit id="6860a3cc1bb85eed802ff8a945080f941221eb86" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;out&lt;/code&gt; can have integral &lt;code&gt;dtype&lt;/code&gt;, but &lt;code&gt;input&lt;/code&gt; must have floating point &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;out&lt;/code&gt; 은 정수 &lt;code&gt;dtype&lt;/code&gt; 을 가질 수 있지만 &lt;code&gt;input&lt;/code&gt; 에는 부동 소수점 &lt;code&gt;dtype&lt;/code&gt; 이 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="57df9e7349923cce6d85797c07fe7176a04c4730" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output&lt;/code&gt;: aggregated embedding values of shape &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;output&lt;/code&gt; : 모양의 집계 된 임베딩 값 &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="20bc1b9a285a7d6c1e598719cab22ec2e30a86ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_padding&lt;/code&gt; controls the additional size added to one side of the output shape. See note below for details.</source>
          <target state="translated">&lt;code&gt;output_padding&lt;/code&gt; 은 출력 모양의 한쪽에 추가되는 추가 크기를 제어합니다. 자세한 내용은 아래 참고를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c43d9da08c42122b25a623aed7e1ec7d4a1ed5e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_size&lt;/code&gt; (optional): the targeted output size</source>
          <target state="translated">&lt;code&gt;output_size&lt;/code&gt; (선택 사항) : 대상 출력 크기</target>
        </trans-unit>
        <trans-unit id="2f9c7a3c6933d1325ba659f80f703c1d21ba8d09" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;output_size&lt;/code&gt; describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with &lt;code&gt;stride &amp;gt; 0&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_size&lt;/code&gt; 는 슬라이딩 로컬 블록의 텐서를 포함하는 대형의 공간 모양을 설명합니다. 여러 입력 모양이 동일한 수의 슬라이딩 블록에 매핑되는 경우 (예 : &lt;code&gt;stride &amp;gt; 0&lt;/code&gt; 모호성을 해결하는 것이 유용합니다 .</target>
        </trans-unit>
        <trans-unit id="d1243c702a5a6e39b0598b1177c888c80420b192" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;p_tensor&lt;/code&gt; should be a tensor containing probabilities to be used for drawing the binary random number.</source>
          <target state="translated">&lt;code&gt;p_tensor&lt;/code&gt; 는 이진 난수를 그리는 데 사용할 확률을 포함하는 텐서 여야합니다.</target>
        </trans-unit>
        <trans-unit id="71d214acd804f5a2164bbac6db12b07145c9f82a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pad_mode&lt;/code&gt; determines the padding method used on &lt;code&gt;input&lt;/code&gt; when &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. See &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt; for all available options. Default is &lt;code&gt;&quot;reflect&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;pad_mode&lt;/code&gt; 는 &lt;code&gt;center&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; 일 때 &lt;code&gt;input&lt;/code&gt; 사용되는 패딩 방법을 결정합니다 . 사용 가능한 모든 옵션 은 &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt; &lt;code&gt;torch.nn.functional.pad()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 . 기본값은 &lt;code&gt;&quot;reflect&quot;&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="98da630d9b8562a9fa983197c13c21292635e0d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pad_sequence&lt;/code&gt; stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size &lt;code&gt;L x *&lt;/code&gt; and if batch_first is False, and &lt;code&gt;T x B x *&lt;/code&gt; otherwise.</source>
          <target state="translated">&lt;code&gt;pad_sequence&lt;/code&gt; 는 새로운 차원을 따라 Tensor 목록을 쌓고 동일한 길이로 채 웁니다. 예를 들어 입력이 &lt;code&gt;L x *&lt;/code&gt; 크기의 시퀀스 목록이고 batch_first가 False 인 경우, 그렇지 않으면 &lt;code&gt;T x B x *&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="5523aea5f1d24240fad2b5cb687abb71fdd8696c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; number of points. See note below for details.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 은 &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; number of points 에 대해 양쪽의 암시 적 제로 채우기 양을 제어합니다 . 자세한 내용은 아래 참고를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="0965d5762b82b5756e9509e59423399f67ccc74c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points for each dimension before reshaping.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 은 모양을 변경하기 전에 각 차원에 대한 &lt;code&gt;padding&lt;/code&gt; 포인트 수 에 대해 양쪽의 암시 적 제로 패딩 의 양을 제어합니다 .</target>
        </trans-unit>
        <trans-unit id="87faaf2c7b8606fb40058aa40aed778ec31e990d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points for each dimension.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 은 각 차원에 대한 점의 &lt;code&gt;padding&lt;/code&gt; 수 에 대해 양쪽에서 암시 적 제로 패딩 의 양을 제어 합니다.</target>
        </trans-unit>
        <trans-unit id="873aa63cc9ed82344812e9f5d81f5ab1f6110075" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding&lt;/code&gt; controls the amount of implicit zero-paddings on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">&lt;code&gt;padding&lt;/code&gt; 은 &lt;code&gt;padding&lt;/code&gt; 포인트 수 에 대해 양쪽에서 암시 적 제로 패딩 의 양을 제어합니다 .</target>
        </trans-unit>
        <trans-unit id="e777921c9b14589989930ecd3404d8f890efee0e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;border&quot;&lt;/code&gt;: use border values for out-of-bound grid locations,</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;border&quot;&lt;/code&gt; : 경계를 벗어난 그리드 위치에 테두리 값을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="9bf609a621bd6df6b08d69937c59fb9bda6a4e18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;reflection&quot;&lt;/code&gt;: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location &lt;code&gt;x = -3.5&lt;/code&gt; reflects by border &lt;code&gt;-1&lt;/code&gt; and becomes &lt;code&gt;x' = 1.5&lt;/code&gt;, then reflects by border &lt;code&gt;1&lt;/code&gt; and becomes &lt;code&gt;x'' = -0.5&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;reflection&quot;&lt;/code&gt; : 경계를 벗어난 그리드 위치에 대해 경계에 반사 된 위치의 값을 사용합니다. 멀리 경계에서 위치는 바인드으로되고, 예를 들면, (정규화) 픽셀 위치까지 반사되는 유지할 &lt;code&gt;x = -3.5&lt;/code&gt; 테두리로 반영 &lt;code&gt;-1&lt;/code&gt; 및해진다 &lt;code&gt;x' = 1.5&lt;/code&gt; , 다음 경계가 반영 &lt;code&gt;1&lt;/code&gt; 및해진다 &lt;code&gt;x'' = -0.5&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="65643d381010e2d95d811ab0babc2d8ceb40b002" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;padding_mode=&quot;zeros&quot;&lt;/code&gt;: use &lt;code&gt;0&lt;/code&gt; for out-of-bound grid locations,</source>
          <target state="translated">&lt;code&gt;padding_mode=&quot;zeros&quot;&lt;/code&gt; : 경계를 벗어난 그리드 위치에 &lt;code&gt;0&lt;/code&gt; 을 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="2fd5be269042a70bd6bfcef4a1beb1fad3f8b126" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;parameters(recurse: bool = True) &amp;rarr; Iterator[torch.nn.parameter.Parameter]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;parameters(recurse: bool = True) &amp;rarr; Iterator[torch.nn.parameter.Parameter]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.parameters&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="46667573594b78772057f040483ee64dbd622e7f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;peak&lt;/code&gt;: maximum value of this metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="389b3925161df2a316dfd68b4bd9f091bc6d9283" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;per_index_weights&lt;/code&gt; (Tensor, optional)</source>
          <target state="translated">&lt;code&gt;per_index_weights&lt;/code&gt; (텐서, 선택 사항)</target>
        </trans-unit>
        <trans-unit id="a3171c9a0247049dc9a6cc2032a93513a93d4b9e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;per_sample_weights&lt;/code&gt; (Tensor, optional). Has the same shape as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;per_sample_weights&lt;/code&gt; (텐서, 선택 사항). &lt;code&gt;input&lt;/code&gt; 과 모양이 같습니다 .</target>
        </trans-unit>
        <trans-unit id="8cc0e49ffec02bd4a765dfe5096e75bedfd9db2d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;perplexity()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.perplexity&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;perplexity()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.perplexity&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e9bcadfa3ee1d7befc56f0936be4d842be3c16e4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pop(key: str) &amp;rarr; Parameter&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.pop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;pop(key: str) &amp;rarr; Parameter&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.pop&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d3bcc0091bf56d060c58f75cf9a87f90235fe42" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pop(key: str) &amp;rarr; torch.nn.modules.module.Module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.pop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;pop(key: str) &amp;rarr; torch.nn.modules.module.Module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.pop&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="24a98184245660e3a7ac51bface7161b6a7b3f8d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;precision_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.precision_matrix&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;precision_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.precision_matrix&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="989e92f4ee5ff560ae5bc4dfe779cf83890dac37" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;precision_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.precision_matrix&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;precision_matrix&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.precision_matrix&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cecc34a8f924fcf63ac4e86b4df399986ca30840" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;predict(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;predict(input: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="56b86f5d799e6640037664a5f1831fa071f4078f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c3493bc83d79bd77fd39f0f9447b71b406fae865" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85cf6eaf45c34228dbd1611b9ab5a873d79740ee" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="090a933a61383603b3d118bbdf4aa0ad817f0a6b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1c946918f94982d7f055dbfb961f23eca91330e6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b2cc360eac6f0f743c468d0ff66f74af414c8ac2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ea70d6d12b038a8bb8b1bae00e34f071bb6cfa2e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.probs&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;probs&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.probs&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3feeb93547a40ff63ef93c1edbe8bdb7276b339e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;prune(t, default_mask=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.prune&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;prune(t, default_mask=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.prune&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="565d205302d70b13671b27025f1c8e59b0062929" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;quant_max&lt;/code&gt; specifies the maximum allowable quantized value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f5a6cfa346a0c7b94a1e9904b1ed88de44fdf1d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;quant_min&lt;/code&gt; specifies the minimum allowable quantized value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="913780f21801b864cc80a11aca24240f818ca455" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;query()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.query&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;query()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.query&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="42956ada5f259f84a85715ab51e1361570f0ed7d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;query()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.query&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;query()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.query&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="677b9d55d28277e15c8895d04bae009e202efa76" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;record(stream=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.record&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;record(stream=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.record&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d09844fee15def183803f8865747640caa36402b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;record_event(event=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.record_event&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;record_event(event=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.record_event&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0006fd153e3e41368ff174db197035583852421" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'mean'&lt;/code&gt; doesn&amp;rsquo;t return the true kl divergence value, please use &lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'batchmean'&lt;/code&gt; which aligns with KL math definition. In the next major release, &lt;code&gt;'mean'&lt;/code&gt; will be changed to be the same as &lt;code&gt;'batchmean'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'mean'&lt;/code&gt; 은 실제 kl 발산 값을 반환하지 않습니다 . KL 수학 정의와 일치 하는 &lt;code&gt;reduction&lt;/code&gt; = &lt;code&gt;'batchmean'&lt;/code&gt; 을 사용하십시오 . 다음 주요 릴리스에서는 &lt;code&gt;'mean'&lt;/code&gt; 이 &lt;code&gt;'batchmean'&lt;/code&gt; 과 동일하게 변경됩니다 .</target>
        </trans-unit>
        <trans-unit id="9c40fd54ed9b4a947e07d2d97749d3a5a6960fd4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;refine_names(*names)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.refine_names&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;refine_names(*names)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.refine_names&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="52d0cb378cea080d9e860cce3397062e4eb8cf9d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register(constraint, factory=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/constraint_registry.html#ConstraintRegistry.register&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a26fb9fd98b2c03b3515681e4d92a08a2f7d05f5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_backward_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_backward_hook&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c3fc6039af03180c3ab53c4d591ca38e5fab4f39" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_buffer&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_buffer&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cb44a0e61508db210c21280300a86c46e5d6bdf7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_forward_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_forward_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_hook&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="158c35384b89f58225b64148f33d94a37ef4fc57" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_forward_pre_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_forward_pre_hook(hook: Callable[..., None]) &amp;rarr; torch.utils.hooks.RemovableHandle&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="936eaf9b14caaa34ea920fae06e38528e4361e53" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_hook(hook)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.register_hook&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_hook(hook)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.register_hook&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5292ac761c061e747a376f42b7b666c69eaa24fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_parameter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.register_parameter&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b1596aad33a69c18bddd2a50faac7da7dfe74e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;remove(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.remove&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;remove(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#BasePruningMethod.remove&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a7c7a4c7bb3781e3a5ff82b15f960d784474e526" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rename(*names, **rename_map)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rename(*names, **rename_map)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3f56c6acc1a2fd9e54a77cd2f6182d7653e821c2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rename_(*names, **rename_map)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rename_(*names, **rename_map)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.rename_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b78f9895d15f5a907942047841acbda1473e2a92" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;requires_grad_(requires_grad: bool = True) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.requires_grad_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;requires_grad_(requires_grad: bool = True) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.requires_grad_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2a6e3641f634a4bff93ea55f39b2b25ad71a52c4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;reset()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.reset&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;reset()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quasirandom.html#SobolEngine.reset&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5491ec86d7e376ac6bea33efc116ab49c35fa70" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;retain_grad()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.retain_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;retain_grad()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.retain_grad&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="090cf3b46507b91da4e8beba01291b289e3e77dd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=())&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=())&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/beta.html#Beta.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2bd5776c0f1632f5db6733aa06453abe06625a2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=())&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=())&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/dirichlet.html#Dirichlet.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bbb76c48567d401540f815e59f54288b26c7afa0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/cauchy.html#Cauchy.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89a3dea0d178cea266882fd923bac68e6afb1730" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="132754a1fbbbe4c56d128d8552cfb597c9123241" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd3aa9735bde87c275aed6b2aa1b6d50488b64bb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/exponential.html#Exponential.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="697d0d3bb0fa76833221e0774b32a8ed16688755" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4c23b1591f7fbc082cf26e66a5647f9b651166a9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/gamma.html#Gamma.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="83ce140fca777035133f8b5378fe71de2725aa7f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="23a657c1505ab22ee7e0cc090c4f0a180b863d54" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/laplace.html#Laplace.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cc5cd46483652be70603ff895d52c554c07edcaf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="97484935d64b162778cb07ed028512a36236bd49" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c7f70a6711453d72e3d21064584eccafcdc5e659" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5cd5b9bf329971a522daad7830d77c7207d51b1f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8d36a2ce3097390cb9f822946242f88496c2dd76" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/studentT.html#StudentT.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a9612a6da3225cd6444ea24794533ce3f88f5c7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f67c6ad8c8042ca3dd310f43e597f9aa98345e98" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.rsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;rsample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/uniform.html#Uniform.rsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="515b8b74876ff2823f83d55893d60380babcb7d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/bernoulli.html#Bernoulli.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="df98dc2d9505c719550f7d13ca1a2d056d0d6964" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/binomial.html#Binomial.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c8520e389ee59fc938f5fcb410180c10908a16b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/categorical.html#Categorical.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e4d5577d4bf3c8aea9bea26b2e1775c8298c9ccc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="48de345ca840eea9a983158ce7206f1344f5f267" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0bf26f9af0906467bbf4906e07e39c3ccae85bb9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/geometric.html#Geometric.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2eed4a58dd6a0f28dccdcea96b25e79364a97d35" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/independent.html#Independent.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="92ce03882dab5d21ae6af8442e52dedc0af5feba" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="736cf98f68599eddb363fbbc40332120748de4f9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multinomial.html#Multinomial.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multinomial.html#Multinomial.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a49feda6159263660fa9ba918f66aaec4e29c635" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e419575223457c8c632a0ffb8e8b48b90457690c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/normal.html#Normal.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="579393b908fbc53d68af044d7e93870f8917e55c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8cfa9cc19c9366c3e03a4bcb6448bfa791c98e3c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/poisson.html#Poisson.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/poisson.html#Poisson.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5eba11ebf71f0246d99455aa55cd41cc0b9be338" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="697525e8de44fdbb2ae54546d6a6360ac2936b47" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample(sample_shape=torch.Size([]))&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0eab96348a16ecc7bf300bb98ee62750899bfb34" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sample_n(n)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.sample_n&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;sample_n(n)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/distribution.html#Distribution.sample_n&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="53d4959fc129c9696a0a22e441012eb62210b274" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;save_for_backward(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.save_for_backward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;save_for_backward(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.save_for_backward&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3955668b3d5b5a4776b67a61b6a0a284a075efd4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;scale(outputs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.scale&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;scale(outputs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.scale&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bebcdb093236ce63461a206c705c66610a3b67c0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;scale&lt;/code&gt; defines the scale factor used for quantization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7865fa7bda73042db81ac481b846f9f7496c8d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;scale_tril&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.scale_tril&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;scale_tril&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.scale_tril&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6273e69620116b2f163652f25b308282602d6381" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;scale_tril&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.scale_tril&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;scale_tril&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.scale_tril&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a92e2c125ea5ab0f087542c4a55b4a17e469b42a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;seed&lt;/code&gt;: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s documentation for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30afcf2664bfd55bdc73c874505670a40ceba5c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.bfloat16()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.bfloat16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.bfloat16()&lt;/code&gt; 은 &lt;code&gt;self.to(torch.bfloat16)&lt;/code&gt; 과 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ba8b07afb3887e1378823dcb6c0026111669eec7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.bool()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.bool)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.bool()&lt;/code&gt; 은 &lt;code&gt;self.to(torch.bool)&lt;/code&gt; 과 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="235e528e147f1d69ea9f027ab1d5f71c3e2b6504" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.byte()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.uint8)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.byte()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.uint8)&lt;/code&gt; 와 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="696fbff031feb55dfc6a3f759d73b60530f439b6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.char()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int8)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.char()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.int8)&lt;/code&gt; 와 동일합니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="dd991c33642a908ee15c44c4567ca13834b6d0e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.double()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float64)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.double()&lt;/code&gt; 은 &lt;code&gt;self.to(torch.float64)&lt;/code&gt; 와 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="bb530f2cded8fb18cd0f6bd9e52d044bc47fa9cf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.float()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float32)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.float()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.float32)&lt;/code&gt; 와 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="c39462395fa123a645a00da7f7061280a0d3b20a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.half()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.float16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.half()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.float16)&lt;/code&gt; 과 동일합니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ccde90fcc390e97ee3b4f21b89a8c42e1d03051e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.int()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int32)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.int()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.int32)&lt;/code&gt; 와 동일합니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5058cc3abe53f662df7631dd5211dc4a50368575" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.long()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int64)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.long()&lt;/code&gt; 은 &lt;code&gt;self.to(torch.int64)&lt;/code&gt; 와 같습니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2b88b4ac2f04fe1799583ed6e4d6bfc76ea2f417" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.rename(**rename_map)&lt;/code&gt; returns a view on tensor that has dims renamed as specified in the mapping &lt;code&gt;rename_map&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self.rename(**rename_map)&lt;/code&gt; 은 &lt;code&gt;rename_map&lt;/code&gt; 매핑에 지정된대로 이름이 변경된 dims가있는 텐서의 뷰를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="6ee77ff93d503175bd69ae14f8b2767c810fb5f1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.rename(*names)&lt;/code&gt; returns a view on tensor, renaming all dimensions positionally using &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. Use &lt;code&gt;self.rename(None)&lt;/code&gt; to drop names on a tensor.</source>
          <target state="translated">&lt;code&gt;self.rename(*names)&lt;/code&gt; 은 텐서에 대한 뷰를 반환하고 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; 를&lt;/a&gt; 사용하여 위치 적으로 모든 차원의 이름을 바꿉니다 . 텐서에 이름을 삭제 하려면 &lt;code&gt;self.rename(None)&lt;/code&gt; 을 사용 하세요 .</target>
        </trans-unit>
        <trans-unit id="11363fccfc09fecbbf6487dc95db457ac7c9d1bb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.short()&lt;/code&gt; is equivalent to &lt;code&gt;self.to(torch.int16)&lt;/code&gt;. See &lt;a href=&quot;#torch.Tensor.to&quot;&gt;&lt;code&gt;to()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self.short()&lt;/code&gt; 는 &lt;code&gt;self.to(torch.int16)&lt;/code&gt; 과 동일합니다 . &lt;a href=&quot;#torch.Tensor.to&quot;&gt; &lt;code&gt;to()&lt;/code&gt; &lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="745574568faaeba1d561f1e5ae137746578c731c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self.where(condition, y)&lt;/code&gt; is equivalent to &lt;code&gt;torch.where(condition, self, y)&lt;/code&gt;. See &lt;a href=&quot;generated/torch.where#torch.where&quot;&gt;&lt;code&gt;torch.where()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;self.where(condition, y)&lt;/code&gt; 는 &lt;code&gt;torch.where(condition, self, y)&lt;/code&gt; . &lt;a href=&quot;generated/torch.where#torch.where&quot;&gt; &lt;code&gt;torch.where()&lt;/code&gt; &lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="80d12cdf60b2ebf7897bf33eeda8316c6146ee5e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt; can have integral &lt;code&gt;dtype&lt;/code&gt;, but &lt;code&gt;p_tensor&lt;/code&gt; must have floating point &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 는 정수 &lt;code&gt;dtype&lt;/code&gt; 을 가질 수 있지만 &lt;code&gt;p_tensor&lt;/code&gt; 는 부동 소수점 &lt;code&gt;dtype&lt;/code&gt; 을 가져야 합니다.</target>
        </trans-unit>
        <trans-unit id="04afe95e87078af73461e45f69231c868514d31c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt; is a scalar &lt;code&gt;float&lt;/code&gt; value, and &lt;code&gt;exponent&lt;/code&gt; is a tensor. The returned tensor &lt;code&gt;out&lt;/code&gt; is of the same shape as &lt;code&gt;exponent&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 는 스칼라 &lt;code&gt;float&lt;/code&gt; 값이고 &lt;code&gt;exponent&lt;/code&gt; 는 텐서입니다. 반환 된 텐서 &lt;code&gt;out&lt;/code&gt; 은 &lt;code&gt;exponent&lt;/code&gt; 와 같은 모양입니다.</target>
        </trans-unit>
        <trans-unit id="9f8bc9073599a1c05ac8a8256c20ef81117a207c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;src&lt;/code&gt; (if it is a Tensor) should have same number of dimensions. It is also required that &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d&lt;/code&gt;, and that &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d != dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; , &lt;code&gt;index&lt;/code&gt; 및 &lt;code&gt;src&lt;/code&gt; (텐서 인 경우)는 동일한 수의 차원을 가져야합니다. 또한 필요가있다 &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; 모든 치수는 &lt;code&gt;d&lt;/code&gt; , 그 &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; 모든 치수는 &lt;code&gt;d != dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="84785e3dd4238879d9071b5f542dcf76287afa5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;self&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;src&lt;/code&gt; should have same number of dimensions. It is also required that &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d&lt;/code&gt;, and that &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; for all dimensions &lt;code&gt;d != dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; , &lt;code&gt;index&lt;/code&gt; 및 &lt;code&gt;src&lt;/code&gt; 는 동일한 수의 차원을 가져야합니다. 또한 필요가있다 &lt;code&gt;index.size(d) &amp;lt;= src.size(d)&lt;/code&gt; 모든 치수는 &lt;code&gt;d&lt;/code&gt; , 그 &lt;code&gt;index.size(d) &amp;lt;= self.size(d)&lt;/code&gt; 모든 치수는 &lt;code&gt;d != dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b8db8aa8a39114a3bb4d5fd0455bd46f531532d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sequences&lt;/code&gt; should be a list of Tensors of size &lt;code&gt;L x *&lt;/code&gt;, where &lt;code&gt;L&lt;/code&gt; is the length of a sequence and &lt;code&gt;*&lt;/code&gt; is any number of trailing dimensions, including zero.</source>
          <target state="translated">&lt;code&gt;sequences&lt;/code&gt; 는 &lt;code&gt;L x *&lt;/code&gt; 크기의 텐서 목록이어야합니다. 여기서 &lt;code&gt;L&lt;/code&gt; 은 시퀀스의 길이이고 &lt;code&gt;*&lt;/code&gt; 는 0을 포함한 후행 차원의 수입니다.</target>
        </trans-unit>
        <trans-unit id="16dd385013823f6fabf7311f8a1894044e454670" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_backoff_factor(new_factor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_backoff_factor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_backoff_factor(new_factor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_backoff_factor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="54e9c47c1dc3e3612df02d2e3ac4bfca8491aee9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_detect_anomaly&lt;/code&gt; will enable or disable the autograd anomaly detection based on its argument &lt;code&gt;mode&lt;/code&gt;. It can be used as a context-manager or as a function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="735447a30cd08376a5904648fe0520b6748f549e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_grad_enabled&lt;/code&gt; will enable or disable grads based on its argument &lt;a href=&quot;torch.mode#torch.mode&quot;&gt;&lt;code&gt;mode&lt;/code&gt;&lt;/a&gt;. It can be used as a context-manager or as a function.</source>
          <target state="translated">&lt;code&gt;set_grad_enabled&lt;/code&gt; 는 인수 &lt;a href=&quot;torch.mode#torch.mode&quot;&gt; &lt;code&gt;mode&lt;/code&gt; &lt;/a&gt; 에 따라 등급 을 활성화 또는 비활성화 합니다 . 컨텍스트 관리자 또는 기능으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2abe5c576ad24bbd3c258956541824a45b25956f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_grad_enabled&lt;/code&gt; will enable or disable grads based on its argument &lt;code&gt;mode&lt;/code&gt;. It can be used as a context-manager or as a function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0532c085f80cb942390aa7cadb9f2ab051e4911" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_growth_factor(new_factor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_factor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_growth_factor(new_factor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_factor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c416378e8a3a45e671cdb91b6795a74800afe930" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_growth_interval(new_interval)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_interval&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_growth_interval(new_interval)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_interval&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8d4cd85f730007e10d76c91dcccb29afa5e7c3c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_materialize_grads(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.set_materialize_grads&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_materialize_grads(value)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#_ContextMethodMixin.set_materialize_grads&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7a841331f9feb2d5c5159d5c906fc881114af61e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;set_result(result: T) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.set_result&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;set_result(result: T) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.set_result&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0fd6f8837947d9dfd2011aecf91b34593d64433" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;share_memory_()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.share_memory_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;share_memory_()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.share_memory_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5ada337e154f8b6156cdfc96e0db2f536f0893f6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;size&lt;/code&gt; is the number of elements in the storage. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the file must contain at least &lt;code&gt;size * sizeof(Type)&lt;/code&gt; bytes (&lt;code&gt;Type&lt;/code&gt; is the type of storage). If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; the file will be created if needed.</source>
          <target state="translated">&lt;code&gt;size&lt;/code&gt; 는 저장소의 요소 수입니다. 경우 &lt;code&gt;shared&lt;/code&gt; 있습니다 &lt;code&gt;False&lt;/code&gt; , 파일은 적어도 있어야합니다 &lt;code&gt;size * sizeof(Type)&lt;/code&gt; (바이트 &lt;code&gt;Type&lt;/code&gt; 의 스토리지 유형입니다). 경우 &lt;code&gt;shared&lt;/code&gt; 인 &lt;code&gt;True&lt;/code&gt; 필요한 경우 파일이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="775a9cd83a519b88484214ea6efd0a87345ed211" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;size_average&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; are in the process of being deprecated, and in the meantime, specifying either of those two args will override &lt;code&gt;reduction&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;size_average&lt;/code&gt; 및 &lt;code&gt;reduce&lt;/code&gt; 는 더 이상 사용되지 않으며 그 동안 두 인수 중 하나를 지정하면 &lt;code&gt;reduction&lt;/code&gt; 을 재정의 합니다.</target>
        </trans-unit>
        <trans-unit id="968d0dce1541ed993d4b1f29fd9e1515b8a4c0e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;sizes&lt;/code&gt; is the new shape of the unflattened dimension and it can be a &lt;code&gt;Tuple[int]&lt;/code&gt; as well as &lt;code&gt;torch.Size&lt;/code&gt; if &lt;code&gt;self&lt;/code&gt; is a &lt;code&gt;Tensor&lt;/code&gt;, or &lt;code&gt;namedshape&lt;/code&gt; (Tuple[(name: str, size: int)]) if &lt;code&gt;self&lt;/code&gt; is a &lt;code&gt;NamedTensor&lt;/code&gt;. The total number of elements in sizes must match the number of elements in the original dim being unflattened.</source>
          <target state="translated">&lt;code&gt;sizes&lt;/code&gt; 평탄화되지 차원의 새로운 형태이며이 될 수 &lt;code&gt;Tuple[int]&lt;/code&gt; 뿐만 아니라 &lt;code&gt;torch.Size&lt;/code&gt; 이 경우 &lt;code&gt;self&lt;/code&gt; A는 &lt;code&gt;Tensor&lt;/code&gt; 또는 &lt;code&gt;namedshape&lt;/code&gt; (튜플 [(상품명 : STR 크기 : INT)]) 경우 &lt;code&gt;self&lt;/code&gt; A는 &lt;code&gt;NamedTensor&lt;/code&gt; . 크기의 총 요소 수는 평면화되지 않는 원래 Dim의 요소 수와 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="456a4e3892349cb56f348cc45d2d43c6a1fdebcc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;small_pool&lt;/code&gt;: statistics for the small allocation pool (as of October 2019, for size &amp;lt; 1MB allocations).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e28ebaeabfc2c0d6b3aa7c33dff1b51cdc74c19f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;split(split_size, dim=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.split&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;split(split_size, dim=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.split&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5876e3fa08356c27debb7a3f6a4e19d5195021f3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2c081f74366d531f41758569d924c8eca4f2aa7e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#LambdaLR.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#LambdaLR.state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1ada0be90e33a76bbafbc1f56bfeaa01a1b591aa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiplicativeLR.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#MultiplicativeLR.state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c08ad35da30a67365b493e0644a936c592c984c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.state_dict&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d70995f5b7bd944127ae56e30134e1589387bff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;state_dict(destination=None, prefix='', keep_vars=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.state_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;state_dict(destination=None, prefix='', keep_vars=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.state_dict&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fb8509e15eb5a947638541d2e0b88165e7180ea7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;static backward(ctx: Any, *grad_outputs: Any) &amp;rarr; Any&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#Function.backward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cff415708f2ed71ffade379cb256d393e96a32e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;static forward(ctx: Any, *args: Any, **kwargs: Any) &amp;rarr; Any&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/function.html#Function.forward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7c32fe489bd168ce5e652b34968afe5811ee806" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lbfgs.html#LBFGS.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lbfgs.html#LBFGS.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9de149823ab6cc499e88cf2cc817d126e5e80261" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5593246a393e92adc93a1efbcab5ec17de682e45" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adadelta.html#Adadelta.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adadelta.html#Adadelta.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="39edfd5ad6baf153e08361380547188143eefa7f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adagrad.html#Adagrad.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adagrad.html#Adagrad.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="734be27da644857d49bc3103ab57150b90a655be" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adam.html#Adam.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adam.html#Adam.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4f92ff490c01d4f83e7e2fe218a9dab264b98f2b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamax.html#Adamax.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamax.html#Adamax.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="16a49260c3ab680e99258b67495ff3263ac4804e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamw.html#AdamW.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/adamw.html#AdamW.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e774b967a23f97b1ebf74b6ccf1242018c2b704f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/asgd.html#ASGD.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/asgd.html#ASGD.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="626962a651305f01375949b728fdc46b05256e21" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rmsprop.html#RMSprop.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rmsprop.html#RMSprop.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ec9f52ab6b2a3f93e5e3beaf6c93d0b45e1013c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rprop.html#Rprop.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/rprop.html#Rprop.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73a4ded8470294bd6ffab79faab14dca429cf0e4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sgd.html#SGD.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sgd.html#SGD.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f451a9681df93cee6c9b6e8955c25d96cd6fda73" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sparse_adam.html#SparseAdam.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(closure=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/sparse_adam.html#SparseAdam.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9478657a6bc48f9b2e80d90d60e5b04ab9b9aae2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(context_id)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(context_id)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/optim/optimizer.html#DistributedOptimizer.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eaa929af4d236d726be638285d3433456e899266" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(epoch=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CosineAnnealingWarmRestarts.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;step(epoch=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/lr_scheduler.html#CosineAnnealingWarmRestarts.step&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87c99f6ec18cdcf33205d9149539b8697242bd0f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;step(optimizer, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.step&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5a5904fe6a7743321ff5c71f315d635b92b3c9e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.stft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;stft(n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.stft&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a2d4b82f8a4a08f439941b4fe2cd2f4e127ee11" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation, a single number or a one-element tuple.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; 는 상호 상관, 단일 숫자 또는 단일 요소 튜플에 대한 보폭을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="a32aa3c0b68f959b763c5ab1ff65ebd608f9255b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation, a single number or a tuple.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; 는 상호 상관, 단일 숫자 또는 튜플에 대한 보폭을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="a7cc6339d97b370d095743b2c41bc1edfe10d964" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the cross-correlation.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; 은 상호 상관에 대한 보폭을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="742806613e72fcf7d80117cd493eb9ec43cefbce" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;stride&lt;/code&gt; controls the stride for the sliding blocks.</source>
          <target state="translated">&lt;code&gt;stride&lt;/code&gt; 은 슬라이딩 블록의 보폭을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="123927d720721388f4548c04cdbbeb644e625c96" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;synchronize()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.synchronize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;synchronize()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.synchronize&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d135547c7fb8d297a6fda376338f6cf550760f13" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;synchronize()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.synchronize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;synchronize()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.synchronize&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1fb4d8f09b4f8a5f30635fdf3d09556b86f86f24" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;table(sort_by=None, row_limit=100, header=None, top_level_events_only=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.table&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56b9b812f41a06ed4638217119c49e950638afeb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tensor&lt;/code&gt; must have the same number of elements in all processes participating in the collective.</source>
          <target state="translated">&lt;code&gt;tensor&lt;/code&gt; 는 집합체에 참여하는 모든 프로세스에서 동일한 수의 요소를 가져야합니다.</target>
        </trans-unit>
        <trans-unit id="224069f965b222e903606022d10e7a98e065b9ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tensor&lt;/code&gt; must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU</source>
          <target state="translated">&lt;code&gt;tensor&lt;/code&gt; 는 집합체에 참여하는 모든 프로세스의 모든 GPU에서 동일한 수의 요소를 가져야합니다. 목록의 각 텐서는 다른 GPU에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="3d41bbb307c82a2c9b49f046126aa9c2345c1070" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;then(callback)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.then&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;then(callback)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.then&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="578af95c3bd45de88eac7fe154557b4d4985f0d0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ac5282afe4693a20c19177c7dc6ba853efba47f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#PackedSequence.to&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3dc0e16d9d7f541291e6dbab067b29c301d2e7ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(device=None, dtype=None, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(device=None, dtype=None, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3e596c69ad0590b92b57b912529efbcc355b682e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(dtype, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(dtype, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ccfbe77f465473aa1da76dc9c34f89236b6724c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(memory_format=torch.channels_last)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(memory_format=torch.channels_last)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f08eeed12af7805bba2c17a64a2bdeecb1e24711" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;to(tensor, non_blocking=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;to(tensor, non_blocking=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.to&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eab345e7e4a5581554ed11c71dbb6edfff8c371e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;tol&lt;/code&gt; is the threshold below which the singular values (or the eigenvalues when &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;) are considered to be 0. If &lt;code&gt;tol&lt;/code&gt; is not specified, &lt;code&gt;tol&lt;/code&gt; is set to &lt;code&gt;S.max() * max(S.size()) * eps&lt;/code&gt; where &lt;code&gt;S&lt;/code&gt; is the singular values (or the eigenvalues when &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;), and &lt;code&gt;eps&lt;/code&gt; is the epsilon value for the datatype of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;tol&lt;/code&gt; 특이 값 (또는 고유 한 아래 임계 값 &lt;code&gt;symmetric&lt;/code&gt; 인 &lt;code&gt;True&lt;/code&gt; 된 경우) 0로 간주 &lt;code&gt;tol&lt;/code&gt; 지정되지 않는 &lt;code&gt;tol&lt;/code&gt; 설정되어 &lt;code&gt;S.max() * max(S.size()) * eps&lt;/code&gt; 여기서 &lt;code&gt;S&lt;/code&gt; 는 특이 값 (또는 &lt;code&gt;symmetric&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 인 경우 고유 값 )이고 &lt;code&gt;eps&lt;/code&gt; 는 &lt;code&gt;input&lt;/code&gt; 의 데이터 유형에 대한 엡실론 값입니다 .</target>
        </trans-unit>
        <trans-unit id="18fe92a88d4b832404cc2dbd206ba724755bd2f8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.Assert(condition, message)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#Assert&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.Assert(condition, message)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#Assert&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d791ae567ecad27f4445cb2aaf15f6029d488510" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_1d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_1d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1656ab860bd5829558905cf483ea11bc0039dbff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_2d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_2d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dd52a2bd34a2723d8cb64d098bfcb5aade97fdc1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.atleast_3d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.atleast_3d(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#atleast_3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a78407916bace32596bf1f5dcbbcf4d7e9472472" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], None] = None) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd.html#backward&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24189923b5df2c777bbb1643dbef32ea502859ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#hessian&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="655eca94dc674e80602f39f23ad2e78efdb630fd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#hvp&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb655f2264643b96afbb377badd06fb8a4bd88bd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#jacobian&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbe8bab816a94f2e27ea7862079923dab6c4dd1f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#jvp&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1158f2bd55c53dd95b14365af5ad0b811f40b315" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#vhp&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73c474dc68b910dd62cd6e5f417a8ed50b6608fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/functional.html#vjp&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c29a9003361ddd793f6e952c10f79bcb0528475a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.grad(outputs: Union[torch.Tensor, Sequence[torch.Tensor]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: bool = False) &amp;rarr; Tuple[torch.Tensor, ...]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd.html#grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="734664edb8a87bf5d2df8a1414f03e7360e57b52" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.gradcheck(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, raise_exception: bool = True, check_sparse_nnz: bool = False, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False) &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/gradcheck.html#gradcheck&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="534e5324b293a276c5679c91a25b58d562165b60" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.gradgradcheck(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], None] = None, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, gen_non_contig_grad_outputs: bool = False, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False) &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/gradcheck.html#gradgradcheck&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="235928e06518774166a068744f478fc229604846" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd.profiler.load_nvprof(path)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#load_nvprof&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.autograd.profiler.load_nvprof(path)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#load_nvprof&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="34ddf556ed3c5a3e9f350626f067001b0cd95331" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.autograd&lt;/code&gt; provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare &lt;code&gt;Tensor&lt;/code&gt; s for which gradients should be computed with the &lt;code&gt;requires_grad=True&lt;/code&gt; keyword. As of now, we only support autograd for floating point &lt;code&gt;Tensor&lt;/code&gt; types ( half, float, double and bfloat16) and complex &lt;code&gt;Tensor&lt;/code&gt; types (cfloat, cdouble).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="807982babd7e2697d02339a41138a44e111d331f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cuda.is_built()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cuda.html#is_built&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cuda.is_built()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cuda.html#is_built&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cad84bdf2432c05fed2fe1958d51387bbc6c0020" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cudnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cudnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="00b7a3cf7f24c2b2bebb11277acabb7c78d50317" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.cudnn.version()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#version&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.cudnn.version()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/cudnn.html#version&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9dfbb3781e5878595a4417510d5aae33943c7ef2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkl.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkl.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="72e4f98e5910cf117b1472f53b8f73df836ca78a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.mkldnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkldnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.mkldnn.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/mkldnn.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7054ed97a3665299406dd42d55c4229d95f96305" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends.openmp.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/openmp.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.backends.openmp.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/backends/openmp.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4db6ff3c64dff7c5f1e9ecefcab9b3fe7f8bd5ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.backends&lt;/code&gt; controls the behavior of various backends that PyTorch supports.</source>
          <target state="translated">&lt;code&gt;torch.backends&lt;/code&gt; 는 PyTorch가 지원하는 다양한 백엔드의 동작을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="4798618741fa5295873eda72f10f638e521941e3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.block_diag(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#block_diag&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.block_diag(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#block_diag&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="561c4a9dc39238e682070f0062e9aef0052df081" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.broadcast_tensors(*tensors) &amp;rarr; List of Tensors&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#broadcast_tensors&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.broadcast_tensors(*tensors) &amp;rarr; List of Tensors&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#broadcast_tensors&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f9abf70758cd66412725ba8672052a7932024490" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cartesian_prod(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cartesian_prod&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cartesian_prod(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cartesian_prod&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eec3865daff49e31eb283e5df540594f2c021646" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cdist&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#cdist&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dd3bb7208ad3447bd28402045aaa3520bba00449" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.chain_matmul(*matrices)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#chain_matmul&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.chain_matmul(*matrices)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#chain_matmul&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b7ede943ebb499689b4fa6f38f84fe9ace9c3ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.channels_last&lt;/code&gt;: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in &lt;code&gt;strides[0] &amp;gt; strides[2] &amp;gt; strides[3] &amp;gt; strides[1] == 1&lt;/code&gt; aka NHWC order.</source>
          <target state="translated">&lt;code&gt;torch.channels_last&lt;/code&gt; : Tensor가 겹치지 않는 고밀도 메모리에 할당되거나 할당됩니다. &lt;code&gt;strides[0] &amp;gt; strides[2] &amp;gt; strides[3] &amp;gt; strides[1] == 1&lt;/code&gt; 일명 NHWC 순서의 값으로 표시되는 보폭 .</target>
        </trans-unit>
        <trans-unit id="7261c6034d86aa2ddc9431902e39f474a13e60a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cholesky_solve(b, u)&lt;/code&gt; can take in 2D inputs &lt;code&gt;b, u&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;c&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.cholesky_solve(b, u)&lt;/code&gt; 는 2D 입력 &lt;code&gt;b, u&lt;/code&gt; 또는 2D 행렬의 배치 인 입력을 받을 수 있습니다 . 입력이 배치 인 경우 배치 된 출력을 반환합니다. &lt;code&gt;c&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d68363efb3452b6e83ed851d4699a5aa0f779741" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.compiled_with_cxx11_abi()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#compiled_with_cxx11_abi&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.compiled_with_cxx11_abi()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#compiled_with_cxx11_abi&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e126cacbad4b261249efa0e8913738a1877bbdfa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.complex128&lt;/code&gt; or &lt;code&gt;torch.cdouble&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.complex128&lt;/code&gt; 또는 &lt;code&gt;torch.cdouble&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f39fd8648813e09f8476a9ac8da4abf78e93db3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.complex64&lt;/code&gt; or &lt;code&gt;torch.cfloat&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.complex64&lt;/code&gt; 또는 &lt;code&gt;torch.cfloat&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="dbaed180a18c68c202c3fc91f47ffeb99ac16e81" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.contiguous_format&lt;/code&gt;: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.</source>
          <target state="translated">&lt;code&gt;torch.contiguous_format&lt;/code&gt; : 텐서는 겹치지 않는 고밀도 메모리에 할당되거나 할당됩니다. 내림차순으로 값으로 표시되는 보폭.</target>
        </trans-unit>
        <trans-unit id="523b34b08b4c9679a92f7f91baee983d53805c56" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.amp.custom_bwd(bwd)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/autocast_mode.html#custom_bwd&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.amp.custom_bwd(bwd)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/autocast_mode.html#custom_bwd&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a70c5dcfbfdb50d44389469406715ab520122e7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.amp.custom_fwd(fwd=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/autocast_mode.html#custom_fwd&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="038f41c15bca336c08a43b4dcc158b2abba0e630" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.amp&lt;/code&gt; provides convenience methods for mixed precision, where some operations use the &lt;code&gt;torch.float32&lt;/code&gt; (&lt;code&gt;float&lt;/code&gt;) datatype and other operations use &lt;code&gt;torch.float16&lt;/code&gt; (&lt;code&gt;half&lt;/code&gt;). Some ops, like linear layers and convolutions, are much faster in &lt;code&gt;float16&lt;/code&gt;. Other ops, like reductions, often require the dynamic range of &lt;code&gt;float32&lt;/code&gt;. Mixed precision tries to match each op to its appropriate datatype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec925a779da730572dfd492f73df7b74c59cfbad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.comm.broadcast(tensor, devices=None, *, out=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/comm.html#broadcast&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78753e66d555b06c3944990a5609e51dde373222" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/comm.html#broadcast_coalesced&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c5d1210f1de4a4712f185c3e53b94493b742258" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/comm.html#gather&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97296fcc6c7b2a5c5ff7091746fb1a2f0b3690e8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.comm.reduce_add(inputs, destination=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/comm.html#reduce_add&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f17e49a07178225ab5e95ba83ec2aa78cf90e5e5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/comm.html#scatter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="663128b73bfbc6218cfbd65b4fc556d23e1fd0fa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.current_blas_handle()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#current_blas_handle&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.current_blas_handle()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#current_blas_handle&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="422c1e5d0222f8d68e610894ff5e58c09683d0d6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.current_device() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#current_device&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70a374c61c59e812abd92e789ff44f0c96954b94" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.current_stream(device: Union[torch.device, str, int, None] = None) &amp;rarr; torch.cuda.streams.Stream&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#current_stream&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4423d6e40ed5806e93c2ea33d09033078c6c9866" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.default_stream(device: Union[torch.device, str, int, None] = None) &amp;rarr; torch.cuda.streams.Stream&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#default_stream&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b147cdea072e08f980e8f382211fc167161bc50b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.device_count() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#device_count&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94176f19f31656b78ed67c1d855651271af8080d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.empty_cache() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#empty_cache&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95bf01cea7574a9e42937b6f4beb8a178041ca98" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_arch_list() &amp;rarr; List[str]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#get_arch_list&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a3973419d44be6b62a2b35204210fa09dddef50" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_device_capability(device: Union[torch.device, str, int, None] = None) &amp;rarr; Tuple[int, int]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#get_device_capability&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9699b788e93f5fce8fd4b5ecbc07e4747f0a710" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_device_name(device: Union[torch.device, str, int, None] = None) &amp;rarr; str&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#get_device_name&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce69093a4a217e4ea78a7737659e20fb37cf137f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_gencode_flags() &amp;rarr; str&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#get_gencode_flags&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="758b019a0fb8e8f34d93067f0ca82174d8a5758a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_rng_state(device: Union[int, str, torch.device] = 'cuda') &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#get_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="297ab14db7454915292d1d843bf4aaa5d908f9c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.get_rng_state_all() &amp;rarr; List[torch.Tensor]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#get_rng_state_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c96d62fd0b1932cea866e6ab0507003c0e6ebb6e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.init()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#init&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.init()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#init&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ceada063f73190ee758dda3542cb67dc603f0ade" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.initial_seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#initial_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84cbd5cc86ad86e5df15fbbe6f82e170dd7bfdb7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.ipc_collect()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#ipc_collect&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.ipc_collect()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#ipc_collect&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2480737d2bd1b47412a754f8fb7b13d6bec8ea75" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.is_available() &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="364103fbe4718c9ba0b250f0fa0a689cd3432671" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="333cc01802288f802a79cea3c2bca3f6732b1ec7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.manual_seed(seed: int) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#manual_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f40ded8d9d3b25c689e88dfe94830451f498464" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.manual_seed_all(seed: int) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#manual_seed_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="585bcf5fd635a9739ee7d1c8c43a0f77d0459d1f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.max_memory_allocated(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#max_memory_allocated&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7179f17d285a86cddff9cce00dddabf849953771" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.max_memory_cached(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#max_memory_cached&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a8469e248897085b4128ac5b6ce6d19578c1eb2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.max_memory_reserved(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#max_memory_reserved&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1368f8b2b7d8584b2e9346ce73eb8d4b325e3380" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_allocated(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_allocated&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bace1108374ba8e22d8e3f1de97ff5126d1c4a46" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_cached(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_cached&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="482c30a0b03885bbaac5cbd300b53852240d126e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_reserved(device: Union[torch.device, str, None, int] = None) &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_reserved&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b186ead96ba6b42c5dbd0863620dc06c5d9b57f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_snapshot()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_snapshot&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.memory_snapshot()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_snapshot&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8286cafb53739cb5b82c32314ee854d19b2ea4e5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_stats(device: Union[torch.device, str, None, int] = None) &amp;rarr; Dict[str, Any]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_stats&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77c494a77c8eea851a64e863f2aa3660a6118737" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.memory_summary(device: Union[torch.device, str, None, int] = None, abbreviated: bool = False) &amp;rarr; str&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#memory_summary&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff3e04cbc2359fb4d4f5d9436c23af54fca781f9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.nvtx.mark(msg)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#mark&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.nvtx.mark(msg)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#mark&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="07e4929d5cb15837b11d2c07a47e5242c24c2633" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.nvtx.range_pop()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#range_pop&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.nvtx.range_pop()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#range_pop&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="37cd20c1d6fd67a5fa9a958d909561aa6d0579d0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.nvtx.range_push(msg)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#range_push&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.nvtx.range_push(msg)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/nvtx.html#range_push&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2826766e94ea9200ac4a72e1ecf4bab31b336a72" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.reset_max_memory_allocated(device: Union[torch.device, str, None, int] = None) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#reset_max_memory_allocated&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3618f00eccc8c651f10e74502c75d180de38d4f8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.reset_max_memory_cached(device: Union[torch.device, str, None, int] = None) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/memory.html#reset_max_memory_cached&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="377f5867474aa0344581b234de0c2de53517a9c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.seed() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77fc87c6824f75f81312170ce115e7af8724ba0d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.seed_all() &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#seed_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a93c5152623d3391c6b60b96ade9dc6e96d4e80" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.set_device(device: Union[torch.device, str, int]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#set_device&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a89a09b9a4101588e57539bb12e591554ba62b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.set_rng_state(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'cuda') &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#set_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="666725f6ad3325df551abe169f689589a8c3ce43" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.set_rng_state_all(new_states: Iterable[torch.Tensor]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/random.html#set_rng_state_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e7f012417e8b0cb3c74fad5ed61dd2f1f67f0fa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.stream(stream)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#stream&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.cuda.stream(stream)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#stream&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e0cb816f3f738479ffc84f93d022af86dff596f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.cuda.synchronize(device: Union[torch.device, str, int] = None) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda.html#synchronize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3684d59244595e52fd2828b50ca6282d62fd4739" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_gather(tensor_list, tensor, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_gather(tensor_list, tensor, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="288bfa32c25fcb8306179eca04e736e2a02186b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_gather_multigpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8413a7e854b6ea51eaa6b4f60e55aab063717a94" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8e54a6af7a1934d02149ec3e7822e2c3aacde8c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_reduce_multigpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b02380125bbd88799d9b414d91da3147fc32cac4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_to_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#all_to_all&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="811431042ba4ac28ba32b7911f195c224a3cc8ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.barrier(group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#barrier&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.barrier(group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#barrier&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="972f101974bfb6f5d4198acab57d132de3def05d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.broadcast(tensor, src, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.broadcast(tensor, src, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="42f137f701d871ed16f93e8f54ccf11f3751a266" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.broadcast_multigpu(tensor_list, src, group=&amp;lt;object object&amp;gt;, async_op=False, src_tensor=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.broadcast_multigpu(tensor_list, src, group=&amp;lt;object object&amp;gt;, async_op=False, src_tensor=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#broadcast_multigpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a91a9c5b16b5a59e71ee3db5306f6bfcf11d1b07" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.gather(tensor, gather_list=None, dst=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#gather&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.gather(tensor, gather_list=None, dst=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#gather&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="469f0c245793ba3173d50a4ba6d0d4b02a50ac40" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_backend(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_backend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_backend(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_backend&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d9abfee4a0b4cccfc17184dd0b1cc26e5d371ee3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_rank(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_rank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_rank(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_rank&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3d2f006f10e722cd486cd5167f1f023ac4348cde" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.get_world_size(group=&amp;lt;object object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_world_size&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.get_world_size(group=&amp;lt;object object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#get_world_size&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3cd2bdef011082ccaf0f6cdb5ff29f262f1a95e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#init_process_group&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#init_process_group&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="be9e65a88a9fe81456b9d46075c4010654d4e9b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.irecv(tensor, src, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#irecv&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.irecv(tensor, src, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#irecv&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="20f496294e21fb47e152e675602dbf3c461a9b08" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed.html#is_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed.html#is_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="223963009f0daaa6463b96377bfe6ba435636eb6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_initialized()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_initialized&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cbd076810efd1ed9ebbabe499f72753a91996038" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_mpi_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_mpi_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_mpi_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_mpi_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db46fe559b0d0bbdf0cef5a5524c681f3d238f08" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.is_nccl_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_nccl_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.is_nccl_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#is_nccl_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6b9418c661191c8470ab193315235ef4714a868" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.isend(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#isend&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.isend(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#isend&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9dd059c98fffcac9f54ce1694e15b99004a8591e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.launch&lt;/code&gt; is a module that spawns up multiple distributed training processes on each of the training nodes.</source>
          <target state="translated">&lt;code&gt;torch.distributed.launch&lt;/code&gt; 는 각 학습 노드에서 여러 분산 학습 프로세스를 생성하는 모듈입니다.</target>
        </trans-unit>
        <trans-unit id="00f170d324ae5b762271dfa97f204b0ab1323083" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(0, 1800), backend=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#new_group&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(0, 1800), backend=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#new_group&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c722cbc71a64e0dc42cb9a9d3c7a5edc03b0d4ef" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.recv(tensor, src=None, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#recv&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.recv(tensor, src=None, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#recv&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4934f4472f321622371e7a7b50861754714e2974" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4853070b0ea1c763754c27135933c602998a4e60" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False, dst_tensor=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False, dst_tensor=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_multigpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73023ab881e640865f6cc8d6a5f7e0c4057841ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_scatter(output, input_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_scatter(output, input_list, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2dbd0e9e1283295f2349e8245482b78e102b7f5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_multigpu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=ReduceOp.SUM, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#reduce_scatter_multigpu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="658d9ec010176f539c7002ca6f7244824cf0ff2e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.functions.async_execution(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/functions.html#async_execution&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.functions.async_execution(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/functions.html#async_execution&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="11ee0ae675da3185be2d88cf7686272070008035" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.get_worker_info(worker_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.get_worker_info(worker_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ac681363a6df2035cf0af007fbe15e5f40b1313" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc.html#init_rpc&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc.html#init_rpc&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a807f1699fa2b1f80a8a85a3c47163cacb1811a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#remote&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#remote&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4298605ee9a8f80fa78f8bafe3d0d10bc0b63530" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_async&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_async&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ea65440f151425bd963fae7e719909e0e6f7799" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_sync&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#rpc_sync&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d1578868b0db95ef02e8c1751bf13334567357d8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.rpc.shutdown(graceful=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#shutdown&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.rpc.shutdown(graceful=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/rpc/api.html#shutdown&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="74962bc84981f59ffb46c79d8d0a32c1fa2c88fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.scatter(tensor, scatter_list=None, src=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#scatter&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.scatter(tensor, scatter_list=None, src=0, group=&amp;lt;object object&amp;gt;, async_op=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#scatter&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1a91666abb9d1adeac875045b37d1d67269e7268" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed.send(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#send&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.distributed.send(tensor, dst, group=&amp;lt;object object&amp;gt;, tag=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributed/distributed_c10d.html#send&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b242b4c4d95c3dfc78fd815399bf2a8ab4321962" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributed&lt;/code&gt; supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.</source>
          <target state="translated">&lt;code&gt;torch.distributed&lt;/code&gt; 는 각각 다른 기능을 가진 3 개의 내장 백엔드를 지원합니다. 아래 표는 CPU / CUDA 텐서와 함께 사용할 수있는 함수를 보여줍니다. MPI는 PyTorch 빌드에 사용 된 구현이 CUDA를 지원하는 경우에만 CUDA를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="48a58c9a3a607d4b481aab2ec93e3d1087ea1b60" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributions.kl.kl_divergence(p, q)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/kl.html#kl_divergence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8bdbef1dc6f9b4bb725b400ce01a939ede91e06" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.distributions.kl.register_kl(type_p, type_q)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/kl.html#register_kl&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cbefef3cb884be3385871dce8b2b62386448a14" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.einsum(equation, *operands) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#einsum&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.einsum(equation, *operands) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#einsum&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="640fe0a067d6b473ac9256af65e952ac6dfc3b61" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.half&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float16&lt;/code&gt; 또는 &lt;code&gt;torch.half&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bae15d4af90dddb94594a94fec29f67ac605b467" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float32&lt;/code&gt; or &lt;code&gt;torch.float&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float32&lt;/code&gt; 또는 &lt;code&gt;torch.float&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90d6fe76faa60bedbd01aec13dc05b425977c0c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.float64&lt;/code&gt; or &lt;code&gt;torch.double&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.float64&lt;/code&gt; 또는 &lt;code&gt;torch.double&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="265f8d449a3becac9b19de67b27384583c69e026" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.futures.collect_all(futures: List[torch.jit.Future]) &amp;rarr; torch.futures.Future[List[torch.jit.Future]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#collect_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.futures.collect_all(futures: List[torch.jit.Future]) &amp;rarr; torch.futures.Future[List[torch.jit.Future]]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#collect_all&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc976900f35641949861ea9c8ea17e248ec3810f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.futures.wait_all(futures: List[torch.jit.Future]) &amp;rarr; List&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#wait_all&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.futures.wait_all(futures: List[torch.jit.Future]) &amp;rarr; List&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#wait_all&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae80701e04934a777dbdb12d87e5a0f70bd523c5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.get_rng_state() &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#get_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.get_rng_state() &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#get_rng_state&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3bcc059f1d0a236e31a5f5312334d77b1ac57459" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#download_url_to_file&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#download_url_to_file&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f87f9cf1e2fb7ea9e5ce5d50dcecf4a61cc11583" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.get_dir()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#get_dir&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.get_dir()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#get_dir&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6f4fba1da5aa27b4afa12ceb8026cb1b5c9a7926" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.help(github, model, force_reload=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#help&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.help(github, model, force_reload=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#help&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="882b251a8fb497576db4d5562fe368823616bfec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.list(github, force_reload=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#list&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.list(github, force_reload=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#list&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5cd79bde08bc8ad0034543204f4d73cd57529deb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.load(repo_or_dir, model, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.load(repo_or_dir, model, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="79c43b2a087c0324bade258ba72699891cf8e21a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load_state_dict_from_url&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#load_state_dict_from_url&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1d5ca2b155b44bdba1204740a35f42adbc43fcba" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.hub.set_dir(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#set_dir&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.hub.set_dir(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/hub.html#set_dir&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5f8c8ccc73401651bfcc5f84e5b8bd9b3b988f2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.index_add_()&lt;/code&gt; when called on a CUDA tensor</source>
          <target state="translated">&lt;code&gt;torch.index_add_()&lt;/code&gt; when called on a CUDA tensor</target>
        </trans-unit>
        <trans-unit id="e8c9e450410f2773a3b1ffb535039963fc75b581" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.initial_seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#initial_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.initial_seed() &amp;rarr; int&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#initial_seed&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a2618a717b800b2250c20f08796353ec5f6174dc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int16&lt;/code&gt; or &lt;code&gt;torch.short&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int16&lt;/code&gt; or &lt;code&gt;torch.short&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c846dd030e173dece1f667ac83f5f7c89e7ef730" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int32&lt;/code&gt; or &lt;code&gt;torch.int&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int32&lt;/code&gt; or &lt;code&gt;torch.int&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f84a6d005309b8b164f747fff43a7112692349b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.int64&lt;/code&gt; or &lt;code&gt;torch.long&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.int64&lt;/code&gt; or &lt;code&gt;torch.long&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2d7902807a818755d110fce5fb71486036a8b000" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_deterministic()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_deterministic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_deterministic()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_deterministic&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b61e78ddb3d215366857032848241968e697c5cf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_storage(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_storage&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_storage(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_storage&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3fba0f4d5b1df772c7aadec0c1a6d0985cd38b1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.is_tensor(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_tensor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.is_tensor(obj)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#is_tensor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e0e4a4b6b75b92120aeb93fcdb25e4b132a47ecd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.istft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#istft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.istft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#istft&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89e22ec54610807a0b00f1d4c91a1d7ec876f10a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.export(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.export(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b8bbb3f29cb40c5d3ea9776b88cf43c34c1878da" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.fork(func, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#fork&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.fork(func, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#fork&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="421559b3abd5edb03900fba11315d83fc334603d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.ignore(drop=False, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#ignore&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.ignore(drop=False, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#ignore&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="eeb12aa66d7656a4ffc0d96d046cf3b6b08a0bde" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.is_scripting()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#is_scripting&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.is_scripting()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#is_scripting&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f88e9a93ad8baaaea0c628df0a8e7ac2c56e5c41" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.load(f, map_location=None, _extra_files=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.load(f, map_location=None, _extra_files=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#load&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e92c3fdd876652458f659e7da2613f06913984b5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.save(m, f, _extra_files=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#save&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.save(m, f, _extra_files=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_serialization.html#save&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6643232796915d1c4cfa96d674412c4c0bb284ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_script.html#script&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_script.html#script&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4db5d8c45560a499d468c5dac1c897f99ebef493" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.script&lt;/code&gt; can be used as a function for modules and functions, and as a decorator &lt;code&gt;@torch.jit.script&lt;/code&gt; for &lt;a href=&quot;../jit_language_reference#id2&quot;&gt;TorchScript Classes&lt;/a&gt; and functions.</source>
          <target state="translated">&lt;code&gt;torch.jit.script&lt;/code&gt; can be used as a function for modules and functions, and as a decorator &lt;code&gt;@torch.jit.script&lt;/code&gt; for &lt;a href=&quot;../jit_language_reference#id2&quot;&gt;TorchScript Classes&lt;/a&gt; and functions.</target>
        </trans-unit>
        <trans-unit id="a98b44186e36671ec40c3bdafe708c3fee23035e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c9049933bf5baeb51bda091d7ad902658b9ddde4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace_module&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=&amp;lt;torch._C.CompilationUnit object&amp;gt;)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_trace.html#trace_module&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="909e5d79046847b4d02782a7a02f0a2ed016af7c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.unused(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#unused&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.unused(fn)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_jit_internal.html#unused&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f65c9de77cddf9268ba06604577821b9c763e902" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.jit.wait(future)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#wait&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.wait(future)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/jit/_async.html#wait&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fb47c5bcc3ae1dfb1d388e26ba7411471ab097bf" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.load(f, map_location=None, pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, **pickle_load_args)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.load(f, map_location=None, pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, **pickle_load_args)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#load&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8f43737887598a9e805be62f92deca699d0575a2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None) &amp;rarr; Tuple[torch.Tensor, torch.Tensor]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lobpcg.html#lobpcg&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None) &amp;rarr; Tuple[torch.Tensor, torch.Tensor]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lobpcg.html#lobpcg&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0bea827671aef5f3b7eda50b4794348264f9aff5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#lu_unpack&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#lu_unpack&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5d5a6b9339a9ae4c05a5ba1a9d565042e21e439d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.manual_seed(seed) &amp;rarr; torch._C.Generator&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#manual_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.manual_seed(seed) &amp;rarr; torch._C.Generator&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#manual_seed&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="073d41ae4569e0590b24bb02a2cde22f383b83d9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.meshgrid(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#meshgrid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.meshgrid(*tensors)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#meshgrid&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="71b82477b63b8cbc60cab0dacf09002f37e60109" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.multiprocessing.get_all_sharing_strategies()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#get_all_sharing_strategies&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.multiprocessing.get_all_sharing_strategies()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#get_all_sharing_strategies&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="568423222b167aee937bca02e04ee35c21524399" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.multiprocessing.get_sharing_strategy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#get_sharing_strategy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.multiprocessing.get_sharing_strategy()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#get_sharing_strategy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7261904a048eb4b1294c411efa5b09810580c38d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.multiprocessing.set_sharing_strategy(new_strategy)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#set_sharing_strategy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.multiprocessing.set_sharing_strategy(new_strategy)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing.html#set_sharing_strategy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0c07019c767f162d31490d7967d92dcee5529d8f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/multiprocessing/spawn.html#spawn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a39a57d1a7c56039fc95abb2d9bced03e8b97998" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.FractionalMaxPool3d&lt;/code&gt; when called on a CUDA tensor that requires grad</source>
          <target state="translated">&lt;code&gt;torch.nn.FractionalMaxPool3d&lt;/code&gt; when called on a CUDA tensor that requires grad</target>
        </trans-unit>
        <trans-unit id="d2ef2c94f97ed017c925779284bb77ad0ca08325" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.ModuleList&lt;/code&gt; which can be used in a TorchScript for loop</source>
          <target state="translated">&lt;code&gt;torch.nn.ModuleList&lt;/code&gt; which can be used in a TorchScript for loop</target>
        </trans-unit>
        <trans-unit id="32366e9745ef65d7bfb28a4d81602df1879fc1f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.adaptive_avg_pool2d(input, output_size)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.adaptive_avg_pool2d(input, output_size)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="91c0ea25976bc55d2262d00f521746f256fd7d87" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.adaptive_avg_pool3d(input, output_size)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.adaptive_avg_pool3d(input, output_size)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#adaptive_avg_pool3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1097d0a9532bd8fca9736e541af64e621dd46ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.affine_grid(theta, size, align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#affine_grid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.affine_grid(theta, size, align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#affine_grid&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c10efc3580c8c69cc035cb33b2726bd8f14a73ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#alpha_dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#alpha_dropout&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c9e17e7f47d7822e2cc0d473c5d222b14fdb92ae" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#batch_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#batch_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3f5717edf64a59d5e363eebc9908da8813501fda" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.bilinear(input1, input2, weight, bias=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.bilinear(input1, input2, weight, bias=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#bilinear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59ddc7997f3b59e0fae45c87ee15abd7cfb5c780" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6d7201823c06556ca24e08ebd1f5fb4265f72f2b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1f9da1009f60918b558c97a1d7a7e883d1c08ef5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.celu(input, alpha=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#celu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.celu(input, alpha=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#celu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="33d37a2b8a8128d94d1481492dac9ef306f0a856" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cosine_embedding_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cosine_embedding_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="57abf7da230c9f41633856a8b01cff56c970873e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cross_entropy&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#cross_entropy&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1407e69c5d1e13d69f6e78ff47b7322aba83bca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#ctc_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#ctc_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f969e2514bd7bfecbf147a73c1d2c27799a7bb5f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6dc24391ef1a7c85edc1df1e473c9eaf7bb6c02" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="616431983c70ac7b49c82cdf658d71df42a37921" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#dropout3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6423f00d771b3620d1b3b711d04b3367cc6b7f97" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.elu(input, alpha=1.0, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#elu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.elu(input, alpha=1.0, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#elu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="066d8804893169dfbca3fe7dd9656b071610464f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5fe5f0a3587fe920a916a80da67623f5a29b0698" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding_bag&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#embedding_bag&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8214fa8ec6f51b9ba17070449e398de1a1a749fb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#feature_alpha_dropout&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#feature_alpha_dropout&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="39356278d3eb3659a3cd4fb5a8875a52d82b1225" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#fold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#fold&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89c06fb7800616c516855b4f04ceecdc17518e9f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.gelu(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.gelu(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gelu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a218695613bb5a0de8369ed33bfc9afab383391" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.glu(input, dim=-1) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#glu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.glu(input, dim=-1) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#glu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7021625a38b746ce1a39ee32b502f2a6db9c4585" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#grid_sample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#grid_sample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a060aa4e1d34c9907ee63c0b90e99edc50dbbb3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gumbel_softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#gumbel_softmax&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5cda7558b3ad9b28cbdef77c8deaf73ae843265e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardshrink(input, lambd=0.5) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardshrink&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardshrink(input, lambd=0.5) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardshrink&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a7788357e1132c7c9f4fa67b9498a151b9b5344" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardsigmoid(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardsigmoid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardsigmoid(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardsigmoid&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c3d47479a5aaf650bd1b5e35f8569bccb683bc13" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardswish(input: torch.Tensor, inplace: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardswish(input: torch.Tensor, inplace: bool = False) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e678c28b805974934fa864eaa15336e3e3c6f449" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardtanh&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hardtanh&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58d4e43d719885efde48902367d9278eb45afaec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hinge_embedding_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#hinge_embedding_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50068fafd498873bbc84c37c079bdad47af2fd62" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#instance_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#instance_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="51feb15b8eec41004d8f24c92329feec6a5d9a4b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4604fd22bca040ed4adef631c4303a0928807837" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#kl_div&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#kl_div&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2ce8753729c7c13600dbf6967ed8327871c4593f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#l1_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#l1_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="042a048fb27abd8cb71530f91c0f8ea35ccf4c3f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#layer_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#layer_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee6d48c9626543647597f538ce12bde573f38c3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#leaky_relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#leaky_relu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a4cf8df1543c4662c88c825d378daccd8af5cac9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.linear(input, weight, bias=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.linear(input, weight, bias=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1894e3d0ea0f07955092bee7408de704155bb002" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#local_response_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#local_response_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="30f3ecb91a729dbee3572f54bfcb95d8e01f9c7f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#log_softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#log_softmax&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="972472a778af86e2cc6ab6c107489a0f2bbf470b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0fdc0f3b7e0449603fc73493406027c87149dbf3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#lp_pool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0958859b9ae499a4857d4cd6aaec3f5f14bd2c86" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#margin_ranking_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#margin_ranking_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2336c5e17dc37a4bcd303ad18064b289aec7a00" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool1d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="909e9fe0f0dd81343d67d3049d4157cb713660c2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool2d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ff10e746b2a75339884b3a9e908991fdf3966f4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#max_unpool3d&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="236f40331ff3004b076767f9b10c6574701143b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#mse_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#mse_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8eeb3878caecd3953dc4e0e2c213444e69c755e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multi_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multi_margin_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f1ee4be3b183fc3aa98b897157f8528b20c0ab5e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_margin_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a0ce36944943aeea79ad873bdc50b48d24a9813" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#multilabel_soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cf1e6a52728af5a23fdafaff75cb840fbffcb4b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#nll_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#nll_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1b1d925500bf42746b1b5d13e110a776e5606671" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#normalize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#normalize&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93db9f8b73ef84aa64b0ef7c4aa681dfd9d1e05b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#pairwise_distance&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#pairwise_distance&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5170cc63198565dc974ddde740562cc7342b749a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#poisson_nll_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#poisson_nll_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d54df6a29bf6ce2dd719fcad0f54888591eb1141" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.prelu(input, weight) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#prelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.prelu(input, weight) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#prelu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="414a24a106fc3b57b23f73bd9ef56f10b48b500d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08c43c85b6fec232c61ab3d0fac43966e9dca202" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.relu6(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu6&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.relu6(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#relu6&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="363444915890f651770dba4e0ee139a8bd0e34ad" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#rrelu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#rrelu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="988850f057511c9e65c7596f53b05e71f72224e3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.selu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#selu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.selu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#selu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6a67420270ae790a8aafa44865eaff88ba0d203f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.sigmoid(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#sigmoid&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.sigmoid(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#sigmoid&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0ba03acac585036f1c75ac22544f646935e1e39" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.silu(input, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#silu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.silu(input, inplace=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#silu&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8976e3b61e7e02e09db919b7126ee6c1b55242b5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#smooth_l1_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#smooth_l1_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b11cb9c00fdb5d5d830ce06235b6b66872822b1a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#soft_margin_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d7a1c434aa606493bf190a0bf88a2893392f5f51" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmax&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmax&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="185fc46c45a103fa23ebbf763c968235e69d4f91" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmin&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softmin&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d76065e3268c5e424219969024a1ca1d8873a52e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.softsign(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softsign&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.softsign(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#softsign&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="850af8d2773f3c55122e00b4af5eca41549cfe3a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.tanh(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanh&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.tanh(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanh&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c1d3cdf7abf7ca22a35e5af528ba4719fba901cb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.tanhshrink(input) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanhshrink&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.tanhshrink(input) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#tanhshrink&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10b4e0d06c12b3eca2b5536d2f4dd8e12f1afe63" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0d87f333f2566c2b6cf4938778c9c85531c6c10" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_with_distance_loss&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#triplet_margin_with_distance_loss&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="16ab53e73c0ea07934feaf16eb7a2b3412c8c52d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#unfold&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#unfold&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e53c770d65bb0ba2d44ae8c0c45269c018a3cfbe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="35c396cdfcf1f7742a14e8fe14ea4839369708d7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="46210cb9fc54c5c8d19c399913fa6564ee472fa6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6f6616285fb50e039b0a50f40c8cc182e30b9ad3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.calculate_gain(nonlinearity, param=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#calculate_gain&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.calculate_gain(nonlinearity, param=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#calculate_gain&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5e152873cf5a634ac4fc22712597672cedec772" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.constant_(tensor, val)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#constant_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.constant_(tensor, val)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#constant_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3e07661fc90ed5f8c330b4c199c178b913c9f1c7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.dirac_(tensor, groups=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#dirac_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.dirac_(tensor, groups=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#dirac_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f79be36611abc8d4a155dd9204a72975ac5e4773" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.eye_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#eye_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.eye_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#eye_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2bf176e81d0d3dc053d580d4a2bca53d53776a5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_normal_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="829f255be8c47a0cc32b32e3e7a39b50f03bbb28" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#kaiming_uniform_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d74bb441e80c3db03c439b39588a3d7289372829" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.normal_(tensor, mean=0.0, std=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.normal_(tensor, mean=0.0, std=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#normal_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cda901e36d8a0dea4c9edd368b65c875ba5b5524" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.ones_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#ones_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.ones_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#ones_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="76cd921a26c8bc82c432454c6208f7007838adc0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.orthogonal_(tensor, gain=1)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#orthogonal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.orthogonal_(tensor, gain=1)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#orthogonal_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8d09409fb3770c88ebcd026eab2c58df33a7730e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.sparse_(tensor, sparsity, std=0.01)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#sparse_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.sparse_(tensor, sparsity, std=0.01)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#sparse_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6ef0d5afce7bb2d1cbfe668611dc89920c0ae59" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.uniform_(tensor, a=0.0, b=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.uniform_(tensor, a=0.0, b=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#uniform_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85278dfd6aeb0f31979ad65f57d07305852e15d6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.xavier_normal_(tensor, gain=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_normal_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.xavier_normal_(tensor, gain=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_normal_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1b21f1cd018490607178af5dd8bf83497f69a9ed" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.xavier_uniform_(tensor, gain=1.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_uniform_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.xavier_uniform_(tensor, gain=1.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#xavier_uniform_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a46d62f5de757e1314a9fdb13f301b29dd3064bb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.init.zeros_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#zeros_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.init.zeros_(tensor)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/init.html#zeros_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cabbf4b7897497900a45a7b2817218dc42a094a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/data_parallel.html#data_parallel&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/parallel/data_parallel.html#data_parallel&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1820a94045081d62850e6015322b8f1bcd529902" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.adaptive_avg_pool2d(input: torch.Tensor, output_size: None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#adaptive_avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.adaptive_avg_pool2d(input: torch.Tensor, output_size: None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#adaptive_avg_pool2d&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e49a56be44c1c84f8d24fa8b0f0a773df40282a1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#avg_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#avg_pool2d&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fc695b6cf5e70404648a96381625d3b4242dfbc0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv1d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv1d&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b4fafcb23bac88610267cfd4c65156bfb6814790" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv2d&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a1f61a8e9a1e417e172cf9ee69a3ac703666b76c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv3d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#conv3d&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cd0daa95478860b380dc09345e82221940cc1573" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.hardswish(input: torch.Tensor, scale: float, zero_point: int) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.hardswish(input: torch.Tensor, scale: float, zero_point: int) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#hardswish&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3111ef1fc4e980e9ab5ca9059aba510612419184" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#interpolate&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#interpolate&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a74909e8cec504214450b346ecf69f391517d12" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.linear(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale: Optional[float] = None, zero_point: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#linear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.linear(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale: Optional[float] = None, zero_point: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#linear&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73c3f5a9236c23bb8967520b8b62180ad2c998ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#max_pool2d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#max_pool2d&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c862b119f84e81983c7b7be77dd88a11b46ed28" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#relu&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.relu(input, inplace=False) &amp;rarr; Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#relu&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b8d860cd9cbb88773d5c3ccd609b1b067ad8e1ec" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="580dc9c2b76d8a1672544e8ee223a9a9de122d26" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_bilinear&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_bilinear&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a1660fcc28bce5d877097c2d858685ee9ac889c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_nearest&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/quantized/functional.html#upsample_nearest&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee48c314ebbb974007123f98f539ecffcf9858aa" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.clip_grad_norm_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.clip_grad_norm_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3c6b682d79cb2d7285847c9284685f7cc01c3e7c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.clip_grad_value_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.clip_grad_value_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6d2fddcb8f6b20b88995ff4d154426897cebd261" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.parameters_to_vector(parameters: Iterable[torch.Tensor]) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.parameters_to_vector(parameters: Iterable[torch.Tensor]) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="19a161f43edebdc3fe1ed0ffe30c004ecf54c828" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.custom_from_mask(module, name, mask)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#custom_from_mask&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.custom_from_mask(module, name, mask)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#custom_from_mask&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db191bbfd3345206e1e35b684c02fd017ba13f9f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#global_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#global_unstructured&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc9622697e3c53544bdd5add28b3a2c57c2527ff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.is_pruned(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#is_pruned&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.is_pruned(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#is_pruned&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="96abbe2d8d461fb8bd2ee1faf7eb77f02db4de18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.l1_unstructured(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#l1_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.l1_unstructured(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#l1_unstructured&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b7d651eb7a59a2dc63e2fc226de440a5894c6c35" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#ln_structured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#ln_structured&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd20f8272b0cdf40603ce7846f3ecd5e93a2297d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.random_structured(module, name, amount, dim)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_structured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.random_structured(module, name, amount, dim)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_structured&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="71942a64f4dce21658109edbb7ad07b1ac54058d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.random_unstructured(module, name, amount)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_unstructured&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.random_unstructured(module, name, amount)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#random_unstructured&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1e7eeb03ba5f784fbc103d766bcd491a80e68b6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.prune.remove(module, name)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#remove&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.prune.remove(module, name)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/prune.html#remove&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="593e2b8edbe9f814474088519b5d51467a8371e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.remove_spectral_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.remove_spectral_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="28b96908ea924c1b164eb56c90b4f06cbbbb7ac7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.remove_weight_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.remove_weight_norm(module: T_module, name: str = 'weight') &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#remove_weight_norm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f70a4d724e38fb65885f35aa43b8a140dbffcef2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_padded_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_padded_sequence&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2e6fdd6a0dda25c6ab4d31cc62e42257e961a19a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pack_sequence&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="044c8f76ea2d98a2cab6ae17546088498a4b752d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_packed_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_packed_sequence&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a814f84114cb8c05fa4c22be341926d765e90456" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_sequence&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/rnn.html#pad_sequence&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="881e9c21d63a899692b7a30837ad6eba7a8c7f16" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.spectral_norm(module: T_module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None) &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#spectral_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.spectral_norm(module: T_module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None) &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/spectral_norm.html#spectral_norm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26317539c048e00421573ffe352a58a215f08fe6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dcc88b942534bdb73dedece8f4a1cbf8e3e89bb5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.nn.utils.weight_norm(module: T_module, name: str = 'weight', dim: int = 0) &amp;rarr; T_module&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#weight_norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.nn.utils.weight_norm(module: T_module, name: str = 'weight', dim: int = 0) &amp;rarr; T_module&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/utils/weight_norm.html#weight_norm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2501cdd2c8ea16585c06298503d128b6a0ffd1cd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#norm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#norm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="acdcacd99d319a55aa1c12b5685ea208060bb46d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.export(model, args, f, export_params=True, verbose=False, training=TrainingMode.EVAL, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.export(model, args, f, export_params=True, verbose=False, training=TrainingMode.EVAL, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="03322601d6585b4a92619debfccba83a650e2502" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.export_to_pretty_string(*args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export_to_pretty_string&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.export_to_pretty_string(*args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#export_to_pretty_string&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e25b4aec2e26a0048d40d93bf92b951ca79caac4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.is_in_onnx_export()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#is_in_onnx_export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.is_in_onnx_export()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#is_in_onnx_export&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5a40567375223347d13ca6f163cb9d650229468a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.operators.shape_as_tensor(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx/operators.html#shape_as_tensor&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.operators.shape_as_tensor(x)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx/operators.html#shape_as_tensor&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="976d3f529500cdddc27096243ac8d2a42aa02e7a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#register_custom_op_symbolic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#register_custom_op_symbolic&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="815d0399a34390b7363b99b8598c5d34a64d4a72" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.onnx.select_model_mode_for_export(model, mode)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#select_model_mode_for_export&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.onnx.select_model_mode_for_export(model, mode)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/onnx.html#select_model_mode_for_export&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4462d16485f0d07cc9e383ed821f0cff226a257b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.optim.lr_scheduler&lt;/code&gt; provides several methods to adjust the learning rate based on the number of epochs. &lt;a href=&quot;#torch.optim.lr_scheduler.ReduceLROnPlateau&quot;&gt;&lt;code&gt;torch.optim.lr_scheduler.ReduceLROnPlateau&lt;/code&gt;&lt;/a&gt; allows dynamic learning rate reducing based on some validation measurements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2bb3e56eec2328917bbfdf27d9f0762230bc816" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.optim.swa_utils&lt;/code&gt; implements Stochastic Weight Averaging (SWA). In particular, &lt;code&gt;torch.optim.swa_utils.AveragedModel&lt;/code&gt; class implements SWA models, &lt;code&gt;torch.optim.swa_utils.SWALR&lt;/code&gt; implements the SWA learning rate scheduler and &lt;code&gt;torch.optim.swa_utils.update_bn()&lt;/code&gt; is a utility function used to update SWA batch normalization statistics at the end of training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59d28dfd09f75864c49c9ad2a7227c7d7578174a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.pca_lowrank(A, q=None, center=True, niter=2)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#pca_lowrank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.pca_lowrank(A, q=None, center=True, niter=2)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#pca_lowrank&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="03de177ec561a49fdd476ac650a4389100c73080" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.preserve_format&lt;/code&gt;: Used in functions like &lt;code&gt;clone&lt;/code&gt; to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow &lt;code&gt;torch.contiguous_format&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.preserve_format&lt;/code&gt; : 입력 텐서의 메모리 형식을 보존하기 위해 &lt;code&gt;clone&lt;/code&gt; 과 같은 함수에서 사용됩니다 . 입력 텐서가 겹치지 않는 고밀도 메모리에 할당되면 출력 텐서 스트라이드가 입력에서 복사됩니다. 그렇지 않으면 출력 스트라이드가 &lt;code&gt;torch.contiguous_format&lt;/code&gt; 을 따릅니다 .</target>
        </trans-unit>
        <trans-unit id="a83f557a11f6ca5ab7329459da86d1ce796df485" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, prehook=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#add_observer_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="728fa6ae5cb87dc3322e03075d12565042433a15" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.add_quant_dequant(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#add_quant_dequant&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.quantization.add_quant_dequant(module)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#add_quant_dequant&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b969265672f0bba5dd0c91fd557ac9c1d8a665ca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#convert&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2dc3065e862d5e054534d190764d3544bf4a8f71" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.default_eval_fn(model, calib_data)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization.html#default_eval_fn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42a69de442217228e1d1770af1a922f1dfd9ed50" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=&amp;lt;function fuse_known_modules&amp;gt;)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/fuse_modules.html#fuse_modules&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22e68c804f152380cc736924a1484680b4628c50" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.get_observer_dict(mod, target_dict, prefix='')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#get_observer_dict&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a599907017a6a7b9448c2389eba3b01b9bf630e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prehook=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#prepare&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="403dc01a998280829aedbf9d3716412242dc8e8d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.prepare_qat(model, mapping=None, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#prepare_qat&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c48d8978e93948df8bbc8deb48627871a9a7b56" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#propagate_qconfig_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07f1f7f6e48628b5d5f7fd138023c8dadee8efbe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#quantize&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b082813667288f034ea63136c83f0ad6b81f6ba2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#quantize_dynamic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5567b54dea1b9f922f788990e9d44c1ee43fa76b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.quantize_qat(model, run_fn, run_args, inplace=False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#quantize_qat&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="198184108219ecd40af802fd7c9872b73ca41452" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.quantization.swap_module(mod, mapping)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/quantization/quantize.html#swap_module&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e36fc33b804c7e7618293ba965fefc2823c5da3a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#fork_rng&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a64cbea6d9928d6bba2396b56357127979dc94a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.get_rng_state() &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#get_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="453085b35dac7c86fc188bb289251935e4fe77b3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.initial_seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#initial_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6be31d7012289c803c5d2164d332a3aabe51b340" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.manual_seed(seed) &amp;rarr; torch._C.Generator&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#manual_seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a12f1c857c5a2f052ba20091dd4c0846153d507a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cb113d7a58d109d6fbef7a098c826df8efe232" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.random.set_rng_state(new_state) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#set_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb81152ea7d6334b6a16b9c3108f06ba82952254" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.save(obj, f: Union[str, os.PathLike, BinaryIO], pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, pickle_protocol=2, _use_new_zipfile_serialization=True) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#save&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.save(obj, f: Union[str, os.PathLike, BinaryIO], pickle_module=&amp;lt;module 'pickle' from '/opt/conda/lib/python3.6/pickle.py'&amp;gt;, pickle_protocol=2, _use_new_zipfile_serialization=True) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/serialization.html#save&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="149a91842a887af3527d7028eea82d5c2bd4684c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.scatter_add_()&lt;/code&gt; when called on a CUDA tensor</source>
          <target state="translated">&lt;code&gt;torch.scatter_add_()&lt;/code&gt; CUDA 텐서에서 호출 될 때 torch.scatter_add_ ()</target>
        </trans-unit>
        <trans-unit id="7829623f67d02ddc28ca91d4d4b46f343d79b572" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.seed() &amp;rarr; int&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#seed&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.seed() &amp;rarr; int&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#seed&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="145f4576eacb498e971a1e27b7989cde7e848da6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_default_dtype(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_dtype&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_default_dtype(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_dtype&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="10d4c72bdc704e59d50cf2910ff7471a3ab49eca" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_default_tensor_type(t)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_tensor_type&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_default_tensor_type(t)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_default_tensor_type&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="65c48c613ed4b2f78ed6e02a56b179139c405127" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_deterministic(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_deterministic&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_deterministic(d)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch.html#set_deterministic&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1a9084271510ec6bbc33b1814e39096e72ad5953" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_tensor_str.html#set_printoptions&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_tensor_str.html#set_printoptions&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="13ffc0d262be2520fdff6d4a0f52ba63324ddfab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.set_rng_state(new_state) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#set_rng_state&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.set_rng_state(new_state) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/random.html#set_rng_state&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ed79abf5eac107985737d970652704fa05dc0ade" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.solve(B, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;B, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;solution, LU&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch.solve(B, A)&lt;/code&gt; 는 2D 입력 &lt;code&gt;B, A&lt;/code&gt; 또는 2D 행렬의 배치 인 입력을 받을 수 있습니다 . 입력이 배치 인 경우 배치 된 출력 &lt;code&gt;solution, LU&lt;/code&gt; 를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b665df2b2d96bef5eb69cda0cd781918b869fa3b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.addmm(mat: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, beta: float = 1.0, alpha: float = 1.0) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#addmm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.addmm(mat: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, beta: float = 1.0, alpha: float = 1.0) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#addmm&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aa2dfd43ccd4a1a35fc8de1d1304f193c8e2a864" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.mm(mat1: torch.Tensor, mat2: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#mm&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.mm(mat1: torch.Tensor, mat2: torch.Tensor) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#mm&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7727e69ca5233c84e597b026bcaedcd72dec04d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#sum&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/sparse.html#sum&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6e65754e6516685feae5bc613180e4719d49f3f8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.split(tensor, split_size_or_sections, dim=0)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#split&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.split(tensor, split_size_or_sections, dim=0)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#split&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e35490d70ccd40103394665a6c8f64a5fda9d8c1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None) &amp;rarr; torch.Tensor&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#stft&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None) &amp;rarr; torch.Tensor&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#stft&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3a3816ac7e969e46494429240e45f840ebeb330a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.strided&lt;/code&gt; represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. These tensors provide multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</source>
          <target state="translated">&lt;code&gt;torch.strided&lt;/code&gt; 는 조밀 한 Tensor를 나타내며 가장 일반적으로 사용되는 메모리 레이아웃입니다. 각 strided 텐서는 관련 데이터를 보유하는 &lt;code&gt;torch.Storage&lt;/code&gt; 가 있습니다. 이러한 텐서 는 스토리지에 대한 다차원적인 &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;스트라이드&lt;/a&gt; 뷰를 제공합니다. 스트라이드는 정수의 목록입니다. k 번째 스트라이드는 Tensor의 k 차원에서 한 요소에서 다음 요소로 이동하는 데 필요한 메모리의 점프를 나타냅니다. 이 개념은 많은 텐서 연산을 효율적으로 수행 할 수있게합니다.</target>
        </trans-unit>
        <trans-unit id="562d195ae00895df7b44cc1509c8dc7828657990" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.svd_lowrank(A, q=6, niter=2, M=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#svd_lowrank&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.svd_lowrank(A, q=6, niter=2, M=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/_lowrank.html#svd_lowrank&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="95d8fe6a396e85b79844a3279acdb3eba5b10f87" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.tensordot(a, b, dims=2)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#tensordot&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.tensordot(a, b, dims=2)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/functional.html#tensordot&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6a2b543d94504add4daf1a3eaf97669036641b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.triangular_solve(b, A)&lt;/code&gt; can take in 2D inputs &lt;code&gt;b, A&lt;/code&gt; or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs &lt;code&gt;X&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.triangular_solve(b, A)&lt;/code&gt; 는 2D 입력 &lt;code&gt;b, A&lt;/code&gt; 또는 2D 행렬의 배치 인 입력을 받을 수 있습니다 . 입력이 배치 인 경우 배치 된 출력을 반환합니다 &lt;code&gt;X&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4d3e0234b25f4c9163c86f9066ba6ade5028b7ab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.bottleneck&lt;/code&gt; is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch&amp;rsquo;s autograd profiler.</source>
          <target state="translated">&lt;code&gt;torch.utils.bottleneck&lt;/code&gt; 은 프로그램의 병목 현상을 디버깅하기위한 초기 단계로 사용할 수있는 도구입니다. Python 프로파일 러 및 PyTorch의 autograd 프로파일 러를 사용하여 스크립트 실행을 요약합니다.</target>
        </trans-unit>
        <trans-unit id="9e9227f91a147c6bfddb1ea807ad178da61c2f01" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.checkpoint.checkpoint(function, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.checkpoint.checkpoint(function, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9f35c5fbe34347d80748ad19cf587e88d314a446" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint_sequential&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/checkpoint.html#checkpoint_sequential&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="82743a1bc0d193a8d5c5ef819ec1fa2a75d3a567" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.BuildExtension(*args, **kwargs) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#BuildExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.BuildExtension(*args, **kwargs) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#BuildExtension&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e1fcb6bd9919299ae2ddc98ca71e5ceaf224bcb1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CUDAExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CUDAExtension&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bacc6afc48e6da0551f4f2b3cb8604b66cb184b0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CppExtension&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#CppExtension&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fa82ce120a2fb2cab4f7a383ceb7f5257beb9cb5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) &amp;rarr; bool&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) &amp;rarr; bool&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="04175775569bb5f47df2787c65e8108be88d672c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.include_paths(cuda: bool = False) &amp;rarr; List[str]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#include_paths&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.include_paths(cuda: bool = False) &amp;rarr; List[str]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#include_paths&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2d2cc603ad32cc623c7afe161c11ea47e80c35fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.is_ninja_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#is_ninja_available&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.is_ninja_available()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#is_ninja_available&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="127692b7ee4252253192d5f2c1ec1661e298d13c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.load(name, sources: List[str], extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda: Optional[bool] = None, is_python_module=True, keep_intermediates=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.load(name, sources: List[str], extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda: Optional[bool] = None, is_python_module=True, keep_intermediates=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bc490ba01580aa87b6738971cbd6c2f4a833c59c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load_inline&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#load_inline&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="282b9c63f989793b3e3cc7e3e2e6b74a269f4472" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.cpp_extension.verify_ninja_availability()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.cpp_extension.verify_ninja_availability()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5c183276893379d98326bdb057810664a413feac" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.data.get_worker_info()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/_utils/worker.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.data.get_worker_info()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/_utils/worker.html#get_worker_info&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8e7828579b817436d5b119f93cfa5535f7ca821e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.data.random_split(dataset: torch.utils.data.dataset.Dataset[T], lengths: Sequence[int], generator: Optional[torch._C.Generator] = &amp;lt;torch._C.Generator object&amp;gt;) &amp;rarr; List[torch.utils.data.dataset.Subset[T]]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/data/dataset.html#random_split&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87bb3cae7ea056553e89c9188b938e7264f5dd5d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist: Set[torch._C.MobileOptimizerType] = None, preserved_methods: List[AnyStr] = None, backend: str = 'CPU')&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/mobile_optimizer.html#optimize_for_mobile&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist: Set[torch._C.MobileOptimizerType] = None, preserved_methods: List[AnyStr] = None, backend: str = 'CPU')&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/utils/mobile_optimizer.html#optimize_for_mobile&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0735be3015f610c6bc97e587de840634628e8077" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torch.where(condition)&lt;/code&gt; is identical to &lt;code&gt;torch.nonzero(condition, as_tuple=True)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch.where(condition)&lt;/code&gt; 은 &lt;code&gt;torch.nonzero(condition, as_tuple=True)&lt;/code&gt; 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="55e6ef83c7ef85f702c91d8eef5ea0dbd8c749b7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.alexnet(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/alexnet.html#alexnet&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.alexnet(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/alexnet.html#alexnet&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0d38d82c4a2980d1cd54f65f24d369937605975c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet121(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet121&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet121(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet121&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5d48becd24b6a141911d63cbcfa4b5388d0e2f2a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet161(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet161&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet161(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet161&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c66a106d591302ae9a23564d21b9f0fecca36a45" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet169(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet169&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet169(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet169&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a4dc470d2c9b1dd63a8b3c3a2f09a2b731ee1ff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.densenet201(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet201&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.densenet201(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/densenet.html#densenet201&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d37101a2a2ce0b122b3186b37ea5f3eebbf7af88" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fddcba641d93f705c38ea5e99e37277ffb808f18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, num_keypoints=17, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/keypoint_rcnn.html#keypointrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, num_keypoints=17, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/keypoint_rcnn.html#keypointrcnn_resnet50_fpn&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ca21903101f18b0557dcbc8fea49917fe47b38fc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/mask_rcnn.html#maskrcnn_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=3, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/mask_rcnn.html#maskrcnn_resnet50_fpn&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8321d00bdd9e79fbbae3bed723d0da2d8ff44e18" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/retinanet.html#retinanet_resnet50_fpn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/detection/retinanet.html#retinanet_resnet50_fpn&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e68c6f2f62746247926299a2e294d2b695793904" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.googlenet(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/googlenet.html#googlenet&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.googlenet(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/googlenet.html#googlenet&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c29cab4aa7a85507392da05a8545872aeefa41c6" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.inception_v3(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/inception.html#inception_v3&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.inception_v3(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/inception.html#inception_v3&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f0c9417db1439ddf7338a83567afce51a263b5dd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_5&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d47807e0f8165cf93963dfc4dc78e77f742858e1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet0_75(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_75&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet0_75(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet0_75&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f5a9e5d5450b4dafb844ae224f4ad180dd226121" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_0&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="77b69bd7be274f38453186938001d277495a6c2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mnasnet1_3(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_3&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mnasnet1_3(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mnasnet.html#mnasnet1_3&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="969edc85d9e553749ddbb016e31fdad62783c095" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.mobilenet_v2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mobilenet.html#mobilenet_v2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.mobilenet_v2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/mobilenet.html#mobilenet_v2&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8bffa4c41d6459f1920f6b09592d01a2d8bed629" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet101(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet101(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet101&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8f51b7405b05ff1069b3eed30d5d1a4b4a048135" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet152(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet152&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet152(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet152&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="58b4a120cb5b9ef8aa6791731d8b227114babaf4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet18&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6c548ae7c119869cb346706a64804cd9a2772320" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet34(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet34&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet34(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet34&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="70c0819d42401f678755dd1664c6da919188b89e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnet50(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnet50(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnet50&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bcdda78061e32c5045ff0ce9dde8acb096118a2f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnext101_32x8d(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext101_32x8d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnext101_32x8d(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext101_32x8d&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4cbcacd435d5999826e0ac993eac4f3fc585eeb4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.resnext50_32x4d(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext50_32x4d&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.resnext50_32x4d(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#resnext50_32x4d&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c6417c808ce60d23a185910dca6539019af3ba56" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet101&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1d366590337e483500734a3650d1ef622765c7eb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#deeplabv3_resnet50&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69c183fd9ad3fbdedaca14cf2875725c54150e77" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.fcn_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet101&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.fcn_resnet101(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet101&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dadae370f707b7834deb997ef740ebf2926cec26" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet50&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=21, aux_loss=None, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/segmentation/segmentation.html#fcn_resnet50&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="985d1d71995c5a858dab5d7f67674d973e8f084d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x0_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x0_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x0_5&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d06ebbf7a18ac9324d20801b61faff9eb323112e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_0&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69605c4002be97c73eb40b6b5dac9238f0a6f129" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_5&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x1_5(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x1_5&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2bb85f32d69a2c716a4157554b61ab0a69d223d2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x2_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.shufflenet_v2_x2_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/shufflenetv2.html#shufflenet_v2_x2_0&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01dc12333c85efbfdfa76203ff11e0218683f488" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.squeezenet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_0&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.squeezenet1_0(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_0&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9c04456665e568beadaf956c8033bee8db4ce676" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.squeezenet1_1(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_1&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.squeezenet1_1(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/squeezenet.html#squeezenet1_1&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1cd5db2e4fbd76c79a9cd5f06e13284c03c17302" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg11(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg11(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50ac0ac46bd916f5d37b20fddc28e9c32dc160d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg11_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg11_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg11_bn&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7519bb1b3c7dacdd2e7ab4174d31903bddfe4872" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg13(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg13(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ec4029edda490c50f9dfa4eaf54f78b869d1dedb" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg13_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg13_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg13_bn&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d89c3bd8e0058098ec4b822cf6070e6491c1046f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg16(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg16(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b33e5eec705427e4d3f1ab25e07ed270b3ac99ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg16_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg16_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg16_bn&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="563b55e8c50e9495de20d93fa1609d3ca5095650" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg19(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg19(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8d2010441ce8fd04d6bccd5dc17972367f6612e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.vgg19_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19_bn&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.vgg19_bn(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/vgg.html#vgg19_bn&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="248d2d3c5cde4f33b37185cfc2f168d45a6085e0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.mc3_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#mc3_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.mc3_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#mc3_18&quot;&gt;[출처]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="049d72606ad866c4a386e75f3387cf447c24e806" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.r2plus1d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r2plus1d_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.r2plus1d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r2plus1d_18&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1f5b003328cdb0ee7e5456320bb4679a846567da" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.video.r3d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r3d_18&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.video.r3d_18(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/video/resnet.html#r3d_18&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="976b5b89506d0a20776447fa2c6b030b688bd840" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.wide_resnet101_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet101_2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.wide_resnet101_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet101_2&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="883785b05ab2fa730630f3ad88bcabd0f8a7b4f0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;torchvision.models.wide_resnet50_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet50_2&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;torchvision.models.wide_resnet50_2(pretrained=False, progress=True, **kwargs)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torchvision/models/resnet.html#wide_resnet50_2&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b402fd70c974e684585008e4fa236d052905e106" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;total_average()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.total_average&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;total_average()&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/autograd/profiler.html#profile.total_average&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a147531339c74e3b87db4d26b7214df4792298dc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;total_length&lt;/code&gt; is useful to implement the &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; pattern in a &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; wrapped in &lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;this FAQ section&lt;/a&gt; for details.</source>
          <target state="translated">&lt;code&gt;total_length&lt;/code&gt; 는 &lt;a href=&quot;torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; 에&lt;/a&gt; 래핑 된 &lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 에서 &lt;code&gt;pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence&lt;/code&gt; 패턴 을 구현하는 데 유용합니다 . 자세한 내용은 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#pack-rnn-unpack-with-data-parallelism&quot;&gt;이 FAQ 섹션&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="7fbb2269f1128397ff52b8755f83fcf30131e444" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;train(mode: bool = True) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.train&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;train(mode: bool = True) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.train&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="551f59eef5367ebe51f5af3c3b6b107cf81e448e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;transform_to(constraint)&lt;/code&gt; looks up a not-necessarily bijective &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; from &lt;code&gt;constraints.real&lt;/code&gt; to the given &lt;code&gt;constraint&lt;/code&gt;. The returned transform is not guaranteed to implement &lt;code&gt;.log_abs_det_jacobian()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17490d06aec9bb591bde79be08f472ba510005d3" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;true&lt;/code&gt; if &lt;code&gt;key&lt;/code&gt; was deleted, otherwise &lt;code&gt;false&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;true&lt;/code&gt; &lt;code&gt;key&lt;/code&gt; 가 삭제 된 경우 true , 그렇지 않으면 &lt;code&gt;false&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="73de5004ed63d179d11e7c607cd226ac72fef763" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;type(dst_type: Union[torch.dtype, str]) &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.type&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;type(dst_type: Union[torch.dtype, str]) &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.type&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b726d74ca27828505ae027190d9488e64e8a432c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unflatten(dim, sizes)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unflatten&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unflatten(dim, sizes)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unflatten&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bb232cf62834f167804d6c86e5ae22859b7db331" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unflattened_size&lt;/code&gt; is the new shape of the unflattened dimension of the tensor and it can be a &lt;code&gt;tuple&lt;/code&gt; of ints or &lt;code&gt;torch.Size&lt;/code&gt; for &lt;code&gt;Tensor&lt;/code&gt; input or a &lt;code&gt;NamedShape&lt;/code&gt; (tuple of &lt;code&gt;(name, size)&lt;/code&gt; tuples) for &lt;code&gt;NamedTensor&lt;/code&gt; input.</source>
          <target state="translated">&lt;code&gt;unflattened_size&lt;/code&gt; 는 텐서의 평탄화 차원의 새로운 형태이며이 될 수 &lt;code&gt;tuple&lt;/code&gt; 의 int 또는 &lt;code&gt;torch.Size&lt;/code&gt; 위한 &lt;code&gt;Tensor&lt;/code&gt; 입력 또는 &lt;code&gt;NamedShape&lt;/code&gt; (의 튜플 &lt;code&gt;(name, size)&lt;/code&gt; 에 대한 터플) &lt;code&gt;NamedTensor&lt;/code&gt; 의 입력.</target>
        </trans-unit>
        <trans-unit id="114ecd93b94acf20f3f824c124da10585acafbf1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unify(A, B)&lt;/code&gt; determines which of the names &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; to propagate to the outputs. It returns the more &lt;em&gt;specific&lt;/em&gt; of the two names, if they match. If the names do not match, then it errors.</source>
          <target state="translated">&lt;code&gt;unify(A, B)&lt;/code&gt; 는 출력에 전파 할 이름 &lt;code&gt;A&lt;/code&gt; 와 &lt;code&gt;B&lt;/code&gt; 를 결정 합니다. 일치하는 경우 두 이름 중 더 &lt;em&gt;구체적인&lt;/em&gt; 이름을 반환합니다 . 이름이 일치하지 않으면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="f81a8067f75099cdee88dc8df395ee2a70a06955" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unique(sorted=True, return_inverse=False, return_counts=False, dim=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unique(sorted=True, return_inverse=False, return_counts=False, dim=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f19f4fd7da0a567778731041cc786315cffe617" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unique_consecutive(return_inverse=False, return_counts=False, dim=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique_consecutive&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unique_consecutive(return_inverse=False, return_counts=False, dim=None)&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/tensor.html#Tensor.unique_consecutive&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a6af85d8db1a5e510fc12dff87d706e58bc84cda" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;unscale_(optimizer)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.unscale_&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;unscale_(optimizer)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.unscale_&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8da2e713f043aee7915ced857730729c9fc23a12" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update(modules: Mapping[str, torch.nn.modules.module.Module]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.update&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;update(modules: Mapping[str, torch.nn.modules.module.Module]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.update&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d0ffe58e4fe60f3499b727a887fb475b5a1fb536" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update(new_scale=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.update&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;update(new_scale=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.update&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4d3c990fce28e84265072d86fa81e98efcd00c95" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update(parameters: Mapping[str, Parameter]) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.update&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;update(parameters: Mapping[str, Parameter]) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.update&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1fdaa5beb50db660bd2bd28cf8e381092fe150e5" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update_bn()&lt;/code&gt; applies the &lt;code&gt;swa_model&lt;/code&gt; to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fc96b3d4184105fae6382899b26dd838a31d453" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update_bn()&lt;/code&gt; assumes that each batch in the dataloader &lt;code&gt;loader&lt;/code&gt; is either a tensors or a list of tensors where the first element is the tensor that the network &lt;code&gt;swa_model&lt;/code&gt; should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the &lt;code&gt;swa_model&lt;/code&gt; by doing a forward pass with the &lt;code&gt;swa_model&lt;/code&gt; on each element of the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8b7da4e120bf3a889b8545c0710e21c6f78d5ba" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;update_bn()&lt;/code&gt; is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader &lt;code&gt;loader&lt;/code&gt; at the end of training:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4d99c2f207829a30f2ddddb647b4a8b864fd17f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;use_external_data_format&lt;/code&gt; argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument &amp;lsquo;f&amp;rsquo; must be a string specifying the location of the model.</source>
          <target state="translated">&lt;code&gt;use_external_data_format&lt;/code&gt; 내보내기 API의 use_external_data_format 인수를 사용하면 ONNX 외부 데이터 형식으로 모델을 내보낼 수 있습니다. 이 옵션을 활성화하면 내보내기에서 일부 모델 매개 변수를 ONNX 파일 자체가 아닌 외부 바이너리 파일에 저장합니다. 이러한 외부 바이너리 파일은 ONNX 파일과 동일한 위치에 저장됩니다. 인수 'f'는 모델의 위치를 ​​지정하는 문자열이어야합니다.</target>
        </trans-unit>
        <trans-unit id="9356c0eda1c275025f5aa20e4ff2a7158f04f8d7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;use_ninja&lt;/code&gt; (bool): If &lt;code&gt;use_ninja&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard &lt;code&gt;setuptools.build_ext&lt;/code&gt;. Fallbacks to the standard distutils backend if Ninja is not available.</source>
          <target state="translated">&lt;code&gt;use_ninja&lt;/code&gt; (bool) : &lt;code&gt;use_ninja&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; (기본값)이면 Ninja 백엔드를 사용하여 빌드를 시도합니다. Ninja는 표준 &lt;code&gt;setuptools.build_ext&lt;/code&gt; 에 비해 컴파일 속도를 크게 높 입니다. Ninja를 사용할 수없는 경우 표준 distutils 백엔드로 대체합니다.</target>
        </trans-unit>
        <trans-unit id="1254a0cf82203e9a1531fc62d7d0d216e9e743f7" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;values() &amp;rarr; Iterable[Parameter]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.values&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;values() &amp;rarr; Iterable[Parameter]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ParameterDict.values&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="96bfd43ff7e24163f6d4c88a8036d233913ce8fe" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;values() &amp;rarr; Iterable[torch.nn.modules.module.Module]&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.values&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;values() &amp;rarr; Iterable[torch.nn.modules.module.Module]&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/container.html#ModuleDict.values&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b6f933da6523428564bd69fe2e08dcce328d36ea" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;var.data&lt;/code&gt; is the same thing as &lt;code&gt;tensor.data&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9f1fb27f5f6b909562e8dd71484afe41d908a8c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;variance&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.variance&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;variance&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.variance&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d3c6d4db027b065490003c21e6a53e0577fde16a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;variance&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.variance&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;variance&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/distributions/von_mises.html#VonMises.variance&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="104e14056b814916b6776efe34cc558abcb82dbd" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait() &amp;rarr; T&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.wait&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;wait() &amp;rarr; T&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/futures.html#Future.wait&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="516fb2fe6efc6e35e85e90b516645bccab59738b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait()&lt;/code&gt; - will block the process until the operation is finished.</source>
          <target state="translated">&lt;code&gt;wait()&lt;/code&gt; -작업이 완료 될 때까지 프로세스를 차단합니다.</target>
        </trans-unit>
        <trans-unit id="941139d436ab4f418d83f1d8c7ec9b9119cbf52b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait()&lt;/code&gt; - will block the process until the operation is finished. &lt;code&gt;is_completed()&lt;/code&gt; is guaranteed to return True once it returns.</source>
          <target state="translated">&lt;code&gt;wait()&lt;/code&gt; -작업이 완료 될 때까지 프로세스를 차단합니다. &lt;code&gt;is_completed()&lt;/code&gt; 는 반환되면 True를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7428aaf08eb6b2d0df0960b6ce19969014e0d8f9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait(stream=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.wait&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;wait(stream=None)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Event.wait&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="290902cc2d859b8d8e7297e1f95d573d58437bab" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait_event(event)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.wait_event&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;wait_event(event)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.wait_event&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b1db9e40ec51a5f6b99fcf1bfebd0eeeb77f054" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;wait_stream(stream)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.wait_stream&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;wait_stream(stream)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/cuda/streams.html#Stream.wait_stream&quot;&gt;[source]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75193eb58fde7a23d585e351a7da7779cc2d15b2" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;weight&lt;/code&gt; (Tensor): the learnable weights of the module of shape &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;weight&lt;/code&gt; (텐서) : 모양 모듈의 학습 가능한 가중치 &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a60b5944e5c5872c821c93c5da8c56e577b4b1c8" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;window&lt;/code&gt; can be a 1-D tensor of size &lt;code&gt;win_length&lt;/code&gt;, e.g., from &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt;&lt;code&gt;torch.hann_window()&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;window&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as if having</source>
          <target state="translated">&lt;code&gt;window&lt;/code&gt; 는 크기가 &lt;code&gt;win_length&lt;/code&gt; 인 1D 텐서 일 수 있습니다 &lt;a href=&quot;torch.hann_window#torch.hann_window&quot;&gt; &lt;code&gt;torch.hann_window()&lt;/code&gt; &lt;/a&gt; 예 : torch.hann_window ()) . 경우 &lt;code&gt;window&lt;/code&gt; 없는 &lt;code&gt;None&lt;/code&gt; (기본값), 그것은 가진 것처럼 취급된다</target>
        </trans-unit>
        <trans-unit id="fcb35e51fe3891a319d8cca2cc7d08a9312f5fd0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;zero_grad(set_to_none: bool = False) &amp;rarr; None&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.zero_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;zero_grad(set_to_none: bool = False) &amp;rarr; None&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/nn/modules/module.html#Module.zero_grad&quot;&gt;[소스]&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0a6dd8772a486c6d3463faf126f8294a404e3a6a" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;zero_grad(set_to_none: bool = False)&lt;/code&gt;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/_modules/torch/optim/optimizer.html#Optimizer.zero_grad&quot;&gt;[source]&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52c5689e7634a1ec117b5ce27ce982c0c2976329" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;zero_point&lt;/code&gt; specifies the quantized value to which 0 in floating point maps to</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e228d56108279e5de96783fa1fcf187bda5858b" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Module)&lt;/em&gt; &amp;ndash; Tuple containing a name and child module</source>
          <target state="translated">&lt;em&gt;(문자열, 모듈)&lt;/em&gt; &amp;ndash; 이름과 자식 모듈을 포함하는 튜플</target>
        </trans-unit>
        <trans-unit id="5fb3c9a95d7120f0c30ae272a854d5feba5d48b6" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Module)&lt;/em&gt; &amp;ndash; Tuple of name and module</source>
          <target state="translated">&lt;em&gt;(문자열, 모듈)&lt;/em&gt; &amp;ndash; 이름 및 모듈의 튜플</target>
        </trans-unit>
        <trans-unit id="319263d4f6bb9e6d32586c5cff620cbc3fa67237" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, Parameter)&lt;/em&gt; &amp;ndash; Tuple containing the name and parameter</source>
          <target state="translated">&lt;em&gt;(문자열, 매개 변수)&lt;/em&gt; &amp;ndash; 이름과 매개 변수를 포함하는 튜플</target>
        </trans-unit>
        <trans-unit id="dc72ec06d2fb8eafc561c043b35fadba57166206" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;(string, torch.Tensor)&lt;/em&gt; &amp;ndash; Tuple containing the name and buffer</source>
          <target state="translated">&lt;em&gt;(string, torch.Tensor)&lt;/em&gt; &amp;ndash; 이름과 버퍼를 포함하는 튜플</target>
        </trans-unit>
        <trans-unit id="a754dfd4e3d629d6e648310d555fb7a646535624" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Beta:&lt;/em&gt; Features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.</source>
          <target state="translated">&lt;em&gt;베타 :&lt;/em&gt; API가 사용자 피드백에 따라 변경 될 수 있거나, 성능이 향상되어야하거나, 운영자에 대한 적용 범위가 아직 완료되지 않았기 때문에 기능이 &lt;em&gt;베타&lt;/em&gt; 로 태그 지정되었습니다. 베타 기능의 경우 Stable 분류를 통해 기능을 확인하기 위해 노력하고 있습니다. 그러나 우리는 이전 버전과의 호환성을 약속하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0a298939dfd8866323bf58103d52da9a00fff3eb" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Module&lt;/em&gt; &amp;ndash; a child module</source>
          <target state="translated">&lt;em&gt;모듈&lt;/em&gt; &amp;ndash; 하위 모듈</target>
        </trans-unit>
        <trans-unit id="55c1945ebbb505a29f113e4158c303559a846bd3" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Module&lt;/em&gt; &amp;ndash; a module in the network</source>
          <target state="translated">&lt;em&gt;모듈&lt;/em&gt; &amp;ndash; 네트워크의 모듈</target>
        </trans-unit>
        <trans-unit id="7efeeaf6ac232e4e1bd4481ee3ccf55f14e08c1b" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Parameter&lt;/em&gt; &amp;ndash; module parameter</source>
          <target state="translated">&lt;em&gt;매개 변수&lt;/em&gt; &amp;ndash; 모듈 매개 변수</target>
        </trans-unit>
        <trans-unit id="b1299c276055de8aa531b38f8e62b765ac94d8a5" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Prototype:&lt;/em&gt; These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</source>
          <target state="translated">&lt;em&gt;프로토 타입 :&lt;/em&gt; 이러한 기능은 런타임 플래그 뒤에있는 경우를 제외하고는 일반적으로 PyPI 또는 Conda와 같은 바이너리 배포의 일부로 사용할 수 없으며 피드백 및 테스트를위한 초기 단계에 있습니다.</target>
        </trans-unit>
        <trans-unit id="d4f12aa72178917bae1166015f8ad3889662fd74" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Stable:&lt;/em&gt; These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).</source>
          <target state="translated">&lt;em&gt;안정적 :&lt;/em&gt; 이러한 기능은 장기적으로 유지되며 일반적으로 문서에 큰 성능 제한이나 공백이 없어야합니다. 또한 이전 버전과의 호환성을 유지할 것으로 기대합니다 (단, 주요 변경 사항이 발생할 수 있으며 미리 한 번의 릴리스가 제공 될 예정입니다).</target>
        </trans-unit>
        <trans-unit id="058a377bedcdaeade66857847812a0e554998ab7" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;returned index satisfies&lt;/em&gt;</source>
          <target state="translated">&lt;em&gt;반환 된 색인이 만족함&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="2310781c0d4ea448d2442fafbd9795c680825b97" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;script-based&lt;/em&gt; means that the model you are trying to export is a &lt;a href=&quot;jit&quot;&gt;ScriptModule&lt;/a&gt;. &lt;code&gt;ScriptModule&lt;/code&gt; is the core data structure in &lt;code&gt;TorchScript&lt;/code&gt;, and &lt;code&gt;TorchScript&lt;/code&gt; is a subset of Python language, that creates serializable and optimizable models from PyTorch code.</source>
          <target state="translated">&lt;em&gt;스크립트 기반&lt;/em&gt; 은 내보내려는 모델이 &lt;a href=&quot;jit&quot;&gt;ScriptModule&lt;/a&gt; 임을 의미합니다 . &lt;code&gt;ScriptModule&lt;/code&gt; 은 &lt;code&gt;TorchScript&lt;/code&gt; 의 핵심 데이터 구조 이며 &lt;code&gt;TorchScript&lt;/code&gt; 는 PyTorch 코드에서 직렬화 및 최적화 가능한 모델을 생성하는 Python 언어의 하위 집합입니다.</target>
        </trans-unit>
        <trans-unit id="f6626f7675f0e61b3b27dbc625972f11d257da92" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;torch.Tensor&lt;/em&gt; &amp;ndash; module buffer</source>
          <target state="translated">&lt;em&gt;torch.Tensor&lt;/em&gt; &amp;ndash; 모듈 버퍼</target>
        </trans-unit>
        <trans-unit id="2908cf8505ca66bca3222f57f081464f869918f4" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;trace-based&lt;/em&gt; means that it operates by executing your model once, and exporting the operators which were actually run during this run. This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won&amp;rsquo;t be accurate. Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.) We recommend examining the model trace and making sure the traced operators look reasonable. If your model contains control flows like for loops and if conditions, &lt;em&gt;trace-based&lt;/em&gt; exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run. If you want to export your model with dynamic control flows, you will need to use the &lt;em&gt;script-based&lt;/em&gt; exporter.</source>
          <target state="translated">&lt;em&gt;추적 기반&lt;/em&gt; 은 모델을 한 번 실행하고이 실행 중에 실제로 실행 된 연산자를 내보내는 방식으로 작동 함을 의미합니다. 즉, 모델이 동적 인 경우 (예 : 입력 데이터에 따라 동작 변경) 내보내기가 정확하지 않습니다. 마찬가지로 추적은 특정 입력 크기에 대해서만 유효 할 수 있습니다 (추적시 명시 적 입력이 필요한 이유 중 하나). 모델 추적을 검사하고 추적 된 연산자가 합리적으로 보이는지 확인하는 것이 좋습니다. 모델에 for 루프 및 if 조건과 같은 제어 흐름이 포함 된 경우 &lt;em&gt;추적 기반&lt;/em&gt; 내보내기는 루프를 풀고 if 조건은이 실행과 정확히 동일한 정적 그래프를 내 보냅니다. 동적 제어 흐름을 사용하여 모델을 내보내려면 &lt;em&gt;스크립트 기반&lt;/em&gt; 수출.</target>
        </trans-unit>
        <trans-unit id="f3e32b5c7adc9869a8580e1b9e6a76857d105f7c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; &amp;ndash; For compatibility, may contain the key &lt;code&gt;async&lt;/code&gt; in place of the &lt;code&gt;non_blocking&lt;/code&gt; argument.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; &amp;ndash; 호환성 을 위해 &lt;code&gt;non_blocking&lt;/code&gt; 인수 대신 &lt;code&gt;async&lt;/code&gt; 키를 포함 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="65f37625b43f8177a68af42c34ddcf34a3a157c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; &amp;ndash; For compatibility, may contain the key &lt;code&gt;async&lt;/code&gt; in place of the &lt;code&gt;non_blocking&lt;/code&gt; argument. The &lt;code&gt;async&lt;/code&gt; arg is deprecated.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; &amp;ndash; 호환성 을 위해 &lt;code&gt;non_blocking&lt;/code&gt; 인수 대신 &lt;code&gt;async&lt;/code&gt; 키를 포함 할 수 있습니다 . &lt;code&gt;async&lt;/code&gt; 인수는 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="72bb91ae39a4577d326534e0a366239f4d2f6032" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; (&lt;em&gt;*args&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash; arguments to invoke &lt;code&gt;func&lt;/code&gt; with.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; ( &lt;em&gt;* args &lt;/em&gt;&lt;em&gt;,&lt;/em&gt; ) &amp;ndash; &lt;code&gt;func&lt;/code&gt; 를 호출 할 인수 입니다.</target>
        </trans-unit>
        <trans-unit id="b5847a4bbfe6774c5d419725faf44bb56045bd86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;**kwargs&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the corresponding kwargs for callable &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;** kwargs&lt;/strong&gt; ( &lt;em&gt;선택 사항&lt;/em&gt; ) &amp;ndash; 호출 가능 &lt;code&gt;model&lt;/code&gt; 해당하는 kwargs입니다 .</target>
        </trans-unit>
        <trans-unit id="06aa7e1e95a236279512459a9af7e6f136714af2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*args&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the corresponding args for callable &lt;code&gt;model&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;* args&lt;/strong&gt; ( &lt;em&gt;선택 사항&lt;/em&gt; ) &amp;ndash; 호출 가능 &lt;code&gt;model&lt;/code&gt; 해당하는 인수입니다 .</target>
        </trans-unit>
        <trans-unit id="4786a01430f5e00df37e0a3c12e05a6d1bccb369" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*dims&lt;/strong&gt; (&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; The desired ordering of dimensions</source>
          <target state="translated">&lt;strong&gt;* dims&lt;/strong&gt; ( &lt;em&gt;int ...&lt;/em&gt; ) &amp;ndash; 원하는 차원 순서</target>
        </trans-unit>
        <trans-unit id="3319d9e4baa0f129856cf05f1bd63cb7e15f6a69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*sizes&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; the desired expanded size</source>
          <target state="translated">&lt;strong&gt;* sizes&lt;/strong&gt; ( &lt;em&gt;torch.Size &lt;/em&gt;&lt;em&gt;또는 &lt;/em&gt;&lt;em&gt;int ...&lt;/em&gt; ) &amp;ndash; 원하는 확장 크기</target>
        </trans-unit>
        <trans-unit id="d88e4c4171034ef9674db627d11a0997babe55fc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; One or more tensors with 0, 1, or 2 dimensions.</source>
          <target state="translated">&lt;strong&gt;* tensors&lt;/strong&gt; &amp;ndash; 0, 1 또는 2 차원을 가진 하나 이상의 텐서.</target>
        </trans-unit>
        <trans-unit id="fdd4b81b9fbfd6112e5abe2aed8111a733309e32" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; any number of 1 dimensional tensors.</source>
          <target state="translated">&lt;strong&gt;* tensors&lt;/strong&gt; &amp;ndash; 임의의 수의 1 차원 텐서.</target>
        </trans-unit>
        <trans-unit id="37d44625a57d67ad98c1d08c2860e67b1f2734af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; &amp;ndash; any number of tensors of the same type</source>
          <target state="translated">&lt;strong&gt;* 텐서&lt;/strong&gt; &amp;ndash; 동일한 유형의 텐서 수</target>
        </trans-unit>
        <trans-unit id="9825552f8894023955ee700f28b1368d1ee3483b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;*tensors&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; tensors that have the same size of the first dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="542ee1fa5dcd61ea46e6f051760a45f8429629ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input square matrix of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; ) &amp;ndash; 크기의 입력 정사각형 행렬</target>
        </trans-unit>
        <trans-unit id="66a7223a6944ee5be2aaeedb91548f693b323337" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;텐서&lt;/a&gt; ) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="2ff3d5c63e7af23218d3654b0732721e878d975c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; ) &amp;ndash; 크기의 입력 텐서</target>
        </trans-unit>
        <trans-unit id="beaf7caea311d31a62bf35cffe9c4e562b9e1679" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the input triangular coefficient matrix of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; ) &amp;ndash; 크기의 입력 삼각 계수 행렬</target>
        </trans-unit>
        <trans-unit id="a1fd8981c104831012bef0ab84387ae714973280" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to factor of size</source>
          <target state="translated">&lt;strong&gt;A&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; ) &amp;ndash; 텐서 대 크기 계수</target>
        </trans-unit>
        <trans-unit id="11cfe4616774835a1dba4fea5c0a02f4903b11f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;B&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the input tensor of size</source>
          <target state="translated">&lt;strong&gt;B&lt;/strong&gt; ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt; ) &amp;ndash; 크기의 입력 텐서</target>
        </trans-unit>
        <trans-unit id="1cbcbf4a0ee70b68dac705e32d72b1cc1467e9a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Check names&lt;/strong&gt;: an operator may perform automatic checks at runtime that check that certain dimension names must match.</source>
          <target state="translated">&lt;strong&gt;이름 확인&lt;/strong&gt; : 운영자는 런타임에 특정 차원 이름이 일치해야하는지 확인하는 자동 확인을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e00a3d6d56d0570d3c90d64bf2e85703c34bcf75" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Check names&lt;/strong&gt;: check that the names of the two tensors &lt;em&gt;match&lt;/em&gt;.</source>
          <target state="translated">&lt;strong&gt;이름 확인&lt;/strong&gt; : 두 텐서의 이름이 &lt;em&gt;일치&lt;/em&gt; 하는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="eaa4895ce797f363fa068fb68e3589cf17ee5901" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Conv packed params hoisting&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::HOIST_CONV_PACKED_PARAMS&lt;/code&gt;): This optimization pass moves convolution packed params to the root module, so that the convolution structs can be deleted. This decreases model size without impacting numerics.</source>
          <target state="translated">&lt;strong&gt;Conv packed params hoisting&lt;/strong&gt; (블랙리스트 옵션 &lt;code&gt;MobileOptimizerType::HOIST_CONV_PACKED_PARAMS&lt;/code&gt; ) :이 최적화 패스는 convolution 묶음 매개 변수를 루트 모듈로 이동하여 convolution 구조체를 삭제할 수 있습니다. 이것은 숫자에 영향을주지 않고 모델 크기를 줄입니다.</target>
        </trans-unit>
        <trans-unit id="467e03838e306f1cc9b995f9ab07314a0c223da6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Conv2D + BatchNorm fusion&lt;/strong&gt; (blacklisting option &lt;code&gt;MobileOptimizerType::CONV_BN_FUSION&lt;/code&gt;): This optimization pass folds &lt;code&gt;Conv2d-BatchNorm2d&lt;/code&gt; into &lt;code&gt;Conv2d&lt;/code&gt; in &lt;code&gt;forward&lt;/code&gt; method of this module and all its submodules. The weight and bias of the &lt;code&gt;Conv2d&lt;/code&gt; are correspondingly updated.</source>
          <target state="translated">&lt;strong&gt;Conv2D + BatchNorm 융합&lt;/strong&gt; (블랙리스트 옵션 &lt;code&gt;MobileOptimizerType::CONV_BN_FUSION&lt;/code&gt; ) :이 최적화 패스는 &lt;code&gt;Conv2d-BatchNorm2d&lt;/code&gt; 를 이 모듈과 모든 하위 모듈의 &lt;code&gt;forward&lt;/code&gt; 메서드 에서 Conv2d 로 &lt;code&gt;Conv2d&lt;/code&gt; 습니다 . &lt;code&gt;Conv2d&lt;/code&gt; 의 가중치와 편향 이 그에 따라 업데이트됩니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
