<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="tensorflow">
    <body>
      <group id="tensorflow">
        <trans-unit id="ab56083300b8a34ad1a803f46ebcd7c664f460f2" translate="yes" xml:space="preserve">
          <source>User can call this function to disable 2.x behavior during complex migrations.</source>
          <target state="translated">사용자는이 기능을 호출하여 복잡한 마이그레이션 중에 2.x 동작을 비활성화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="587ea8c1cdf68f3aaebe6d4c8fb0dc04c9f49662" translate="yes" xml:space="preserve">
          <source>User-written layers and models can achieve the same behavior with code that looks like:</source>
          <target state="translated">User-written layers and models can achieve the same behavior with code that looks like:</target>
        </trans-unit>
        <trans-unit id="7076897558f527fd7b476c32a2d7999a7bce1794" translate="yes" xml:space="preserve">
          <source>Users can call this method to get some facts of the TPU system, like total number of cores, number of TPU workers and the devices. E.g.</source>
          <target state="translated">Users can call this method to get some facts of the TPU system, like total number of cores, number of TPU workers and the devices. E.g.</target>
        </trans-unit>
        <trans-unit id="67147ed6d1d38b4f4b015cf112ec870c25df3f0b" translate="yes" xml:space="preserve">
          <source>Users can pass strategy specific options to &lt;code&gt;options&lt;/code&gt; argument. An example to enable bucketizing dynamic shapes in &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy#run&quot;&gt;&lt;code&gt;TPUStrategy.run&lt;/code&gt;&lt;/a&gt; is:</source>
          <target state="translated">Users can pass strategy specific options to &lt;code&gt;options&lt;/code&gt; argument. An example to enable bucketizing dynamic shapes in &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy#run&quot;&gt; &lt;code&gt;TPUStrategy.run&lt;/code&gt; &lt;/a&gt; is:</target>
        </trans-unit>
        <trans-unit id="f72b22db9ab4d4d8bbc6f80831bcd32ed79c81e6" translate="yes" xml:space="preserve">
          <source>Users can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the &lt;code&gt;reader_func&lt;/code&gt; and &lt;code&gt;shard_func&lt;/code&gt; parameters.</source>
          <target state="translated">Users can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the &lt;code&gt;reader_func&lt;/code&gt; and &lt;code&gt;shard_func&lt;/code&gt; parameters.</target>
        </trans-unit>
        <trans-unit id="157f011efb63e43996c3b578c255e7d5545ba313" translate="yes" xml:space="preserve">
          <source>Users can write it to file for offline analysis by tfprof commandline or graphical interface.</source>
          <target state="translated">tfprof 명령 줄 또는 그래픽 인터페이스로 오프라인 분석을 위해 파일에 파일을 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e5a32aab95198164a5ff787e69d28cbf95af2893" translate="yes" xml:space="preserve">
          <source>Users may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.</source>
          <target state="translated">Users may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.</target>
        </trans-unit>
        <trans-unit id="6748ccb2ebd37a27cc38d08c0979684dcbf55e30" translate="yes" xml:space="preserve">
          <source>Users may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential shard_func could be written.</source>
          <target state="translated">Users may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential shard_func could be written.</target>
        </trans-unit>
        <trans-unit id="e7108f46888ed24215d4773b328e7e496d911304" translate="yes" xml:space="preserve">
          <source>Users may write the following code to asynchronuously invoke &lt;code&gt;train_step_fn&lt;/code&gt; and log the &lt;code&gt;loss&lt;/code&gt; metric for every &lt;code&gt;num_steps&lt;/code&gt; steps in a training loop. &lt;code&gt;train_step_fn&lt;/code&gt; internally consumes data using &lt;code&gt;iterator.get_next()&lt;/code&gt;, and may throw OutOfRangeError when running out of data. In the case:</source>
          <target state="translated">Users may write the following code to asynchronuously invoke &lt;code&gt;train_step_fn&lt;/code&gt; and log the &lt;code&gt;loss&lt;/code&gt; metric for every &lt;code&gt;num_steps&lt;/code&gt; steps in a training loop. &lt;code&gt;train_step_fn&lt;/code&gt; internally consumes data using &lt;code&gt;iterator.get_next()&lt;/code&gt; , and may throw OutOfRangeError when running out of data. In the case:</target>
        </trans-unit>
        <trans-unit id="a0ec32691eaba97311f1cccd2dad97641a499fa1" translate="yes" xml:space="preserve">
          <source>Users must not modify any collections used in nest while this function is running.</source>
          <target state="translated">이 기능이 실행되는 동안 사용자는 중첩에 사용 된 모음을 수정해서는 안됩니다.</target>
        </trans-unit>
        <trans-unit id="06b0870bd9d2eea911ba0bf24a44afc23e3b6358" translate="yes" xml:space="preserve">
          <source>Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs.</source>
          <target state="translated">사용자는 동일한 tf.Example 인스턴스에서 구문 분석되었으므로 기능의 구문 분석 스펙을 레이블 및 가중치 (있는 경우)와 결합해야합니다. 이 유틸리티는 이러한 사양을 결합합니다.</target>
        </trans-unit>
        <trans-unit id="182c1f1a391ebf26b9c0b763f027f3059f9e6bd2" translate="yes" xml:space="preserve">
          <source>Users of &lt;code&gt;step_fn&lt;/code&gt; may perform &lt;code&gt;run()&lt;/code&gt; calls without running hooks by accessing the &lt;code&gt;session&lt;/code&gt;. A &lt;code&gt;run()&lt;/code&gt; call with hooks may be performed using &lt;code&gt;run_with_hooks()&lt;/code&gt;. Computation flow can be interrupted using &lt;code&gt;request_stop()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;step_fn&lt;/code&gt; 의 사용자 는 &lt;code&gt;session&lt;/code&gt; 에 액세스하여 후크를 실행하지 않고 &lt;code&gt;run()&lt;/code&gt; 호출을 수행 할 수 있습니다 . &lt;code&gt;run()&lt;/code&gt; 후크 호출을 사용하여 수행 될 수있다 &lt;code&gt;run_with_hooks()&lt;/code&gt; . &lt;code&gt;request_stop()&lt;/code&gt; 사용하여 계산 흐름을 중단 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2f188fa1f43efe8bdab97db20fb68ecadd592af6" translate="yes" xml:space="preserve">
          <source>Users will just instantiate a layer and then treat it as a callable.</source>
          <target state="translated">사용자는 계층을 인스턴스화 한 다음 호출 가능 계층으로 취급합니다.</target>
        </trans-unit>
        <trans-unit id="c72c8f3ece626a4705405cb3a7b8af47ef729815" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy&lt;/code&gt; loss average over classes and weighted sum over the batch. Namely, if the input logits have shape &lt;code&gt;[batch_size, n_classes]&lt;/code&gt;, the loss is the average over &lt;code&gt;n_classes&lt;/code&gt; and the weighted sum over &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="translated">클래스에 대한 &lt;code&gt;sigmoid_cross_entropy&lt;/code&gt; 손실 평균을 사용 하고 배치에 대한 가중치 합계를 사용합니다. 즉, 입력 로짓의 모양이 &lt;code&gt;[batch_size, n_classes]&lt;/code&gt; 인 경우 손실은 &lt;code&gt;n_classes&lt;/code&gt; 에 대한 평균 과 &lt;code&gt;batch_size&lt;/code&gt; 에 대한 가중치 합계 입니다.</target>
        </trans-unit>
        <trans-unit id="11a9b701f656adfafd6806eb9e7ef756d209ed2e" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; loss, which is the same as &lt;code&gt;BinaryClassHead&lt;/code&gt;. The differences compared to &lt;code&gt;BinaryClassHead&lt;/code&gt; are:</source>
          <target state="translated">&lt;code&gt;BinaryClassHead&lt;/code&gt; 와 동일한 &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; 손실을 사용합니다 . &lt;code&gt;BinaryClassHead&lt;/code&gt; 와 비교 한 차이점 은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="3d8209fd0815d68d4830b41bd06d14a5cb947aa2" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; loss.</source>
          <target state="translated">&lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; 를 사용합니다 . 손실을 .</target>
        </trans-unit>
        <trans-unit id="e004400a6e3754313301e3fd9261a2c189b3474a" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sparse_softmax_cross_entropy&lt;/code&gt; loss.</source>
          <target state="translated">&lt;code&gt;sparse_softmax_cross_entropy&lt;/code&gt; 사용 손실을 .</target>
        </trans-unit>
        <trans-unit id="3e2b61f4b530febb6503deed927b41c373ddaaf3" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;str(value)&lt;/code&gt;, except for &lt;code&gt;bytes&lt;/code&gt; typed inputs, which are converted using &lt;code&gt;as_str&lt;/code&gt;.</source>
          <target state="translated">용도 &lt;code&gt;str(value)&lt;/code&gt; 을 제외하고 &lt;code&gt;bytes&lt;/code&gt; 사용하여 변환되어 입력 입력, &lt;code&gt;as_str&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="2973e6aa554e21daa4adef06a74539c445b14d95" translate="yes" xml:space="preserve">
          <source>Uses utf-8 encoding for text by default.</source>
          <target state="translated">기본적으로 텍스트에 utf-8 인코딩을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="217a51c3037aab24f119676b38ba458afe249401" translate="yes" xml:space="preserve">
          <source>Using &lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt;&lt;/a&gt; for sparse multiplication:</source>
          <target state="translated">희소 곱셈에 &lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt; &lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt; &lt;/a&gt; 사용 :</target>
        </trans-unit>
        <trans-unit id="159ac66854831fd41477c6614f26e3ff55888f88" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;pos&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt; with same shape as &lt;code&gt;input&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 같은 모양의 &lt;code&gt;pos&lt;/code&gt; 및 &lt;code&gt;len&lt;/code&gt; 사용 :</target>
        </trans-unit>
        <trans-unit id="20acdd8f39a8fcbb172a5237734f68ee0d2be386" translate="yes" xml:space="preserve">
          <source>Using float64 is similar to mixed precision. Either the global policy can be set to float64, or &lt;code&gt;dtype='float64'&lt;/code&gt; can be passed to individual layers. For example, to set the global policy:</source>
          <target state="translated">float64 사용은 혼합 정밀도와 유사합니다. 글로벌 정책을 float64로 설정하거나 &lt;code&gt;dtype='float64'&lt;/code&gt; 를 개별 레이어로 전달할 수 있습니다. 예를 들어, 글로벌 정책을 설정하려면 다음을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="f6b2bb4ba02dab533aac4565623cbaf2d9684521" translate="yes" xml:space="preserve">
          <source>Using graphs directly (deprecated)</source>
          <target state="translated">그래프 직접 사용 (더 이상 사용되지 않음)</target>
        </trans-unit>
        <trans-unit id="45c1897ccbe225e4b4bb0b5776304a91ccc424dd" translate="yes" xml:space="preserve">
          <source>Using scalar &lt;code&gt;pos&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt;:</source>
          <target state="translated">스칼라 &lt;code&gt;pos&lt;/code&gt; 및 &lt;code&gt;len&lt;/code&gt; 사용 :</target>
        </trans-unit>
        <trans-unit id="55be0948f7db3ef4c4e1ab2c5b8f53c076ac5356" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;TensorBoard&lt;/code&gt; callback will work when eager execution is enabled, with the restriction that outputting histogram summaries of weights and gradients is not supported. Consequently, &lt;code&gt;histogram_freq&lt;/code&gt; will be ignored.</source>
          <target state="translated">은 Using &lt;code&gt;TensorBoard&lt;/code&gt; 의 열망 실행 중량 및 구배 출력 히스토그램 요약이 지원되지 않음을 제한하여, 사용할 때 콜백하여 작동 할 것이다. 결과적으로 &lt;code&gt;histogram_freq&lt;/code&gt; 는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="10b4feb7db468a7b136fb7eedc7778e772ed7977" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;Uniform&lt;/code&gt; distribution as an example:</source>
          <target state="translated">은 Using &lt;code&gt;Uniform&lt;/code&gt; 예로 분포를 :</target>
        </trans-unit>
        <trans-unit id="a70f4ea74dc28a0759892b77d899be11c4a630a4" translate="yes" xml:space="preserve">
          <source>Using the SavedModel format</source>
          <target state="translated">저장된 모델 형식 사용</target>
        </trans-unit>
        <trans-unit id="2d5a6c95117b542db8bf88cf405c6beac37d2a3c" translate="yes" xml:space="preserve">
          <source>Using the above module would produce &lt;a href=&quot;variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt;s and &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s whose names included the module name:</source>
          <target state="translated">위의 모듈을 사용하면 이름에 모듈 이름이 포함 된 &lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt; 및 tf.Tensor 가 생성 &lt;a href=&quot;variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7ad33ae89a6d279a16ce6134859a442eac2581c4" translate="yes" xml:space="preserve">
          <source>Using the default job_name of worker, you can schedule ops to run remotely as follows:</source>
          <target state="translated">worker의 기본 job_name을 사용하여 다음과 같이 op가 원격으로 실행되도록 예약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5a6462b40df0780171c71c9fb37d8c4d54d01a02" translate="yes" xml:space="preserve">
          <source>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; will also be placed on the specified device as well.</source>
          <target state="translated">이 전략을 사용하면 해당 범위에서 작성된 모든 변수가 지정된 장치에 배치됩니다. 이 전략을 통해 분배 된 입력은 지정된 장치로 프리 페치됩니다. 또한 &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; 를 통해 호출 된 모든 함수 는 지정된 장치에도 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="0921ab75268541486ce05861f2d20fe9143695fe" translate="yes" xml:space="preserve">
          <source>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via &lt;code&gt;strategy.run&lt;/code&gt; will also be placed on the specified device as well.</source>
          <target state="translated">Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via &lt;code&gt;strategy.run&lt;/code&gt; will also be placed on the specified device as well.</target>
        </trans-unit>
        <trans-unit id="71b01f087dc4062f216f4b3e8e8c06a8b29e198c" translate="yes" xml:space="preserve">
          <source>Usually, this does not necessarily mean that the layer is run in inference mode (which is normally controlled by the &lt;code&gt;training&lt;/code&gt; argument that can be passed when calling a layer). &quot;Frozen state&quot; and &quot;inference mode&quot; are two separate concepts.</source>
          <target state="translated">일반적으로 이것은 반드시 레이어가 추론 모드에서 실행되는 것을 의미하지는 않습니다 (일반적으로 레이어를 호출 할 때 전달 될 수있는 &lt;code&gt;training&lt;/code&gt; 인수에 의해 제어 됨 ). &quot;동결 상태&quot;와 &quot;추론 모드&quot;는 서로 다른 두 가지 개념입니다.</target>
        </trans-unit>
        <trans-unit id="6cf4bb7b1492b783c6bc69633b0fe1693771c5d9" translate="yes" xml:space="preserve">
          <source>Utilities for ImageNet data preprocessing &amp;amp; prediction decoding.</source>
          <target state="translated">ImageNet 데이터 전처리 및 예측 디코딩을위한 유틸리티</target>
        </trans-unit>
        <trans-unit id="08fb7be2421c2bdf2a78c1776aba074edf57a478" translate="yes" xml:space="preserve">
          <source>Utilities for preprocessing sequence data.</source>
          <target state="translated">전처리 시퀀스 데이터를위한 유틸리티.</target>
        </trans-unit>
        <trans-unit id="ae6bf25944794d7df424031b54eed9e8c6e9c537" translate="yes" xml:space="preserve">
          <source>Utilities for text input preprocessing.</source>
          <target state="translated">텍스트 입력 전처리를위한 유틸리티.</target>
        </trans-unit>
        <trans-unit id="41900282bff2287f65ac5f2919d6b167c32aa670" translate="yes" xml:space="preserve">
          <source>Utilities for writing compatible code</source>
          <target state="translated">호환 코드 작성을위한 유틸리티</target>
        </trans-unit>
        <trans-unit id="4209295b2fe1eb250fb221ec6cd68f34f854faae" translate="yes" xml:space="preserve">
          <source>Utility class for generating batches of temporal data.</source>
          <target state="translated">일괄 데이터 일괄 처리를 생성하기위한 유틸리티 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="3f80dd65b77c781d8eb1f4f65d0db44d5b92a1de" translate="yes" xml:space="preserve">
          <source>Utility function to build TensorInfo proto from a Tensor. (deprecated)</source>
          <target state="translated">Tensor에서 TensorInfo 프로토를 빌드하는 유틸리티 기능. (더 이상 사용되지 않음)</target>
        </trans-unit>
        <trans-unit id="d66171638f7d7dc0d33fb68081060b45caf826a1" translate="yes" xml:space="preserve">
          <source>Utility function to build a SignatureDef protocol buffer.</source>
          <target state="translated">SignatureDef 프로토콜 버퍼를 구축하기위한 유틸리티 기능.</target>
        </trans-unit>
        <trans-unit id="a1fc6fdd2556af786588697b8b4da900e88da0b4" translate="yes" xml:space="preserve">
          <source>Utility functions for building and inspecting SignatureDef protos.</source>
          <target state="translated">SignatureDef 프로토 타입 제작 및 검사를위한 유틸리티 기능.</target>
        </trans-unit>
        <trans-unit id="41a205ecd48940b5d4f27244d92bd652c4f4f6d1" translate="yes" xml:space="preserve">
          <source>Utility functions to assist with setup and construction of the SavedModel proto.</source>
          <target state="translated">SavedModel 프로토의 설정 및 구성을 지원하는 유틸리티 기능.</target>
        </trans-unit>
        <trans-unit id="25e1689ca50193bb7593b638ace3275be8fc9300" translate="yes" xml:space="preserve">
          <source>Utility methods to create simple input_fns.</source>
          <target state="translated">간단한 input_fn을 작성하기위한 유틸리티 메소드.</target>
        </trans-unit>
        <trans-unit id="98c6a5322edf6827b19ea21d2e974ec573fb37d2" translate="yes" xml:space="preserve">
          <source>V2 Compatibility</source>
          <target state="translated">V2 호환성</target>
        </trans-unit>
        <trans-unit id="b45432e089497cb1e4b6a50a251afeb8f8ec8634" translate="yes" xml:space="preserve">
          <source>V2 format specific: merges the metadata files of sharded checkpoints. The</source>
          <target state="translated">V2 format specific: merges the metadata files of sharded checkpoints. The</target>
        </trans-unit>
        <trans-unit id="6f4dce99bbaf012216e610738a930ee809e1a1dd" translate="yes" xml:space="preserve">
          <source>VGG16 model for Keras.</source>
          <target state="translated">Keras 용 VGG16 모델.</target>
        </trans-unit>
        <trans-unit id="16578957bc13d8fcce797647de9c6287bbab95bd" translate="yes" xml:space="preserve">
          <source>VGG19 model for Keras.</source>
          <target state="translated">Keras 용 VGG19 모델.</target>
        </trans-unit>
        <trans-unit id="b2ac8a00dcd90f10a8f8eab14df7c47a44d8bc39" translate="yes" xml:space="preserve">
          <source>Valid keyword args are:</source>
          <target state="translated">Valid keyword args are:</target>
        </trans-unit>
        <trans-unit id="2b984695d87811bddc59458f1d995336a48f8d61" translate="yes" xml:space="preserve">
          <source>Valid values for whence are: 0: start of the file (default) 1: relative to the current position of the file 2: relative to the end of file. &lt;code&gt;offset&lt;/code&gt; is usually negative.</source>
          <target state="translated">Valid values for whence are: 0: start of the file (default) 1: relative to the current position of the file 2: relative to the end of file. &lt;code&gt;offset&lt;/code&gt; is usually negative.</target>
        </trans-unit>
        <trans-unit id="35d73c1e3e748be7af4094274c7c669844f7b39a" translate="yes" xml:space="preserve">
          <source>Validate and return float type based on &lt;code&gt;tensors&lt;/code&gt; and &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;tensors&lt;/code&gt; 및 &lt;code&gt;dtype&lt;/code&gt; 에 따라 float 유형을 확인하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7cfbc94b8562d35a1a624063af44283471348553" translate="yes" xml:space="preserve">
          <source>Validated type.</source>
          <target state="translated">검증 된 유형.</target>
        </trans-unit>
        <trans-unit id="fbc4d5994f37368bc38796f9fcac6177e5b079ae" translate="yes" xml:space="preserve">
          <source>Value Error: If input contains string value</source>
          <target state="translated">Value Error: If input contains string value</target>
        </trans-unit>
        <trans-unit id="8b6107bdd0fa4996fbf7660de172ff5a7b7753b6" translate="yes" xml:space="preserve">
          <source>Value convertible to &lt;a href=&quot;dtypes/dtype&quot;&gt;&lt;code&gt;tf.DType&lt;/code&gt;&lt;/a&gt;. The type of the tensor values.</source>
          <target state="translated">Value convertible to &lt;a href=&quot;dtypes/dtype&quot;&gt; &lt;code&gt;tf.DType&lt;/code&gt; &lt;/a&gt;. The type of the tensor values.</target>
        </trans-unit>
        <trans-unit id="15c75ed5285ab847c51c1d1b073c1ffd6af1e2d3" translate="yes" xml:space="preserve">
          <source>Value convertible to &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt;. The shape of the tensor.</source>
          <target state="translated">Value convertible to &lt;a href=&quot;tensorshape&quot;&gt; &lt;code&gt;tf.TensorShape&lt;/code&gt; &lt;/a&gt;. The shape of the tensor.</target>
        </trans-unit>
        <trans-unit id="fe02012fec01937ecf4625a4bb543d51186a14cc" translate="yes" xml:space="preserve">
          <source>Value of new time step. Can be a variable or a constant</source>
          <target state="translated">Value of new time step. Can be a variable or a constant</target>
        </trans-unit>
        <trans-unit id="bf813cf306f9fe6920d27a32a73e1a8ab0b8ac28" translate="yes" xml:space="preserve">
          <source>Value of tensor to set.</source>
          <target state="translated">Value of tensor to set.</target>
        </trans-unit>
        <trans-unit id="c0d2a5aeb6f41dd496d2ccf306df13e6658541ae" translate="yes" xml:space="preserve">
          <source>Value to return if flagname is not defined. Defaults to None.</source>
          <target state="translated">Value to return if flagname is not defined. Defaults to None.</target>
        </trans-unit>
        <trans-unit id="e4123cacba8af0210e8bc3a45f165b7510fbe81e" translate="yes" xml:space="preserve">
          <source>Value to set for indices not specified in &lt;code&gt;self&lt;/code&gt;. Defaults to zero. &lt;code&gt;default_value&lt;/code&gt; must be broadcastable to &lt;code&gt;self.shape[self.ragged_rank + 1:]&lt;/code&gt;.</source>
          <target state="translated">Value to set for indices not specified in &lt;code&gt;self&lt;/code&gt; . Defaults to zero. &lt;code&gt;default_value&lt;/code&gt; must be broadcastable to &lt;code&gt;self.shape[self.ragged_rank + 1:]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2c7617def04f30bf9e69162b03566ab37d2f02c8" translate="yes" xml:space="preserve">
          <source>Value to set the tensor to, as a Numpy array (of the same shape).</source>
          <target state="translated">Value to set the tensor to, as a Numpy array (of the same shape).</target>
        </trans-unit>
        <trans-unit id="44cdd81777401755111de56bb90c9f9e4f31fb50" translate="yes" xml:space="preserve">
          <source>ValueError if &lt;code&gt;num_packs&lt;/code&gt; is negative.</source>
          <target state="translated">&lt;code&gt;num_packs&lt;/code&gt; 가 음수 이면 ValueError 입니다.</target>
        </trans-unit>
        <trans-unit id="b03333735384a88c2a20365a2ab9d19d0855bf6a" translate="yes" xml:space="preserve">
          <source>ValueError if data format is unrecognized, if &lt;code&gt;value&lt;/code&gt; has less than two dimensions when &lt;code&gt;data_format&lt;/code&gt; is 'N..C'/&lt;code&gt;None&lt;/code&gt; or &lt;code&gt;value&lt;/code&gt; has less then three dimensions when &lt;code&gt;data_format&lt;/code&gt; is &lt;code&gt;NC..&lt;/code&gt;, if &lt;code&gt;bias&lt;/code&gt; does not have exactly one dimension (is a vector), or if the size of &lt;code&gt;bias&lt;/code&gt; does not match the size of the channel dimension of &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">경우에 ValueError 데이터 포맷은 인식되지 않는 경우, &lt;code&gt;value&lt;/code&gt; 미만인 경우 두 개의 차원이 &lt;code&gt;data_format&lt;/code&gt; 가 'N..C가'/ IS &lt;code&gt;None&lt;/code&gt; 또는 &lt;code&gt;value&lt;/code&gt; 때 미만 입체적있다 &lt;code&gt;data_format&lt;/code&gt; 가 있다 &lt;code&gt;NC..&lt;/code&gt; , 만일 &lt;code&gt;bias&lt;/code&gt; (정확히 하나의 차원이없는이 없다 벡터), 또는 만약 크기 &lt;code&gt;bias&lt;/code&gt; 의 채널 치수의 크기와 일치하지 않는 &lt;code&gt;value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="28e00764e50e68a9e4b435811b832e1599c2d7ff" translate="yes" xml:space="preserve">
          <source>ValueError when attempting to use experimental_compile, but XLA support is not enabled.</source>
          <target state="translated">ValueError when attempting to use experimental_compile, but XLA support is not enabled.</target>
        </trans-unit>
        <trans-unit id="f7604a24858b57e5b1686ec99368d1d81fb1d5a4" translate="yes" xml:space="preserve">
          <source>ValueError: When set pad_to_max_output_size to False for batched input.</source>
          <target state="translated">ValueError: When set pad_to_max_output_size to False for batched input.</target>
        </trans-unit>
        <trans-unit id="d1cad1ac50b9d58fe699af8d3719130c04596c12" translate="yes" xml:space="preserve">
          <source>ValueRowIds(key,)</source>
          <target state="translated">ValueRowIds(key,)</target>
        </trans-unit>
        <trans-unit id="2aa729287cefae889cb308c2489e301fd72ddfcc" translate="yes" xml:space="preserve">
          <source>Values are generated when TensorFlow is compiled, and are static for each TensorFlow package. The return value is a dictionary with string keys such as:</source>
          <target state="translated">Values are generated when TensorFlow is compiled, and are static for each TensorFlow package. The return value is a dictionary with string keys such as:</target>
        </trans-unit>
        <trans-unit id="9989c8b816fd103fa17ba450218cc3f655b85003" translate="yes" xml:space="preserve">
          <source>Values are merged in order, so if an index appears in both &lt;code&gt;indices[m][i]&lt;/code&gt; and &lt;code&gt;indices[n][j]&lt;/code&gt; for &lt;code&gt;(m,i) &amp;lt; (n,j)&lt;/code&gt; the slice &lt;code&gt;data[n][j]&lt;/code&gt; will appear in the merged result. If you do not need this guarantee, ParallelDynamicStitch might perform better on some devices.</source>
          <target state="translated">값이 순서대로 병합 그렇다면 모두 인덱스 나타날 &lt;code&gt;indices[m][i]&lt;/code&gt; 와 &lt;code&gt;indices[n][j]&lt;/code&gt; 대 &lt;code&gt;(m,i) &amp;lt; (n,j)&lt;/code&gt; , 슬라이스 &lt;code&gt;data[n][j]&lt;/code&gt; 것 병합 된 결과에 나타납니다. 이 보증이 필요하지 않으면 ParallelDynamicStitch가 일부 장치에서 더 잘 수행 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="31517cd57e02e0d5266cab70a1c38e47fcd19844" translate="yes" xml:space="preserve">
          <source>Values are not loaded immediately, but when the initializer is run (typically by running a &lt;a href=&quot;../global_variables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.global_variables_initializer&lt;/code&gt;&lt;/a&gt; op).</source>
          <target state="translated">값은 즉시로드되지 않지만 이니셜 라이저가 실행될 때 (일반적으로 &lt;a href=&quot;../global_variables_initializer&quot;&gt; &lt;code&gt;tf.compat.v1.global_variables_initializer&lt;/code&gt; &lt;/a&gt; op)로드됩니다.</target>
        </trans-unit>
        <trans-unit id="c7b1102521bf7adb2b3f3d054e8e942970be2d94" translate="yes" xml:space="preserve">
          <source>Values can also have the same locality as a variable, which is a mirrored value but residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to &lt;a href=&quot;../../../distribute/strategyextended#update&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt;&lt;/a&gt; to update the value of a variable. You may use &lt;a href=&quot;../../../distribute/strategyextended#colocate_vars_with&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt;&lt;/a&gt; to give a variable the same locality as another variable. This is useful, for example, for &quot;slot&quot; variables used by an optimizer for keeping track of statistics used to update a primary/model variable. You may convert a per-replica value to a variable's locality by using &lt;a href=&quot;../../../distribute/strategyextended#reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../distribute/strategyextended#batch_reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">값은 변수와 동일한 위치를 가질 수 있으며, 이는 미러링 된 값이지만 변수와 동일한 장치에 있습니다 (계산 장치와 반대). 이러한 값은 변수 값을 업데이트 하기 위해 &lt;a href=&quot;../../../distribute/strategyextended#update&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt; &lt;/a&gt; 에 대한 호출로 전달 될 수 있습니다 . &lt;a href=&quot;../../../distribute/strategyextended#colocate_vars_with&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt; &lt;/a&gt; 를 사용하여 다른 변수와 동일한 지역성을 변수에 제공 할 수 있습니다 . 예를 들어, 기본 / 모델 변수를 업데이트하는 데 사용되는 통계를 추적하기 위해 옵티마이 저가 사용하는 &quot;슬롯&quot;변수에 유용합니다. &lt;a href=&quot;../../../distribute/strategyextended#reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../../distribute/strategyextended#batch_reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt; &lt;/a&gt; 를 사용하여 복제 별 값을 변수의 지역으로 변환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2143538d5a76006c2fb34d48a187c9999b4ce092" translate="yes" xml:space="preserve">
          <source>Values can also have the same locality as a variable, which is a mirrored value but residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to &lt;a href=&quot;strategyextended#update&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt;&lt;/a&gt; to update the value of a variable. You may use &lt;a href=&quot;strategyextended#colocate_vars_with&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt;&lt;/a&gt; to give a variable the same locality as another variable. This is useful, for example, for &quot;slot&quot; variables used by an optimizer for keeping track of statistics used to update a primary/model variable. You may convert a per-replica value to a variable's locality by using &lt;a href=&quot;strategyextended#reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;strategyextended#batch_reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">값은 변수와 동일한 위치를 가질 수 있으며, 이는 미러링 된 값이지만 변수와 동일한 장치에 있습니다 (계산 장치와 반대). 이러한 값은 변수 값을 업데이트 하기 위해 &lt;a href=&quot;strategyextended#update&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt; &lt;/a&gt; 에 대한 호출로 전달 될 수 있습니다 . &lt;a href=&quot;strategyextended#colocate_vars_with&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt; &lt;/a&gt; 를 사용하여 다른 변수와 동일한 지역성을 변수에 제공 할 수 있습니다 . 예를 들어, 기본 / 모델 변수를 업데이트하는 데 사용되는 통계를 추적하기 위해 옵티마이 저가 사용하는 &quot;슬롯&quot;변수에 유용합니다. &lt;a href=&quot;strategyextended#reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;strategyextended#batch_reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt; &lt;/a&gt; 를 사용하여 복제 별 값을 변수의 지역으로 변환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ee2d365c74f97141598697d434a691d20762875a" translate="yes" xml:space="preserve">
          <source>Values in &lt;code&gt;arr&lt;/code&gt; outside of the range [0, size) are ignored.</source>
          <target state="translated">Values in &lt;code&gt;arr&lt;/code&gt; outside of the range [0, size) are ignored.</target>
        </trans-unit>
        <trans-unit id="93fb0db8d97fc795e57ce06a9fa4d44911eba728" translate="yes" xml:space="preserve">
          <source>Values may be merged in parallel, so if an index appears in both &lt;code&gt;indices[m][i]&lt;/code&gt; and &lt;code&gt;indices[n][j]&lt;/code&gt;, the result may be invalid. This differs from the normal DynamicStitch operator that defines the behavior in that case.</source>
          <target state="translated">Values may be merged in parallel, so if an index appears in both &lt;code&gt;indices[m][i]&lt;/code&gt; and &lt;code&gt;indices[n][j]&lt;/code&gt; , the result may be invalid. This differs from the normal DynamicStitch operator that defines the behavior in that case.</target>
        </trans-unit>
        <trans-unit id="d15a2f51d1f3c105d253f174b82490fef1d2d0a2" translate="yes" xml:space="preserve">
          <source>Values of the sparse gradient to be applied.</source>
          <target state="translated">Values of the sparse gradient to be applied.</target>
        </trans-unit>
        <trans-unit id="332770f6ea1035cc31b26b51a3ff43d26f36a8a6" translate="yes" xml:space="preserve">
          <source>Values returned by all methods, such as &lt;code&gt;matmul&lt;/code&gt; or &lt;code&gt;determinant&lt;/code&gt; will be cast to &lt;code&gt;DTYPE&lt;/code&gt;.</source>
          <target state="translated">같은 모든 메소드에 의해 반환 된 값 &lt;code&gt;matmul&lt;/code&gt; 또는 &lt;code&gt;determinant&lt;/code&gt; 캐스팅됩니다 &lt;code&gt;DTYPE&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="472814b4275e6a3c35876afc6f85f152eeb4868b" translate="yes" xml:space="preserve">
          <source>Values to be associated with keys. Must be a tensor of the same shape as &lt;code&gt;keys&lt;/code&gt; and match the table's value type.</source>
          <target state="translated">Values to be associated with keys. Must be a tensor of the same shape as &lt;code&gt;keys&lt;/code&gt; and match the table's value type.</target>
        </trans-unit>
        <trans-unit id="7e1d325bbb77e43c2a2a39b8f1ed0e1639c81e61" translate="yes" xml:space="preserve">
          <source>Values to pad with, passed to &lt;a href=&quot;../dataset#padded_batch&quot;&gt;&lt;code&gt;tf.data.Dataset.padded_batch&lt;/code&gt;&lt;/a&gt;. Defaults to padding with 0.</source>
          <target state="translated">Values to pad with, passed to &lt;a href=&quot;../dataset#padded_batch&quot;&gt; &lt;code&gt;tf.data.Dataset.padded_batch&lt;/code&gt; &lt;/a&gt;. Defaults to padding with 0.</target>
        </trans-unit>
        <trans-unit id="152b6b1ae03ca0564ec1afcfa23945d6679fe3af" translate="yes" xml:space="preserve">
          <source>Values to put in the TensorProto.</source>
          <target state="translated">Values to put in the TensorProto.</target>
        </trans-unit>
        <trans-unit id="771c4420b1aa778f676175f583b791c3f76deedd" translate="yes" xml:space="preserve">
          <source>VarHandleOp</source>
          <target state="translated">VarHandleOp</target>
        </trans-unit>
        <trans-unit id="ff83e8cfb2acccae7a99a5aa63151589e41e200d" translate="yes" xml:space="preserve">
          <source>VarIsInitializedOp</source>
          <target state="translated">VarIsInitializedOp</target>
        </trans-unit>
        <trans-unit id="19de69cb601f53a4ea7af22a65c71ae63251365c" translate="yes" xml:space="preserve">
          <source>Variable</source>
          <target state="translated">Variable</target>
        </trans-unit>
        <trans-unit id="8b550e406a084548380628fd2909bf25b85b7d1f" translate="yes" xml:space="preserve">
          <source>Variable Constraint</source>
          <target state="translated">변수 제약</target>
        </trans-unit>
        <trans-unit id="dc8c6831002ca69f6ebeab63140354a283153e34" translate="yes" xml:space="preserve">
          <source>Variable Constraints</source>
          <target state="translated">Variable Constraints</target>
        </trans-unit>
        <trans-unit id="87bff689d1fa1dea41f1caabbfa56196f9dac995" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt; , &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../../variable_creator_scope&quot;&gt; &lt;code&gt;tf.variable_creator_scope&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4ebe6d40e82094b23daf6be3c6e12b5de702a887" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt; , &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../variable_creator_scope&quot;&gt; &lt;code&gt;tf.variable_creator_scope&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="ad8c28ec75b4fb97533632feee539c8715d6500d" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt; , &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../variable_creator_scope&quot;&gt; &lt;code&gt;tf.variable_creator_scope&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="56732160b0c125a3a8776369974b1d8fe83bf273" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt; , &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../variable_creator_scope&quot;&gt; &lt;code&gt;tf.variable_creator_scope&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="7dd58eeb1186ca442949f2a63231fa7bc6934171" translate="yes" xml:space="preserve">
          <source>Variable name to use for the first structure in assertion messages.</source>
          <target state="translated">Variable name to use for the first structure in assertion messages.</target>
        </trans-unit>
        <trans-unit id="c4e78d0db2266afd0792266861e4c967b5ab1437" translate="yes" xml:space="preserve">
          <source>Variable name to use for the second structure.</source>
          <target state="translated">Variable name to use for the second structure.</target>
        </trans-unit>
        <trans-unit id="66091c2f5a648f0c372811ab148bd71631287d1d" translate="yes" xml:space="preserve">
          <source>Variable name.</source>
          <target state="translated">변수 이름.</target>
        </trans-unit>
        <trans-unit id="0be93ad0375437f556470057db6cb269fac15f01" translate="yes" xml:space="preserve">
          <source>Variable or tensor.</source>
          <target state="translated">Variable or tensor.</target>
        </trans-unit>
        <trans-unit id="8d982c2fd72628882252beffffe7353608ac38df" translate="yes" xml:space="preserve">
          <source>Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing &lt;code&gt;losses&lt;/code&gt; under a &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will propagate gradients back to the corresponding variables.</source>
          <target state="translated">변수 정규화 텐서는이 속성에 액세스 할 때 만들어 지므로 더욱 안전합니다. &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; 에서&lt;/a&gt; &lt;code&gt;losses&lt;/code&gt; 에 액세스 하면 그라디언트가 해당 변수로 다시 전파됩니다.</target>
        </trans-unit>
        <trans-unit id="9becbe8191297fcb35681c2cb0fd8d5255180628" translate="yes" xml:space="preserve">
          <source>Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident. For details, see the &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;Variable Scope How To&lt;/a&gt;, here we present only a few basic examples.</source>
          <target state="translated">변수 범위를 사용하면 실수로 만들거나 공유하지 않는 확인을 제공하면서 새 변수를 작성하고 이미 작성된 변수를 공유 할 수 있습니다. 자세한 내용은 &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;변수 범위 사용법을&lt;/a&gt; 참조하십시오. 여기에는 몇 가지 기본 예제 만 나와 있습니다.</target>
        </trans-unit>
        <trans-unit id="78a94c97c2babdd8664aa38c452645afd7c11def" translate="yes" xml:space="preserve">
          <source>Variable scope object to carry defaults to provide to &lt;code&gt;get_variable&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;get_variable&lt;/code&gt; 에 제공하기 위해 기본값을 전달하는 변수 범위 오브젝트 .</target>
        </trans-unit>
        <trans-unit id="656181555fa5b90a78a2907e7cdc150cdb7d9969" translate="yes" xml:space="preserve">
          <source>Variable semantics in TensorFlow 2 are eager execution friendly. The above code is roughly equivalent to:</source>
          <target state="translated">Variable semantics in TensorFlow 2 are eager execution friendly. The above code is roughly equivalent to:</target>
        </trans-unit>
        <trans-unit id="31ae50eddd5315630d388c7fc7d3dd4ac0f0c588" translate="yes" xml:space="preserve">
          <source>Variable shape. Defaults to scalar if unspecified.</source>
          <target state="translated">Variable shape. Defaults to scalar if unspecified.</target>
        </trans-unit>
        <trans-unit id="c9c726ae1bb253cefd1f8e18cd504aa6e24e0e2b" translate="yes" xml:space="preserve">
          <source>Variable to set to a new value.</source>
          <target state="translated">Variable to set to a new value.</target>
        </trans-unit>
        <trans-unit id="2fa42dfef00dd6f5716bd283c68f5a7c3df717eb" translate="yes" xml:space="preserve">
          <source>Variable, possibly mirrored to multiple devices, to operate on.</source>
          <target state="translated">Variable, possibly mirrored to multiple devices, to operate on.</target>
        </trans-unit>
        <trans-unit id="fad16e163b18a5adad4a6b1f9bafad182b1fc5e8" translate="yes" xml:space="preserve">
          <source>Variable-size shapes are allowed by setting the corresponding shape dimensions to 0 in the shape attr. In this case DequeueMany will pad up to the maximum size of any given element in the minibatch. See below for details.</source>
          <target state="translated">Variable-size shapes are allowed by setting the corresponding shape dimensions to 0 in the shape attr. In this case DequeueMany will pad up to the maximum size of any given element in the minibatch. See below for details.</target>
        </trans-unit>
        <trans-unit id="4f736193c89669392caf0903ec1e19730e643644" translate="yes" xml:space="preserve">
          <source>Variable. The number of training steps this Optimizer has run.</source>
          <target state="translated">변하기 쉬운. 이 옵티마이 저가 실행 한 교육 단계 수입니다.</target>
        </trans-unit>
        <trans-unit id="fbd24bae4ab2c5fb1757b5c4452669855276459f" translate="yes" xml:space="preserve">
          <source>VariableScope for the created subgraph; defaults to &quot;bidirectional_rnn&quot;</source>
          <target state="translated">VariableScope for the created subgraph; defaults to &quot;bidirectional_rnn&quot;</target>
        </trans-unit>
        <trans-unit id="9a6f1a6e7f1ea3415ae9f018191ab201ea3b0fac" translate="yes" xml:space="preserve">
          <source>VariableScope for the created subgraph; defaults to &quot;rnn&quot;.</source>
          <target state="translated">VariableScope for the created subgraph; defaults to &quot;rnn&quot;.</target>
        </trans-unit>
        <trans-unit id="3580913cdba362b6cc2d19d6b7d1f9e9219c622d" translate="yes" xml:space="preserve">
          <source>VariableShape</source>
          <target state="translated">VariableShape</target>
        </trans-unit>
        <trans-unit id="751393cca20cc522b2f5c1a18350f77fe8d88bea" translate="yes" xml:space="preserve">
          <source>VariableV2</source>
          <target state="translated">VariableV2</target>
        </trans-unit>
        <trans-unit id="f628de42a66fa9f6ab10a5ed8f37f3bfec4938d4" translate="yes" xml:space="preserve">
          <source>Variables are assigned to local CPU or the only GPU. If there is more than one GPU, compute operations (other than variable update operations) will be replicated across all GPUs.</source>
          <target state="translated">변수는 로컬 CPU 또는 유일한 GPU에 할당됩니다. GPU가 두 개 이상인 경우 컴퓨팅 작업 (가변 업데이트 작업 제외)이 모든 GPU에 복제됩니다.</target>
        </trans-unit>
        <trans-unit id="70a30a529e57576d822cecd0f271a7b5d39f84b7" translate="yes" xml:space="preserve">
          <source>Variables are automatically tracked when assigned to attributes of types inheriting from &lt;a href=&quot;module&quot;&gt;&lt;code&gt;tf.Module&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;module&quot;&gt; &lt;code&gt;tf.Module&lt;/code&gt; &lt;/a&gt; 에서 상속되는 유형의 속성에 지정되면 변수가 자동으로 추적됩니다 .</target>
        </trans-unit>
        <trans-unit id="1655acbcc407ca5c5cc17d4daaba10766362006a" translate="yes" xml:space="preserve">
          <source>Variables are often captured and manipulated by &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s. This works the same way the un-decorated function would have:</source>
          <target state="translated">변수는 종종 &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 에&lt;/a&gt; 의해 캡처되고 조작됩니다 . 이것은 꾸며지지 않은 함수와 같은 방식으로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="9422ebfe912588a94b4db8ceb623ec6943b8ae1a" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; must be owned outside the function and be created only once:</source>
          <target state="translated">&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; 내에서 작성된 변수 는 함수 외부에서 소유해야하며 한 번만 작성해야합니다.</target>
        </trans-unit>
        <trans-unit id="a4f9c7fa0b18154ca4691caf51f88ee40eea34cb" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt;.</source>
          <target state="translated">Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="29e458106bb18014f5f1ce0d551df4f487281340" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt;.</source>
          <target state="translated">Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="198e5fbedc6e80bc4c09f5cf7cecc37f23ca5651" translate="yes" xml:space="preserve">
          <source>Variables created inside the strategy scope are &quot;owned&quot; by it:</source>
          <target state="translated">전략 범위 내에서 생성 된 변수는 다음과 같이 &quot;소유&quot;합니다.</target>
        </trans-unit>
        <trans-unit id="ecf6360262e709b4fb787134a5207269f78afa79" translate="yes" xml:space="preserve">
          <source>Variables created outside the strategy are not owned by it:</source>
          <target state="translated">전략 외부에서 작성된 변수는 소유하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4e85b51c74744b26c07cb066853860450f1f5376" translate="yes" xml:space="preserve">
          <source>Variables must be tracked by assigning them to an attribute of a tracked object or to an attribute of &lt;code&gt;obj&lt;/code&gt; directly. TensorFlow objects (e.g. layers from &lt;a href=&quot;../keras/layers&quot;&gt;&lt;code&gt;tf.keras.layers&lt;/code&gt;&lt;/a&gt;, optimizers from &lt;a href=&quot;../train&quot;&gt;&lt;code&gt;tf.train&lt;/code&gt;&lt;/a&gt;) track their variables automatically. This is the same tracking scheme that &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; uses, and an exported &lt;code&gt;Checkpoint&lt;/code&gt; object may be restored as a training checkpoint by pointing &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt; to the SavedModel's &quot;variables/&quot; subdirectory. Currently variables are the only stateful objects supported by &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, but others (e.g. tables) will be supported in the future.</source>
          <target state="translated">추적 된 객체의 속성이나 &lt;code&gt;obj&lt;/code&gt; 의 속성에 변수를 지정하여 변수를 추적해야합니다 . TensorFlow 객체 (예 : &lt;a href=&quot;../keras/layers&quot;&gt; &lt;code&gt;tf.keras.layers&lt;/code&gt; 의&lt;/a&gt; 레이어, tf.train 의 옵티 &lt;a href=&quot;../train&quot;&gt; &lt;code&gt;tf.train&lt;/code&gt; &lt;/a&gt; )는 변수를 자동으로 추적합니다. 이것은 &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; 가&lt;/a&gt; 사용 하는 것과 동일한 추적 체계이며 , &lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt; 를 SavedModel의 &quot;variables /&quot;서브 디렉토리 로 지정 하여 내 보낸 &lt;code&gt;Checkpoint&lt;/code&gt; 오브젝트를 훈련 체크 포인트로 복원 할 수 있습니다 . 현재 변수는 &lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt; 에서 지원하는 유일한 상태 저장 객체 이지만 향후 다른 객체 (예 : 테이블)도 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="e84fa29887783568cfad44fb6f314a273a26c8b5" translate="yes" xml:space="preserve">
          <source>Variables must be tracked by assigning them to an attribute of a tracked object or to an attribute of &lt;code&gt;obj&lt;/code&gt; directly. TensorFlow objects (e.g. layers from &lt;a href=&quot;../keras/layers&quot;&gt;&lt;code&gt;tf.keras.layers&lt;/code&gt;&lt;/a&gt;, optimizers from &lt;a href=&quot;../train&quot;&gt;&lt;code&gt;tf.train&lt;/code&gt;&lt;/a&gt;) track their variables automatically. This is the same tracking scheme that &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; uses, and an exported &lt;code&gt;Checkpoint&lt;/code&gt; object may be restored as a training checkpoint by pointing &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt; to the SavedModel's &quot;variables/&quot; subdirectory. Currently, variables are the only stateful objects supported by &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, but others (e.g. tables) will be supported in the future.</source>
          <target state="translated">Variables must be tracked by assigning them to an attribute of a tracked object or to an attribute of &lt;code&gt;obj&lt;/code&gt; directly. TensorFlow objects (e.g. layers from &lt;a href=&quot;../keras/layers&quot;&gt; &lt;code&gt;tf.keras.layers&lt;/code&gt; &lt;/a&gt;, optimizers from &lt;a href=&quot;../train&quot;&gt; &lt;code&gt;tf.train&lt;/code&gt; &lt;/a&gt;) track their variables automatically. This is the same tracking scheme that &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; uses, and an exported &lt;code&gt;Checkpoint&lt;/code&gt; object may be restored as a training checkpoint by pointing &lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt; to the SavedModel's &quot;variables/&quot; subdirectory. Currently, variables are the only stateful objects supported by &lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt;, but others (e.g. tables) will be supported in the future.</target>
        </trans-unit>
        <trans-unit id="51a7e747ba069953d31cbdce4de5483d9446f49f" translate="yes" xml:space="preserve">
          <source>Variables, placeholders, and independent operations can also be stored, as shown in the following example.</source>
          <target state="translated">다음 예제와 같이 변수, 자리 표시 자 및 독립 작업도 저장할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="612d2ee1679ef5637187e20c4629a406547bfef9" translate="yes" xml:space="preserve">
          <source>Variables:</source>
          <target state="translated">Variables:</target>
        </trans-unit>
        <trans-unit id="bb96e64e18e9a621cc7ceb2976a5e75548f771d4" translate="yes" xml:space="preserve">
          <source>Variance is defined as,</source>
          <target state="translated">분산은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="d282722043467708dfd05bebcd66f9f1d34aee3d" translate="yes" xml:space="preserve">
          <source>Variance of a tensor, alongside the specified axis.</source>
          <target state="translated">지정된 축과 함께 텐서의 변화.</target>
        </trans-unit>
        <trans-unit id="547d7dc902ba966407aa0728767694e953ad6b04" translate="yes" xml:space="preserve">
          <source>Variance of batch.</source>
          <target state="translated">Variance of batch.</target>
        </trans-unit>
        <trans-unit id="99189c611d030a5281d712eaa2b886309eb67a6b" translate="yes" xml:space="preserve">
          <source>Variance.</source>
          <target state="translated">Variance.</target>
        </trans-unit>
        <trans-unit id="108fd2238efb49f633daf84e4120ae9f47cfdec6" translate="yes" xml:space="preserve">
          <source>Various libraries built on top of the core TensorFlow library take care of creating some or all of these pieces and storing them in well known collections in the graph. The &lt;code&gt;Scaffold&lt;/code&gt; class helps pick these pieces from the graph collections, creating and adding them to the collections if needed.</source>
          <target state="translated">핵심 TensorFlow 라이브러리 위에 구축 된 다양한 라이브러리는 이러한 일부 또는 전부를 생성하고 그래프의 잘 알려진 컬렉션에 저장합니다. &lt;code&gt;Scaffold&lt;/code&gt; 클래스를 작성하고 필요한 경우 컬렉션에 추가, 그래프 컬렉션에서이 작품을 선택하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="a6a6801f04fd08c4cf3ed0c985c50e0b3cc573dd" translate="yes" xml:space="preserve">
          <source>Vector length = Maximum element in vector &lt;code&gt;values&lt;/code&gt; is 5. Adding 1, which is 6 will be the vector length.</source>
          <target state="translated">벡터 길이 = 벡터 &lt;code&gt;values&lt;/code&gt; 최대 요소 는 5입니다. 1을 더하면 6이 벡터 길이가됩니다.</target>
        </trans-unit>
        <trans-unit id="f5115b80229d0ab74c010431ae650645a075fb17" translate="yes" xml:space="preserve">
          <source>Vector of coordinatewise logits.</source>
          <target state="translated">좌표 로짓의 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="444c2b1f0bb53e9255195603b2fe9758758fc528" translate="yes" xml:space="preserve">
          <source>Vector of coordinatewise probabilities.</source>
          <target state="translated">좌표 확률의 벡터</target>
        </trans-unit>
        <trans-unit id="ce1a5d4c90bdb53d4d10dd069e9eaa942f080fdb" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the exclusive upper limits for each range.</source>
          <target state="translated">Vector or scalar &lt;code&gt;Tensor&lt;/code&gt; . Specifies the exclusive upper limits for each range.</target>
        </trans-unit>
        <trans-unit id="a3f9cb652ec383e8e638e09260439b3155ece9c8" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the first entry for each range if &lt;code&gt;limits&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;; otherwise, specifies the range limits, and the first entries default to &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">Vector or scalar &lt;code&gt;Tensor&lt;/code&gt; . Specifies the first entry for each range if &lt;code&gt;limits&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt; ; otherwise, specifies the range limits, and the first entries default to &lt;code&gt;0&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6a78cb729e962d878d29fc681da48899c9408f1c" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the increment for each range. Defaults to &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">Vector or scalar &lt;code&gt;Tensor&lt;/code&gt; . Specifies the increment for each range. Defaults to &lt;code&gt;1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ca486267fdece68b69e71338f98f70bc8a117412" translate="yes" xml:space="preserve">
          <source>Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)</source>
          <target state="translated">Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)</target>
        </trans-unit>
        <trans-unit id="89f3f7fe4c1640033ee74781a079e4dad0989210" translate="yes" xml:space="preserve">
          <source>Verbosity mode, 0 or 1.</source>
          <target state="translated">Verbosity mode, 0 or 1.</target>
        </trans-unit>
        <trans-unit id="ec03954e111311ff95bd45ec3b8a12f4f9d9e291" translate="yes" xml:space="preserve">
          <source>Verifies whether all flags pass validation.</source>
          <target state="translated">Verifies whether all flags pass validation.</target>
        </trans-unit>
        <trans-unit id="bf8d82dcaf9576f668b800d91e2cd8a9bd872212" translate="yes" xml:space="preserve">
          <source>Vertical coordinate of the top-left corner of the result in the input.</source>
          <target state="translated">Vertical coordinate of the top-left corner of the result in the input.</target>
        </trans-unit>
        <trans-unit id="3cbf1afdceff80dfc15073d9ab60779a3946f421" translate="yes" xml:space="preserve">
          <source>View aliases</source>
          <target state="translated">View aliases</target>
        </trans-unit>
        <trans-unit id="31a7604a3c742ea454ff128dd267ca1e16472796" translate="yes" xml:space="preserve">
          <source>View source</source>
          <target state="translated">소스보기</target>
        </trans-unit>
        <trans-unit id="a11a63ecf581d6810e65e62968e93200e384c05b" translate="yes" xml:space="preserve">
          <source>View source on GitHub</source>
          <target state="translated">GitHub에서 소스보기</target>
        </trans-unit>
        <trans-unit id="6737eca3bba31c47e227cdb4b10d88148b47097b" translate="yes" xml:space="preserve">
          <source>Visit the &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/input&quot;&gt;tutorial&lt;/a&gt; on distributed input for more examples and caveats.</source>
          <target state="translated">Visit the &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/input&quot;&gt;tutorial&lt;/a&gt; on distributed input for more examples and caveats.</target>
        </trans-unit>
        <trans-unit id="d07bb0bd533bc6a8f9cecf11b29b925415f286e8" translate="yes" xml:space="preserve">
          <source>Vocabulary information for warm-starting.</source>
          <target state="translated">웜 스타트를위한 어휘 정보.</target>
        </trans-unit>
        <trans-unit id="1ec053f91b0e4f43b0ff9856f1f431195070e64b" translate="yes" xml:space="preserve">
          <source>WARNING: Experimental interface, subject to change.</source>
          <target state="translated">경고 : 실험 인터페이스는 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec7aa071ac596c65b3d8973549288f00dd9060fe" translate="yes" xml:space="preserve">
          <source>WARNING: If &lt;code&gt;sloppy&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the order of produced elements is not deterministic.</source>
          <target state="translated">경고 : 경우에 &lt;code&gt;sloppy&lt;/code&gt; 있다 &lt;code&gt;True&lt;/code&gt; 이면 생성 된 요소의 순서가 결정적이지 않습니다.</target>
        </trans-unit>
        <trans-unit id="20a9fc48c3697f74feddc8ad93eb4c90b0d2bc47" translate="yes" xml:space="preserve">
          <source>WARNING: This function is nondeterministic, since it starts a separate thread for each tensor.</source>
          <target state="translated">경고 :이 기능은 각 텐서마다 별도의 스레드를 시작하므로 결정적이지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e747855abeb092a18ebf6b91688f8d9d0024a93d" translate="yes" xml:space="preserve">
          <source>WARNING: tf.Variable objects by default have a non-intuitive memory model. A Variable is represented internally as a mutable Tensor which can non-deterministically alias other Tensors in a graph. The set of operations which consume a Variable and can lead to aliasing is undetermined and can change across TensorFlow versions. Avoid writing code which relies on the value of a Variable either changing or not changing as other operations happen. For example, using Variable objects or simple functions thereof as predicates in a &lt;a href=&quot;../../cond&quot;&gt;&lt;code&gt;tf.cond&lt;/code&gt;&lt;/a&gt; is dangerous and error-prone:</source>
          <target state="translated">경고 : tf.Variable 객체에는 기본적으로 직관적이지 않은 메모리 모델이 있습니다. 변수는 내부적으로 변경 가능한 텐서로 표시되며, 그래프에서 다른 텐서의 비 결정적으로 별명을 지정할 수 있습니다. 변수를 소비하고 앨리어싱을 유발할 수있는 일련의 작업은 결정되지 않으며 TensorFlow 버전에서 변경 될 수 있습니다. 다른 연산이 발생할 때 변하거나 변하지 않는 변수의 값에 의존하는 코드를 작성하지 마십시오. 예를 들어, &lt;a href=&quot;../../cond&quot;&gt; &lt;code&gt;tf.cond&lt;/code&gt; &lt;/a&gt; 에서 변수 객체 또는 간단한 함수를 술어로 사용하면 위험하고 오류가 발생하기 쉽습니다.</target>
        </trans-unit>
        <trans-unit id="1d2a3e7fd14becda3ad79ab623ee02dcfb3c4815" translate="yes" xml:space="preserve">
          <source>WRONG:</source>
          <target state="translated">WRONG:</target>
        </trans-unit>
        <trans-unit id="b55f8ed036408ccdecc6c75f53cbf2cbe602c833" translate="yes" xml:space="preserve">
          <source>Wait for threads to terminate.</source>
          <target state="translated">스레드가 종료 될 때까지 기다리십시오.</target>
        </trans-unit>
        <trans-unit id="480279e397ed207ff6dcb2138e6d19a402c6acab" translate="yes" xml:space="preserve">
          <source>Wait till the Coordinator is told to stop.</source>
          <target state="translated">코디네이터가 중지 할 때까지 기다리십시오.</target>
        </trans-unit>
        <trans-unit id="fd3c945de3bbe026196979524e2513d29b8047c4" translate="yes" xml:space="preserve">
          <source>Wait until the thread terminates.</source>
          <target state="translated">스레드가 끝날 때까지 기다리십시오.</target>
        </trans-unit>
        <trans-unit id="5fcf196e2d36a225ff59aa7e988a3bab54a62688" translate="yes" xml:space="preserve">
          <source>Warm-start all TRAINABLE variables:</source>
          <target state="translated">모든 TRAINABLE 변수를 웜 스타트하십시오.</target>
        </trans-unit>
        <trans-unit id="77a75fce3b44ee7c1b7330f94b51f223f12aced8" translate="yes" xml:space="preserve">
          <source>Warm-start all variables (including non-TRAINABLE):</source>
          <target state="translated">모든 변수를 웜 스타트 (트레이닝 가능하지 않음 포함)</target>
        </trans-unit>
        <trans-unit id="385cb07f1382ec501efc556d03af6fccd0c3bc88" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the embedding parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in the current model:</source>
          <target state="translated">모든 가중치를 웜 스타트하지만 &lt;code&gt;sc_vocab_file&lt;/code&gt; 에 해당하는 내장 매개 변수 는 현재 모델에서 사용 된 것과 다른 vocab을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="584c6949a375264781ffa7a3f30012c7d4ac99ba" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in current checkpoint and the parameters corresponding to &lt;code&gt;sc_vocab_list&lt;/code&gt; have a different name from the current checkpoint:</source>
          <target state="translated">모든 가중치를 웜 스타트하지만 &lt;code&gt;sc_vocab_file&lt;/code&gt; 에 해당하는 매개 변수 는 현재 검사 점에서 사용 된 것과 다른 vocab을 &lt;code&gt;sc_vocab_list&lt;/code&gt; 해당하는 매개 변수는 현재 검사 점과 다른 이름을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="aee3196d9918b3cb93e150d394afe537d71e12b7" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in current checkpoint, and only 100 of those entries were used:</source>
          <target state="translated">모든 가중치를 웜 스타트하지만 &lt;code&gt;sc_vocab_file&lt;/code&gt; 에 해당하는 매개 변수 는 현재 검사 점에서 사용 된 것과 다른 vocab 을 가지며 해당 항목 중 100 개만 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="cdac2057c8ebeb309f57754501e81125700d9646" translate="yes" xml:space="preserve">
          <source>Warm-start all weights in the model (input layer and hidden weights). Either the directory or a specific checkpoint can be provided (in the case of the former, the latest checkpoint will be used):</source>
          <target state="translated">모델의 모든 가중치 (입력 레이어 및 숨겨진 가중치)를 웜 스타트합니다. 디렉토리 또는 특정 체크 포인트를 제공 할 수 있습니다 (이전의 경우 최신 체크 포인트가 사용됨).</target>
        </trans-unit>
        <trans-unit id="5b13568e15d19ce108f966610dd72d6f433fdac5" translate="yes" xml:space="preserve">
          <source>Warm-start non-TRAINABLE variables &quot;v1&quot;, &quot;v1/Momentum&quot;, and &quot;v2&quot; but not &quot;v2/momentum&quot;:</source>
          <target state="translated">비 훈련 변수 &quot;v1&quot;, &quot;v1 / Momentum&quot;및 &quot;v2&quot;를 웜 스타트하지만 &quot;v2 / momentum&quot;은 사용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a2a6608bb90e8edcca0d24db3c7888d49894818d" translate="yes" xml:space="preserve">
          <source>Warm-start only &lt;code&gt;sc_vocab_file&lt;/code&gt; embeddings (and no other variables), which have a different vocab from the one used in the current model:</source>
          <target state="translated">현재 모델에서 사용 된 것과 다른 vocab을 갖는 &lt;code&gt;sc_vocab_file&lt;/code&gt; 임베드 만 (웜 과 다른 변수는 제외) 웜 스타트 만하십시오 .</target>
        </trans-unit>
        <trans-unit id="8454c4aaa6884d63b21f47b182bea32dff06b1d9" translate="yes" xml:space="preserve">
          <source>Warm-start only the embeddings (input layer):</source>
          <target state="translated">임베딩 (입력 레이어) 만 웜 스타트합니다.</target>
        </trans-unit>
        <trans-unit id="053dd1f00688a5e1c6f7bb11994384338437241d" translate="yes" xml:space="preserve">
          <source>Warm-starts a model using the given settings.</source>
          <target state="translated">주어진 설정을 사용하여 모델을 웜 스타트합니다.</target>
        </trans-unit>
        <trans-unit id="a935669a6f94647cbab43dac31886ff159458161" translate="yes" xml:space="preserve">
          <source>Warning class expected to be triggered.</source>
          <target state="translated">Warning class expected to be triggered.</target>
        </trans-unit>
        <trans-unit id="005fd2adc416b2f84c9268603969a31365422ae0" translate="yes" xml:space="preserve">
          <source>We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.</source>
          <target state="translated">우리는 훈련의 시작에서 잊어 버리는 규모를 줄이기 위해 forget_bias (default : 1)를 forget gate의 바이어스에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="ea902771c0c609fef23987d09209f89de6ea6624" translate="yes" xml:space="preserve">
          <source>We assume that the word frequencies follow Zipf's law (s=1) to derive a numerical approximation of frequency(rank):</source>
          <target state="translated">우리는 단어 frequency가 Zipf의 법칙 (s = 1)에 따라 주파수 (순위)의 수치 근사를 도출한다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="ee7344fa99f19a3db805ed04f05f653d28a573dc" translate="yes" xml:space="preserve">
          <source>We call it an 'accidental hit' when one of the target classes matches one of the sampled classes. This operation reports accidental hits as triples &lt;code&gt;(index, id, weight)&lt;/code&gt;, where &lt;code&gt;index&lt;/code&gt; represents the row number in &lt;code&gt;true_classes&lt;/code&gt;, &lt;code&gt;id&lt;/code&gt; represents the position in &lt;code&gt;sampled_candidates&lt;/code&gt;, and weight is &lt;code&gt;-FLOAT_MAX&lt;/code&gt;.</source>
          <target state="translated">대상 클래스 중 하나가 샘플링 된 클래스 중 하나와 일치 할 때이를 '우발적 히트'라고합니다. 이 조작 실수로 트리플 히트보고 &lt;code&gt;(index, id, weight)&lt;/code&gt; , &lt;code&gt;index&lt;/code&gt; 내의 행 번호를 나타내고 &lt;code&gt;true_classes&lt;/code&gt; 를 , &lt;code&gt;id&lt;/code&gt; 있는 위치 나타낸다 &lt;code&gt;sampled_candidates&lt;/code&gt; 를 , 체중이다 &lt;code&gt;-FLOAT_MAX&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3e90ccb70a2df6831216576e3e93ad8f84e29aaa" translate="yes" xml:space="preserve">
          <source>We can again draw the effect, this time using the symbols &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;o&lt;/code&gt; to distinguish the patches:</source>
          <target state="translated">이번에는 패치를 구별하기 위해 &lt;code&gt;*&lt;/code&gt; , &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;+&lt;/code&gt; 및 &lt;code&gt;o&lt;/code&gt; 기호를 사용하여 효과를 다시 그릴 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3ec1d6719eaed84adca414b94403fe4b9bce8b6a" translate="yes" xml:space="preserve">
          <source>We can also, insert entire slices of a higher rank tensor all at once. For example, if we wanted to insert two slices in the first dimension of a rank-3 tensor with two matrices of new values.</source>
          <target state="translated">또한 상위 텐서의 전체 슬라이스를 한 번에 삽입 할 수도 있습니다. 예를 들어, 새로운 값의 두 행렬로 순위 3 텐서의 첫 번째 차원에 두 개의 슬라이스를 삽입하려는 경우.</target>
        </trans-unit>
        <trans-unit id="75a6bd3ac0e29b2d99ddde37882f73b9484c4c89" translate="yes" xml:space="preserve">
          <source>We can compute the mean and variance of the batch</source>
          <target state="translated">배치의 평균과 분산을 계산할 수 있습니다</target>
        </trans-unit>
        <trans-unit id="141a486e547cdd8105ede80b487557001daeacfc" translate="yes" xml:space="preserve">
          <source>We can construct a CsvDataset from it as follows:</source>
          <target state="translated">다음과 같이 CsvDataset을 구성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="798032cd2519c55922a86a65a97aa8223721d2bf" translate="yes" xml:space="preserve">
          <source>We can use arguments:</source>
          <target state="translated">우리는 인수를 사용할 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="bfe0a32bb694084e323bdf39d8ca5b3bf5acf3b4" translate="yes" xml:space="preserve">
          <source>We first define two int64 tensors &lt;code&gt;paddings&lt;/code&gt; and &lt;code&gt;crops&lt;/code&gt; of shape &lt;code&gt;[num_spatial_dims, 2]&lt;/code&gt; based on the value of &lt;code&gt;padding&lt;/code&gt; and the spatial dimensions of the &lt;code&gt;input&lt;/code&gt;:</source>
          <target state="translated">먼저 &lt;code&gt;padding&lt;/code&gt; 값 과 &lt;code&gt;input&lt;/code&gt; 의 공간 치수를 기반으로 두 개의 int64 텐서 &lt;code&gt;paddings&lt;/code&gt; 과 모양 &lt;code&gt;[num_spatial_dims, 2]&lt;/code&gt; &lt;code&gt;crops&lt;/code&gt; 을 정의 합니다 .</target>
        </trans-unit>
        <trans-unit id="c362e7cae577801d3a077b086c30ca6fb0a66da0" translate="yes" xml:space="preserve">
          <source>We first solve &lt;code&gt;x_0 = A_00.solve(y_0)&lt;/code&gt;. Proceeding inductively, we solve for &lt;code&gt;x_k&lt;/code&gt;, &lt;code&gt;k = 1..n&lt;/code&gt;, given &lt;code&gt;x_0..x_(k-1)&lt;/code&gt;.</source>
          <target state="translated">We first solve &lt;code&gt;x_0 = A_00.solve(y_0)&lt;/code&gt; . Proceeding inductively, we solve for &lt;code&gt;x_k&lt;/code&gt; , &lt;code&gt;k = 1..n&lt;/code&gt; , given &lt;code&gt;x_0..x_(k-1)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="5d6f96925005fe200b4a2292648a063218b208f4" translate="yes" xml:space="preserve">
          <source>We keep track of which flag is defined by which module so that we can later sort the flags by module.</source>
          <target state="translated">우리는 어떤 플래그가 어떤 모듈에 의해 정의되는지 추적하여 나중에 모듈별로 플래그를 정렬 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e09363c49b193f1ac1770f115d6d8136f2ddc43c" translate="yes" xml:space="preserve">
          <source>We next use the scale_factor to adjust min_range and max_range as follows:</source>
          <target state="translated">다음으로 scale_factor를 사용하여 다음과 같이 min_range 및 max_range를 조정합니다.</target>
        </trans-unit>
        <trans-unit id="6d1134ee5b3f5d2c7da4505ac717468ba6967616" translate="yes" xml:space="preserve">
          <source>We presuppose that the &lt;code&gt;sampled_candidates&lt;/code&gt; are unique.</source>
          <target state="translated">우리는 것을 전제로 &lt;code&gt;sampled_candidates&lt;/code&gt; 이 독특합니다.</target>
        </trans-unit>
        <trans-unit id="f093997d3edad673438d9d10c06d984c5a8a5f76" translate="yes" xml:space="preserve">
          <source>We recommend that descendants of &lt;code&gt;Layer&lt;/code&gt; implement the following methods:</source>
          <target state="translated">&lt;code&gt;Layer&lt;/code&gt; 의 하위 항목은 다음 방법을 구현하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="bd2045e3591e6b6a7950e1ad85e808c50d3fcbe0" translate="yes" xml:space="preserve">
          <source>We recommend using &lt;a href=&quot;https://github.com/tensorflow/io&quot;&gt;https://github.com/tensorflow/io&lt;/a&gt; to load your HDF5 data into a tf.data Dataset and passing that dataset to Keras.</source>
          <target state="translated">We recommend using &lt;a href=&quot;https://github.com/tensorflow/io&quot;&gt;https://github.com/tensorflow/io&lt;/a&gt; to load your HDF5 data into a tf.data Dataset and passing that dataset to Keras.</target>
        </trans-unit>
        <trans-unit id="664f0b931d5bb248f3df03fa1a45aa162e8bb7fc" translate="yes" xml:space="preserve">
          <source>We recommend using https://github.com/tensorflow/io to load your HDF5 data into a tf.data Dataset and passing that dataset to Keras.</source>
          <target state="translated">https://github.com/tensorflow/io를 사용하여 HDF5 데이터를 tf.data 데이터 세트에로드하고 해당 데이터 세트를 Keras에 전달하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="e6b1e174afc80b9d454ce2447d6b106dd858888d" translate="yes" xml:space="preserve">
          <source>We retrieve the information from the GCE APIs every time this method is called.</source>
          <target state="translated">이 메소드가 호출 될 때마다 GCE API에서 정보를 검색합니다.</target>
        </trans-unit>
        <trans-unit id="2e18048f9e04e3012f6199dfe11668988e00f3d9" translate="yes" xml:space="preserve">
          <source>We retrieve the information from the Kubernetes master every time this method is called.</source>
          <target state="translated">이 메소드가 호출 될 때마다 Kubernetes 마스터에서 정보를 검색합니다.</target>
        </trans-unit>
        <trans-unit id="ee86e3990a38d113aeda3e686a64aed6a6c21a17" translate="yes" xml:space="preserve">
          <source>We specify the size-related attributes as:</source>
          <target state="translated">크기 관련 속성을 다음과 같이 지정합니다.</target>
        </trans-unit>
        <trans-unit id="88a4bbb7071448507fb5ea156b09b64279b62233" translate="yes" xml:space="preserve">
          <source>We use the following notation for (complex) matrix and right-hand sides in the batch:</source>
          <target state="translated">We use the following notation for (complex) matrix and right-hand sides in the batch:</target>
        </trans-unit>
        <trans-unit id="d371d0dcdb3a29109c4aec0bd9662ca4cc225c96" translate="yes" xml:space="preserve">
          <source>We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers).</source>
          <target state="translated">입력 데이터 세트가 전역 배치 크기로 배치되어 있다고 가정합니다. 이 가정을 통해 각 배치를 모든 복제본 (한 명 이상의 작업자)으로 나누기 위해 최선을 다할 것입니다.</target>
        </trans-unit>
        <trans-unit id="c1e896bf769945f02ba4270d6713548acfff861f" translate="yes" xml:space="preserve">
          <source>Web-safe means that the encoder uses - and _ instead of + and /.</source>
          <target state="translated">웹 안전은 인코더가 + 및 / 대신-및 _를 사용함을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="a5ecd420b68c6ca62b2880bcb67d127d7527fd45" translate="yes" xml:space="preserve">
          <source>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, some entries in &lt;code&gt;layer.updates&lt;/code&gt; may be dependent on &lt;code&gt;a&lt;/code&gt; and some on &lt;code&gt;b&lt;/code&gt;. This method automatically keeps track of dependencies.</source>
          <target state="translated">가중치 업데이트 (예 : BatchNormalization 레이어의 이동 평균 및 분산 업데이트)는 레이어를 호출 할 때 전달 된 입력에 따라 달라질 수 있습니다. 다른 입력에 동일한 층 재사용 할 때 이에 및 &lt;code&gt;b&lt;/code&gt; , 일부 항목 &lt;code&gt;layer.updates&lt;/code&gt; 은 에 의존 할 수 의 일부 &lt;code&gt;b&lt;/code&gt; . 이 방법은 자동으로 종속성을 추적합니다. &lt;code&gt;a&lt;/code&gt; &lt;code&gt;a&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="012994d31dee39cea10e34d13b123636095aef6d" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has shape &lt;code&gt;[batch_size]&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">가중 손실 &lt;code&gt;logits&lt;/code&gt; 과 동일한 유형의 &lt;code&gt;Tensor&lt;/code&gt; . &lt;code&gt;reduction&lt;/code&gt; 가 &lt;code&gt;NONE&lt;/code&gt; 인 경우 모양은 &lt;code&gt;[batch_size]&lt;/code&gt; 입니다 . 그렇지 않으면 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="9d1005c1318b48c01558d66a2ac013b81040339a" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;labels&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">가중 손실 &lt;code&gt;logits&lt;/code&gt; 과 동일한 유형의 &lt;code&gt;Tensor&lt;/code&gt; . 경우 &lt;code&gt;reduction&lt;/code&gt; 없는 &lt;code&gt;NONE&lt;/code&gt; ,이 같은 모양이 &lt;code&gt;labels&lt;/code&gt; ; 그렇지 않으면 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="834de6c05706f24229c420c387a2f8167f0b45d5" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;logits&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">가중 손실 &lt;code&gt;logits&lt;/code&gt; 과 동일한 유형의 &lt;code&gt;Tensor&lt;/code&gt; . 경우 &lt;code&gt;reduction&lt;/code&gt; 없는 &lt;code&gt;NONE&lt;/code&gt; ,이 같은 형상을 갖는다 &lt;code&gt;logits&lt;/code&gt; 을 ; 그렇지 않으면 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="b075652dc6120aaf1d41f8a4dc2daf694ffa4a69" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;losses&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;losses&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">가중 손실 &lt;code&gt;Tensor&lt;/code&gt; 와 동일한 유형의 &lt;code&gt;losses&lt;/code&gt; . 경우 &lt;code&gt;reduction&lt;/code&gt; 없는 &lt;code&gt;NONE&lt;/code&gt; ,이 같은 형상을 갖는다 &lt;code&gt;losses&lt;/code&gt; ; 그렇지 않으면 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="f8f11d4c7a3da229788db34932231d37e05fbe0b" translate="yes" xml:space="preserve">
          <source>Weighted loss float &lt;code&gt;Tensor&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has shape &lt;code&gt;[batch_size, d0, .. dN-1]&lt;/code&gt;; otherwise, it is scalar. (Note &lt;code&gt;dN-1&lt;/code&gt; because all loss functions reduce by 1 dimension, usually axis=-1.)</source>
          <target state="translated">가중 손실 플로트 &lt;code&gt;Tensor&lt;/code&gt; . 경우 &lt;code&gt;reduction&lt;/code&gt; 없는 &lt;code&gt;NONE&lt;/code&gt; 본 모양 갖는다 &lt;code&gt;[batch_size, d0, .. dN-1]&lt;/code&gt; ; 그렇지 않으면 스칼라입니다. ( 모든 손실 함수는 일반적으로 축 = 1 인 1 차원 씩 줄어들 기 때문에 &lt;code&gt;dN-1&lt;/code&gt; 에 유의하십시오 .)</target>
        </trans-unit>
        <trans-unit id="37a14c45a506771a19d570992ffb74e2a8ababd3" translate="yes" xml:space="preserve">
          <source>Weighted loss float &lt;code&gt;Tensor&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;labels&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">가중 손실 플로트 &lt;code&gt;Tensor&lt;/code&gt; . 경우 &lt;code&gt;reduction&lt;/code&gt; 없는 &lt;code&gt;NONE&lt;/code&gt; ,이 같은 모양이 &lt;code&gt;labels&lt;/code&gt; ; 그렇지 않으면 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="23f94c92282719a4052aebc7d476aba406e80f05" translate="yes" xml:space="preserve">
          <source>Weights values as a list of numpy arrays.</source>
          <target state="translated">numpy 배열의 목록으로 가중치 값.</target>
        </trans-unit>
        <trans-unit id="79699365f3dd7ea5c60a1201001103f0e4d8e414" translate="yes" xml:space="preserve">
          <source>What &lt;code&gt;master&lt;/code&gt; string to use</source>
          <target state="translated">무엇 &lt;code&gt;master&lt;/code&gt; 사용에 문자열</target>
        </trans-unit>
        <trans-unit id="8126fc60320a74dd637a7bb9a583488f0b14222c" translate="yes" xml:space="preserve">
          <source>What happens in &lt;code&gt;adapt&lt;/code&gt;: Compute mean and variance of the data and store them as the layer's weights. &lt;code&gt;adapt&lt;/code&gt; should be called before &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;evaluate&lt;/code&gt;, or &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">무엇에 어떻게 &lt;code&gt;adapt&lt;/code&gt; : 계산은 평균과 데이터의 분산과 레이어의 무게로 저장할 수 있습니다. &lt;code&gt;adapt&lt;/code&gt; 전에 호출되어야한다 &lt;code&gt;fit&lt;/code&gt; , &lt;code&gt;evaluate&lt;/code&gt; , 또는 &lt;code&gt;predict&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f4112aa72544bbd54c25bceb58f7f58b932e32d8" translate="yes" xml:space="preserve">
          <source>What to return in test phase (tensor or callable that returns a tensor).</source>
          <target state="translated">What to return in test phase (tensor or callable that returns a tensor).</target>
        </trans-unit>
        <trans-unit id="17f24426650f12852c89205be844ef319f76644b" translate="yes" xml:space="preserve">
          <source>What to return in train phase (tensor or callable that returns a tensor).</source>
          <target state="translated">What to return in train phase (tensor or callable that returns a tensor).</target>
        </trans-unit>
        <trans-unit id="def8b9f5f06fb4c06699fb40ca04320258ae46d4" translate="yes" xml:space="preserve">
          <source>What to return otherwise (tensor or callable that returns a tensor).</source>
          <target state="translated">What to return otherwise (tensor or callable that returns a tensor).</target>
        </trans-unit>
        <trans-unit id="e11f03a4f4c3dbc0b65542222647d127c17f0502" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">What's under the hood of this method, when we say the &lt;a href=&quot;../../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../../data/experimental/autoshardpolicy&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt; &lt;/a&gt; through &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#AUTO&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt; &lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../../data/tfrecorddataset&quot;&gt; &lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;../../../../data/textlinedataset&quot;&gt; &lt;code&gt;tf.data.TextLineDataset&lt;/code&gt; &lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt; &lt;/a&gt; to be &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#OFF&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="6f8e0522d952680a4b2c6fbddea797a2677b1547" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">What's under the hood of this method, when we say the &lt;a href=&quot;../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../data/experimental/autoshardpolicy&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt; &lt;/a&gt; through &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../data/experimental/autoshardpolicy#AUTO&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt; &lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../data/tfrecorddataset&quot;&gt; &lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;../../../data/textlinedataset&quot;&gt; &lt;code&gt;tf.data.TextLineDataset&lt;/code&gt; &lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt; &lt;/a&gt; to be &lt;a href=&quot;../../../data/experimental/autoshardpolicy#OFF&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="2a0d7396d4c78509ccd91f8c0cfaa89f6bac9680" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">What's under the hood of this method, when we say the &lt;a href=&quot;../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../data/experimental/autoshardpolicy&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt; &lt;/a&gt; through &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../data/experimental/autoshardpolicy#AUTO&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt; &lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../data/tfrecorddataset&quot;&gt; &lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;../../data/textlinedataset&quot;&gt; &lt;code&gt;tf.data.TextLineDataset&lt;/code&gt; &lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt; &lt;/a&gt; to be &lt;a href=&quot;../../data/experimental/autoshardpolicy#OFF&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5e73d3cb0a89b521b7e9847b33f0048e348f89d5" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">What's under the hood of this method, when we say the &lt;a href=&quot;../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../data/experimental/autoshardpolicy&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt; &lt;/a&gt; through &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;. By default, it is set to &lt;a href=&quot;../data/experimental/autoshardpolicy#AUTO&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt; &lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../data/tfrecorddataset&quot;&gt; &lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;../data/textlinedataset&quot;&gt; &lt;code&gt;tf.data.TextLineDataset&lt;/code&gt; &lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../data/experimental/distributeoptions#auto_shard_policy&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt; &lt;/a&gt; to be &lt;a href=&quot;../data/experimental/autoshardpolicy#OFF&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c4633647d41640957a472ab68c0262290690dfdb" translate="yes" xml:space="preserve">
          <source>When 'TF_CONFIG' environment variable is set, it parses cluster_spec, task_type and task_id from 'TF_CONFIG' and turns into a multi-worker strategy which mirrored models on GPUs of all machines in a cluster. In the current implementation, it uses all GPUs in a cluster and it assumes all workers have the same number of GPUs.</source>
          <target state="translated">When 'TF_CONFIG' environment variable is set, it parses cluster_spec, task_type and task_id from 'TF_CONFIG' and turns into a multi-worker strategy which mirrored models on GPUs of all machines in a cluster. In the current implementation, it uses all GPUs in a cluster and it assumes all workers have the same number of GPUs.</target>
        </trans-unit>
        <trans-unit id="97233c5c3c8d20a5e9ffbdbe3b7ccb6a51db597b" translate="yes" xml:space="preserve">
          <source>When 'TF_CONFIG' environment variable is set, it parses cluster_spec, task_type and task_id from 'TF_CONFIG' and turns into a multi-worker strategy which mirrores models on GPUs of all machines in a cluster. In the current implementation, it uses all GPUs in a cluster and it assumes all workers have the same number of GPUs.</source>
          <target state="translated">'TF_CONFIG'환경 변수가 설정되면 'TF_CONFIG'에서 cluster_spec, task_type 및 task_id를 구문 분석하고 클러스터에있는 모든 머신의 GPU에서 모델을 미러링하는 다중 작업자 전략으로 바뀝니다. 현재 구현에서는 클러스터의 모든 GPU를 사용하며 모든 작업자가 동일한 수의 GPU를 가지고 있다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="f5e1d55c89f446271b947fe72a857c539201c7dc" translate="yes" xml:space="preserve">
          <source>When 'antialias' is true, the sampling filter will anti-alias the input image as well as interpolate. When downsampling an image with &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;anti-aliasing&lt;/a&gt; the sampling filter kernel is scaled in order to properly anti-alias the input image signal. 'antialias' has no effect when upsampling an image.</source>
          <target state="translated">'앤티 앨리어스'가 true 인 경우 샘플링 필터는 입력 이미지를 앤티 앨리어싱하고 보간합니다. &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;앤티 앨리어싱을 사용&lt;/a&gt; 하여 이미지를 다운 샘플링 하면 입력 이미지 신호를 적절히 앤티 앨리어싱하기 위해 샘플링 필터 커널의 크기를 조정합니다. 앤티 앨리어스는 이미지를 업 샘플링 할 때 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a125e391abcaadbe33801ab8eddcf23c35279113" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;True&lt;/code&gt;, additional assertions might be embedded in the graph. Default value: &lt;code&gt;False&lt;/code&gt; (i.e., no graph assertions are added).</source>
          <target state="translated">When &lt;code&gt;True&lt;/code&gt; , additional assertions might be embedded in the graph. Default value: &lt;code&gt;False&lt;/code&gt; (i.e., no graph assertions are added).</target>
        </trans-unit>
        <trans-unit id="735a581f2af66e2fcb181fd57a9ddb799b2eb257" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;antialias&lt;/code&gt; is true, the sampling filter will anti-alias the input image as well as interpolate. When downsampling an image with &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;anti-aliasing&lt;/a&gt; the sampling filter kernel is scaled in order to properly anti-alias the input image signal. &lt;code&gt;antialias&lt;/code&gt; has no effect when upsampling an image:</source>
          <target state="translated">When &lt;code&gt;antialias&lt;/code&gt; is true, the sampling filter will anti-alias the input image as well as interpolate. When downsampling an image with &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;anti-aliasing&lt;/a&gt; the sampling filter kernel is scaled in order to properly anti-alias the input image signal. &lt;code&gt;antialias&lt;/code&gt; has no effect when upsampling an image:</target>
        </trans-unit>
        <trans-unit id="9aacbccc1a065c95aea812e79f50b9fb0809ae76" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;distribute&lt;/code&gt; or &lt;code&gt;experimental_distribute.train_distribute&lt;/code&gt; and &lt;code&gt;experimental_distribute.remote_cluster&lt;/code&gt; is set, this method will start a client running on the current host which connects to the &lt;code&gt;remote_cluster&lt;/code&gt; for training and evaluation.</source>
          <target state="translated">때 &lt;code&gt;distribute&lt;/code&gt; 또는 &lt;code&gt;experimental_distribute.train_distribute&lt;/code&gt; 및 &lt;code&gt;experimental_distribute.remote_cluster&lt;/code&gt; 이 설정되어,이 방법은 연결 현재 호스트에서 실행되는 클라이언트 시작합니다 &lt;code&gt;remote_cluster&lt;/code&gt; 교육 및 평가를.</target>
        </trans-unit>
        <trans-unit id="01497840225ce1bb0dcac0f82d53eb47485d512b" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;enable&lt;/code&gt; is set to None, an appropriate value will be picked automatically. The value picked may change between TensorFlow releases.</source>
          <target state="translated">&lt;code&gt;enable&lt;/code&gt; 이 None으로 설정 되면 적절한 값이 자동으로 선택됩니다. 선택된 값은 TensorFlow 릴리스간에 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3a92e9802f62e2ea66187929bf4db58022152e26" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;nesterov=False&lt;/code&gt;, this rule becomes:</source>
          <target state="translated">When &lt;code&gt;nesterov=False&lt;/code&gt; , this rule becomes:</target>
        </trans-unit>
        <trans-unit id="c64f1c94124dc71ee97e99b0200e5f2b295d3204" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;queues&lt;/code&gt; is not a list of &lt;code&gt;QueueBase&lt;/code&gt; objects, or when the data types of &lt;code&gt;queues&lt;/code&gt; are not all the same.</source>
          <target state="translated">When &lt;code&gt;queues&lt;/code&gt; is not a list of &lt;code&gt;QueueBase&lt;/code&gt; objects, or when the data types of &lt;code&gt;queues&lt;/code&gt; are not all the same.</target>
        </trans-unit>
        <trans-unit id="d965c5a7f6cd6a88315eeacabf229f5d784d0a97" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;rescale&lt;/code&gt; is set to a value, rescaling is applied to sample data before computing the internal data stats.</source>
          <target state="translated">When &lt;code&gt;rescale&lt;/code&gt; is set to a value, rescaling is applied to sample data before computing the internal data stats.</target>
        </trans-unit>
        <trans-unit id="dda3a421cfde51cef62429c270613980d6f95f4a" translate="yes" xml:space="preserve">
          <source>When True, &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; may generate fewer, graphs that are less specialized on input shapes.</source>
          <target state="translated">When True, &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; may generate fewer, graphs that are less specialized on input shapes.</target>
        </trans-unit>
        <trans-unit id="d2203660cd3c839935256a662d4fa7b611fae192" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="translated">When a &lt;a href=&quot;../../../../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt; , we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; , &lt;code&gt;model.evaluate&lt;/code&gt; , &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</target>
        </trans-unit>
        <trans-unit id="a77de95801a4cff5844b758a2ad8179bb09d2c1a" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="translated">When a &lt;a href=&quot;../../../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt; , we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; , &lt;code&gt;model.evaluate&lt;/code&gt; , &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</target>
        </trans-unit>
        <trans-unit id="5bc23220d2b015520f09eca2cbd06a9bdbef9712" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="translated">When a &lt;a href=&quot;../../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt; , we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; , &lt;code&gt;model.evaluate&lt;/code&gt; , &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</target>
        </trans-unit>
        <trans-unit id="8157664b0c676127bfe8a48270b035a599288e2e" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="translated">When a &lt;a href=&quot;../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt; , we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt; , &lt;code&gt;model.fit&lt;/code&gt; , &lt;code&gt;model.evaluate&lt;/code&gt; , &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</target>
        </trans-unit>
        <trans-unit id="5d3c29ba7c1915daf444dfa9480a120b9d259cc0" translate="yes" xml:space="preserve">
          <source>When a DistributionStrategy is used, this function may only be called in a cross-replica context.</source>
          <target state="translated">DistributionStrategy를 사용하는 경우이 함수는 상호 복제 컨텍스트에서만 호출 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9ee3d0211a7961d1892863b309bfae8dc55d02c2" translate="yes" xml:space="preserve">
          <source>When a Summary op is instantiated, a SummaryDescription of associated metadata is stored in its NodeDef. This method retrieves the description.</source>
          <target state="translated">Summary op가 인스턴스화되면 관련 메타 데이터의 SummaryDescription이 NodeDef에 저장됩니다. 이 메소드는 설명을 검색합니다.</target>
        </trans-unit>
        <trans-unit id="468fae491a93893feb36e0fc65b7a7ac2c612355" translate="yes" xml:space="preserve">
          <source>When a global &lt;a href=&quot;../../../../keras/mixed_precision/experimental/policy&quot;&gt;&lt;code&gt;tf.keras.mixed_precision.experimental.Policy&lt;/code&gt;&lt;/a&gt; is set, a Keras layer's dtype will default to the global policy instead of floatx. Layers will automatically cast inputs to the policy's compute_dtype.</source>
          <target state="translated">When a global &lt;a href=&quot;../../../../keras/mixed_precision/experimental/policy&quot;&gt; &lt;code&gt;tf.keras.mixed_precision.experimental.Policy&lt;/code&gt; &lt;/a&gt; is set, a Keras layer's dtype will default to the global policy instead of floatx. Layers will automatically cast inputs to the policy's compute_dtype.</target>
        </trans-unit>
        <trans-unit id="067c92a0436a2a055b3eb1915882349b4d833f6c" translate="yes" xml:space="preserve">
          <source>When a op's float-type output tensor contains any Infinity or NaN, an &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt; will be thrown, with an error message that reveals the following information:</source>
          <target state="translated">When a op's float-type output tensor contains any Infinity or NaN, an &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt; &lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt; &lt;/a&gt; will be thrown, with an error message that reveals the following information:</target>
        </trans-unit>
        <trans-unit id="9e2c0bd0de968bcd753736a8da6d2db14223d5cf" translate="yes" xml:space="preserve">
          <source>When a op's float-type output tensor contains any Infinity or NaN, an &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt; will be thrown, with an error message that reveals the following information: - The type of the op that generated the tensor with bad numerics. - Data type (dtype) of the tensor. - Shape of the tensor (to the extent known at the time of eager execution or graph construction). - Name of the containing graph (if available). - (Graph mode only): The stack trace of the intra-graph op's creation, with a stack-height limit and a path-length limit for visual clarity. The stack frames that belong to the user's code (as opposed to tensorflow's internal code) are highlighted with a text arrow (&quot;-&amp;gt;&quot;). - (Eager mode only): How many of the offending tensor's elements are &lt;code&gt;Infinity&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, respectively.</source>
          <target state="translated">op의 float-type 출력 텐서에 Infinity 또는 NaN이 포함 된 경우 다음 정보를 나타내는 오류 메시지와 함께 &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt; &lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt; &lt;/a&gt; 가 발생 합니다 .- 잘못된 숫자로 텐서를 생성 한 op의 유형입니다. -텐서의 데이터 유형 (dtype). -텐서의 모양 (열망 한 실행 또는 그래프 구성시 알려진 범위까지). -포함 그래프의 이름입니다 (사용 가능한 경우). -(그래프 모드 만 해당) : 시각적 선명도를 위해 스택 높이 제한과 경로 길이 제한이있는 그래프 내 op 생성의 스택 추적. 사용자 코드에 속하는 스택 프레임 (tensorflow의 내부 코드가 아닌)은 텍스트 화살표 ( &quot;-&amp;gt;&quot;)로 강조 표시됩니다. -(열심 모드 전용) : 문제가되는 텐서의 수s의 요소 &lt;code&gt;Infinity&lt;/code&gt; 및 &lt;code&gt;NaN&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b898b5ad811100780dad4051c33f639fd70aa1be" translate="yes" xml:space="preserve">
          <source>When a tf.random operation is built with XLA, the implementation doesn't pass the user provided seed to the XLA compiler. As such, the XLA compiler generates a random number and uses it as a seed when compiling the operation. This implementation causes a violation of the Tensorflow defined semantics in two aspects. First, changing the value of the user defined seed doesn't change the numbers generated by the operation. Second, when a seed is not specified, running the program multiple times will generate the same numbers.</source>
          <target state="translated">tf.random 조작이 XLA로 빌드되면 구현은 사용자 제공 시드를 XLA 컴파일러에 전달하지 않습니다. 따라서 XLA 컴파일러는 난수를 생성하고 조작을 컴파일 할 때이를 시드로 사용합니다. 이 구현은 두 가지 측면에서 Tensorflow 정의 시맨틱을 위반합니다. 먼저, 사용자 정의 시드의 값을 변경해도 작업으로 생성 된 숫자는 변경되지 않습니다. 둘째, 시드가 지정되지 않은 경우 프로그램을 여러 번 실행하면 동일한 숫자가 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="493652c7a3597db99acb0cc5aae0cc36b4d15e47" translate="yes" xml:space="preserve">
          <source>When a trackable object is exported via &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save()&lt;/code&gt;&lt;/a&gt;, all the &lt;code&gt;Asset&lt;/code&gt;s reachable from it are copied into the SavedModel assets directory. Upon loading, the assets and the serialized functions that depend on them will refer to the correct filepaths inside the SavedModel directory.</source>
          <target state="translated">추적 가능한 객체를 &lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save()&lt;/code&gt; &lt;/a&gt; 를 통해 내 보내면 해당 객체 에서 도달 가능한 모든 &lt;code&gt;Asset&lt;/code&gt; 이 SavedModel 자산 디렉토리에 복사됩니다. 로드시 자산 및 이에 종속 된 직렬화 된 기능은 SavedModel 디렉토리 내의 올바른 파일 경로를 참조합니다.</target>
        </trans-unit>
        <trans-unit id="6f1183817b3726ec7266e15fa411e144c4d1a1bb" translate="yes" xml:space="preserve">
          <source>When accessing the value of a TensorShape dimension, use this utility, like this:</source>
          <target state="translated">TensorShape 차원의 값에 액세스 할 때 다음과 같이이 유틸리티를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5eeff351258cc34299c1acc2c35d15d6a807f5fe" translate="yes" xml:space="preserve">
          <source>When arguments have invalid value.</source>
          <target state="translated">When arguments have invalid value.</target>
        </trans-unit>
        <trans-unit id="e15681c5c4d660d694d5dcc65f824f2819119a6a" translate="yes" xml:space="preserve">
          <source>When attempting to multiply a nD tensor with a nD tensor, it reproduces the Theano behavior. (e.g. &lt;code&gt;(2, 3) * (4, 3, 5) -&amp;gt; (2, 4, 5)&lt;/code&gt;)</source>
          <target state="translated">nD 텐서와 nD 텐서를 곱하려고하면 Theano 동작을 재현합니다. (예 : &lt;code&gt;(2, 3) * (4, 3, 5) -&amp;gt; (2, 4, 5)&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="c84b6c98a486c1f1ed40714b3413c2d05ebb290e" translate="yes" xml:space="preserve">
          <source>When attempting to normalize on an empty ensemble or an ensemble of trees which have no splits. Or when attempting to normalize and feature importances have negative values.</source>
          <target state="translated">When attempting to normalize on an empty ensemble or an ensemble of trees which have no splits. Or when attempting to normalize and feature importances have negative values.</target>
        </trans-unit>
        <trans-unit id="1c06f27a3b19deaf8176a82559892b7885f9b161" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), determines the CPU budget to use. Values greater than the number of schedulable CPU cores are allowed but may result in CPU contention. If None, defaults to the number of schedulable CPU cores.</source>
          <target state="translated">&lt;code&gt;autotune&lt;/code&gt; 통해 자동 튜닝이 활성화되면 사용할 CPU 예산을 결정합니다. 예약 가능한 CPU 코어 수보다 큰 값은 허용되지만 CPU 경합이 발생할 수 있습니다. None 인 경우 기본적으로 예약 가능한 CPU 코어 수로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="aa75a73b6c69b49d6e19ddace76d0dc9e6d20863" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), determines whether to also autotune buffer sizes for datasets with parallelism. If None, defaults to False.</source>
          <target state="translated">&lt;code&gt;autotune&lt;/code&gt; 통해 자동 튜닝이 활성화 된 경우 병렬 처리가있는 데이터 세트의 버퍼 크기도 자동 튜닝할지 여부를 결정합니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="283f72551210c9459423ecb79011f0f8519629e6" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), identifies the algorithm to use for the autotuning optimization.</source>
          <target state="translated">&lt;code&gt;autotune&lt;/code&gt; 통해 자동 튜닝이 활성화되면 자동 튜닝 최적화에 사용할 알고리즘을 식별합니다.</target>
        </trans-unit>
        <trans-unit id="e6deb11b689fcef12dab268b644289f6586fd39c" translate="yes" xml:space="preserve">
          <source>When both &lt;code&gt;squeeze_dims&lt;/code&gt; and &lt;code&gt;axis&lt;/code&gt; are specified.</source>
          <target state="translated">When both &lt;code&gt;squeeze_dims&lt;/code&gt; and &lt;code&gt;axis&lt;/code&gt; are specified.</target>
        </trans-unit>
        <trans-unit id="32eb733d84fe74265b206fb4da02dbc0b66960e1" translate="yes" xml:space="preserve">
          <source>When building a complex model that uses many queues it is often difficult to gather all the queue runners that need to be run. This convenience function allows you to add a queue runner to a well known collection in the graph.</source>
          <target state="translated">많은 큐를 사용하는 복잡한 모델을 빌드 할 때 실행해야하는 모든 큐 러너를 수집하기가 어려운 경우가 많습니다. 이 편리한 기능을 사용하면 그래프에서 잘 알려진 콜렉션에 큐 러너를 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e5bcf632e0c577d3a09737c01de056496ad7c080" translate="yes" xml:space="preserve">
          <source>When building a machine learning model it is often convenient to distinguish between variables holding the trainable model parameters and other variables such as a &lt;code&gt;global step&lt;/code&gt; variable used to count training steps. To make this easier, the variable constructor supports a &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; parameter. If &lt;code&gt;True&lt;/code&gt;, the new variable is also added to the graph collection &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt;. The convenience function &lt;code&gt;trainable_variables()&lt;/code&gt; returns the contents of this collection. The various &lt;code&gt;Optimizer&lt;/code&gt; classes use this collection as the default list of variables to optimize.</source>
          <target state="translated">기계 학습 모델을 구축 할 때 훈련 가능한 모델 파라미터를 보유하는 변수와 훈련 단계를 계산하는 데 사용되는 &lt;code&gt;global step&lt;/code&gt; 변수 와 같은 다른 변수를 구분하는 것이 종종 편리 합니다. 이를 쉽게하기 위해 변수 생성자는 &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; 매개 변수를 지원합니다 . 경우 &lt;code&gt;True&lt;/code&gt; , 새 변수는 그래프 콜렉션에 추가됩니다 &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; . 편리한 함수 &lt;code&gt;trainable_variables()&lt;/code&gt; 는이 컬렉션의 내용을 반환합니다. 다양한 &lt;code&gt;Optimizer&lt;/code&gt; 클래스는이 콜렉션을 최적화 할 기본 변수 목록으로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="68a44f742db33e692724433083d54a9691c316e1" translate="yes" xml:space="preserve">
          <source>When building a machine learning model it is often convenient to distinguish between variables holding trainable model parameters and other variables such as a &lt;code&gt;step&lt;/code&gt; variable used to count training steps. To make this easier, the variable constructor supports a &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; parameter. &lt;a href=&quot;gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; watches trainable variables by default:</source>
          <target state="translated">기계 학습 모델을 구축 할 때 훈련 가능한 모델 파라미터를 보유하는 변수와 훈련 단계를 계산하는 데 사용되는 &lt;code&gt;step&lt;/code&gt; 변수 와 같은 다른 변수를 구분하는 것이 종종 편리 합니다. 이를 쉽게하기 위해 변수 생성자는 &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; 매개 변수를 지원합니다 . &lt;a href=&quot;gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt; 는 기본적으로 학습 가능한 변수를 감시합니다.</target>
        </trans-unit>
        <trans-unit id="5eef9f253a5e93ec36275a968646b63d028cda2b" translate="yes" xml:space="preserve">
          <source>When building an eager SparseTensor if &lt;code&gt;dense_shape&lt;/code&gt; is unknown or contains unknown elements (None or -1).</source>
          <target state="translated">When building an eager SparseTensor if &lt;code&gt;dense_shape&lt;/code&gt; is unknown or contains unknown elements (None or -1).</target>
        </trans-unit>
        <trans-unit id="13d3fbe26af35daa9bb0d8a47a6e4b1e39a6da13" translate="yes" xml:space="preserve">
          <source>When building ops to compute gradients, the TensorFlow gradient system will return an error when trying to lookup the gradient of this op, because no gradient must ever be registered for this function. This op exists to prevent subtle bugs from silently returning unimplemented gradients in some corner cases.</source>
          <target state="translated">When building ops to compute gradients, the TensorFlow gradient system will return an error when trying to lookup the gradient of this op, because no gradient must ever be registered for this function. This op exists to prevent subtle bugs from silently returning unimplemented gradients in some corner cases.</target>
        </trans-unit>
        <trans-unit id="11e07c6fbcce2a9ab5d49a2c9fc58743be530d36" translate="yes" xml:space="preserve">
          <source>When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account. Normally, the gradient generator adds ops to a graph to compute the derivatives of a specified 'loss' by recursively finding out inputs that contributed to its computation. If you insert this op in the graph it inputs are masked from the gradient generator. They are not taken into account for computing gradients.</source>
          <target state="translated">그라디언트를 계산하기 위해 op를 작성할 때이 op는 입력의 기여를 고려하지 않습니다. 일반적으로 그라디언트 생성기는 계산에 기여한 입력을 재귀 적으로 찾아서 지정된 '손실'의 미분을 계산하기 위해 그래프에 op를 추가합니다. 그래프에이 op를 삽입하면 입력이 그래디언트 생성기에서 마스킹됩니다. 그라디언트 계산에는 고려되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0d35f4de41040fd9e01e658fd5001879a66a9c2d" translate="yes" xml:space="preserve">
          <source>When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to &lt;code&gt;.cache()&lt;/code&gt; will have no effect until the cache file is removed or the filename is changed.</source>
          <target state="translated">파일을 캐싱 할 때 캐시 된 데이터는 여러 실행에서 유지됩니다. 데이터를 통한 첫 번째 반복조차도 캐시 파일에서 읽습니다. &lt;code&gt;.cache()&lt;/code&gt; 호출하기 전에 입력 파이프 라인을 변경해도 캐시 파일이 제거되거나 파일 이름이 변경 될 때까지 적용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b38df97ed344603f1f1650ff0bfe2600641ba555" translate="yes" xml:space="preserve">
          <source>When calculating the gradient of a weighted loss contributions from both &lt;code&gt;losses&lt;/code&gt; and &lt;code&gt;weights&lt;/code&gt; are considered. If your &lt;code&gt;weights&lt;/code&gt; depend on some model parameters but you do not want this to affect the loss gradient, you need to apply &lt;a href=&quot;../../../stop_gradient&quot;&gt;&lt;code&gt;tf.stop_gradient&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;weights&lt;/code&gt; before passing them to &lt;code&gt;compute_weighted_loss&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;losses&lt;/code&gt; 과 &lt;code&gt;weights&lt;/code&gt; 의 가중치 손실 기여도를 계산할 때 고려됩니다. 귀하의 경우 &lt;code&gt;weights&lt;/code&gt; 일부 모델 매개 변수에 따라 달라집니다하지만이 손실 기울기에 영향을 미칠 싶지 않아, 당신은 적용 할 필요가 &lt;a href=&quot;../../../stop_gradient&quot;&gt; &lt;code&gt;tf.stop_gradient&lt;/code&gt; 를&lt;/a&gt; 에 &lt;code&gt;weights&lt;/code&gt; 그들에게 전달하기 전에 &lt;code&gt;compute_weighted_loss&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="05f81ef4361feddebda74fb8dc805bcdaf94d0cf" translate="yes" xml:space="preserve">
          <source>When called inside a strategy.run call and input is not directly taken from the args of the &lt;code&gt;strategy.run&lt;/code&gt; call. Also if the size of any sequence in &lt;code&gt;features&lt;/code&gt; does not match corresponding sequence in &lt;code&gt;feature_config&lt;/code&gt;. Similarly for &lt;code&gt;weights&lt;/code&gt;, if not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">When called inside a strategy.run call and input is not directly taken from the args of the &lt;code&gt;strategy.run&lt;/code&gt; call. Also if the size of any sequence in &lt;code&gt;features&lt;/code&gt; does not match corresponding sequence in &lt;code&gt;feature_config&lt;/code&gt; . Similarly for &lt;code&gt;weights&lt;/code&gt; , if not &lt;code&gt;None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ebd806d2188a9c9df38773d42df546a4427672df" translate="yes" xml:space="preserve">
          <source>When called inside a strategy.run call and inside XLA control flow.</source>
          <target state="translated">When called inside a strategy.run call and inside XLA control flow.</target>
        </trans-unit>
        <trans-unit id="e31beadd763eb7afc2237f8c90a1ca311bfd077a" translate="yes" xml:space="preserve">
          <source>When called, the default graph is the one that will be launched in the session. The hook can modify the graph by adding new operations to it. After the &lt;code&gt;begin()&lt;/code&gt; call the graph will be finalized and the other callbacks can not modify the graph anymore. Second call of &lt;code&gt;begin()&lt;/code&gt; on the same graph, should not change the graph.</source>
          <target state="translated">호출 될 때 기본 그래프는 세션에서 시작되는 그래프입니다. 후크는 새로운 작업을 추가하여 그래프를 수정할 수 있습니다. (가) 한 후 &lt;code&gt;begin()&lt;/code&gt; 그래프가 완성됩니다 호출하고 다른 콜백은 더 이상 그래프를 수정할 수 없습니다. 동일한 그래프에서 &lt;code&gt;begin()&lt;/code&gt; 의 두 번째 호출은 그래프를 변경하지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="97ac3a446111c899472a5c5300f96b04aabb3a5c" translate="yes" xml:space="preserve">
          <source>When checkpointing your model, you should include your &lt;a href=&quot;tpuembedding&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TPUEmbedding&lt;/code&gt;&lt;/a&gt; object in the checkpoint. It is a trackable object and saving it will save the embedding tables and their optimizer slot variables:</source>
          <target state="translated">When checkpointing your model, you should include your &lt;a href=&quot;tpuembedding&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.TPUEmbedding&lt;/code&gt; &lt;/a&gt; object in the checkpoint. It is a trackable object and saving it will save the embedding tables and their optimizer slot variables:</target>
        </trans-unit>
        <trans-unit id="0f92e2d94d9415d9d31e53c6519e2640ef5146f3" translate="yes" xml:space="preserve">
          <source>When combining specs, &lt;code&gt;dev&lt;/code&gt; will take precedence over the current spec. So for instance:</source>
          <target state="translated">When combining specs, &lt;code&gt;dev&lt;/code&gt; will take precedence over the current spec. So for instance:</target>
        </trans-unit>
        <trans-unit id="7d7e4d9de8d529939e37509a2b98e74b3e5ae079" translate="yes" xml:space="preserve">
          <source>When combining specs, &lt;code&gt;dev&lt;/code&gt; will take precidence over the current spec. So for instance:</source>
          <target state="translated">사양을 결합 할 때 &lt;code&gt;dev&lt;/code&gt; 는 현재 사양보다 우선합니다. 예를 들어 :</target>
        </trans-unit>
        <trans-unit id="02d1051ddba9ce90040906dafc1d04ba61223ccf" translate="yes" xml:space="preserve">
          <source>When constructed with a &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; parameter, a &lt;code&gt;FileWriter&lt;/code&gt; instead forms a compatibility layer over new graph-based summaries (&lt;code&gt;tf.contrib.summary&lt;/code&gt;) to facilitate the use of new summary writing with pre-existing code that expects a &lt;code&gt;FileWriter&lt;/code&gt; instance.</source>
          <target state="translated">&lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt; 매개 변수로 구성되면 &lt;code&gt;FileWriter&lt;/code&gt; 는 대신 새로운 그래프 기반 요약 ( &lt;code&gt;tf.contrib.summary&lt;/code&gt; )에 대해 호환성 계층을 형성 하여 &lt;code&gt;FileWriter&lt;/code&gt; 를 예상하는 기존 코드와 함께 새로운 요약 작성을 쉽게 사용할 수 있습니다. 예.</target>
        </trans-unit>
        <trans-unit id="e77d83f8aae8dd347eba34fe1096849a87e1d28d" translate="yes" xml:space="preserve">
          <source>When constructed with a &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; parameter, a &lt;code&gt;FileWriter&lt;/code&gt; instead forms a compatibility layer over new graph-based summaries to facilitate the use of new summary writing with pre-existing code that expects a &lt;code&gt;FileWriter&lt;/code&gt; instance.</source>
          <target state="translated">When constructed with a &lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt; parameter, a &lt;code&gt;FileWriter&lt;/code&gt; instead forms a compatibility layer over new graph-based summaries to facilitate the use of new summary writing with pre-existing code that expects a &lt;code&gt;FileWriter&lt;/code&gt; instance.</target>
        </trans-unit>
        <trans-unit id="aa47f312128ac2a3285768f283bc01e3ade90f81" translate="yes" xml:space="preserve">
          <source>When consuming SavedModels asynchronously (the producer is a separate process), the SavedModel directory will appear before all files have been written, and &lt;a href=&quot;load&quot;&gt;&lt;code&gt;tf.saved_model.load&lt;/code&gt;&lt;/a&gt; will fail if pointed at an incomplete SavedModel. Rather than checking for the directory, check for &quot;saved_model_dir/saved_model.pb&quot;. This file is written atomically as the last &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt; file operation.</source>
          <target state="translated">SavedModels를 비동기식으로 사용하는 경우 (생산자는 별도의 프로세스 임) 모든 파일이 작성되기 전에 SavedModel 디렉토리가 나타나고 불완전한 SavedModel을 가리키면 &lt;a href=&quot;load&quot;&gt; &lt;code&gt;tf.saved_model.load&lt;/code&gt; &lt;/a&gt; 가 실패합니다. 디렉토리를 확인하는 대신 &quot;saved_model_dir / saved_model.pb&quot;를 확인하십시오. 이 파일은 마지막 &lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt; 파일 작업 으로 원자 적으로 작성 됩니다.</target>
        </trans-unit>
        <trans-unit id="b17afd826ceac8e996a7c85541a3c3196c1b6382" translate="yes" xml:space="preserve">
          <source>When creating a distributed dataset that is to be passed to the enqueue operation a special input option must be specified:</source>
          <target state="translated">When creating a distributed dataset that is to be passed to the enqueue operation a special input option must be specified:</target>
        </trans-unit>
        <trans-unit id="2db902f77ce37ed9d0c7614f639ced1da648ed42" translate="yes" xml:space="preserve">
          <source>When desired_channels is set, if the input contains fewer channels than this then the last channel will be duplicated to give the requested number, else if the input has more channels than requested then the additional channels will be ignored.</source>
          <target state="translated">desired_channels가 설정되면 입력에 이보다 적은 수의 채널이 포함 된 경우 마지막 채널이 복제되어 요청 된 수를 제공합니다. 그렇지 않으면 입력에 요청 된 것보다 많은 채널이 있으면 추가 채널이 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="87468be68aa798b87299469214116a53bdacd212" translate="yes" xml:space="preserve">
          <source>When documenting the shape of a RaggedTensor, ragged dimensions can be indicated by enclosing them in parentheses. For example, the shape of a 3-D &lt;code&gt;RaggedTensor&lt;/code&gt; that stores the fixed-size word embedding for each word in a sentence, for each sentence in a batch, could be written as &lt;code&gt;[num_sentences, (num_words), embedding_size]&lt;/code&gt;. The parentheses around &lt;code&gt;(num_words)&lt;/code&gt; indicate that dimension is ragged, and that the length of each element list in that dimension may vary for each item.</source>
          <target state="translated">RaggedTensor의 모양을 문서화 할 때 울퉁불퉁 한 치수는 괄호로 묶어 표시 할 수 있습니다. 예를 들어, 각 단어에 대한 고정 크기 단어 임베드를 문장에 저장 하는 3 차원 &lt;code&gt;RaggedTensor&lt;/code&gt; 의 모양은 각 문장에 대해 배치로 &lt;code&gt;[num_sentences, (num_words), embedding_size]&lt;/code&gt; 로 쓸 수 있습니다 . 괄호 &lt;code&gt;(num_words)&lt;/code&gt; 는 차원이 고르지 않았으며 해당 차원의 각 요소 목록 길이가 항목마다 다를 수 있음을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="933ca78b0f0aab236f7c30d0b7d8200f4284ead1" translate="yes" xml:space="preserve">
          <source>When doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.</source>
          <target state="translated">When doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.</target>
        </trans-unit>
        <trans-unit id="bac64cb9d33552a3e872e7c5fb90f9f381853ebe" translate="yes" xml:space="preserve">
          <source>When doing log-odds NCE, the result of this op should be passed through a SparseToDense op, then added to the logits of the sampled candidates. This has the effect of 'removing' the sampled labels that match the true labels by making the classifier sure that they are sampled labels.</source>
          <target state="translated">When doing log-odds NCE, the result of this op should be passed through a SparseToDense op, then added to the logits of the sampled candidates. This has the effect of 'removing' the sampled labels that match the true labels by making the classifier sure that they are sampled labels.</target>
        </trans-unit>
        <trans-unit id="bd3848450984baae9593cd8c988bd98ec36231eb" translate="yes" xml:space="preserve">
          <source>When each worker has more than one GPU, operations will be replicated on all GPUs. Even though operations may be replicated, variables are not and each worker shares a common view for which parameter server a variable is assigned to.</source>
          <target state="translated">각 작업자가 둘 이상의 GPU를 가지고 있으면 모든 GPU에서 작업이 복제됩니다. 작업이 복제 될 수는 있지만 변수는 그렇지 않으며 각 작업자는 변수가 할당 된 매개 변수 서버에 대한 공통보기를 공유합니다.</target>
        </trans-unit>
        <trans-unit id="1dfe4b0bd2fe2cdea509ee0800c577901d8b6927" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;gate_gradients&lt;/code&gt;, &lt;code&gt;aggregation_method&lt;/code&gt;, and &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; are ignored.</source>
          <target state="translated">&lt;code&gt;gate_gradients&lt;/code&gt; 실행이 활성화되면 gate_gradients , &lt;code&gt;aggregation_method&lt;/code&gt; 및 &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; 는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="148b464485213e809e25fd2427e6df10ddb7913a" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;momentum&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;learning_rate&lt;/code&gt; 및 &lt;code&gt;momentum&lt;/code&gt; 은 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능할 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6f483d1759bf06f09369c7c2c6504ed890b4f074" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt; can be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;learning_rate&lt;/code&gt; 는 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능일 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6eb0adbeafdc09876e4693f8ca8e165f9c0c0985" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;beta_1&lt;/code&gt;, &lt;code&gt;beta_2&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;learning_rate&lt;/code&gt; , &lt;code&gt;beta_1&lt;/code&gt; , &lt;code&gt;beta_2&lt;/code&gt; 및 &lt;code&gt;epsilon&lt;/code&gt; 은 각각 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능할 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="88cf25c3162efb5267e479cbb3ca75d9e84020c2" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;decay&lt;/code&gt;, &lt;code&gt;momentum&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;learning_rate&lt;/code&gt; , &lt;code&gt;decay&lt;/code&gt; , &lt;code&gt;momentum&lt;/code&gt; 및 &lt;code&gt;epsilon&lt;/code&gt; 은 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능할 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a51c69c938604c7d227cd40cf800aac6b43df507" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;rho&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열성적인 실행이 활성화되면 &lt;code&gt;learning_rate&lt;/code&gt; , &lt;code&gt;rho&lt;/code&gt; 및 &lt;code&gt;epsilon&lt;/code&gt; 은 각각 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능할 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="78865de3a670b7422c397c1e12a0d6935fb3e356" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;loss&lt;/code&gt; should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of &lt;code&gt;var_list&lt;/code&gt; if not None, else with respect to any trainable variables created during the execution of the &lt;code&gt;loss&lt;/code&gt; function. &lt;code&gt;gate_gradients&lt;/code&gt;, &lt;code&gt;aggregation_method&lt;/code&gt;, &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; and &lt;code&gt;grad_loss&lt;/code&gt; are ignored when eager execution is enabled.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;loss&lt;/code&gt; 은 인수를 사용하지 않고 최소화 할 값을 계산하는 Python 함수 여야합니다. &lt;code&gt;var_list&lt;/code&gt; 의 요소와 관련하여 최소화 (및 기울기 계산)가 수행됩니다 . 그렇지 않으면 &lt;code&gt;loss&lt;/code&gt; 함수 실행 중에 생성 된 학습 가능한 변수와 관련이 있습니다. &lt;code&gt;gate_gradients&lt;/code&gt; 실행이 활성화되면 gate_gradients , &lt;code&gt;aggregation_method&lt;/code&gt; , &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; 및 &lt;code&gt;grad_loss&lt;/code&gt; 가 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="a739b33250bbc25214beaa9599eadabdb7bb4665" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;var_list&lt;/code&gt; must specify a &lt;code&gt;list&lt;/code&gt; or &lt;code&gt;dict&lt;/code&gt; of variables to save. Otherwise, a &lt;code&gt;RuntimeError&lt;/code&gt; will be raised.</source>
          <target state="translated">열망 실행이 활성화되면, &lt;code&gt;var_list&lt;/code&gt; 는 지정해야합니다 &lt;code&gt;list&lt;/code&gt; 또는 &lt;code&gt;dict&lt;/code&gt; 저장하는 변수를. 그렇지 않으면 &lt;code&gt;RuntimeError&lt;/code&gt; 가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="58de3a1f5573eceefbc002055806cdf8a10b2832" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, any callable object in the &lt;code&gt;control_inputs&lt;/code&gt; list will be called.</source>
          <target state="translated">열망하는 실행이 활성화되면 &lt;code&gt;control_inputs&lt;/code&gt; 목록의 호출 가능한 객체 가 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="91a58274042c41959ab2122ef94206a6a2a7792d" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, code inside an init_scope block runs with eager execution enabled even when tracing a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. For example:</source>
          <target state="translated">열망 실행이 활성화 된 때, init_scope 블록 내부 코드는 트레이스시 열망 실행도 가능으로 실행 &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 를&lt;/a&gt; . 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="7644973d82350f411122d3333c3da0b3f33b32bb" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, executes ops specified by &lt;code&gt;fn&lt;/code&gt; on each replica. Otherwise, builds a graph to execute the ops on each replica.</source>
          <target state="translated">열망하는 실행이 활성화되면 각 복제본에서 &lt;code&gt;fn&lt;/code&gt; 으로 지정된 op를 실행합니다 . 그렇지 않으면 각 복제본에서 op를 실행하는 그래프를 작성합니다.</target>
        </trans-unit>
        <trans-unit id="f2c02abca5b8ce6f6c1965307850901a9a73db40" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, learning_rate can be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면 learning_rate는 인수를 사용하지 않고 사용할 실제 값을 반환하는 호출 가능일 수 있습니다. 이것은 옵티 마이저 함수의 다른 호출에서 이러한 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cfdc83fece145a8a3ec7bfd53296a084c4dce787" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, returns &lt;code&gt;True&lt;/code&gt; in most cases. However, this API might return &lt;code&gt;False&lt;/code&gt; in the following use cases.</source>
          <target state="translated">열망하는 실행이 활성화되면 대부분의 경우 &lt;code&gt;True&lt;/code&gt; 를 반환합니다 . 그러나이 API 는 다음 사용 사례에서 &lt;code&gt;False&lt;/code&gt; 를 반환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="fca6e6a28c0b1d4d30760e6ed2b604579bf812fc" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, as outside &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, there is no graph.</source>
          <target state="translated">열망 실행을 사용하면, 혼합 정밀도 그래프 재기록 만 내 활성화 &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; 외부 같이 &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; 어떠한 그래프 없다.</target>
        </trans-unit>
        <trans-unit id="cd5cd16ed9249edf9d475e4c4f731cb6a6ee674b" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, as outside &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, there is no graph.</source>
          <target state="translated">When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;s, as outside &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;s, there is no graph.</target>
        </trans-unit>
        <trans-unit id="b6892adfb044f1fdf15d846e00b609e83b202685" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, as outside &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, there is no graph.</source>
          <target state="translated">열망 실행을 사용하면, 혼합 정밀도 그래프 재기록 만 내 활성화 &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; 외부 같이 &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; 어떠한 그래프 없다.</target>
        </trans-unit>
        <trans-unit id="530f59ba986dbe23a70377c034d053b8d4dc39ba" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, as outside &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, there is no graph.</source>
          <target state="translated">When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;s, as outside &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;s, there is no graph.</target>
        </trans-unit>
        <trans-unit id="c4ced4e377065fa4ed9d5e9a06ab2254081c3e07" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor. This can be useful for changing the learning rate value across different invocations of optimizer functions.</source>
          <target state="translated">열망하는 실행이 활성화되면이 함수는 쇠퇴 된 학습 속도 텐서를 반환하는 함수를 반환합니다. 이것은 옵티 마이저 함수의 다른 호출에서 학습 속도 값을 변경하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d2e71dc36f6cf8dc4084a1f274669076eb23a06d" translate="yes" xml:space="preserve">
          <source>When enabled, TensorFlow runtime will collection information that can later be exported and consumed by TensorBoard. The trace is activated across the entire TensorFlow runtime and affects all threads of execution.</source>
          <target state="translated">활성화되면 TensorFlow 런타임은 나중에 TensorBoard에서 내보내고 사용할 수있는 정보를 수집합니다. 추적은 전체 TensorFlow 런타임에서 활성화되며 모든 실행 스레드에 영향을줍니다.</target>
        </trans-unit>
        <trans-unit id="4de00d923ffd1999737d2b1cd351b2302e234977" translate="yes" xml:space="preserve">
          <source>When enabled, the dtype of Keras layers defaults to floatx (which is typically float32) instead of None. In addition, layers will automatically cast floating-point inputs to the layer's dtype.</source>
          <target state="translated">When enabled, the dtype of Keras layers defaults to floatx (which is typically float32) instead of None. In addition, layers will automatically cast floating-point inputs to the layer's dtype.</target>
        </trans-unit>
        <trans-unit id="c296a7d5f98201bab11940527df0b91bbde60a8b" translate="yes" xml:space="preserve">
          <source>When enum_class is empty.</source>
          <target state="translated">When enum_class is empty.</target>
        </trans-unit>
        <trans-unit id="261d0f228aa5675f9d37b49b2a148dd964803430" translate="yes" xml:space="preserve">
          <source>When enum_class is not a subclass of Enum.</source>
          <target state="translated">When enum_class is not a subclass of Enum.</target>
        </trans-unit>
        <trans-unit id="2564a8f9143928f594ea2074e98db1c0c92dbe63" translate="yes" xml:space="preserve">
          <source>When enum_values is empty.</source>
          <target state="translated">When enum_values is empty.</target>
        </trans-unit>
        <trans-unit id="4382685a76810fa4eefafb6b927a319a12869925" translate="yes" xml:space="preserve">
          <source>When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None, or the shapes are not all broadcastable.</source>
          <target state="translated">When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None, or the shapes are not all broadcastable.</target>
        </trans-unit>
        <trans-unit id="947feea8d5b04906f82595b03627acd7039c1568" translate="yes" xml:space="preserve">
          <source>When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None.</source>
          <target state="translated">When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None.</target>
        </trans-unit>
        <trans-unit id="f4e19c79dfb2ba307013a448936b4d81569267d0" translate="yes" xml:space="preserve">
          <source>When executed in a graph, this op outputs its input tensor as-is.</source>
          <target state="translated">그래프에서 실행될 때이 op는 입력 텐서를 그대로 출력합니다.</target>
        </trans-unit>
        <trans-unit id="4cecfcec22d59fa24fae039affcbd309fee3e65f" translate="yes" xml:space="preserve">
          <source>When executed, the Tensor &lt;code&gt;a&lt;/code&gt; will have the name &lt;code&gt;MyOp/a&lt;/code&gt;.</source>
          <target state="translated">실행되면 Tensor &lt;code&gt;a&lt;/code&gt; 의 이름은 &lt;code&gt;MyOp/a&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="ae9202cb150768cbd21e4a56c23a5fe6d721d720" translate="yes" xml:space="preserve">
          <source>When executed, the Tensors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, will have names &lt;code&gt;MyOp/a&lt;/code&gt;, &lt;code&gt;MyOp/b&lt;/code&gt;, and &lt;code&gt;MyOp/c&lt;/code&gt;.</source>
          <target state="translated">실행되면 텐서 &lt;code&gt;a&lt;/code&gt; , &lt;code&gt;b&lt;/code&gt; , &lt;code&gt;c&lt;/code&gt; 의 이름은 &lt;code&gt;MyOp/a&lt;/code&gt; , &lt;code&gt;MyOp/b&lt;/code&gt; 및 &lt;code&gt;MyOp/c&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="fe804bdbbf36ce79d174e2e04fa5a4b653cbe404" translate="yes" xml:space="preserve">
          <source>When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; decorator:</source>
          <target state="translated">When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; decorator:</target>
        </trans-unit>
        <trans-unit id="b4b0c36f73c370281ff533b8718b05a1bf7fc23a" translate="yes" xml:space="preserve">
          <source>When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; decorator:</source>
          <target state="translated">When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; decorator:</target>
        </trans-unit>
        <trans-unit id="50bb271b7e16cf32dee4edabc5aa3fa56b53275c" translate="yes" xml:space="preserve">
          <source>When executing eagerly, either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added).</source>
          <target state="translated">열심히 실행할 때 복원 할 변수가 이미 작성된 경우 즉시 값을 지정하거나 변수가 작성 될 때까지 복원을 연기합니다. 이 호출 이후에 추가 된 종속성은 검사 점에 해당 개체가있는 경우 일치합니다 (복원 요청은 예상되는 종속성이 추가되기를 기다리는 추적 가능한 개체에서 대기합니다).</target>
        </trans-unit>
        <trans-unit id="9f2dc68be8f34aeff876ac6aadeee84459d20f11" translate="yes" xml:space="preserve">
          <source>When executing eagerly, map_fn does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;code&gt;tf.contrib.eager.defun&lt;/code&gt; decorator,</source>
          <target state="translated">열심히 실행할 때, map_fn이 경우에도 병렬로 실행되지 않습니다 &lt;code&gt;parallel_iterations&lt;/code&gt; 가 여전히 사용하여 병렬로 함수를 실행의 성능 이점을 얻을 수 있습니다 1.&amp;gt; 값으로 설정되어 &lt;code&gt;tf.contrib.eager.defun&lt;/code&gt; 의 , 장식을</target>
        </trans-unit>
        <trans-unit id="243113db76fb85d3b379322eb6df8fc099c87214" translate="yes" xml:space="preserve">
          <source>When executing in a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; or building a model using &lt;a href=&quot;keras/input&quot;&gt;&lt;code&gt;tf.keras.Input&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor#shape&quot;&gt;&lt;code&gt;Tensor.shape&lt;/code&gt;&lt;/a&gt; may return a partial shape (including &lt;code&gt;None&lt;/code&gt; for unknown dimensions). See &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">When executing in a &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt; or building a model using &lt;a href=&quot;keras/input&quot;&gt; &lt;code&gt;tf.keras.Input&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;tensor#shape&quot;&gt; &lt;code&gt;Tensor.shape&lt;/code&gt; &lt;/a&gt; may return a partial shape (including &lt;code&gt;None&lt;/code&gt; for unknown dimensions). See &lt;a href=&quot;tensorshape&quot;&gt; &lt;code&gt;tf.TensorShape&lt;/code&gt; &lt;/a&gt; for more details.</target>
        </trans-unit>
        <trans-unit id="0ce10899f8aee013da9a14ad538ad1fde8148656" translate="yes" xml:space="preserve">
          <source>When executing in a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, or building a model using &lt;a href=&quot;keras/input&quot;&gt;&lt;code&gt;tf.keras.Input&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor#set_shape&quot;&gt;&lt;code&gt;Tensor.set_shape&lt;/code&gt;&lt;/a&gt; will &lt;em&gt;merge&lt;/em&gt; the given &lt;code&gt;shape&lt;/code&gt; with the current shape of this tensor, and set the tensor's shape to the merged value (see &lt;a href=&quot;tensorshape#merge_with&quot;&gt;&lt;code&gt;tf.TensorShape.merge_with&lt;/code&gt;&lt;/a&gt; for details):</source>
          <target state="translated">When executing in a &lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;, or building a model using &lt;a href=&quot;keras/input&quot;&gt; &lt;code&gt;tf.keras.Input&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;tensor#set_shape&quot;&gt; &lt;code&gt;Tensor.set_shape&lt;/code&gt; &lt;/a&gt; will &lt;em&gt;merge&lt;/em&gt; the given &lt;code&gt;shape&lt;/code&gt; with the current shape of this tensor, and set the tensor's shape to the merged value (see &lt;a href=&quot;tensorshape#merge_with&quot;&gt; &lt;code&gt;tf.TensorShape.merge_with&lt;/code&gt; &lt;/a&gt; for details):</target>
        </trans-unit>
        <trans-unit id="961e80919c470474bd25500c2cb03aaf480dc1f1" translate="yes" xml:space="preserve">
          <source>When feeding features into &lt;code&gt;embedding.enqueue&lt;/code&gt; they can be &lt;a href=&quot;../../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s, &lt;a href=&quot;../../../sparse/sparsetensor&quot;&gt;&lt;code&gt;tf.SparseTensor&lt;/code&gt;&lt;/a&gt;s or &lt;a href=&quot;../../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;s. When the argument &lt;code&gt;max_sequence_length&lt;/code&gt; is 0, the default, you should expect a output of &lt;code&gt;embedding.dequeue&lt;/code&gt; for this feature of shape &lt;code&gt;(batch_size, dim)&lt;/code&gt;. If &lt;code&gt;max_sequence_length&lt;/code&gt; is greater than 0, the feature is embedded as a sequence and padded up to the given length. The shape of the output for this feature will be &lt;code&gt;(batch_size, max_sequence_length, dim)&lt;/code&gt;.</source>
          <target state="translated">When feeding features into &lt;code&gt;embedding.enqueue&lt;/code&gt; they can be &lt;a href=&quot;../../../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;s, &lt;a href=&quot;../../../sparse/sparsetensor&quot;&gt; &lt;code&gt;tf.SparseTensor&lt;/code&gt; &lt;/a&gt;s or &lt;a href=&quot;../../../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;s. When the argument &lt;code&gt;max_sequence_length&lt;/code&gt; is 0, the default, you should expect a output of &lt;code&gt;embedding.dequeue&lt;/code&gt; for this feature of shape &lt;code&gt;(batch_size, dim)&lt;/code&gt; . If &lt;code&gt;max_sequence_length&lt;/code&gt; is greater than 0, the feature is embedded as a sequence and padded up to the given length. The shape of the output for this feature will be &lt;code&gt;(batch_size, max_sequence_length, dim)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d5aadb062c18b16c150d95636430095568153cdb" translate="yes" xml:space="preserve">
          <source>When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as &lt;code&gt;ON_READ&lt;/code&gt;.</source>
          <target state="translated">When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as &lt;code&gt;ON_READ&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2f8e307109474cb2469b10f7555dc21ae2944eac" translate="yes" xml:space="preserve">
          <source>When graph building, &lt;code&gt;assert_consumed()&lt;/code&gt; indicates that all of the restore ops that will be created for this checkpoint have been created. They can be run via the &lt;code&gt;run_restore_ops()&lt;/code&gt; method of the status object:</source>
          <target state="translated">그래프 작성시 &lt;code&gt;assert_consumed()&lt;/code&gt; 는이 검사 점에 대해 생성 될 모든 복원 작업이 생성되었음을 나타냅니다. 상태 객체 의 &lt;code&gt;run_restore_ops()&lt;/code&gt; 메소드를 통해 실행할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="79f071b74a52cc1dc25696f12242191a62439461" translate="yes" xml:space="preserve">
          <source>When graph building, restoration ops are added to the graph but not run immediately.</source>
          <target state="translated">그래프 작성시, 복원 작업이 그래프에 추가되지만 즉시 실행되지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="50bb22b935fa898c607810aea861b6f9cab61dc5" translate="yes" xml:space="preserve">
          <source>When in TF V1 mode (that is, outside &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;) Assert needs a control dependency on the output to ensure the assertion executes:</source>
          <target state="translated">When in TF V1 mode (that is, outside &lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;) Assert needs a control dependency on the output to ensure the assertion executes:</target>
        </trans-unit>
        <trans-unit id="1c8e82ac3193c4ec5ccc1a2ed347b087c3eee96b" translate="yes" xml:space="preserve">
          <source>When indexing keyword argument is not one of &lt;code&gt;xy&lt;/code&gt; or &lt;code&gt;ij&lt;/code&gt;.</source>
          <target state="translated">When indexing keyword argument is not one of &lt;code&gt;xy&lt;/code&gt; or &lt;code&gt;ij&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="45d6ec71b2e803e49adc786399511f890dec3b98" translate="yes" xml:space="preserve">
          <source>When indices are not consistent.</source>
          <target state="translated">When indices are not consistent.</target>
        </trans-unit>
        <trans-unit id="3400d0e372dbb4d788999038a1fa59202da9146e" translate="yes" xml:space="preserve">
          <source>When indices is a 1D tensor, this operation is equivalent to &lt;a href=&quot;scatter_update&quot;&gt;&lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">인덱스가 1D 텐서 인 경우이 작업은 &lt;a href=&quot;scatter_update&quot;&gt; &lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt; &lt;/a&gt; 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="d2e52528636ed4172136496c48a5c9c765f21cc1" translate="yes" xml:space="preserve">
          <source>When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. If the input is &lt;code&gt;x&lt;/code&gt; and the operation &lt;code&gt;x * W&lt;/code&gt;, and we want to initialize &lt;code&gt;W&lt;/code&gt; uniformly at random, we need to pick &lt;code&gt;W&lt;/code&gt; from</source>
          <target state="translated">딥 네트워크를 초기화 할 때는 원칙적으로 입력 분산의 스케일을 일정하게 유지하는 것이 유리하므로 최종 레이어에 도달하여 폭발하거나 감소하지 않습니다. 입력이 경우 &lt;code&gt;x&lt;/code&gt; 및 운영 &lt;code&gt;x * W&lt;/code&gt; , 우리는 초기화 할 &lt;code&gt;W&lt;/code&gt; 균일하게 무작위로, 우리는 선택해야합니다 &lt;code&gt;W&lt;/code&gt; 에서</target>
        </trans-unit>
        <trans-unit id="12f77eb406d2a652e408c888d240e8eb2d9cb755" translate="yes" xml:space="preserve">
          <source>When invoking a signature in an exported SavedModel, &lt;code&gt;Tensor&lt;/code&gt; arguments are identified by name. These names will come from the Python function's argument names by default. They may be overridden by specifying a &lt;code&gt;name=...&lt;/code&gt; argument in the corresponding &lt;a href=&quot;../tensorspec&quot;&gt;&lt;code&gt;tf.TensorSpec&lt;/code&gt;&lt;/a&gt; object. Explicit naming is required if multiple &lt;code&gt;Tensor&lt;/code&gt;s are passed through a single argument to the Python function.</source>
          <target state="translated">내 보낸 저장된 모델에서 서명을 호출 할 때 이름으로 &lt;code&gt;Tensor&lt;/code&gt; 인수가 식별됩니다. 이 이름은 기본적으로 Python 함수의 인수 이름에서 유래합니다. 해당 &lt;a href=&quot;../tensorspec&quot;&gt; &lt;code&gt;tf.TensorSpec&lt;/code&gt; &lt;/a&gt; 오브젝트 에 &lt;code&gt;name=...&lt;/code&gt; 인수를 지정하여 대체 할 수 있습니다 . 여러 개의 &lt;code&gt;Tensor&lt;/code&gt; 가 단일 인수를 통해 Python 함수에 전달 되는 경우 명시 적 명명이 필요 합니다.</target>
        </trans-unit>
        <trans-unit id="aaecdf3e3e9d1409f6d5971a54eef9da389f9db7" translate="yes" xml:space="preserve">
          <source>When loading a weight file in TensorFlow format, returns the same status object as &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from &lt;code&gt;Model&lt;/code&gt;, immediately if it is already built).</source>
          <target state="translated">TensorFlow 형식으로 가중치 파일을로드 할 때 &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt; 와 동일한 상태 오브젝트를 리턴 합니다. 그래프 작성시, 네트워크가 빌드 되 자마자 복원 ops가 자동으로 실행됩니다 (처음 에 이미 빌드 된 경우 &lt;code&gt;Model&lt;/code&gt; 에서 상속되는 사용자 정의 클래스를 호출 할 때).</target>
        </trans-unit>
        <trans-unit id="ad0500aa71cc2e13561fbb9f3ccb38e89738eb04" translate="yes" xml:space="preserve">
          <source>When loading a weight file in TensorFlow format, returns the same status object as &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from &lt;code&gt;Model&lt;/code&gt;, immediately if it is already built).</source>
          <target state="translated">TensorFlow 형식으로 가중치 파일을로드 할 때 &lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt; 와 동일한 상태 오브젝트를 리턴 합니다. 그래프 작성시, 네트워크가 빌드 되 자마자 복원 ops가 자동으로 실행됩니다 (처음 에 이미 빌드 된 경우 &lt;code&gt;Model&lt;/code&gt; 에서 상속되는 사용자 정의 클래스를 호출 할 때).</target>
        </trans-unit>
        <trans-unit id="a3a6d69d15b5f2b1e2641796ab481024196abbd1" translate="yes" xml:space="preserve">
          <source>When loading weights in HDF5 format, returns &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">HDF5 형식으로 가중치를로드 할 때 &lt;code&gt;None&lt;/code&gt; 을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="279df71b03d4f1d2b500da71eaeb34af2e345a5e" translate="yes" xml:space="preserve">
          <source>When many instances of this Op are being run concurrently with the same container/shared_name in the same device, some will output zero-shaped Tensors and others will output Tensors of size up to max_batch_size.</source>
          <target state="translated">When many instances of this Op are being run concurrently with the same container/shared_name in the same device, some will output zero-shaped Tensors and others will output Tensors of size up to max_batch_size.</target>
        </trans-unit>
        <trans-unit id="caebbca103b05daa9db25a1f91ee52187b92666d" translate="yes" xml:space="preserve">
          <source>When mixed precision training is used, most layers will instead have a float16 or bfloat16 compute dtype and a float32 variable dtype, and so the layer does not have a single dtype. See &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html&quot;&gt;this link&lt;/a&gt; for more information on mixed precision training. When the variable dtype does not match the compute dtype, variables will be automatically casted to the compute dtype to avoid type errors. In this case, &lt;a href=&quot;../../layers/layer#dtype&quot;&gt;&lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt;&lt;/a&gt; refers to the variable dtype, not the compute dtype.</source>
          <target state="translated">혼합 정밀 트레이닝을 사용하는 경우 대부분의 레이어에는 float16 또는 bfloat16 계산 dtype 및 float32 변수 dtype이 있으므로 레이어에 단일 dtype이 없습니다. 혼합 정밀 교육에 대한 자세한 내용 은 &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html&quot;&gt;이 링크&lt;/a&gt; 를 참조하십시오 . 변수 dtype이 계산 dtype과 일치하지 않으면 유형 오류를 피하기 위해 변수가 계산 dtype에 자동으로 캐스팅됩니다. 이 경우 &lt;a href=&quot;../../layers/layer#dtype&quot;&gt; &lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt; &lt;/a&gt; 은 계산 dtype이 아니라 변수 dtype을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="31e5d2d74d34d722ae24b0d3557c28c55ecaa6d6" translate="yes" xml:space="preserve">
          <source>When mixed precision training is used, most layers will instead have a float16 or bfloat16 compute dtype and a float32 variable dtype, and so the layer does not have a single dtype. When the variable dtype does not match the compute dtype, variables will be automatically casted to the compute dtype to avoid type errors. In this case, &lt;a href=&quot;../../layers/layer#dtype&quot;&gt;&lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt;&lt;/a&gt; refers to the variable dtype, not the compute dtype. See &lt;a href=&quot;https://www.tensorflow.org/guide/keras/mixed_precision&quot;&gt;the mixed precision guide&lt;/a&gt; for more information on how to use mixed precision.</source>
          <target state="translated">When mixed precision training is used, most layers will instead have a float16 or bfloat16 compute dtype and a float32 variable dtype, and so the layer does not have a single dtype. When the variable dtype does not match the compute dtype, variables will be automatically casted to the compute dtype to avoid type errors. In this case, &lt;a href=&quot;../../layers/layer#dtype&quot;&gt; &lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt; &lt;/a&gt; refers to the variable dtype, not the compute dtype. See &lt;a href=&quot;https://www.tensorflow.org/guide/keras/mixed_precision&quot;&gt;the mixed precision guide&lt;/a&gt; for more information on how to use mixed precision.</target>
        </trans-unit>
        <trans-unit id="12dfe9c646058fc3b646723517e40b30a435cf72" translate="yes" xml:space="preserve">
          <source>When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</source>
          <target state="translated">When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</target>
        </trans-unit>
        <trans-unit id="e594a784be178d46ade99237614f3d81b376cb0c" translate="yes" xml:space="preserve">
          <source>When multiple identical random ops are wrapped in a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, their behaviors change because the ops no long share the same counter. For example:</source>
          <target state="translated">여러 개의 동일한 임의 op가 &lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 에&lt;/a&gt; 래핑 되면 ops가 더 이상 동일한 카운터를 공유하지 않기 때문에 동작이 변경됩니다. 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="2fdc190461b8260acb4e4a0e40690fe99685019d" translate="yes" xml:space="preserve">
          <source>When multiple steps of profiles are available, select which step's profile to use. If -1, use average of all available steps.</source>
          <target state="translated">When multiple steps of profiles are available, select which step's profile to use. If -1, use average of all available steps.</target>
        </trans-unit>
        <trans-unit id="fbe07c906de9609ba47c0d5dc628f10d68cc6dd7" translate="yes" xml:space="preserve">
          <source>When no keyword arguments (kwargs) are passed.</source>
          <target state="translated">When no keyword arguments (kwargs) are passed.</target>
        </trans-unit>
        <trans-unit id="5b5b5298719274d2e99620e5ca7a91daabe32b28" translate="yes" xml:space="preserve">
          <source>When not &lt;code&gt;None&lt;/code&gt;, the probability we will drop out a given coordinate.</source>
          <target state="translated">When not &lt;code&gt;None&lt;/code&gt; , the probability we will drop out a given coordinate.</target>
        </trans-unit>
        <trans-unit id="9f319b17be698e68a66d41691883dcc29fc7f9c6" translate="yes" xml:space="preserve">
          <source>When not None, the probability we will drop out a given coordinate.</source>
          <target state="translated">When not None, the probability we will drop out a given coordinate.</target>
        </trans-unit>
        <trans-unit id="f589095e3d5fab094abecc7eb9696d68f8a9c36d" translate="yes" xml:space="preserve">
          <source>When operating in a v1-style graph context, ops are not executed in the same order as specified in the code; TensorFlow will attempt to execute ops in parallel or in an order convienient to the result it is computing. &lt;a href=&quot;group&quot;&gt;&lt;code&gt;tf.group&lt;/code&gt;&lt;/a&gt; allows you to request that one or more results finish before execution continues.</source>
          <target state="translated">When operating in a v1-style graph context, ops are not executed in the same order as specified in the code; TensorFlow will attempt to execute ops in parallel or in an order convienient to the result it is computing. &lt;a href=&quot;group&quot;&gt; &lt;code&gt;tf.group&lt;/code&gt; &lt;/a&gt; allows you to request that one or more results finish before execution continues.</target>
        </trans-unit>
        <trans-unit id="c3fc5b0a3b9961cb77e88c8942ce57281e9ad832" translate="yes" xml:space="preserve">
          <source>When passed &lt;code&gt;trainable=True&lt;/code&gt;, the &lt;code&gt;Variable()&lt;/code&gt; constructor automatically adds new variables to the graph collection &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt;. This convenience function returns the contents of that collection.</source>
          <target state="translated">&lt;code&gt;trainable=True&lt;/code&gt; 를 전달 하면 &lt;code&gt;Variable()&lt;/code&gt; 생성자가 그래프 컬렉션 &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; 에 새 변수를 자동으로 추가합니다 . 이 편의 함수는 해당 컬렉션의 내용을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5b6a28f5fad1b02dbaf2abf3b2bcae0d38a29c7b" translate="yes" xml:space="preserve">
          <source>When reading a single input file, you can shard elements as follows:</source>
          <target state="translated">단일 입력 파일을 읽을 때 다음과 같이 요소를 분할 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5d6c412525d35ed49e0a016246bbab95a1ad4343" translate="yes" xml:space="preserve">
          <source>When run, it returns a 1-D tensor containing the names of uninitialized variables if there are any, or an empty array if there are none.</source>
          <target state="translated">실행될 때 초기화되지 않은 변수의 이름이 있으면 1 차원 텐서가 있거나 비어있는 배열이 없으면 1 차원 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4cd96e8d064773862a00f08fa46097675a613f49" translate="yes" xml:space="preserve">
          <source>When run, reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if &lt;code&gt;tensor&lt;/code&gt; has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes &lt;code&gt;tensor&lt;/code&gt; as-is.</source>
          <target state="translated">실행시 &lt;code&gt;tensor&lt;/code&gt; 에 숫자 (NaN) 또는 무한대 (Inf)가 아닌 값이 있으면 &lt;code&gt;InvalidArgument&lt;/code&gt; 오류 가보고됩니다. 그렇지 않으면 &lt;code&gt;tensor&lt;/code&gt; 그대로 전달 합니다.</target>
        </trans-unit>
        <trans-unit id="9a7fb4ad12e1e9b24a4a905d1465aa8fe6223f81" translate="yes" xml:space="preserve">
          <source>When run, reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if &lt;code&gt;tensor&lt;/code&gt; has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes &lt;code&gt;tensor&lt;/code&gt; as-is. Unlike CheckNumerics (V1), CheckNumericsV2 distinguishes -Inf and +Inf in the errors it throws.</source>
          <target state="translated">When run, reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if &lt;code&gt;tensor&lt;/code&gt; has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes &lt;code&gt;tensor&lt;/code&gt; as-is. Unlike CheckNumerics (V1), CheckNumericsV2 distinguishes -Inf and +Inf in the errors it throws.</target>
        </trans-unit>
        <trans-unit id="a08b94134a846722329d59d0e06daaabf9e81c0e" translate="yes" xml:space="preserve">
          <source>When run, the returned Op will raise the exception &lt;code&gt;FailedPreconditionError&lt;/code&gt; if any of the variables has not yet been initialized.</source>
          <target state="translated">변수가 아직 초기화되지 않은 경우 반환 된 Op는 예외 &lt;code&gt;FailedPreconditionError&lt;/code&gt; 를 발생 시킵니다 .</target>
        </trans-unit>
        <trans-unit id="612235a5e3b2319598b46d42d78d5e3939c685ba" translate="yes" xml:space="preserve">
          <source>When running in graph mode, you must evaluate the tensor returned by &lt;code&gt;tf.tables_initializer()&lt;/code&gt; before evaluating the tensor returned by this class's &lt;code&gt;lookup()&lt;/code&gt; method. Example usage in graph mode:</source>
          <target state="translated">그래프 모드에서 실행 하는 경우이 클래스의 &lt;code&gt;lookup()&lt;/code&gt; 메소드가 리턴 한 텐서를 평가하기 전에 &lt;code&gt;tf.tables_initializer()&lt;/code&gt; 리턴 한 텐서를 평가해야합니다 . 그래프 모드에서의 사용 예 :</target>
        </trans-unit>
        <trans-unit id="7eea4af19c3fd0977360ee69dfc63bed5794fb89" translate="yes" xml:space="preserve">
          <source>When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:</source>
          <target state="translated">그래프 모드에서 실행할 때이 작업이 실행되도록하려면이 작업에 대한 종속성을 추가해야합니다. 오퍼레이션에 종속성을 추가하는 예 :</target>
        </trans-unit>
        <trans-unit id="83f395cc48e2b9d73bdec46f8abf442be27de226" translate="yes" xml:space="preserve">
          <source>When saving in HDF5 format, the weight file has:</source>
          <target state="translated">When saving in HDF5 format, the weight file has:</target>
        </trans-unit>
        <trans-unit id="23b4a5289feb34fa15683e70f83d2559b44972e8" translate="yes" xml:space="preserve">
          <source>When saving in HDF5 format, the weight file has: - &lt;code&gt;layer_names&lt;/code&gt; (attribute), a list of strings (ordered names of model layers). - For every layer, a &lt;code&gt;group&lt;/code&gt; named &lt;code&gt;layer.name&lt;/code&gt; - For every such layer group, a group attribute &lt;code&gt;weight_names&lt;/code&gt;, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor.</source>
          <target state="translated">HDF5 형식으로 저장할 때 가중치 파일에는 다음이 있습니다.- &lt;code&gt;layer_names&lt;/code&gt; (속성), 문자열 목록 (모델 레이어의 순서화 된 이름). -모든 레이어에서 이름이 &lt;code&gt;layer.name&lt;/code&gt; 인 &lt;code&gt;group&lt;/code&gt; -모든 해당 레이어 그룹에 대해 그룹 속성 &lt;code&gt;weight_names&lt;/code&gt; , 문자열 목록 (레이어의 가중치 텐서 순서). -레이어의 모든 가중치에 대해 가중치 텐서의 이름을 딴 가중치 값을 저장하는 데이터 세트.</target>
        </trans-unit>
        <trans-unit id="c275f46f84d94fb1e86dc221ba0e2772a4b65944" translate="yes" xml:space="preserve">
          <source>When saving in TensorFlow format, all objects referenced by the network are saved in the same format as &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;, including any &lt;code&gt;Layer&lt;/code&gt; instances or &lt;code&gt;Optimizer&lt;/code&gt; instances assigned to object attributes. For networks constructed from inputs and outputs using &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt;, &lt;code&gt;Layer&lt;/code&gt; instances used by the network are tracked/saved automatically. For user-defined classes which inherit from &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Layer&lt;/code&gt; instances must be assigned to object attributes, typically in the constructor. See the documentation of &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">TensorFlow 형식으로 저장하면 네트워크에서 참조하는 모든 객체는 객체 속성에 할당 된 &lt;code&gt;Layer&lt;/code&gt; 인스턴스 또는 &lt;code&gt;Optimizer&lt;/code&gt; 인스턴스를 포함하여 &lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 와 동일한 형식으로 저장됩니다 . 하여 입력 및 출력에서 생성 네트워크의 &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt; , &lt;code&gt;Layer&lt;/code&gt; 네트워크에 의해 사용되는 경우 자동으로 저장 / 추적된다. &lt;a href=&quot;../model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 에서 상속되는 사용자 정의 클래스의 경우 일반적으로 생성자에서 &lt;code&gt;Layer&lt;/code&gt; 인스턴스를 객체 속성에 할당해야합니다. 자세한 내용은 &lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 설명서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="58ff97a8bb50647c4ab49957faee2741efb3b467" translate="yes" xml:space="preserve">
          <source>When saving in TensorFlow format, all objects referenced by the network are saved in the same format as &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;, including any &lt;code&gt;Layer&lt;/code&gt; instances or &lt;code&gt;Optimizer&lt;/code&gt; instances assigned to object attributes. For networks constructed from inputs and outputs using &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt;, &lt;code&gt;Layer&lt;/code&gt; instances used by the network are tracked/saved automatically. For user-defined classes which inherit from &lt;a href=&quot;model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Layer&lt;/code&gt; instances must be assigned to object attributes, typically in the constructor. See the documentation of &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">TensorFlow 형식으로 저장하면 네트워크에서 참조하는 모든 객체는 객체 속성에 할당 된 &lt;code&gt;Layer&lt;/code&gt; 인스턴스 또는 &lt;code&gt;Optimizer&lt;/code&gt; 인스턴스를 포함하여 &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 와 동일한 형식으로 저장됩니다 . 하여 입력 및 출력에서 생성 네트워크의 &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt; , &lt;code&gt;Layer&lt;/code&gt; 네트워크에 의해 사용되는 경우 자동으로 저장 / 추적된다. &lt;a href=&quot;model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 에서 상속되는 사용자 정의 클래스의 경우 일반적으로 생성자에서 &lt;code&gt;Layer&lt;/code&gt; 인스턴스를 객체 속성에 할당해야합니다. 자세한 내용은 &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 설명서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="784c3ca10c1f2c8309f633399d4061eef62d41bd" translate="yes" xml:space="preserve">
          <source>When shape_x and shape_y are Tensors representing shapes (i.e. the result of calling tf.shape on another Tensor) this computes a Tensor which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.</source>
          <target state="translated">shape_x 및 shape_y가 모양을 나타내는 텐서 일 때 (즉, 다른 텐서에서 tf.shape를 호출 한 결과) 이것은 shape_x 및 shape_y 모양의 텐서에 적용된 방송 op의 결과의 모양 인 텐서를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="382f3819ea47b409f9da2d9eacd00a3f2dc7a1b2" translate="yes" xml:space="preserve">
          <source>When shape_x and shape_y are fully known TensorShapes this computes a TensorShape which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.</source>
          <target state="translated">shape_x 및 shape_y가 완전히 알려진 TensorShapes 인 경우 shape_x 및 shape_y 모양의 텐서에 적용된 방송 op의 결과의 모양 인 TensorShape를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b25a4e208695ca5ecbc4dac0901769cfec692d0d" translate="yes" xml:space="preserve">
          <source>When sparse_delta.indices is a 1D tensor, this operation is equivalent to &lt;code&gt;scatter_update&lt;/code&gt;.</source>
          <target state="translated">sparse_delta.indices가 1D 텐서 인 경우이 작업은 &lt;code&gt;scatter_update&lt;/code&gt; 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="d43e498906b2adf0ec7959c38a4a8cf0f9fa7d0b" translate="yes" xml:space="preserve">
          <source>When starting a dedicated tf.data dispatch process, use join() to block indefinitely after starting up the server.</source>
          <target state="translated">When starting a dedicated tf.data dispatch process, use join() to block indefinitely after starting up the server.</target>
        </trans-unit>
        <trans-unit id="934351906a686e71e8e5a944d6c4af715197c11e" translate="yes" xml:space="preserve">
          <source>When starting a dedicated tf.data worker process, use join() to block indefinitely after starting up the server.</source>
          <target state="translated">When starting a dedicated tf.data worker process, use join() to block indefinitely after starting up the server.</target>
        </trans-unit>
        <trans-unit id="5fee3dffe2d54b65ce0a85843a0d39fb34dec340" translate="yes" xml:space="preserve">
          <source>When that Op is run it tries to increment the variable by &lt;code&gt;1&lt;/code&gt;. If incrementing the variable would bring it above &lt;code&gt;limit&lt;/code&gt; then the Op raises the exception &lt;code&gt;OutOfRangeError&lt;/code&gt;.</source>
          <target state="translated">해당 Op가 실행되면 변수를 &lt;code&gt;1&lt;/code&gt; 씩 증가시킵니다 . 변수를 증가 시키면 &lt;code&gt;limit&lt;/code&gt; 를 초과 하면 Op는 예외 &lt;code&gt;OutOfRangeError&lt;/code&gt; 를 발생 시킵니다 .</target>
        </trans-unit>
        <trans-unit id="a324d7f1e796820d05403b217fea01aa17af935a" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;GraphDef&lt;/code&gt; is larger than 2GB.</source>
          <target state="translated">When the &lt;code&gt;GraphDef&lt;/code&gt; is larger than 2GB.</target>
        </trans-unit>
        <trans-unit id="1856bfeda6ff30a3afd3efdbb33b44e8e2b972ce" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be &lt;code&gt;non_singular&lt;/code&gt;.</source>
          <target state="translated">When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be &lt;code&gt;non_singular&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="56cd8af2612b32012d09c54f7e15310ad702ad5c" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be positive definite and self adjoint.</source>
          <target state="translated">When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be positive definite and self adjoint.</target>
        </trans-unit>
        <trans-unit id="8ae5f82f4e0944c8fdd12da34fb5e45708d0e8f6" translate="yes" xml:space="preserve">
          <source>When the CrossShardOptimizer is constructed with &lt;code&gt;reduction == losses.Reduction.MEAN&lt;/code&gt; (default), this function scales the loss by &lt;code&gt;1.0 / num_shards&lt;/code&gt; before computing the gradients. Assuming the optimizer uses the default implementation of &lt;code&gt;compute_gradients()&lt;/code&gt;, the gradients of the scaled loss are scaled by &lt;code&gt;1.0 / num_shards&lt;/code&gt; compared to the gradients of the original loss. This scaling factor is important because &lt;code&gt;apply_gradients()&lt;/code&gt; sums gradients across shards, rather than averaging them. However, the scaling factor must be taken into account when clipping the norm of the gradients or performing other postprocessing.</source>
          <target state="translated">CrossShardOptimizer가 &lt;code&gt;reduction == losses.Reduction.MEAN&lt;/code&gt; (기본값)으로 구성된 경우이 함수 는 기울기를 계산하기 전에 손실을 &lt;code&gt;1.0 / num_shards&lt;/code&gt; 조정 합니다. 옵티마이 &lt;code&gt;compute_gradients()&lt;/code&gt; 의 기본 구현을 사용한다고 가정하면 스케일링 된 손실 의 그라디언트는 원래 손실의 그라디언트에 비해 &lt;code&gt;1.0 / num_shards&lt;/code&gt; 로 스케일됩니다 . &lt;code&gt;apply_gradients()&lt;/code&gt; 는 샤드의 그라디언트를 평균화하지 않고 합산 하기 때문에이 스케일링 계수가 중요 합니다. 그러나 그라디언트의 표준을 자르거나 다른 사후 처리를 수행 할 때는 스케일링 계수를 고려해야합니다.</target>
        </trans-unit>
        <trans-unit id="62b085231fa47179e1be16442d315153db83ffe4" translate="yes" xml:space="preserve">
          <source>When the Op is run, it reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if multiple values in the summaries to merge use the same tag.</source>
          <target state="translated">Op가 실행될 때 병합 할 요약의 여러 값이 동일한 태그를 사용하는 경우 &lt;code&gt;InvalidArgument&lt;/code&gt; 오류를 보고합니다 .</target>
        </trans-unit>
        <trans-unit id="a17f60db047668212c07e424cf0f21e2e8e6e67e" translate="yes" xml:space="preserve">
          <source>When the RNN layer is not stateful.</source>
          <target state="translated">When the RNN layer is not stateful.</target>
        </trans-unit>
        <trans-unit id="b8b5dd0fb66417bcf00267bf905ed39a50b9ed42" translate="yes" xml:space="preserve">
          <source>When the batch size of the RNN layer is unknown.</source>
          <target state="translated">When the batch size of the RNN layer is unknown.</target>
        </trans-unit>
        <trans-unit id="04c6456a46e3c6718694244e79e5c206d69bdf28" translate="yes" xml:space="preserve">
          <source>When the file to be loaded is not found.</source>
          <target state="translated">When the file to be loaded is not found.</target>
        </trans-unit>
        <trans-unit id="7c426d27facbfbf4fa665efb6cfbd82b969658b0" translate="yes" xml:space="preserve">
          <source>When the input numpy array is not compatible with the RNN layer state, either size wise or dtype wise.</source>
          <target state="translated">When the input numpy array is not compatible with the RNN layer state, either size wise or dtype wise.</target>
        </trans-unit>
        <trans-unit id="3584f2f08e6dde164e436c25cc19bcd86167e861" translate="yes" xml:space="preserve">
          <source>When the timeout argument is not present or None, the operation will block until the thread terminates.</source>
          <target state="translated">시간 종료 인수가 없거나 없음이면 스레드가 종료 될 때까지 작업이 차단됩니다.</target>
        </trans-unit>
        <trans-unit id="93008cafbefd82e0878a6b1b3873678eb838ef6d" translate="yes" xml:space="preserve">
          <source>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</source>
          <target state="translated">제한 시간 인수가 존재하지만 없음이 아닌 경우 조작의 제한 시간을 초 (또는 소수)로 지정하는 부동 소수점 숫자 여야합니다. join ()은 항상 None을 반환하므로 join () 뒤에 isAlive ()를 호출하여 시간 초과가 발생했는지 여부를 결정해야합니다. 스레드가 여전히 활성 상태이면 join () 호출이 시간 초과됩니다.</target>
        </trans-unit>
        <trans-unit id="df9a68fd13d29e9ba4462924730a99e7d6cbe88d" translate="yes" xml:space="preserve">
          <source>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call is_alive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</source>
          <target state="translated">When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call is_alive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</target>
        </trans-unit>
        <trans-unit id="df29949db6d12bcc6440a4ff55b2a3b63f98638c" translate="yes" xml:space="preserve">
          <source>When the underlying interpreter fails raise ValueError.</source>
          <target state="translated">When the underlying interpreter fails raise ValueError.</target>
        </trans-unit>
        <trans-unit id="79079716d44440c95fa5813292d42c940b3c76a3" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. If gradients are instead computed with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, loss scaling will not be applied, which will likely cause your model not to converge due to float16 underflow problems. To apply lossing scaling with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_scaled_loss&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_scaled_loss&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_unscaled_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_unscaled_gradients&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer&quot;&gt;&lt;code&gt;keras.mixed_precision.experimental.LossScaleOptimizer&lt;/code&gt;&lt;/a&gt; for details how to do this.</source>
          <target state="translated">When this function is used, gradients should be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt; . If gradients are instead computed with &lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;, loss scaling will not be applied, which will likely cause your model not to converge due to float16 underflow problems. To apply lossing scaling with &lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_scaled_loss&quot;&gt; &lt;code&gt;LossScaleOptimizer.get_scaled_loss&lt;/code&gt; &lt;/a&gt; and &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_unscaled_gradients&quot;&gt; &lt;code&gt;LossScaleOptimizer.get_unscaled_gradients&lt;/code&gt; &lt;/a&gt;. See &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer&quot;&gt; &lt;code&gt;keras.mixed_precision.experimental.LossScaleOptimizer&lt;/code&gt; &lt;/a&gt; for details how to do this.</target>
        </trans-unit>
        <trans-unit id="5afcb3cf29a4af014bb340f2f0c516a9732b6d10" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. Gradients should not be computed with &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. This is because the returned optimizer will apply loss scaling, and &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will not. If you do directly use &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, your model may not converge due to float16 underflow problems.</source>
          <target state="translated">이 기능을 사용하는 경우에는, 그라데이션 어느 호출하여 계산 반환 최적화인가되어야 &lt;code&gt;opt.minimize()&lt;/code&gt; 또는 &lt;code&gt;opt.compute_gradients()&lt;/code&gt; 다음에 &lt;code&gt;opt.apply_gradients()&lt;/code&gt; . 그래디언트는 &lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt; 로 계산해서는 안됩니다 . 리턴 된 옵티마이 &lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 손실 스케일링을 적용하고 tf.gradients 또는 &lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt; 가 적용되지 않기 때문입니다. &lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; 를&lt;/a&gt; 직접 사용하는 경우 float16 언더 플로 문제로 인해 모델이 수렴되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="caac8e6b5e9c55ec72ae9a078f806e80238a5f0b" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. Gradients should not be computed with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. This is because the returned optimizer will apply loss scaling, and &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will not. If you do directly use &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, your model may not converge due to float16 underflow problems.</source>
          <target state="translated">이 기능을 사용하는 경우에는, 그라데이션 어느 호출하여 계산 반환 최적화인가되어야 &lt;code&gt;opt.minimize()&lt;/code&gt; 또는 &lt;code&gt;opt.compute_gradients()&lt;/code&gt; 다음에 &lt;code&gt;opt.apply_gradients()&lt;/code&gt; . 그래디언트는 &lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt; 로 계산해서는 안됩니다 . 리턴 된 옵티마이 &lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 손실 스케일링을 적용하고 tf.gradients 또는 &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt; 가 적용되지 않기 때문입니다. &lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; 를&lt;/a&gt; 직접 사용하는 경우 float16 언더 플로 문제로 인해 모델이 수렴되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="00a169e018bcf148a01078627fb468a85b4f57d1" translate="yes" xml:space="preserve">
          <source>When this is called, the graph is finalized and ops can no longer be added to the graph.</source>
          <target state="translated">이것이 호출되면 그래프가 완성되고 더 이상 그래프에 op를 추가 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="69ace3a238de55ef5f17d0593a3426df7375ca43" translate="yes" xml:space="preserve">
          <source>When this is true, the Adam update formula is changed from &lt;code&gt;m / (sqrt(v) + epsilon)&lt;/code&gt; to &lt;code&gt;m / sqrt(v + epsilon**2)&lt;/code&gt;. This option improves the performance of TPU training and is not expected to harm model quality.</source>
          <target state="translated">When this is true, the Adam update formula is changed from &lt;code&gt;m / (sqrt(v) + epsilon)&lt;/code&gt; to &lt;code&gt;m / sqrt(v + epsilon**2)&lt;/code&gt; . This option improves the performance of TPU training and is not expected to harm model quality.</target>
        </trans-unit>
        <trans-unit id="f06429721d1fc01498f2381f4946b62026a0283c" translate="yes" xml:space="preserve">
          <source>When this op finishes, all ops in &lt;code&gt;inputs&lt;/code&gt; have finished. This op has no output.</source>
          <target state="translated">이 op가 완료되면 &lt;code&gt;inputs&lt;/code&gt; 모든 op가 완료된 것입니다. 이 op에는 출력이 없습니다.</target>
        </trans-unit>
        <trans-unit id="e93fcaa76656fcbb53b75ad595ffb1773a2d63ac" translate="yes" xml:space="preserve">
          <source>When tracing a function, no ops are being executed, shapes may be unknown. See the &lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;Concrete Functions Guide&lt;/a&gt; for details.</source>
          <target state="translated">When tracing a function, no ops are being executed, shapes may be unknown. See the &lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;Concrete Functions Guide&lt;/a&gt; for details.</target>
        </trans-unit>
        <trans-unit id="db72b6d879a1722ab8b371aaaee80aaed3e96f22" translate="yes" xml:space="preserve">
          <source>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.</source>
          <target state="translated">모델을 훈련 할 때 훈련 된 매개 변수의 이동 평균을 유지하는 것이 종종 유리합니다. 평균 매개 변수를 사용하는 평가는 때때로 최종 훈련 된 값보다 훨씬 더 나은 결과를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="927a8a21e9e3520f84a7ae9e37d363926dbaebe1" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 코사인 붕괴 기능을 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8d86dcb91899a78227813e8c0d47037c6ee19dfd" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function with restarts to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 다시 시작한 코사인 붕괴 기능을 제공된 초기 학습 속도로 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90010651f41909cacf9403b614efcb8bd58e1369" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a linear cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 선형 코사인 감쇠 함수를 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6a9c3656ff265f4bda198cf8e5dfd044fb933790" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a noisy linear cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 잡음이있는 선형 코사인 감쇠 함수를 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="64048e6ec72e37dd6f4daf26af9d33a56f72d8c7" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 지수 감쇠 기능을 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d181a155b50eadc32c09739aa94b40b51fe01a65" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate. It requires an &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 지수 감쇠 기능을 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4f6fc97dc42e006ec89048c76f7f47de1eeb3d9a" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an inverse decay function to a provided initial learning rate. It requires an &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 함수는 역 감쇠 기능을 제공된 초기 학습 속도에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;global_step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4ddff8c9e5ff83f95e35cc80fb672fdcfe082715" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 코사인 붕괴 기능을 옵티 마이저 단계에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="512b0ee6d1762189943ee9ddad5e78bda622dc14" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a cosine decay function with restarts to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 다시 시작하는 코사인 붕괴 기능을 옵티 마이저 단계에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="93d56e5e126f6377bdf1a06e2189c730d031c2b4" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a linear cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 선형 코사인 감쇠 함수를 옵티 마이저 단계에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="57bf3cf4acea41746a7afc38b8058640564015fe" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a noisy linear cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 노이즈 선형 코사인 감쇠 함수를 옵티 마이저 단계에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="94deb77d4f75eb579f72f3db2e455ef2c44c44e1" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies an exponential decay function to an optimizer step, given a provided initial learning rate.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 지수 감쇠 기능을 옵티 마이저 단계에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="fb18244719acd2825faa9890d721d54e05f0a037" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies the inverse decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">모델을 훈련 할 때 훈련이 진행됨에 따라 학습 속도를 낮추는 것이 좋습니다. 이 스케줄은 제공된 초기 학습 속도가 주어지면 역 감쇠 기능을 옵티 마이저 단계에 적용합니다. 소멸 된 학습률을 계산 하려면 &lt;code&gt;step&lt;/code&gt; 값 이 필요합니다 . 각 훈련 단계에서 증가시키는 TensorFlow 변수를 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0cf6ec8113a82c824118b19bb4f2d871de8f3f5d" translate="yes" xml:space="preserve">
          <source>When used with &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;, outside of built-in training loops such as &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt;&lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt;, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error.</source>
          <target state="translated">&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt; &lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt; 과 같은 내장 교육 루프 외부에서 &lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt; 와 함께 사용하는 경우 'SUM'또는 'NONE'축소 유형을 사용하고 교육 루프에서 손실을 명시 적으로 줄이십시오. 'AUTO'또는 'SUM_OVER_BATCH_SIZE'를 사용하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="c6a537963fdbb8e6b3429be0c961e7b0d2a0a449" translate="yes" xml:space="preserve">
          <source>When used, it overrides name_ and is not made unique. If a template of the same scope/unique_name already exists and reuse is false, an error is raised. Defaults to None.</source>
          <target state="translated">When used, it overrides name_ and is not made unique. If a template of the same scope/unique_name already exists and reuse is false, an error is raised. Defaults to None.</target>
        </trans-unit>
        <trans-unit id="58c97e844aea42b09b6f861449f7ea173f99a04f" translate="yes" xml:space="preserve">
          <source>When using InputLayer with Keras Sequential model, it can be skipped by moving the input_shape parameter to the first layer after the InputLayer.</source>
          <target state="translated">When using InputLayer with Keras Sequential model, it can be skipped by moving the input_shape parameter to the first layer after the InputLayer.</target>
        </trans-unit>
        <trans-unit id="4ecb7ac1c54db33d719cbd8ca1f72cb0bec98edc" translate="yes" xml:space="preserve">
          <source>When using a custom callable for &lt;code&gt;split&lt;/code&gt;, the data received by the callable will have the 1st dimension squeezed out - instead of &lt;code&gt;[[&quot;string to split&quot;], [&quot;another string to split&quot;]]&lt;/code&gt;, the Callable will see &lt;code&gt;[&quot;string to split&quot;, &quot;another string to split&quot;]&lt;/code&gt;. The callable should return a Tensor with the first dimension containing the split tokens - in this example, we should see something like &lt;code&gt;[[&quot;string&quot;, &quot;to&quot;, &quot;split], [&quot;another&quot;, &quot;string&quot;, &quot;to&quot;, &quot;split&quot;]]&lt;/code&gt;. This makes the callable site natively compatible with &lt;a href=&quot;../../../../strings/split&quot;&gt;&lt;code&gt;tf.strings.split()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">When using a custom callable for &lt;code&gt;split&lt;/code&gt; , the data received by the callable will have the 1st dimension squeezed out - instead of &lt;code&gt;[[&quot;string to split&quot;], [&quot;another string to split&quot;]]&lt;/code&gt; , the Callable will see &lt;code&gt;[&quot;string to split&quot;, &quot;another string to split&quot;]&lt;/code&gt; . The callable should return a Tensor with the first dimension containing the split tokens - in this example, we should see something like &lt;code&gt;[[&quot;string&quot;, &quot;to&quot;, &quot;split], [&quot;another&quot;, &quot;string&quot;, &quot;to&quot;, &quot;split&quot;]]&lt;/code&gt; . This makes the callable site natively compatible with &lt;a href=&quot;../../../../strings/split&quot;&gt; &lt;code&gt;tf.strings.split()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4658ae77687cb1c8967c8186bb04e6986f0779aa" translate="yes" xml:space="preserve">
          <source>When using a custom callable for &lt;code&gt;standardize&lt;/code&gt;, the data received by the callable will be exactly as passed to this layer. The callable should return a tensor of the same shape as the input.</source>
          <target state="translated">When using a custom callable for &lt;code&gt;standardize&lt;/code&gt; , the data received by the callable will be exactly as passed to this layer. The callable should return a tensor of the same shape as the input.</target>
        </trans-unit>
        <trans-unit id="e0724c7bfb29f8f0c6cdcbe1954184f010886b9c" translate="yes" xml:space="preserve">
          <source>When using multiple critical sections on the same resources, there is no guarantee of exclusive access to those resources. This behavior is disallowed by default (but see the kwarg &lt;code&gt;exclusive_resource_access&lt;/code&gt;).</source>
          <target state="translated">동일한 리소스에서 여러 중요 섹션을 사용하는 경우 해당 리소스에 독점적으로 액세스 할 수있는 것은 아닙니다. 이 동작은 기본적으로 허용되지 않습니다 (kwarg &lt;code&gt;exclusive_resource_access&lt;/code&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="005ecd21a6428476fbceb142b210d5384fc2790a" translate="yes" xml:space="preserve">
          <source>When using the default, an appropriate policy will be picked automatically. The default policy may change over time.</source>
          <target state="translated">기본값을 사용하면 적절한 정책이 자동으로 선택됩니다. 기본 정책은 시간이 지남에 따라 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="67d49a53dc96e389873c164cb198f138ecc7ff2f" translate="yes" xml:space="preserve">
          <source>When using these moments for batch normalization (see &lt;a href=&quot;../../../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">배치 정규화에이 모멘트를 사용하는 경우 ( &lt;a href=&quot;../../../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt; 참조 ) :</target>
        </trans-unit>
        <trans-unit id="591b74326af491d6abfab42a671eba7ebb5be83e" translate="yes" xml:space="preserve">
          <source>When using these moments for batch normalization (see &lt;a href=&quot;batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">배치 정규화에이 모멘트를 사용하는 경우 ( &lt;a href=&quot;batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt; 참조 ) :</target>
        </trans-unit>
        <trans-unit id="ea3d2206487c99a74b50234d9380045457ab56b2" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide an &lt;code&gt;input_shape&lt;/code&gt; argument (tuple of integers or &lt;code&gt;None&lt;/code&gt;, e.g. &lt;code&gt;(10, 128)&lt;/code&gt; for sequences of 10 vectors of 128-dimensional vectors, or &lt;code&gt;(None, 128)&lt;/code&gt; for variable-length sequences of 128-dimensional vectors.</source>
          <target state="translated">이 레이어를 모델의 첫 번째 레이어로 사용하는 &lt;code&gt;(10, 128)&lt;/code&gt; 차원 벡터의 10 개 벡터 시퀀스의 경우 &lt;code&gt;input_shape&lt;/code&gt; 인수 (정수의 터플 또는 &lt;code&gt;None&lt;/code&gt; , 예 : (10, 128) 또는 가변 길이의 경우 &lt;code&gt;(None, 128)&lt;/code&gt; 를 제공하십시오. 128 차원 벡터의 서열.</target>
        </trans-unit>
        <trans-unit id="0a409624f20a1b12afa97b438af1e2c02851ec80" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 128, 1)&lt;/code&gt; for 128x128x128 volumes with a single channel, in &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">이 레이어를 모델에서 첫 번째 레이어로 사용하는 경우 키워드 인수 &lt;code&gt;input_shape&lt;/code&gt; (정수의 튜플, 샘플 축을 포함하지 않음)를 제공하십시오 (예 : 단일 채널이있는 128x128x128 볼륨의 경우 &lt;code&gt;input_shape=(128, 128, 128, 1)&lt;/code&gt; , 에서 &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="14d463d2a8afe47183522c1acdade8296e826653" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 128, 3)&lt;/code&gt; for a 128x128x128 volume with 3 channels if &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">이 레이어를 모델의 첫 번째 레이어로 사용하는 경우 키워드 인수 &lt;code&gt;input_shape&lt;/code&gt; (정수의 튜플, 샘플 축을 포함하지 않음)를 제공하십시오 (예 : 3 개의 채널이있는 128x128x128 볼륨의 경우 &lt;code&gt;input_shape=(128, 128, 128, 3)&lt;/code&gt; &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="b700c0ba9e5af2d79f0ea705fd60f2846a05cd69" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 3)&lt;/code&gt; for 128x128 RGB pictures in &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">이 레이어를 모델의 첫 번째 레이어로 사용하는 경우 &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; 에서 키워드 인수 &lt;code&gt;input_shape&lt;/code&gt; (정수의 튜플, 샘플 축은 포함하지 않음)를 입력하십시오 (예 : &lt;code&gt;input_shape=(128, 128, 3)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="39c55fa13fdb10b59035551c9a262071686d849a" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 3)&lt;/code&gt; for data with 128 time steps and 3 channels.</source>
          <target state="translated">When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 3)&lt;/code&gt; for data with 128 time steps and 3 channels.</target>
        </trans-unit>
        <trans-unit id="90420d916accea41b90f4795021da2ee04f4f6d6" translate="yes" xml:space="preserve">
          <source>When variables are assigned to multiple workers, each worker writes its own section of the checkpoint. These sections are then merged/re-indexed to behave as a single checkpoint. This avoids copying all variables to one worker, but does require that all workers see a common filesystem.</source>
          <target state="translated">변수가 여러 작업자에게 할당되면 각 작업자는 검사 점의 자체 섹션을 씁니다. 그런 다음이 섹션들은 병합 / 재 인덱싱되어 단일 체크 포인트로 작동합니다. 이렇게하면 모든 변수를 한 작업자에게 복사하지 않아도되지만 모든 작업자가 공통 파일 시스템을 볼 수 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="d43c37e91c1c99cab60a11a374e0ae141f677fad" translate="yes" xml:space="preserve">
          <source>When writing a TensorFlow program, the main object that is manipulated and passed around is the &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">When writing a TensorFlow program, the main object that is manipulated and passed around is the &lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5727fc2aeafda851fc9ab281a980ef4c906dc39b" translate="yes" xml:space="preserve">
          <source>When you build a model for training you usually need ops to initialize variables, a &lt;code&gt;Saver&lt;/code&gt; to checkpoint them, an op to collect summaries for the visualizer, and so on.</source>
          <target state="translated">훈련을위한 모델을 구축 할 때는 일반적으로 변수를 초기화 하는 op, 변수 를 체크 포인트 하는 &lt;code&gt;Saver&lt;/code&gt; , 비주얼 라이저에 대한 요약을 수집하는 op 등이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="2571f0d8bc64be0b275df26c714df2c7b618a158" translate="yes" xml:space="preserve">
          <source>When you iterate over a dataset containing the &lt;code&gt;distribute&lt;/code&gt; transformation, the tf.data service creates a &quot;job&quot; which produces data for the dataset iteration.</source>
          <target state="translated">When you iterate over a dataset containing the &lt;code&gt;distribute&lt;/code&gt; transformation, the tf.data service creates a &quot;job&quot; which produces data for the dataset iteration.</target>
        </trans-unit>
        <trans-unit id="39d490a1a6c041251d0a268b335edd8cf95e7a4b" translate="yes" xml:space="preserve">
          <source>When you later call the &lt;code&gt;create_threads()&lt;/code&gt; method, the &lt;code&gt;QueueRunner&lt;/code&gt; will create one thread for each op in &lt;code&gt;enqueue_ops&lt;/code&gt;. Each thread will run its enqueue op in parallel with the other threads. The enqueue ops do not have to all be the same op, but it is expected that they all enqueue tensors in &lt;code&gt;queue&lt;/code&gt;.</source>
          <target state="translated">나중에 호출 할 때 &lt;code&gt;create_threads()&lt;/code&gt; 방법의 &lt;code&gt;QueueRunner&lt;/code&gt; 는 각 연산에 대한 하나 개의 스레드를 생성합니다 &lt;code&gt;enqueue_ops&lt;/code&gt; . 각 스레드는 다른 스레드와 병렬로 대기열에 대기합니다. 인큐 조작은 모두 동일한 op 일 필요는 없지만 큐에서 텐서를 모두 &lt;code&gt;queue&lt;/code&gt; 할 것으로 예상 됩니다 .</target>
        </trans-unit>
        <trans-unit id="d3a2e77e985cdd0c4ca04ebfb7ec15ae2eed6e37" translate="yes" xml:space="preserve">
          <source>When you launch the graph, variables have to be explicitly initialized before you can run Ops that use their value. You can initialize a variable by running its &lt;em&gt;initializer op&lt;/em&gt;, restoring the variable from a save file, or simply running an &lt;code&gt;assign&lt;/code&gt; Op that assigns a value to the variable. In fact, the variable &lt;em&gt;initializer op&lt;/em&gt; is just an &lt;code&gt;assign&lt;/code&gt; Op that assigns the variable's initial value to the variable itself.</source>
          <target state="translated">그래프를 시작할 때 값을 사용하는 Ops를 실행하기 전에 변수를 명시 적으로 초기화해야합니다. &lt;em&gt;초기화 프로그램 op&lt;/em&gt; 를 실행하거나 저장 파일에서 변수를 복원하거나 변수 에 값을 지정 하는 &lt;code&gt;assign&lt;/code&gt; Op를 실행하여 변수를 초기화 할 수 있습니다 . 실제로 변수 &lt;em&gt;이니셜 라이저 op&lt;/em&gt; 는 변수의 초기 값을 변수 자체에 할당 하는 &lt;code&gt;assign&lt;/code&gt; Op입니다.</target>
        </trans-unit>
        <trans-unit id="36dcf90c997814649120f194530d033746d75e2c" translate="yes" xml:space="preserve">
          <source>Whenever &lt;code&gt;partial_pivoting&lt;/code&gt; is true and the backend is XLA.</source>
          <target state="translated">Whenever &lt;code&gt;partial_pivoting&lt;/code&gt; is true and the backend is XLA.</target>
        </trans-unit>
        <trans-unit id="2e2218d66173c9095f9d7c0c982dd0a7b7b1d0cb" translate="yes" xml:space="preserve">
          <source>Whenever possible, the session will raise a more specific subclass of &lt;code&gt;OpError&lt;/code&gt; from the &lt;a href=&quot;../errors&quot;&gt;&lt;code&gt;tf.errors&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="translated">가능할 때마다 세션은 &lt;a href=&quot;../errors&quot;&gt; &lt;code&gt;tf.errors&lt;/code&gt; &lt;/a&gt; 모듈 에서 보다 구체적인 &lt;code&gt;OpError&lt;/code&gt; 서브 클래스를 발생 시킵니다 .</target>
        </trans-unit>
        <trans-unit id="525f61b6729229b1a8854a9e50bcaa52ea96b131" translate="yes" xml:space="preserve">
          <source>Where</source>
          <target state="translated">Where</target>
        </trans-unit>
        <trans-unit id="a5a5757b0332ab74cd51749fc4f26f2282d43f68" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;j&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th &lt;code&gt;True&lt;/code&gt; entry of &lt;code&gt;mask[a1...aA]&lt;/code&gt;.</source>
          <target state="translated">여기서 &lt;code&gt;j&lt;/code&gt; 는 &lt;code&gt;mask[a1...aA]&lt;/code&gt; 의 &lt;code&gt;i&lt;/code&gt; 번째 &lt;code&gt;True&lt;/code&gt; 항목입니다 .</target>
        </trans-unit>
        <trans-unit id="a9e5e6939cbf334efa1154ae5d005e3955d0f0b7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;key&lt;/code&gt; is a feature key whose values are used to partition the values. Partitions are listed from outermost to innermost.</source>
          <target state="translated">여기서 &lt;code&gt;key&lt;/code&gt; 는 값을 분할하는 데 사용되는 기능 키입니다. 파티션은 가장 바깥 쪽에서 가장 안쪽으로 나열됩니다.</target>
        </trans-unit>
        <trans-unit id="17e66517ae2fa6c4b3f8472462247a694203eb5e" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt;, and &lt;code&gt;day&lt;/code&gt; specify the date beyond which binaries that consume a model are expected to have been updated to include the new operations. This date is typically at least 3 weeks beyond the date the code that adds the new operation is committed.</source>
          <target state="translated">어디 &lt;code&gt;year&lt;/code&gt; , &lt;code&gt;month&lt;/code&gt; , 그리고 &lt;code&gt;day&lt;/code&gt; 모델을 소비하는 바이너리가 새로운 작업을 포함하도록 업데이트되었습니다 것으로 예상되는 이상으로 날짜를 지정합니다. 이 날짜는 일반적으로 새 작업을 추가하는 코드가 커밋 된 날짜 이후 3 주 이상입니다.</target>
        </trans-unit>
        <trans-unit id="743987e43173da0b07d4299cc71758449720108f" translate="yes" xml:space="preserve">
          <source>Where &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt;, &lt;em&gt;M&lt;/em&gt; = &lt;code&gt;ndims(indices)&lt;/code&gt;, and &lt;em&gt;B&lt;/em&gt; = &lt;code&gt;batch_dims&lt;/code&gt;. Note that &lt;code&gt;params.shape[:batch_dims]&lt;/code&gt; must be identical to &lt;code&gt;indices.shape[:batch_dims]&lt;/code&gt;.</source>
          <target state="translated">여기서 &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt; , &lt;em&gt;M&lt;/em&gt; = &lt;code&gt;ndims(indices)&lt;/code&gt; , &lt;em&gt;B&lt;/em&gt; = &lt;code&gt;batch_dims&lt;/code&gt; 입니다. 참고 &lt;code&gt;params.shape[:batch_dims]&lt;/code&gt; 동일해야 &lt;code&gt;indices.shape[:batch_dims]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="13aa13f1ced8020c65b238cd2e5743aaab5f6a97" translate="yes" xml:space="preserve">
          <source>Where &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt;.</source>
          <target state="translated">여기서 &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="7e741bc3dcef0123eeda11543758853be2aac149" translate="yes" xml:space="preserve">
          <source>Where:</source>
          <target state="translated">Where:</target>
        </trans-unit>
        <trans-unit id="df5ee0a432194eaf058dafaec560ccdd5751f85a" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the first dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;../../gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="translated">반면에 &lt;a href=&quot;../../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt; &lt;code&gt;indices&lt;/code&gt; 정의의 첫 번째 차원으로 분할 &lt;code&gt;params&lt;/code&gt; 에서 &lt;a href=&quot;../../gather_nd&quot;&gt; &lt;code&gt;tf.gather_nd&lt;/code&gt; &lt;/a&gt; , &lt;code&gt;indices&lt;/code&gt; 처음에 정의 슬라이스 &lt;code&gt;N&lt;/code&gt; 의 치수 &lt;code&gt;params&lt;/code&gt; , 여기서 &lt;code&gt;N = indices.shape[-1]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="db72094af8e11b94ee83cae58cea0ebfb59e1a33" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the &lt;code&gt;axis&lt;/code&gt; dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;../gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="translated">Whereas in &lt;a href=&quot;../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt; &lt;code&gt;indices&lt;/code&gt; defines slices into the &lt;code&gt;axis&lt;/code&gt; dimension of &lt;code&gt;params&lt;/code&gt; , in &lt;a href=&quot;../gather_nd&quot;&gt; &lt;code&gt;tf.gather_nd&lt;/code&gt; &lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt; , where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="37006b5e3234b3a81d34b49c1399e20287f6f5fd" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the first dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="translated">반면에 &lt;a href=&quot;gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt; &lt;code&gt;indices&lt;/code&gt; 정의의 첫 번째 차원으로 분할 &lt;code&gt;params&lt;/code&gt; 에서 &lt;a href=&quot;gather_nd&quot;&gt; &lt;code&gt;tf.gather_nd&lt;/code&gt; &lt;/a&gt; , &lt;code&gt;indices&lt;/code&gt; 처음에 정의 슬라이스 &lt;code&gt;N&lt;/code&gt; 의 치수 &lt;code&gt;params&lt;/code&gt; , 여기서 &lt;code&gt;N = indices.shape[-1]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d4b7353826912b9cec703766decc544dcf83f10b" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;output&lt;/code&gt; is expected to be a logits tensor. By default, we consider that &lt;code&gt;output&lt;/code&gt; encodes a probability distribution.</source>
          <target state="translated">Whether &lt;code&gt;output&lt;/code&gt; is expected to be a logits tensor. By default, we consider that &lt;code&gt;output&lt;/code&gt; encodes a probability distribution.</target>
        </trans-unit>
        <trans-unit id="47c0b61274d31e65caf92de4ae562ae2c5114355" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution.</source>
          <target state="translated">Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution.</target>
        </trans-unit>
        <trans-unit id="fe485b5aa69407b2cec559631c713bb745bddf4c" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. **Note - Using from_logits=True may be more numerically stable.</source>
          <target state="translated">Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. **Note - Using from_logits=True may be more numerically stable.</target>
        </trans-unit>
        <trans-unit id="c8c9ab93a2df9a01da2c64742b40fc42d6c91dfb" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. &lt;strong&gt;Note - Using from_logits=True is more numerically stable.&lt;/strong&gt;</source>
          <target state="translated">Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. &lt;strong&gt;Note - Using from_logits=True is more numerically stable.&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="8fd73dec5012589d7460a67d8190f1604f4a14a8" translate="yes" xml:space="preserve">
          <source>Whether GPU-CPU memory swap is enabled for this loop.</source>
          <target state="translated">Whether GPU-CPU memory swap is enabled for this loop.</target>
        </trans-unit>
        <trans-unit id="d6717cbbf5b0f27136d029a9a5a4fe644c3d0bdb" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;DType&lt;/code&gt; is unsigned.</source>
          <target state="translated">&lt;code&gt;DType&lt;/code&gt; 이 서명되지 않았 는지 여부</target>
        </trans-unit>
        <trans-unit id="1e01d5432153fe1934c596d7a5942310bb55c381" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;History&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</source>
          <target state="translated">Whether a &lt;code&gt;History&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</target>
        </trans-unit>
        <trans-unit id="224e3e261d520f419e922338344450c9f2dfc0d7" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;ProgbarLogger&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</source>
          <target state="translated">Whether a &lt;code&gt;ProgbarLogger&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</target>
        </trans-unit>
        <trans-unit id="8aa3f3168594096d3d8b8e19131df9c4c2c6f451" translate="yes" xml:space="preserve">
          <source>Whether autograph should be applied on &lt;code&gt;func&lt;/code&gt; before tracing a graph. Data-dependent control flow requires &lt;code&gt;autograph=True&lt;/code&gt;. For more information, see the &lt;a href=&quot;https://www.tensorflow.org/guide/function&quot;&gt;tf.function and AutoGraph guide&lt;/a&gt;.</source>
          <target state="translated">Whether autograph should be applied on &lt;code&gt;func&lt;/code&gt; before tracing a graph. Data-dependent control flow requires &lt;code&gt;autograph=True&lt;/code&gt; . For more information, see the &lt;a href=&quot;https://www.tensorflow.org/guide/function&quot;&gt;tf.function and AutoGraph guide&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e52c3c2213cac47314d63bd666108cb982d0a526" translate="yes" xml:space="preserve">
          <source>Whether backprop is enabled for this while loop.</source>
          <target state="translated">Whether backprop is enabled for this while loop.</target>
        </trans-unit>
        <trans-unit id="2a906b7b12a6aaefca1bb38dbd9c872d24af6915" translate="yes" xml:space="preserve">
          <source>Whether bias centering needs to occur. Bias centering refers to the first node in the very first tree returning the prediction that is aligned with the original labels distribution. For example, for regression problems, the first node will return the mean of the labels. For binary classification problems, it will return a logit for a prior probability of label 1.</source>
          <target state="translated">Whether bias centering needs to occur. Bias centering refers to the first node in the very first tree returning the prediction that is aligned with the original labels distribution. For example, for regression problems, the first node will return the mean of the labels. For binary classification problems, it will return a logit for a prior probability of label 1.</target>
        </trans-unit>
        <trans-unit id="43c8b1e245711e513da5fb3224a94014bfbd2ea6" translate="yes" xml:space="preserve">
          <source>Whether checkpointing is needed.</source>
          <target state="translated">검사 점이 필요한지 여부</target>
        </trans-unit>
        <trans-unit id="d4eda7288db97494a48ea76ccf902c7ac0f935a7" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensor_list&lt;/code&gt; is a single example.</source>
          <target state="translated">Whether each tensor in &lt;code&gt;tensor_list&lt;/code&gt; is a single example.</target>
        </trans-unit>
        <trans-unit id="95fd49cdfd4929bef2715197b8eb6fc30ab04a73" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensor_list_list&lt;/code&gt; is a single example.</source>
          <target state="translated">Whether each tensor in &lt;code&gt;tensor_list_list&lt;/code&gt; is a single example.</target>
        </trans-unit>
        <trans-unit id="ccb1e098286182b19dc3309570dc9cdd8118f6d3" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensors&lt;/code&gt; is a single example.</source>
          <target state="translated">Whether each tensor in &lt;code&gt;tensors&lt;/code&gt; is a single example.</target>
        </trans-unit>
        <trans-unit id="d6a6ce44ff2b603ee993694c9f88f39e13a58c1f" translate="yes" xml:space="preserve">
          <source>Whether initialization is needed.</source>
          <target state="translated">초기화가 필요한지 여부</target>
        </trans-unit>
        <trans-unit id="b62856eef3b197086ffc08e6c1915a9c8bdb4a27" translate="yes" xml:space="preserve">
          <source>Whether only account the statistics of displayed profiler nodes.</source>
          <target state="translated">표시된 프로파일 러 노드의 통계 만 고려하는지 여부</target>
        </trans-unit>
        <trans-unit id="b2a9b9493710259f1453769c41e2339bd8290c9f" translate="yes" xml:space="preserve">
          <source>Whether only weights are saved, or the whole model is saved.</source>
          <target state="translated">Whether only weights are saved, or the whole model is saved.</target>
        </trans-unit>
        <trans-unit id="369fdea0194aadd9a179680eb99f4f5771059465" translate="yes" xml:space="preserve">
          <source>Whether operations should be dispatched synchronously. Valid values:</source>
          <target state="translated">Whether operations should be dispatched synchronously. Valid values:</target>
        </trans-unit>
        <trans-unit id="584fa0c419a0180a4a9a48f62c36f8aa85cbba56" translate="yes" xml:space="preserve">
          <source>Whether or not the embedding is trainable. Default is True.</source>
          <target state="translated">Whether or not the embedding is trainable. Default is True.</target>
        </trans-unit>
        <trans-unit id="cd70d2454f94f5a5bd23acd3b6d8d74be52ad65b" translate="yes" xml:space="preserve">
          <source>Whether or not the enum is to be case-sensitive.</source>
          <target state="translated">Whether or not the enum is to be case-sensitive.</target>
        </trans-unit>
        <trans-unit id="5af2fcf03aaec3220bccfce000595207f5de5efd" translate="yes" xml:space="preserve">
          <source>Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during export.</source>
          <target state="translated">Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during export.</target>
        </trans-unit>
        <trans-unit id="541b0e69634db2eb96093b9893572ce18575fde6" translate="yes" xml:space="preserve">
          <source>Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during import.</source>
          <target state="translated">Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during import.</target>
        </trans-unit>
        <trans-unit id="f2329674feeb4db85ce54cddb74a65691271f9d8" translate="yes" xml:space="preserve">
          <source>Whether or not to close all open fd's in the child after forking.</source>
          <target state="translated">Whether or not to close all open fd's in the child after forking.</target>
        </trans-unit>
        <trans-unit id="7c16fc5a5fb92977ae5605b686e098e91fe724b8" translate="yes" xml:space="preserve">
          <source>Whether saving summaries is needed.</source>
          <target state="translated">요약 저장이 필요한지 여부</target>
        </trans-unit>
        <trans-unit id="a67af11a445e7ad4ad271581ae16018da270fae5" translate="yes" xml:space="preserve">
          <source>Whether shape inference is enabled.</source>
          <target state="translated">Whether shape inference is enabled.</target>
        </trans-unit>
        <trans-unit id="213556493385771188ee3ad68ba9d0c3e3df86d3" translate="yes" xml:space="preserve">
          <source>Whether the &lt;code&gt;TensorArray&lt;/code&gt; can grow past its initial size.</source>
          <target state="translated">Whether the &lt;code&gt;TensorArray&lt;/code&gt; can grow past its initial size.</target>
        </trans-unit>
        <trans-unit id="b529b545844f1f5f6b86ac51e751e32c783eddaa" translate="yes" xml:space="preserve">
          <source>Whether the &lt;code&gt;input_bytes&lt;/code&gt; data is in little-endian format. Data will be converted into host byte order if necessary.</source>
          <target state="translated">Whether the &lt;code&gt;input_bytes&lt;/code&gt; data is in little-endian format. Data will be converted into host byte order if necessary.</target>
        </trans-unit>
        <trans-unit id="254e76001f48c136c6f2b62d065c4a944ffb37c6" translate="yes" xml:space="preserve">
          <source>Whether the Reader implementation can serialize its state.</source>
          <target state="translated">Reader 구현이 상태를 직렬화 할 수 있는지 여부</target>
        </trans-unit>
        <trans-unit id="8e46d69392dfbc82012e7be9a6824ef93b52a84b" translate="yes" xml:space="preserve">
          <source>Whether the layer is dynamic (eager-only); set in the constructor.</source>
          <target state="translated">Whether the layer is dynamic (eager-only); set in the constructor.</target>
        </trans-unit>
        <trans-unit id="b6e0e7bc8fc1c73b7aaaf3cab72814f4b9d143dc" translate="yes" xml:space="preserve">
          <source>Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of &lt;code&gt;layer.trainable_weights&lt;/code&gt;.</source>
          <target state="translated">Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of &lt;code&gt;layer.trainable_weights&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="decf851b03ac228e9313cade0b5957f7b666dae0" translate="yes" xml:space="preserve">
          <source>Whether the outputs need to be produced in deterministic order. If None, defaults to True.</source>
          <target state="translated">출력을 결정 론적 순서로 생성해야하는지 여부 None이면 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="204afdb37d75d1615b12cbddd4b547cac2445374" translate="yes" xml:space="preserve">
          <source>Whether the resources required by &lt;code&gt;fn&lt;/code&gt; should be exclusive to this &lt;code&gt;CriticalSection&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;. You may want to set this to &lt;code&gt;False&lt;/code&gt; if you will be accessing a resource in read-only mode in two different CriticalSections.</source>
          <target state="translated">Whether the resources required by &lt;code&gt;fn&lt;/code&gt; should be exclusive to this &lt;code&gt;CriticalSection&lt;/code&gt; . Default: &lt;code&gt;True&lt;/code&gt; . You may want to set this to &lt;code&gt;False&lt;/code&gt; if you will be accessing a resource in read-only mode in two different CriticalSections.</target>
        </trans-unit>
        <trans-unit id="6ca153ec1da7024d082dd6c5ca6490e0c9cc967f" translate="yes" xml:space="preserve">
          <source>Whether the scaling parameter of the layer should be trainable. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">Whether the scaling parameter of the layer should be trainable. Defaults to &lt;code&gt;False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f124179378e71744cfb3660434035ba0a965f11a" translate="yes" xml:space="preserve">
          <source>Whether the strategy uses between-graph replication or not.</source>
          <target state="translated">전략이 그래프 간 복제를 사용하는지 여부</target>
        </trans-unit>
        <trans-unit id="7b189cbeed35b5f550585a0be2719b6e1b80cbe7" translate="yes" xml:space="preserve">
          <source>Whether the threads should be marked as &lt;code&gt;daemons&lt;/code&gt;, meaning they don't block program exit.</source>
          <target state="translated">Whether the threads should be marked as &lt;code&gt;daemons&lt;/code&gt; , meaning they don't block program exit.</target>
        </trans-unit>
        <trans-unit id="ffa50da689f93e998f89852cb17efdd428d40e8f" translate="yes" xml:space="preserve">
          <source>Whether this is the last update for the progress bar. If &lt;code&gt;None&lt;/code&gt;, defaults to &lt;code&gt;current &amp;gt;= self.target&lt;/code&gt;.</source>
          <target state="translated">Whether this is the last update for the progress bar. If &lt;code&gt;None&lt;/code&gt; , defaults to &lt;code&gt;current &amp;gt;= self.target&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="844772b73edafce629bee3e7ae25983a4b946d09" translate="yes" xml:space="preserve">
          <source>Whether this layer supports computing a mask using &lt;code&gt;compute_mask&lt;/code&gt;.</source>
          <target state="translated">Whether this layer supports computing a mask using &lt;code&gt;compute_mask&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc8aa6f730bdacc1ba24ab5a192fd71d34266e12" translate="yes" xml:space="preserve">
          <source>Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.</source>
          <target state="translated">Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.</target>
        </trans-unit>
        <trans-unit id="25debb8437d8df1e4a47ad52d47cdffdeaed2fdb" translate="yes" xml:space="preserve">
          <source>Whether to add latency measurements on all edges. Defaults to False.</source>
          <target state="translated">모든 에지에서 대기 시간 측정을 추가할지 여부 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="ea1060a10c23d0b7356e53c3f78b04cde7c784c6" translate="yes" xml:space="preserve">
          <source>Whether to add python code trace information. Used to support &quot;code&quot; view.</source>
          <target state="translated">Whether to add python code trace information. Used to support &quot;code&quot; view.</target>
        </trans-unit>
        <trans-unit id="bd79c6c27494dd0eff20142fcddaa9263851ca92" translate="yes" xml:space="preserve">
          <source>Whether to allow the expansion in the non-concat dimensions. Defaulted to False.</source>
          <target state="translated">Whether to allow the expansion in the non-concat dimensions. Defaulted to False.</target>
        </trans-unit>
        <trans-unit id="60ffb7d2f619b0cd73858e4eae2a324f1c86a19d" translate="yes" xml:space="preserve">
          <source>Whether to apply decay in a discrete staircase, as opposed to continuous, fashion.</source>
          <target state="translated">Whether to apply decay in a discrete staircase, as opposed to continuous, fashion.</target>
        </trans-unit>
        <trans-unit id="98be7901e41c6f7d3da8e520ead11178c148bd11" translate="yes" xml:space="preserve">
          <source>Whether to apply default graph optimizations. If False, only graph optimizations that have been explicitly enabled will be applied.</source>
          <target state="translated">Whether to apply default graph optimizations. If False, only graph optimizations that have been explicitly enabled will be applied.</target>
        </trans-unit>
        <trans-unit id="04a904d718701877a2e0c411c4fe23b4c4e3dd17" translate="yes" xml:space="preserve">
          <source>Whether to apply default static optimizations. If False, only static optimizations that have been explicitly enabled will be applied.</source>
          <target state="translated">기본 정적 최적화 적용 여부입니다. False 인 경우 명시 적으로 활성화 된 정적 최적화 만 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="0dfe28cb36c9e4043cc3c258d3d2b5adbf5c6fcb" translate="yes" xml:space="preserve">
          <source>Whether to automatically tune performance knobs. If None, defaults to True.</source>
          <target state="translated">성능 노브를 자동으로 조정할지 여부 None이면 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="e49bbcda0833c641972e00b20f44fee93023536d" translate="yes" xml:space="preserve">
          <source>Whether to close the &lt;code&gt;summary_writer&lt;/code&gt;. Defaults to &lt;code&gt;True&lt;/code&gt; if the summary writer was created by the supervisor, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">Whether to close the &lt;code&gt;summary_writer&lt;/code&gt; . Defaults to &lt;code&gt;True&lt;/code&gt; if the summary writer was created by the supervisor, &lt;code&gt;False&lt;/code&gt; otherwise.</target>
        </trans-unit>
        <trans-unit id="48089f8ff308f6b2e1f3c7040fd63434bb0bf7a9" translate="yes" xml:space="preserve">
          <source>Whether to close the summary writer when closing the session. Defaults to True.</source>
          <target state="translated">Whether to close the summary writer when closing the session. Defaults to True.</target>
        </trans-unit>
        <trans-unit id="06ba131eb14ad989473a1a1d258716794efd0b30" translate="yes" xml:space="preserve">
          <source>Whether to convert the comparison operators, like equality. This is soon to be deprecated as support is being added to the Tensor class.</source>
          <target state="translated">Whether to convert the comparison operators, like equality. This is soon to be deprecated as support is being added to the Tensor class.</target>
        </trans-unit>
        <trans-unit id="e7f5fb7a744d7e22b32c2f61ab042329f9cf9f1e" translate="yes" xml:space="preserve">
          <source>Whether to eliminate no-op transformations. If None, defaults to True.</source>
          <target state="translated">무 변환 변환 제거 여부. None이면 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="55bb7493c6bec3c764e8919ffd2eed1794554953" translate="yes" xml:space="preserve">
          <source>Whether to enable JIT compilation.</source>
          <target state="translated">Whether to enable JIT compilation.</target>
        </trans-unit>
        <trans-unit id="8c98d9351c340a8a2394b518e7da0cf49c193b3a" translate="yes" xml:space="preserve">
          <source>Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter &lt;code&gt;node_def&lt;/code&gt; and returns a python bool.</source>
          <target state="translated">Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter &lt;code&gt;node_def&lt;/code&gt; and returns a python bool.</target>
        </trans-unit>
        <trans-unit id="f04c00e059b1defa60bb4eed8c8d8bfc3869da01" translate="yes" xml:space="preserve">
          <source>Whether to enable soft placement.</source>
          <target state="translated">Whether to enable soft placement.</target>
        </trans-unit>
        <trans-unit id="f9d095bc84ce17a933a7edf15d114454108ce476" translate="yes" xml:space="preserve">
          <source>Whether to enabled device placement logging.</source>
          <target state="translated">Whether to enabled device placement logging.</target>
        </trans-unit>
        <trans-unit id="fa1073b5ec09137db6c46e9a17f5e29476dc90c4" translate="yes" xml:space="preserve">
          <source>Whether to expand nested models into clusters.</source>
          <target state="translated">Whether to expand nested models into clusters.</target>
        </trans-unit>
        <trans-unit id="593fbed8bd96575db44e786eedbea4f61bf29dd7" translate="yes" xml:space="preserve">
          <source>Whether to follow symlinks inside class subdirectories (default: False).</source>
          <target state="translated">Whether to follow symlinks inside class subdirectories (default: False).</target>
        </trans-unit>
        <trans-unit id="2ccd7cdfaf0055cafb944adeab223f3187d39f6f" translate="yes" xml:space="preserve">
          <source>Whether to fuse filter dataset that predicts random_uniform &amp;lt; rate into a sampling dataset. If None, defaults to False.</source>
          <target state="translated">random_uniform &amp;lt;비율을 예측하는 필터 데이터 세트를 샘플링 데이터 세트에 통합할지 여부입니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="3d46b05c402499222a82d4000612dbe3e69e3543" translate="yes" xml:space="preserve">
          <source>Whether to fuse filter transformations. If None, defaults to False.</source>
          <target state="translated">필터 변환을 통합할지 여부 None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="14622b8d2b800d5545010933d2dce355f7878e20" translate="yes" xml:space="preserve">
          <source>Whether to fuse map and batch transformations. If None, defaults to True.</source>
          <target state="translated">맵 및 배치 변환을 통합할지 여부입니다. None이면 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="89d5416ec9ee3bbc766239f06ad6082f104072da" translate="yes" xml:space="preserve">
          <source>Whether to fuse map and filter transformations. If None, defaults to False.</source>
          <target state="translated">맵 및 필터 변환을 통합할지 여부입니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="ed238b5a1a92f1cdcc3146430339e440df04eeef" translate="yes" xml:space="preserve">
          <source>Whether to fuse map transformations. If None, defaults to False.</source>
          <target state="translated">맵 변환을 통합할지 여부입니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="5bf1515795584c1b97bc7e2ce679a96ff89a3700" translate="yes" xml:space="preserve">
          <source>Whether to fuse shuffle and repeat transformations. If None, defaults to True.</source>
          <target state="translated">셔플을 융합하고 변환을 반복할지 여부. None이면 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="5d8df6452e0ce74559d9d9e2cb01ae9313b462e1" translate="yes" xml:space="preserve">
          <source>Whether to hoist &lt;code&gt;tf.random_uniform()&lt;/code&gt; ops out of map transformations. If None, defaults to False.</source>
          <target state="translated">&lt;code&gt;tf.random_uniform()&lt;/code&gt; 을 호이스트할지 여부 는 맵 변환에서 제외됩니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="3d0ff5f97e1e7bc8d1a39a62cacd607bcbc343f6" translate="yes" xml:space="preserve">
          <source>Whether to include the constant &lt;code&gt;log(z!)&lt;/code&gt; term in computing the poisson loss. See &lt;a href=&quot;../nn/log_poisson_loss&quot;&gt;&lt;code&gt;tf.nn.log_poisson_loss&lt;/code&gt;&lt;/a&gt; for the full documentation.</source>
          <target state="translated">Whether to include the constant &lt;code&gt;log(z!)&lt;/code&gt; term in computing the poisson loss. See &lt;a href=&quot;../nn/log_poisson_loss&quot;&gt; &lt;code&gt;tf.nn.log_poisson_loss&lt;/code&gt; &lt;/a&gt; for the full documentation.</target>
        </trans-unit>
        <trans-unit id="95573f5b8478c4ff858958db70f9484eb3ff2864" translate="yes" xml:space="preserve">
          <source>Whether to include the fully-connected layer at the top of the network.</source>
          <target state="translated">Whether to include the fully-connected layer at the top of the network.</target>
        </trans-unit>
        <trans-unit id="1b6d4f7d016891ba7c3664590ae67883c943bcc2" translate="yes" xml:space="preserve">
          <source>Whether to include the fully-connected layer at the top of the network. Defaults to True.</source>
          <target state="translated">Whether to include the fully-connected layer at the top of the network. Defaults to True.</target>
        </trans-unit>
        <trans-unit id="a96a6b7996a34915e32caf67d322039a64480302" translate="yes" xml:space="preserve">
          <source>Whether to interpret &lt;code&gt;y_pred&lt;/code&gt; as a tensor of &lt;a href=&quot;https://en.wikipedia.org/wiki/Logit&quot;&gt;logit&lt;/a&gt; values. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; contains probabilities (i.e., values in [0, 1]). **Note - Using from_logits=True may be more numerically stable.</source>
          <target state="translated">Whether to interpret &lt;code&gt;y_pred&lt;/code&gt; as a tensor of &lt;a href=&quot;https://en.wikipedia.org/wiki/Logit&quot;&gt;logit&lt;/a&gt; values. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; contains probabilities (i.e., values in [0, 1]). **Note - Using from_logits=True may be more numerically stable.</target>
        </trans-unit>
        <trans-unit id="c54c4b54df643bc4abc643e55545e0c75e91b663" translate="yes" xml:space="preserve">
          <source>Whether to introduce 'slack' in the last &lt;code&gt;prefetch&lt;/code&gt; of the input pipeline, if it exists. This may reduce CPU contention with accelerator host-side activity at the start of a step. The slack frequency is determined by the number of devices attached to this input pipeline. If None, defaults to False.</source>
          <target state="translated">입력 파이프 라인 의 마지막 &lt;code&gt;prefetch&lt;/code&gt; 에 '슬랙'을 도입할지 여부 (있는 경우). 이는 단계 시작시 액셀러레이터 호스트 측 활동으로 CPU 경합을 줄일 수 있습니다. 슬랙 주파수는이 입력 파이프 라인에 연결된 장치의 수에 의해 결정됩니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="415f3117867c673985b61a262949466ab1564948" translate="yes" xml:space="preserve">
          <source>Whether to mark this name as being used.</source>
          <target state="translated">Whether to mark this name as being used.</target>
        </trans-unit>
        <trans-unit id="2d1aefded232219f79087eafa756226ded0158a7" translate="yes" xml:space="preserve">
          <source>Whether to only keep the model that has achieved the &quot;best performance&quot; so far, or whether to save the model at the end of every epoch regardless of performance.</source>
          <target state="translated">Whether to only keep the model that has achieved the &quot;best performance&quot; so far, or whether to save the model at the end of every epoch regardless of performance.</target>
        </trans-unit>
        <trans-unit id="914df09e28d1c64c2b3ad87baeeb42bb6e13c04f" translate="yes" xml:space="preserve">
          <source>Whether to output all intermediates from functional control flow ops.</source>
          <target state="translated">기능 제어 흐름 op에서 모든 중간체를 출력할지 여부.</target>
        </trans-unit>
        <trans-unit id="1dadddd88645b104b7d6bbaecaa2b1fda6cebd75" translate="yes" xml:space="preserve">
          <source>Whether to pad the end of &lt;code&gt;signal&lt;/code&gt; with &lt;code&gt;pad_value&lt;/code&gt;.</source>
          <target state="translated">Whether to pad the end of &lt;code&gt;signal&lt;/code&gt; with &lt;code&gt;pad_value&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b76369cf9f1df3341ba7b7b298f504b7086664d1" translate="yes" xml:space="preserve">
          <source>Whether to pad the end of &lt;code&gt;signals&lt;/code&gt; with zeros when the provided frame length and step produces a frame that lies partially past its end.</source>
          <target state="translated">Whether to pad the end of &lt;code&gt;signals&lt;/code&gt; with zeros when the provided frame length and step produces a frame that lies partially past its end.</target>
        </trans-unit>
        <trans-unit id="02821348292f56a84707e75ec632d05c62475235" translate="yes" xml:space="preserve">
          <source>Whether to parallelize copying of batch elements. If None, defaults to False.</source>
          <target state="translated">배치 요소 복사를 병렬화할지 여부 None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="735760deb2a9c67ef0c7efaa6a50691fc4a6f0e0" translate="yes" xml:space="preserve">
          <source>Whether to parallelize stateless map transformations. If None, defaults to False.</source>
          <target state="translated">상태 비 저장 맵 변환을 병렬화할지 여부 None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="8eda9b273c93d039ba7a242609f9446311332c6d" translate="yes" xml:space="preserve">
          <source>Whether to preserve the aspect ratio. If this is set, then &lt;code&gt;images&lt;/code&gt; will be resized to a size that fits in &lt;code&gt;size&lt;/code&gt; while preserving the aspect ratio of the original image. Scales up the image if &lt;code&gt;size&lt;/code&gt; is bigger than the current size of the &lt;code&gt;image&lt;/code&gt;. Defaults to False.</source>
          <target state="translated">Whether to preserve the aspect ratio. If this is set, then &lt;code&gt;images&lt;/code&gt; will be resized to a size that fits in &lt;code&gt;size&lt;/code&gt; while preserving the aspect ratio of the original image. Scales up the image if &lt;code&gt;size&lt;/code&gt; is bigger than the current size of the &lt;code&gt;image&lt;/code&gt; . Defaults to False.</target>
        </trans-unit>
        <trans-unit id="af633c59d5db9e3af600703e972fa65df59d21b3" translate="yes" xml:space="preserve">
          <source>Whether to recursively convert any functions that the converted function may call.</source>
          <target state="translated">Whether to recursively convert any functions that the converted function may call.</target>
        </trans-unit>
        <trans-unit id="331d800dc7aa1366aba63d94549fbc90867f575a" translate="yes" xml:space="preserve">
          <source>Whether to replace the C0 control characters &lt;code&gt;(U+0000 - U+001F)&lt;/code&gt; with the &lt;code&gt;replacement_char&lt;/code&gt;.</source>
          <target state="translated">Whether to replace the C0 control characters &lt;code&gt;(U+0000 - U+001F)&lt;/code&gt; with the &lt;code&gt;replacement_char&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1f08303e15a79462237d5a6f5ab69bc0d741f795" translate="yes" xml:space="preserve">
          <source>Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt;.</source>
          <target state="translated">Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d631ffee25c1c8d5962cb92c91bc7155c1b86dbc" translate="yes" xml:space="preserve">
          <source>Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt;. Defaults to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt; . Defaults to &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="af90abadf38a68efbae5eab044cc111e83bae73f" translate="yes" xml:space="preserve">
          <source>Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.</source>
          <target state="translated">Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.</target>
        </trans-unit>
        <trans-unit id="bf889dca2e1b92ac166e4e9099cdfdcbbe292012" translate="yes" xml:space="preserve">
          <source>Whether to save the GraphDef and MetaGraphDef to &lt;code&gt;checkpoint_dir&lt;/code&gt;. The GraphDef is saved after the session is created as &lt;code&gt;graph.pbtxt&lt;/code&gt;. MetaGraphDefs are saved out for every checkpoint as &lt;code&gt;model.ckpt-*.meta&lt;/code&gt;.</source>
          <target state="translated">Whether to save the GraphDef and MetaGraphDef to &lt;code&gt;checkpoint_dir&lt;/code&gt; . The GraphDef is saved after the session is created as &lt;code&gt;graph.pbtxt&lt;/code&gt; . MetaGraphDefs are saved out for every checkpoint as &lt;code&gt;model.ckpt-*.meta&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c75f3049cc336ca24bf2e733a248d74d9f9c0730" translate="yes" xml:space="preserve">
          <source>Whether to shuffle output samples, or instead draw them in chronological order.</source>
          <target state="translated">Whether to shuffle output samples, or instead draw them in chronological order.</target>
        </trans-unit>
        <trans-unit id="34ea722ba14f3f99938856c7d83cff00e23dae81" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.</source>
          <target state="translated">Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.</target>
        </trans-unit>
        <trans-unit id="248b22a81980263fa482dd9c28f486987117fdb6" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.</source>
          <target state="translated">Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.</target>
        </trans-unit>
        <trans-unit id="cbcae28d63feefddbed17928b77788494d83948d" translate="yes" xml:space="preserve">
          <source>Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt.</source>
          <target state="translated">Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt.</target>
        </trans-unit>
        <trans-unit id="22ee2250a0c89ccf48c596cca64f08347b735454" translate="yes" xml:space="preserve">
          <source>Whether to start the standard services and the queue runners.</source>
          <target state="translated">Whether to start the standard services and the queue runners.</target>
        </trans-unit>
        <trans-unit id="cfad559685c1e01540d88fa4658891788a35c214" translate="yes" xml:space="preserve">
          <source>Whether to start the standard services, such as checkpoint, summary and step counter.</source>
          <target state="translated">Whether to start the standard services, such as checkpoint, summary and step counter.</target>
        </trans-unit>
        <trans-unit id="a4197ca406a3d4336b3bdffbd1a95b8c81b6a0d1" translate="yes" xml:space="preserve">
          <source>Whether to store intermediate values needed for gradients on the CPU instead of GPU.</source>
          <target state="translated">Whether to store intermediate values needed for gradients on the CPU instead of GPU.</target>
        </trans-unit>
        <trans-unit id="f85068103e5372bea510e0ae9c69724c9d066b10" translate="yes" xml:space="preserve">
          <source>Whether to subtract &lt;code&gt;b&lt;/code&gt; from &lt;code&gt;a&lt;/code&gt;, vs vice versa.</source>
          <target state="translated">Whether to subtract &lt;code&gt;b&lt;/code&gt; from &lt;code&gt;a&lt;/code&gt; , vs vice versa.</target>
        </trans-unit>
        <trans-unit id="10f6a87284583be6f6880d0e9062e36a3a101464" translate="yes" xml:space="preserve">
          <source>Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</source>
          <target state="translated">Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</target>
        </trans-unit>
        <trans-unit id="72847fae2b2b235f6c8342af13371de425b6b16d" translate="yes" xml:space="preserve">
          <source>Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</source>
          <target state="translated">Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</target>
        </trans-unit>
        <trans-unit id="bcc78610692aa1b78a0b8fce45e7b1a35f60fedd" translate="yes" xml:space="preserve">
          <source>Whether to unroll the RNN or to use a symbolic &lt;code&gt;while_loop&lt;/code&gt;.</source>
          <target state="translated">Whether to unroll the RNN or to use a symbolic &lt;code&gt;while_loop&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8319c980207aacd2a41a4c25d0f4519649627a11" translate="yes" xml:space="preserve">
          <source>Whether to use &lt;a href=&quot;https://arxiv.org/abs/1702.03275&quot;&gt;Batch Renormalization&lt;/a&gt;. This adds extra variables during training. The inference is the same for either value of this parameter.</source>
          <target state="translated">Whether to use &lt;a href=&quot;https://arxiv.org/abs/1702.03275&quot;&gt;Batch Renormalization&lt;/a&gt;. This adds extra variables during training. The inference is the same for either value of this parameter.</target>
        </trans-unit>
        <trans-unit id="bd041b7a31c14b8f4c0154e4f0e31652d226ed84" translate="yes" xml:space="preserve">
          <source>Whether to use &lt;code&gt;ResourceVariable&lt;/code&gt;.</source>
          <target state="translated">Whether to use &lt;code&gt;ResourceVariable&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b32a3b0296e56c0857535ebb732043e2ca01eb48" translate="yes" xml:space="preserve">
          <source>Whether to use Batch Renormalization (Ioffe, 2017). This adds extra variables during training. The inference is the same for either value of this parameter.</source>
          <target state="translated">Whether to use Batch Renormalization (Ioffe, 2017). This adds extra variables during training. The inference is the same for either value of this parameter.</target>
        </trans-unit>
        <trans-unit id="b7ceb047bcc05fae84d262bcb0f0a4c42d80112e" translate="yes" xml:space="preserve">
          <source>Whether to use ChooseFastestBranchDataset with this transformation. If True, the pipeline picks between the vectorized and original segment at runtime based on their iterations speed. If None, defaults to False.</source>
          <target state="translated">이 변환에 ChooseFastestBranchDataset을 사용할지 여부입니다. True 인 경우 파이프 라인은 반복 속도에 따라 런타임시 벡터화 된 세그먼트와 원래 세그먼트 사이를 선택합니다. None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="b41a583b4e8ecad70d2b783981605d5fb07b5daa" translate="yes" xml:space="preserve">
          <source>Whether to use an anti-aliasing filter when downsampling an image.</source>
          <target state="translated">Whether to use an anti-aliasing filter when downsampling an image.</target>
        </trans-unit>
        <trans-unit id="2d7f954867443ea6a778475601ca4e3cc6e2dabd" translate="yes" xml:space="preserve">
          <source>Whether to use anti-aliasing when resizing. See 'image.resize()'.</source>
          <target state="translated">Whether to use anti-aliasing when resizing. See 'image.resize()'.</target>
        </trans-unit>
        <trans-unit id="c343c5190311c54955a31561f20b178e46089ae4" translate="yes" xml:space="preserve">
          <source>Whether to use autograph to compile python and eager style code for efficient graph-mode execution.</source>
          <target state="translated">Whether to use autograph to compile python and eager style code for efficient graph-mode execution.</target>
        </trans-unit>
        <trans-unit id="0ee94bf5c9ec84d6138e0ac95db29424ae3a5d6b" translate="yes" xml:space="preserve">
          <source>Whether to use batch normalization after each hidden layer.</source>
          <target state="translated">Whether to use batch normalization after each hidden layer.</target>
        </trans-unit>
        <trans-unit id="66b047600fb7cdf1794d2271af3d1f8a45f50c04" translate="yes" xml:space="preserve">
          <source>Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.</source>
          <target state="translated">Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="77a0de4187b80648dee2790bc3e0723da8ae8a3a" translate="yes" xml:space="preserve">
          <source>Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d4f7ae19a819559a645d06cbc4b8c196628f89ca" translate="yes" xml:space="preserve">
          <source>Whether to vectorize map transformations. If None, defaults to False.</source>
          <target state="translated">지도 변환을 벡터화할지 여부 None이면 기본값은 False입니다.</target>
        </trans-unit>
        <trans-unit id="f123cab967534816dc9b84ddbe1b892a217e9a8b" translate="yes" xml:space="preserve">
          <source>Whether to visits subdirectories pointed to by symlinks. Defaults to False.</source>
          <target state="translated">Whether to visits subdirectories pointed to by symlinks. Defaults to False.</target>
        </trans-unit>
        <trans-unit id="081ce8e1451357f99365d88262e4c58272efd31e" translate="yes" xml:space="preserve">
          <source>Whether to wait for checkpoint to become available.</source>
          <target state="translated">Whether to wait for checkpoint to become available.</target>
        </trans-unit>
        <trans-unit id="69219f557de4f53ff935206788ba0bcdb1a1ab26" translate="yes" xml:space="preserve">
          <source>Whether we should overwrite any existing model at the target location, or instead ask the user with a manual prompt.</source>
          <target state="translated">Whether we should overwrite any existing model at the target location, or instead ask the user with a manual prompt.</target>
        </trans-unit>
        <trans-unit id="63419d87417ca59c7ed9321136943312244206c5" translate="yes" xml:space="preserve">
          <source>Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</source>
          <target state="translated">Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</target>
        </trans-unit>
        <trans-unit id="bbe339fdd2ca55a9615f6086f8ab79036c1a5e5d" translate="yes" xml:space="preserve">
          <source>Whether you are running on your machine or in the cluster you can use the following values for the --master flag:</source>
          <target state="translated">머신에서 실행하든 클러스터에서 실행하든 --master 플래그에 다음 값을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3c232ac685e13a6b93f5f74209d44fc6f31c57d9" translate="yes" xml:space="preserve">
          <source>Which axis to join along. The default behavior is to join all elements, producing a scalar.</source>
          <target state="translated">Which axis to join along. The default behavior is to join all elements, producing a scalar.</target>
        </trans-unit>
        <trans-unit id="f517d8a1e3e3925fb5d9497fd0a8aca7ed0a886e" translate="yes" xml:space="preserve">
          <source>Which profile step to use for profiling.</source>
          <target state="translated">프로파일 링에 사용할 프로파일 단계.</target>
        </trans-unit>
        <trans-unit id="d86a5675cc6478e7030415aaa83cd72801292548" translate="yes" xml:space="preserve">
          <source>While</source>
          <target state="translated">While</target>
        </trans-unit>
        <trans-unit id="1c51ff24d93bea432325c091aee3e33a6598af2c" translate="yes" xml:space="preserve">
          <source>While &lt;a href=&quot;../../../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; save in the same format, note that the root of the resulting checkpoint is the object the save method is attached to. This means saving a &lt;a href=&quot;../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;save_weights&lt;/code&gt; and loading into a &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; with a &lt;code&gt;Model&lt;/code&gt; attached (or vice versa) will not match the &lt;code&gt;Model&lt;/code&gt;'s variables. See the &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;guide to training checkpoints&lt;/a&gt; for details. Prefer &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;a href=&quot;../../../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; for training checkpoints.</source>
          <target state="translated">&lt;a href=&quot;../../../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt; 는 동일한 형식으로 저장 되지만 결과 검사 점의 루트는 save 메소드가 첨부 된 오브젝트입니다. 즉, &lt;a href=&quot;../../../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 사용하여 &lt;code&gt;save_weights&lt;/code&gt; 저장하고 &lt;code&gt;Model&lt;/code&gt; 이 첨부 된 (또는 그 반대) &lt;a href=&quot;../../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 로 로드 하면 &lt;code&gt;Model&lt;/code&gt; 변수 와 일치하지 않습니다 . 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;교육 점검&lt;/a&gt; 사항 안내서를 참조 하십시오. 훈련 체크 포인트에 대해서는 &lt;a href=&quot;../../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 보다 &lt;a href=&quot;../../../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt; 를 선호 하십시오 .</target>
        </trans-unit>
        <trans-unit id="af5169034174d26fcbbcf3598b642bb6291299f0" translate="yes" xml:space="preserve">
          <source>While &lt;a href=&quot;../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; save in the same format, note that the root of the resulting checkpoint is the object the save method is attached to. This means saving a &lt;a href=&quot;../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;save_weights&lt;/code&gt; and loading into a &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; with a &lt;code&gt;Model&lt;/code&gt; attached (or vice versa) will not match the &lt;code&gt;Model&lt;/code&gt;'s variables. See the &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;guide to training checkpoints&lt;/a&gt; for details. Prefer &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;a href=&quot;../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; for training checkpoints.</source>
          <target state="translated">&lt;a href=&quot;../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt; 는 동일한 형식으로 저장 되지만 결과 검사 점의 루트는 save 메소드가 첨부 된 오브젝트입니다. 즉, &lt;a href=&quot;../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt; 사용하여 &lt;code&gt;save_weights&lt;/code&gt; 저장하고 &lt;code&gt;Model&lt;/code&gt; 이 첨부 된 (또는 그 반대) &lt;a href=&quot;checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 로 로드 하면 &lt;code&gt;Model&lt;/code&gt; 변수 와 일치하지 않습니다 . 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;교육 점검&lt;/a&gt; 사항 안내서를 참조 하십시오. 훈련 체크 포인트에 대해서는 &lt;a href=&quot;checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 보다 &lt;a href=&quot;../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt; 를 선호 하십시오 .</target>
        </trans-unit>
        <trans-unit id="df92ff2e348ec38c9d24bb6a65e3a867cbde6530" translate="yes" xml:space="preserve">
          <source>While &lt;code&gt;fn&lt;/code&gt; is running in the critical section, no other functions which wish to use this critical section may run.</source>
          <target state="translated">While &lt;code&gt;fn&lt;/code&gt; is running in the critical section, no other functions which wish to use this critical section may run.</target>
        </trans-unit>
        <trans-unit id="721826e30de58e9f292f39ea411e1bcf62df9289" translate="yes" xml:space="preserve">
          <source>While it is possible to use Variables with Lambda layers, this practice is discouraged as it can easily lead to bugs. For instance, consider the following layer:</source>
          <target state="translated">Lambda 레이어에 변수를 사용할 수는 있지만 버그로 쉽게 이어질 수 있으므로이 방법은 권장하지 않습니다. 예를 들어 다음 레이어를 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="b7ab51f21fc0c3654c10e148c98ad2c3c102cbcf" translate="yes" xml:space="preserve">
          <source>While the formats are the same, do not mix &lt;code&gt;save_weights&lt;/code&gt; and &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;. Checkpoints saved by &lt;a href=&quot;../model#save_weights&quot;&gt;&lt;code&gt;Model.save_weights&lt;/code&gt;&lt;/a&gt; should be loaded using &lt;a href=&quot;../model#load_weights&quot;&gt;&lt;code&gt;Model.load_weights&lt;/code&gt;&lt;/a&gt;. Checkpoints saved using &lt;a href=&quot;../../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; should be restored using the corresponding &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. Prefer &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;code&gt;save_weights&lt;/code&gt; for training checkpoints.</source>
          <target state="translated">형식은 동일하지만 &lt;code&gt;save_weights&lt;/code&gt; 와 &lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; 를&lt;/a&gt; 혼합하지 마십시오 . 구원 체크 포인트 &lt;a href=&quot;../model#save_weights&quot;&gt; &lt;code&gt;Model.save_weights&lt;/code&gt; 를&lt;/a&gt; 사용하여로드해야 &lt;a href=&quot;../model#load_weights&quot;&gt; &lt;code&gt;Model.load_weights&lt;/code&gt; 을&lt;/a&gt; . &lt;a href=&quot;../../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt; 를 사용하여 저장된 체크 포인트 는 해당 &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; 를&lt;/a&gt; 사용하여 복원해야합니다 . 훈련 체크 포인트에 대해 &lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 보다 &lt;code&gt;save_weights&lt;/code&gt; 를 선호 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4d3c899f37d2d7cda82e12e4a5abbb5d78c486a6" translate="yes" xml:space="preserve">
          <source>While the formats are the same, do not mix &lt;code&gt;save_weights&lt;/code&gt; and &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;. Checkpoints saved by &lt;a href=&quot;model#save_weights&quot;&gt;&lt;code&gt;Model.save_weights&lt;/code&gt;&lt;/a&gt; should be loaded using &lt;a href=&quot;model#load_weights&quot;&gt;&lt;code&gt;Model.load_weights&lt;/code&gt;&lt;/a&gt;. Checkpoints saved using &lt;a href=&quot;../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; should be restored using the corresponding &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. Prefer &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;code&gt;save_weights&lt;/code&gt; for training checkpoints.</source>
          <target state="translated">형식은 동일하지만 &lt;code&gt;save_weights&lt;/code&gt; 와 &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; 를&lt;/a&gt; 혼합하지 마십시오 . 구원 체크 포인트 &lt;a href=&quot;model#save_weights&quot;&gt; &lt;code&gt;Model.save_weights&lt;/code&gt; 를&lt;/a&gt; 사용하여로드해야 &lt;a href=&quot;model#load_weights&quot;&gt; &lt;code&gt;Model.load_weights&lt;/code&gt; 을&lt;/a&gt; . &lt;a href=&quot;../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt; 를 사용하여 저장된 체크 포인트 는 해당 &lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; 를&lt;/a&gt; 사용하여 복원해야합니다 . 훈련 체크 포인트에 대해 &lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt; 보다 &lt;code&gt;save_weights&lt;/code&gt; 를 선호 하십시오 .</target>
        </trans-unit>
        <trans-unit id="0b0e77019b80f4349b00c82de8efee158e5fc102" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm.</source>
          <target state="translated">While using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm.</target>
        </trans-unit>
        <trans-unit id="f64f547104eca8ffa08df75b0d127da08fb96ecc" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, the variables created within strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.</source>
          <target state="translated">분포 전략을 사용하는 동안 전략 범위 내에서 생성 된 변수는 모든 복제본에 복제되며 all-reduce 알고리즘을 사용하여 동기화 상태를 유지할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3d98a83dc2837ad96d068975c35687e2b85ed52c" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, the variables created within the strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.</source>
          <target state="translated">While using distribution strategies, the variables created within the strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.</target>
        </trans-unit>
        <trans-unit id="1d21e39cec1dbe57a37b90b0658bf80c4c7b4e18" translate="yes" xml:space="preserve">
          <source>WholeFileReader</source>
          <target state="translated">WholeFileReader</target>
        </trans-unit>
        <trans-unit id="c10e00cc1061d25c1e75d6a6c9f37b8de149d520" translate="yes" xml:space="preserve">
          <source>WholeFileReaderV2</source>
          <target state="translated">WholeFileReaderV2</target>
        </trans-unit>
        <trans-unit id="4bc1c4e835b1b69225a6dcb4af4e28a2c06f52c5" translate="yes" xml:space="preserve">
          <source>Wide &amp;amp; Deep Model for regression and classification problems.</source>
          <target state="translated">회귀 및 분류 문제를위한 Wide &amp;amp; Deep Model.</target>
        </trans-unit>
        <trans-unit id="ce79ee760dc1e0dcf6f5314aeb909e856d4d894b" translate="yes" xml:space="preserve">
          <source>Width Multiplier (alpha) | ImageNet Acc | Multiply-Adds (M) | Params (M)</source>
          <target state="translated">Width Multiplier (alpha) | ImageNet Acc | Multiply-Adds (M) | Params (M)</target>
        </trans-unit>
        <trans-unit id="681d52e4c1304a1ea24152480eaca88c3ee9ae33" translate="yes" xml:space="preserve">
          <source>Width of output image.</source>
          <target state="translated">Width of output image.</target>
        </trans-unit>
        <trans-unit id="0c58a6d122599315399444067e820708cd4baecf" translate="yes" xml:space="preserve">
          <source>Width of the result.</source>
          <target state="translated">Width of the result.</target>
        </trans-unit>
        <trans-unit id="664add438097fbd4307f814de8e62a10f8905588" translate="yes" xml:space="preserve">
          <source>Wikipedia</source>
          <target state="translated">Wikipedia</target>
        </trans-unit>
        <trans-unit id="f1e4a70a64ee6594b6f0a462b10bce93878f9218" translate="yes" xml:space="preserve">
          <source>Will NOT work in 2.x:</source>
          <target state="translated">2.x에서는 작동하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1c0b2946d9427a7aeee307da8f37197ad5dcc849" translate="yes" xml:space="preserve">
          <source>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</source>
          <target state="translated">필요한 경우 대기열에서 작업 단위를 대기열에서 제외시킵니다 (예 : 이전 파일로 완료된 후 Reader가 새 파일에서 읽기를 시작해야하는 경우).</target>
        </trans-unit>
        <trans-unit id="a8c3a8bad2b347c018d978650b0ad1f0a7153646" translate="yes" xml:space="preserve">
          <source>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</source>
          <target state="translated">필요한 경우 (예 : 이전 파일로 완료 한 후 Reader가 새 파일에서 읽기를 시작해야하는 경우) 대기열에서 작업 단위를 대기열에서 제외시킵니다. 마지막 배치 이전에도 num_records 미만을 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4ba78cccf43ec749a3b3f63b99ca41d71764e130" translate="yes" xml:space="preserve">
          <source>Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</source>
          <target state="translated">Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</target>
        </trans-unit>
        <trans-unit id="f6ac5cb110e37a221155d9f3ad0cb5d4f454d263" translate="yes" xml:space="preserve">
          <source>Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than &lt;code&gt;num_records&lt;/code&gt; even before the last batch.</source>
          <target state="translated">Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than &lt;code&gt;num_records&lt;/code&gt; even before the last batch.</target>
        </trans-unit>
        <trans-unit id="261f27a1f3bf63caf50878a078d18d17744de09c" translate="yes" xml:space="preserve">
          <source>Will make devices on the cluster available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices.</source>
          <target state="translated">클러스터의 장치를 사용할 수있게합니다. 이것을 두 번 이상 호출하면 작동하지만 이전 원격 장치의 텐서 핸들은 무효화됩니다.</target>
        </trans-unit>
        <trans-unit id="f6897db1eb6c38899798f94d9b426756d03b49e8" translate="yes" xml:space="preserve">
          <source>Will make devices on the remote host available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices.</source>
          <target state="translated">원격 호스트의 장치를 사용할 수있게합니다. 이것을 두 번 이상 호출하면 작동하지만 이전 원격 장치의 텐서 핸들은 무효화됩니다.</target>
        </trans-unit>
        <trans-unit id="b4a06c844340471315b54b0dfa65c9f440e60872" translate="yes" xml:space="preserve">
          <source>Will the SparseTensor &lt;code&gt;A&lt;/code&gt; fit in memory if densified?</source>
          <target state="translated">밀도가 높은 경우 SparseTensor &lt;code&gt;A&lt;/code&gt; 가 메모리에 맞습니까?</target>
        </trans-unit>
        <trans-unit id="0922711fb24b94894ef90c6a73b66b5cfaf62cb6" translate="yes" xml:space="preserve">
          <source>Will work in 1.x and 2.x (though deprecated in 2.x):</source>
          <target state="translated">1.x 및 2.x에서 작동합니다 (2.x에서는 더 이상 사용되지 않음).</target>
        </trans-unit>
        <trans-unit id="ce21a08c1a63e9b77f2a1c614615c252ca550863" translate="yes" xml:space="preserve">
          <source>WindowDataset</source>
          <target state="translated">WindowDataset</target>
        </trans-unit>
        <trans-unit id="7969fd6214a6335899a24dac6e8f7bd3aba9e39c" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;A&lt;/code&gt; the dense representation of this &lt;code&gt;Operator&lt;/code&gt;,</source>
          <target state="translated">와 이의 밀도 표현 &lt;code&gt;Operator&lt;/code&gt; , &lt;code&gt;A&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ff9e0aefc629ac53bd25ea34ae417e2aeaf0766f" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; where n is:</source>
          <target state="translated">With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt; , samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; where n is:</target>
        </trans-unit>
        <trans-unit id="234aca2d96fedfdf359061dabe09edd1bc5db7b3" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; where n is: - number of input units in the weight tensor, if mode = &quot;fan_in&quot; - number of output units, if mode = &quot;fan_out&quot; - average of the numbers of input and output units, if mode = &quot;fan_avg&quot;</source>
          <target state="translated">로 &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt; , 샘플 (절단 후, 만약 사용) 0의 평균 및 표준 편차 절두 / untruncated 정규 분포로부터 그려 &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; 여기서, n은 : - mode = &quot;fan_in&quot;인 경우 가중치 텐서의 입력 단위 수-mode = &quot;fan_out&quot;인 경우 출력 단위 수-mode = &quot;fan_avg&quot;인 경우 입력 및 출력 단위 수의 평균</target>
        </trans-unit>
        <trans-unit id="4f723d75a7c2fa1bd113ada83e2655f70f9292fe" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is:</source>
          <target state="translated">With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt; , samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; , where &lt;code&gt;n&lt;/code&gt; is:</target>
        </trans-unit>
        <trans-unit id="8ff32194c2a7d6162d325ca83d7afa7acc313404" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt;, samples are drawn from a uniform distribution within &lt;code&gt;[-limit, limit]&lt;/code&gt;, where &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt;.</source>
          <target state="translated">With &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt; , samples are drawn from a uniform distribution within &lt;code&gt;[-limit, limit]&lt;/code&gt; , where &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f5835e02899f12405b436a06a34c5acc97b07515" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt;, samples are drawn from a uniform distribution within [-limit, limit], with &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt;.</source>
          <target state="translated">로 &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt; , 시료와, [-limit, 한계] 내에서 균일 한 분포로부터 그려 &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f8c1f4cf8df7060458ea559fb96747e733cc6916" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;height_shift_range=2&lt;/code&gt; possible values are integers &lt;code&gt;[-1, 0, +1]&lt;/code&gt;, same as with &lt;code&gt;height_shift_range=[-1, 0, +1]&lt;/code&gt;, while with &lt;code&gt;height_shift_range=1.0&lt;/code&gt; possible values are floats in the interval [-1.0, +1.0).</source>
          <target state="translated">함께 &lt;code&gt;height_shift_range=2&lt;/code&gt; 가지 값은 정수 &lt;code&gt;[-1, 0, +1]&lt;/code&gt; 과 동일 &lt;code&gt;height_shift_range=[-1, 0, +1]&lt;/code&gt; 로하면서 &lt;code&gt;height_shift_range=1.0&lt;/code&gt; 가능한 값은 구간 [-1.0 수레이다 + 1.0).</target>
        </trans-unit>
        <trans-unit id="8d71d2aae030803f505395187c99c592d13d4623" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;preserve_aspect_ratio=True&lt;/code&gt;, the aspect ratio is preserved, so &lt;code&gt;size&lt;/code&gt; is the maximum for each dimension:</source>
          <target state="translated">With &lt;code&gt;preserve_aspect_ratio=True&lt;/code&gt; , the aspect ratio is preserved, so &lt;code&gt;size&lt;/code&gt; is the maximum for each dimension:</target>
        </trans-unit>
        <trans-unit id="2f0be93f34340dbd9dbce5872c897c11744e5be0" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;width_shift_range=2&lt;/code&gt; possible values are integers &lt;code&gt;[-1, 0, +1]&lt;/code&gt;, same as with &lt;code&gt;width_shift_range=[-1, 0, +1]&lt;/code&gt;, while with &lt;code&gt;width_shift_range=1.0&lt;/code&gt; possible values are floats in the interval [-1.0, +1.0).</source>
          <target state="translated">함께 &lt;code&gt;width_shift_range=2&lt;/code&gt; 개 가지 값은 정수 &lt;code&gt;[-1, 0, +1]&lt;/code&gt; 동일와 같이, &lt;code&gt;width_shift_range=[-1, 0, +1]&lt;/code&gt; 함께 동시에 &lt;code&gt;width_shift_range=1.0&lt;/code&gt; 가능한 값 수레 구간 [-1.0에 +, 1.0).</target>
        </trans-unit>
        <trans-unit id="b3c5ce075d1d1fd6a0d1aad83b3d8c72b5bbfb7b" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the first dimension, which is &lt;code&gt;height&lt;/code&gt;. Otherwise output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="translated">1 in 2 기회 와 함께 &lt;code&gt;height&lt;/code&gt; 의 첫 번째 차원을 따라 뒤집힌 &lt;code&gt;image&lt;/code&gt; 의 내용을 출력합니다 . 그렇지 않으면 이미지를있는 그대로 출력하십시오. 배치 이미지를 전달할 때 각 이미지는 다른 이미지와 독립적으로 임의로 뒤집 힙니다.</target>
        </trans-unit>
        <trans-unit id="40d888cff933beb09db3d4f6596d9067e2878308" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the first dimension, which is &lt;code&gt;height&lt;/code&gt;. Otherwise, output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="translated">With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the first dimension, which is &lt;code&gt;height&lt;/code&gt; . Otherwise, output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</target>
        </trans-unit>
        <trans-unit id="dafbf9ee96525bc588fdd1a819189c2c04be4c99" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the second dimension, which is &lt;code&gt;width&lt;/code&gt;. Otherwise output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="translated">1 in 2 기회 와 함께 &lt;code&gt;width&lt;/code&gt; 의 두 번째 차원을 따라 뒤집힌 &lt;code&gt;image&lt;/code&gt; 의 내용을 출력합니다 . 그렇지 않으면 이미지를있는 그대로 출력하십시오. 배치 이미지를 전달할 때 각 이미지는 다른 이미지와 독립적으로 임의로 뒤집 힙니다.</target>
        </trans-unit>
        <trans-unit id="eb0883b4bf2a7901f51949844c31237c43d86ecb" translate="yes" xml:space="preserve">
          <source>With a &lt;code&gt;Coordinator&lt;/code&gt;, exceptions are reported to the coordinator and forgotten by the &lt;code&gt;QueueRunner&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;Coordinator&lt;/code&gt; 사용하면 예외가 코디네이터 에게보고되고 &lt;code&gt;QueueRunner&lt;/code&gt; 에 의해 잊혀 집니다 .</target>
        </trans-unit>
        <trans-unit id="bf97c0b53fe731e0899e5aa3d7d1c5c2588e5d9e" translate="yes" xml:space="preserve">
          <source>With default values, it returns element-wise &lt;code&gt;max(x, 0)&lt;/code&gt;.</source>
          <target state="translated">기본값을 사용하면 요소 별 &lt;code&gt;max(x, 0)&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="db85e495eb55636b020a2a32657d103899296f2f" translate="yes" xml:space="preserve">
          <source>With default values, this returns the standard ReLU activation: &lt;code&gt;max(x, 0)&lt;/code&gt;, the element-wise maximum of 0 and the input tensor.</source>
          <target state="translated">기본값을 사용하면 표준 ReLU 활성화 &lt;code&gt;max(x, 0)&lt;/code&gt; , 요소 별 최대 값 0 및 입력 텐서가 리턴 됩니다.</target>
        </trans-unit>
        <trans-unit id="8620f83bce4d323fd62431b72a3e3cd320c09979" translate="yes" xml:space="preserve">
          <source>With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:</source>
          <target state="translated">With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:</target>
        </trans-unit>
        <trans-unit id="e7517aaf4d18e2c4d14a26fcfe70e076851f68b1" translate="yes" xml:space="preserve">
          <source>With eager execution this is a shape assertion, that returns the input:</source>
          <target state="translated">With eager execution this is a shape assertion, that returns the input:</target>
        </trans-unit>
        <trans-unit id="f568c7dfa8797dfe677f7dd6f0a4ee1b0b5cbdba" translate="yes" xml:space="preserve">
          <source>With eager execution this operates as a shape assertion. Here the shapes match:</source>
          <target state="translated">With eager execution this operates as a shape assertion. Here the shapes match:</target>
        </trans-unit>
        <trans-unit id="8b2addb5c48d67261f0b9d05eab9810322d19c59" translate="yes" xml:space="preserve">
          <source>With forwardprop, we specify a length-three vector in advance which multiplies the Jacobian. The &lt;code&gt;primals&lt;/code&gt; constructor argument is the parameter (a &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt;) we're specifying a vector for, and the &lt;code&gt;tangents&lt;/code&gt; argument is the &quot;vector&quot; in Jacobian-vector product. If our goal is to compute the entire Jacobian matrix, forwardprop computes one column at a time while backprop computes one row at a time. Since the Jacobian in the linear regression example has only one row, backprop requires fewer invocations:</source>
          <target state="translated">forwardprop를 사용하면 Jacobian을 곱하는 길이 3의 벡터를 미리 지정합니다. &lt;code&gt;primals&lt;/code&gt; 의 생성자 인수는 매개 변수 (A입니다 &lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt; ) 우리가 벡터를 지정하고, 그리고 &lt;code&gt;tangents&lt;/code&gt; 인수가 코비 - 벡터 제품에서 &quot;벡터&quot;입니다. 우리의 목표가 전체 Jacobian 행렬을 계산하는 것이라면, forwardprop은 한 번에 하나의 열을 계산하는 반면 backprop는 한 번에 하나의 행을 계산합니다. 선형 회귀 예제의 Jacobian에는 행이 하나만 있으므로 backprop은 더 적은 호출을 요구합니다.</target>
        </trans-unit>
        <trans-unit id="81932d1ace29ea95c92392af364f6a28c2c74781" translate="yes" xml:space="preserve">
          <source>With this definition, the gradient at x=100 will be correctly evaluated as 1.0.</source>
          <target state="translated">이 정의를 사용하면 x = 100의 기울기가 1.0으로 올바르게 평가됩니다.</target>
        </trans-unit>
        <trans-unit id="317a1dfdd7bd7e17378c7a66cabfcee42b3896df" translate="yes" xml:space="preserve">
          <source>With y = f(x), computes the theoretical and numeric Jacobian dy/dx.</source>
          <target state="translated">y = f (x)를 사용하면 이론 및 숫자 Jacobian dy / dx를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="659abe1496bd7876fc022d3e2b6b9cead33d30d8" translate="yes" xml:space="preserve">
          <source>Within a particular block, exactly one of these two things will be true:</source>
          <target state="translated">특정 블록 내에서이 두 가지 중 정확히 하나가 맞습니다.</target>
        </trans-unit>
        <trans-unit id="13fe9d84fb92bd3745e88227dc1de58a44afddd1" translate="yes" xml:space="preserve">
          <source>Within a training loop, this argument sets how often host calls are performed during training. Host calls will be evaluated every n steps within a training loop where n is the value of this argument.</source>
          <target state="translated">Within a training loop, this argument sets how often host calls are performed during training. Host calls will be evaluated every n steps within a training loop where n is the value of this argument.</target>
        </trans-unit>
        <trans-unit id="1e7aeeddf89020677af5b4472b077bda01df8909" translate="yes" xml:space="preserve">
          <source>Within each worker, we will also split the data among all the worker devices (if more than one a present), and this will happen even if multi-worker sharding is disabled using the method above.</source>
          <target state="translated">각 작업자 내에서 데이터를 모든 작업자 장치 (하나 이상 존재하는 경우)간에 분할 할 수 있으며, 위의 방법을 사용하여 여러 작업자 샤딩이 비활성화 된 경우에도 발생합니다.</target>
        </trans-unit>
        <trans-unit id="4d99119d2f84477c4512754d5e0fcbfbe0f69b29" translate="yes" xml:space="preserve">
          <source>Within the &lt;code&gt;with sv.managed_session()&lt;/code&gt; block all variables in the graph have been initialized. In addition, a few services have been started to checkpoint the model and add summaries to the event log.</source>
          <target state="translated">내 &lt;code&gt;with sv.managed_session()&lt;/code&gt; 그래프의 모든 변수가 초기화 된 블록. 또한, 모델을 체크 포인트하고 요약을 이벤트 로그에 추가하기 위해 몇 가지 서비스가 시작되었습니다.</target>
        </trans-unit>
        <trans-unit id="ab6e550ba7dbaa5e409cc2c91f32bbe98623ef14" translate="yes" xml:space="preserve">
          <source>Without &lt;a href=&quot;set_seed&quot;&gt;&lt;code&gt;tf.random.set_seed&lt;/code&gt;&lt;/a&gt; but with a &lt;code&gt;seed&lt;/code&gt; argument is specified, small changes to function graphs or previously executed operations will change the returned value. See &lt;a href=&quot;set_seed&quot;&gt;&lt;code&gt;tf.random.set_seed&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">않고 &lt;a href=&quot;set_seed&quot;&gt; &lt;code&gt;tf.random.set_seed&lt;/code&gt; &lt;/a&gt; 로모그래퍼하지만 &lt;code&gt;seed&lt;/code&gt; 인수 지정된 함수 그래프 또는 이전에 실행 된 조작에 작은 변화는 리턴 값을 변경한다. 자세한 내용은 &lt;a href=&quot;set_seed&quot;&gt; &lt;code&gt;tf.random.set_seed&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c8ccb2ad531bd87f8226e54b9b6a39a6b2307357" translate="yes" xml:space="preserve">
          <source>Without a &lt;code&gt;Coordinator&lt;/code&gt;, exceptions are captured by the &lt;code&gt;QueueRunner&lt;/code&gt; and made available in this &lt;code&gt;exceptions_raised&lt;/code&gt; property.</source>
          <target state="translated">&lt;code&gt;Coordinator&lt;/code&gt; 가 없으면 &lt;code&gt;QueueRunner&lt;/code&gt; 가 예외를 캡처 하여이 &lt;code&gt;exceptions_raised&lt;/code&gt; 특성 에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e696e263864301b6aa5902a80a4d0bff70cd1786" translate="yes" xml:space="preserve">
          <source>Word embeddings</source>
          <target state="translated">단어 임베딩</target>
        </trans-unit>
        <trans-unit id="d98a4a13cdc22212a308bf1d89d6b7b049c03e2f" translate="yes" xml:space="preserve">
          <source>Worker devices vs. parameter devices: Most replica computations will happen on worker devices. Since we don't yet support model parallelism, there will be one worker device per replica. When using parameter servers or central storage, the set of devices holding variables may be different, otherwise the parameter devices might match the worker devices.</source>
          <target state="translated">작업자 장치와 매개 변수 장치 : 대부분의 복제 계산은 작업자 장치에서 수행됩니다. 아직 모델 병렬 처리를 지원하지 않기 때문에 복제 본당 하나의 작업자 장치가 있습니다. 매개 변수 서버 또는 중앙 스토리지를 사용할 때 변수를 보유하는 장치 세트가 다를 수 있습니다. 그렇지 않으면 매개 변수 장치가 작업자 장치와 일치 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e84ec097b7a7143f51f1fbc334ced683171b4c6c" translate="yes" xml:space="preserve">
          <source>Worker heartbeat op.</source>
          <target state="translated">Worker heartbeat op.</target>
        </trans-unit>
        <trans-unit id="d20a3ffc32dcd238f452807ec295544fb73ee86b" translate="yes" xml:space="preserve">
          <source>WorkerHeartbeat</source>
          <target state="translated">WorkerHeartbeat</target>
        </trans-unit>
        <trans-unit id="398f69182b10972e971055c07ca464bed84b735b" translate="yes" xml:space="preserve">
          <source>Working with Bounding Boxes</source>
          <target state="translated">Working with Bounding Boxes</target>
        </trans-unit>
        <trans-unit id="51eeae9fe36f031b9bbc4e7ce891844c997b1b08" translate="yes" xml:space="preserve">
          <source>WrapDatasetVariant</source>
          <target state="translated">WrapDatasetVariant</target>
        </trans-unit>
        <trans-unit id="7a41e51968848ca6ebbc48f555f3248a673bee90" translate="yes" xml:space="preserve">
          <source>Wrapped inputs (identity standins that have additional metadata). These are also are also tf.Tensor's.</source>
          <target state="translated">랩핑 된 입력 (추가 메타 데이터가있는 ID standin). 이것들은 또한 tf.Tensor입니다.</target>
        </trans-unit>
        <trans-unit id="384f837341c0fc0fa71de4eb8e929b11e7daf27f" translate="yes" xml:space="preserve">
          <source>Wrapped outputs (identity standins that have additional metadata). These are also tf.Tensor's.</source>
          <target state="translated">랩핑 된 출력 (추가 메타 데이터가있는 ID standin). 이것들은 또한 tf.Tensor입니다.</target>
        </trans-unit>
        <trans-unit id="a298562930ec81f39bad36c204ae1b1059782591" translate="yes" xml:space="preserve">
          <source>Wrapped values: In order to represent values parallel across devices (either replicas or the devices associated with a particular value), we wrap them in a &quot;PerReplica&quot; or &quot;Mirrored&quot; object that contains a map from replica id to values. &quot;PerReplica&quot; is used when the value may be different across replicas, and &quot;Mirrored&quot; when the value are the same.</source>
          <target state="translated">랩핑 된 값 : 장치 (복제본 또는 특정 값과 연관된 장치)에서 병렬로 값을 나타 내기 위해 복제본 ID에서 값으로의 맵을 포함하는 &quot;PerReplica&quot;또는 &quot;Mirrored&quot;객체로 래핑합니다. &quot;복제본&quot;은 값이 복제본마다 다를 수있는 경우에 사용되고 값이 같은 경우 &quot;미러 됨&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="890d066e219e9b18bb1c24edca8341903adbf397" translate="yes" xml:space="preserve">
          <source>Wrapper allowing a stack of RNN cells to behave as a single cell.</source>
          <target state="translated">RNN 셀 스택이 단일 셀처럼 작동하도록하는 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="b4013106a25f53b1f217b9c3881270bfb6839489" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#add_to_collection&quot;&gt;&lt;code&gt;Graph.add_to_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#add_to_collection&quot;&gt; &lt;code&gt;Graph.add_to_collection()&lt;/code&gt; &lt;/a&gt; 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="caa4198da3fc275ac76081f0b3d1bfe03cdd54fe" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#add_to_collections&quot;&gt;&lt;code&gt;Graph.add_to_collections()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#add_to_collections&quot;&gt; &lt;code&gt;Graph.add_to_collections()&lt;/code&gt; &lt;/a&gt; 래퍼 .</target>
        </trans-unit>
        <trans-unit id="264584356d25721fe348ac95a81a23f4cca709f4" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#container&quot;&gt;&lt;code&gt;Graph.container()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#container&quot;&gt; &lt;code&gt;Graph.container()&lt;/code&gt; &lt;/a&gt; 용 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="55f30bd6389eac4a761d1daf54731878db305f29" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#device&quot;&gt;&lt;code&gt;Graph.device()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#device&quot;&gt; &lt;code&gt;Graph.device()&lt;/code&gt; &lt;/a&gt; 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="0d1e4b32d773cedbb0330530a2382ac0bddfaba8" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#get_collection&quot;&gt;&lt;code&gt;Graph.get_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#get_collection&quot;&gt; &lt;code&gt;Graph.get_collection()&lt;/code&gt; &lt;/a&gt; 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="190d7f80a83427a732312702b0799aa4fd4270f6" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#get_collection_ref&quot;&gt;&lt;code&gt;Graph.get_collection_ref()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;../../graph#get_collection_ref&quot;&gt; &lt;code&gt;Graph.get_collection_ref()&lt;/code&gt; &lt;/a&gt; 래퍼 .</target>
        </trans-unit>
        <trans-unit id="89aa6df88d5fbd573c88df4510194a5615a938d5" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;graph#control_dependencies&quot;&gt;&lt;code&gt;Graph.control_dependencies()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">기본 그래프를 사용하는 &lt;a href=&quot;graph#control_dependencies&quot;&gt; &lt;code&gt;Graph.control_dependencies()&lt;/code&gt; &lt;/a&gt; 래퍼 .</target>
        </trans-unit>
        <trans-unit id="495b2dd43c40b93e9963a63ddff35ea9979905aa" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collection&quot;&gt;&lt;code&gt;Graph.add_to_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collection&quot;&gt; &lt;code&gt;Graph.add_to_collection()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="d79bebe0b09c5426685f0e539708d5d444124cc5" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collections&quot;&gt;&lt;code&gt;Graph.add_to_collections()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collections&quot;&gt; &lt;code&gt;Graph.add_to_collections()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="128db28adf836bfc21dd5fc549881d12855001a9" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#container&quot;&gt;&lt;code&gt;Graph.container()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#container&quot;&gt; &lt;code&gt;Graph.container()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="189fdde0fe51377c9c7df91b00d623f673fa3f34" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#device&quot;&gt;&lt;code&gt;Graph.device()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#device&quot;&gt; &lt;code&gt;Graph.device()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="04af0f377456628e691bf3f85dc843ed001f5c1b" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection&quot;&gt;&lt;code&gt;Graph.get_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection&quot;&gt; &lt;code&gt;Graph.get_collection()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="bed252c3eb66e3bb7424c1c2a3122d3cfea4dfb8" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection_ref&quot;&gt;&lt;code&gt;Graph.get_collection_ref()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection_ref&quot;&gt; &lt;code&gt;Graph.get_collection_ref()&lt;/code&gt; &lt;/a&gt; using the default graph.</target>
        </trans-unit>
        <trans-unit id="2f37a810bede5b1390f81fea4b6be44aec37fbe5" translate="yes" xml:space="preserve">
          <source>Wrapper for using the Scikit-Learn API with Keras models.</source>
          <target state="translated">Keras 모델과 함께 Scikit-Learn API를 사용하기위한 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="1fff217e51e30156f66782f095344b6ef97a4794" translate="yes" xml:space="preserve">
          <source>Wrappers for primitive Neural Net (NN) Operations.</source>
          <target state="translated">원시 신경망 (NN) 작업을위한 래퍼.</target>
        </trans-unit>
        <trans-unit id="7837b0cc643e2f001702979a842efc255ca9da70" translate="yes" xml:space="preserve">
          <source>Wrappers take another layer and augment it in various ways. Do not use this class as a layer, it is only an abstract base class. Two usable wrappers are the &lt;code&gt;TimeDistributed&lt;/code&gt; and &lt;code&gt;Bidirectional&lt;/code&gt; wrappers.</source>
          <target state="translated">래퍼는 다른 층을 취해 다양한 방식으로 보강합니다. 이 클래스를 레이어로 사용하지 마십시오.이 클래스는 추상 기본 클래스입니다. 사용 가능한 두 래퍼는 &lt;code&gt;TimeDistributed&lt;/code&gt; 및 &lt;code&gt;Bidirectional&lt;/code&gt; 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="3381242ca6a26b413c24d45326c44a72bdf78124" translate="yes" xml:space="preserve">
          <source>Wraps &lt;code&gt;call&lt;/code&gt;, applying pre- and post-processing steps.</source>
          <target state="translated">사전 처리 및 사후 처리 단계를 적용하여 &lt;code&gt;call&lt;/code&gt; 래핑 합니다.</target>
        </trans-unit>
        <trans-unit id="c62cdbd485fcae9d06ca69b5b8397cfed846cb17" translate="yes" xml:space="preserve">
          <source>Wraps a given text to a maximum line length and returns it.</source>
          <target state="translated">주어진 텍스트를 최대 줄 길이로 감싸서 반환합니다.</target>
        </trans-unit>
        <trans-unit id="8829b71bb0967a49348c7e7adf87bb1886403153" translate="yes" xml:space="preserve">
          <source>Wraps a python function and uses it as a TensorFlow op.</source>
          <target state="translated">파이썬 함수를 감싸서 TensorFlow op로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="8abaa30206e6292e5e70829b82947753e40ac335" translate="yes" xml:space="preserve">
          <source>Wraps a python function into a TensorFlow op that executes it eagerly.</source>
          <target state="translated">파이썬 함수를 TensorFlow op에 래핑하여 열심히 실행합니다.</target>
        </trans-unit>
        <trans-unit id="82ab6329b627d91bd83e44d4fe219ae9ac70441c" translate="yes" xml:space="preserve">
          <source>Wraps a value that may/may not be present at runtime.</source>
          <target state="translated">런타임에있을 수도 있고 없을 수도있는 값을 래핑합니다.</target>
        </trans-unit>
        <trans-unit id="19c9eb1de0862700ff8502bf5e2ae450855f9133" translate="yes" xml:space="preserve">
          <source>Wraps arbitrary expressions as a &lt;code&gt;Layer&lt;/code&gt; object.</source>
          <target state="translated">임의의 표현식을 &lt;code&gt;Layer&lt;/code&gt; 객체 로 래핑 합니다.</target>
        </trans-unit>
        <trans-unit id="352659d98f226f42ae5f9c0345757253d7e85e6e" translate="yes" xml:space="preserve">
          <source>Wraps the TF 1.x function fn into a graph function.</source>
          <target state="translated">TF 1.x 함수 fn을 그래프 함수로 래핑합니다.</target>
        </trans-unit>
        <trans-unit id="20b643b52957c38a95449d4001fccba2e0bcd12a" translate="yes" xml:space="preserve">
          <source>Write &lt;code&gt;value&lt;/code&gt; into index &lt;code&gt;index&lt;/code&gt; of the TensorArray.</source>
          <target state="translated">쓰기 &lt;code&gt;value&lt;/code&gt; 인덱스로 &lt;code&gt;index&lt;/code&gt; TensorArray의.</target>
        </trans-unit>
        <trans-unit id="483cdae75a267d35fd6e83b5653511d5a80cff0f" translate="yes" xml:space="preserve">
          <source>Write a customized optimizer.</source>
          <target state="translated">맞춤형 최적화 도구를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="c8c630765322886f9847c22284b5fa8d2851f2ce" translate="yes" xml:space="preserve">
          <source>Write a histogram summary.</source>
          <target state="translated">히스토그램 요약을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="b0075d853115d3caa9b112367f26a9151c343826" translate="yes" xml:space="preserve">
          <source>Write a scalar summary.</source>
          <target state="translated">스칼라 요약을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="76fe5c36d30ae305e5a6503ba46502eb4a13331d" translate="yes" xml:space="preserve">
          <source>Write a string record to the file.</source>
          <target state="translated">파일에 문자열 레코드를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="5b53b6d51e993b431470b37c217c59753ff1cc50" translate="yes" xml:space="preserve">
          <source>Write a text summary.</source>
          <target state="translated">텍스트 요약을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="d21f1a898ccfeda0fe2e731850a28cfafae9bb2b" translate="yes" xml:space="preserve">
          <source>Write an audio summary.</source>
          <target state="translated">오디오 요약을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="21a6eeb1ed26a2be90d3ee27390e8e2bda8c0962" translate="yes" xml:space="preserve">
          <source>Write an image summary.</source>
          <target state="translated">이미지 요약을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="61f330bc93ccdaf24823ff722556ec64642374a9" translate="yes" xml:space="preserve">
          <source>Write data via Write and read via Read or Pack.</source>
          <target state="translated">Write data via Write and read via Read or Pack.</target>
        </trans-unit>
        <trans-unit id="0b36ad4c95f8d96567af24f9c3edf891e090ad84" translate="yes" xml:space="preserve">
          <source>Write the serialized data to one or more files</source>
          <target state="translated">Write the serialized data to one or more files</target>
        </trans-unit>
        <trans-unit id="2a6db3a59303fe109b5b5137b804a970c912e147" translate="yes" xml:space="preserve">
          <source>Write this:</source>
          <target state="translated">이것을 쓰십시오 :</target>
        </trans-unit>
        <trans-unit id="6226f3d4ff16e7406bb0cdaedd7746abc5236636" translate="yes" xml:space="preserve">
          <source>WriteAudioSummary</source>
          <target state="translated">WriteAudioSummary</target>
        </trans-unit>
        <trans-unit id="94dee2fafe1d10d5ce6db659eb0c0fcb3d5842be" translate="yes" xml:space="preserve">
          <source>WriteFile</source>
          <target state="translated">WriteFile</target>
        </trans-unit>
        <trans-unit id="1ea63605e83eb910f0b5d75fe28edb99127b24d1" translate="yes" xml:space="preserve">
          <source>WriteGraphSummary</source>
          <target state="translated">WriteGraphSummary</target>
        </trans-unit>
        <trans-unit id="e5637c5d46d26a2b2c82fac03d3b096952d4bf28" translate="yes" xml:space="preserve">
          <source>WriteHistogramSummary</source>
          <target state="translated">WriteHistogramSummary</target>
        </trans-unit>
        <trans-unit id="6b51491885b3d25ccb268be4c798a6b10e3d5b9d" translate="yes" xml:space="preserve">
          <source>WriteImageSummary</source>
          <target state="translated">WriteImageSummary</target>
        </trans-unit>
        <trans-unit id="4a572e40863e9455f5578aec02c3c8842448412b" translate="yes" xml:space="preserve">
          <source>WriteRawProtoSummary</source>
          <target state="translated">WriteRawProtoSummary</target>
        </trans-unit>
        <trans-unit id="88f81acb7e965e0b6b73b00739698ed9296482c6" translate="yes" xml:space="preserve">
          <source>WriteScalarSummary</source>
          <target state="translated">WriteScalarSummary</target>
        </trans-unit>
        <trans-unit id="cb73c41075bf3f01d3eb7791f75623bd77006f1a" translate="yes" xml:space="preserve">
          <source>WriteSummary</source>
          <target state="translated">WriteSummary</target>
        </trans-unit>
        <trans-unit id="29a4cf60a26ded7a2c3aed4855011f0c8495cfe3" translate="yes" xml:space="preserve">
          <source>Writes &lt;code&gt;MetaGraphDef&lt;/code&gt; to save_path/filename.</source>
          <target state="translated">save_path / filename에 &lt;code&gt;MetaGraphDef&lt;/code&gt; 를 씁니다 .</target>
        </trans-unit>
        <trans-unit id="9fa9b02b6070f8899e280b451f66b6bd6b575f93" translate="yes" xml:space="preserve">
          <source>Writes &lt;code&gt;Summary&lt;/code&gt; protocol buffers to event files.</source>
          <target state="translated">&lt;code&gt;Summary&lt;/code&gt; 프로토콜 버퍼를 이벤트 파일에 씁니다 .</target>
        </trans-unit>
        <trans-unit id="babca584f4aae5499f0f80472fec83895337e06c" translate="yes" xml:space="preserve">
          <source>Writes a &lt;code&gt;SavedModel&lt;/code&gt; protocol buffer to disk.</source>
          <target state="translated">&lt;code&gt;SavedModel&lt;/code&gt; 프로토콜 버퍼를 디스크에 씁니다 .</target>
        </trans-unit>
        <trans-unit id="ec531d54bcd6301e96265e18d2ce6d1e6c9a4e3b" translate="yes" xml:space="preserve">
          <source>Writes a dataset to a TFRecord file.</source>
          <target state="translated">데이터 세트를 TFRecord 파일에 씁니다.</target>
        </trans-unit>
        <trans-unit id="233cc84a3671355de983f6c125f1b2c0c8c7fc01" translate="yes" xml:space="preserve">
          <source>Writes a generic summary to the default SummaryWriter if one exists.</source>
          <target state="translated">기본 요약 작성기 (있는 경우)에 일반 요약을 작성합니다.</target>
        </trans-unit>
        <trans-unit id="d15430e669e23755ec2b520aa11323169d62858a" translate="yes" xml:space="preserve">
          <source>Writes a graph proto to a file.</source>
          <target state="translated">그래프 프로토 파일을 파일에 씁니다.</target>
        </trans-unit>
        <trans-unit id="9976fe10182472bc6e870e0863e6f6d04fac02a5" translate="yes" xml:space="preserve">
          <source>Writes a set of weights into the opaque params buffer so they can be used in upcoming training or inferences.</source>
          <target state="translated">Writes a set of weights into the opaque params buffer so they can be used in upcoming training or inferences.</target>
        </trans-unit>
        <trans-unit id="0faccf4f9a4005aa59d77d68c1f6677070d11c1f" translate="yes" xml:space="preserve">
          <source>Writes a summary using raw &lt;a href=&quot;../../compat/v1/summary&quot;&gt;&lt;code&gt;tf.compat.v1.Summary&lt;/code&gt;&lt;/a&gt; protocol buffers.</source>
          <target state="translated">원시 &lt;a href=&quot;../../compat/v1/summary&quot;&gt; &lt;code&gt;tf.compat.v1.Summary&lt;/code&gt; &lt;/a&gt; . 요약 프로토콜 버퍼를 사용하여 요약을 작성 합니다.</target>
        </trans-unit>
        <trans-unit id="5adf0b1064881763cf04269a02e41a3ea90a5906" translate="yes" xml:space="preserve">
          <source>Writes a training checkpoint.</source>
          <target state="translated">훈련 체크 포인트를 작성합니다.</target>
        </trans-unit>
        <trans-unit id="f2b611268944160d96a5b9a8fcf79249e74eb83f" translate="yes" xml:space="preserve">
          <source>Writes contents to the file at input filename. Creates file and recursively</source>
          <target state="translated">입력 파일 이름으로 파일에 내용을 씁니다. 파일을 만들고 재귀 적으로</target>
        </trans-unit>
        <trans-unit id="7fb1d95dfee1bc896877fefd9cbe8b4b9c44e0bf" translate="yes" xml:space="preserve">
          <source>Writes file_content to the file. Appends to the end of the file.</source>
          <target state="translated">file_content를 파일에 씁니다. 파일의 끝에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="a2ce6c85367b8678f1f61797d0dc74828b05ea19" translate="yes" xml:space="preserve">
          <source>Writes new value to variable's memory. Doesn't add ops to the graph.</source>
          <target state="translated">변수의 메모리에 새로운 값을 씁니다. 그래프에 op를 추가하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="99b81ac6b0204ef18bee769984926a4ea6b12daf" translate="yes" xml:space="preserve">
          <source>Writes the given dataset to the given file using the TFRecord format.</source>
          <target state="translated">Writes the given dataset to the given file using the TFRecord format.</target>
        </trans-unit>
        <trans-unit id="45f84d521a5351104bc075acf3695dd6bad6898e" translate="yes" xml:space="preserve">
          <source>Writing custom layers and models with Keras</source>
          <target state="translated">Keras로 사용자 정의 레이어 및 모델 작성</target>
        </trans-unit>
        <trans-unit id="c6a0c4f8b902d47bac80ddaf6bf34b2fbf4f9666" translate="yes" xml:space="preserve">
          <source>Xception V1 model for Keras.</source>
          <target state="translated">Keras 용 Xception V1 모델.</target>
        </trans-unit>
        <trans-unit id="1b60eef1486f042121cb1abb403bfa5983083402" translate="yes" xml:space="preserve">
          <source>Xdivy</source>
          <target state="translated">Xdivy</target>
        </trans-unit>
        <trans-unit id="9ed9396da589e127b120b5abc619086b34774295" translate="yes" xml:space="preserve">
          <source>Xlog1py</source>
          <target state="translated">Xlog1py</target>
        </trans-unit>
        <trans-unit id="12f9191163bb7e9e63172afc062b7b9642ca43a1" translate="yes" xml:space="preserve">
          <source>Xlogy</source>
          <target state="translated">Xlogy</target>
        </trans-unit>
        <trans-unit id="bccf09988d791dbda2da42e387167025b7d54ccb" translate="yes" xml:space="preserve">
          <source>YAML string or open file encoding a model configuration.</source>
          <target state="translated">YAML string or open file encoding a model configuration.</target>
        </trans-unit>
        <trans-unit id="dfea80b5fe13bab375ae2dddcdd3dbb43f1a3633" translate="yes" xml:space="preserve">
          <source>Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset, which is a derivative work from original NIST datasets. MNIST dataset is made available under the terms of the &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;Creative Commons Attribution-Share Alike 3.0 license.&lt;/a&gt;</source>
          <target state="translated">Yann LeCun과 Corinna Cortes는 원본 NIST 데이터 세트에서 파생 된 MNIST 데이터 세트의 저작권을 보유합니다. MNIST 데이터 세트는 &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;Creative Commons Attribution-Share Alike 3.0 라이센스 조건에 따라 제공됩니다.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="f0d948a8bdb9eb6bb818e3240fb3b2534756d8f7" translate="yes" xml:space="preserve">
          <source>Yields predictions for given features.</source>
          <target state="translated">지정된 기능에 대한 예측을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c970e3f1e790a2a4cd28b40401902501b9bc2d74" translate="yes" xml:space="preserve">
          <source>Yields:</source>
          <target state="translated">Yields:</target>
        </trans-unit>
        <trans-unit id="e930f451f4aa0e180bfec9e3ca9b3c51172a0d23" translate="yes" xml:space="preserve">
          <source>You can access a layer's regularization penalties by calling &lt;code&gt;layer.losses&lt;/code&gt; after calling the layer on inputs.</source>
          <target state="translated">입력에서 레이어를 호출 한 후 &lt;code&gt;layer.losses&lt;/code&gt; 를 호출하여 레이어 의 정규화 페널티에 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="59874a67ef5f7bd864b9ef3d3bb9393f8444ef02" translate="yes" xml:space="preserve">
          <source>You can access the raw &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; object used by &lt;code&gt;SingularMonitoredSession&lt;/code&gt;, whereas in MonitoredSession the raw session is private. This can be used:</source>
          <target state="translated">&lt;code&gt;SingularMonitoredSession&lt;/code&gt; 에서 사용 하는 원시 &lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt; 객체에 액세스 할 수 있지만 MonitoredSession에서는 원시 세션이 비공개입니다. 이것은 사용될 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="6ccb3baa3a0cbd712e9c239e9968fd1fb9ad400a" translate="yes" xml:space="preserve">
          <source>You can add an outer &lt;code&gt;batch&lt;/code&gt; axis by passing &lt;code&gt;axis=0&lt;/code&gt;:</source>
          <target state="translated">You can add an outer &lt;code&gt;batch&lt;/code&gt; axis by passing &lt;code&gt;axis=0&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="e7c18d1ca2444e96db7357ab3243c23ba7a401f2" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;../../../../distribute/cluster_resolver/clusterresolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="translated">또한 통과 할 수 &lt;a href=&quot;../../../../distribute/cluster_resolver/clusterresolver&quot;&gt; &lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt; 의&lt;/a&gt; 전략을 인스턴스화 할 때 인스턴스를. task_type, task_id 등은 &lt;code&gt;TF_CONFIG&lt;/code&gt; env var가 아닌 리졸버 인스턴스에서 구문 분석됩니다 .</target>
        </trans-unit>
        <trans-unit id="6e5fb70392b78f99083230f0a97aa193ef7fda3b" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;../cluster_resolver/clusterresolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="translated">또한 통과 할 수 &lt;a href=&quot;../cluster_resolver/clusterresolver&quot;&gt; &lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt; 의&lt;/a&gt; 전략을 인스턴스화 할 때 인스턴스를. task_type, task_id 등은 &lt;code&gt;TF_CONFIG&lt;/code&gt; env var가 아닌 리졸버 인스턴스에서 구문 분석됩니다 .</target>
        </trans-unit>
        <trans-unit id="5e1ab8575cb5168a5735bdcf646fbc24611fcf94" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="translated">You can also pass a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver&quot;&gt; &lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt; &lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</target>
        </trans-unit>
        <trans-unit id="ea8ecb649a2869edbe22ed0fa4b60b444f6b3240" translate="yes" xml:space="preserve">
          <source>You can also pass the following additional pieces to the constructor:</source>
          <target state="translated">다음 추가 조각을 생성자에 전달할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="beae5b5e28d39839a95e5dcf5642d815e8f5ff5c" translate="yes" xml:space="preserve">
          <source>You can also specify &lt;code&gt;config&lt;/code&gt; of the loss to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Loss&lt;/code&gt; class</source>
          <target state="translated">You can also specify &lt;code&gt;config&lt;/code&gt; of the loss to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Loss&lt;/code&gt; class</target>
        </trans-unit>
        <trans-unit id="c69555354a0bc222fe10d4f7bc39675e8b5f8158" translate="yes" xml:space="preserve">
          <source>You can also specify &lt;code&gt;config&lt;/code&gt; of the metric to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Metric&lt;/code&gt; class</source>
          <target state="translated">You can also specify &lt;code&gt;config&lt;/code&gt; of the metric to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Metric&lt;/code&gt; class</target>
        </trans-unit>
        <trans-unit id="52dc5982a3012b85661499626a8a8038241b7ffd" translate="yes" xml:space="preserve">
          <source>You can also use &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; to debug your models at runtime using Python tools, i.e., you can isolate portions of your code that you want to debug, wrap them in Python functions and insert &lt;code&gt;pdb&lt;/code&gt; tracepoints or print statements as desired, and wrap those functions in &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; &lt;/a&gt; 을 사용하여 Python 도구를 사용하여 런타임에 모델을 디버깅 할 수도 있습니다 . 즉, 디버깅하려는 코드 부분을 분리하고 Python 함수로 랩핑하고 원하는대로 &lt;code&gt;pdb&lt;/code&gt; 추적 점을 삽입 하거나 명령문을 인쇄하고 랩핑 할 수 있습니다. &lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; 의&lt;/a&gt; 함수 .</target>
        </trans-unit>
        <trans-unit id="6039b0dfd851cbc8275eabb888826394ae1b2ab1" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="d6714d05f3ec58957845e4e02d40676c59a816f4" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="153ea4bbc50ebf5e0a8403f29bb6788447deb11c" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="9ac6704fae664e72c4f0645536563ff0de5afe80" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="dcaf3f8131e1ea14866dcfb7fa41b1b0194e1d80" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="3385c4f6d32325dcf782cc9941db6f2f275241a0" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="68d5075223b70a3c64aec568d9f4b72b9521b3e8" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="3234565ee9ebae226cb6498a83d504945b3da8d1" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt; &lt;code&gt;tf.TypeSpec&lt;/code&gt; &lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="e9bf7e26d5c0cf51e4e4917f1eb23c3d37328691" translate="yes" xml:space="preserve">
          <source>You can cast a Keras variable but it still returns a Keras tensor.</source>
          <target state="translated">Keras 변수를 캐스트 할 수 있지만 여전히 Keras 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="3a3ea187f9997f3359ee54a5e203867aab3b8b27" translate="yes" xml:space="preserve">
          <source>You can create a &lt;a href=&quot;distributediterator&quot;&gt;&lt;code&gt;tf.distribute.DistributedIterator&lt;/code&gt;&lt;/a&gt; by calling &lt;code&gt;iter&lt;/code&gt; on a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; or creating a python loop over a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">You can create a &lt;a href=&quot;distributediterator&quot;&gt; &lt;code&gt;tf.distribute.DistributedIterator&lt;/code&gt; &lt;/a&gt; by calling &lt;code&gt;iter&lt;/code&gt; on a &lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt; or creating a python loop over a &lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c77d6bc9128a9a471ae009d158974ccffab599c2" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">당신이 사용하는 근로자에 걸쳐 데이터 세트 샤딩 해제 할 수 있습니다 &lt;code&gt;auto_shard&lt;/code&gt; 의 에서 옵션 &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="abcd404b6ec568dc3b6a192e74ff39fd2a467efc" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">당신이 사용하는 근로자에 걸쳐 데이터 세트 샤딩 해제 할 수 있습니다 &lt;code&gt;auto_shard&lt;/code&gt; 의 에서 옵션 &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="755b5985c40de671a4eb4bfb403b9e1a2cab5a03" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">당신이 사용하는 근로자에 걸쳐 데이터 세트 샤딩 해제 할 수 있습니다 &lt;code&gt;auto_shard&lt;/code&gt; 의 에서 옵션 &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="823e308b708edca072a13279a53e3cee2ec00199" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">당신이 사용하는 근로자에 걸쳐 데이터 세트 샤딩 해제 할 수 있습니다 &lt;code&gt;auto_shard&lt;/code&gt; 의 에서 옵션 &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="77348208b8f388d909966c4b5fc03169d2df6ba3" translate="yes" xml:space="preserve">
          <source>You can find more information about TensorBoard &lt;a href=&quot;https://www.tensorflow.org/get_started/summaries_and_tensorboard&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">TensorBoard에 대한 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/get_started/summaries_and_tensorboard&quot;&gt;여기&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="0be849f93096dd779e7a88c6ab36a4f32c9e43a7" translate="yes" xml:space="preserve">
          <source>You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:</source>
          <target state="translated">다음과 같은 전역 배치 크기를 사용하여 'SUM_OVER_BATCH_SIZE'를 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="afa5195343dbaa120879b06f025ea84ad87e6a32" translate="yes" xml:space="preserve">
          <source>You can modify the operations in place, but modifications to the list such as inserts/delete have no effect on the list of operations known to the graph.</source>
          <target state="translated">제자리에서 작업을 수정할 수 있지만 삽입 / 삭제와 같은 목록 수정은 그래프에 알려진 작업 목록에 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1c3d65193eb861cca9fdcde225478e0055db490e" translate="yes" xml:space="preserve">
          <source>You can now use table in functions like &lt;a href=&quot;../../../nn/embedding_lookup&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup&lt;/code&gt;&lt;/a&gt; to perform your embedding lookup and pass to your model.</source>
          <target state="translated">You can now use table in functions like &lt;a href=&quot;../../../nn/embedding_lookup&quot;&gt; &lt;code&gt;tf.nn.embedding_lookup&lt;/code&gt; &lt;/a&gt; to perform your embedding lookup and pass to your model.</target>
        </trans-unit>
        <trans-unit id="2c02260f75018990666dbe2089789cfb29e9e718" translate="yes" xml:space="preserve">
          <source>You can pass None to clear the control dependencies:</source>
          <target state="translated">제어 종속성을 지우려면 없음을 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="215f1df4bc1261bbb6f65d825790eea599447ab5" translate="yes" xml:space="preserve">
          <source>You can pass any of the returned values to &lt;code&gt;restore()&lt;/code&gt;.</source>
          <target state="translated">반환 된 값을 &lt;code&gt;restore()&lt;/code&gt; 전달할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ac470d84ff347998661ececab5ea81ac9cc6a909" translate="yes" xml:space="preserve">
          <source>You can pass the result of evaluating any summary op, using &lt;code&gt;tf.Session.run&lt;/code&gt; or &lt;a href=&quot;../../../tensor#eval&quot;&gt;&lt;code&gt;tf.Tensor.eval&lt;/code&gt;&lt;/a&gt;, to this function. Alternatively, you can pass a &lt;a href=&quot;../summary&quot;&gt;&lt;code&gt;tf.compat.v1.Summary&lt;/code&gt;&lt;/a&gt; protocol buffer that you populate with your own data. The latter is commonly done to report evaluation results in event files.</source>
          <target state="translated">&lt;code&gt;tf.Session.run&lt;/code&gt; 또는 &lt;a href=&quot;../../../tensor#eval&quot;&gt; &lt;code&gt;tf.Tensor.eval&lt;/code&gt; 을&lt;/a&gt; 사용하여 요약 op 평가 결과 를이 함수로 전달할 수 있습니다 . 또는, 당신은 전달할 수 있습니다&lt;a href=&quot;../summary&quot;&gt; &lt;code&gt;tf.compat.v1.Summary&lt;/code&gt; &lt;/a&gt;자체 데이터로 채운 tf.compat.v1.Summary 프로토콜 버퍼를. 후자는 일반적으로 평가 결과를 이벤트 파일에보고하기 위해 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="69155a67a44012b4f51b2dc8d0ccd534aa7907a1" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: Fit a Keras model when decaying 1/t with a rate of 0.5:</source>
          <target state="translated">이 일정을 &lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;학습 속도로 tf.keras.optimizers.Optimizer에. 예 : 0.5의 비율로 1 / t를 감쇄 할 때 Keras 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="48f5a8f4038134ce54531b0e709a77dbf5681703" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5):</source>
          <target state="translated">이 일정을 &lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;학습 속도로 tf.keras.optimizers.Optimizer에. 예 : sqrt (즉, power = 0.5)를 사용하여 10000 단계로 0.1에서 0.01로 감소하면서 모델을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="40b491f9b541bdf48952295bf5d5a0ca64ae94b3" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: When fitting a Keras model, decay every 100000 steps with a base of 0.96:</source>
          <target state="translated">이 일정을 &lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;학습 속도로 tf.keras.optimizers.Optimizer에. 예 : Keras 모델을 피팅 할 때 0.96의 기준으로 100000 단계마다 감쇠합니다.</target>
        </trans-unit>
        <trans-unit id="573311646e6161dd40916f132e58fdf271d90b7d" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. The learning rate schedule is also serializable and deserializable using &lt;a href=&quot;serialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;deserialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 스케줄을 학습 속도로 &lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; 에&lt;/a&gt; 직접 전달할 수 있습니다 . 학습 속도 일정은 &lt;a href=&quot;serialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;deserialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt; 를&lt;/a&gt; 사용하여 직렬화 및 역 직렬화 가능합니다. 합니다.</target>
        </trans-unit>
        <trans-unit id="5458dcea309d742dc1ae87c297fbd433242a68c9" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizers/optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. The learning rate schedule is also serializable and deserializable using &lt;a href=&quot;../optimizers/schedules/serialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../optimizers/schedules/deserialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 스케줄을 학습 속도로 &lt;a href=&quot;../optimizers/optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; 에&lt;/a&gt; 직접 전달할 수 있습니다 . 학습 속도 일정은 &lt;a href=&quot;../optimizers/schedules/serialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../optimizers/schedules/deserialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt; 를&lt;/a&gt; 사용하여 직렬화 및 역 직렬화 가능합니다. 합니다.</target>
        </trans-unit>
        <trans-unit id="154d72e5d23674ce441818a8ed845bb4df350073" translate="yes" xml:space="preserve">
          <source>You can provide logits of classes as &lt;code&gt;y_pred&lt;/code&gt;, since argmax of logits and probabilities are same.</source>
          <target state="translated">You can provide logits of classes as &lt;code&gt;y_pred&lt;/code&gt; , since argmax of logits and probabilities are same.</target>
        </trans-unit>
        <trans-unit id="0779a4d81e795874a050ef63a764491cc356c1c7" translate="yes" xml:space="preserve">
          <source>You can return from this call a &lt;code&gt;SessionRunArgs&lt;/code&gt; object indicating ops or tensors to add to the upcoming &lt;code&gt;run()&lt;/code&gt; call. These ops/tensors will be run together with the ops/tensors originally passed to the original run() call. The run args you return can also contain feeds to be added to the run() call.</source>
          <target state="translated">이 호출 에서 다가오는 &lt;code&gt;run()&lt;/code&gt; 추가 할 ops 또는 텐서를 나타내는 &lt;code&gt;SessionRunArgs&lt;/code&gt; 객체를 반환 할 수 있습니다. 호출 . 이러한 op / tensors는 원래 run () 호출에 원래 전달 된 ops / tenors와 함께 실행됩니다. 리턴 한 실행 인수에는 run () 호출에 추가 할 피드도 포함될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9ae0a59fa6ec6a5af526342b6817b045dd0db4e8" translate="yes" xml:space="preserve">
          <source>You can set the distribution options of a dataset through the &lt;code&gt;experimental_distribute&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;experimental_distribute&lt;/code&gt; 속성을 통해 데이터 세트의 배포 옵션을 설정할 수 있습니다 . 이 속성은 &lt;a href=&quot;distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt; 의 인스턴스입니다 .</target>
        </trans-unit>
        <trans-unit id="38287133e2e375cb065b8aff1223591e0429addd" translate="yes" xml:space="preserve">
          <source>You can set the optimization options of a dataset through the &lt;code&gt;experimental_optimization&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;optimizationoptions&quot;&gt;&lt;code&gt;tf.data.experimental.OptimizationOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;experimental_optimization&lt;/code&gt; 속성을 통해 데이터 세트의 최적화 옵션을 설정할 수 있습니다 . 이 속성은 &lt;a href=&quot;optimizationoptions&quot;&gt; &lt;code&gt;tf.data.experimental.OptimizationOptions&lt;/code&gt; &lt;/a&gt; 의 인스턴스입니다 . .</target>
        </trans-unit>
        <trans-unit id="a4942db36ae427216d9ed0a514b6b4124827857d" translate="yes" xml:space="preserve">
          <source>You can set the stats options of a dataset through the &lt;code&gt;experimental_stats&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;statsoptions&quot;&gt;&lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt;&lt;/a&gt;. For example, to collect latency stats on all dataset edges, use the following pattern:</source>
          <target state="translated">&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;experimental_stats&lt;/code&gt; 속성을 통해 데이터 세트의 통계 옵션을 설정할 수 있습니다 . 이 속성은 &lt;a href=&quot;statsoptions&quot;&gt; &lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt; &lt;/a&gt; 의 인스턴스입니다 . 예를 들어 모든 데이터 세트 에지에서 대기 시간 통계를 수집하려면 다음 패턴을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="0286d405fc4b694404c86890a3b0233533e44d1a" translate="yes" xml:space="preserve">
          <source>You can set the threading options of a dataset through the &lt;code&gt;experimental_threading&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;threadingoptions&quot;&gt;&lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;experimental_threading&lt;/code&gt; 속성을 통해 데이터 세트의 스레딩 옵션을 설정할 수 있습니다 . 이 속성은 &lt;a href=&quot;threadingoptions&quot;&gt; &lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt; &lt;/a&gt; 의 인스턴스입니다 .</target>
        </trans-unit>
        <trans-unit id="5821880102274e82922a0313317c1a0e00262ef7" translate="yes" xml:space="preserve">
          <source>You can specify the initial state of RNN layers numerically by calling &lt;code&gt;reset_states&lt;/code&gt; with the keyword argument &lt;code&gt;states&lt;/code&gt;. The value of &lt;code&gt;states&lt;/code&gt; should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.</source>
          <target state="translated">키워드 인수 &lt;code&gt;states&lt;/code&gt; 와 함께 &lt;code&gt;reset_states&lt;/code&gt; 를 호출하여 RNN 계층의 초기 상태를 숫자로 지정할 수 있습니다 . &lt;code&gt;states&lt;/code&gt; 의 가치 numpy 배열이거나 RNN 계층의 초기 상태를 나타내는 numpy 배열 목록이어야합니다.</target>
        </trans-unit>
        <trans-unit id="7903e8eb7e3c0e1f135ecaca1aabba76f1186224" translate="yes" xml:space="preserve">
          <source>You can then use &lt;code&gt;TimeDistributed&lt;/code&gt; to apply a &lt;code&gt;Conv2D&lt;/code&gt; layer to each of the 10 timesteps, independently:</source>
          <target state="translated">You can then use &lt;code&gt;TimeDistributed&lt;/code&gt; to apply a &lt;code&gt;Conv2D&lt;/code&gt; layer to each of the 10 timesteps, independently:</target>
        </trans-unit>
        <trans-unit id="0934b024d096c8c6f1a29dcdebc76afd185fc96d" translate="yes" xml:space="preserve">
          <source>You can then use &lt;code&gt;TimeDistributed&lt;/code&gt; to apply a &lt;code&gt;Dense&lt;/code&gt; layer to each of the 10 timesteps, independently:</source>
          <target state="translated">그런 다음 &lt;code&gt;TimeDistributed&lt;/code&gt; 를 사용 하여 독립적으로 10 개의 타임 스텝 각각에 &lt;code&gt;Dense&lt;/code&gt; 레이어 를 적용 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="49b9bca1daa199f82797585539058cf0bff378d8" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;get_replica_context&quot;&gt;&lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt;&lt;/a&gt; to get an instance of &lt;code&gt;ReplicaContext&lt;/code&gt;. This should be inside your replicated step function, such as in a &lt;a href=&quot;strategy#experimental_run_v2&quot;&gt;&lt;code&gt;tf.distribute.Strategy.experimental_run_v2&lt;/code&gt;&lt;/a&gt; call.</source>
          <target state="translated">당신은 사용할 수 있습니다 &lt;a href=&quot;get_replica_context&quot;&gt; &lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt; &lt;/a&gt; 의 인스턴스를 얻을 수 &lt;code&gt;ReplicaContext&lt;/code&gt; 을 . 이것은 &lt;a href=&quot;strategy#experimental_run_v2&quot;&gt; &lt;code&gt;tf.distribute.Strategy.experimental_run_v2&lt;/code&gt; &lt;/a&gt; 호출 과 같이 복제 된 단계 함수 내에 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="f52e741d1756d64c06640ac0efef1ab8aa73c071" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;get_replica_context&quot;&gt;&lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt;&lt;/a&gt; to get an instance of &lt;code&gt;ReplicaContext&lt;/code&gt;. This should be inside your replicated step function, such as in a &lt;a href=&quot;strategy#run&quot;&gt;&lt;code&gt;tf.distribute.Strategy.run&lt;/code&gt;&lt;/a&gt; call.</source>
          <target state="translated">You can use &lt;a href=&quot;get_replica_context&quot;&gt; &lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt; &lt;/a&gt; to get an instance of &lt;code&gt;ReplicaContext&lt;/code&gt; . This should be inside your replicated step function, such as in a &lt;a href=&quot;strategy#run&quot;&gt; &lt;code&gt;tf.distribute.Strategy.run&lt;/code&gt; &lt;/a&gt; call.</target>
        </trans-unit>
        <trans-unit id="bdb676d9df499211a1b26621113c16cf595d42c7" translate="yes" xml:space="preserve">
          <source>You can use the &lt;code&gt;reduce&lt;/code&gt; API to aggregate results across replicas and use this as a return value from one iteration over a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;. Or you can use &lt;a href=&quot;../keras/metrics&quot;&gt;&lt;code&gt;tf.keras.metrics&lt;/code&gt;&lt;/a&gt; (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.</source>
          <target state="translated">You can use the &lt;code&gt;reduce&lt;/code&gt; API to aggregate results across replicas and use this as a return value from one iteration over a &lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;. Or you can use &lt;a href=&quot;../keras/metrics&quot;&gt; &lt;code&gt;tf.keras.metrics&lt;/code&gt; &lt;/a&gt; (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.</target>
        </trans-unit>
        <trans-unit id="cbd80087e0ae69c397a70f9c2b3b4a4570f57c0a" translate="yes" xml:space="preserve">
          <source>You can use the &lt;code&gt;reduce&lt;/code&gt; API to aggregate results across replicas and use this as a return value from one iteration over the distributed dataset. Or you can use &lt;a href=&quot;../keras/metrics&quot;&gt;&lt;code&gt;tf.keras.metrics&lt;/code&gt;&lt;/a&gt; (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.</source>
          <target state="translated">&lt;code&gt;reduce&lt;/code&gt; API를 사용하여 복제본간에 결과를 집계하고이를 분산 데이터 세트에 대한 한 번의 반복에서 리턴 값으로 사용할 수 있습니다. 또는 &lt;a href=&quot;../keras/metrics&quot;&gt; &lt;code&gt;tf.keras.metrics&lt;/code&gt; &lt;/a&gt; 를 사용할 수 있습니다 (예 : 손실, 정확도 등)를 사용하여 지정된 에포크의 단계마다 메트릭을 누적 .</target>
        </trans-unit>
        <trans-unit id="849e0f628a638e3510c8fe215b32ce36d057b4e1" translate="yes" xml:space="preserve">
          <source>You can use the Dense layer as you would expect:</source>
          <target state="translated">예상대로 고밀도 레이어를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ffad7cddd33e66d73417586d2382f64196240d65" translate="yes" xml:space="preserve">
          <source>You can use this function to read events written to an event file. It returns a Python iterator that yields &lt;code&gt;Event&lt;/code&gt; protocol buffers.</source>
          <target state="translated">이 기능을 사용하여 이벤트 파일에 기록 된 이벤트를 읽을 수 있습니다. &lt;code&gt;Event&lt;/code&gt; 프로토콜 버퍼 를 생성하는 Python 반복자를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="8746f4908e5bf18e74cecb0263457015fb63a862" translate="yes" xml:space="preserve">
          <source>You could also use vocabulary lookup before crossing:</source>
          <target state="translated">교차하기 전에 어휘 조회를 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="de365a46ec65ae586c7c15b2194ec9eca625cd28" translate="yes" xml:space="preserve">
          <source>You could simply do:</source>
          <target state="translated">You could simply do:</target>
        </trans-unit>
        <trans-unit id="2314a9b2a621f208986f86ed4dc02fe23da32fe1" translate="yes" xml:space="preserve">
          <source>You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively.</source>
          <target state="translated">서브 클래스에서이 메소드를 대체 할 수 있습니다. 표준 run () 메소드는 args 및 kwargs 인수에서 각각 순차적 인 키워드 인수와 키워드 인수를 사용하여 객체의 생성자에 전달 된 호출 가능 객체를 대상 인수 (있는 경우)로 호출합니다.</target>
        </trans-unit>
        <trans-unit id="a9f39d075c01a25e9820f6a67df1a4ab21c825d3" translate="yes" xml:space="preserve">
          <source>You may pass descendant of &lt;a href=&quot;strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; to &lt;a href=&quot;../estimator/runconfig&quot;&gt;&lt;code&gt;tf.estimator.RunConfig&lt;/code&gt;&lt;/a&gt; to specify how a &lt;a href=&quot;../estimator/estimator&quot;&gt;&lt;code&gt;tf.estimator.Estimator&lt;/code&gt;&lt;/a&gt; should distribute its computation. See &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support&quot;&gt;guide&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../estimator/estimator&quot;&gt; &lt;code&gt;tf.estimator.Estimator&lt;/code&gt; &lt;/a&gt; 가 계산을 분배 하는 방법을 지정 하기 위해 &lt;a href=&quot;../estimator/runconfig&quot;&gt; &lt;code&gt;tf.estimator.RunConfig&lt;/code&gt; &lt;/a&gt; 하위 항목을 &lt;a href=&quot;strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt; 로 전달할 수 있습니다 . &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support&quot;&gt;가이드&lt;/a&gt; 참조 .</target>
        </trans-unit>
        <trans-unit id="4caa6c70be3989fa593907ab37426f9ba5e5385e" translate="yes" xml:space="preserve">
          <source>You may provide either a constant &lt;code&gt;window_size&lt;/code&gt; or a window size determined by the key through &lt;code&gt;window_size_func&lt;/code&gt;.</source>
          <target state="translated">당신은 상수를 하나 제공 할 수 &lt;code&gt;window_size&lt;/code&gt; 또는 통해 키에 의해 결정되는 창 크기 &lt;code&gt;window_size_func&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="246729df98351fdf262d7a7f6c3229e8ff176c5b" translate="yes" xml:space="preserve">
          <source>You must have set the task_type and task_id object properties before calling this function, or pass in the &lt;code&gt;task_type&lt;/code&gt; and &lt;code&gt;task_id&lt;/code&gt; parameters when using this function. If you do both, the function parameters will override the object properties.</source>
          <target state="translated">이 함수를 호출하기 전에 task_type 및 task_id 오브젝트 특성을 설정했거나이 함수 를 사용할 때 &lt;code&gt;task_type&lt;/code&gt; 및 &lt;code&gt;task_id&lt;/code&gt; 매개 변수를 전달해야 합니다. 두 가지를 모두 수행하면 함수 매개 변수가 객체 속성보다 우선합니다.</target>
        </trans-unit>
        <trans-unit id="843e6067b5e147efe5ecbea6ee6b24b54cc1cca5" translate="yes" xml:space="preserve">
          <source>You number checkpoint filenames by passing a value to the optional &lt;code&gt;global_step&lt;/code&gt; argument to &lt;code&gt;save()&lt;/code&gt;:</source>
          <target state="translated">선택적인 &lt;code&gt;global_step&lt;/code&gt; 인수에 값을 전달하여 checkpoint 파일 이름 을 &lt;code&gt;save()&lt;/code&gt; sav ()). .</target>
        </trans-unit>
        <trans-unit id="47945af2dd44ab9b9f3ac220d707a317fa8bb12b" translate="yes" xml:space="preserve">
          <source>You should not use this class directly, but instead instantiate one of its subclasses such as &lt;a href=&quot;sgd&quot;&gt;&lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;adam&quot;&gt;&lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="translated">You should not use this class directly, but instead instantiate one of its subclasses such as &lt;a href=&quot;sgd&quot;&gt; &lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;adam&quot;&gt; &lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt; &lt;/a&gt;, etc.</target>
        </trans-unit>
        <trans-unit id="8d943ec0774ebc46f132a6e012d0ef8ae2735a29" translate="yes" xml:space="preserve">
          <source>You should use this instead of the variable itself to initialize another variable with a value that depends on the value of this variable.</source>
          <target state="translated">변수 대신이 변수를 사용하여이 변수의 값에 종속 된 값으로 다른 변수를 초기화해야합니다.</target>
        </trans-unit>
        <trans-unit id="5bcb3cc3036b6c1787fd4b43072cd172e65fd9dd" translate="yes" xml:space="preserve">
          <source>You typically pass looper threads to the supervisor &lt;code&gt;Join()&lt;/code&gt; method.</source>
          <target state="translated">일반적으로 루퍼 스레드를 수퍼바이저 &lt;code&gt;Join()&lt;/code&gt; 메소드에 전달합니다.</target>
        </trans-unit>
        <trans-unit id="866c1e20e145360ff1c2d3fac51cd421f0959538" translate="yes" xml:space="preserve">
          <source>You usually do not need to call this method as all ops that need the value of the variable call it automatically through a &lt;code&gt;convert_to_tensor()&lt;/code&gt; call.</source>
          <target state="translated">변수의 값이 필요한 모든 op가 &lt;code&gt;convert_to_tensor()&lt;/code&gt; 호출을 통해 자동으로 호출하므로 일반적으로이 메소드를 호출 할 필요는 없습니다 .</target>
        </trans-unit>
        <trans-unit id="28b1f658f43504f27e0b8dad06c688c5569a5b43" translate="yes" xml:space="preserve">
          <source>You want os.path.exists() to always return true during testing.</source>
          <target state="translated">테스트하는 동안 os.path.exists ()가 항상 true를 반환하기를 원합니다.</target>
        </trans-unit>
        <trans-unit id="558865a16feb9f751b8bcebf46a954afbeba0b24" translate="yes" xml:space="preserve">
          <source>YouTube</source>
          <target state="translated">YouTube</target>
        </trans-unit>
        <trans-unit id="743b60db1fe7f2ede7836bb4e7bd6e58b1ef9754" translate="yes" xml:space="preserve">
          <source>Zeiler, 2012</source>
          <target state="translated">Zeiler, 2012</target>
        </trans-unit>
        <trans-unit id="8cf5dacc30bc54f166bd31cec0b947c674cc3e7e" translate="yes" xml:space="preserve">
          <source>Zero or more tensors to group.</source>
          <target state="translated">Zero or more tensors to group.</target>
        </trans-unit>
        <trans-unit id="7c152968210a9e92278d1a3f141b891e56657c41" translate="yes" xml:space="preserve">
          <source>Zero-pad the start and end of dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; of the input according to &lt;code&gt;paddings&lt;/code&gt; to produce &lt;code&gt;padded&lt;/code&gt; of shape &lt;code&gt;padded_shape&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;paddings&lt;/code&gt; 에 따라 입력 의 치수 &lt;code&gt;[1, ..., M]&lt;/code&gt; 의 시작과 끝을 제로 패드로 &lt;code&gt;padded&lt;/code&gt; &lt;code&gt;padded_shape&lt;/code&gt; 모양의 패딩 생성 .</target>
        </trans-unit>
        <trans-unit id="ec7ae133fd762cea93146fc3c7001608432a59c7" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 1D input (e.g. temporal sequence).</source>
          <target state="translated">1D 입력을위한 제로 패딩 레이어 (예 : 시간 시퀀스).</target>
        </trans-unit>
        <trans-unit id="9b78ee89fd119fbc58af59c013afefa4ada9d7ed" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 2D input (e.g. picture).</source>
          <target state="translated">2D 입력을위한 제로 패딩 레이어 (예 : 그림).</target>
        </trans-unit>
        <trans-unit id="81f93ad811573d34a01e1dda72548f7bcac0984e" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 3D data (spatial or spatio-temporal).</source>
          <target state="translated">3D 데이터를위한 제로 패딩 레이어 (공간 또는 시공간).</target>
        </trans-unit>
        <trans-unit id="4650f7edf1724b78bd849b1c2cb32632cbf26f09" translate="yes" xml:space="preserve">
          <source>Zero-pads and then rearranges (permutes) blocks of spatial data into batch. More specifically, this op outputs a copy of the input tensor where values from the &lt;code&gt;height&lt;/code&gt; and &lt;code&gt;width&lt;/code&gt; dimensions are moved to the &lt;code&gt;batch&lt;/code&gt; dimension. After the zero-padding, both &lt;code&gt;height&lt;/code&gt; and &lt;code&gt;width&lt;/code&gt; of the input must be divisible by the block size.</source>
          <target state="translated">패드를 제로로 만든 다음 공간 데이터 블록을 배치로 재정렬 (순열)합니다. 보다 구체적으로,이 op는 &lt;code&gt;height&lt;/code&gt; 및 &lt;code&gt;width&lt;/code&gt; 치수 의 값 이 &lt;code&gt;batch&lt;/code&gt; 치수 로 이동 되는 입력 텐서의 사본을 출력 합니다. 제로 패딩 후 입력의 &lt;code&gt;height&lt;/code&gt; 와 &lt;code&gt;width&lt;/code&gt; 를 블록 크기로 나눌 수 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="dfee31bddce3aa2ae0eb9ab0a81354d098ee985f" translate="yes" xml:space="preserve">
          <source>ZerosLike</source>
          <target state="translated">ZerosLike</target>
        </trans-unit>
        <trans-unit id="d4dde75ca731d6afffc873406bc9b30fd639401e" translate="yes" xml:space="preserve">
          <source>Zeta</source>
          <target state="translated">Zeta</target>
        </trans-unit>
        <trans-unit id="37a40c343b1c25e2aa4647448f2479d812035f3e" translate="yes" xml:space="preserve">
          <source>ZipDataset</source>
          <target state="translated">ZipDataset</target>
        </trans-unit>
        <trans-unit id="cbfaea632aa3eb5b0ee3bf0fe5b02a033aca96d1" translate="yes" xml:space="preserve">
          <source>Zone of the GCE instance group.</source>
          <target state="translated">Zone of the GCE instance group.</target>
        </trans-unit>
        <trans-unit id="9b808978f0a72336077098f027b8f727415da270" translate="yes" xml:space="preserve">
          <source>Zone where the TPUs are located. If omitted or empty, we will assume that the zone of the TPU is the same as the zone of the GCE VM, which we will try to discover from the GCE metadata service.</source>
          <target state="translated">Zone where the TPUs are located. If omitted or empty, we will assume that the zone of the TPU is the same as the zone of the GCE VM, which we will try to discover from the GCE metadata service.</target>
        </trans-unit>
        <trans-unit id="690066f3139e111a254b9b22702420163ce9e6c3" translate="yes" xml:space="preserve">
          <source>[-128, 127] for signed, num_bits = 8, or</source>
          <target state="translated">[-128, 127] for signed, num_bits = 8, or</target>
        </trans-unit>
        <trans-unit id="7f0f7b84b517d1011bde2787a1af440ee2991478" translate="yes" xml:space="preserve">
          <source>[0, 255] for unsigned, num_bits = 8.</source>
          <target state="translated">[0, 255] for unsigned, num_bits = 8.</target>
        </trans-unit>
        <trans-unit id="cd22950e542e583caceeef78dc9ae8586fc6a2c9" translate="yes" xml:space="preserve">
          <source>[1] Nicholas J. Higham (2002). Accuracy and Stability of Numerical Algorithms: Second Edition. SIAM. p. 175. ISBN 978-0-89871-802-7.</source>
          <target state="translated">[1] Nicholas J. Higham (2002). 수치 알고리즘의 정확성과 안정성 : 2 판. 시암. 피. 175. ISBN 978-0-89871-802-7.</target>
        </trans-unit>
        <trans-unit id="b99fc78b4d7545fcd138b54f0e14096e6f4551ca" translate="yes" xml:space="preserve">
          <source>[1] http://en.wikipedia.org/wiki/Gamma_correction</source>
          <target state="translated">[1] http://en.wikipedia.org/wiki/Gamma_correction</target>
        </trans-unit>
        <trans-unit id="8b402dbbbcbda420a8c8a62323dde3dbd63d7a5e" translate="yes" xml:space="preserve">
          <source>[1]: G. Strang. 'Linear Algebra and Its Applications, 2nd Ed.' Academic Press, Inc., 1980, pp. 139-142.</source>
          <target state="translated">[1] : G. Strang. '선형 대수와 그 응용, 2 판' Academic Press, Inc., 1980, 139-142 쪽.</target>
        </trans-unit>
        <trans-unit id="3431dfde47f5f77802dc8c1e589fe801ed9dd223" translate="yes" xml:space="preserve">
          <source>[Flag], a new list of Flag instances. Caller may update this list as</source>
          <target state="translated">[Flag], 새로운 Flag 인스턴스 목록. 발신자는이 목록을 다음과 같이 업데이트 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8a492c08a6fb4ab5ad781dcaefb3c630980c2a92" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to &lt;a href=&quot;../../../estimator/vocabinfo&quot;&gt;&lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt;&lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</source>
          <target state="translated">[Optional] Dict of variable names (strings) to &lt;a href=&quot;../../../estimator/vocabinfo&quot;&gt; &lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt; &lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</target>
        </trans-unit>
        <trans-unit id="040ab6c4e7ceb839df30c29d661e2122a9ea0ad3" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to &lt;a href=&quot;vocabinfo&quot;&gt;&lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt;&lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</source>
          <target state="translated">[Optional] Dict of variable names (strings) to &lt;a href=&quot;vocabinfo&quot;&gt; &lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt; &lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</target>
        </trans-unit>
        <trans-unit id="7ccfc72b7041b89963d68b36a1571281cdd9debc" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to name of the previously-trained variable in &lt;code&gt;ckpt_to_initialize_from&lt;/code&gt;. If not explicitly provided, the name of the variable is assumed to be same between previous checkpoint and current model. Note that this has no effect on the set of variables that is warm-started, and only controls name mapping (use &lt;code&gt;vars_to_warm_start&lt;/code&gt; for controlling what variables to warm-start).</source>
          <target state="translated">[Optional] Dict of variable names (strings) to name of the previously-trained variable in &lt;code&gt;ckpt_to_initialize_from&lt;/code&gt; . If not explicitly provided, the name of the variable is assumed to be same between previous checkpoint and current model. Note that this has no effect on the set of variables that is warm-started, and only controls name mapping (use &lt;code&gt;vars_to_warm_start&lt;/code&gt; for controlling what variables to warm-start).</target>
        </trans-unit>
        <trans-unit id="88dac723b45d7ec9ed5d2521d06d37e6754801f3" translate="yes" xml:space="preserve">
          <source>[Optional] One of the following:</source>
          <target state="translated">[Optional] One of the following:</target>
        </trans-unit>
        <trans-unit id="56fcc4459de1190d856f1a05d4fa018ace19dfbd" translate="yes" xml:space="preserve">
          <source>[Required] A string specifying the directory with checkpoint file(s) or path to checkpoint from which to warm-start the model parameters.</source>
          <target state="translated">[Required] A string specifying the directory with checkpoint file(s) or path to checkpoint from which to warm-start the model parameters.</target>
        </trans-unit>
        <trans-unit id="8ee2e7d87ae9f9bdabd7b4bb725a904d7d206cb7" translate="yes" xml:space="preserve">
          <source>[[w(1, 0), w(1, 2), 0.5], [w(0, 0), w(0, 2), -0.5], [0.25, -0.25, 42]]</source>
          <target state="translated">[[w(1, 0), w(1, 2), 0.5], [w(0, 0), w(0, 2), -0.5], [0.25, -0.25, 42]]</target>
        </trans-unit>
        <trans-unit id="7fbe9b43ff0aeb17fee44c25d05877c8cdce9b59" translate="yes" xml:space="preserve">
          <source>[batch * prod(block_shape)] + [padded_shape[1] / block_shape[0], ..., padded_shape[M] / block_shape[M-1]] + remaining_shape</source>
          <target state="translated">[일괄 * 제품 (block_shape)] + [padded_shape [1] / block_shape [0], ..., padded_shape [M] / block_shape [M-1]] + 잔여 모양</target>
        </trans-unit>
        <trans-unit id="cbf73d5213642961c32b5b788775c8bd0559cdc7" translate="yes" xml:space="preserve">
          <source>[batch, height - 2 * (filter_width - 1), width - 2 * (filter_height - 1), out_channels].</source>
          <target state="translated">[batch, height - 2 * (filter_width - 1), width - 2 * (filter_height - 1), out_channels].</target>
        </trans-unit>
        <trans-unit id="4f44d9fad75faea26dd9cc4bc783f18b2b9a3245" translate="yes" xml:space="preserve">
          <source>[batch, height, width, out_channels].</source>
          <target state="translated">[batch, height, width, out_channels].</target>
        </trans-unit>
        <trans-unit id="69883aca3ab9b1512329fe1f31a31a2297068297" translate="yes" xml:space="preserve">
          <source>[batch&lt;em&gt;block_size&lt;/em&gt;block_size, height_pad/block_size, width_pad/block_size, depth]</source>
          <target state="translated">[배치 &lt;em&gt;블록&lt;/em&gt; _ 크기 &lt;em&gt;블록&lt;/em&gt; _ 크기, 높이 _ 패드 / 블록 _ 크기, 너비 _ 패드 / 블록 _ 크기, 깊이]</target>
        </trans-unit>
        <trans-unit id="a99f9eae6edb2aeb1e4c6a1434cfbcae41146183" translate="yes" xml:space="preserve">
          <source>[batch] + [padded_shape[1] / block_shape[0], block_shape[0], ..., padded_shape[M] / block_shape[M-1], block_shape[M-1]] + remaining_shape</source>
          <target state="translated">[일괄 처리] + [padded_shape [1] / block_shape [0], block_shape [0], ..., padded_shape [M] / block_shape [M-1], block_shape [M-1]] + 나머지 모양</target>
        </trans-unit>
        <trans-unit id="a018842c23398495c71954bf778c4b46f8056996" translate="yes" xml:space="preserve">
          <source>[batch_size, num_channels] + output_spatial_shape</source>
          <target state="translated">[배치 크기, num_channels] + output_spatial_shape</target>
        </trans-unit>
        <trans-unit id="51a4c96f9f2ac53566fd73823e05a471562bcdfc" translate="yes" xml:space="preserve">
          <source>[filename1, filename2, ... filenameN] as strings</source>
          <target state="translated">문자열로 [filename1, filename2, ... filenameN]</target>
        </trans-unit>
        <trans-unit id="d8a02b4024d1f91043f88e51a21c86cf6a2b1f8b" translate="yes" xml:space="preserve">
          <source>[input_min, input_max] are scalar floats that specify the range for the float interpretation of the 'input' data. For example, if input_min is -1.0f and input_max is 1.0f, and we are dealing with quint16 quantized data, then a 0 value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.</source>
          <target state="translated">[input_min, input_max] are scalar floats that specify the range for the float interpretation of the 'input' data. For example, if input_min is -1.0f and input_max is 1.0f, and we are dealing with quint16 quantized data, then a 0 value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.</target>
        </trans-unit>
        <trans-unit id="143b12af78e15f28dfa0f3a68b2560666f6d5995" translate="yes" xml:space="preserve">
          <source>[min_range, max_range] are scalar floats that specify the range for the 'input' data. The 'mode' attribute controls exactly which calculations are used to convert the float values to their quantized equivalents. The 'round_mode' attribute controls which rounding tie-breaking algorithm is used when rounding float values to their quantized equivalents.</source>
          <target state="translated">[min_range, max_range]는 '입력'데이터의 범위를 지정하는 스칼라 부동 소수점입니다. 'mode'속성은 부동 소수점 값을 양자화 된 등가로 변환하는 데 사용되는 계산을 정확하게 제어합니다. 'round_mode'속성은 부동 소수점 값을 양자화 된 동등 값으로 반올림 할 때 사용할 반올림 타이 브레이킹 알고리즘을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="1fa2da75f60f4f3a22685794db19c7508c0c1baa" translate="yes" xml:space="preserve">
          <source>[min_range, max_range] are scalar floats that specify the range for the output. The 'mode' attribute controls exactly which calculations are used to convert the float values to their quantized equivalents.</source>
          <target state="translated">[min_range, max_range]는 출력 범위를 지정하는 스칼라 부동 소수점입니다. 'mode'속성은 부동 소수점 값을 양자화 된 등가로 변환하는 데 사용되는 계산을 정확하게 제어합니다.</target>
        </trans-unit>
        <trans-unit id="8bebaab027f4bff5b0c289cd914950701e571abd" translate="yes" xml:space="preserve">
          <source>[num_batches, input_spatial_shape[0], ..., input_spatial_shape[N-1], num_input_channels],</source>
          <target state="translated">[num_batches, input_spatial_shape [0], ..., input_spatial_shape [N-1], num_input_channels],</target>
        </trans-unit>
        <trans-unit id="8f8157d4fe8eda67ef4b3a3b791fa8bb98678ccc" translate="yes" xml:space="preserve">
          <source>[spatial_filter_shape[0], ..., spatial_filter_shape[N-1], num_input_channels, num_output_channels],</source>
          <target state="translated">[spatial_filter_shape [0], ..., patial_filter_shape [N-1], num_input_channels, num_output_channels],</target>
        </trans-unit>
        <trans-unit id="ccfaace27f3147695ce3d29304da376491a69186" translate="yes" xml:space="preserve">
          <source>[str], a list of strings, usually sys.argv[1:], which may contain one or more flagfile directives of the form --flagfile=&quot;./filename&quot;. Note that the name of the program (sys.argv[0]) should be omitted.</source>
          <target state="translated">[str], a list of strings, usually sys.argv[1:], which may contain one or more flagfile directives of the form --flagfile=&quot;./filename&quot;. Note that the name of the program (sys.argv[0]) should be omitted.</target>
        </trans-unit>
        <trans-unit id="a05d1e5d9cec162aa920d1c81e7d248fa2814404" translate="yes" xml:space="preserve">
          <source>[str], a list of the flag names to be checked.</source>
          <target state="translated">[str], a list of the flag names to be checked.</target>
        </trans-unit>
        <trans-unit id="5c26fe712edb3b507e5043f8b806fc26990923cd" translate="yes" xml:space="preserve">
          <source>[str], a non-empty list of string values in the enum.</source>
          <target state="translated">[str], a non-empty list of string values in the enum.</target>
        </trans-unit>
        <trans-unit id="3cd81efe3c17628b43be50a6bfd943ed33227190" translate="yes" xml:space="preserve">
          <source>[str], a non-empty list of strings with the possible values for the flag.</source>
          <target state="translated">[str], a non-empty list of strings with the possible values for the flag.</target>
        </trans-unit>
        <trans-unit id="7dbf8f891a3cf1f1fcd98f054581759232646314" translate="yes" xml:space="preserve">
          <source>[str], names of the flags.</source>
          <target state="translated">[str], names of the flags.</target>
        </trans-unit>
        <trans-unit id="0305e4e8314312d7aab76df54a2e11f81f096064" translate="yes" xml:space="preserve">
          <source>[str], the parsed flag value.</source>
          <target state="translated">[str], 파싱 된 플래그 값.</target>
        </trans-unit>
        <trans-unit id="06a4ee1356819500657e1855f1838410080d3362" translate="yes" xml:space="preserve">
          <source>\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \).</source>
          <target state="translated">\ (c_ {jklm} = \ sum_i a_ {ijk} b_ {lmi} \).</target>
        </trans-unit>
        <trans-unit id="7c88dd9089928bec50c06f1f5a9b847c1bc1189e" translate="yes" xml:space="preserve">
          <source>\(B(x; a, b) = \int_0^x t^{a-1} (1 - t)^{b-1} dt\)</source>
          <target state="translated">\ (B (x; a, b) = \ int_0 ^ xt ^ {a-1} (1-t) ^ {b-1} dt \)</target>
        </trans-unit>
        <trans-unit id="5dd8170c96a11e0fac54775aa5e03b07d872abb3" translate="yes" xml:space="preserve">
          <source>\(Gamma(a, x) = int_{x}^{\infty} t^{a-1} exp(-t) dt\)</source>
          <target state="translated">\ (감마 (a, x) = int_ {x} ^ {\ infty} t ^ {a-1} exp (-t) dt \)</target>
        </trans-unit>
        <trans-unit id="a4f68edbf74b4238178695327b403faa736fbba4" translate="yes" xml:space="preserve">
          <source>\(I_x(a, b) = \frac{B(x; a, b)}{B(a, b)}\)</source>
          <target state="translated">\ (I_x (a, b) = \ frac {B (x; a, b)} {B (a, b)} \)</target>
        </trans-unit>
        <trans-unit id="9b977890dfd27609aefecb3c22cf9edc2b5b6c7c" translate="yes" xml:space="preserve">
          <source>\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\)</source>
          <target state="translated">\ (P (a, x) = 감마 (a, x) / 감마 (a) = 1-Q (a, x) \)</target>
        </trans-unit>
        <trans-unit id="3c43fa9908d595b3f6dccdce0d125247c097b9f4" translate="yes" xml:space="preserve">
          <source>\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\)</source>
          <target state="translated">\ (Q (a, x) = 감마 (a, x) / 감마 (a) = 1-P (a, x) \)</target>
        </trans-unit>
        <trans-unit id="37291deb0e6026c081430c96a800990fc3edcaa7" translate="yes" xml:space="preserve">
          <source>\(\beta\)</source>
          <target state="translated">\(\beta\)</target>
        </trans-unit>
        <trans-unit id="ff41b6103716ee7c998d6e81784bdf83320ba7f2" translate="yes" xml:space="preserve">
          <source>\(\ell_1\,\,penalty =\ell_1\sum_{i=0}^n|x_i|\)</source>
          <target state="translated">\ (\ ell_1 \, \, 페널티 = \ ell_1 \ sum_ {i = 0} ^ n | x_i | \)</target>
        </trans-unit>
        <trans-unit id="9335deefcc4fff06a8f47ea88b57b74222c6ee2a" translate="yes" xml:space="preserve">
          <source>\(\ell_2\,\,penalty =\ell_2\sum_{i=0}^nx_i^2\)</source>
          <target state="translated">\ (\ ell_2 \, \, 페널티 = \ ell_2 \ sum_ {i = 0} ^ nx_i ^ 2 \)</target>
        </trans-unit>
        <trans-unit id="c62e7aed7aab85e0bc7679b7b1654447c10071b7" translate="yes" xml:space="preserve">
          <source>\(\frac{\gamma(x-\mu)}{\sigma}+\beta\)</source>
          <target state="translated">\(\frac{\gamma(x-\mu)}{\sigma}+\beta\)</target>
        </trans-unit>
        <trans-unit id="4a0660a89253bc5ba688c6d45badc9f8ddf6380e" translate="yes" xml:space="preserve">
          <source>\(\psi^{(a)}(x) = \frac{d^a}{dx^a} \psi(x)\)</source>
          <target state="translated">\ (\ psi ^ {(a)} (x) = \ frac {d ^ a} {dx ^ a} \ psi (x) \)</target>
        </trans-unit>
        <trans-unit id="57cfcb3ce0118881ab066bb534ef79062b9ae613" translate="yes" xml:space="preserve">
          <source>\(\sigma_{t,i} = (\sqrt{n_{t,i}} - \sqrt{n_{t-1,i}}) / \alpha\)</source>
          <target state="translated">\ (\ sigma_ {t, i} = (\ sqrt {n_ {t, i}}-\ sqrt {n_ {t-1, i}}) / \ alpha \)</target>
        </trans-unit>
        <trans-unit id="c402d7a692489cd97ce624829b28b4af2556f395" translate="yes" xml:space="preserve">
          <source>\(\zeta(x, q) = \sum_{n=0}^{\infty} (q + n)^{-x}\)</source>
          <target state="translated">\ (\ zeta (x, q) = \ sum_ {n = 0} ^ {\ infty} (q + n) ^ {-x} \)</target>
        </trans-unit>
        <trans-unit id="ea1119f555233a8d758e9686e809fbc51e48a520" translate="yes" xml:space="preserve">
          <source>\(gamma(a, x) = \\int_{0}^{x} t^{a-1} exp(-t) dt\)</source>
          <target state="translated">\ (감마 (a, x) = \\ int_ {0} ^ {x} t ^ {a-1} exp (-t) dt \)</target>
        </trans-unit>
        <trans-unit id="553f6c65213b82afcc0c043d8233d0d411b11d70" translate="yes" xml:space="preserve">
          <source>\(i\)</source>
          <target state="translated">\(i\)</target>
        </trans-unit>
        <trans-unit id="cad97ed2e69a3cdad8082dd06f3ff6c16e2928db" translate="yes" xml:space="preserve">
          <source>\(lbeta(x)[i1, ..., in] = Log(|Beta(x[i1, ..., in, :])|)\)</source>
          <target state="translated">\ (lbeta (x) [i1, ..., in] = Log (| 베타 (x [i1, ..., in, :])]) \)</target>
        </trans-unit>
        <trans-unit id="e122b351d32a233056a0dc92eef090ec82833970" translate="yes" xml:space="preserve">
          <source>\(log(exp(A)) = A\)</source>
          <target state="translated">\ (log (exp (A)) = A \)</target>
        </trans-unit>
        <trans-unit id="a6e6475c1d10a33b250fca653ee8cfd040c5e589" translate="yes" xml:space="preserve">
          <source>\(lr_t := \text{learning\_rate} * \sqrt{1 - beta_2^t} / (1 - beta_1^t)\)</source>
          <target state="translated">\ (lr_t : = \ text {learning \ _rate} * \ sqrt {1-beta_2 ^ t} / (1-beta_1 ^ t) \)</target>
        </trans-unit>
        <trans-unit id="9f6d1fd07abb052ec6fac4e52945c2c9c481cefa" translate="yes" xml:space="preserve">
          <source>\(m_0 := 0 \text{(Initialize initial 1st moment vector)}\)</source>
          <target state="translated">\ (m_0 : = 0 \ text {(초기 1 차 모멘트 벡터 초기화)} \)</target>
        </trans-unit>
        <trans-unit id="e3dada6e7ecd7d65376b81a16036f8e723616d85" translate="yes" xml:space="preserve">
          <source>\(m_t := beta_1 * m_{t-1} + (1 - beta_1) * g\)</source>
          <target state="translated">\ (m_t : = beta_1 * m_ {t-1} + (1-beta_1) * g \)</target>
        </trans-unit>
        <trans-unit id="36cf9049b0822fc2658dd5393471d4a9bc14521f" translate="yes" xml:space="preserve">
          <source>\(n_{t,i} = n_{t-1,i} + g_{t,i}^{2}\)</source>
          <target state="translated">\ (n_ {t, i} = n_ {t-1, i} + g_ {t, i} ^ {2} \)</target>
        </trans-unit>
        <trans-unit id="e58cad3c2a2e7d2a68a0cec41e11be1d3da882e6" translate="yes" xml:space="preserve">
          <source>\(output_i = 1/N_i \sum_{j...} data[j...]\) where the sum is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; with \N_i\ being the number of occurrences of id \i\.</source>
          <target state="translated">합 이상의 튜플이다 \ (output_i = 1 / N_i \ sum_ {...} J 데이터 [J ...] \) &lt;code&gt;j...&lt;/code&gt; 되도록 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; \ N_i와 \ id \ i \의 발생 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="eff44ad7c5ba3196137c591dc7d87b8984229557" translate="yes" xml:space="preserve">
          <source>\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\) where the sum is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; with \N_i\ being the number of occurrences of id \i\.</source>
          <target state="translated">\ (output_i = 1 / SQRT (N_i) \ sum_ {...} J 데이터 [J ...] \) 합계 위에 튜플이다 &lt;code&gt;j...&lt;/code&gt; 되도록 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 와 \ N_i \는 id \ i \의 발생 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="fd27c9aa6e45dbe63e023f46fe5fdca3d93fbc19" translate="yes" xml:space="preserve">
          <source>\(output_i = \max_{j...} data[j...]\) where max is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">\ (output_i = \ max_ {j ...} data [j ...] \) 여기서 max는 튜플 &lt;code&gt;j...&lt;/code&gt; 초과 하여 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="529b95e92276e5f1ac9b067c9474b5798a2da7c5" translate="yes" xml:space="preserve">
          <source>\(output_i = \min_{j...} data_[j...]\) where min is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">\ (output_i = \ min_ {j ...} data_ [j ...] \) 여기서 min은 튜플 &lt;code&gt;j...&lt;/code&gt; 초과 하여 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="1bf71665be24061201465110b49af7b8c6429f1f" translate="yes" xml:space="preserve">
          <source>\(output_i = \prod_{j...} data[j...]\) where the product is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">제품 위에 튜플이다 \ (output_i = \ prod_ J {...} 데이터 [J ...] \) &lt;code&gt;j...&lt;/code&gt; 되도록 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="bcf88899f36fe32859ba4eebac881718864b3a68" translate="yes" xml:space="preserve">
          <source>\(predictions_i\) be the predictions for all classes for example &lt;code&gt;i&lt;/code&gt;, \(targets_i\) be the target class for example &lt;code&gt;i&lt;/code&gt;, \(out_i\) be the output for example &lt;code&gt;i&lt;/code&gt;,</source>
          <target state="translated">\ (predictions_i \)는 모든 클래스 (예 : &lt;code&gt;i&lt;/code&gt; )에 대한 예측 이고 \ (targets_i \)는 예를 들어 &lt;code&gt;i&lt;/code&gt; 의 대상 클래스 이고 \ (out_i \)는 예를 들어 &lt;code&gt;i&lt;/code&gt; 의 출력입니다 .</target>
        </trans-unit>
        <trans-unit id="eb6c03745499b6a9eb903031a775345057a8d436" translate="yes" xml:space="preserve">
          <source>\(t := 0 \text{(Initialize timestep)}\)</source>
          <target state="translated">\ (t : = 0 \ text {(초기화 단계)} \)</target>
        </trans-unit>
        <trans-unit id="467d92f4cfe1c3cd183f798d119891376eebfa6b" translate="yes" xml:space="preserve">
          <source>\(t := t + 1\)</source>
          <target state="translated">\ (t : = t + 1 \)</target>
        </trans-unit>
        <trans-unit id="e5334cf7b7c8be8a554d2684840ca9468e2d3597" translate="yes" xml:space="preserve">
          <source>\(t = t + 1\)</source>
          <target state="translated">\ (t = t + 1 \)</target>
        </trans-unit>
        <trans-unit id="1a4815b823d6a6bd996f5174a896a936100c82c4" translate="yes" xml:space="preserve">
          <source>\(v_0 := 0 \text{(Initialize initial 2nd moment vector)}\)</source>
          <target state="translated">\ (v_0 : = 0 \ text {(초기 2 차 모멘트 벡터 초기화)} \)</target>
        </trans-unit>
        <trans-unit id="fe2806cab7064b169c451375ea09662f2387835c" translate="yes" xml:space="preserve">
          <source>\(v_hat_0 := 0 \text{(Initialize initial 2nd moment vector)}\)</source>
          <target state="translated">\ (v_hat_0 : = 0 \ text {(초기 2 차 모멘트 벡터 초기화)} \)</target>
        </trans-unit>
        <trans-unit id="95246ed68fccf6f4caddd1734e3779b6b21f4b80" translate="yes" xml:space="preserve">
          <source>\(v_hat_t := max(v_hat_{t-1}, v_t)\)</source>
          <target state="translated">\ (v_hat_t : = max (v_hat_ {t-1}, v_t) \)</target>
        </trans-unit>
        <trans-unit id="4f4d3c48642e5d2288a5d23ba51224f14bb68e11" translate="yes" xml:space="preserve">
          <source>\(v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g\)</source>
          <target state="translated">\ (v_t : = beta_2 * v_ {t-1} + (1-beta_2) * g * g \)</target>
        </trans-unit>
        <trans-unit id="ceb29146344acce0db4068ea0c682c74bad08207" translate="yes" xml:space="preserve">
          <source>\(variable := variable - lr_t * m_t / (\sqrt{v_hat_t} + \epsilon)\)</source>
          <target state="translated">\ (변수 : = 변수-lr_t * m_t / (\ sqrt {v_hat_t} + \ epsilon) \)</target>
        </trans-unit>
        <trans-unit id="2a5065284239af79ffad5fd4634aca805131acd5" translate="yes" xml:space="preserve">
          <source>\(variable := variable - lr_t * m_t / (\sqrt{v_t} + \epsilon)\)</source>
          <target state="translated">\ (변수 : = 변수-lr_t * m_t / (\ sqrt {v_t} + \ epsilon) \)</target>
        </trans-unit>
        <trans-unit id="9c42d66b235e7446c048f4defb1b795d3ee38b09" translate="yes" xml:space="preserve">
          <source>\(w_{i}\)</source>
          <target state="translated">\(w_{i}\)</target>
        </trans-unit>
        <trans-unit id="3b25b3312f2ef7cacd25276352e73b5ab48846cf" translate="yes" xml:space="preserve">
          <source>\(w_{t,i} = - ((\beta+\sqrt{n+{t}}) / \alpha + \lambda_{2})^{-1} * (z_{i} - sgn(z_{i}) * \lambda_{1}) if \abs{z_{i}} &amp;gt; \lambda_{i} else 0\)</source>
          <target state="translated">\ (w_ {t, i} =-((\ beta + \ sqrt {n + {t}}) / \ alpha + \ lambda_ {2}) ^ {-1} * (z_ {i}-sgn (z_ {i }) * \ lambda_ {1}) \ abs {z_ {i}}&amp;gt; \ lambda_ {i} 그렇지 않으면 0 \)</target>
        </trans-unit>
        <trans-unit id="ba90f7c8012e09317d8c902b37294fee9c1c8163" translate="yes" xml:space="preserve">
          <source>\(y = \beta + \sum_{i=1}^{N} w_{i} * x_{i}\)</source>
          <target state="translated">\ (y = \ beta + \ sum_ {i = 1} ^ {N} w_ {i} * x_ {i} \)</target>
        </trans-unit>
        <trans-unit id="2dec4db0fcb32e5c79e8ecb6c6414661d0289001" translate="yes" xml:space="preserve">
          <source>\(z_{t,i} = z_{t-1,i} + g_{t,i} - \sigma_{t,i} * w_{t,i}\)</source>
          <target state="translated">\ (z_ {t, i} = z_ {t-1, i} + g_ {t, i}-\ sigma_ {t, i} * w_ {t, i} \)</target>
        </trans-unit>
        <trans-unit id="31d5b9df6c8b26532e6975198879e8066c930cd4" translate="yes" xml:space="preserve">
          <source>], 'bias': [</source>
          <target state="translated">], '편견': [</target>
        </trans-unit>
        <trans-unit id="ebb0f535eb870cf683880d2973390bc12e206de5" translate="yes" xml:space="preserve">
          <source>], _NumericColumn( key='numeric_feature2', shape=(2,)): [</source>
          <target state="translated">], _NumericColumn (키 = '숫자 2', 모양 = (2,)) : [</target>
        </trans-unit>
        <trans-unit id="a961fbc1cc31eed81a5be94725a3b10dfcfb3f0e" translate="yes" xml:space="preserve">
          <source>]} If a column creates no variables, its value will be an empty list. Note that cols_to_vars will also contain a string key 'bias' that maps to a list of Variables.</source>
          <target state="translated">]} 열이 변수를 만들지 않으면 값이 빈 목록이됩니다. cols_to_vars에는 변수 목록에 매핑되는 문자열 키 'bias'도 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="82253180c6e96af25f2a21fff7bcfa0218b5a1e9" translate="yes" xml:space="preserve">
          <source>_normal_initializer</source>
          <target state="translated">_normal_initializer</target>
        </trans-unit>
        <trans-unit id="87ea43bbd5b9352fbaeea30b2cc056ed63741cfd" translate="yes" xml:space="preserve">
          <source>_uniform_initializer</source>
          <target state="translated">_uniform_initializer</target>
        </trans-unit>
        <trans-unit id="d5a25e2ec3739e3d8ae17e7c5a3f6cbea4f5abd6" translate="yes" xml:space="preserve">
          <source>a (major,minor) pair that indicates the minimum CUDA compute capability required, or None if no requirement.</source>
          <target state="translated">a (major,minor) pair that indicates the minimum CUDA compute capability required, or None if no requirement.</target>
        </trans-unit>
        <trans-unit id="dd7aed71c0916f4e514b1b87472e1aaec7a3d81c" translate="yes" xml:space="preserve">
          <source>a 1-D numpy array whose size depends on the algorithm.</source>
          <target state="translated">a 1-D numpy array whose size depends on the algorithm.</target>
        </trans-unit>
        <trans-unit id="09cc8a0e53d6d87d6a7d95e6a2118c05a24fa639" translate="yes" xml:space="preserve">
          <source>a 1-D tensor whose size depends on the algorithm.</source>
          <target state="translated">크기가 알고리즘에 의존하는 1 차원 텐서.</target>
        </trans-unit>
        <trans-unit id="93178b091b08f4efbffb2adcc54e5c4fc936daed" translate="yes" xml:space="preserve">
          <source>a 1D tensor. Dimensions: out_units</source>
          <target state="translated">a 1D tensor. Dimensions: out_units</target>
        </trans-unit>
        <trans-unit id="60f13d546f38be6d76d9d2976a24c54009d2eb5a" translate="yes" xml:space="preserve">
          <source>a 2D tensor. Dimensions typically: batch, in_units</source>
          <target state="translated">a 2D tensor. Dimensions typically: batch, in_units</target>
        </trans-unit>
        <trans-unit id="abdaa4ef567c5801d72513ba2ff400234446d877" translate="yes" xml:space="preserve">
          <source>a 2D tensor. Dimensions typically: in_units, out_units</source>
          <target state="translated">a 2D tensor. Dimensions typically: in_units, out_units</target>
        </trans-unit>
        <trans-unit id="0832663c1e8c4bc4a737a50ba43d01171dfb1b52" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; whose elements are to be written to a file</source>
          <target state="translated">a &lt;a href=&quot;../dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; whose elements are to be written to a file</target>
        </trans-unit>
        <trans-unit id="63da673dc60479d55d81b4dac49f99e2addfa764" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, its output must match the output of the linear model.</source>
          <target state="translated">a &lt;a href=&quot;../model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;, its output must match the output of the linear model.</target>
        </trans-unit>
        <trans-unit id="23a2b4256e67b7643d6e462770d9d1ceff2c6a95" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;layer&quot;&gt;&lt;code&gt;tf.keras.layers.Layer&lt;/code&gt;&lt;/a&gt; instance.</source>
          <target state="translated">a &lt;a href=&quot;layer&quot;&gt; &lt;code&gt;tf.keras.layers.Layer&lt;/code&gt; &lt;/a&gt; instance.</target>
        </trans-unit>
        <trans-unit id="9b7b7f1e42a3452df01c243bc4ff9f9c0a1e957a" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt; to merge with</source>
          <target state="translated">a &lt;a href=&quot;options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt; to merge with</target>
        </trans-unit>
        <trans-unit id="fc2f84a127d1432687ba858bc82ffee5c3694cbb" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;DeviceSpec&lt;/code&gt;</source>
          <target state="translated">a &lt;code&gt;DeviceSpec&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="532617217fec8780a205ebf785c1bcfcd14f51e0" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;DeviceSpec&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;DeviceSpec&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7a0723a300b3e5a3037c5eec9b6314d294f925e8" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SavedModel&lt;/code&gt; proto containing the Tensorflow backend graph. Separate graphs are saved for prediction (serving), train, and evaluation. If the model has not been compiled, then only the graph computing predictions will be exported.</source>
          <target state="translated">&lt;code&gt;SavedModel&lt;/code&gt; 백엔드 그래프를 포함 하는 저장된 모델 프로토. 예측 (서빙), 훈련 및 평가를 위해 별도의 그래프가 저장됩니다. 모델이 컴파일되지 않은 경우 그래프 컴퓨팅 예측 만 내보내집니다.</target>
        </trans-unit>
        <trans-unit id="2fe7e88159041491b684ee0b2eef3649a559ee4b" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SaverDef&lt;/code&gt; protocol buffer.</source>
          <target state="translated">a &lt;code&gt;SaverDef&lt;/code&gt; protocol buffer.</target>
        </trans-unit>
        <trans-unit id="fe9334150fba500e634d08a0fd10f09c0e5c3458" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SparseTensor&lt;/code&gt; operand whose dtype is real, and indices lexicographically ordered.</source>
          <target state="translated">a &lt;code&gt;SparseTensor&lt;/code&gt; operand whose dtype is real, and indices lexicographically ordered.</target>
        </trans-unit>
        <trans-unit id="89fda5dd98ad8f5391967d857c89af91abadfb1e" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;sample_shape(x) + self.batch_shape&lt;/code&gt; with values of type &lt;code&gt;self.dtype&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;sample_shape(x) + self.batch_shape&lt;/code&gt; with values of type &lt;code&gt;self.dtype&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="814d7841b7f08b3882cd124daea00a4d94c3529b" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(alpha + beta)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(alpha + beta)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="28f44ad32a3986701c510c4b3dcbe94a32df1ffe" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(lam)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(lam)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7a5aa938284d01a870254921f425995a35654dd9" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; with prepended dimensions &lt;code&gt;sample_shape&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;Tensor&lt;/code&gt; with prepended dimensions &lt;code&gt;sample_shape&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e4c4f77ba88c28c41bf33a3d3025e3b1e6cda5e8" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt;, or a dict of string to &lt;code&gt;Tensor&lt;/code&gt;, specifying input nodes that will be fed.</source>
          <target state="translated">a &lt;code&gt;Tensor&lt;/code&gt; , or a dict of string to &lt;code&gt;Tensor&lt;/code&gt; , specifying input nodes that will be fed.</target>
        </trans-unit>
        <trans-unit id="cfb18628ed3de27b4a1a234981f499e9dd433d38" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;str&lt;/code&gt; describing the contraction, in the same format as &lt;code&gt;numpy.einsum&lt;/code&gt;.</source>
          <target state="translated">a &lt;code&gt;str&lt;/code&gt; describing the contraction, in the same format as &lt;code&gt;numpy.einsum&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a4f5c14667db94a7f69f7b3c58a35864d1d15ebd" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tf.ConfigProto&lt;/code&gt; object.</source>
          <target state="translated">a &lt;code&gt;tf.ConfigProto&lt;/code&gt; object.</target>
        </trans-unit>
        <trans-unit id="fd5e709c8e1b6963fd07bdd51f6a8e7d5b1a115e" translate="yes" xml:space="preserve">
          <source>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) a # 2-D tensor</source>
          <target state="translated">a = tf.constant ([1, 2, 3, 4, 5, 6], shape = [2, 3]) a # 2 차원 텐서</target>
        </trans-unit>
        <trans-unit id="d3508e6212f2bb1a5bb507a6ad9f56dc1d60653b" translate="yes" xml:space="preserve">
          <source>a = tf.constant([[1, 2], [3, 4]]) tf.reduce_min(a)</source>
          <target state="translated">a = tf.constant([[1, 2], [3, 4]]) tf.reduce_min(a)</target>
        </trans-unit>
        <trans-unit id="b60dce959a5722be8d9ace37f6ea58387cc7a978" translate="yes" xml:space="preserve">
          <source>a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]) a # 3-D tensor</source>
          <target state="translated">a = tf.constant (np.arange (1, 13, dtype = np.int32), shape = [2, 2, 3]) a # 3 차원 텐서</target>
        </trans-unit>
        <trans-unit id="d5554b43b2c1e709f84d89ff59708b34ac43de5d" translate="yes" xml:space="preserve">
          <source>a ClusterResolver</source>
          <target state="translated">a ClusterResolver</target>
        </trans-unit>
        <trans-unit id="12fec67c34e62bc4fe609123ff4e68b7b09d5716" translate="yes" xml:space="preserve">
          <source>a ConfigProto used to set session parameters, or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">a ConfigProto used to set session parameters, or &lt;code&gt;None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cac282a286b145e90bbae0cb74e2d64c49b7f387" translate="yes" xml:space="preserve">
          <source>a GraphNodeProto that records the results.</source>
          <target state="translated">결과를 기록하는 GraphNodeProto</target>
        </trans-unit>
        <trans-unit id="02fdc18cf71e7d138c4670ddd2b89252810f0b7c" translate="yes" xml:space="preserve">
          <source>a Mirrored object.</source>
          <target state="translated">미러 된 객체</target>
        </trans-unit>
        <trans-unit id="6a65d1b63a2aa1329f486140387a944abe2d35ab" translate="yes" xml:space="preserve">
          <source>a MultiGraphNodeProto that records the results.</source>
          <target state="translated">결과를 기록하는 MultiGraphNodeProto</target>
        </trans-unit>
        <trans-unit id="3f756fe3a0f877dbf55d583613842de50b650bca" translate="yes" xml:space="preserve">
          <source>a Tensor or list of Tensors.</source>
          <target state="translated">a Tensor or list of Tensors.</target>
        </trans-unit>
        <trans-unit id="44a87c17dbe409e98a432e4764a0ca67503ad529" translate="yes" xml:space="preserve">
          <source>a TrtConversionParams instance.</source>
          <target state="translated">a TrtConversionParams instance.</target>
        </trans-unit>
        <trans-unit id="6adb18be2c65016a8dd91dcca0917351f5c5ea50" translate="yes" xml:space="preserve">
          <source>a boolean indicating whether the input boxes and scores are sorted in descending order by the score.</source>
          <target state="translated">a boolean indicating whether the input boxes and scores are sorted in descending order by the score.</target>
        </trans-unit>
        <trans-unit id="ae8444bacbe1b98ca2a1d9caa1bde03aa7aa8447" translate="yes" xml:space="preserve">
          <source>a callable taking two parameters, a variable and a list of slot names to create for it. This function should return a dict with the slot names as keys and the created variables as values. When set to None (the default), uses the built-in variable creation.</source>
          <target state="translated">a callable taking two parameters, a variable and a list of slot names to create for it. This function should return a dict with the slot names as keys and the created variables as values. When set to None (the default), uses the built-in variable creation.</target>
        </trans-unit>
        <trans-unit id="7b5462387fade93b713b97cb5dd916b0db334352" translate="yes" xml:space="preserve">
          <source>a callable that takes a single &lt;code&gt;DType&lt;/code&gt; argument and returns a Python &lt;code&gt;boolean&lt;/code&gt; indicating whether the dtype is to be included in the data dumping. Examples:</source>
          <target state="translated">단일 &lt;code&gt;DType&lt;/code&gt; 인수를 사용 하여 dtype이 데이터 덤핑에 포함되는지 여부를 나타내는 Python &lt;code&gt;boolean&lt;/code&gt; 리턴 하는 호출 가능 . 예 :</target>
        </trans-unit>
        <trans-unit id="dadf9b05790e565e376db89427e055d3cb91682a" translate="yes" xml:space="preserve">
          <source>a checkpoint containing the model weights.</source>
          <target state="translated">모델 가중치를 포함하는 체크 포인트</target>
        </trans-unit>
        <trans-unit id="7cbfb1c56d61348ae16c7cd2face5f9db9cab352" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">a dict of string to &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6b740bb539e2724c4812a91601e69e1c90a91fbd" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">a dict of string to &lt;code&gt;Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="79c49bc5c338207d05434ecb9d28173a38bb6f68" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;VarLenFeature&lt;/code&gt;/&lt;code&gt;FixedLenFeature&lt;/code&gt;.</source>
          <target state="translated">a dict of string to &lt;code&gt;VarLenFeature&lt;/code&gt; / &lt;code&gt;FixedLenFeature&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="813fdedf351b9814fc880a7e57d9c99855ec1cd5" translate="yes" xml:space="preserve">
          <source>a dict of string to additional groups of receiver tensors, each of which may be a &lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;SparseTensor&lt;/code&gt;, or dict of string to &lt;code&gt;Tensor&lt;/code&gt; or&lt;code&gt;SparseTensor&lt;/code&gt;. These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature &lt;code&gt;Tensor&lt;/code&gt;s &lt;em&gt;downstream&lt;/em&gt; of the tf.parse_example() op. Defaults to None.</source>
          <target state="translated">a dict of string to additional groups of receiver tensors, each of which may be a &lt;code&gt;Tensor&lt;/code&gt; , &lt;code&gt;SparseTensor&lt;/code&gt; , or dict of string to &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;SparseTensor&lt;/code&gt; . These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature &lt;code&gt;Tensor&lt;/code&gt; s &lt;em&gt;downstream&lt;/em&gt; of the tf.parse_example() op. Defaults to None.</target>
        </trans-unit>
        <trans-unit id="7bd9e9cef7c898d18e42b1f2f9c5375410bd03be" translate="yes" xml:space="preserve">
          <source>a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;Here are details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</source>
          <target state="translated">a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;Here are details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</target>
        </trans-unit>
        <trans-unit id="8c591343285301a08eb1eb57b9c63187b713f4d7" translate="yes" xml:space="preserve">
          <source>a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</source>
          <target state="translated">a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</target>
        </trans-unit>
        <trans-unit id="8527fc8500abdc7bc32e70d3d9218debeaf5253f" translate="yes" xml:space="preserve">
          <source>a feature with &lt;code&gt;key=column.name&lt;/code&gt; whose &lt;code&gt;value&lt;/code&gt; is a &lt;code&gt;SparseTensor&lt;/code&gt;.</source>
          <target state="translated">으로 기능 &lt;code&gt;key=column.name&lt;/code&gt; &lt;code&gt;value&lt;/code&gt; A는 &lt;code&gt;SparseTensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ded58018cf323ca8b228f31fb7eeecbe3e4bab57" translate="yes" xml:space="preserve">
          <source>a float &lt;code&gt;Tensor&lt;/code&gt; giving the predicted values. Required.</source>
          <target state="translated">a float &lt;code&gt;Tensor&lt;/code&gt; giving the predicted values. Required.</target>
        </trans-unit>
        <trans-unit id="ca1e3709c53c6fef2ab454a19e61ce4ffd76640f" translate="yes" xml:space="preserve">
          <source>a float &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">a float &lt;code&gt;Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8fa1eefb32219e9ccd295492ff2de693d41b5058" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of 2pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. A positive values means rotating counter clock-wise, while a negative value means clock-wise. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;factor=(-0.2, 0.3)&lt;/code&gt; results in an output rotation by a random amount in the range &lt;code&gt;[-20% * 2pi, 30% * 2pi]&lt;/code&gt;. &lt;code&gt;factor=0.2&lt;/code&gt; results in an output rotating by a random amount in the range &lt;code&gt;[-20% * 2pi, 20% * 2pi]&lt;/code&gt;.</source>
          <target state="translated">a float represented as fraction of 2pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. A positive values means rotating counter clock-wise, while a negative value means clock-wise. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;factor=(-0.2, 0.3)&lt;/code&gt; results in an output rotation by a random amount in the range &lt;code&gt;[-20% * 2pi, 30% * 2pi]&lt;/code&gt; . &lt;code&gt;factor=0.2&lt;/code&gt; results in an output rotating by a random amount in the range &lt;code&gt;[-20% * 2pi, 20% * 2pi]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7f704c1b4eddb94e2f57ea74f007a87f33d697ca" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting horizontally. A negative value means shifting image left, while a positive value means shifting image right. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted left by 20%, and shifted right by 30%. &lt;code&gt;width_factor=0.2&lt;/code&gt; results in an output height shifted left or right by 20%.</source>
          <target state="translated">a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting horizontally. A negative value means shifting image left, while a positive value means shifting image right. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted left by 20%, and shifted right by 30%. &lt;code&gt;width_factor=0.2&lt;/code&gt; results in an output height shifted left or right by 20%.</target>
        </trans-unit>
        <trans-unit id="91c973265bac7ff051653d0165345d52a97d0e23" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting vertically. A negative value means shifting image up, while a positive value means shifting image down. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;height_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted by a random amount in the range [-20%, +30%]. &lt;code&gt;height_factor=0.2&lt;/code&gt; results in an output height shifted by a random amount in the range [-20%, +20%].</source>
          <target state="translated">a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting vertically. A negative value means shifting image up, while a positive value means shifting image down. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;height_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted by a random amount in the range [-20%, +30%]. &lt;code&gt;height_factor=0.2&lt;/code&gt; results in an output height shifted by a random amount in the range [-20%, +20%].</target>
        </trans-unit>
        <trans-unit id="7049c0b7f85f9f5d90a665c9a2c18068cc1f50bb" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming horizontally. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(0.2, 0.3)&lt;/code&gt; result in an output zooming out between 20% to 30%. &lt;code&gt;width_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zooming in between 20% to 30%. Defaults to &lt;code&gt;None&lt;/code&gt;, i.e., zooming vertical and horizontal directions by preserving the aspect ratio.</source>
          <target state="translated">a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming horizontally. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(0.2, 0.3)&lt;/code&gt; result in an output zooming out between 20% to 30%. &lt;code&gt;width_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zooming in between 20% to 30%. Defaults to &lt;code&gt;None&lt;/code&gt; , i.e., zooming vertical and horizontal directions by preserving the aspect ratio.</target>
        </trans-unit>
        <trans-unit id="2ec50d28cf2adbe8d493bb972ad6ae6b32140707" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming vertically. When represented as a single float, this value is used for both the upper and lower bound. A positive value means zooming out, while a negative value means zooming in. For instance, &lt;code&gt;height_factor=(0.2, 0.3)&lt;/code&gt; result in an output zoomed out by a random amount in the range [+20%, +30%]. &lt;code&gt;height_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zoomed in by a random amount in the range [+20%, +30%].</source>
          <target state="translated">a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming vertically. When represented as a single float, this value is used for both the upper and lower bound. A positive value means zooming out, while a negative value means zooming in. For instance, &lt;code&gt;height_factor=(0.2, 0.3)&lt;/code&gt; result in an output zoomed out by a random amount in the range [+20%, +30%]. &lt;code&gt;height_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zoomed in by a random amount in the range [+20%, +30%].</target>
        </trans-unit>
        <trans-unit id="134107ca0c019b55908b80f4021d2430226ea2bd" translate="yes" xml:space="preserve">
          <source>a float representing the threshold for box scores. Boxes with a score that is not larger than this threshold will be suppressed.</source>
          <target state="translated">a float representing the threshold for box scores. Boxes with a score that is not larger than this threshold will be suppressed.</target>
        </trans-unit>
        <trans-unit id="5f94882e30fda2ab68c451ff7e157ffcd9c9395b" translate="yes" xml:space="preserve">
          <source>a float representing the threshold for deciding whether boxes overlap too much with respect to IoU (intersection over union).</source>
          <target state="translated">a float representing the threshold for deciding whether boxes overlap too much with respect to IoU (intersection over union).</target>
        </trans-unit>
        <trans-unit id="eb70148942a0d807c9b8eab06234b1263dd9f73d" translate="yes" xml:space="preserve">
          <source>a float value.</source>
          <target state="translated">a float value.</target>
        </trans-unit>
        <trans-unit id="95a34729e004ba21708276447a0c52f66f075797" translate="yes" xml:space="preserve">
          <source>a float. The maximum absolute difference allowed.</source>
          <target state="translated">a float. The maximum absolute difference allowed.</target>
        </trans-unit>
        <trans-unit id="8cade3d579ad24100c211eef3761440915ebda65" translate="yes" xml:space="preserve">
          <source>a floating point value. The learning rate.</source>
          <target state="translated">a floating point value. The learning rate.</target>
        </trans-unit>
        <trans-unit id="96653b90a10cbc5bd2bee44ca5eac4e329ab5a1e" translate="yes" xml:space="preserve">
          <source>a function that compares two evaluation results and returns true if current evaluation result is better. Follows the signature:</source>
          <target state="translated">a function that compares two evaluation results and returns true if current evaluation result is better. Follows the signature:</target>
        </trans-unit>
        <trans-unit id="a9a62fd00059c46a517d9a5d09090fa30f1d118d" translate="yes" xml:space="preserve">
          <source>a function that does accumulation. If None, then &lt;a href=&quot;../math/add_n&quot;&gt;&lt;code&gt;tf.math.add_n&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">a function that does accumulation. If None, then &lt;a href=&quot;../math/add_n&quot;&gt; &lt;code&gt;tf.math.add_n&lt;/code&gt; &lt;/a&gt; is used.</target>
        </trans-unit>
        <trans-unit id="6e212799b3b3be2bf8bb202585b8b934660586c8" translate="yes" xml:space="preserve">
          <source>a function that takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).</source>
          <target state="translated">a function that takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).</target>
        </trans-unit>
        <trans-unit id="e12532b57f71320a99fe67d455119ce2516227a2" translate="yes" xml:space="preserve">
          <source>a function that takes no arguments and returns a &lt;code&gt;ServingInputReceiver&lt;/code&gt;.</source>
          <target state="translated">a function that takes no arguments and returns a &lt;code&gt;ServingInputReceiver&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="1bd914955f1635d519f5b5015a7943bdbe1e789c" translate="yes" xml:space="preserve">
          <source>a generator function that yields input data as a list or tuple, which will be used to execute the converted signature for calibration. All the returned input data should have the same shape. Example: &lt;code&gt;def input_fn(): yield input1, input2, input3&lt;/code&gt;</source>
          <target state="translated">a generator function that yields input data as a list or tuple, which will be used to execute the converted signature for calibration. All the returned input data should have the same shape. Example: &lt;code&gt;def input_fn(): yield input1, input2, input3&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b35ea09d187677febf5398dd21d3f485aef04a83" translate="yes" xml:space="preserve">
          <source>a generator function that yields input data as a list or tuple, which will be used to execute the converted signature to generate TRT engines. Example: `def input_fn():</source>
          <target state="translated">a generator function that yields input data as a list or tuple, which will be used to execute the converted signature to generate TRT engines. Example: `def input_fn():</target>
        </trans-unit>
        <trans-unit id="1862cc4d53bc4e8a702743354cc37627a768a2f8" translate="yes" xml:space="preserve">
          <source>a generator function which yields data</source>
          <target state="translated">a generator function which yields data</target>
        </trans-unit>
        <trans-unit id="98c108f990a30bbb49abae38f009c82ac5f3a790" translate="yes" xml:space="preserve">
          <source>a generator to be copied from.</source>
          <target state="translated">a generator to be copied from.</target>
        </trans-unit>
        <trans-unit id="420880ced69a6abdc0537ffab2bcb1d1ec757577" translate="yes" xml:space="preserve">
          <source>a handle to defined flag.</source>
          <target state="translated">a handle to defined flag.</target>
        </trans-unit>
        <trans-unit id="e9fc2e7b4a4a43e13919ca0d29891f8252c3bdeb" translate="yes" xml:space="preserve">
          <source>a keras.Model instance.</source>
          <target state="translated">keras.Model 인스턴스.</target>
        </trans-unit>
        <trans-unit id="825f328ed6cb35d3a0907c277ea200144eea6b05" translate="yes" xml:space="preserve">
          <source>a list of Mirrored objects.</source>
          <target state="translated">미러링 된 객체의 목록</target>
        </trans-unit>
        <trans-unit id="8af7fc543dbba60d65534412fdeec31fcf0fd35c" translate="yes" xml:space="preserve">
          <source>a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of &lt;code&gt;get_weights&lt;/code&gt;).</source>
          <target state="translated">a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of &lt;code&gt;get_weights&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="64df7eba5e731a4dc70dbacc22e2fd9fd9e1d032" translate="yes" xml:space="preserve">
          <source>a list of checkpoint paths, typically the results of &lt;code&gt;Saver.save()&lt;/code&gt; or those of &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint()&lt;/code&gt;&lt;/a&gt;, regardless of sharded/non-sharded or V1/V2.</source>
          <target state="translated">a list of checkpoint paths, typically the results of &lt;code&gt;Saver.save()&lt;/code&gt; or those of &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt; &lt;code&gt;tf.train.latest_checkpoint()&lt;/code&gt; &lt;/a&gt;, regardless of sharded/non-sharded or V1/V2.</target>
        </trans-unit>
        <trans-unit id="cfa77622762529e62cce47be75b5d7ef8f04b09b" translate="yes" xml:space="preserve">
          <source>a list of checkpoint paths.</source>
          <target state="translated">a list of checkpoint paths.</target>
        </trans-unit>
        <trans-unit id="8652d1441c07c34c2491a5fef88f30f328252a67" translate="yes" xml:space="preserve">
          <source>a list of device strings such as &lt;code&gt;['/gpu:0', '/gpu:1']&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, all available GPUs are used. If no GPUs are found, CPU is used.</source>
          <target state="translated">a list of device strings such as &lt;code&gt;['/gpu:0', '/gpu:1']&lt;/code&gt; . If &lt;code&gt;None&lt;/code&gt; , all available GPUs are used. If no GPUs are found, CPU is used.</target>
        </trans-unit>
        <trans-unit id="5dd923854fce47fe5a5fbe46663ed04995594696" translate="yes" xml:space="preserve">
          <source>a list of float values.</source>
          <target state="translated">a list of float values.</target>
        </trans-unit>
        <trans-unit id="3cae41a96c3b2e908d36cbc657c22fa2a8eb5b02" translate="yes" xml:space="preserve">
          <source>a list of gradients, one for each element of target. Defaults to None.</source>
          <target state="translated">a list of gradients, one for each element of target. Defaults to None.</target>
        </trans-unit>
        <trans-unit id="ca7a0932035e5c9de07f92110d3540ab4ff578d3" translate="yes" xml:space="preserve">
          <source>a list of loss tensors.</source>
          <target state="translated">손실 텐서 목록.</target>
        </trans-unit>
        <trans-unit id="8466476c20c2b4575b81f48bbef14bb81277fd56" translate="yes" xml:space="preserve">
          <source>a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched.</source>
          <target state="translated">a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched.</target>
        </trans-unit>
        <trans-unit id="3f3cccd7e469ea448da74293963e6a99fb093741" translate="yes" xml:space="preserve">
          <source>a list of prediction keys. Key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.LOGITS or 'logits'.</source>
          <target state="translated">a list of prediction keys. Key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.LOGITS or 'logits'.</target>
        </trans-unit>
        <trans-unit id="fe1e44e58043e3bac62424bc232c0f828907d045" translate="yes" xml:space="preserve">
          <source>a list of tuples &lt;code&gt;(tensor, value)&lt;/code&gt;. &lt;code&gt;value&lt;/code&gt; should be a Numpy array.</source>
          <target state="translated">a list of tuples &lt;code&gt;(tensor, value)&lt;/code&gt; . &lt;code&gt;value&lt;/code&gt; should be a Numpy array.</target>
        </trans-unit>
        <trans-unit id="c2e03752a3549be3776b9abdbd4707084bba9047" translate="yes" xml:space="preserve">
          <source>a list of variables that need to be averaged. Only needed if variable_averages is passed in.</source>
          <target state="translated">a list of variables that need to be averaged. Only needed if variable_averages is passed in.</target>
        </trans-unit>
        <trans-unit id="3eb0f04e7fc6ee40cdb193ac62baee8bb1ca6431" translate="yes" xml:space="preserve">
          <source>a list of variables that require to use of the moving average variable name to be restored. If None, it will default to variables.moving_average_variables() + variables.trainable_variables()</source>
          <target state="translated">a list of variables that require to use of the moving average variable name to be restored. If None, it will default to variables.moving_average_variables() + variables.trainable_variables()</target>
        </trans-unit>
        <trans-unit id="91a77371e3824445e17865c21a8864e288e638bc" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors (or IndexedSlices, or None), one for each element in &lt;code&gt;sources&lt;/code&gt;. Returned structure is the same as the structure of &lt;code&gt;sources&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;sources&lt;/code&gt; 각 요소마다 하나씩 Tensor (또는 IndexedSlices 또는 None)의 목록 또는 중첩 구조 . 반환 된 구조는 &lt;code&gt;sources&lt;/code&gt; 의 구조와 동일 합니다 .</target>
        </trans-unit>
        <trans-unit id="7c876daa0f22003807573109c8a715b9d71802ff" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors or Variables to be differentiated.</source>
          <target state="translated">a list or nested structure of Tensors or Variables to be differentiated.</target>
        </trans-unit>
        <trans-unit id="4276967efbeb15913c139768efe32fe80016a5bd" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors or Variables. &lt;code&gt;target&lt;/code&gt; will be differentiated against elements in &lt;code&gt;sources&lt;/code&gt;.</source>
          <target state="translated">a list or nested structure of Tensors or Variables. &lt;code&gt;target&lt;/code&gt; will be differentiated against elements in &lt;code&gt;sources&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b2b8519688d6e08f0ef3f1f53563b370c82f252c" translate="yes" xml:space="preserve">
          <source>a list or tuple of &lt;code&gt;DType&lt;/code&gt; objects or strings that can be converted to &lt;code&gt;DType&lt;/code&gt; objects via &lt;a href=&quot;../../dtypes/as_dtype&quot;&gt;&lt;code&gt;tf.as_dtype()&lt;/code&gt;&lt;/a&gt;. Examples:</source>
          <target state="translated">목록 또는 튜플 &lt;code&gt;DType&lt;/code&gt; 변환 할 수있는 개체 또는 문자열 &lt;code&gt;DType&lt;/code&gt; 통해 개체 &lt;a href=&quot;../../dtypes/as_dtype&quot;&gt; &lt;code&gt;tf.as_dtype()&lt;/code&gt; &lt;/a&gt; . 예 :</target>
        </trans-unit>
        <trans-unit id="4a6bdafb64b93a495b672af20865dc481b0ed23a" translate="yes" xml:space="preserve">
          <source>a list or tuple of prediction keys. Each key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.CLASSES or 'classes'. If not specified, it will return the predictions for all valid keys.</source>
          <target state="translated">a list or tuple of prediction keys. Each key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.CLASSES or 'classes'. If not specified, it will return the predictions for all valid keys.</target>
        </trans-unit>
        <trans-unit id="1e062962fff519e51826af13e2686d1b25c61f1c" translate="yes" xml:space="preserve">
          <source>a name for the op that creates the writer.</source>
          <target state="translated">a name for the op that creates the writer.</target>
        </trans-unit>
        <trans-unit id="82f9e1f9d52a6496cd6e1fbefeba7b546e34e817" translate="yes" xml:space="preserve">
          <source>a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; behavior.</source>
          <target state="translated">a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; behavior.</target>
        </trans-unit>
        <trans-unit id="ef8e153138ea9a5535d4306bcbaa35a18fc052c8" translate="yes" xml:space="preserve">
          <source>a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; behavior.</source>
          <target state="translated">a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; behavior.</target>
        </trans-unit>
        <trans-unit id="beb20ec17e12b3a92d15870c8fb928b39be9384e" translate="yes" xml:space="preserve">
          <source>a new instance of &lt;code&gt;RunConfig&lt;/code&gt;.</source>
          <target state="translated">의 새로운 인스턴스 &lt;code&gt;RunConfig&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7ebfcefc6e47ac4ed9512e51f768d3cbe5c823cb" translate="yes" xml:space="preserve">
          <source>a numpy array.</source>
          <target state="translated">numpy 배열</target>
        </trans-unit>
        <trans-unit id="583166dd6bf07228d6040841ccf402891050a5e4" translate="yes" xml:space="preserve">
          <source>a numpy ndarray.</source>
          <target state="translated">a numpy ndarray.</target>
        </trans-unit>
        <trans-unit id="d253db5a4761b53c7ffde3781cb4f03e7cb07091" translate="yes" xml:space="preserve">
          <source>a path relative to tensorflow root. e.g. &quot;core/platform&quot;.</source>
          <target state="translated">a path relative to tensorflow root. e.g. &quot;core/platform&quot;.</target>
        </trans-unit>
        <trans-unit id="d434acfa4631c87791b616c82b7e6083ce9a28b9" translate="yes" xml:space="preserve">
          <source>a positive float represented as fraction of value, or a tuple of size 2 representing lower and upper bound. When represented as a single float, lower = upper. The contrast factor will be randomly picked between [1.0 - lower, 1.0 + upper].</source>
          <target state="translated">a positive float represented as fraction of value, or a tuple of size 2 representing lower and upper bound. When represented as a single float, lower = upper. The contrast factor will be randomly picked between [1.0 - lower, 1.0 + upper].</target>
        </trans-unit>
        <trans-unit id="9b17aca81e79c7530a3bc5fdc0659c98cdbd45a6" translate="yes" xml:space="preserve">
          <source>a premade LinearModel, its output must match the output of the dnn model.</source>
          <target state="translated">a premade LinearModel, its output must match the output of the dnn model.</target>
        </trans-unit>
        <trans-unit id="259c33a51ca2823379036c66fbe8b38a134e8892" translate="yes" xml:space="preserve">
          <source>a python scalar or a scalar tensor. Mean of the random values to generate.</source>
          <target state="translated">a python scalar or a scalar tensor. Mean of the random values to generate.</target>
        </trans-unit>
        <trans-unit id="a5d4da2cd944c3f7f7ef6b9224fdc61a3e90e225" translate="yes" xml:space="preserve">
          <source>a python scalar or a scalar tensor. Standard deviation of the random values to generate.</source>
          <target state="translated">a python scalar or a scalar tensor. Standard deviation of the random values to generate.</target>
        </trans-unit>
        <trans-unit id="3267c112f6d4d3c0519780ece91f770b5ea07cc2" translate="yes" xml:space="preserve">
          <source>a rank (N+2) &lt;code&gt;filter&lt;/code&gt; Tensor of shape</source>
          <target state="translated">랭크 (N + 2) &lt;code&gt;filter&lt;/code&gt; 형상 텐서</target>
        </trans-unit>
        <trans-unit id="567bea64cd23153635d3d86fe4dc29e83b999e55" translate="yes" xml:space="preserve">
          <source>a rank (N+2) &lt;code&gt;filters&lt;/code&gt; Tensor of shape</source>
          <target state="translated">랭크 (N + 2) &lt;code&gt;filters&lt;/code&gt; 모양의 텐서</target>
        </trans-unit>
        <trans-unit id="a9d06e247ca03db1069906ff93f237957d9b8b9a" translate="yes" xml:space="preserve">
          <source>a scalar integer &lt;code&gt;Tensor&lt;/code&gt; representing the maximum number of boxes to be selected by non max suppression.</source>
          <target state="translated">a scalar integer &lt;code&gt;Tensor&lt;/code&gt; representing the maximum number of boxes to be selected by non max suppression.</target>
        </trans-unit>
        <trans-unit id="3fca9faa64f5b46a4d7ce176156b7c88545b1baa" translate="yes" xml:space="preserve">
          <source>a shape</source>
          <target state="translated">모양</target>
        </trans-unit>
        <trans-unit id="1e3fab1cb04b3c63a683f54ad32d8e23b5b601ed" translate="yes" xml:space="preserve">
          <source>a single argument or a list of arguments (typically a list of default values); a single argument is converted internally into a list containing one item.</source>
          <target state="translated">a single argument or a list of arguments (typically a list of default values); a single argument is converted internally into a list containing one item.</target>
        </trans-unit>
        <trans-unit id="7cbfc6ae061d8de0cb83e913ad03f6acad9950c0" translate="yes" xml:space="preserve">
          <source>a single data type (float32, int32, or string, for example)</source>
          <target state="translated">a single data type (float32, int32, or string, for example)</target>
        </trans-unit>
        <trans-unit id="d6b451769762c48595037c2a99820e1960c8d664" translate="yes" xml:space="preserve">
          <source>a single or a list the remote server addr in host-port format.</source>
          <target state="translated">a single or a list the remote server addr in host-port format.</target>
        </trans-unit>
        <trans-unit id="3b5a8f6cdc727f3954a46a20a08afdbba957f018" translate="yes" xml:space="preserve">
          <source>a size entry is interpreted as &lt;em&gt;any&lt;/em&gt; size if it is None or '.'.</source>
          <target state="translated">a size entry is interpreted as &lt;em&gt;any&lt;/em&gt; size if it is None or '.'.</target>
        </trans-unit>
        <trans-unit id="1598d219bbf50d48513b4d681ac2e664678a0fb2" translate="yes" xml:space="preserve">
          <source>a size entry is interpreted as an explicit size if it can be parsed as an integer primitive.</source>
          <target state="translated">a size entry is interpreted as an explicit size if it can be parsed as an integer primitive.</target>
        </trans-unit>
        <trans-unit id="fc48ba5f69c2b3b1cd2c64475359fd4da8d724e2" translate="yes" xml:space="preserve">
          <source>a string added between each string being joined.</source>
          <target state="translated">a string added between each string being joined.</target>
        </trans-unit>
        <trans-unit id="b3901c8cf09bba87b3019c11d2d00ae2d940cb1a" translate="yes" xml:space="preserve">
          <source>a string for the name of the executor to be used to execute functions defined by tf.contrib.eager.defun.</source>
          <target state="translated">a string for the name of the executor to be used to execute functions defined by tf.contrib.eager.defun.</target>
        </trans-unit>
        <trans-unit id="f317a91fbf6fd20ea76003a27e77d226e69d5e9f" translate="yes" xml:space="preserve">
          <source>a string of the form /job:</source>
          <target state="translated">/ job 형식의 문자열 :</target>
        </trans-unit>
        <trans-unit id="592805594d7acec2e035ccb78cd57e76d0c89267" translate="yes" xml:space="preserve">
          <source>a string path indicating where to write the TFRecord data.</source>
          <target state="translated">a string path indicating where to write the TFRecord data.</target>
        </trans-unit>
        <trans-unit id="28879ce16025e8aea78e0eed31c3828842400532" translate="yes" xml:space="preserve">
          <source>a string resource path relative to tensorflow/</source>
          <target state="translated">a string resource path relative to tensorflow/</target>
        </trans-unit>
        <trans-unit id="cf98d147fdc1a4586c1261348ed08e3ea9d55344" translate="yes" xml:space="preserve">
          <source>a string resource path relative to tensorflow/.</source>
          <target state="translated">a string resource path relative to tensorflow/.</target>
        </trans-unit>
        <trans-unit id="382e66fb08d6f4af34d47af5189e64c302e25ab8" translate="yes" xml:space="preserve">
          <source>a string specifying the directory in which to write an event file.</source>
          <target state="translated">a string specifying the directory in which to write an event file.</target>
        </trans-unit>
        <trans-unit id="b8d4b27e20cdf3399f6ff473e703a75399bc6632" translate="yes" xml:space="preserve">
          <source>a string specifying the path to an existing SavedModel.</source>
          <target state="translated">a string specifying the path to an existing SavedModel.</target>
        </trans-unit>
        <trans-unit id="5c782ca359f58ff03f46b2cba60080dcab6928ea" translate="yes" xml:space="preserve">
          <source>a string specifying the path to the SavedModel directory.</source>
          <target state="translated">a string specifying the path to the SavedModel directory.</target>
        </trans-unit>
        <trans-unit id="e66ef56dd71685a637ed8bde76e00efab58fe5da" translate="yes" xml:space="preserve">
          <source>a string-type Tensor to summarize.</source>
          <target state="translated">a string-type Tensor to summarize.</target>
        </trans-unit>
        <trans-unit id="56c95190656de2a73fe1c13de36287330da4d209" translate="yes" xml:space="preserve">
          <source>a string. The address of the master to use for eval. Defaults to master if not set.</source>
          <target state="translated">a string. The address of the master to use for eval. Defaults to master if not set.</target>
        </trans-unit>
        <trans-unit id="c2db3d823dea26a64286f31522b26f56802a1985" translate="yes" xml:space="preserve">
          <source>a string. The address of the master to use for training.</source>
          <target state="translated">a string. The address of the master to use for training.</target>
        </trans-unit>
        <trans-unit id="9a00a84ed543b41fac41116a0ecb87c993c7cda7" translate="yes" xml:space="preserve">
          <source>a summary_pb2.SummaryDescription</source>
          <target state="translated">summary_pb2. 요약 설명</target>
        </trans-unit>
        <trans-unit id="45f3aa41e8ac0fd197acaacc063a6ad881fe4154" translate="yes" xml:space="preserve">
          <source>a tensor</source>
          <target state="translated">a tensor</target>
        </trans-unit>
        <trans-unit id="e4ae58d9fb87a25f2deeed86b25ef281f7e84171" translate="yes" xml:space="preserve">
          <source>a tensor of rank 1 or higher with a shape of [..., num_boxes].</source>
          <target state="translated">a tensor of rank 1 or higher with a shape of [..., num_boxes].</target>
        </trans-unit>
        <trans-unit id="1b4fd33d17d7c9e469c896528f1317da39655996" translate="yes" xml:space="preserve">
          <source>a tensor of rank 2 or higher with a shape of [..., num_boxes, 4]. Dimensions except the last two are batch dimensions.</source>
          <target state="translated">a tensor of rank 2 or higher with a shape of [..., num_boxes, 4]. Dimensions except the last two are batch dimensions.</target>
        </trans-unit>
        <trans-unit id="cb28536b86a99e6d9c4639674cb74b760427a486" translate="yes" xml:space="preserve">
          <source>a tensor or list of tensors</source>
          <target state="translated">a tensor or list of tensors</target>
        </trans-unit>
        <trans-unit id="640aa7ca6a8766d889609357ce01db334a546544" translate="yes" xml:space="preserve">
          <source>a tuple of (&lt;code&gt;sampled_candidates&lt;/code&gt;, &lt;code&gt;true_expected_count&lt;/code&gt;, &lt;code&gt;sampled_expected_count&lt;/code&gt;) returned by a &lt;code&gt;*_candidate_sampler&lt;/code&gt; function. (if None, we default to &lt;code&gt;log_uniform_candidate_sampler&lt;/code&gt;)</source>
          <target state="translated">a tuple of ( &lt;code&gt;sampled_candidates&lt;/code&gt; , &lt;code&gt;true_expected_count&lt;/code&gt; , &lt;code&gt;sampled_expected_count&lt;/code&gt; ) returned by a &lt;code&gt;*_candidate_sampler&lt;/code&gt; function. (if None, we default to &lt;code&gt;log_uniform_candidate_sampler&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="fcfc12d63102b45a3e6037e53f6854edb7f5b381" translate="yes" xml:space="preserve">
          <source>a tuple of 2 integers, specifying the strides of the convolution along the width and height.</source>
          <target state="translated">a tuple of 2 integers, specifying the strides of the convolution along the width and height.</target>
        </trans-unit>
        <trans-unit id="d183d11d0d54a060d7009b34542317f6fc714e19" translate="yes" xml:space="preserve">
          <source>a tuple of 2 integers, specifying the width and height of the 2D convolution window.</source>
          <target state="translated">a tuple of 2 integers, specifying the width and height of the 2D convolution window.</target>
        </trans-unit>
        <trans-unit id="57a238277396d1dc9a17a850fb1ad6b9aa782459" translate="yes" xml:space="preserve">
          <source>a tuple of a single integer, specifying the length of the 1D convolution window.</source>
          <target state="translated">a tuple of a single integer, specifying the length of the 1D convolution window.</target>
        </trans-unit>
        <trans-unit id="143bed0bba33eacbf68b60d1ca77684c689464bf" translate="yes" xml:space="preserve">
          <source>a tuple of a single integer, specifying the stride length of the convolution.</source>
          <target state="translated">a tuple of a single integer, specifying the stride length of the convolution.</target>
        </trans-unit>
        <trans-unit id="ab7dc476556d1061e224a2a7a8aa11501a5707ed" translate="yes" xml:space="preserve">
          <source>a tuple of strings, which describes all the TPU devices in the system.</source>
          <target state="translated">a tuple of strings, which describes all the TPU devices in the system.</target>
        </trans-unit>
        <trans-unit id="aea224a6c5fec351ebdbae08e0bd3ba1b0c63d75" translate="yes" xml:space="preserve">
          <source>a tuple with (output_row, output_col).</source>
          <target state="translated">a tuple with (output_row, output_col).</target>
        </trans-unit>
        <trans-unit id="8afd66969dd06ed1eba4fe64521e14d7727d738b" translate="yes" xml:space="preserve">
          <source>a tuple/list of strings.</source>
          <target state="translated">a tuple/list of strings.</target>
        </trans-unit>
        <trans-unit id="47ed70eec68109426c23a40515035b914c0da955" translate="yes" xml:space="preserve">
          <source>a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.</source>
          <target state="translated">a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.</target>
        </trans-unit>
        <trans-unit id="b8ebd3b7d7e693185180008e1a8aaa05435c3d8c" translate="yes" xml:space="preserve">
          <source>a vector of dtype STATE_TYPE representing the initial counter for the RNG, whose length is algorithm-specific.,</source>
          <target state="translated">a vector of dtype STATE_TYPE representing the initial counter for the RNG, whose length is algorithm-specific.,</target>
        </trans-unit>
        <trans-unit id="a0cfcb777f5630e0744dd02bb3be535bf23c0558" translate="yes" xml:space="preserve">
          <source>a vector of dtype STATE_TYPE representing the initial state of the RNG, whose length and semantics are algorithm-specific. If it's a variable, the generator will reuse it instead of creating a new variable.</source>
          <target state="translated">a vector of dtype STATE_TYPE representing the initial state of the RNG, whose length and semantics are algorithm-specific. If it's a variable, the generator will reuse it instead of creating a new variable.</target>
        </trans-unit>
        <trans-unit id="af97be5a0dcfe7dd967ed814e5c51b34d26e758a" translate="yes" xml:space="preserve">
          <source>a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector.</source>
          <target state="translated">a) 루프 변수가 SparseTensor 인 경우 모양 불변 값은 TensorShape ([r]) 여야합니다. 여기서 r은 스파 스 텐서가 나타내는 밀도 텐서의 순위입니다. SparseTensor의 3 가지 텐서 모양이 ([없음], [없음, r], [r])임을 의미합니다. 참고 : 여기에서 변하지 않는 모양은 SparseTensor.dense_shape 속성의 모양입니다. 벡터 모양이어야합니다.</target>
        </trans-unit>
        <trans-unit id="addb12381e6be0fb67d7d9c7e9625d221a558d6e" translate="yes" xml:space="preserve">
          <source>a: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt;. b: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt; with the same type and rank as &lt;code&gt;a&lt;/code&gt;. type: The type of both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. transpose_a: If True, &lt;code&gt;a&lt;/code&gt; transposed before multiplication. transpose_b: If True, &lt;code&gt;b&lt;/code&gt; transposed before multiplication. adjoint_a: If True, &lt;code&gt;a&lt;/code&gt; adjointed before multiplication. adjoint_b: If True, &lt;code&gt;b&lt;/code&gt; adjointed before multiplication.</source>
          <target state="translated">a: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt; . b: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt; with the same type and rank as &lt;code&gt;a&lt;/code&gt; . type: The type of both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; . transpose_a: If True, &lt;code&gt;a&lt;/code&gt; transposed before multiplication. transpose_b: If True, &lt;code&gt;b&lt;/code&gt; transposed before multiplication. adjoint_a: If True, &lt;code&gt;a&lt;/code&gt; adjointed before multiplication. adjoint_b: If True, &lt;code&gt;b&lt;/code&gt; adjointed before multiplication.</target>
        </trans-unit>
        <trans-unit id="a35ea6647dc95b90fe453d98f66e284fc5f0c506" translate="yes" xml:space="preserve">
          <source>a[0] = 0 : the first value of the sequence is 0</source>
          <target state="translated">a [0] = 0 : 시퀀스의 첫 번째 값은 0</target>
        </trans-unit>
        <trans-unit id="deda22cbcad48bb3a1413dc8ff28a54f1e5592e8" translate="yes" xml:space="preserve">
          <source>a[end] = input_row_length : the last value of the sequence is the size</source>
          <target state="translated">a [end] = input_row_length : 시퀀스의 마지막 값은 크기입니다</target>
        </trans-unit>
        <trans-unit id="eb8f95bc156db1900a569b1735ddc3da656d19bc" translate="yes" xml:space="preserve">
          <source>about sharing states in tensorflow.</source>
          <target state="translated">about sharing states in tensorflow.</target>
        </trans-unit>
        <trans-unit id="f0f846898da4ec61cf83b3e2c8495d2978a0200c" translate="yes" xml:space="preserve">
          <source>absolute tolerance for bfloat16.</source>
          <target state="translated">absolute tolerance for bfloat16.</target>
        </trans-unit>
        <trans-unit id="0ae04d0b7afda8def3fdb79e35983e0773b0864f" translate="yes" xml:space="preserve">
          <source>absolute tolerance for float16.</source>
          <target state="translated">absolute tolerance for float16.</target>
        </trans-unit>
        <trans-unit id="3cfa07cb0663ab1cde306433e352a29097f65419" translate="yes" xml:space="preserve">
          <source>absolute tolerance for float32.</source>
          <target state="translated">absolute tolerance for float32.</target>
        </trans-unit>
        <trans-unit id="c4fd56eead7560ee583f1b8caad28fbd463bee25" translate="yes" xml:space="preserve">
          <source>absolute tolerance.</source>
          <target state="translated">absolute tolerance.</target>
        </trans-unit>
        <trans-unit id="3e1f32f51354c2febc7caaaf4ffeef8cbb8f07cf" translate="yes" xml:space="preserve">
          <source>accum += grad * grad prox_v = var - lr * grad * (1 / sqrt(accum)) var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}</source>
          <target state="translated">accum += grad * grad prox_v = var - lr * grad * (1 / sqrt(accum)) var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}</target>
        </trans-unit>
        <trans-unit id="26c2261a427773f28da9c962c61401a44f52fafa" translate="yes" xml:space="preserve">
          <source>accum += grad * grad var -= lr * grad * (1 / (sqrt(accum) + epsilon))</source>
          <target state="translated">accum += grad * grad var -= lr * grad * (1 / (sqrt(accum) + epsilon))</target>
        </trans-unit>
        <trans-unit id="9b39d4103e267bfff6b19f03a0c61954510b506e" translate="yes" xml:space="preserve">
          <source>accum += grad * grad var -= lr * grad * (1 / sqrt(accum))</source>
          <target state="translated">accum += grad * grad var -= lr * grad * (1 / sqrt(accum))</target>
        </trans-unit>
        <trans-unit id="fbdff22455472e83bbec28e270efc5acd84cec93" translate="yes" xml:space="preserve">
          <source>accum = accum * momentum + grad var -= lr * accum</source>
          <target state="translated">accum = accum * momentum + grad var -= lr * accum</target>
        </trans-unit>
        <trans-unit id="cabf3744d4cc537b306d1c2d4e5c98eab9321c1c" translate="yes" xml:space="preserve">
          <source>accum = accum * momentum - lr * grad var += accum</source>
          <target state="translated">accum = accum * momentum - lr * grad var += accum</target>
        </trans-unit>
        <trans-unit id="77aa7bbfc2a1935f26fc03cc068c50ab9de9da1b" translate="yes" xml:space="preserve">
          <source>accum = rho() * accum + (1 - rho()) * grad.square(); update = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad; update_accum = rho() * update_accum + (1 - rho()) * update.square(); var -= update;</source>
          <target state="translated">accum = rho() * accum + (1 - rho()) * grad.square(); update = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad; update_accum = rho() * update_accum + (1 - rho()) * update.square(); var -= update;</target>
        </trans-unit>
        <trans-unit id="e072c3a42e30a463ae763379bfd4971f6786c3db" translate="yes" xml:space="preserve">
          <source>accum_new = accum + grad * grad linear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2 var = (sign(linear) * l1 - linear) / quadratic if |linear| &amp;gt; l1 else 0.0 accum = accum_new</source>
          <target state="translated">accum_new = accum + grad * grad linear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2 var = (sign(linear) * l1 - linear) / quadratic if |linear| &amp;gt; l1 else 0.0 accum = accum_new</target>
        </trans-unit>
        <trans-unit id="9207a4d89e0769ab8d4e61e6e2f82d2491a3631b" translate="yes" xml:space="preserve">
          <source>actual distribution of the values to maximize the usage of the lower bit depth and adjusting the output min and max ranges accordingly.</source>
          <target state="translated">actual distribution of the values to maximize the usage of the lower bit depth and adjusting the output min and max ranges accordingly.</target>
        </trans-unit>
        <trans-unit id="336367a90c28a1ba840266e48935292b8c0ec99a" translate="yes" xml:space="preserve">
          <source>add and relu and requantize fusion.</source>
          <target state="translated">add and relu and requantize fusion.</target>
        </trans-unit>
        <trans-unit id="3eebca9963a4e9d6f0fcc02adef3bd6f1fb5966b" translate="yes" xml:space="preserve">
          <source>add and relu fusion.</source>
          <target state="translated">add and relu fusion.</target>
        </trans-unit>
        <trans-unit id="577f333710522bc0627398bf6ba75e178aa283d7" translate="yes" xml:space="preserve">
          <source>add.</source>
          <target state="translated">add.</target>
        </trans-unit>
        <trans-unit id="b1b7d0394cceca9bd35a4316061103e356401833" translate="yes" xml:space="preserve">
          <source>additional keyword arguments to be passed to the underlying &lt;code&gt;assertAllClose&lt;/code&gt; call.</source>
          <target state="translated">additional keyword arguments to be passed to the underlying &lt;code&gt;assertAllClose&lt;/code&gt; call.</target>
        </trans-unit>
        <trans-unit id="9d0442ecdcd49f18c5f26a6c7ac65eb7852e6a16" translate="yes" xml:space="preserve">
          <source>adjoints (conjugate transposes).</source>
          <target state="translated">인접 (접합 전치).</target>
        </trans-unit>
        <trans-unit id="f0e72c8db0ec39595db76cd9ab3ea0b01fa0a54e" translate="yes" xml:space="preserve">
          <source>adjusted_dilation_rate is an int64 tensor of shape [max(spatial&lt;em&gt;dims)], adjusted&lt;/em&gt;{paddings,crops} are int64 tensors of shape [max(spatial_dims), 2]</source>
          <target state="translated">adjust_dilation_rate는 모양의 int64 텐서 [max (spatial &lt;em&gt;dims)], 조정 된&lt;/em&gt; {paddings, crops}는 모양의 int64 텐서입니다 [max (spatial_dims), 2]</target>
        </trans-unit>
        <trans-unit id="05ab368bb50c30f0ad36866b483ced3c24cef7fa" translate="yes" xml:space="preserve">
          <source>adjusted_dilation_rate[spatial_dims[i] - 1] = dilation_rate[i] adjusted_paddings[spatial_dims[i] - 1, :] = paddings[i, :] adjusted_crops[spatial_dims[i] - 1, :] = crops[i, :]</source>
          <target state="translated">adjust_dilation_rate [spatial_dims [i]-1] = dilation_rate [i] 조정 _paddings [spatial_dims [i]-1, :] = 채움 [i, :] 조정 _crops [spatial_dims [i]-1, :] = 작물 [i, :]</target>
        </trans-unit>
        <trans-unit id="a4cbf8207b85c3ddcaf0003b5cf461ae49770af7" translate="yes" xml:space="preserve">
          <source>after each call to &lt;code&gt;Saver.save()&lt;/code&gt;</source>
          <target state="translated">after each call to &lt;code&gt;Saver.save()&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="10582f7ebb86265685f2d94ac9bf45194c26c416" translate="yes" xml:space="preserve">
          <source>aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class &lt;a href=&quot;../../variableaggregation&quot;&gt;&lt;code&gt;tf.VariableAggregation&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">집계 : 분산 변수가 집계되는 방법을 나타냅니다. 허용되는 값은 &lt;a href=&quot;../../variableaggregation&quot;&gt; &lt;code&gt;tf.VariableAggregation&lt;/code&gt; &lt;/a&gt; 클래스에 정의 된 상수 입니다.</target>
        </trans-unit>
        <trans-unit id="95142e8db6f0e670c6cdd282ebda2a8dc3235286" translate="yes" xml:space="preserve">
          <source>aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class &lt;a href=&quot;variableaggregation&quot;&gt;&lt;code&gt;tf.VariableAggregation&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">집계 : 분산 변수가 집계되는 방법을 나타냅니다. 허용되는 값은 &lt;a href=&quot;variableaggregation&quot;&gt; &lt;code&gt;tf.VariableAggregation&lt;/code&gt; &lt;/a&gt; 클래스에 정의 된 상수 입니다.</target>
        </trans-unit>
        <trans-unit id="0b9a9b6a555fda3290e62d85d68e6dc900f6aa72" translate="yes" xml:space="preserve">
          <source>alias for &quot;input&quot; argument.</source>
          <target state="translated">alias for &quot;input&quot; argument.</target>
        </trans-unit>
        <trans-unit id="40ab33a302eea4103a7a9fe7699eb8c64bfbebd3" translate="yes" xml:space="preserve">
          <source>alias for expand_nonconcat_dim</source>
          <target state="translated">alias for expand_nonconcat_dim</target>
        </trans-unit>
        <trans-unit id="6b47a9fb6a6098b56fa005c9bf092d1ede95a90d" translate="yes" xml:space="preserve">
          <source>alpha = input_row_length / output_row_length : our reduction ratio</source>
          <target state="translated">alpha = input_row_length / output_row_length : 축소 비율</target>
        </trans-unit>
        <trans-unit id="13c37d0b0868b315be5754e60b93df3ddf6e1515" translate="yes" xml:space="preserve">
          <source>amount of weight decay to apply; None means that the weights are not decayed.</source>
          <target state="translated">amount of weight decay to apply; None means that the weights are not decayed.</target>
        </trans-unit>
        <trans-unit id="584cfc3585963e88777945fd0c74c8fed91e27d7" translate="yes" xml:space="preserve">
          <source>amount of weight decay to apply; None means that the weights are not decayed. Weights are decayed by multiplying the weight by this factor each step.</source>
          <target state="translated">amount of weight decay to apply; None means that the weights are not decayed. Weights are decayed by multiplying the weight by this factor each step.</target>
        </trans-unit>
        <trans-unit id="6be75b3f1c53a534ff3d3d88d27d62dd86a8b144" translate="yes" xml:space="preserve">
          <source>an &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt; scalar representing data to be folded in to the seed.</source>
          <target state="translated">an &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt; scalar representing data to be folded in to the seed.</target>
        </trans-unit>
        <trans-unit id="21498301c88a4012ed64c1b4b7225d60e8644258" translate="yes" xml:space="preserve">
          <source>an &lt;code&gt;int&lt;/code&gt; shows until which global step should we wait.</source>
          <target state="translated">an &lt;code&gt;int&lt;/code&gt; shows until which global step should we wait.</target>
        </trans-unit>
        <trans-unit id="60271ab37cee858c4218f052e21a7603d914d273" translate="yes" xml:space="preserve">
          <source>an OrderedDict, where the keys are the feature column names and the values are importances. It is sorted by importance.</source>
          <target state="translated">an OrderedDict, where the keys are the feature column names and the values are importances. It is sorted by importance.</target>
        </trans-unit>
        <trans-unit id="5e7cd4b2e10dade102ff7156179e9913d87e9c02" translate="yes" xml:space="preserve">
          <source>an RNG seed (a tensor with shape [2] and dtype &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;). (When using XLA, only &lt;code&gt;int32&lt;/code&gt; is allowed.)</source>
          <target state="translated">an RNG seed (a tensor with shape [2] and dtype &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt; ). (When using XLA, only &lt;code&gt;int32&lt;/code&gt; is allowed.)</target>
        </trans-unit>
        <trans-unit id="3f4a60ab5b3884b1dd6263e0c0406bab2fcc7011" translate="yes" xml:space="preserve">
          <source>an RNNCell, a projection to output_size is added to it.</source>
          <target state="translated">an RNNCell, a projection to output_size is added to it.</target>
        </trans-unit>
        <trans-unit id="4795fec293585f2b8f245ac28ac09c142cad999f" translate="yes" xml:space="preserve">
          <source>an approximation of the area under the P-R curve.</source>
          <target state="translated">an approximation of the area under the P-R curve.</target>
        </trans-unit>
        <trans-unit id="d4d758ce3efd3e79d82b2ead6ef75b7be77f612b" translate="yes" xml:space="preserve">
          <source>an arbitrarily nested structure.</source>
          <target state="translated">an arbitrarily nested structure.</target>
        </trans-unit>
        <trans-unit id="c4b9296a58496856781b45283ebf3905a9774cf6" translate="yes" xml:space="preserve">
          <source>an arbitrarily nested structure. Note, numpy arrays are considered atoms and are not flattened.</source>
          <target state="translated">an arbitrarily nested structure. Note, numpy arrays are considered atoms and are not flattened.</target>
        </trans-unit>
        <trans-unit id="78bf9b0c50970fcea95462ac064c671c66592d16" translate="yes" xml:space="preserve">
          <source>an enum value of &lt;a href=&quot;../../../../distribute/inputreplicationmode&quot;&gt;&lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt;&lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; on their worker.</source>
          <target state="translated">an enum value of &lt;a href=&quot;../../../../distribute/inputreplicationmode&quot;&gt; &lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt; &lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; on their worker.</target>
        </trans-unit>
        <trans-unit id="c1d9638fbc99061f41f5f1718d95aa9d9b2ba446" translate="yes" xml:space="preserve">
          <source>an enum value of &lt;a href=&quot;../../../distribute/inputreplicationmode&quot;&gt;&lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt;&lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; on their worker.</source>
          <target state="translated">an enum value of &lt;a href=&quot;../../../distribute/inputreplicationmode&quot;&gt; &lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt; &lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; on their worker.</target>
        </trans-unit>
        <trans-unit id="3759390b842509817496ad6964a87f97c5843a65" translate="yes" xml:space="preserve">
          <source>an input generator that can be used to generate input samples for the model. This must be a callable object that returns an object that supports the &lt;code&gt;iter()&lt;/code&gt; protocol (e.g. a generator function). The elements generated must have same type and shape as inputs to the model.</source>
          <target state="translated">an input generator that can be used to generate input samples for the model. This must be a callable object that returns an object that supports the &lt;code&gt;iter()&lt;/code&gt; protocol (e.g. a generator function). The elements generated must have same type and shape as inputs to the model.</target>
        </trans-unit>
        <trans-unit id="8feb6baf60936cf60390ee049e7a07e3ffd4392a" translate="yes" xml:space="preserve">
          <source>an input sequence.</source>
          <target state="translated">an input sequence.</target>
        </trans-unit>
        <trans-unit id="2d46e772e042b91ec9dff778f5110d720bafa374" translate="yes" xml:space="preserve">
          <source>an instance of &lt;a href=&quot;../configproto&quot;&gt;&lt;code&gt;tf.compat.v1.ConfigProto&lt;/code&gt;&lt;/a&gt; proto used to configure the session. It's the &lt;code&gt;config&lt;/code&gt; argument of constructor of &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">an instance of &lt;a href=&quot;../configproto&quot;&gt; &lt;code&gt;tf.compat.v1.ConfigProto&lt;/code&gt; &lt;/a&gt; proto used to configure the session. It's the &lt;code&gt;config&lt;/code&gt; argument of constructor of &lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="1a37f2503f46094f2d360fd3f7fc596a57b2fd3f" translate="yes" xml:space="preserve">
          <source>an instance of &lt;a href=&quot;topology&quot;&gt;&lt;code&gt;tf.tpu.experimental.Topology&lt;/code&gt;&lt;/a&gt;, which describes the physical topology of TPU system.</source>
          <target state="translated">an instance of &lt;a href=&quot;topology&quot;&gt; &lt;code&gt;tf.tpu.experimental.Topology&lt;/code&gt; &lt;/a&gt;, which describes the physical topology of TPU system.</target>
        </trans-unit>
        <trans-unit id="cd841e8ca3fc9c05fb813b0d2f5d9103dc9c0ba8" translate="yes" xml:space="preserve">
          <source>an instance of &lt;code&gt;tf.train.experimental/ClusterDeviceFilters&lt;/code&gt; that specify device filters to the remote tasks in cluster.</source>
          <target state="translated">an instance of &lt;code&gt;tf.train.experimental/ClusterDeviceFilters&lt;/code&gt; that specify device filters to the remote tasks in cluster.</target>
        </trans-unit>
        <trans-unit id="6e2d70312bfddfcd3ddb72da0a20ed277ffcd67f" translate="yes" xml:space="preserve">
          <source>an integer or 1-D numpy array.</source>
          <target state="translated">an integer or 1-D numpy array.</target>
        </trans-unit>
        <trans-unit id="2085d387df6482fce1af122d6acdb0118638ce0b" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</source>
          <target state="translated">an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</target>
        </trans-unit>
        <trans-unit id="12a0347d0bf66b603b4cf339883f33c0d26b06b7" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</source>
          <target state="translated">an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</target>
        </trans-unit>
        <trans-unit id="d3402e6e4b4d2964f62630d5d955c60e92608fd4" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any &lt;code&gt;strides&lt;/code&gt; value != 1.</source>
          <target state="translated">an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any &lt;code&gt;strides&lt;/code&gt; value != 1.</target>
        </trans-unit>
        <trans-unit id="58266c81cd572b21187fd9645689ccfc5a6f9adf" translate="yes" xml:space="preserve">
          <source>an integer representing the number of boxes in a tile, i.e., the maximum number of boxes per image that can be used to suppress other boxes in parallel; larger tile_size means larger parallelism and potentially more redundant work.</source>
          <target state="translated">an integer representing the number of boxes in a tile, i.e., the maximum number of boxes per image that can be used to suppress other boxes in parallel; larger tile_size means larger parallelism and potentially more redundant work.</target>
        </trans-unit>
        <trans-unit id="12f87a898c564b834112575d56e2166669cc6140" translate="yes" xml:space="preserve">
          <source>an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying a stride value != 1.</source>
          <target state="translated">an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying a stride value != 1.</target>
        </trans-unit>
        <trans-unit id="36442510bd24021bdca61b0550d53fc4b7f41549" translate="yes" xml:space="preserve">
          <source>an integer: 1 or 2. 1 corresponds to V1, 2 corresponds to V2. (Defaults to V1). With V1, &lt;code&gt;export_saved_model()&lt;/code&gt; adds rewrite() and TPUPartitionedCallOp() for user; while in v2, user is expected to add rewrite(), TPUPartitionedCallOp() etc in their model_fn. A helper function &lt;code&gt;inference_on_tpu&lt;/code&gt; is provided for V2. brn_tpu_estimator.py includes examples for both versions i.e. TPUEstimatorExportTest and TPUEstimatorExportV2Test.</source>
          <target state="translated">an integer: 1 or 2. 1 corresponds to V1, 2 corresponds to V2. (Defaults to V1). With V1, &lt;code&gt;export_saved_model()&lt;/code&gt; adds rewrite() and TPUPartitionedCallOp() for user; while in v2, user is expected to add rewrite(), TPUPartitionedCallOp() etc in their model_fn. A helper function &lt;code&gt;inference_on_tpu&lt;/code&gt; is provided for V2. brn_tpu_estimator.py includes examples for both versions i.e. TPUEstimatorExportTest and TPUEstimatorExportV2Test.</target>
        </trans-unit>
        <trans-unit id="15bd4e656cd87110cb6d06a7f5b3f1e300030cdd" translate="yes" xml:space="preserve">
          <source>an optional &lt;code&gt;dilation_rate&lt;/code&gt; tensor of shape &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; specifying the filter upsampling/input downsampling rate, and an optional list of N &lt;code&gt;strides&lt;/code&gt; (defaulting [1]*N), this computes for each N-D spatial output position (x[0], ..., x[N-1]):</source>
          <target state="translated">필터 업 샘플링 / 입력 다운 샘플링 속도를 지정하는 모양 &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; 의 선택적 &lt;code&gt;dilation_rate&lt;/code&gt; 텐서 및 선택적 N &lt;code&gt;strides&lt;/code&gt; 목록 (기본값 [1] * N)은 각 ND 공간 출력 위치 (x [0], ..., x [N-1]) :</target>
        </trans-unit>
        <trans-unit id="0ee24f38b8e109fbd5639cc2b47bec50e32475ed" translate="yes" xml:space="preserve">
          <source>an optional &lt;code&gt;dilations&lt;/code&gt; tensor of shape &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; specifying the filter upsampling/input downsampling rate, and an optional list of N &lt;code&gt;strides&lt;/code&gt; (defaulting [1]*N), this computes for each N-D spatial output position (x[0], ..., x[N-1]):</source>
          <target state="translated">필터 업 샘플링 / 입력 다운 샘플링 속도를 지정하는 모양 &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; 의 선택적 &lt;code&gt;dilations&lt;/code&gt; 텐서 및 선택적 N &lt;code&gt;strides&lt;/code&gt; 목록 (기본값 [1] * N)은 각 ND 공간 출력 위치 (x [0], ..., x [N-1]) :</target>
        </trans-unit>
        <trans-unit id="ab989fce67e1a28670d6c810e49317fa6212a61a" translate="yes" xml:space="preserve">
          <source>an optional name for the operation.</source>
          <target state="translated">an optional name for the operation.</target>
        </trans-unit>
        <trans-unit id="3f33599b0487789dd89792f29c86c4b410cef6e4" translate="yes" xml:space="preserve">
          <source>an optional string of the form /job:</source>
          <target state="translated">an optional string of the form /job:</target>
        </trans-unit>
        <trans-unit id="cffa50a32cb13a240d705317bcec65dd1f31b6ad" translate="yes" xml:space="preserve">
          <source>and</source>
          <target state="translated">and</target>
        </trans-unit>
        <trans-unit id="ebd46f618c63c3e212b1d778f0a25f4660c23598" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;SparseFeature&lt;/code&gt; config with 2 &lt;code&gt;index_key&lt;/code&gt;s</source>
          <target state="translated">및 &lt;code&gt;SparseFeature&lt;/code&gt; 설정 2 &lt;code&gt;index_key&lt;/code&gt; 의</target>
        </trans-unit>
        <trans-unit id="6140381926bf0d082343ace25ade2e3cf221f627" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;default_value&lt;/code&gt; is &lt;code&gt;x&lt;/code&gt;, then the output will be a dense &lt;code&gt;[3, 5]&lt;/code&gt; string tensor with values:</source>
          <target state="translated">및 &lt;code&gt;default_value&lt;/code&gt; 이고 &lt;code&gt;x&lt;/code&gt; , 출력 밀도가 될 것이다 &lt;code&gt;[3, 5]&lt;/code&gt; 값 문자열 텐서 :</target>
        </trans-unit>
        <trans-unit id="90e193944d6b8498e0257c00b2820369a0de3479" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;ids&lt;/code&gt; is:</source>
          <target state="translated">and &lt;code&gt;ids&lt;/code&gt; is:</target>
        </trans-unit>
        <trans-unit id="b2d759c667e68226ece3acd5aa4e1cc6620b0ca0" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;max&lt;/code&gt; to 'outputs' tensor of same shape as &lt;code&gt;inputs&lt;/code&gt;.</source>
          <target state="translated">및 &lt;code&gt;max&lt;/code&gt; '출력'텐서 동일 형상에 &lt;code&gt;inputs&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="78fbe88a4e8420d083776e826c4b0cd5bb232375" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;shape&lt;/code&gt; is &lt;code&gt;[9, -1]&lt;/code&gt;, then the output will be a &lt;code&gt;SparseTensor&lt;/code&gt; of shape &lt;code&gt;[9, 4]&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; / &lt;code&gt;values&lt;/code&gt;:</source>
          <target state="translated">와 &lt;code&gt;shape&lt;/code&gt; 인 &lt;code&gt;[9, -1]&lt;/code&gt; , 출력이 될 것이다 &lt;code&gt;SparseTensor&lt;/code&gt; 형상 &lt;code&gt;[9, 4]&lt;/code&gt; 및 &lt;code&gt;indices&lt;/code&gt; / &lt;code&gt;values&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="23cce7bb518f569f4a001bd92a904ee1f95e7599" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;to_retain = [True, False, False, True]&lt;/code&gt;, then the output will be a &lt;code&gt;SparseTensor&lt;/code&gt; of shape &lt;code&gt;[4, 5]&lt;/code&gt; with 2 non-empty values:</source>
          <target state="translated">및 &lt;code&gt;to_retain = [True, False, False, True]&lt;/code&gt; , 출력은 것 &lt;code&gt;SparseTensor&lt;/code&gt; 형상 &lt;code&gt;[4, 5]&lt;/code&gt; 2 비어 값 :</target>
        </trans-unit>
        <trans-unit id="3b2da781f92810fe2701eb25993573d612b9c1c1" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;vocab_size = 200&lt;/code&gt;, then the output will be a &lt;code&gt;[2, 3, 200]&lt;/code&gt; dense bool tensor with False everywhere except at positions</source>
          <target state="translated">및 &lt;code&gt;vocab_size = 200&lt;/code&gt; , 출력이 될 것이다 &lt;code&gt;[2, 3, 200]&lt;/code&gt; 밀도 BOOL 텐서 거짓 어디에나 위치에서 제외</target>
        </trans-unit>
        <trans-unit id="fb47192727f17143a048825f4ab10f7ac6f7e0a3" translate="yes" xml:space="preserve">
          <source>and False elsewhere in &lt;code&gt;output&lt;/code&gt;.</source>
          <target state="translated">그리고 &lt;code&gt;output&lt;/code&gt; 다른 곳에 False .</target>
        </trans-unit>
        <trans-unit id="41290a0f5cbc0ce6a4dfe0924ac8b26a039d1f24" translate="yes" xml:space="preserve">
          <source>and concatenates them into a Tensor of shape:</source>
          <target state="translated">and concatenates them into a Tensor of shape:</target>
        </trans-unit>
        <trans-unit id="c7d1a416a2e3a434f40699d9880f90f9fb160f7e" translate="yes" xml:space="preserve">
          <source>and having size</source>
          <target state="translated">and having size</target>
        </trans-unit>
        <trans-unit id="2223100c859ddb72353f03c35f828882929aa04b" translate="yes" xml:space="preserve">
          <source>and if &lt;code&gt;M = N&lt;/code&gt;,</source>
          <target state="translated">그리고 만약 &lt;code&gt;M = N&lt;/code&gt; ,</target>
        </trans-unit>
        <trans-unit id="1eb3d95c362b4e4d700bf804104fec0c967f459d" translate="yes" xml:space="preserve">
          <source>and process 2 prints</source>
          <target state="translated">and process 2 prints</target>
        </trans-unit>
        <trans-unit id="0ba895928ad073edb53326fb61cbb432a1464bec" translate="yes" xml:space="preserve">
          <source>and that &lt;code&gt;value&lt;/code&gt; has shape</source>
          <target state="translated">and that &lt;code&gt;value&lt;/code&gt; has shape</target>
        </trans-unit>
        <trans-unit id="34d19665daa7f1410ef46ef88c82079153b1d866" translate="yes" xml:space="preserve">
          <source>and then compute a normalized (x), including a small factor ({\epsilon}) for numerical stability.</source>
          <target state="translated">그리고 수치 적 안정성을 위해 작은 계수 ({\ epsilon})를 포함하여 정규화 (x)를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d8047f0fdc4d1e4560c970ac0b82f51065da8044" translate="yes" xml:space="preserve">
          <source>and then compute a normalized &lt;code&gt;x_i_normalized&lt;/code&gt;, including a small factor &lt;code&gt;epsilon&lt;/code&gt; for numerical stability.</source>
          <target state="translated">and then compute a normalized &lt;code&gt;x_i_normalized&lt;/code&gt; , including a small factor &lt;code&gt;epsilon&lt;/code&gt; for numerical stability.</target>
        </trans-unit>
        <trans-unit id="6cc52111330674297a42724a1642fd203137a2d3" translate="yes" xml:space="preserve">
          <source>and therefore</source>
          <target state="translated">and therefore</target>
        </trans-unit>
        <trans-unit id="78e8b25cc130b65509b44b60a2a5f53ed883a7db" translate="yes" xml:space="preserve">
          <source>append(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -&amp;gt; None</source>
          <target state="translated">append(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -&amp;gt; None</target>
        </trans-unit>
        <trans-unit id="30ff52b5b667d11b2ffe908d3a6ddb3a80cc4c30" translate="yes" xml:space="preserve">
          <source>arbitrary function</source>
          <target state="translated">arbitrary function</target>
        </trans-unit>
        <trans-unit id="9aafd247e69aac8e786d351d47d5cdf462d0745f" translate="yes" xml:space="preserve">
          <source>arithmetic_optimization: Simplify arithmetic ops with common sub-expression elimination and arithmetic simplification.</source>
          <target state="translated">산술 최적화 : 공통 하위 표현식 제거 및 산술 단순화를 통해 산술 연산을 단순화합니다.</target>
        </trans-unit>
        <trans-unit id="25c002c4154dbe462f9f8f1755f73522b0671750" translate="yes" xml:space="preserve">
          <source>array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32)</source>
          <target state="translated">배열 ([[0., 0., 0.], [0., 0., 0.]], dtype = float32)</target>
        </trans-unit>
        <trans-unit id="50893921378b303e1550a97d489bc7de28064118" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Test samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Test samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</target>
        </trans-unit>
        <trans-unit id="89cdb649416fd6a0dc9e7f39d49b6ab03d5879e8" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Training samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Training samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</target>
        </trans-unit>
        <trans-unit id="850565e7e77632b17d2cc1ddc239a6bfe2cae1b3" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; Class probability estimates. In the case of binary classification, to match the scikit-learn API, will return an array of shape &lt;code&gt;(n_samples, 2)&lt;/code&gt; (instead of &lt;code&gt;(n_sample, 1)&lt;/code&gt; as in Keras).</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; Class probability estimates. In the case of binary classification, to match the scikit-learn API, will return an array of shape &lt;code&gt;(n_samples, 2)&lt;/code&gt; (instead of &lt;code&gt;(n_sample, 1)&lt;/code&gt; as in Keras).</target>
        </trans-unit>
        <trans-unit id="65baf57d96f3cc1470e57342810a1de5f6a27f59" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Class predictions.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Class predictions.</target>
        </trans-unit>
        <trans-unit id="ae0244ecc60195106b25820595b8db35a60ed5ec" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Predictions.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Predictions.</target>
        </trans-unit>
        <trans-unit id="3416fc16abdc6956ef8e2ee3753937f82806270c" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c4e9940ae53eb744fbac597e70feee59a95b69b8" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; or &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="translated">array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; or &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c0c679f2bbcaf59c6d5d58d588516277aa10113f" translate="yes" xml:space="preserve">
          <source>as cpu and gpu are mutually exclusive. All entries are optional.</source>
          <target state="translated">CPU와 GPU는 상호 배타적입니다. 모든 항목은 선택 사항입니다.</target>
        </trans-unit>
        <trans-unit id="d81b6defc8e7983d824c066efff71d103284c806" translate="yes" xml:space="preserve">
          <source>as inputs.</source>
          <target state="translated">as inputs.</target>
        </trans-unit>
        <trans-unit id="307d5c7893e1f246cb5e563745a0402addd02464" translate="yes" xml:space="preserve">
          <source>assertSameElements([1, 1, 1, 0, 0, 0], [0, 1]) # Doesn't raise an AssertionError</source>
          <target state="translated">assertSameElements ([1, 1, 1, 0, 0, 0], [0, 1]) # AssertionError를 발생시키지 않습니다</target>
        </trans-unit>
        <trans-unit id="2454abb84d63a230ebc92ebe78cf3aff74dea1e0" translate="yes" xml:space="preserve">
          <source>assertSetEqual uses ducktyping to support different types of sets, and is optimized for sets specifically (parameters must support a difference method).</source>
          <target state="translated">assertSetEqual은 ducktyping을 사용하여 다양한 유형의 집합을 지원하며 집합에 대해 특별히 최적화됩니다 (매개 변수는 차이 방법을 지원해야 함).</target>
        </trans-unit>
        <trans-unit id="223c6148b033dfedd56732627314f568565b952f" translate="yes" xml:space="preserve">
          <source>assertTotallyOrdered will check that instances can be ordered correctly. For example,</source>
          <target state="translated">assertTotallyOrdered는 인스턴스를 올바르게 주문할 수 있는지 확인합니다. 예를 들어</target>
        </trans-unit>
        <trans-unit id="260d4f1366ba09adb6172ca8fd634126e0e09014" translate="yes" xml:space="preserve">
          <source>associative container. Elements are ordered by key.</source>
          <target state="translated">associative container. Elements are ordered by key.</target>
        </trans-unit>
        <trans-unit id="88d718f8abfa216a9d9fcc60c2c44acc25a8cb82" translate="yes" xml:space="preserve">
          <source>at &lt;code&gt;ckpt_path&lt;/code&gt; and potentially reorders its rows and columns using the specified remappings.</source>
          <target state="translated">at &lt;code&gt;ckpt_path&lt;/code&gt; and potentially reorders its rows and columns using the specified remappings.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
