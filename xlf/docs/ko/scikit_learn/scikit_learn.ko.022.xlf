<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="21ade7db23177cbafdee1241d820ab218814e247" translate="yes" xml:space="preserve">
          <source>There are two ways to specify multiple scoring metrics for the &lt;code&gt;scoring&lt;/code&gt; parameter:</source>
          <target state="translated">&lt;code&gt;scoring&lt;/code&gt; 매개 변수에 여러 스코어링 메트릭을 지정하는 두 가지 방법이 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c9e248c64205afb9beabc74d09eccb0781bdceea" translate="yes" xml:space="preserve">
          <source>There exist several strategies to perform Bayesian ridge regression. This implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where updates of the regularization parameters are done as suggested in (MacKay, 1992). Note that according to A New View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these update rules do not guarantee that the marginal likelihood is increasing between two consecutive iterations of the optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdf4947857438b0c51097ba679415b8309e576f2" translate="yes" xml:space="preserve">
          <source>There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.</source>
          <target state="translated">MDS 알고리즘에는 메트릭과 비 메트릭의 두 가지 유형이 있습니다. Scikit-learn에서 &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; 클래스 는 두 가지를 모두 구현합니다. Metric MDS에서, 입력 유사성 매트릭스는 메트릭으로부터 발생하고 (따라서 삼각 불평등을 고려함), 출력 두 점 사이의 거리는 유사성 또는 비 유사성 데이터에 가능한 한 가깝게 설정됩니다. 비 메트릭 버전에서 알고리즘은 거리의 순서를 유지하려고 시도하므로 임베디드 공간의 거리와 유사성 / 비 유사성 간의 단조로운 관계를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="9e945ec56932f8495741411aac1ef0391f41ea70" translate="yes" xml:space="preserve">
          <source>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</source>
          <target state="translated">진실을 회복 할 것이라는 보장은 없습니다. 먼저 올바른 수의 클러스터를 선택하기가 어렵습니다. 둘째, 알고리즘은 초기화에 민감하며 scikit-learn은이 문제를 완화하기 위해 몇 가지 트릭을 사용하지만 로컬 최소값에 속할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a5fa0b690cccf03ea1d32deadc1c15c3039ee96d" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse가&lt;/a&gt; 지원하는 형식으로 모든 매트릭스에 제공된 희소 데이터에 대한 지원이 내장되어 있습니다. 그러나 최대 효율을 위해서는 &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix에&lt;/a&gt; 정의 된대로 CSR 매트릭스 형식을 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="46a0af0f073c85f45d179011b96dd0c21ed6963a" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bf90eed7a42877d9e674dd4cae64f5ef3af07a9" translate="yes" xml:space="preserve">
          <source>There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (&lt;code&gt;LassoCV&lt;/code&gt; or &lt;code&gt;LassoLarsCV&lt;/code&gt;), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (&lt;code&gt;LassoLarsIC&lt;/code&gt;) tends, on the opposite, to set high values of alpha.</source>
          <target state="translated">0이 아닌 계수의 복구를 위해 알파 매개 변수를 선택하는 일반적인 규칙은 없습니다. 교차 유효성 검사 ( &lt;code&gt;LassoCV&lt;/code&gt; 또는 &lt;code&gt;LassoLarsCV&lt;/code&gt; ) 로 설정할 수 있지만 불충분 한 모델로 이어질 수 있습니다. 관련이없는 소수의 변수를 포함하면 예측 점수에 해가되지 않습니다. BIC ( &lt;code&gt;LassoLarsIC&lt;/code&gt; )는 높은 알파 값을 설정하는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="ff0ab54a4c7a8a1cf35484dbae4d9dce95d5c026" translate="yes" xml:space="preserve">
          <source>There might be a difference in the scores obtained between &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;solver=liblinear&lt;/code&gt; or &lt;code&gt;LinearSVC&lt;/code&gt; and the external liblinear library directly, when &lt;code&gt;fit_intercept=False&lt;/code&gt; and the fit &lt;code&gt;coef_&lt;/code&gt; (or) the data to be predicted are zeroes. This is because for the sample(s) with &lt;code&gt;decision_function&lt;/code&gt; zero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;LinearSVC&lt;/code&gt; predict the negative class, while liblinear predicts the positive class. Note that a model with &lt;code&gt;fit_intercept=False&lt;/code&gt; and having many samples with &lt;code&gt;decision_function&lt;/code&gt; zero, is likely to be a underfit, bad model and you are advised to set &lt;code&gt;fit_intercept=True&lt;/code&gt; and increase the intercept_scaling.</source>
          <target state="translated">&lt;code&gt;fit_intercept=False&lt;/code&gt; 이고 fit &lt;code&gt;coef_&lt;/code&gt; (또는) 예측 데이터가 0 일 때 &lt;code&gt;solver=liblinear&lt;/code&gt; 또는 &lt;code&gt;LinearSVC&lt;/code&gt; 를사용한 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 과 외부 liblinear 라이브러리간에 직접 얻은 점수에 차이가있을 수 있습니다 . &lt;code&gt;decision_function&lt;/code&gt; _ &lt;code&gt;LinearSVC&lt;/code&gt; 0 인 샘플의 경우 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 및 LinearSVC 가 음수 클래스를 예측하는 반면 liblinear는 양수 클래스를 예측하기 때문입니다. 와 모델 있습니다 &lt;code&gt;fit_intercept=False&lt;/code&gt; 및 많은 샘플을 가지고 &lt;code&gt;decision_function&lt;/code&gt; 제로는 underfit, 나쁜 모델이 될 가능성이 높습니다 당신은 설정하는 것이 좋다 &lt;code&gt;fit_intercept=True&lt;/code&gt; 인터셉트 스케일링을 참 이십시오.</target>
        </trans-unit>
        <trans-unit id="4a4216d2986ca1c413a45927f7f8dc07ca811204" translate="yes" xml:space="preserve">
          <source>Therefore, a logarithmic (&lt;code&gt;np.log1p&lt;/code&gt;) and an exponential function (&lt;code&gt;np.expm1&lt;/code&gt;) will be used to transform the targets before training a linear regression model and using it for prediction.</source>
          <target state="translated">따라서 로그 ( &lt;code&gt;np.log1p&lt;/code&gt; ) 및 지수 함수 ( &lt;code&gt;np.expm1&lt;/code&gt; )를 사용하여 선형 회귀 모델을 학습하고 예측에 사용하기 전에 대상을 변환합니다.</target>
        </trans-unit>
        <trans-unit id="6912f2dbecf0d873a7ad1021b00fc015a0232427" translate="yes" xml:space="preserve">
          <source>These are transformers that are not intended to be used on features, only on supervised learning targets. See also &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transforming target in regression&lt;/a&gt; if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.</source>
          <target state="translated">이 기능은 기능에 사용되지 않으며 감독되는 학습 목표에만 사용됩니다. 학습을 위해 예측 대상을 변환하지만 원래 (변환되지 않은) 공간에서 모델을 평가하려면 &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;회귀 분석에서 대상 변환을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="9c615abfdf912d61f49e70314e0bacb6d3789d48" translate="yes" xml:space="preserve">
          <source>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</source>
          <target state="translated">이러한 분류기는 쉽게 계산할 수 있고 본질적으로 멀티 클래스이며 실제로 잘 작동하는 것으로 입증되었으며 튜닝 할 하이퍼 파라미터가 없기 때문에 폐쇄 형 솔루션이 있기 때문에 매력적입니다.</target>
        </trans-unit>
        <trans-unit id="fde55c65b8197fd0cbaa58300d107768af9ed389" translate="yes" xml:space="preserve">
          <source>These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.</source>
          <target state="translated">이러한 제약은 특정 로컬 구조를 적용하는 데 유용하지만 특히 샘플 수가 많은 경우 알고리즘을 더 빠르게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="ebd210a6b7ae21f6cafc3c41203edaaa46e25728" translate="yes" xml:space="preserve">
          <source>These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.</source>
          <target state="translated">이 데이터 세트는 scikit-learn에서 구현 된 다양한 알고리즘의 동작을 신속하게 설명하는 데 유용합니다. 그러나 실제 기계 학습 작업을 대표하기에는 너무 작은 경우가 많습니다.</target>
        </trans-unit>
        <trans-unit id="8d2ab26191aa60fb366e6a03a6fce9c7d01208ba" translate="yes" xml:space="preserve">
          <source>These environment variables should be set before importing scikit-learn.</source>
          <target state="translated">scikit-learn을 가져 오기 전에 이러한 환경 변수를 설정해야합니다.</target>
        </trans-unit>
        <trans-unit id="b821932825baa77bb11f8b1f95c8cc38b1d75bf5" translate="yes" xml:space="preserve">
          <source>These estimators are called similarly to their counterparts, with &amp;lsquo;CV&amp;rsquo; appended to their name.</source>
          <target state="translated">이 견적서는 이름에 'CV'가 추가되어 상대방과 유사하게 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="4895efc31f112ba17d5e8de5c88b9b84cbaac29a" translate="yes" xml:space="preserve">
          <source>These estimators are described in more detail below in &lt;a href=&quot;#histogram-based-gradient-boosting&quot;&gt;Histogram-Based Gradient Boosting&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31d7d129bfc794bb111166051ddf1fbfd1e21090" translate="yes" xml:space="preserve">
          <source>These estimators are still &lt;strong&gt;experimental&lt;/strong&gt;: their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import &lt;code&gt;enable_hist_gradient_boosting&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd57beb09d61462992a46fc07ecdff56e4d9c660" translate="yes" xml:space="preserve">
          <source>These estimators fit multiple regression problems (or tasks) jointly, while inducing sparse coefficients. While the inferred coefficients may differ between the tasks, they are constrained to agree on the features that are selected (non-zero coefficients).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fac22fa1718df2cc93fd78246f559caa4e598cf3" translate="yes" xml:space="preserve">
          <source>These examples illustrate the main features of the releases of scikit-learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bef6d4f1ba3e716c219b98d63998eb428a7438d" translate="yes" xml:space="preserve">
          <source>These families of algorithms are useful to find linear relations between two multivariate datasets: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; arguments of the &lt;code&gt;fit&lt;/code&gt; method are 2D arrays.</source>
          <target state="translated">이 알고리즘 계열은 두 개의 다변량 데이터 집합간에 선형 관계를 찾는 데 유용 합니다. &lt;code&gt;fit&lt;/code&gt; 방법 의 &lt;code&gt;X&lt;/code&gt; 및 &lt;code&gt;Y&lt;/code&gt; 인수 는 2D 배열입니다.</target>
        </trans-unit>
        <trans-unit id="1f2f0c6c7df23095dba3fa00aa5227521de2ff47" translate="yes" xml:space="preserve">
          <source>These fast estimators first bin the input samples &lt;code&gt;X&lt;/code&gt; into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are not yet supported, for instance some loss functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e90c7c20b495d718d6ee1761b52661a07d31f2ac" translate="yes" xml:space="preserve">
          <source>These figures aid in illustrating how a point cloud can be very flat in one direction&amp;ndash;which is where PCA comes in to choose a direction that is not flat.</source>
          <target state="translated">이러한 수치는 점 구름이 한 방향으로 매우 평평한 방법을 설명하는 데 도움이됩니다. PCA가 평평하지 않은 방향을 선택하게됩니다.</target>
        </trans-unit>
        <trans-unit id="5b3c06102d71e9aa21ce3635f214fb7544bfe93e" translate="yes" xml:space="preserve">
          <source>These functions have an &lt;code&gt;multioutput&lt;/code&gt; keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is &lt;code&gt;'uniform_average'&lt;/code&gt;, which specifies a uniformly weighted mean over outputs. If an &lt;code&gt;ndarray&lt;/code&gt; of shape &lt;code&gt;(n_outputs,)&lt;/code&gt; is passed, then its entries are interpreted as weights and an according weighted average is returned. If &lt;code&gt;multioutput&lt;/code&gt; is &lt;code&gt;'raw_values'&lt;/code&gt; is specified, then all unaltered individual scores or losses will be returned in an array of shape &lt;code&gt;(n_outputs,)&lt;/code&gt;.</source>
          <target state="translated">이 함수에는 각 개별 목표의 점수 또는 손실을 평균화하는 방법을 지정 하는 &lt;code&gt;multioutput&lt;/code&gt; 키워드 인수가 있습니다. 기본값은 &lt;code&gt;'uniform_average'&lt;/code&gt; 이며 출력에 균일하게 가중치를 적용한 평균을 지정합니다. 모양 의 &lt;code&gt;ndarray&lt;/code&gt; &lt;code&gt;(n_outputs,)&lt;/code&gt; 가 전달되면 해당 항목이 가중치로 해석되고 그에 따른 가중치 평균이 반환됩니다. 경우 &lt;code&gt;multioutput&lt;/code&gt; 인 &lt;code&gt;'raw_values'&lt;/code&gt; 가 지정되는 모든 변경되지 않은 개별 득점 또는 손실 형상의 배열에 반환한다 &lt;code&gt;(n_outputs,)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8f744234c4fef479ca52103694dddbbb39569d67" translate="yes" xml:space="preserve">
          <source>These functions return a tuple &lt;code&gt;(X, y)&lt;/code&gt; consisting of a &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; and an array of length &lt;code&gt;n_samples&lt;/code&gt; containing the targets &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">이 함수 는 &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy 배열 &lt;code&gt;X&lt;/code&gt; 및 대상 &lt;code&gt;y&lt;/code&gt; 를 포함하는 길이 &lt;code&gt;n_samples&lt;/code&gt; 배열 로 구성된 튜플 &lt;code&gt;(X, y)&lt;/code&gt; 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7ea4b087a47ece8eb67eda1c13d069b3e5f40d70" translate="yes" xml:space="preserve">
          <source>These generators produce a matrix of features and corresponding discrete targets.</source>
          <target state="translated">이 생성기는 피쳐 매트릭스와 해당하는 개별 타겟을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="ff830f502c3a30a0f4f3f842eff62f7849f20cad" translate="yes" xml:space="preserve">
          <source>These histogram-based estimators can be &lt;strong&gt;orders of magnitude faster&lt;/strong&gt; than &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; when the number of samples is larger than tens of thousands of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7870342f67a34d74383fe781d3e91593569275ef" translate="yes" xml:space="preserve">
          <source>These images how similar features are merged together using feature agglomeration.</source>
          <target state="translated">이 이미지는 기능 응집을 사용하여 유사한 기능이 어떻게 병합되는지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="2f6550a6d877a222d311c7f7d2e4efbab68b15f3" translate="yes" xml:space="preserve">
          <source>These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;), but also to build precomputed kernels, or similarity matrices.</source>
          <target state="translated">이 행렬은 Ward 클러스터링 ( &lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;계층 적 클러스터링&lt;/a&gt; ) 과 같은 연결 정보를 사용하는 추정기에서 연결을 부과 할 뿐만 아니라 사전 계산 된 커널 또는 유사성 매트릭스를 빌드 하는 데 사용될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f1476efa19922a5a7b34be362fc1e06a3ae81118" translate="yes" xml:space="preserve">
          <source>These metrics &lt;strong&gt;require the knowledge of the ground truth classes&lt;/strong&gt; while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).</source>
          <target state="translated">이러한 측정법 &lt;strong&gt;은&lt;/strong&gt; 실제 사용 가능한 거의 없지만 실제지도 &lt;strong&gt;수업에 대한 지식이&lt;/strong&gt; 필요하거나 인간 감독자가 직접지도하는 학습 환경과 같이 수동으로 할당해야합니다.</target>
        </trans-unit>
        <trans-unit id="102278a50cfd01c3d7a2cbdc18b76a5ce6b39772" translate="yes" xml:space="preserve">
          <source>These models allow for response variables to have error distributions other than a normal distribution:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b78634dcd7b938ef7f64bb3d2756751845f06a1" translate="yes" xml:space="preserve">
          <source>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt;&lt;code&gt;SelectKBest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt;&lt;code&gt;SelectPercentile&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">이 객체는 일 변량 점수와 p- 값을 반환하는 점수 매기기 함수를 입력으로 &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt; &lt;code&gt;SelectKBest&lt;/code&gt; &lt;/a&gt; (또는 SelectKBest 및 &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt; &lt;code&gt;SelectPercentile&lt;/code&gt; 에&lt;/a&gt; 대한 점수 만 ).</target>
        </trans-unit>
        <trans-unit id="7d570e698357345b9d25fb0f909251adbc540e40" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the attributes &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(b\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc9887aaf0c2ed20eb0f603451b50a8d7e26af7c" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the attributes &lt;code&gt;dual_coef_&lt;/code&gt; which holds the product \(y_i \alpha_i\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(b\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3403571e1d1dd77b054e08b8e749a4d9b5c4d316" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\)</source>
          <target state="translated">이 매개 변수는 차이 \ (\ alpha_i-\ alpha_i ^ * \)를 보유하는 &lt;code&gt;dual_coef_&lt;/code&gt; 멤버 ,지지 벡터를 보유하는 &lt;code&gt;support_vectors_&lt;/code&gt; 및 독립 항 \ (\ rho \)을 보유하는 &lt;code&gt;intercept_&lt;/code&gt; 를 통해 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9254aef96f1c8727db185406da2727e74822dda9" translate="yes" xml:space="preserve">
          <source>These quantities are also related to the (\(F_1\)) score, which is defined as the harmonic mean of precision and recall.</source>
          <target state="translated">이 수량은 (\ (F_1 \)) 점수와도 관련이 있으며,이 점수는 정확도와 재현의 조화 평균으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="8ec6209edcb97e57d934d74900289c4f7467ca16" translate="yes" xml:space="preserve">
          <source>These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.</source>
          <target state="translated">이들은지도 격자의 각 지점에서 측정 된 14 개의 지형지 물을 나타냅니다. 그리드의 위도 / 경도 값은 아래에 설명되어 있습니다. 누락 된 데이터는 -9999 값으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="e11ef00d62661e429b4b798a345a9c8d62a348e6" translate="yes" xml:space="preserve">
          <source>These steps are performed either a maximum number of times (&lt;code&gt;max_trials&lt;/code&gt;) or until one of the special stop criteria are met (see &lt;code&gt;stop_n_inliers&lt;/code&gt; and &lt;code&gt;stop_score&lt;/code&gt;). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.</source>
          <target state="translated">이 단계는 최대 횟수 ( &lt;code&gt;max_trials&lt;/code&gt; ) 또는 특수 중지 기준 중 하나가 충족 될 때까지 수행됩니다 ( &lt;code&gt;stop_n_inliers&lt;/code&gt; 및 &lt;code&gt;stop_score&lt;/code&gt; 참조 ). 최종 모델은 이전에 결정된 최상의 모델의 모든 내부 샘플 (합의 세트)을 사용하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="ffccb4ceb37928160a8178b1e93c390573c106cb" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;6&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eff89650d9905b662c6df80228e926f2f144c91b" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="translated">이 세 거리는 베타-분화 패밀리의 특별한 경우이며, 각각 \ (\ beta = 2, 1, 0 \) &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt; 입니다. 베타 발산은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="100dafc268c3e9d0b628da1715aed2440b14ba32" translate="yes" xml:space="preserve">
          <source>These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt;) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.</source>
          <target state="translated">이러한 처리량은 단일 프로세스에서 달성됩니다. 응용 프로그램의 처리량을 늘리는 확실한 방법 은 동일한 모델을 공유하는 추가 인스턴스 (일반적으로 &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt; 때문에 Python의 프로세스)를 생성하는 것 입니다. 부하를 분산시키기 위해 기계를 추가 할 수도 있습니다. 이를 달성하는 방법에 대한 자세한 설명은이 문서의 범위를 벗어납니다.</target>
        </trans-unit>
        <trans-unit id="b826ad64b72daa0458c9fdfb2862fce60b4db70b" translate="yes" xml:space="preserve">
          <source>They also have built-in support for missing values, which avoids the need for an imputer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e63971597c4df5f2dc150f90b566e1219b6a632a" translate="yes" xml:space="preserve">
          <source>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</source>
          <target state="translated">그것들은 희소하지 않으며, 즉 전체 샘플 / 기능 정보를 사용하여 예측을 수행한다.</target>
        </trans-unit>
        <trans-unit id="26f84bd6fe103542912ebb1fb588d29515a2fd6d" translate="yes" xml:space="preserve">
          <source>They can be loaded using the following functions:</source>
          <target state="translated">다음 기능을 사용하여로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f7ddbd9f45ee3f137b5eb656135dce7584351da0" translate="yes" xml:space="preserve">
          <source>They expose a &lt;code&gt;split&lt;/code&gt; method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</source>
          <target state="translated">입력 데이터 세트가 분할되도록 허용하고 선택한 교차 유효성 검사 전략의 각 반복에 대해 트레인 / 테스트 세트 색인을 생성 하는 &lt;code&gt;split&lt;/code&gt; 방법을 노출합니다 .</target>
        </trans-unit>
        <trans-unit id="03bbcc98e1d567dd0afc112fe378a99851c98226" translate="yes" xml:space="preserve">
          <source>They lose efficiency in high dimensional spaces &amp;ndash; namely when the number of features exceeds a few dozens.</source>
          <target state="translated">고차원 공간에서, 즉 피처 수가 수십을 초과하면 효율성이 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="a673690dbc33eee17ee6f3995f817097918876ff" translate="yes" xml:space="preserve">
          <source>This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</source>
          <target state="translated">이 스케일러는 중앙값을 제거하고 Quantile 범위 (기본값은 IQR : Interquartile Range)에 따라 데이터를 스케일링합니다. IQR은 1 분위 (25 분위)와 3 분위 (75 분위) 사이의 범위입니다.</target>
        </trans-unit>
        <trans-unit id="d9994b645c162ae9e80a2c6bc1c81390f2e92325" translate="yes" xml:space="preserve">
          <source>This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation helper function cross_val_score to warn when there is an error while fitting the estimator.</source>
          <target state="translated">이 경고는 메타 추정기 GridSearchCV 및 RandomizedSearchCV 및 교차 유효성 검증 헬퍼 함수 cross_val_score에서 사용되어 추정기를 피팅하는 동안 오류가 발생했을 때 경고합니다.</target>
        </trans-unit>
        <trans-unit id="7f5e3d23312509826c13f1663d34b7e7912ac4af" translate="yes" xml:space="preserve">
          <source>This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by &lt;code&gt;n_clusters&lt;/code&gt;. If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</source>
          <target state="translated">이 알고리즘은 입력 데이터를 CFT의 잎에서 직접 얻은 서브 클러스터 세트로 줄이기 때문에 인스턴스 또는 데이터 축소 방법으로 볼 수 있습니다. 이 축소 된 데이터는 글로벌 클러스터 러에 공급하여 추가 처리 할 수 ​​있습니다. 이 글로벌 클러스터 러는 &lt;code&gt;n_clusters&lt;/code&gt; 로 설정할 수 있습니다 . &lt;code&gt;n_clusters&lt;/code&gt; 가 None으로 설정되어 있으면 리프 의 하위 클러스터가 직접 판독됩니다. 그렇지 않으면 전역 클러스터링 단계에서 이러한 하위 클러스터를 전역 클러스터 (라벨)로 레이블을 지정하고 샘플은 가장 가까운 하위 클러스터의 전역 레이블에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="895ec6bb1e64f58ea59b9ae781fa620e703125a9" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 알고리즘은 문헌의 여러 작품을 포함합니다. 데이터 세트의 임의의 부분 집합이 샘플의 임의의 부분 집합으로 그려 질 때이 알고리즘을 Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]이라고&lt;/a&gt; 합니다. 샘플을 교체하여 채취하는 경우이 방법을 배깅 (Baging) &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]이라고&lt;/a&gt; 합니다. 데이터 세트의 임의의 부분 집합이 특징의 임의의 부분 집합으로 그려 질 때,이 방법은 무작위 부분 공간으로 알려져있다 &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . 마지막으로,베이스 추정기가 샘플과 피처의 서브셋을 기반으로 할 때이 방법을 랜덤 패치 &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="870122c945d57891e89c2ba931542331f1b4afdb" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 알고리즘은 문헌의 여러 작품을 포함합니다. 데이터 세트의 임의의 부분 집합이 샘플의 임의의 부분 집합으로 그려 질 때이 알고리즘을 Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]이라고&lt;/a&gt; 합니다. 샘플을 교체하여 채취하는 경우이 방법을 배깅 (Baging) &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]이라고&lt;/a&gt; 합니다. 데이터 세트의 임의의 부분 집합이 특징의 임의의 부분 집합으로 그려 질 때,이 방법은 무작위 부분 공간으로 알려져있다 &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . 마지막으로,베이스 추정기가 샘플과 피처의 서브셋을 기반으로 할 때이 방법을 랜덤 패치 &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="866e774dfaaba96a720c3090067c49c13cac935f" translate="yes" xml:space="preserve">
          <source>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, &lt;code&gt;n_iter&lt;/code&gt; can be set &amp;lt;=2 (at the cost of loss of precision).</source>
          <target state="translated">이 알고리즘은 랜덤 화를 사용하여 계산 속도를 높이기 위해 (대개 매우 좋은) 근사한 특이 값 분해를 찾습니다. 적은 수의 구성 요소 만 추출하려는 큰 행렬에서는 특히 빠릅니다. 더 빠른 속도를 얻기 위해 &lt;code&gt;n_iter&lt;/code&gt; 는 &amp;lt;= 2로 설정 될 수 있습니다 (정밀도 손실 비용).</target>
        </trans-unit>
        <trans-unit id="c5eff13aab5e02ba6e328f932a803b28ba41ee60" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size * n_features&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory. For sparse matrices, the input is converted to dense in batches (in order to be able to subtract the mean) which avoids storing the entire dense matrix at any one time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77977d31f5357c834112cbb27bbc98b5c64c10d7" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory.</source>
          <target state="translated">이 알고리즘은 &lt;code&gt;batch_size&lt;/code&gt; 순서로 메모리 복잡성이 일정 하므로 전체 파일을 메모리에로드하지 않고도 np.memmap 파일을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f443979a9a019e4d6947465617b7828e98c7b59f" translate="yes" xml:space="preserve">
          <source>This algorithm is illustrated below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9586aed3b1a5b6a2c44b32af5cc0558b6ad496a6" translate="yes" xml:space="preserve">
          <source>This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.</source>
          <target state="translated">이 알고리즘은 k = 2에 대한 정규화 된 컷을 해결합니다. 정규화 된 스펙트럼 클러스터링입니다.</target>
        </trans-unit>
        <trans-unit id="01c65a021c885e4e00baaaa5c6652a314a97daa3" translate="yes" xml:space="preserve">
          <source>This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.</source>
          <target state="translated">이 알고리즘은 항상 외부 큐가 없을 때 사용할 컴포넌트 수를 결정하기 위해 보유 데이터 또는 정보 이론적 기준이 필요한 액세스 가능한 모든 컴포넌트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="ddf04fe856314e7dd4ddddf49f4086029e04d842" translate="yes" xml:space="preserve">
          <source>This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:</source>
          <target state="translated">이것은이 분산 잡음이 존재하는 경우 확률 적 PCA보다 더 나은 모델 선택을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="3307a2458ebbefda8ea7fe8b898077ec4cadcf2c" translate="yes" xml:space="preserve">
          <source>This also works where final estimator is &lt;code&gt;None&lt;/code&gt;: all prior transformations are applied.</source>
          <target state="translated">최종 추정기가 &lt;code&gt;None&lt;/code&gt; 경우에도 작동합니다 . 모든 이전 변환이 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="c63b80512d8853cd76b24210ee07543da5c60fc4" translate="yes" xml:space="preserve">
          <source>This assumption is the base of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;Vector Space Model&lt;/a&gt; often used in text classification and clustering contexts.</source>
          <target state="translated">이 가정은 텍스트 분류 및 클러스터링 컨텍스트에서 자주 사용되는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;벡터 공간 모델&lt;/a&gt; 의 기초입니다 .</target>
        </trans-unit>
        <trans-unit id="a8d2386009078ecaad8deb62662ebfbef6798072" translate="yes" xml:space="preserve">
          <source>This attribute is not available if &lt;code&gt;refit&lt;/code&gt; is a function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8142b653ecd3322ad782e8a8807635d6c4c0325e" translate="yes" xml:space="preserve">
          <source>This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">이 교정으로 로그 손실이 줄어 듭니다. 대안은 기본 추정기의 수를 늘려서 로그 손실이 비슷하게 감소했음을 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="3ba422e075fa10216e381bf46530abda4e719fab" translate="yes" xml:space="preserve">
          <source>This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.</source>
          <target state="translated">이 호출은 높은 차원의 공간에서 문제가 될 수있는 apxq 행렬의 추정이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="345a3bd30c8553d3251f728b344d1bd100958670" translate="yes" xml:space="preserve">
          <source>This can be confirmed on a independent testing set with similar remarks:</source>
          <target state="translated">이는 비슷한 설명이있는 독립적 인 테스트 세트에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e60927eda9d0f07135f56fa39eaa1a8f1f9c2813" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="translated">이는 모델의 하이퍼 파라미터에 대한 &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;정보가없는&lt;/a&gt; 우선 순위 를 도입하여 수행 할 수 있습니다 . &lt;a href=&quot;#id2&quot;&gt;릿지 회귀 분석에&lt;/a&gt; 사용 된 \ (\ ell_ {2} \) 정규화 는 정밀도 \ (\ lambda ^ {-1} \)를 사용하여 매개 변수 \ (w \)보다 가우시안 하에서 최대 사후 추정값을 찾는 것과 같습니다. &lt;code&gt;lambda&lt;/code&gt; 수동으로 설정하는 대신 데이터에서 추정 할 임의의 변수로 처리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="bdd26b00d9d2e1f23dcd81c9f002391507919ff9" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#ridge-regression&quot;&gt;Ridge regression and classification&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa181018cfeb3219e1e67073f9c58ca90a0c4faa" translate="yes" xml:space="preserve">
          <source>This can be done by using the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; utility function.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt; 유틸리티 기능 을 사용하여 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="821e5435b62e56a883478da64d6731f6479103d5" translate="yes" xml:space="preserve">
          <source>This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised.</source>
          <target state="translated">입력 파일의 실제 기능 수보다 더 높은 값으로 설정할 수 있지만 더 낮은 값으로 설정하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="4a0bd36c1ccd6d51b38a900236d58695657d5aff" translate="yes" xml:space="preserve">
          <source>This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.</source>
          <target state="translated">이 클래스는 가우스 혼합 분포의 모수에 대한 대략적인 후방 분포를 유추 할 수 있습니다. 데이터에서 유효 개수의 구성 요소를 유추 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6130cf2c7564234b715447525f16ff596ebe842e" translate="yes" xml:space="preserve">
          <source>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</source>
          <target state="translated">이 클래스는 고정 된 시간 간격으로 관찰되는 시계열 데이터 샘플을 교차 검증하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="88afb4091eb2a25b2e2017ee8508b1ff5c5ae125" translate="yes" xml:space="preserve">
          <source>This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</source>
          <target state="translated">이 클래스는 데이터 세트의 다양한 서브 샘플에 다수의 무작위 의사 결정 트리 (일명 여분의 트리)에 맞는 메타 추정기를 구현하고 예측 정확도를 개선하고 과적 합을 제어하기 위해 평균화를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="dd60f6580be8e1908408c4fbfd8d3915f46e55b1" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">이 클래스는 liblinear, newton-cg, lbfgs 최적화 프로그램을 사용하여 로지스틱 회귀를 구현합니다. newton-cg, sag 및 lbfgs 솔버는 기본 공식을 사용한 L2 정규화 만 지원합니다. liblinear 솔버는 L2 페널티에 대해서만 이중 공식을 사용하여 L1 및 L2 정규화를 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="44487ffdb33876a6a42d281dea9cfa59f8e7af24" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="350693d0493245dcbd676a8f10e001d5020f745e" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="translated">이 클래스는 'liblinear'라이브러리, 'newton-cg', 'sag'및 'lbfgs'솔버를 사용하여 정규화 된 로지스틱 회귀를 구현합니다. 밀집된 입력과 드문 드문 한 입력을 모두 처리 할 수 ​​있습니다. 최적의 성능을 위해 C 순서 배열 또는 64 비트 부동 소수점을 포함하는 CSR 행렬을 사용하십시오. 다른 입력 형식은 변환 및 복사됩니다.</target>
        </trans-unit>
        <trans-unit id="7a8a7622df62d8a1b619a206bf179dc5e9e851ab" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. &lt;strong&gt;Note that regularization is applied by default&lt;/strong&gt;. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c107aea4a3526193efe1f31d97ccab92891d87f" translate="yes" xml:space="preserve">
          <source>This class implements the Graphical Lasso algorithm.</source>
          <target state="translated">이 클래스는 그래픽 올가미 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="456e7e6f7be68de425401d6f66fe28d8a1090538" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost-SAMME [2].</source>
          <target state="translated">이 클래스는 AdaBoost-SAMME [2]로 알려진 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="003b6bf538c58eeda3c6fd28b21967bfd8b6c40e" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost.R2 [2].</source>
          <target state="translated">이 클래스는 AdaBoost.R2 [2]로 알려진 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="63c4b52b10df12140782204ea7cf58fe0edab9de" translate="yes" xml:space="preserve">
          <source>This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">이 클래스는 가중치 분포에 대해 두 가지 유형의 사전 분배를 구현합니다. Dirichlet 분포를 갖는 유한 혼합물 모델 및 Dirichlet Process를 사용한 무한 혼합 모델. 실제로 Dirichlet Process 추론 알고리즘은 근사치이며 고정 된 최대 개수의 구성 요소 (스틱 분리 표현이라고 함)와 함께 잘린 분포를 사용합니다. 실제로 사용되는 구성 요소의 수는 거의 항상 데이터에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="e15e7e7d8d04d41fdd2a4f34183baa0366e86dea" translate="yes" xml:space="preserve">
          <source>This class inherits from PLS with mode=&amp;rdquo;A&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;, norm_y_weights=True and algorithm=&amp;rdquo;nipals&amp;rdquo;, but svd should provide similar results up to numerical errors.</source>
          <target state="translated">이 클래스는 mode =&amp;rdquo;A&amp;rdquo;및 deflation_mode =&amp;rdquo;canonical&amp;rdquo;, norm_y_weights = True 및 algorithm =&amp;rdquo;nipals&amp;rdquo;를 사용하여 PLS에서 상속하지만 svd는 숫자 오류까지 유사한 결과를 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="a0a1d1daa83e6ad727cb13fd589f28f37b44df20" translate="yes" xml:space="preserve">
          <source>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</source>
          <target state="translated">이 클래스는 ValueError 및 AttributeError를 모두 상속하여 예외 처리 및 이전 버전과의 호환성을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="d34b4e0a14d3a494edeba399329a47fb49b3ee6c" translate="yes" xml:space="preserve">
          <source>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</source>
          <target state="translated">이 클래스는 DictVectorizer 및 CountVectorizer에 대한 메모리 부족 대안으로, 대규모 (온라인) 학습 및 메모리가 부족한 상황 (예 : 임베디드 장치에서 예측 코드 실행)에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="8e34d865883b535f68990f01a240eaab9f87f080" translate="yes" xml:space="preserve">
          <source>This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">따라서이 클래스는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 의 초기 단계에서 사용하기에 적합합니다 .</target>
        </trans-unit>
        <trans-unit id="eef0760dd6d25fd731b9abce61656453bb690cee" translate="yes" xml:space="preserve">
          <source>This class is useful when the behavior of &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt; is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; provides a random sample (with replacement) of the train / test splits generated by &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 클래스는 &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt; 의 동작 이 필요한 경우에 유용 하지만 \ (P \) 그룹이 보류 된 모든 가능한 파티션을 생성하는 것이 엄청나게 비쌀 정도로 그룹 수는 충분히 큽니다. 이러한 시나리오에서 &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt; 은 &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; 에&lt;/a&gt; 의해 생성 된 기차 / 테스트 분할의 무작위 샘플 (교체 포함)을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="9d924f7a026f05763f1c88ce464db057fdea146c" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;a href=&quot;#sklearn.neighbors.DistanceMetric.get_metric&quot;&gt;&lt;code&gt;get_metric&lt;/code&gt;&lt;/a&gt; class method and the metric string identifier (see below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edf4c29bfa2afe43016dc0b6660ad50132bd51ec" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;code&gt;get_metric&lt;/code&gt; class method and the metric string identifier (see below). For example, to use the Euclidean distance:</source>
          <target state="translated">이 클래스는 빠른 거리 측정 기능에 대한 균일 한 인터페이스를 제공합니다. &lt;code&gt;get_metric&lt;/code&gt; 클래스 메소드와 메트릭 문자열 식별자 (아래 참조) 를 통해 다양한 메트릭에 액세스 할 수 있습니다 . 예를 들어, 유클리드 거리를 사용하려면 :</target>
        </trans-unit>
        <trans-unit id="336f533fd7cf96bc18f597ff7211d31f0cd62386" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.</source>
          <target state="translated">이 클래스는 밀도가 높고 드문 드문 한 입력을 모두 지원하며 멀티 클래스 지원은 일대일 구성표에 따라 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="aebcf6792861578bf4b4a69a0be71898d4023b6a" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input.</source>
          <target state="translated">이 클래스는 고밀도 및 희소 입력을 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="4042c6697e9df3310aa51f4f2bbe289c3d222c6e" translate="yes" xml:space="preserve">
          <source>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">이 클래스는 이름에 해당하는 행렬 열을 계산하기 위해 해시 함수를 사용하여 기호 기능 이름 (문자열) 시퀀스를 scipy.sparse 행렬로 변환합니다. 사용 된 해시 함수는 서명 된 32 비트 버전의 Murmurhash3입니다.</target>
        </trans-unit>
        <trans-unit id="afb2ca6e635ea8a6abf9d5cec38428c8ecdc147c" translate="yes" xml:space="preserve">
          <source>This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">이 분류 데이터 세트는 다차원 표준 정규 분포를 취하고 중첩 된 동심 다차원 구체로 구분 된 클래스를 정의하여 대략 같은 수의 샘플이 각 클래스 (\ (\ chi ^ 2 \) 분포의 사 분위수)에 있도록 구성합니다. .</target>
        </trans-unit>
        <trans-unit id="db081d4f3b8473550c6831dd235016867584f8ea" translate="yes" xml:space="preserve">
          <source>This classifier first converts the target values into &lt;code&gt;{-1, 1}&lt;/code&gt; and then treats the problem as a regression task (multi-output regression in the multiclass case).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c043eb0cb49ef78462962da4adb4099e130c09c" translate="yes" xml:space="preserve">
          <source>This classifier is sometimes referred to as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Least-squares_support-vector_machine&quot;&gt;Least Squares Support Vector Machines&lt;/a&gt; with a linear kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a35e593acabc67ebdf8fae8d0484d5f85056d4a7" translate="yes" xml:space="preserve">
          <source>This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.</source>
          <target state="translated">이 분류기는 다른 (실제) 분류기와 비교하기위한 간단한 기준으로 유용합니다. 실제 문제에는 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="4ceac36d1efc9bb429dd84350330a84101bb5c8c" translate="yes" xml:space="preserve">
          <source>This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:</source>
          <target state="translated">이 분류 기준은 주제 분류와 관련이 거의없는 메타 데이터를 제거했기 때문에 많은 F 점수를 잃었습니다. 훈련 데이터에서이 메타 데이터를 제거하면 더 많이 손실됩니다.</target>
        </trans-unit>
        <trans-unit id="064e5da463cfecd3ca166c4023a79d0a6500160d" translate="yes" xml:space="preserve">
          <source>This combination is implementing in &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt;, a transformer class that is mostly API compatible with &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is stateless, meaning that you don&amp;rsquo;t have to call &lt;code&gt;fit&lt;/code&gt; on it:</source>
          <target state="translated">이 조합에서 구현되는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt; 대부분과 호환 API입니다 변압기 클래스 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 는&lt;/a&gt; 당신이 호출 할 필요가 없습니다 것을 의미 상태를 저장 &lt;code&gt;fit&lt;/code&gt; 그것을 :</target>
        </trans-unit>
        <trans-unit id="1d56591f4aa7f0ebb864c2d8835af7dfb7f8f26b" translate="yes" xml:space="preserve">
          <source>This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument &lt;code&gt;axis=1&lt;/code&gt;, and reduce it to an array of size [M].</source>
          <target state="translated">이렇게하면 집계 된 피처의 값이 단일 값으로 결합되고 모양 [M, N] 배열과 키워드 인수 &lt;code&gt;axis=1&lt;/code&gt; 을 수용하고 크기 [M] 배열로 줄여야합니다.</target>
        </trans-unit>
        <trans-unit id="26c788fe31e79bd1bbf24fbba8a1b8342ff15ce1" translate="yes" xml:space="preserve">
          <source>This consumes less memory than shuffling the data directly.</source>
          <target state="translated">이것은 데이터를 직접 섞는 것보다 적은 메모리를 소비합니다.</target>
        </trans-unit>
        <trans-unit id="9c8cf431c4f1299ae41612f84a50a597aa4a1059" translate="yes" xml:space="preserve">
          <source>This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.</source>
          <target state="translated">입력 포인트와 해시 함수의 내적을 얻은 다음 투영의 부호 (양 / 음)에 따라 투영을 이진 문자열 배열로 변환하여 입력 데이터 점의 이진 해시를 만듭니다. 정렬 된 이진 해시 배열이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="75f340063df2a6996986c297d7d4423dc4417e05" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">이 교차 유효성 검사 개체는 StratifiedKFold와 ShuffleSplit의 병합으로, 계층화 된 무작위 배를 반환합니다. 접기는 각 클래스의 샘플 비율을 유지하여 만들어집니다.</target>
        </trans-unit>
        <trans-unit id="ab90d891a9b7f61040a0bd2fc78eae2488d53a3a" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.</source>
          <target state="translated">이 교차 유효성 검사 개체는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 변형입니다 . k 번째 스플릿에서는 첫 번째 k 폴드를 기차 세트로, (k + 1) 폴드를 테스트 세트로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c3d6a6171342c06e134b7087a7e83e3f80ba8a79" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="translated">이 교차 유효성 검사 개체는 계층화 된 접기를 반환하는 KFold 변형입니다. 접기는 각 클래스의 샘플 비율을 유지하여 만들어집니다.</target>
        </trans-unit>
        <trans-unit id="2c34ca372157ddad0d3d18ffb66a8717775276f6" translate="yes" xml:space="preserve">
          <source>This data sets consists of 3 different types of irises&amp;rsquo; (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray</source>
          <target state="translated">이 데이터 세트는 150x4 numpy.ndarray에 저장된 3 가지 유형의 홍채 (Setosa, Versicolour 및 Virginica) 꽃잎과 꽃받침 길이로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="158d23b76a72fb850a277110200a4b319b51d7f5" translate="yes" xml:space="preserve">
          <source>This database is also available through the UW CS ftp server:</source>
          <target state="translated">이 데이터베이스는 UW CS ftp 서버를 통해서도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8f0ed5f875aa21e65ca9d6116dc0e918ff34c0ff" translate="yes" xml:space="preserve">
          <source>This dataset consists of 20,640 samples and 9 features.</source>
          <target state="translated">이 데이터 세트는 20,640 개의 샘플과 9 개의 기능으로 구성되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="2e15d3adbea56bdf31e76cb4ee55fcf6dfb7c05f" translate="yes" xml:space="preserve">
          <source>This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:</source>
          <target state="translated">이 데이터 세트는 인터넷을 통해 수집 된 유명한 사람들의 JPEG 사진 모음이며 모든 세부 사항은 공식 웹 사이트에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1c0b9129c637e05601004735cb7bfdbdba431796" translate="yes" xml:space="preserve">
          <source>This dataset is described in Celeux et al [1]. as:</source>
          <target state="translated">이 데이터 세트는 Celeux et al [1]에 설명되어 있습니다. 같이:</target>
        </trans-unit>
        <trans-unit id="e92704f97c6e77c413a0405e7cf7dd2e32191660" translate="yes" xml:space="preserve">
          <source>This dataset is described in Friedman [1] and Breiman [2].</source>
          <target state="translated">이 데이터 세트는 Friedman [1] 및 Breiman [2]에 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="473d3b557a1c23b381f633c2c2acf0c38c2a328f" translate="yes" xml:space="preserve">
          <source>This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&amp;rsquo;d have to first transform it into a feature vector with length 64.</source>
          <target state="translated">이 데이터 세트는 1797 8x8 이미지로 구성됩니다. 아래에 표시된 것과 같이 각 이미지는 손으로 쓴 숫자입니다. 이와 같은 8x8 그림을 활용하려면 먼저 길이가 64 인 특징 벡터로 변환해야합니다.</target>
        </trans-unit>
        <trans-unit id="39a3f4ca0d0c444ed57d3ff159dafbaf0cf5d2c9" translate="yes" xml:space="preserve">
          <source>This dataset is suitable for multi-ouput regression tasks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73cdbbbbdb25af126933f658f4b065e86b9c1a55" translate="yes" xml:space="preserve">
          <source>This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).</source>
          <target state="translated">이 데이터 세트는 종의 지리적 분포를 나타냅니다. 데이터 세트는 Phillips et. 알. (2006).</target>
        </trans-unit>
        <trans-unit id="e72397d5f7691c5c60df49442bb007cee4717786" translate="yes" xml:space="preserve">
          <source>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</source>
          <target state="translated">이 데이터 세트는 인구 조사 블록 그룹당 하나의 행을 사용하여 1990 년 미국 인구 조사에서 파생되었습니다. 블록 그룹은 미국 인구 조사국이 표본 데이터를 게시하는 가장 작은 지리적 단위입니다 (블록 그룹의 인구는 일반적으로 600 ~ 3,000 명).</target>
        </trans-unit>
        <trans-unit id="47bf559cf1abc9fb33a96a120f583ad0bcd0f9f8" translate="yes" xml:space="preserve">
          <source>This dataset was obtained from the StatLib repository. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</source>
          <target state="translated">이 데이터 세트는 StatLib 저장소에서 얻었습니다. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4668c093fae272aec3196fc6d39e4e5a4eede980" translate="yes" xml:space="preserve">
          <source>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="translated">이 데이터 세트는 Carnegie Mellon University에서 유지 관리되는 StatLib 라이브러리에서 가져 왔습니다.</target>
        </trans-unit>
        <trans-unit id="9b1b90be23e0adb200134bcc2570822a79ae6e88" translate="yes" xml:space="preserve">
          <source>This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.</source>
          <target state="translated">이것은 소량의 레이블링 된 데이터가 있어도 레이블 전파 학습이 좋은 경계를 학습 함을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="33d36bf521beb70b7b2f4b7e5d24d58f96f46c86" translate="yes" xml:space="preserve">
          <source>This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; for normalization):</source>
          <target state="translated">이 설명은 분류기로 공급하기에 적합한 희소 한 2 차원 행렬로 벡터화 될 수 있습니다 ( 정규화 를 위해 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;text.TfidfTransformer&lt;/code&gt; &lt;/a&gt; 로 파이프 된 후 ).</target>
        </trans-unit>
        <trans-unit id="12f7e332bc936dbb460a17349dda07780cab7782" translate="yes" xml:space="preserve">
          <source>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</source>
          <target state="translated">이는이 함수를 사용하여 하나의 메트릭 만 반환하는 경우 어떤 경고를할지 결정합니다.</target>
        </trans-unit>
        <trans-unit id="a1a66d0ad9255f63c11b93170b95da4e6eeaea4f" translate="yes" xml:space="preserve">
          <source>This downscaling is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; for &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;.</source>
          <target state="translated">이 다운 스케일링은 &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;에서 &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; 라고합니다.</target>
        </trans-unit>
        <trans-unit id="edeebc499fdf886cf2b1fe82f9cc25a148384f70" translate="yes" xml:space="preserve">
          <source>This early stopping strategy is activated if &lt;code&gt;early_stopping=True&lt;/code&gt;; otherwise the stopping criterion only uses the training loss on the entire input data. To better control the early stopping strategy, we can specify a parameter &lt;code&gt;validation_fraction&lt;/code&gt; which set the fraction of the input dataset that we keep aside to compute the validation score. The optimization will continue until the validation score did not improve by at least &lt;code&gt;tol&lt;/code&gt; during the last &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations. The actual number of iterations is available at the attribute &lt;code&gt;n_iter_&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;early_stopping=True&lt;/code&gt; 인 경우이 조기 중지 전략이 활성화됩니다 . 그렇지 않으면 정지 기준은 전체 입력 데이터에 대한 훈련 손실 만 사용합니다. 조기 중지 전략을보다 효과적으로 제어 하기 위해 유효성 검사 점수를 계산하기 위해 보관해야하는 입력 데이터 집합의 비율을 설정하는 &lt;code&gt;validation_fraction&lt;/code&gt; 매개 변수를 지정할 수 있습니다 . 검증 점수 이상으로 개선되지 않았다 때까지 최적화가 계속됩니다 &lt;code&gt;tol&lt;/code&gt; 지난 동안 &lt;code&gt;n_iter_no_change&lt;/code&gt; 의 반복. 실제 반복 횟수는 &lt;code&gt;n_iter_&lt;/code&gt; 속성에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6528dcf2523991bd357ca56474b42d537ada09b8" translate="yes" xml:space="preserve">
          <source>This embedding can also &amp;lsquo;work&amp;rsquo; even if the &lt;code&gt;adjacency&lt;/code&gt; variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).</source>
          <target state="translated">&lt;code&gt;adjacency&lt;/code&gt; 변수가 그래프의 인접 행렬이 아니라 샘플 사이의 유사성 또는 유사성 행렬 (예 : 유클리드 거리 행렬 또는 k-NN 행렬의 열 커널) 인 경우에도이 임베딩은 '작동'할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ac8e43e8e0acd749c6d9f51af67c9e65cc70e9b4" translate="yes" xml:space="preserve">
          <source>This enables ducktyping by hasattr returning True according to the sub-estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662188aeeffeee289aab2f0d97150266d90c022d" translate="yes" xml:space="preserve">
          <source>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</source>
          <target state="translated">이 인코딩은 표준 데이터가 포함 된 많은 scikit-learn 추정기, 특히 선형 모델 및 SVM에 범주 형 데이터를 제공하는 데 필요합니다.</target>
        </trans-unit>
        <trans-unit id="752036d9bd5ae374e975c046f504e0b38de39538" translate="yes" xml:space="preserve">
          <source>This estimator</source>
          <target state="translated">이 견적 자</target>
        </trans-unit>
        <trans-unit id="f8a8301fe86e970315ab1f664d0852d178958868" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ceeb9f387752a577aeb048b3f21cd319ecfb48" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="translated">이 추정기를 사용하면 입력의 다른 열 또는 열 하위 집합을 개별적으로 변환하고 결과를 단일 피처 공간으로 결합 할 수 있습니다. 이는 여러 특징 추출 메커니즘 또는 변환을 단일 변환기로 결합하기 위해 이기종 또는 컬럼 데이터에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="02199e2b9b2bd941c7464261eedf68a6fd2d82e2" translate="yes" xml:space="preserve">
          <source>This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.</source>
          <target state="translated">이 추정기는 입력 데이터와 병렬로 변환기 객체 목록을 적용한 다음 결과를 연결합니다. 이는 여러 기능 추출 메커니즘을 단일 변압기에 결합하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="a93ab3d6ae360c030a56bd4fabca46c42abdaf54" translate="yes" xml:space="preserve">
          <source>This estimator approximates a slightly different version of the additive chi squared kernel then &lt;code&gt;metric.additive_chi2&lt;/code&gt; computes.</source>
          <target state="translated">이 추정기는 약간 다른 버전의 추가 카이 제곱 커널과 &lt;code&gt;metric.additive_chi2&lt;/code&gt; 다음 metric.additive_chi2가 계산합니다.</target>
        </trans-unit>
        <trans-unit id="53ea424698dba4ed1ec741d2d0ce2fbd09225a41" translate="yes" xml:space="preserve">
          <source>This estimator can be used to model different GLMs depending on the &lt;code&gt;power&lt;/code&gt; parameter, which determines the underlying distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57f1dab8dd3e838e06f9128461ff865667eb2891" translate="yes" xml:space="preserve">
          <source>This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">이 추정기는 다변량 회귀를 기본적으로 지원합니다 (예 : y가 2 차원 배열 [n_samples, n_targets] 인 경우).</target>
        </trans-unit>
        <trans-unit id="534f21211e3056d3896d05b949c661c4f992dd5f" translate="yes" xml:space="preserve">
          <source>This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7130a824407818977c24f15960ee70da6f59d9ca" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the &lt;code&gt;partial_fit&lt;/code&gt; method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab45bcef3fd31ebffe9c4724334c2d64921dae44" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="translated">이 추정기는 확률 적 기울기 하강 (SGD) 학습을 통해 정규화 된 선형 모델을 구현합니다. 손실의 기울기는 한 번에 각 샘플마다 추정되며 감소하는 일정 (일명 학습 속도)에 따라 모델이 업데이트됩니다. SGD는 미니 배치 (온라인 / 핵심) 학습을 허용합니다. partial_fit 방법을 참조하십시오. 기본 학습 속도 일정을 사용하여 최상의 결과를 얻으려면 데이터의 평균 및 단위 분산이 0이어야합니다.</target>
        </trans-unit>
        <trans-unit id="7005478518d9bca348fcae966cb157ede91131ca" translate="yes" xml:space="preserve">
          <source>This estimator is much faster than &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; for big datasets (n_samples &amp;gt;= 10 000).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="505ce7903463cb2537c439b476cf7830c59f9934" translate="yes" xml:space="preserve">
          <source>This estimator is much faster than &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; for big datasets (n_samples &amp;gt;= 10 000).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fdb17bfdb14f498d9377f8ca9b7cc2199a41d7f" translate="yes" xml:space="preserve">
          <source>This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.</source>
          <target state="translated">이 추정기는 상태 비 저장 (생성자 매개 변수 외에)이며, fit 메소드는 파이프 라인에서 사용될 때 아무 것도하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b10e025b81eb3a8b5b21ad615292ad7338f1e2a3" translate="yes" xml:space="preserve">
          <source>This estimator is still &lt;strong&gt;experimental&lt;/strong&gt; for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt;: convergence criteria (&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/issues/14338&quot;&gt;#14338&lt;/a&gt;), default estimators (&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/issues/13286&quot;&gt;#13286&lt;/a&gt;), and use of random state (&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/issues/15611&quot;&gt;#15611&lt;/a&gt;). To use it, you need to explicitly import &lt;code&gt;enable_iterative_imputer&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ba8a49b5a7145a75b81ed477df1577dc2f0d079" translate="yes" xml:space="preserve">
          <source>This estimator is still &lt;strong&gt;experimental&lt;/strong&gt; for now: the predictions and the API might change without any deprecation cycle. To use it, you need to explicitly import &lt;code&gt;enable_hist_gradient_boosting&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c7878b26bf7f32e88e4f83c2d2d772f7f3023cd" translate="yes" xml:space="preserve">
          <source>This estimator is still &lt;strong&gt;experimental&lt;/strong&gt; for now: the predictions and the API might change without any deprecation cycle. To use it, you need to explicitly import &lt;code&gt;enable_iterative_imputer&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8c9fb1adc7eae2561c12b442ef315ae822391e8" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42d08f109b32ce107e9f058e7449521b0b6eab28" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.</source>
          <target state="translated">이 추정기는 각 기능을 개별적으로 조정하고 트레이닝 세트의 지정된 범위 (예 : 0과 1)에 있도록 변환합니다.</target>
        </trans-unit>
        <trans-unit id="ce7850baf5a7a3e7ef75716db2d2e4af71c92137" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.</source>
          <target state="translated">이 추정기는 훈련 세트에서 각 특징의 최대 절대 값이 1.0이되도록 각각의 특징을 개별적으로 스케일하고 변환합니다. 데이터를 이동 / 중심화하지 않으므로 희소성을 파괴하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0ff92b3f8e56701f386ccbc5279ec5fdb2974bc0" translate="yes" xml:space="preserve">
          <source>This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</source>
          <target state="translated">이 추정기는 훈련 세트에서 각 특징의 최대 절대 값이 1.0이되도록 각각의 특징을 개별적으로 스케일링합니다.</target>
        </trans-unit>
        <trans-unit id="354214ed410106228bbb593cd82a49f32a2508f8" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.</source>
          <target state="translated">이 추정기는 두 개의 알고리즘, 즉 빠른 무작위 SVD 솔버와 ARPACK을 (X * XT) 또는 (XT * X)에서 고유 한 솔버로 사용하는 &quot;순진한&quot;알고리즘 중 더 효율적인 알고리즘을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="0a991e8d6eff0a5b7f5a276b54be6da66c4dae45" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on &lt;code&gt;X * X.T&lt;/code&gt; or &lt;code&gt;X.T * X&lt;/code&gt;, whichever is more efficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="438c738ad18e280c91d7884cd24490873bfed375" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc, making sure that the estimator complies with &lt;code&gt;scikit-learn&lt;/code&gt; conventions as detailed in &lt;a href=&quot;https://scikit-learn.org/0.23/developers/develop.html#rolling-your-own-estimator&quot;&gt;Rolling your own estimator&lt;/a&gt;. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5861f671a7b7a1c102cd9c37c8bf2e273de13" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="translated">이 추정기는 입력 검증, 형태 등에 대한 광범위한 테스트 슈트를 실행합니다. Estimator 클래스가 sklearn.base의 해당 믹스 인에서 상속하는 경우 분류기, 회귀 자, 클러스터링 또는 변환기에 대한 추가 테스트가 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="7011f5e2b484f266188acd0aba4f3d3175f11ed0" translate="yes" xml:space="preserve">
          <source>This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).</source>
          <target state="translated">이 예는 또한 리지드 회귀를 조건이 잘못된 행렬에 적용하는 것이 유용하다는 것을 보여줍니다. 이러한 행렬의 경우 목표 변수가 약간 변경되면 계산 된 가중치가 크게 달라질 수 있습니다. 이러한 경우이 변동 (노이즈)을 줄이기 위해 특정 정규화 (알파)를 설정하는 것이 유용합니다.</target>
        </trans-unit>
        <trans-unit id="66087b50dbc537ea3a892d00ee467bc12eeadd33" translate="yes" xml:space="preserve">
          <source>This example applies to &lt;a href=&quot;../../datasets/index#olivetti-faces-dataset&quot;&gt;The Olivetti faces dataset&lt;/a&gt; different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f82721c4674480adff4241dbcac8e543f16c868" translate="yes" xml:space="preserve">
          <source>This example applies to olivetti_faces different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; &lt;/a&gt; 모듈의 olivetti_faces 다른 감독되지 않은 매트릭스 분해 (치수 감소) 방법에 적용됩니다 ( &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;구성 요소의 신호 분해 (매트릭스 인수 분해 문제)&lt;/a&gt; 장 참조 ).</target>
        </trans-unit>
        <trans-unit id="209229078f7e258d4985eec4947d8dca83616df9" translate="yes" xml:space="preserve">
          <source>This example balances model complexity and cross-validated score by finding a decent accuracy within 1 standard deviation of the best accuracy score while minimising the number of PCA components [1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33924f5409489cd3edd1b22f28ee011b17a585da" translate="yes" xml:space="preserve">
          <source>This example compares 2 dimensionality reduction strategies:</source>
          <target state="translated">이 예는 2 가지 차원 축소 전략을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="d2a9de2244899372ce613f7320d0c8868284b849" translate="yes" xml:space="preserve">
          <source>This example compares different (linear) dimensionality reduction methods applied on the Digits data set. The data set contains images of digits from 0 to 9 with approximately 180 samples of each class. Each image is of dimension 8x8 = 64, and is reduced to a two-dimensional data point.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ba8c26b14d0dc5555ed6b18d75cbed15c385668" translate="yes" xml:space="preserve">
          <source>This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.</source>
          <target state="translated">이 예에서는 중첩되지 않은 중첩 유효성 검사 전략을 홍채 데이터 세트의 분류 자에서 비교합니다. 중첩 교차 검증 (CV)은 종종 하이퍼 파라미터도 최적화해야하는 모델을 훈련시키는 데 사용됩니다. 중첩 된 CV는 기본 모델 및 (하이퍼) 매개 변수 검색의 일반화 오류를 추정합니다. 중첩되지 않은 CV를 최대화하는 매개 변수를 선택하면 모델이 데이터 세트에 바이어스되어 지나치게 낙관적 인 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="7ef1cb506f6769f9ea8f57cc850c6090d62898eb" translate="yes" xml:space="preserve">
          <source>This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.</source>
          <target state="translated">이 예에서는 make_blobs를 사용하여 생성 된 100,000 개의 샘플과 2 개의 기능이있는 합성 데이터 세트에서 Birch (글로벌 클러스터링 단계 유무) 및 MiniBatchKMeans의 타이밍을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="8901e1f5225dc1b7e06d2fabb8f06f19a3753c45" translate="yes" xml:space="preserve">
          <source>This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;Pipeline&lt;/code&gt; to optimize over different classes of estimators in a single CV run &amp;ndash; unsupervised &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;NMF&lt;/code&gt; dimensionality reductions are compared to univariate feature selection during the grid search.</source>
          <target state="translated">이 예제는 차원 축소를 지원하고 지원 벡터 분류기를 사용한 예측을 수행하는 파이프 라인을 구성합니다. 단일 CV 실행에서 다양한 클래스의 추정량을 최적화하기 위해 &lt;code&gt;GridSearchCV&lt;/code&gt; 및 &lt;code&gt;Pipeline&lt;/code&gt; 을 사용하는 방법을 보여줍니다. 감독되지 않은 &lt;code&gt;PCA&lt;/code&gt; 및 &lt;code&gt;NMF&lt;/code&gt; 차원 감소는 그리드 검색 중 일 변량 피처 선택과 비교됩니다.</target>
        </trans-unit>
        <trans-unit id="ebd831df4448cf766c6eb0e33a12d358fbfa3057" translate="yes" xml:space="preserve">
          <source>This example demonstrates Gradient Boosting to produce a predictive model from an ensemble of weak predictive models. Gradient boosting can be used for regression and classification problems. Here, we will train a model to tackle a diabetes regression task. We will obtain the results from &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 regression trees of depth 4.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b084d11db0bc218128b35a7afe55ac9fa7c4daf1" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:</source>
          <target state="translated">이 예는 능형 회귀를 사용하여 차수가 n_degree 인 함수를 근사하는 방법을 보여줍니다. 구체적으로, n_samples 1d 포인트에서 Vandermonde 행렬을 만들면 충분합니다. 이것은 n_samples x n_degree + 1이며 다음 형식을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="8509d7895b98e9b3d2d07ed03eae90baa68f1c72" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.</source>
          <target state="translated">이 예제는 Spectral Biclustering 알고리즘을 사용하여 바둑판 데이터 세트를 생성하고 bicluster하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="fb9a20a0b6ffdb624c38c357324f211d180af27f" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.</source>
          <target state="translated">이 예제는 Spectral Co-Clustering 알고리즘을 사용하여 데이터 세트를 생성하고 bicluster하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="9b609f5368296ac180488994b6106bba8395b9b3" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e024e96a6e8109f327c2d890a3a85ee374e03bd" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.</source>
          <target state="translated">이 예제는 다양한 유형의 기능이 포함 된 데이터 세트에서 &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하는 방법을 보여줍니다 . 우리는 20 개의 뉴스 그룹 데이터 세트를 사용하고 제목 줄과 본문에 대해 본문의 임시 기능과 함께 별도의 파이프 라인에서 주제별 기능을 계산합니다. ColumnTransformer를 사용하여 가중치와 결합하고 마지막으로 결합 된 기능 세트에 대한 분류자를 훈련시킵니다.</target>
        </trans-unit>
        <trans-unit id="76d491fec0fed042e6d7927f2dc124b698f9bded" translate="yes" xml:space="preserve">
          <source>This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &amp;lsquo;comp.os.ms-windows.misc&amp;rsquo; category is excluded because it contains many posts containing nothing but data.</source>
          <target state="translated">이 예는 20 개의 뉴스 그룹 데이터 세트에 대한 스펙트럼 공동 클러스터링 알고리즘을 보여줍니다. 'comp.os.ms-windows.misc'카테고리는 데이터 만 포함하는 게시물이 많으므로 제외됩니다.</target>
        </trans-unit>
        <trans-unit id="6186f51b55d8cba756254a15fe651e5e8504791a" translate="yes" xml:space="preserve">
          <source>This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components.</source>
          <target state="translated">이 예는 가우스 랜덤 변수의 혼합에서 샘플링되지 않은 데이터에 맞는 가우스 혼합 모델의 동작을 보여줍니다. 데이터 세트는 잡음이있는 사인 곡선을 따라 느슨하게 이격 된 100 개의 점으로 구성됩니다. 따라서 가우스 구성 요소의 수에 대한 근거 값은 없습니다.</target>
        </trans-unit>
        <trans-unit id="a1949f51dde2d60d7d4d1e707d145c1301537434" translate="yes" xml:space="preserve">
          <source>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.</source>
          <target state="translated">이 예제는 레이블 확산 모델을 학습하여 필기 숫자를 매우 적은 레이블 집합으로 분류함으로써 반지도 학습의 힘을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="c6020c6a7334e89ced3b2c5f4b02f4ed9989e37f" translate="yes" xml:space="preserve">
          <source>This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called &lt;strong&gt;underfitting&lt;/strong&gt;. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will &lt;strong&gt;overfit&lt;/strong&gt; the training data, i.e. it learns the noise of the training data. We evaluate quantitatively &lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.</source>
          <target state="translated">이 예는 과적 합 및 과적 합 문제와 다항식 피처와 함께 선형 회귀를 사용하여 비선형 함수를 근사화하는 방법을 보여줍니다. 그림은 코사인 함수의 일부인 근사하려는 함수를 보여줍니다. 또한 실제 함수의 샘플과 다른 모델의 근사값이 표시됩니다. 모델의 각도는 다항식입니다. 우리는 선형 함수 (1 차 다항식)가 훈련 샘플에 적합하지 않다는 것을 알 수 있습니다. 이것을 &lt;strong&gt;언더 피팅&lt;/strong&gt; 이라고 &lt;strong&gt;합니다&lt;/strong&gt; . 차수가 4 인 다항식은 실제 함수를 거의 완벽하게 근사합니다. 그러나, 더 높은 정도의 모델은 훈련 데이터에 &lt;strong&gt;적합&lt;/strong&gt; 할 것입니다 . 즉, 훈련 데이터의 소음을 학습합니다. 정량적으로 평가&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt; 단순성과&lt;/strong&gt;교차 검증을 사용하여 &lt;strong&gt;과적 합&lt;/strong&gt; / 검증 세트에서 평균 제곱 오차 (MSE)를 계산할수록 모형이 훈련 데이터에서 올바르게 일반화 될 가능성이 낮습니다.</target>
        </trans-unit>
        <trans-unit id="a2d558b6f5e9fa98f7c27a3a7352d216289c9012" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt;&lt;code&gt;PowerTransformer&lt;/code&gt;&lt;/a&gt; to map data from various distributions to a normal distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab8d51c9ac9762c2930c84a5a03c1c12345a6627" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; to map data from various distributions to a normal distribution.</source>
          <target state="translated">이 예제는 &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; 를 통한 Box-Cox 및 Yeo-Johnson 변환의 사용을 보여줍니다 .PowerTransformer 는 다양한 분포에서 정규 분포로 데이터를 매핑합니다.</target>
        </trans-unit>
        <trans-unit id="9b62ef0be0bf7ce49acab00a23e04164c0becf9d" translate="yes" xml:space="preserve">
          <source>This example does not perform any learning over the data (see &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of classification based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in geospatial coordinates.</source>
          <target state="translated">이 예제는 데이터에 대한 학습을 ​​수행하지 않습니다 ( 이 데이터 세트의 속성을 기반으로 한 분류 예제는 &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;종 분포 모델링&lt;/a&gt; 참조 ). 관측 된 데이터 포인트의 커널 밀도 추정값을 지리 공간 좌표로 표시합니다.</target>
        </trans-unit>
        <trans-unit id="03eea75ae69c05ca0f9dc1a13d89bbd210d48145" translate="yes" xml:space="preserve">
          <source>This example doesn&amp;rsquo;t show it, as we&amp;rsquo;re in a low-dimensional space, but another advantage of the Dirichlet process model is that it can fit full covariance matrices effectively even when there are less examples per cluster than there are dimensions in the data, due to regularization properties of the inference algorithm.</source>
          <target state="translated">이 예는 우리가 저 차원 공간에 있기 때문에 그것을 보여주지 않지만 Dirichlet 프로세스 모델의 또 다른 장점은 추론 알고리즘의 정규화 속성으로 인한 데이터.</target>
        </trans-unit>
        <trans-unit id="7b398c0b1dbb0f3edb9cc17097a9c9f8696a24cc" translate="yes" xml:space="preserve">
          <source>This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.</source>
          <target state="translated">이 예에서는 여러 가지 감독되지 않은 학습 기술을 사용하여 과거 시세의 변형에서 주식 시장 구조를 추출합니다.</target>
        </trans-unit>
        <trans-unit id="41937f256baaea1198c4d043d4bb81b61df0d50b" translate="yes" xml:space="preserve">
          <source>This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.</source>
          <target state="translated">이 예는 최소 제곱 손실과 깊이 4의 회귀 트리가 500 인 그라디언트 부스팅 모델에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="e8121408498cf6fb8c886d50eef2580465ff3307" translate="yes" xml:space="preserve">
          <source>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &amp;ldquo;Gaussian quantiles&amp;rdquo; clusters (see &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.</source>
          <target state="translated">이 예제는 두 개의 &quot;Gaussian Quantile&quot;클러스터 ( &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt; &lt;/a&gt; 참조 ) 로 구성된 비선형 적으로 분리 가능한 분류 데이터 세트에 AdaBoosted 의사 결정 스텀프를 맞추고 의사 결정 경계 및 의사 결정 점수를 표시합니다. 결정 점수의 분포는 클래스 A 및 B의 샘플에 대해 별도로 표시됩니다. 각 샘플의 예측 된 클래스 레이블은 결정 점수의 부호에 의해 결정됩니다. 결정 점수가 0보다 큰 샘플은 B로 분류되고 그렇지 않으면 A로 분류됩니다. 결정 점수의 크기는 예측 된 클래스 레이블과 유사도를 결정합니다. 또한, 예를 들어, 일부 값 이상의 결정 점수를 갖는 샘플만을 선택함으로써, 원하는 순도의 클래스 B를 포함하는 새로운 데이터 세트를 구축 할 수있다.</target>
        </trans-unit>
        <trans-unit id="02c230a18f98a72777c5f2652062014b16511fe8" translate="yes" xml:space="preserve">
          <source>This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the nearest neighbor along each axis.</source>
          <target state="translated">이 예제는 그래프를 표시하기 위해 시각화가 중요하므로 상당한 양의 시각화 관련 코드가 있습니다. 과제 중 하나는 라벨을 겹쳐서 겹치는 것을 최소화하는 것입니다. 이를 위해 각 축을 따라 가장 가까운 이웃의 방향을 기반으로 휴리스틱을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="6aeec4c0fd48150be3eb10918e11b6fbb03261c5" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="259139974bd9ca7304e763dff02af979bb7908e6" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;code&gt;DotProduct&lt;/code&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="translated">이 예는 XOR 데이터의 GPC를 보여줍니다. 고정식 등방성 커널 ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; )과 고정되지 않은 커널 ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt; )이 비교됩니다. 이 특정 데이터 집합 에서 클래스 경계는 선형이고 좌표 축과 일치하기 때문에 &lt;code&gt;DotProduct&lt;/code&gt; 커널은 훨씬 더 나은 결과를 얻습니다. 그러나 실제로 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt; 와 같은 고정 커널은 더 나은 결과를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="a467b781e30c272f4f5e4645f1261bd726b14e8d" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results.</source>
          <target state="translated">이 예는 XOR 데이터의 GPC를 보여줍니다. 고정식 등방성 커널 (RBF)과 고정되지 않은 커널 (DotProduct)이 비교됩니다. 이 특정 데이터 집합에서 클래스 경계는 선형이고 좌표 축과 일치하기 때문에 DotProduct 커널은 훨씬 더 나은 결과를 얻습니다. 일반적으로 고정 커널은 종종 더 나은 결과를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="722f803000769d905ffb0b191f686c49464dc23e" translate="yes" xml:space="preserve">
          <source>This example illustrates a generic implementation of a meta-estimator which extends clustering by inducing a classifier from the cluster labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="397c051adb668f2e353216b8af31fde49c4b91b4" translate="yes" xml:space="preserve">
          <source>This example illustrates a learned distance metric that maximizes the nearest neighbors classification accuracy. It provides a visual representation of this metric compared to the original point space. Please refer to the &lt;a href=&quot;../../modules/neighbors#nca&quot;&gt;User Guide&lt;/a&gt; for more information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b43f5231cb55a1a95a64bd8afe5472e7955fc98b" translate="yes" xml:space="preserve">
          <source>This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.</source>
          <target state="translated">이 예는 배깅 앙상블에 대한 단일 추정기의 예상 평균 제곱 오차의 바이어스-분산 분해를 보여주고 비교합니다.</target>
        </trans-unit>
        <trans-unit id="0a3450a632d656139c95a4f138b569cec22ba6ef" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The first figure compares the learned model of KRR and SVR when both complexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are very similar; however, fitting KRR is approx. seven times faster than fitting SVR (both with grid-search). However, prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">이 예는 사인파 목표 함수와 매 5 번째 데이터 포인트에 추가 된 강한 노이즈로 구성된 인공 데이터 세트의 두 가지 방법을 보여줍니다. 첫 번째 그림은 그리드 검색을 사용하여 RBF 커널의 복잡성 / 규정 화 및 대역폭을 최적화 할 때 KRR 및 SVR의 학습 모델을 비교합니다. 학습 된 기능은 매우 유사합니다. 그러나 KRR 피팅은 약입니다. SVR 피팅보다 7 배 더 빠릅니다 (둘 다 그리드 검색 사용). 그러나 SVR을 사용하면 100000 개의 목표 값을 예측하는 것이 트리보다 빠릅니다. 100 개의 교육 데이터 포인트 중 1/3이 지원 벡터로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="bdbaaa805869c3c13d157f267aeffaf332ff1284" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">이 예제는 사인파 목표 함수와 강한 노이즈로 구성된 인공 데이터 세트의 두 가지 방법을 보여줍니다. 이 그림은 주기적 기능 학습에 적합한 ExpSineSquared 커널을 기반으로 KRR 및 GPR의 학습 모델을 비교합니다. 커널의 하이퍼 파라미터는 커널의 부드러움 (l)과 주기성 (p)을 제어합니다. 또한 데이터의 노이즈 레벨은 커널의 추가 WhiteKernel 구성 요소와 KRR의 정규화 매개 변수 alpha에 의해 GPR에 의해 명시 적으로 학습됩니다.</target>
        </trans-unit>
        <trans-unit id="745a420beb7d4cfe3dcb516bc28a36f2576e5395" translate="yes" xml:space="preserve">
          <source>This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">이 예는 시그 모이 드 보정이 3 등급 분류 문제에 대해 예측 된 확률을 변경하는 방법을 보여줍니다. 3 개의 모서리가 3 개의 클래스에 해당하는 표준 2 심플 렉스가 그림에 나와 있습니다. 화살표는 보정되지 않은 분류기에 의해 예측 된 확률 벡터에서 홀드 아웃 유효성 검사 세트에서 시그 모이 드 보정 후 동일한 분류기에 의해 예측 된 확률 벡터를 가리 킵니다. 색상은 인스턴스의 실제 클래스를 나타냅니다 (빨간색 : 클래스 1, 녹색 : 클래스 2, 파란색 : 클래스 3).</target>
        </trans-unit>
        <trans-unit id="12c733d8527ccd2c1862324a52d9d453fa3717b9" translate="yes" xml:space="preserve">
          <source>This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications are outliers detection, observations ranking, clustering, &amp;hellip; For visualization purpose, the cubic root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]</source>
          <target state="translated">이 예는 Mahalanobis 거리가 외부 데이터에 의해 어떻게 영향을 받는지 보여줍니다. 오염 분포에서 얻은 관측치는 실제 가우시안 분포에서 오는 관측과 구별 할 수 없습니다. MCD 기반 마할 라 노비스 거리를 사용하면 두 집단이 구별 될 수 있습니다. 관련 응용 프로그램은 이상치 탐지, 관측치 순위, 군집화입니다.&amp;hellip; 시각화 목적을 위해 Mahalanobis 거리의 입방근은 상자 그림에 표시됩니다 (윌슨과 힐 퍼티가 제안한대로).</target>
        </trans-unit>
        <trans-unit id="e6c2656adbcd3c5e59b375bc18300061f72c931d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping using many fewer estimators. This can significantly reduce training time, memory usage and prediction latency.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 모델에서 조기 중지를 사용하여 더 적은 수의 추정기를 사용하여 조기 중지하지 않고 구축 된 모델과 비교하여 거의 동일한 정확도를 달성 하는 방법을 보여줍니다 . 이를 통해 교육 시간, 메모리 사용량 및 예측 대기 시간을 크게 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c9534999032b13d85edbba90f8420279af14435d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping. This can significantly reduce training time. Note that scores differ between the stopping criteria even from early iterations because some of the training data is held out with the validation stopping criterion.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt; 모델 에서 조기 중지를 사용하여 조기 중지 없이 빌드 된 모델과 거의 동일한 정확도를 달성 하는 방법을 보여줍니다 . 이것은 훈련 시간을 크게 줄일 수 있습니다. 일부 교육 데이터는 유효성 검증 중지 기준으로 수행되므로 초기 반복에서도 중지 기준간에 점수가 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c861b8fa50d5165e1e9cdc8044114c0ba76be7f1" translate="yes" xml:space="preserve">
          <source>This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt;. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; 를&lt;/a&gt; 사용하여 서로 다른 전처리 및 기능 추출 파이프 라인을 여러 기능의 하위 집합에 적용하는 방법을 보여줍니다 . 이 기능은 이기종 데이터 형식을 포함하는 데이터 집합의 경우 특히 유용합니다. 숫자 형 기능을 확장하고 범주 형 기능을 1- 핫 인코딩 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="7688b615fac5133028d275260a50b4a9e7d6a213" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</source>
          <target state="translated">이 예는 WhiteKernel을 포함하는 합 커널이있는 GPR이 데이터의 노이즈 레벨을 추정 할 수 있음을 보여줍니다. LLM (log-marginal-likelihood) 랜드 스케이프의 그림은 LML의 두 개의 로컬 최대 값이 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="aab3b4258aabcd6bfe55c7c5adf04622c59229dc" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">이 예는 WhiteKernel을 포함하는 합 커널이있는 GPR이 데이터의 노이즈 레벨을 추정 할 수 있음을 보여줍니다. LLM (log-marginal-likelihood) 랜드 스케이프의 그림은 LML의 두 개의 로컬 최대 값이 있음을 보여줍니다. 첫 번째는 노이즈 수준이 높고 길이가 큰 모델에 해당하며 노이즈별로 데이터의 모든 변형을 설명합니다. 두 번째는 더 작은 소음 수준과 더 짧은 길이의 스케일을 가지며, 이는 무 잡음 기능 관계에 의한 대부분의 변동을 설명합니다. 두 번째 모델은 가능성이 높습니다. 그러나 하이퍼 파라미터의 초기 값에 따라 그래디언트 기반 최적화도 고 소음 솔루션으로 수렴 될 수 있습니다. 따라서 다른 초기화에 대해 최적화를 여러 번 반복하는 것이 중요합니다.</target>
        </trans-unit>
        <trans-unit id="d19a1528c92e37a37fb4fd9cad2cad1ac3d91123" translate="yes" xml:space="preserve">
          <source>This example illustrates the differences between univariate F-test statistics and mutual information.</source>
          <target state="translated">이 예는 일 변량 F- 검정 통계와 상호 정보의 차이점을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="7867032d06fe0e647e7865e3d58419806006e425" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of monotonic constraints on a gradient boosting estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b411e019157b9b0953b37eb36e27bc6a8ffb3f5f" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of the parameters &lt;code&gt;gamma&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; of the Radial Basis Function (RBF) kernel SVM.</source>
          <target state="translated">이 예는 매개 변수 &lt;code&gt;gamma&lt;/code&gt; 의 영향을 보여줍니다. RBF (Radial Basis Function) 커널 SVM 및 &lt;code&gt;C&lt;/code&gt; 의 .</target>
        </trans-unit>
        <trans-unit id="0f49634dcf1598fde3c403bd7ddd702e3816c634" translate="yes" xml:space="preserve">
          <source>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.</source>
          <target state="translated">이 예는 실제 데이터 세트에 대한 강력한 공분산 추정의 필요성을 보여줍니다. 이상치 탐지와 데이터 구조를보다 잘 이해하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="dc09ff23a3cae32be328d47d0a0a7e64185cd75a" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</source>
          <target state="translated">이 예는 다른 하이퍼 파라미터를 선택한 RBF 커널에 대한 GPC의 예상 확률을 보여줍니다. 첫 번째 그림은 임의로 선택된 하이퍼 파라미터와 최대 로그-마진 우도 (LML)에 해당하는 하이퍼 파라미터가있는 GPC의 예측 확률을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="d49b62c58d1a1ad4b52cfdb4df97043fcb25dfc7" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">이 예는 홍채 데이터 세트의 2 차원 버전에서 등방성 및 이방성 RBF 커널에 대한 GPC의 예측 확률을 보여줍니다. 이방성 RBF 커널은 두 피처 치수에 서로 다른 길이 스케일을 할당하여 약간 더 높은 로그 마진 가능성을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="f8e8b4dfa6bc8bbc237c34d26328609c3561c0d3" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="translated">이 예는 홍채 데이터 세트의 2 차원 버전에서 등방성 및 이방성 RBF 커널에 대한 GPC의 예측 확률을 보여줍니다. 이것은 비 이진 분류에 대한 GPC의 적용 가능성을 보여줍니다. 이방성 RBF 커널은 두 피처 치수에 서로 다른 길이 스케일을 할당하여 약간 더 높은 로그 마진 가능성을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="d60d503f722a9b87495f756d071794c2e2c52164" translate="yes" xml:space="preserve">
          <source>This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10 samples are shown for both prior and posterior.</source>
          <target state="translated">이 예는 다른 커널을 사용하는 GPR의 전후를 보여줍니다. 평균, 표준 편차 및 10 개의 샘플이 이전 및 이후 모두에 대해 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="404891c56bd0f5f01fae61c2e12336745bf204d5" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of Gaussian processes for regression and classification tasks on data that are not in fixed-length feature vector form. This is achieved through the use of kernel functions that operates directly on discrete structures such as variable-length sequences, trees, and graphs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b759d3b33b67e0716b08ee9d527244bce9326eed" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of Poisson, Gamma and Tweedie regression on the &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;French Motor Third-Party Liability Claims dataset&lt;/a&gt;, and is inspired by an R tutorial &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d658c6dcc61e946966ad0e8390aa06974df82a41" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of log-linear Poisson regression on the &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;French Motor Third-Party Liability Claims dataset&lt;/a&gt; from &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; and compares it with a linear model fitted with the usual least squared error and a non-linear GBRT model fitted with the Poisson loss (and a log-link).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e4f09a45ae58596e6f6f82a5f372f88442c0679" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator to perform multi-output regression. A random forest regressor is used, which supports multi-output regression natively, so the results can be compared.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;다중 출력&lt;/a&gt; 사용을 보여줍니다. 메타 추정기를 사용하여 다중 출력 회귀를 수행 . 다중 출력 회귀를 기본적으로 지원하는 임의의 포리스트 회귀가 사용되므로 결과를 비교할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b7cdc524f76e4e3da39b555dc7fb49145ad1abf9" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the print_changed_only global parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a4413b8994df2fb3a9be31d12e5c3eff453a657" translate="yes" xml:space="preserve">
          <source>This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.</source>
          <target state="translated">이 예는 피쳐 공간에서 시각적으로 두 가지 성분 분석 기법을 사용한 결과를 비교하여 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a61e83a5c393fe6eb13b5e3a4d37f5a941d5abef" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;2&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cd2616385e5968100e838cea35843639b5d4649" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="translated">이 예는 Hastie et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[&lt;/a&gt; 2009 ]의 그림 10.2를 기반으로 하며 이산 SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; 부스팅 알고리즘과 실제 SAMME.R 부스팅 알고리즘 간의 성능 차이를 보여줍니다 . 두 알고리즘 모두 대상 Y가 10 개의 입력 기능이있는 비선형 함수 인 이진 분류 작업에서 평가됩니다.</target>
        </trans-unit>
        <trans-unit id="f74f72b73b7f5fc12f50525ee3a073668f796ed6" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &amp;ldquo;Gaussian Processes for Machine Learning&amp;rdquo; [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">이 예는 &quot;기계 학습을위한 가우스 프로세스&quot;[RW2006]의 5.4.3 섹션을 기반으로합니다. 로그 마진 우도에서 기울기 상승을 사용하는 복잡한 커널 엔지니어링 및 하이퍼 파라미터 최적화의 예를 보여줍니다. 데이터는 1958 년에서 2001 년 사이 하와이의 Mauna Loa Observatory에서 수집 한 월별 평균 대기 CO2 농도 (부피 율 (ppmv))로 구성됩니다. 목표는 시간 t의 함수로 CO2 농도를 모델링하는 것입니다. .</target>
        </trans-unit>
        <trans-unit id="b3b5741f90375b259727a574f2a0488e7637e5bc" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt;. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="translated">이 예는 &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt; 의 섹션 5.4.3을 기반으로 합니다. 로그 마진 우도에서 기울기 상승을 사용하는 복잡한 커널 엔지니어링 및 하이퍼 파라미터 최적화의 예를 보여줍니다. 이 데이터는 1958 년에서 1997 년 사이 하와이의 Mauna Loa Observatory에서 수집 한 월별 평균 대기 CO2 농도 (부피 율 (ppmv))로 구성됩니다. 목표는 시간 t의 함수로 CO2 농도를 모델링하는 것입니다. .</target>
        </trans-unit>
        <trans-unit id="9ecc1c02a68f38d70271669af08df8c1497b1d98" translate="yes" xml:space="preserve">
          <source>This example is commented in the &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;tutorial section of the user manual&lt;/a&gt;.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;사용자 매뉴얼&lt;/a&gt; 의 튜토리얼 섹션에 주석 처리되어 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9143eb314f05280f157b4081755ef4be5d0d1857" translate="yes" xml:space="preserve">
          <source>This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.</source>
          <target state="translated">이 예는 k- 평균이 직관적이지 않고 예상치 못한 클러스터를 생성하는 상황을 설명하기위한 것입니다. 처음 세 줄거리에서 입력 데이터는 k- 평균이 만들어 내고 바람직하지 않은 클러스터가 생성된다는 암시적인 가정을 따르지 않습니다. 마지막 그림에서 k- 평균은 크기가 균일하지 않은 얼룩에도 불구하고 직관적 인 군집을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0549792fdb7404b1799803948805283b1004db4d" translate="yes" xml:space="preserve">
          <source>This example plots several randomly generated classification datasets. For easy visualization, all datasets have 2 features, plotted on the x and y axis. The color of each point represents its class label.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="301c1da5ab62941da3ba93fb4d30f8869ad9b8b5" translate="yes" xml:space="preserve">
          <source>This example plots the corresponding dendrogram of a hierarchical clustering using AgglomerativeClustering and the dendrogram method available in scipy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb84e7488212d58818869cb1a848aafb46dc6573" translate="yes" xml:space="preserve">
          <source>This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.</source>
          <target state="translated">이 예는 각 클래스의 공분산 타원체와 LDA 및 QDA에 의해 학습 된 결정 경계를 표시합니다. 타원체는 각 클래스에 대한 이중 표준 편차를 표시합니다. LDA를 사용하면 표준 편차는 모든 클래스에 대해 동일하지만 각 클래스에는 QDA에 대한 자체 표준 편차가 있습니다.</target>
        </trans-unit>
        <trans-unit id="61cf8846c08926de131cab630666b0d7a1ff4033" translate="yes" xml:space="preserve">
          <source>This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class models with a Dirichlet distribution prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt;) and a Dirichlet process prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt;). On each figure, we plot the results for three different values of the weight concentration prior.</source>
          <target state="translated">이 예에서는 &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 클래스 모델에 의해 적합 화 된 장난감 데이터 셋 (가우시안 3 개의 혼합물)에서 얻은 타원체를 이전에 Dirichlet 분포 ( &lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt; )와 Dirichlet 프로세스 이전 ( &lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt; )으로 표시합니다. 각 그림에서, 우리는 이전에 무게 농도의 세 가지 다른 값에 대한 결과를 플롯합니다.</target>
        </trans-unit>
        <trans-unit id="e769643d14b1851635097bc926523b99ade21d56" translate="yes" xml:space="preserve">
          <source>This example presents how to chain KNeighborsTransformer and TSNE in a pipeline. It also shows how to wrap the packages &lt;code&gt;annoy&lt;/code&gt; and &lt;code&gt;nmslib&lt;/code&gt; to replace KNeighborsTransformer and perform approximate nearest neighbors. These packages can be installed with &lt;code&gt;pip install annoy nmslib&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0d46d4faf4d4d45c7ba844c05b8ff931d890d6c" translate="yes" xml:space="preserve">
          <source>This example presents the different strategies implemented in KBinsDiscretizer:</source>
          <target state="translated">이 예는 KBinsDiscretizer에서 구현 된 다양한 전략을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="83f4e337d08dbc1524d014ed0118fe74e8fd5454" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aff95617011fc0f39a1d4573107679df90fa3c83" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="translated">이 예제는 Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 의 그림 1을 재현 하고 다중 클래스 문제에서 부스팅이 예측 정확도를 개선하는 방법을 보여줍니다. 분류 데이터 세트는 10 차원 표준 정규 분포를 취하고 중첩 된 동심 10 차원 구체로 구분 된 3 개의 클래스를 정의하여 대략 같은 수의 샘플이 각 클래스 (\ (\ chi ^ 2 \) 분포의 사 분위수)에 있도록 구성됩니다. ).</target>
        </trans-unit>
        <trans-unit id="f0193d5eae04ea4c45d1272eb8f24ceb76514e1e" translate="yes" xml:space="preserve">
          <source>This example serves as a visual check that IPCA is able to find a similar projection of the data to PCA (to a sign flip), while only processing a few samples at a time. This can be considered a &amp;ldquo;toy example&amp;rdquo;, as IPCA is intended for large datasets which do not fit in main memory, requiring incremental approaches.</source>
          <target state="translated">이 예제는 IPCA가 한 번에 몇 개의 샘플 만 처리하면서 PCA (sign flip)에 대한 유사한 데이터 프로젝션을 찾을 수 있는지 시각적으로 확인합니다. IPCA는 주 메모리에 맞지 않고 증분 방식이 필요한 대규모 데이터 세트를위한 것이기 때문에 이것은 &quot;장난감 사례&quot;로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3dfa9a56451c107730b94ff094ad060fe6660b05" translate="yes" xml:space="preserve">
          <source>This example should be taken with a grain of salt, as the intuition conveyed does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly. Moreover, using feature discretization and one-hot encoding increases the number of features, which easily lead to overfitting when the number of samples is small.</source>
          <target state="translated">전달 된 직관이 반드시 실제 데이터 세트로 전달되는 것은 아니기 때문에이 예는 소금 한 알로 취해야합니다. 특히 고차원 공간에서 데이터를보다 쉽게 ​​선형으로 분리 할 수 ​​있습니다. 또한, 특징 이산화 및 one-hot 인코딩을 사용하면 기능의 수가 증가하여 샘플 수가 적을 때 쉽게 과적 합하게됩니다.</target>
        </trans-unit>
        <trans-unit id="ad21e470ff2bffe875ca12e50c6f8a883eaa0515" translate="yes" xml:space="preserve">
          <source>This example shows an example usage of the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">이 예제는 &lt;code&gt;split&lt;/code&gt; 메소드의 사용법 예제를 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="05321a1b8964aa5eaada463be8f8b6ab44686817" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.</source>
          <target state="translated">이 예는 2D 데이터 세트에서 다양한 이상 감지 알고리즘의 특성을 보여줍니다. 데이터 세트에는 알고리즘이 멀티 모달 데이터에 대처하는 능력을 설명하기 위해 하나 또는 두 개의 모드 (고밀도 영역)가 있습니다.</target>
        </trans-unit>
        <trans-unit id="9671740bdc9e0f010272719df08d61d30b070724" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different clustering algorithms on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.</source>
          <target state="translated">이 예제는 &quot;관심&quot;이지만 여전히 2D 인 데이터 세트에 대한 다른 클러스터링 알고리즘의 특성을 보여줍니다. 마지막 데이터 세트를 제외하고, 이러한 각 데이터 세트 알고리즘 쌍의 매개 변수는 우수한 클러스터링 결과를 생성하도록 조정되었습니다. 일부 알고리즘은 다른 알고리즘보다 매개 변수 값에 더 민감합니다.</target>
        </trans-unit>
        <trans-unit id="408c25df8162bc85c75adf89aefb6c4283aab313" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D.</source>
          <target state="translated">이 예제는 &quot;관심&quot;이지만 여전히 2D 인 데이터 세트의 계층 적 클러스터링에 대한 서로 다른 연결 방법의 특성을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="ee904b77cbf769dbe7d1093eb852f864329f4bb5" translate="yes" xml:space="preserve">
          <source>This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.</source>
          <target state="translated">이 예는 강력한 비모수 밀도 추정 기법 인 KDE (커널 밀도 추정)를 사용하여 데이터 집합에 대한 생성 모델을 학습하는 방법을 보여줍니다. 이 생성 모델을 사용하면 새로운 샘플을 추출 할 수 있습니다. 이 새로운 샘플은 데이터의 기본 모델을 반영합니다.</target>
        </trans-unit>
        <trans-unit id="54102d8f78c42d496181e5bcdf5a40bdaee3e42d" translate="yes" xml:space="preserve">
          <source>This example shows how quantile regression can be used to create prediction intervals.</source>
          <target state="translated">이 예제는 Quantile Regression을 사용하여 예측 간격을 만드는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="682ea376dc5fd413204f119c7903367cc1b71149" translate="yes" xml:space="preserve">
          <source>This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.</source>
          <target state="translated">이 예는 BernoulliRBM 기능 추출기와 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 분류기 를 사용하여 분류 파이프 라인을 작성하는 방법을 보여줍니다 . 전체 모델의 하이퍼 파라미터 (학습 속도, 숨겨진 레이어 크기, 정규화)는 그리드 검색으로 최적화되었지만 런타임 제약 조건으로 인해 검색이 재현되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e6287a37f5ab2f7e58b3aa65bfc9b371f8d2e434" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">이 예제는 캘리포니아 주택 데이터 세트에 대해 훈련 된 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 에서 부분 의존도를 얻는 방법을 보여줍니다 . 예는 &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; 에서 가져 왔습니다 .</target>
        </trans-unit>
        <trans-unit id="acf93c17a79b2f475e2915043f3e806cbddc60a2" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; and a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd4b844c3488b502b915a6b11d22b13911beaa74" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores.</source>
          <target state="translated">이 예는 분류 점수를 향상시키기 위해 SVC (support vector classifier)를 실행하기 전에 일 변량 기능 선택을 수행하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="f7865c444403c29f04e970d66c2c5367cc834a8b" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores. We use the iris dataset (4 features) and add 36 non-informative features. We can find that our model achieves best performance when we select around 10% of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47a26e628df959c3e5ed3ffe4f4f3490e8927a8d" translate="yes" xml:space="preserve">
          <source>This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.</source>
          <target state="translated">이 예는 MNIST 데이터 세트에 대해 훈련 된 MLPClassifier에서 일부 첫 번째 레이어 가중치를 플로팅하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="dd07bd7e7afed03f3c2a3c74458c42dc14028dfe" translate="yes" xml:space="preserve">
          <source>This example shows how to plot the decision surface for four SVM classifiers with different kernels.</source>
          <target state="translated">이 예는 다른 커널을 가진 4 개의 SVM 분류기에 대한 결정 화면을 그리는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="981971245cdda71fb264be7304dc1e201a08b23b" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; to visualize prediction errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52dcf2b8bf47a153b3ba3beca30c9af85b850fad" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;code&gt;cross_val_predict&lt;/code&gt; to visualize prediction errors.</source>
          <target state="translated">이 예제는 &lt;code&gt;cross_val_predict&lt;/code&gt; 를 사용 하여 예측 오류를 시각화 하는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="42d1610b65eff631d96069dbbdf35236afd1d573" translate="yes" xml:space="preserve">
          <source>This example shows how to use Permutation Importances as an alternative that can mitigate those limitations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="351c02b1031f149a08df70cbeed39cd2a6bb9ec7" translate="yes" xml:space="preserve">
          <source>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.</source>
          <target state="translated">이 예는 커널 PCA가 데이터를 선형으로 분리 할 수있는 데이터의 투영을 찾을 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="607fdb6fda0285694fd1dd982f83082f2f9a6687" translate="yes" xml:space="preserve">
          <source>This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.</source>
          <target state="translated">이 예제는 결 측값을 대치하면 결 측값이 포함 된 샘플을 폐기하는 것보다 더 나은 결과를 얻을 수 있음을 보여줍니다. 대치가 항상 예측을 개선하는 것은 아니므로 교차 검증을 통해 확인하십시오. 때때로 행을 삭제하거나 마커 값을 사용하는 것이 더 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="b6045a3110197ccfd64402c3c879acac73bdaadb" translate="yes" xml:space="preserve">
          <source>This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.</source>
          <target state="translated">이 예는 정보 이론적 기준 (BIC)을 사용하여 가우스 혼합 모델로 모델 선택을 수행 할 수 있음을 보여줍니다. 모형 선택은 공분산 유형과 모형의 성분 수와 관련이 있습니다. 이 경우 AIC는 올바른 결과 (시간 절약을 위해 표시되지 않음)도 제공하지만 문제가 올바른 모델을 식별하는 경우 BIC가 더 적합합니다. 베이지안 절차와 달리 그러한 추론은 사전에 무료입니다.</target>
        </trans-unit>
        <trans-unit id="7ed4db944a196b1cb5ab23d8834c95cd5421c757" translate="yes" xml:space="preserve">
          <source>This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces.</source>
          <target state="translated">이 예는 파이프 라인을 사용하여 비선형 피처를 추가하는 선형 모델로 비선형 회귀를 수행 할 수 있음을 보여줍니다. 커널 방법은이 아이디어를 확장하고 매우 높은 (무한한) 차원 형상 공간을 유도 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="48dcc848c2d6e1561f6c336d60b0fb518f9ab59a" translate="yes" xml:space="preserve">
          <source>This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.</source>
          <target state="translated">이 예는 K- 폴드 교차 검증에서 생성 된 서로 다른 데이터 세트의 ROC 응답을 보여줍니다. 이러한 모든 곡선을 취하면 곡선 아래의 평균 면적을 계산하고 훈련 세트가 다른 하위 집합으로 분할 될 때 곡선의 분산을 볼 수 있습니다. 이것은 대략 분류 데이터 출력이 훈련 데이터의 변경에 의해 어떻게 영향을 받는지, 그리고 K- 폴드 교차 검증에 의해 생성 된 스플릿이 어떻게 다른지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="7b92e840bf44fca60c7edc394c5ccf3da0546857" translate="yes" xml:space="preserve">
          <source>This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.</source>
          <target state="translated">이 예는 데이터에서 로컬 구조를 캡처하기 위해 연결성 그래프를 적용한 효과를 보여줍니다. 그래프는 단순히 20 개의 가장 가까운 이웃의 그래프입니다.</target>
        </trans-unit>
        <trans-unit id="a122bd5b47879a72d10715b2e2741901d74ebd5f" translate="yes" xml:space="preserve">
          <source>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in &lt;strong&gt;computed tomography&lt;/strong&gt; (CT).</source>
          <target state="translated">이 예는 다른 각도를 따라 획득 한 평행 투영 세트에서 이미지를 재구성하는 것을 보여줍니다. 이러한 데이터 세트는 &lt;strong&gt;컴퓨터 단층 촬영&lt;/strong&gt; (CT)으로 획득됩니다 .</target>
        </trans-unit>
        <trans-unit id="277c7e399c7f521a9367c3279ba6605ffa32bb5b" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="translated">이 예는 이미지 분류 작업 (얼굴)에서 픽셀의 중요성을 평가하기 위해 나무 숲을 사용하는 방법을 보여줍니다. 픽셀이 높을수록 더 중요합니다.</target>
        </trans-unit>
        <trans-unit id="1fc99c46b3490d43b644ebffe104cba9573e1195" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the impurity-based importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="181df9c721e88b620fbcd3038af372d2bc17958d" translate="yes" xml:space="preserve">
          <source>This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.</source>
          <target state="translated">이 예는 다중 출력 추정기를 사용하여 이미지를 완성하는 것을 보여줍니다. 목표는 상반신이 주어진 얼굴의 하반부를 예측하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c194d9ad3fd820ff97a5b60a54a77ad5e096ac46" translate="yes" xml:space="preserve">
          <source>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:</source>
          <target state="translated">이 예제는 다중 레이블 문서 분류 문제를 시뮬레이션합니다. 데이터 세트는 다음 프로세스를 기반으로 임의로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="beca62f6aeebe1ac71c16bdf04b277689fc51da6" translate="yes" xml:space="preserve">
          <source>This example uses &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.</source>
          <target state="translated">이 예에서는 이미지의 복셀 대 복셀 차이로 생성 된 그래프에서 &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;스펙트럼 클러스터링&lt;/a&gt; 을 사용 하여이 이미지를 부분적으로 균질 한 여러 영역으로 나눕니다.</target>
        </trans-unit>
        <trans-unit id="b1b3f16a0bb367262a72b24da0008ca16f86a096" translate="yes" xml:space="preserve">
          <source>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.</source>
          <target state="translated">이 예에서는 얼굴을 구성하는 20 x 20 개 이미지 패치 세트를 학습하기 위해 큰면의 얼굴 세트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="0ad2a4281006cf2ce3833b3619a37abf58d678e0" translate="yes" xml:space="preserve">
          <source>This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.</source>
          <target state="translated">이 예제는 서로 다른 스케일러, 변환기 및 노멀 라이저를 사용하여 데이터를 사전 정의 된 범위 내로 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="e1bfbae38c6acd2b8b362eacb6ca0227cf12cdc6" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; class to demonstrate the principles of Kernel Density Estimation in one dimension.</source>
          <target state="translated">이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; 를 사용합니다 . 클래스를 한 차원에서 커널 밀도 추정의 원칙을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a99077db11e6455dfdf5d225265c8630cb316352" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c91a9c343e9d52fecff06520d2432c55d78e2a0" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;. In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="translated">이 예에서는 사용 &lt;code&gt;scipy.stats&lt;/code&gt; 의 같은 샘플링 파라미터를위한 많은 유용한 분포 포함 모듈 &lt;code&gt;expon&lt;/code&gt; , &lt;code&gt;gamma&lt;/code&gt; , &lt;code&gt;uniform&lt;/code&gt; 또는 &lt;code&gt;randint&lt;/code&gt; 를 . 원칙적 으로 값을 샘플링하는 &lt;code&gt;rvs&lt;/code&gt; (random variate sample) 방법을 제공하는 모든 함수를 전달할 수 있습니다 . 호출받는 &lt;code&gt;rvs&lt;/code&gt; 함수 호출에 연속 가능한 파라미터 값들로부터 독립적 랜덤 샘플을 제공한다.</target>
        </trans-unit>
        <trans-unit id="d9381762b80079a0275cf5384c50652951b2c3b8" translate="yes" xml:space="preserve">
          <source>This example uses the only the first feature of the &lt;code&gt;diabetes&lt;/code&gt; dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.</source>
          <target state="translated">이 예는 이 회귀 기술의 2 차원 플롯을 설명하기 위해 &lt;code&gt;diabetes&lt;/code&gt; 데이터 세트 의 첫 번째 특징 만 사용합니다 . 그림에서 직선을 볼 수 있습니다. 선형 회귀 분석에서 데이터 집합의 관측 된 반응과 선형 근사로 예측 된 반응 사이의 잔차 제곱합을 최소화하는 직선을 그리는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="c6bb5d76743a81844f0fc5afc16345d399cae103" translate="yes" xml:space="preserve">
          <source>This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.</source>
          <target state="translated">이 예는 SGD 및 Adam을 포함한 다양한 확률 적 학습 전략에 대한 일부 훈련 손실 곡선을 보여줍니다. 시간 제약 때문에 L-BFGS가 더 적합한 몇 가지 작은 데이터 세트를 사용합니다. 그러나이 예제에서 보여지는 일반적인 추세는 더 큰 데이터 세트로 이어지는 것 같습니다.</target>
        </trans-unit>
        <trans-unit id="65646a35859e04e16667e59a0e282463725f9c9e" translate="yes" xml:space="preserve">
          <source>This example visualizes the behavior of several common scikit-learn objects for comparison.</source>
          <target state="translated">이 예는 비교를 위해 몇 가지 일반적인 scikit-learn 객체의 동작을 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="49dcb9492cd2c3de6ca468ca869fddbd4adf1109" translate="yes" xml:space="preserve">
          <source>This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification.</source>
          <target state="translated">이 예제에서는 여러 트리에서 제공하는 파티션을 시각화하고 비선형 차원 축소 또는 비선형 분류에 변환을 사용하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="6942c7999112e031e7e9f390f17a63a6ea65b8dd" translate="yes" xml:space="preserve">
          <source>This example was inspired by the &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html&quot;&gt;XGBoost documentation&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6888e16176d5ba984d30e5b2aecac9e36e3202eb" translate="yes" xml:space="preserve">
          <source>This example will also work by replacing &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; with &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt;. Setting the &lt;code&gt;loss&lt;/code&gt; parameter of the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; equal to &lt;code&gt;hinge&lt;/code&gt; will yield behaviour such as that of a SVC with a linear kernel.</source>
          <target state="translated">이 예제는 &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; 를 &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt; 로 대체하여 작동합니다 . &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;loss&lt;/code&gt; 매개 변수 를 &lt;code&gt;hinge&lt;/code&gt; 와 동일하게 설정하면 선형 커널이있는 SVC의 동작과 같은 동작이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="b7e7228e1bc6d35fed471b6c3015699404cca0fb" translate="yes" xml:space="preserve">
          <source>This example will generate three figures.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d6705d9213c8c7167b6f8a12276b073092deeca" translate="yes" xml:space="preserve">
          <source>This example will provide some hints in interpreting coefficient in linear models, pointing at problems that arise when either the linear model is not appropriate to describe the dataset, or when features are correlated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08633b59361c5b4332dffd09b9ac681bbe920080" translate="yes" xml:space="preserve">
          <source>This example, inspired from Chen&amp;rsquo;s publication [1], shows a comparison of the estimated MSE of the LW and OAS methods, using Gaussian distributed data.</source>
          <target state="translated">Chen의 간행물 [1]에서 영감을 얻은이 예는 가우스 분포 데이터를 사용하여 LW 및 OAS 방법의 추정 MSE를 비교 한 것입니다.</target>
        </trans-unit>
        <trans-unit id="8aeed7fa163e961c6e7fadbb3ef8c5a658c9cc15" translate="yes" xml:space="preserve">
          <source>This examples demonstrates how to precompute the k nearest neighbors before using them in KNeighborsClassifier. KNeighborsClassifier can compute the nearest neighbors internally, but precomputing them can have several benefits, such as finer parameter control, caching for multiple use, or custom implementations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9260a90e6c35e520398765702d07497fe04f1a8" translate="yes" xml:space="preserve">
          <source>This examples shows how a classifier is optimized by cross-validation, which is done using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; object on a development set that comprises only half of the available labeled data.</source>
          <target state="translated">이 예제는 사용 가능한 레이블이 지정된 데이터의 절반 만 포함하는 개발 세트 에서 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; 오브젝트를 사용하여 교차 유효성 검증에 의해 분류자가 최적화되는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="e16048c7f7cf75798d53fe7e675cc81cd1d6af8b" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.</source>
          <target state="translated">이 예에서는 인공림 분류 작업에서 기능의 중요성을 평가하기 위해 나무 숲을 사용하는 방법을 보여줍니다. 빨간색 막대는 트리 간 변동성과 함께 포리스트의 기능 중요도입니다.</target>
        </trans-unit>
        <trans-unit id="cdab1575e5f24de85ab913b14f241e0bd17fea2d" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the impurity-based feature importances of the forest, along with their inter-trees variability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fb4f14900902539137295018be0a0c7a07b1094" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Cross-validated estimators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">이 연습은 &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;모델 선택&lt;/a&gt; 의 &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;교차 검증 된 추정기&lt;/a&gt; 부분 : &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;과학 데이터 처리를위한 통계 학습에 대한 학습서&lt;/a&gt; 의 추정기 및 매개 변수 선택 섹션에서 사용 됩니다 .</target>
        </trans-unit>
        <trans-unit id="621b0c8349abb129c7bc150bd9745f46f49af3ab" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Cross-validation generators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">이 연습은 &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;모델 선택&lt;/a&gt; 의 &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;교차 검증 생성기&lt;/a&gt; 부분 : &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;과학적 데이터 처리를위한 통계 학습에 대한 학습서&lt;/a&gt; 의 추정기 및 매개 변수 선택 섹션에서 사용 됩니다 .</target>
        </trans-unit>
        <trans-unit id="6915ba6643fea2f9e387885428a6e6189c7df3bf" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Classification&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">이 실습은 &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;지도 학습&lt;/a&gt; 의 &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;분류&lt;/a&gt; 부분 : &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;과학 데이터 처리를위한 통계 학습에 대한 A 학습서&lt;/a&gt; 의 고차원 관측치에서 출력 변수 예측 섹션에서 사용 됩니다 .</target>
        </trans-unit>
        <trans-unit id="421684adc1a996556d28fe46ff4c101c2f3063ef" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Using kernels&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="translated">이 실습은 &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;과학 학습을위한 통계 학습에 대한 A 학습서&lt;/a&gt; 의 &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;고차원 관찰에서 출력 변수 예측&lt;/a&gt; 섹션의 학습 학습 : &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;커널 사용&lt;/a&gt; 파트 에서 사용 됩니다 .</target>
        </trans-unit>
        <trans-unit id="dadb46eeaf7a2bf2f8f61dc107ba2d3f5d55d33a" translate="yes" xml:space="preserve">
          <source>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)\). Then the log loss of the whole set is</source>
          <target state="translated">이것은 다음과 같이 멀티 클래스 사례로 확장됩니다. 샘플 \ (i \)에 레이블이있는 경우 샘플 세트에 대한 실제 레이블을 1-K K 이진 표시기 행렬 \ (Y \)로 인코딩합니다 (예 : \ (y_ {i, k} = 1 \)). \ (k \)는 일련의 \ (K \) 레이블에서 가져옵니다. \ (P \)를 확률 추정값의 행렬로하고, \ (p_ {i, k} = \ operatorname {Pr} (t_ {i, k} = 1) \)을 사용하십시오. 그런 다음 전체 세트의 로그 손실은</target>
        </trans-unit>
        <trans-unit id="826a67cf49f96f56e23921af61e52712fab61d33" translate="yes" xml:space="preserve">
          <source>This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt;, &lt;code&gt;adjusted_rand_index&lt;/code&gt; or &lt;code&gt;average_precision&lt;/code&gt; and returns a callable that scores an estimator&amp;rsquo;s output.</source>
          <target state="translated">이 팩토리 함수는 GridSearchCV 및 cross_val_score에서 사용하기위한 스코어링 기능을 래핑합니다. &lt;code&gt;accuracy_score&lt;/code&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; 점수 , mean_squared_error , &lt;code&gt;adjusted_rand_index&lt;/code&gt; _rand_index 또는 &lt;code&gt;average_precision&lt;/code&gt; 과 같은 점수 함수를 사용 하여 추정기의 출력에 점수를 매기는 호출 가능 항목을 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="5eb7a4eb6e2161d3d9f2a7b4c7010506ccf35ad1" translate="yes" xml:space="preserve">
          <source>This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:</source>
          <target state="translated">이 기능은 sepal 길이 (cm)에 해당합니다. Quantile 변환이 적용되면 해당 랜드 마크는 이전에 정의한 백분위 수에 가깝게 접근합니다.</target>
        </trans-unit>
        <trans-unit id="18a1d2c5a41fd4d57af7a6bb802060cade230322" translate="yes" xml:space="preserve">
          <source>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</source>
          <target state="translated">이 기능 선택 알고리즘은 원하는 출력 (y)이 아닌 기능 (X) 만보고 감독되지 않은 학습에 사용될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="677c582ff4a458e9dc8e636909bbbb985fe5cce6" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; preprocessor. This preprocessor transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="translated">이 그림은 &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt; 프리 프로세서를 사용하여 작성됩니다 . 이 전처리 기는 입력 데이터 매트릭스를 주어진 정도의 새로운 데이터 매트릭스로 변환합니다. 다음과 같이 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6faa801ac2c09333248247cbfc3515a179a790a8" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adbd1df9acbf84e51fe5dc83e34aca6a9423eabf" translate="yes" xml:space="preserve">
          <source>This figure shows an example of such an ROC curve:</source>
          <target state="translated">이 그림은 이러한 ROC 곡선의 예를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="e1207da0df5038f5f29891db83b7d5022ead8471" translate="yes" xml:space="preserve">
          <source>This folder is used by some large dataset loaders to avoid downloading the data several times.</source>
          <target state="translated">이 폴더는 데이터를 여러 번 다운로드하지 않도록 일부 대형 데이터 세트 로더에서 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="697a12fdadac01e298b3e16a8634659c2b054014" translate="yes" xml:space="preserve">
          <source>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</source>
          <target state="translated">이 형식은 한 줄에 하나의 샘플이있는 텍스트 기반 형식입니다. 값이 0 인 기능을 저장하지 않으므로 스파 스 데이터 세트에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="a2fe6f0ee6734c2a60bcbb1d0d95dc9dfd886002" translate="yes" xml:space="preserve">
          <source>This format is used as the default format for both svmlight and the libsvm command line programs.</source>
          <target state="translated">이 형식은 svmlight 및 libsvm 명령 행 프로그램 모두에 대한 기본 형식으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="a2e505f490185afab8e1242d5832b6872eb9a667" translate="yes" xml:space="preserve">
          <source>This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then &lt;code&gt;dot(x, x)&lt;/code&gt; and/or &lt;code&gt;dot(y, y)&lt;/code&gt; can be pre-computed.</source>
          <target state="translated">이 공식은 다른 방법으로 거리를 계산하는 것보다 두 가지 장점이 있습니다. 첫째, 희소 데이터를 처리 할 때 계산 효율이 높습니다. 둘째, 하나의 인수는 변하지 만 다른 인수는 변경되지 않은 경우 &lt;code&gt;dot(x, x)&lt;/code&gt; 및 / 또는 &lt;code&gt;dot(y, y)&lt;/code&gt; 를 미리 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fd76e45c139161a6c2340aa524dcf0429afb583e" translate="yes" xml:space="preserve">
          <source>This function computes Cohen&amp;rsquo;s kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</source>
          <target state="translated">이 함수 는 분류 문제에 대한 두 어노 테이터 간의 일치 레벨을 나타내는 점수 인 Cohen 's kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]을&lt;/a&gt; 계산합니다 . 다음과 같이 정의됩니다</target>
        </trans-unit>
        <trans-unit id="1a26f30c64bc915159ba37349551edb033f8db68" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).</source>
          <target state="translated">이 함수는 지정된 거리에 따라 가장 가까운 Y의 행 인덱스 인 X의 각 행에 대해 계산합니다.</target>
        </trans-unit>
        <trans-unit id="40ca8ec3788866f2480b1619115207e644d05cac" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.</source>
          <target state="translated">이 함수는 지정된 거리에 따라 가장 가까운 Y의 행 인덱스 인 X의 각 행에 대해 계산합니다. 최소 거리도 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6e14f241e1c03d6b67a6c0f22515d375ae432487" translate="yes" xml:space="preserve">
          <source>This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.</source>
          <target state="translated">이 함수는 모듈을 크롤링하고 BaseEstimator에서 상속 된 모든 클래스를 가져옵니다. 테스트 모듈에 정의 된 클래스는 포함되지 않습니다. 기본적으로 GridSearchCV와 같은 meta_estimators도 포함되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="311d27372cf5315019acfac7480657777c623446" translate="yes" xml:space="preserve">
          <source>This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.</source>
          <target state="translated">이 함수는 기능을 numpy 배열 또는 scipy 희소 행렬로 추출하지 않습니다. 또한 load_content가 false이면 파일을 메모리에로드하려고 시도하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="28042729acf4bf75d343c82f013b112dad91f4c8" translate="yes" xml:space="preserve">
          <source>This function generates a GraphViz representation of the decision tree, which is then written into &lt;code&gt;out_file&lt;/code&gt;. Once exported, graphical renderings can be generated using, for example:</source>
          <target state="translated">이 함수는 의사 결정 트리의 GraphViz 표현을 생성 한 다음 &lt;code&gt;out_file&lt;/code&gt; 에 작성됩니다 . 일단 내 보내면 다음과 같은 그래픽 렌더링을 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90af5dc07a0d6b1b987fbe6284966c35b8f7dbed" translate="yes" xml:space="preserve">
          <source>This function implements Test 1 in:</source>
          <target state="translated">이 함수는 다음과 같이 테스트 1을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="51510777ab8c25d02b45243629e172250d646e40" translate="yes" xml:space="preserve">
          <source>This function is called with the estimated model and the randomly selected data: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with &lt;code&gt;is_data_valid&lt;/code&gt;. &lt;code&gt;is_model_valid&lt;/code&gt; should therefore only be used if the estimated model is needed for making the rejection decision.</source>
          <target state="translated">이 함수는 추정 모델과 무작위로 선택된 데이터 &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt; 와 함께 호출됩니다 . 반환 값이 False이면 현재 무작위로 선택된 하위 샘플을 건너 뜁니다. 이 함수를 사용하여 샘플을 거부하면 &lt;code&gt;is_data_valid&lt;/code&gt; 보다 계산 비용이 많이 듭니다 . 따라서 &lt;code&gt;is_model_valid&lt;/code&gt; 는 추정 모델이 거부 결정을 내리는 데 필요한 경우에만 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="a4283f593950d3e9c84617d07a78fd011e78bfa4" translate="yes" xml:space="preserve">
          <source>This function is called with the randomly selected data before the model is fitted to it: &lt;code&gt;is_data_valid(X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped.</source>
          <target state="translated">이 함수는 모델이 적합하기 전에 임의로 선택된 데이터로 호출됩니다 : &lt;code&gt;is_data_valid(X, y)&lt;/code&gt; . 반환 값이 False이면 현재 무작위로 선택된 하위 샘플을 건너 뜁니다.</target>
        </trans-unit>
        <trans-unit id="7289fd594a0de96a89a572bf0a7bd6e9501fda52" translate="yes" xml:space="preserve">
          <source>This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.</source>
          <target state="translated">이 함수는 결과가 단일 플랫 목록으로 연결되고 샘플 벡터가 모두 동일한 수의 기능을 갖도록 제한된다는 점을 제외하고 파일 목록에 대해 load_svmlight_file을 매핑하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="828fa7414b6e6676bd49f6624bf5ec1232d13777" translate="yes" xml:space="preserve">
          <source>This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.</source>
          <target state="translated">이 함수를 사용하면 미리 알려진 고정 된 클래스 레이블 세트에 대해이 변환을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="910452d7cd5e91358a13a185cadee882cea17632" translate="yes" xml:space="preserve">
          <source>This function modifies the estimator in-place.</source>
          <target state="translated">이 함수는 추정기를 제자리에서 수정합니다.</target>
        </trans-unit>
        <trans-unit id="3e0bd9f3948e27380d0112f277598d80d17269d2" translate="yes" xml:space="preserve">
          <source>This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">이 함수에는 실제 이진 값과 목표 점수가 필요하며, 이는 긍정적 클래스의 확률 추정치, 신뢰도 값 또는 이진 결정일 수 있습니다. 다음은 &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; 함수 를 사용하는 방법에 대한 간단한 예입니다 .</target>
        </trans-unit>
        <trans-unit id="2ddc8c678c75b432a0f0fafa6490a9a69a784bf8" translate="yes" xml:space="preserve">
          <source>This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.</source>
          <target state="translated">이 함수는 실제 결과와 가능한 결과의 예상 확률 사이의 평균 제곱 차이의 점수를 반환합니다. 실제 결과는 1 또는 0 (true 또는 false)이어야하지만 실제 결과의 예측 확률은 0과 1 사이의 값일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fbdeef434a34fee928d2d9974f81cfc54768558d" translate="yes" xml:space="preserve">
          <source>This function returns posterior probabilities of classification according to each class on an array of test vectors X.</source>
          <target state="translated">이 함수는 테스트 벡터 X의 배열에서 각 클래스에 따라 분류의 후방 확률을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="f7adc46ef6325367984cfe5d6cabca879706d70d" translate="yes" xml:space="preserve">
          <source>This function returns the Silhouette Coefficient for each sample.</source>
          <target state="translated">이 함수는 각 샘플에 대한 실루엣 계수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="30221178098fdd3682a8c91454092d6226b25e4f" translate="yes" xml:space="preserve">
          <source>This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt;&lt;code&gt;silhouette_samples&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 함수는 모든 샘플에 대한 평균 실루엣 계수를 반환합니다. 각 샘플의 값을 얻으려면 &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt; &lt;code&gt;silhouette_samples&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="bd812dc35d867d7152375e5009e2698c8db08fc0" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.</source>
          <target state="translated">이 함수는 단순히 유효한 쌍 거리 측정법을 반환합니다. 유효한 각 문자열에 대한 맵핑에 대한 설명을 허용하기 위해 존재합니다.</target>
        </trans-unit>
        <trans-unit id="fa4705a70e55596dcf2ace89a6d2a8d09a9fcccf" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.</source>
          <target state="translated">이 함수는 단순히 유효한 쌍 거리 측정법을 반환합니다. 그러나 각 유효한 문자열에 대한 자세한 맵핑 설명이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="d1a7a45215b31f0644d6e686c87b329e59419299" translate="yes" xml:space="preserve">
          <source>This function won&amp;rsquo;t compute the intercept.</source>
          <target state="translated">이 함수는 절편을 계산하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a808622a520f852134a2d8734b9e29ce0a669efe" translate="yes" xml:space="preserve">
          <source>This function works with dense 2D arrays only.</source>
          <target state="translated">이 기능은 고밀도 2D 어레이에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="19a23bf41918ee91955f633ce4d818d45490ef26" translate="yes" xml:space="preserve">
          <source>This function&amp;rsquo;s formula is as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7541ac358f5bb3bc5dcdfc1193ff72d6ac233a66" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.</source>
          <target state="translated">이 생성기 방법은 부스팅이 반복 될 때마다 앙상블 예측 클래스 확률을 산출하므로 각 부스트 후 테스트 세트에서 예측 된 클래스 확률을 결정하는 것과 같은 모니터링이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="7c1ad29f5d19940cf714626cd821b0934b5bc400" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.</source>
          <target state="translated">이 생성기 방법은 부스팅의 각 반복 후에 앙상블 예측을 생성하고, 따라서 각 부스트 후 테스트 세트에 대한 예측을 결정하는 것과 같은 모니터링을 허용한다.</target>
        </trans-unit>
        <trans-unit id="acc74c06c673308a3e484c230b5ff2c8348cfe79" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.</source>
          <target state="translated">이 생성기 방법은 각 부스팅 반복 후 앙상블 점수를 산출하므로 각 부스트 후 테스트 세트의 점수를 결정하는 등의 모니터링이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="1fe14f98435d58bb1786320c156e4b531f66e48b" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_multilabel_classification#sklearn.datasets.make_multilabel_classification&quot;&gt;&lt;code&gt;make_multilabel_classification&lt;/code&gt;&lt;/a&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb7462acd1f1e763247c87d170997bea5c436272" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="translated">이것은 &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; 데이터 셋 생성기를 보여줍니다 . 각 샘플은 두 클래스 (각각 최대 50 개)의 개수로 구성되며, 두 클래스마다 다르게 배포됩니다.</target>
        </trans-unit>
        <trans-unit id="b80113eed9b4b9668cc4e8b638bede5d1f2cf638" translate="yes" xml:space="preserve">
          <source>This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). It may attract a higher memory complexity when querying these nearest neighborhoods, depending on the &lt;code&gt;algorithm&lt;/code&gt;.</source>
          <target state="translated">이 구현은 모든 인접 쿼리를 대량 계산하여 메모리 복잡성을 O (nd)로 증가시킵니다. 여기서 d는 평균 이웃 수이며 원본 DBSCAN은 메모리 복잡도 O (n)를 갖습니다. &lt;code&gt;algorithm&lt;/code&gt; 에 따라 가장 가까운 이웃을 쿼리 할 때 메모리 복잡성이 높아질 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="48627963240be2350bc0acdc732c6b5348961a90" translate="yes" xml:space="preserve">
          <source>This implementation deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes, then computing only the distances to unprocessed points when constructing the cluster order. Note that we do not employ a heap to manage the expansion candidates, so the time complexity will be O(n^2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aed3668d0e58719270c1129ddb01322705c078e7" translate="yes" xml:space="preserve">
          <source>This implementation follows what is explained in the original paper &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;1&lt;/a&gt;. For the optimisation method, it currently uses scipy&amp;rsquo;s L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92eb61902d4dfd6571a464d87feff72fe7b32901" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="translated">이 구현은 Rubinstein, R., Zibulevsky, M. 및 Elad, M., Batch Orthogonal Matching Purchase Technical Report-CS Technion, 2008 년 4 월을 사용한 K-SVD 알고리즘의 효율적인 구현을 기반으로합니다. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt; http : //www.cs. technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="12813af32368fc3d74f568cdec1c9e38f408115a" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fb112845601277f8931b295b857e73c1428c8fb" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="translated">이 구현은 kd-trees 또는 ball-trees를 사용할 수없는 경우 (예를 들어 희소 행렬) 전체 쌍별 유사성 매트릭스를 구성하기 때문에 기본적으로 메모리 효율적이지 않습니다. 이 행렬은 n ^ 2 float를 소비합니다. 이 문제를 해결하기위한 몇 가지 메커니즘은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="34d2660e23d61d989853bcf418296ddcc27d9606" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e691dc0883398091a16d8a17daee0ec7cd95f29" translate="yes" xml:space="preserve">
          <source>This implementation is inspired by &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc51c30dcd01f51cada4be15777f17eb95eb7cbd" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="translated">이 구현은 대규모 응용 프로그램을위한 것이 아닙니다. 특히 scikit-learn은 GPU를 지원하지 않습니다. 딥 러닝 아키텍처를 구축 할 수있는 훨씬 더 유연한 유연성을 제공하는 프레임 워크뿐만 아니라 훨씬 빠른 GPU 기반 구현에 대해서는 &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;관련 프로젝트를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="228da043cb5452c21accbc429ac2d994bed0f4a9" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e30a130e6449e6025aca5fcf59ecb737e97cb91" translate="yes" xml:space="preserve">
          <source>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at:</source>
          <target state="translated">이 구현은 Cython으로 작성되었으며 상당히 빠릅니다. 그러나 더 빠른 API 호환 로더는 다음에서도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b488dd9d3cb1238d47d93805595214963db6dd0c" translate="yes" xml:space="preserve">
          <source>This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</source>
          <target state="translated">이 구현은 scipy.sparse.csr_matrix를 사용하여 수의 스파 스 표현을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="42bc7bbc3f5bd0df8efbfbad62976f6ec6db583b" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that 3 PLS packages provided in the R language (R-project):</source>
          <target state="translated">이 구현은 3 가지 PLS 패키지가 R 언어 (R- 프로젝트)로 제공되는 것과 동일한 결과를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="09c013dbb84b7e3d406ea0732bc3a8236ed37cbd" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that the &amp;ldquo;plspm&amp;rdquo; package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; of the &amp;ldquo;mixOmics&amp;rdquo; package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.</source>
          <target state="translated">이 구현은 plsca (X, Y) 함수를 사용하여 R 언어 (R- 프로젝트)로 제공되는&amp;ldquo;plspm&amp;rdquo;패키지와 동일한 결과를 제공합니다. 결과는 &quot;mixOmics&quot;패키지 의 함수 &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; 과 동일하거나 동일 선상에 있습니다. 차이점은 mixOmics 구현이 y_weights를 1로 정규화하지 않기 때문에 Wold 알고리즘을 정확하게 구현하지 않는다는 사실에 의존합니다.</target>
        </trans-unit>
        <trans-unit id="36e4a374c505873717456a086d5c9ed44e5157f6" translate="yes" xml:space="preserve">
          <source>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</source>
          <target state="translated">이 구현은 scipy.sparse 매트릭스를 중심에 두는 것을 거부합니다. 왜냐하면 스파 스를 비 스파 스로 만들고 메모리 소진 문제로 인해 프로그램을 충돌시킬 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="6684c1532df5f751b6b61c242ea952621dc3f4e8" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense and sparse numpy arrays of floating point values.</source>
          <target state="translated">이 구현은 부동 소수점 값의 밀도가 높고 희소 한 numpy 배열로 표시되는 데이터로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="b0df3cb22108e4cd0ed0fcd534ed03e427412c64" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays of floating point values for the features.</source>
          <target state="translated">이 구현은 기능에 대한 부동 소수점 값의 밀도가 높은 numpy 배열로 표시되는 데이터로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="4abb3ee00da8e0ef45c7b10f884f8d272712ca81" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.</source>
          <target state="translated">이 구현은 밀도가 높은 numpy 배열 또는 부동 소수점 값의 희소 scipy 배열로 표시되는 데이터로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="c3c22c958df17cff584f8c572beeb762a9a0290e" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).</source>
          <target state="translated">이 구현은 기능에 대해 밀도가 높거나 드문 드문 부동 소수점 값 배열로 표시되는 데이터로 작동합니다. 적합한 모델은 손실 매개 변수로 제어 할 수 있습니다. 기본적으로 선형 서포트 벡터 머신 (SVM)에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="c43a7d8bb7931a79100804db2f074a29d45e4b6b" translate="yes" xml:space="preserve">
          <source>This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called &amp;ldquo;Concentration of Measure&amp;rdquo; or &amp;ldquo;Curse of Dimensionality&amp;rdquo; for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.</source>
          <target state="translated">이 측정 값은 텍스트 데이터와 같은 고차원 데이터 세트에 대해 &quot;측정 농도&quot;또는 &quot;차원의 크기&quot;라고하는 현상으로 고통받는 것처럼 보이기 때문에 Silhouette Coefficient에서 보이지 않습니다. V- 측정 및 조정 랜드 지수와 같은 다른 측정 값은 정보 이론 기반 평가 점수입니다. 거리보다는 클러스터 할당에만 기반하므로 차원의 저주에 영향을받지 않습니다.</target>
        </trans-unit>
        <trans-unit id="18d0a71ee91c72fbdd2b834782dff2f0f441f1d5" translate="yes" xml:space="preserve">
          <source>This index signifies the average &amp;lsquo;similarity&amp;rsquo; between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0d4ffe805942e66e32866a4eb458e728074d78e" translate="yes" xml:space="preserve">
          <source>This initially creates clusters of points normally distributed (std=1) about vertices of an &lt;code&gt;n_informative&lt;/code&gt;-dimensional hypercube with sides of length &lt;code&gt;2*class_sep&lt;/code&gt; and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.</source>
          <target state="translated">이것은 초기 에 길이 &lt;code&gt;2*class_sep&lt;/code&gt; 의 측면을 갖는 &lt;code&gt;n_informative&lt;/code&gt; 차원 하이퍼 큐브의 정점에 대해 정규 분포 (std = 1) 점 의 클러스터를 생성하고 각 클래스에 동일한 수의 클러스터를 할당합니다. 이 기능들 간의 상호 의존성을 소개하고 다양한 유형의 추가 노이즈를 데이터에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="57108bb9f4ef70d6ebe6d73913a67e52e844d200" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b14c6be212126cf2e3bdc1fe1c6fe8f3c7dc46d" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; as at version 0.20 and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="translated">이 인터페이스는 0.20 버전에서 &lt;strong&gt;실험&lt;/strong&gt; 중이며 이후 릴리스에서는 예고없이 속성이 변경 될 수 있습니다 ( &lt;code&gt;data&lt;/code&gt; 및 &lt;code&gt;target&lt;/code&gt; 에는 약간만 변경해야 함 ).</target>
        </trans-unit>
        <trans-unit id="7f6be37b4617684744b3ccc169d2c583b6e3ddc1" translate="yes" xml:space="preserve">
          <source>This is a convenience alias to &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; to do random permutations of the collections.</source>
          <target state="translated">이것은 컬렉션의 무작위 순열을 수행 하기 위해 &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; 하기 위한 편리한 별명 입니다.</target>
        </trans-unit>
        <trans-unit id="4632bc2ee98a17257db1d248b06f38b79a53d4ef" translate="yes" xml:space="preserve">
          <source>This is a convenience function; the transformation is done using the default settings for &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이것은 편리한 기능입니다. 변환은 &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 의 기본 설정을 사용하여 수행됩니다 . 고급 사용 (스톱 워드 필터링, n- 그램 추출 등)을 위해 fetch_20newsgroups를 사용자 정의 &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2b3dbf5e1c5e08d66c77786f2bcbc632afc0312a" translate="yes" xml:space="preserve">
          <source>This is a convenience routine for the sake of testing. For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.</source>
          <target state="translated">이것은 테스트를위한 편리한 루틴입니다. 많은 메트릭의 경우 scipy.spatial.distance.cdist 및 scipy.spatial.distance.pdist의 유틸리티가 더 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="7a67e0a846a2ab3c88cb06fc2b7950cd30914abe" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</source>
          <target state="translated">UCI ML 유방암 위스콘신 (진단) 데이터 세트의 사본입니다. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22bae61d9be3213577df5087cb01b12cfdf8dff4" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Wine recognition datasets. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</source>
          <target state="translated">이것은 UCI ML 와인 인식 데이터 세트의 사본입니다. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c52f45448ee0e84b694b7c38bdbed7fd0e586461" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML housing dataset. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</source>
          <target state="translated">UCI ML 하우징 데이터 세트의 사본입니다. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a6d742ac48191eb71d1c4aabf6003187f0d21a9d" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dee2146876159a5a0b048cd24a612fae4a810ca" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="translated">이것은 UCI ML 손으로 쓴 숫자 데이터 세트의 테스트 세트 사본입니다. &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="60177910f782c7923853f8284b0287f57e3bf220" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd4a60c08b29c6d6af237f8cfa14740252c7d04a" translate="yes" xml:space="preserve">
          <source>This is a general function, given points on a curve. For computing the area under the ROC-curve, see &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;. For an alternative way to summarize a precision-recall curve, see &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이것은 곡선의 점이 주어지면 일반적인 기능입니다. ROC 곡선 아래 영역을 계산하려면 &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 . 정밀 회수 곡선을 요약하는 다른 방법은 &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3944e3eb8c8ea1f918637d2548f35fa74cd97d2a" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting with &lt;code&gt;transformer_weights&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37510c6c60985c0ea76b5bcc4364db965e5a12fd" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">이것은 ColumnTransformer 생성자의 약자입니다. 변압기의 이름을 요구하지 않으며 허용하지도 않습니다. 대신, 유형에 따라 이름이 자동으로 부여됩니다. 또한 가중치를 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="022d95ed0540e35ea0bdce867e9a502603bc5f51" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="translated">이것은 FeatureUnion 생성자의 약자입니다. 변압기의 이름을 요구하지 않으며 허용하지도 않습니다. 대신, 유형에 따라 이름이 자동으로 부여됩니다. 또한 가중치를 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="52a890ca0cc5d284d366294db21e8c380349733e" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</source>
          <target state="translated">이것은 파이프 라인 생성자의 약자입니다. 추정 자의 이름을 요구하지 않으며 허용하지도 않습니다. 대신 이름이 자동으로 소문자로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="bcbf4cb6d3eb7ea12d02241a9a60f7a4e6044f4d" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.predict(X)&lt;/code&gt;.</source>
          <target state="translated">이것은 &lt;code&gt;estimator_.predict(X)&lt;/code&gt; 의 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="17ebe8027dbbfba976bb150f0e9172e06d0c02ec" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.score(X, y)&lt;/code&gt;.</source>
          <target state="translated">이것은 &lt;code&gt;estimator_.score(X, y)&lt;/code&gt; 의 래퍼입니다 .</target>
        </trans-unit>
        <trans-unit id="602675ab661ad893c615b29b13c1d56146fcfa0b" translate="yes" xml:space="preserve">
          <source>This is an alternative to passing a &lt;code&gt;backend='backend_name'&lt;/code&gt; argument to the &lt;code&gt;Parallel&lt;/code&gt; class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</source>
          <target state="translated">이는 &lt;code&gt;backend='backend_name'&lt;/code&gt; 인수를 &lt;code&gt;Parallel&lt;/code&gt; 클래스 생성자 에 전달하는 대안 입니다. joblib를 내부적으로 사용하지만 백엔드 인수를 자체 API에 노출시키지 않는 라이브러리 코드를 호출 할 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="47f9c3947e84bb8a914aa6f2f19cb2c5e42e970f" translate="yes" xml:space="preserve">
          <source>This is an example of &lt;strong&gt;bias/variance tradeoff&lt;/strong&gt;: the larger the ridge &lt;code&gt;alpha&lt;/code&gt; parameter, the higher the bias and the lower the variance.</source>
          <target state="translated">이는 &lt;strong&gt;치우침 / 분산 트레이드 오프&lt;/strong&gt; 의 예입니다 . 릿지 &lt;code&gt;alpha&lt;/code&gt; 매개 변수가 클수록 치우침이 높고 분산이 더 낮습니다.</target>
        </trans-unit>
        <trans-unit id="d75c7c933c17fefbabe7c2e292b885d0ecac3a21" translate="yes" xml:space="preserve">
          <source>This is an example of applying &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).</source>
          <target state="translated">이것은 문서 모음에 &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt; 을 적용 하고 모음의 주제 구조에 대한 추가 모델을 추출 하는 예입니다 . 결과는 주제 목록으로, 각각 용어 목록으로 표시됩니다 (무게는 표시되지 않음).</target>
        </trans-unit>
        <trans-unit id="53176f2993974522405fa17c9a80a84e38a4969c" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&amp;rsquo;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch.</source>
          <target state="translated">다음은 scikit-learn이 코어 외부 접근 방식을 사용하여 분류에 사용되는 방법을 보여주는 예입니다. 주 메모리에 맞지 않는 데이터에서 학습합니다. 우리는 온라인 분류기, 즉 partial_fit 메소드를 지원하는 분류기를 사용합니다. 시간이 지남에 따라 기능 공간이 동일하게 유지되도록 각 예제를 동일한 기능 공간으로 투영하는 HashingVectorizer를 활용합니다. 이것은 각 분류에 새로운 기능 (단어)이 나타날 수있는 텍스트 분류의 경우에 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="1620bf9fc1f7795235eabc8e2a67657eca16390d" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</source>
          <target state="translated">이것은 scikit-learn을 사용하여 bag-of-words 접근법을 사용하여 주제별로 문서를 분류하는 방법을 보여줍니다. 이 예에서는 scipy.sparse 행렬을 사용하여 기능을 저장하고 희소 행렬을 효율적으로 처리 할 수있는 다양한 분류기를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="687fdb042e4ef171de769c4722977550577ec678" translate="yes" xml:space="preserve">
          <source>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.</source>
          <target state="translated">다음은 scikit-learn을 사용하여 bag-of-words 접근 방식을 사용하여 주제별로 문서를 클러스터링하는 방법을 보여줍니다. 이 예에서는 표준 numpy 배열 대신 scipy.sparse 행렬을 사용하여 기능을 저장합니다.</target>
        </trans-unit>
        <trans-unit id="c505c2f7b70a5aa0d5582bdc56a7d9627b32a4d8" translate="yes" xml:space="preserve">
          <source>This is an example showing the prediction latency of various scikit-learn estimators.</source>
          <target state="translated">다양한 사이 킷 학습 추정기의 예측 지연 시간을 보여주는 예입니다.</target>
        </trans-unit>
        <trans-unit id="9e4f7a05490ee1267f03d9980bace7147baa0b76" translate="yes" xml:space="preserve">
          <source>This is an extension of the algorithm in scipy.stats.mode.</source>
          <target state="translated">이것은 scipy.stats.mode에있는 알고리즘의 확장입니다.</target>
        </trans-unit>
        <trans-unit id="375819c22c211b4c7fc97205acd724c3a575f620" translate="yes" xml:space="preserve">
          <source>This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that there will be no speedup with liblinear solver, since it does not handle warm-starting.</source>
          <target state="translated">이것은 이전 모델의 결과를 사용하여 솔루션 세트를 따라 계산 속도를 높이고 여러 매개 변수에 대해 LogisticRegression을 순차적으로 호출하는 것보다 빠르게 구현하는 구현입니다. liblinear 솔버는 웜 스타트를 처리하지 않으므로 속도가 향상되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="89098058da4c55a1db96b87aadaa162a0a15baba" translate="yes" xml:space="preserve">
          <source>This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">이것은 scikit-learn 추정기 인터페이스를 구현하는 것으로 가정합니다. 평가자가 &lt;code&gt;score&lt;/code&gt; 함수 를 제공 하거나 &lt;code&gt;scoring&lt;/code&gt; 를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="a9f1a5b0fa7d00ad69170d1ab81bf1031dee11a2" translate="yes" xml:space="preserve">
          <source>This is called a &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation.</source>
          <target state="translated">이를 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 교차 검증 이라고합니다 .</target>
        </trans-unit>
        <trans-unit id="2e974743bc0fdffbf7238debaf0ee76bb5a5d9b2" translate="yes" xml:space="preserve">
          <source>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</source>
          <target state="translated">유클리드 (L2) 정규화로 벡터를 단위 구에 투영하고 그 점 곱이 벡터로 표시된 점 사이의 각도의 코사인이기 때문에 이것을 코사인 유사성이라고합니다.</target>
        </trans-unit>
        <trans-unit id="9b01365512b47448f649e450ddd111360a73cfc3" translate="yes" xml:space="preserve">
          <source>This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and is a core problem that machine learning addresses.</source>
          <target state="translated">이것을 &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;차원&lt;/a&gt; 의 저주 라고하며 기계 학습이 해결하는 핵심 문제입니다.</target>
        </trans-unit>
        <trans-unit id="25e5e11d6a0e13a60841f1cb72db59989d03472f" translate="yes" xml:space="preserve">
          <source>This is currently implemented in the following classes:</source>
          <target state="translated">이것은 현재 다음 클래스에서 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="a774a1be5070f83615f896d6d2ec16ebfbe92e4e" translate="yes" xml:space="preserve">
          <source>This is done in 2 steps:</source>
          <target state="translated">이것은 2 단계로 이루어집니다 :</target>
        </trans-unit>
        <trans-unit id="0c564a0d4cfad247ff47792f5a12558130a84f0c" translate="yes" xml:space="preserve">
          <source>This is equivalent to fit followed by transform, but more efficiently implemented.</source>
          <target state="translated">이는 적합하지만 변형과 동일하지만보다 효율적으로 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="831022bba18e9ed70a7a762cd8243e7523afeddb" translate="yes" xml:space="preserve">
          <source>This is especially useful when the whole dataset is too big to fit in memory at once.</source>
          <target state="translated">이것은 전체 데이터 세트가 한 번에 메모리에 맞지 않을 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="75c0bef753e28aecf63e47221c52fe362a027981" translate="yes" xml:space="preserve">
          <source>This is implemented as &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.</source>
          <target state="translated">이것은 &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; 로 구현되며 , 각 가능한 클래스 쌍에 대한 결정 결과를 예측하는 추정자가 대부분의 투표로 클래스의 레이블을 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="9b2a6723fed7b2d139e18e341020f963dc7f8450" translate="yes" xml:space="preserve">
          <source>This is implemented by linking the points X into the graph of geodesic distances of the training data. First the &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.</source>
          <target state="translated">이것은 점 X를 훈련 데이터의 측지 거리의 그래프에 연결함으로써 구현됩니다. 먼저 X의 가장 가까운 &lt;code&gt;n_neighbors&lt;/code&gt; 이웃이 훈련 데이터에서 발견되며, 이들 X로부터 각 지점에서 훈련 데이터의 각 지점까지의 가장 짧은 측지 거리는 커널을 구성하기 위해 계산됩니다. X의 삽입은이 커널이 훈련 세트의 포함 벡터에 투영 된 것입니다.</target>
        </trans-unit>
        <trans-unit id="51ae8a82b3eff6fb295f70aa28171d41c73ac3db" translate="yes" xml:space="preserve">
          <source>This is implemented in &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt;. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; constructor parameter. This parameter has no influence on &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; &lt;/a&gt; 에서 구현됩니다 . &lt;code&gt;n_components&lt;/code&gt; 생성자 매개 변수를 사용하여 원하는 차원을 설정할 수 있습니다 . 이 매개 변수는 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt; &lt;/a&gt; 에 영향을 미치지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="8989f9efb82b77a4f24da08f40263dc49964cf0b" translate="yes" xml:space="preserve">
          <source>This is implemented in the &lt;code&gt;transform&lt;/code&gt; method. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; parameter. This parameter has no influence on the &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de8220f3c95931fc4241bdd5334e66fca04da2f0" translate="yes" xml:space="preserve">
          <source>This is known as &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이것을 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; 이라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="1c0b6d6227299e9452904f2af6316a83adba923a" translate="yes" xml:space="preserve">
          <source>This is minimized if \(h(x_i)\) is fitted to predict a value that is proportional to the negative gradient \(-g_i\). Therefore, at each iteration, &lt;strong&gt;the estimator&lt;/strong&gt;\(h_m\)&lt;strong&gt;is fitted to predict the negative gradients of the samples&lt;/strong&gt;. The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d1ef16c8ffe6df8c7a1a81b132f67cdda1b92ea" translate="yes" xml:space="preserve">
          <source>This is more efficient than calling fit followed by transform.</source>
          <target state="translated">이것은 fit을 호출 한 다음 변환하는 것보다 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="1dfb3afc660617ced3002fefa24847b5bb1a14dd" translate="yes" xml:space="preserve">
          <source>This is mostly equivalent to calling:</source>
          <target state="translated">이것은 대부분 다음을 호출하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bbd51f304b678a157464ff7ab03c52123d04218d" translate="yes" xml:space="preserve">
          <source>This is not a symmetric function.</source>
          <target state="translated">이것은 대칭 기능이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="0b5c42967b0e34c52656dd80a4e659c6f0fa2181" translate="yes" xml:space="preserve">
          <source>This is not exactly the same as &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt;. The authors of &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</source>
          <target state="translated">이것은 &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt; 과 정확히 동일하지 않습니다 . &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; 의 저자는 위의 버전을 선호합니다. 항상 양의 한정판 입니다. 커널은 부가 적이므로 모든 구성 요소 \ (x_i \)를 포함하기 위해 개별적으로 처리 할 수 ​​있습니다. 이를 통해 Monte Carlo 샘플링을 사용하여 근사화하는 대신 일정한 간격으로 푸리에 변환을 샘플링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f7f802fcac19c1c8ed1c55f673312d761a2c94a7" translate="yes" xml:space="preserve">
          <source>This is not the case for &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt;: both are bound by the relationship:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; 의 경우에는 해당되지 않습니다 . 둘 다 관계에 의해 구속됩니다.</target>
        </trans-unit>
        <trans-unit id="5d73f5b087f2ecb2dadb8778d3d18cfc19507034" translate="yes" xml:space="preserve">
          <source>This is not true for &lt;code&gt;mutual_info_score&lt;/code&gt;, which is therefore harder to judge:</source>
          <target state="translated">&lt;code&gt;mutual_info_score&lt;/code&gt; 에는 해당되지 않으므로 판단하기가 더 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="982101a3d677907e48e034c807cde26531a0468b" translate="yes" xml:space="preserve">
          <source>This is only available if no vocabulary was given.</source>
          <target state="translated">어휘가없는 경우에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0ca1a333516e3b52907835d092958662636ea528" translate="yes" xml:space="preserve">
          <source>This is particularly important for doing grid searches:</source>
          <target state="translated">그리드 검색을 수행 할 때 특히 중요합니다.</target>
        </trans-unit>
        <trans-unit id="2413be66af5e312ff97e484aff33c56868971fbe" translate="yes" xml:space="preserve">
          <source>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&amp;rsquo;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp;amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</source>
          <target state="translated">패턴 인식 문헌에서 가장 잘 알려진 데이터베이스 일 것입니다. 피셔의 논문은이 분야의 고전이며 오늘날 자주 언급됩니다. (예를 들어 Duda &amp;amp; Hart를 참조하십시오.) 데이터 세트에는 각각 50 개의 인스턴스로 구성된 3 개의 클래스가 포함되며, 각 클래스는 홍채 식물의 유형을 나타냅니다. 한 클래스는 다른 클래스와 선형으로 분리 가능합니다. 후자는 서로 선형으로 분리 할 수 ​​없습니다.</target>
        </trans-unit>
        <trans-unit id="32ef6d8e689e0cbccf726fb7a94339c2864c487f" translate="yes" xml:space="preserve">
          <source>This is present only if &lt;code&gt;refit&lt;/code&gt; is not False.</source>
          <target state="translated">이것은 &lt;code&gt;refit&lt;/code&gt; 가 거짓이 아닌 경우에만 존재합니다 .</target>
        </trans-unit>
        <trans-unit id="717414c2af196799a3d1dc1aec1269a995318377" translate="yes" xml:space="preserve">
          <source>This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.</source>
          <target state="translated">이는 오류 세트 크기와 비슷하지만 관련이 있고 관련이없는 레이블의 수에 따라 가중치가 적용됩니다. 최고의 성능은 순위 손실 0으로 달성됩니다.</target>
        </trans-unit>
        <trans-unit id="00034266cafd87d919959c8134650d981f2c4466" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="translated">이것은 scikit-learn의 클래스 및 함수 참조입니다. 클래스 및 함수 기본 사양이 사용에 대한 전체 지침을 제공하기에 충분하지 않을 수 있으므로 자세한 내용 은 &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;전체 사용자 안내서&lt;/a&gt; 를 참조하십시오 . API에서 반복되는 개념에 대한 참조 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;는 공통 용어 및 API 요소 용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0706513e1a92902a9b1d8477404304aebd12678b" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;https://scikit-learn.org/0.23/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="486cb190a77f54ef106791f7b4834d94d87b42d4" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns &lt;code&gt;y_pred&lt;/code&gt; probabilities for its training data &lt;code&gt;y_true&lt;/code&gt;. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c0b38fb17b983574b86706f2172c4c4bac1c6a5" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier&amp;rsquo;s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="translated">이것은 (다항식) 로지스틱 회귀 및 신경망과 같은 그것의 확장에 사용되는 손실 함수로, 확률 분류기의 예측이 주어지면 진정한 라벨의 음의 로그 가능성으로 정의됩니다. 로그 손실은 두 개 이상의 레이블에 대해서만 정의됩니다. {0,1}에 실제 레이블 yt가 있고 단일 확률 yp가 yt = 1 인 단일 표본의 경우 로그 손실은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a9111de5c7f4d9db0e2dc4faf926c5c47ae0eb12" translate="yes" xml:space="preserve">
          <source>This is the result of calling &lt;code&gt;method&lt;/code&gt;</source>
          <target state="translated">이것은 &lt;code&gt;method&lt;/code&gt; 호출의 결과입니다</target>
        </trans-unit>
        <trans-unit id="611492c50f944d397ce592f12eabee4801e28de1" translate="yes" xml:space="preserve">
          <source>This is the structured version, that takes into account some topological structure between samples.</source>
          <target state="translated">이것은 샘플 사이의 일부 토폴로지 구조를 고려한 구조화 된 버전입니다.</target>
        </trans-unit>
        <trans-unit id="eb0a6c68cdbb70ef7265210b8b8760243faa6912" translate="yes" xml:space="preserve">
          <source>This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.</source>
          <target state="translated">이는 다른 방법으로는 직접 맞출 수없는 구현에 절편 항을 맞추는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="1c420e62ce6697ac415dbca5798c0153080b025c" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">이전에 사용한 모델의 저장된 속성을 재사용해야하는 경우에 유용합니다. False로 설정하면 모든 호출에 맞게 계수가 다시 작성됩니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="036c7996302f5b904a8b61c47c147a7a9733676a" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2abc93ddea964d3eb32a39867456d5dfe5ef9180" translate="yes" xml:space="preserve">
          <source>This is visible if we compare the standard deviations of different features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42f2c0052d88e51f5df6c26752c192f81040ec01" translate="yes" xml:space="preserve">
          <source>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; accepts &lt;code&gt;scipy.sparse&lt;/code&gt; matrices. (Note that the tf-idf functionality in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; can produce normalized vectors, in which case &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; is equivalent to &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt;, only slower.)</source>
          <target state="translated">이 커널은 tf-idf 벡터로 표시되는 문서의 유사성을 계산하는 데 널리 사용됩니다. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;scipy.sparse&lt;/code&gt; 행렬을 허용 합니다. (의 TF-IDF 기능 유의 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 는 정규화 벡터를 생산할 수는있는 경우 &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt; &lt;code&gt;cosine_similarity&lt;/code&gt; 는&lt;/a&gt; 동등 &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt; 만 느리다.)</target>
        </trans-unit>
        <trans-unit id="f1fbcea40cf4aba903be6eddf36c859a1718e3b8" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth.</source>
          <target state="translated">이 커널은 무한히 차별화가 가능합니다. 즉, 공분산 함수로이 커널을 사용하는 GP는 모든 차수의 평균 제곱 도함수를 가지므로 매우 부드럽습니다.</target>
        </trans-unit>
        <trans-unit id="ffa176d62610a7e6d304ab9efadadddacf7fa00e" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. See &lt;a href=&quot;#redc669bcbe98-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;, Chapter 4, Section 4.2, for further details of the RBF kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ad60f9f3ef06c9a3898414b0afc593eaff20c1d" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</source>
          <target state="translated">이 커널은 무한히 차별화가 가능합니다. 즉, 공분산 함수로이 커널을 사용하는 GP는 모든 차수의 평균 제곱 도함수를 가지므로 매우 부드럽습니다. RBF 커널로 인한 GP의 앞뒤는 다음 그림과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6cbded70a18b870dfff7fda8d59e204ebd77900f" translate="yes" xml:space="preserve">
          <source>This kind of singular profiles is often seen in practice, for instance:</source>
          <target state="translated">이러한 종류의 단일 프로파일은 종종 다음과 같이 실제로 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="30529b10016c80d5c9ab1e213672fabc65487573" translate="yes" xml:space="preserve">
          <source>This last point is expected due to the nature of the problem: the occurrence of accidents is mostly dominated by circumstantial causes that are not captured in the columns of the dataset and can indeed be considered as purely random.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df4414d1e47e9ea41e98b8c6e13b5eeb49e06ac" translate="yes" xml:space="preserve">
          <source>This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes &amp;ldquo;for free&amp;rdquo; as no additional data is needed and can be used for model selection.</source>
          <target state="translated">이 생략 부분은 별도의 유효성 검사 세트에 의존하지 않고 일반화 오류를 추정하는 데 사용할 수 있습니다. 이 데이터는 추가 데이터가 필요 없으며 모델 선택에 사용할 수 있으므로 &quot;무료&quot;로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="4f52c7809ab985057ba93af9b88459b0d10b33a2" translate="yes" xml:space="preserve">
          <source>This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.</source>
          <target state="translated">이는 손실 함수가 그 영향을 완전히 무시하지 않는 이상 특이 치에 의해 크게 영향을받지 않도록합니다.</target>
        </trans-unit>
        <trans-unit id="2fb338a66a8f54901bcaa2314035cd86710a7d6d" translate="yes" xml:space="preserve">
          <source>This means each coefficient \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4826ff4b754b03a246d73c38aa2c971ffdf335e1" translate="yes" xml:space="preserve">
          <source>This means each weight \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="translated">이것은 각 가중치 \ (w_ {i} \)가 0을 중심으로하고 정밀도 \ (\ lambda_ {i} \)를 가진 가우시안 분포에서 도출됨을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="de2308f740624c092eea038ac13deb5af2b1fa80" translate="yes" xml:space="preserve">
          <source>This means that any classifiers handling multi-output multiclass or multi-task classification tasks, support the multi-label classification task as a special case. Multi-task classification is similar to the multi-output classification task with different model formulations. For more information, see the relevant estimator documentation.</source>
          <target state="translated">이는 다중 출력 다중 클래스 또는 다중 작업 분류 작업을 처리하는 모든 분류자가 다중 레이블 분류 작업을 특수한 경우로 지원함을 의미합니다. 다중 작업 분류는 다른 모델 구성을 사용하는 다중 출력 분류 작업과 유사합니다. 자세한 정보는 관련 추정기 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="6157bc0b8c2f67c8a593bf2d12852c000be43e72" translate="yes" xml:space="preserve">
          <source>This measure is not adjusted for chance. Therefore &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; might be preferred.</source>
          <target state="translated">이 측정 값은 우연히 조정되지 않았습니다. 따라서 &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt; 가 선호 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cca48ea1404a91d2df7b18cac656df30067cb12b" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.</source>
          <target state="translated">이 방법을 사용하면 각 부스팅 반복 후 모니터링 (즉, 테스트 세트의 오류 결정)이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="778da8cdceb825f45e49260c13130972c415ba18" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each stage.</source>
          <target state="translated">이 방법을 사용하면 각 단계 후에 모니터링 (즉, 테스트 세트에서 오류를 판별) 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="893e20b8eaafff147aad0ee519c22b2be0783798" translate="yes" xml:space="preserve">
          <source>This method allows to generalize prediction to &lt;em&gt;new observations&lt;/em&gt; (not in the training set). Only available for novelty detection (when novelty is set to True).</source>
          <target state="translated">이 방법을 사용하면 예측을 &lt;em&gt;새로운 관측치&lt;/em&gt; 로 일반화 할 수 있습니다 (훈련 세트가 아님). 참신 탐지에만 사용할 수 있습니다 (참신이 True로 설정된 경우).</target>
        </trans-unit>
        <trans-unit id="e13bbab31202d773dbd5b155d7e0b7799ca54f84" translate="yes" xml:space="preserve">
          <source>This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n, p) this method has a cost of \(O(n p^2)\), assuming that \(n \geq p\).</source>
          <target state="translated">이 방법은 X의 특이 값 분해를 사용하여 최소 제곱 솔루션을 계산합니다. X가 크기 (n, p)의 행렬 인 경우이 방법은 \ (n \ geq p \).</target>
        </trans-unit>
        <trans-unit id="dc0919b0c811a79ed295c00492df459fa5ab93e8" translate="yes" xml:space="preserve">
          <source>This method doesn&amp;rsquo;t do anything. It exists purely for compatibility with the scikit-learn transformer API.</source>
          <target state="translated">이 방법은 아무것도하지 않습니다. scikit-learn 변환기 API와의 호환성을 위해 존재합니다.</target>
        </trans-unit>
        <trans-unit id="89035c0aee87e0125ffcf7bf5f0dbe56fcfb74a9" translate="yes" xml:space="preserve">
          <source>This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">이 방법은 약간의 성능과 수치 안정성 오버 헤드를 가지므로 오버 헤드를 숨기려면 가능한 한 (메모리 예산에 맞는 한) 데이터 청크에 대해 partial_fit을 호출하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="a344504140059db6f88458066c961354f795c6a7" translate="yes" xml:space="preserve">
          <source>This method has some performance overhead hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">이 방법에는 약간의 성능 오버 헤드가 있으므로 오버 헤드를 숨기려면 가능한 한 (메모리 예산에 맞는 한) 데이터 청크에 대해 partial_fit을 호출하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="03d61ee68976e414aa4354d3a63287b0819f990d" translate="yes" xml:space="preserve">
          <source>This method has the same order of complexity as &lt;a href=&quot;#ordinary-least-squares&quot;&gt;Ordinary Least Squares&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ceeeb1e178fb9959d4ffe786c9917a8fc68013bb" translate="yes" xml:space="preserve">
          <source>This method has the same order of complexity than an &lt;a href=&quot;#ordinary-least-squares&quot;&gt;Ordinary Least Squares&lt;/a&gt;.</source>
          <target state="translated">이 방법은 &lt;a href=&quot;#ordinary-least-squares&quot;&gt;일반 최소 제곱&lt;/a&gt; 과 동일한 순서의 복잡성을 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="8149d43c4db3023940e20ddee18dfd4caaeb24b1" translate="yes" xml:space="preserve">
          <source>This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning.</source>
          <target state="translated">이 방법은 코어 외부 또는 온라인 학습을 구현하기 위해 데이터 집합의 여러 청크에서 연속적으로 여러 번 호출 될 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="7914e16bc2009e2d4fc76b2006a65a031a42eb76" translate="yes" xml:space="preserve">
          <source>This method is just there to implement the usual API and hence work in pipelines.</source>
          <target state="translated">이 방법은 일반적인 API를 구현하기위한 것이므로 파이프 라인에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="53a4e9f6af590018ff1a37b6f68db2405dd79282" translate="yes" xml:space="preserve">
          <source>This method is just there to mark the fact that this transformer can work in a streaming setup.</source>
          <target state="translated">이 방법은이 변환기가 스트리밍 설정에서 작동 할 수 있다는 사실을 표시하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="1ad1f3e2c791502dcf55d0ea0c930c86843ade76" translate="yes" xml:space="preserve">
          <source>This method is meant to be called concurrently by the multiprocessing callback. We rely on the thread-safety of dispatch_one_batch to protect against concurrent consumption of the unprotected iterator.</source>
          <target state="translated">이 메소드는 멀티 프로세싱 콜백에 의해 동시에 호출됩니다. 우리는 dispatch_one_batch의 스레드 안전성에 의존하여 보호되지 않은 반복자의 동시 소비를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="4e81340dab29281a8d6b3bd99833383bb408f46c" translate="yes" xml:space="preserve">
          <source>This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference.</source>
          <target state="translated">이 방법은 결정적이지 않습니다. X에서 자유 에너지라고하는 수량을 계산 한 다음 임의로 손상된 X 버전에서 수량을 계산하고 차이의 로지스틱 함수 로그를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="483c17ab697933f17e74386d9739e36cf3fc93e7" translate="yes" xml:space="preserve">
          <source>This method is only available for log loss and modified Huber loss.</source>
          <target state="translated">이 방법은 로그 손실 및 수정 된 Huber 손실에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d7475ebc10f647671bee9a4be7afee1b81279276" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">이 방법은 거리 행렬을 입력으로 취하는 안전한 방법을 제공하면서 벡터 배열을 사용하는 다른 많은 알고리즘과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="b5d8e2fef5ebecb0c66f96f2926a64438412f355" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a kernel matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">이 방법은 커널 행렬을 입력으로 취하는 안전한 방법을 제공하면서 벡터 배열을 취하는 다른 많은 알고리즘과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="c662dd229414f848149c194436efac32b71db4c2" translate="yes" xml:space="preserve">
          <source>This method returns a Fortran-ordered array. To convert it to a C-ordered array, use &amp;lsquo;np.ascontiguousarray&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc2db598aaa9c1a0947d8f73a1238d30285a532" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.</source>
          <target state="translated">이 메서드는 벡터 배열 또는 거리 행렬을 가져 와서 거리 행렬을 반환합니다. 입력이 벡터 배열 인 경우 거리가 계산됩니다. 입력 값이 거리 행렬이면 대신 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="924736c0bae89c3f4376281e6a105fa549739eb4" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.</source>
          <target state="translated">이 메서드는 벡터 배열이나 커널 행렬을 가져 와서 커널 행렬을 반환합니다. 입력이 벡터 배열 인 경우 커널이 계산됩니다. 입력이 커널 행렬이면 대신 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="b80df7fbbffdde8aff9c30af4a5bee17e602075b" translate="yes" xml:space="preserve">
          <source>This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.</source>
          <target state="translated">이 방법은 피쳐를 균일하거나 정규 분포를 따르도록 변환합니다. 따라서 특정 기능에 대해이 변환은 가장 빈번한 값을 퍼뜨리는 경향이 있습니다. 또한 (마진) 이상치의 영향을 줄입니다. 따라서 강력한 전처리 체계입니다.</target>
        </trans-unit>
        <trans-unit id="88cc56a80a6739c5287afd3119dab66fa95d86a9" translate="yes" xml:space="preserve">
          <source>This method will raise a &lt;code&gt;ValueError&lt;/code&gt; if any of the estimators do not have &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a6448f2646e45809baed97b35289a513191212b" translate="yes" xml:space="preserve">
          <source>This method works similarly to the builtin &lt;code&gt;apply&lt;/code&gt;, except that the function is called only if the cache is not up to date.</source>
          <target state="translated">이 메소드 는 캐시가 최신 상태가 아닌 경우에만 함수가 호출된다는 점을 제외하고 내장 &lt;code&gt;apply&lt;/code&gt; 와 유사하게 작동 합니다.</target>
        </trans-unit>
        <trans-unit id="3b270b097c02b54b15c6706faba2a05992e48391" translate="yes" xml:space="preserve">
          <source>This metric is furthermore symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.</source>
          <target state="translated">이 메트릭은 더 대칭 적입니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 동일한 점수 값이 반환됩니다. 이는 실제 사실을 알 수없는 경우 동일한 데이터 세트에 대해 두 개의 독립적 인 레이블 지정 전략의 일치를 측정하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b8da4b4fabd4786b82c03e2c15a17659173e15c8" translate="yes" xml:space="preserve">
          <source>This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score value in any way.</source>
          <target state="translated">이 메트릭은 레이블의 절대 값과 무관합니다. 클래스 또는 클러스터 레이블 값의 순열은 점수 값을 변경하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bfbb6fef2be45da43d1153172735208301268ab3" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">이 메트릭은 대칭이 아닙니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 일반적으로 다른 &lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; 를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="d6ecae2ce63387462768b5daf2f548b32eba4de4" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">이 메트릭은 대칭이 아닙니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 일반적으로 다른 &lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; 가 반환 됩니다.</target>
        </trans-unit>
        <trans-unit id="b4ddf27eda44a85481c7034e578198a043591cb2" translate="yes" xml:space="preserve">
          <source>This metric is not well-defined for single samples and will return a NaN value if n_samples is less than two.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3d29b108d9b9b14881da7a1115d336ab614cf19" translate="yes" xml:space="preserve">
          <source>This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated to each sample.</source>
          <target state="translated">이 메트릭은 다중 레이블 순위 문제에서 사용되며, 목표는 각 샘플과 관련된 레이블의 순위를 높이는 것입니다.</target>
        </trans-unit>
        <trans-unit id="aca1523dd1402afa978fd168a95010ba6eea69bb" translate="yes" xml:space="preserve">
          <source>This might be clearer with an example: consider a three class problem with class 0 having three support vectors \(v^{0}_0, v^{1}_0, v^{2}_0\) and class 1 and 2 having two support vectors \(v^{0}_1, v^{1}_1\) and \(v^{0}_2, v^{1}_2\) respectively. For each support vector \(v^{j}_i\), there are two dual coefficients. Let&amp;rsquo;s call the coefficient of support vector \(v^{j}_i\) in the classifier between classes \(i\) and \(k\)\(\alpha^{j}_{i,k}\). Then &lt;code&gt;dual_coef_&lt;/code&gt; looks like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70091de439c388c847d5db9bb63c11ef6af9aff3" translate="yes" xml:space="preserve">
          <source>This might be made more clear by an example:</source>
          <target state="translated">이것은 예를 통해 더 명확해질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a18c0c170fadd6145ebf30e97149394844cb89ac" translate="yes" xml:space="preserve">
          <source>This mixin provides a feature selector implementation with &lt;code&gt;transform&lt;/code&gt; and &lt;code&gt;inverse_transform&lt;/code&gt; functionality given an implementation of &lt;code&gt;_get_support_mask&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87c35466b9ea86a2466ad7ff0fff224c499553d9" translate="yes" xml:space="preserve">
          <source>This model has many parameters, however the default values are quite reasonable (please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details):</source>
          <target state="translated">이 모델에는 많은 매개 변수가 있지만 기본값은 상당히 합리적입니다 (자세한 내용은 &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;참조 설명서&lt;/a&gt; 를 참조 하십시오).</target>
        </trans-unit>
        <trans-unit id="a96824cdb923fbde7424d806cedfbff3d185c97a" translate="yes" xml:space="preserve">
          <source>This model is an extension of the Sequential Karhunen-Loeve Transform from: &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</source>
          <target state="translated">이 모델은 &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; 참조 &lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="22862405516241c579b3ee92c4c4e899025ea8d7" translate="yes" xml:space="preserve">
          <source>This model is an extension of the Sequential Karhunen-Loeve Transform from: &lt;em&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/em&gt; See &lt;a href=&quot;https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5eaf31e8c5d94894f814f950dadb507546a62c7a" translate="yes" xml:space="preserve">
          <source>This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.</source>
          <target state="translated">이 모델은 기본 레이블 전파 알고리즘과 유사하지만 정규화 된 그래프 라플라시안 및 레이블 전체의 소프트 클램핑을 기반으로 선호도 매트릭스를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="31cfc2556e1a899818d23b9328ee0744c892d322" translate="yes" xml:space="preserve">
          <source>This model optimizes the log-loss function using LBFGS or stochastic gradient descent.</source>
          <target state="translated">이 모델은 LBFGS 또는 확률 적 경사 하강을 사용하여 로그 손실 기능을 최적화합니다.</target>
        </trans-unit>
        <trans-unit id="a4064d8d27531f23ac21cbcbd5928e344df2525d" translate="yes" xml:space="preserve">
          <source>This model optimizes the squared-loss using LBFGS or stochastic gradient descent.</source>
          <target state="translated">이 모델은 LBFGS 또는 확률 적 경사 하강을 사용하여 제곱 손실을 최적화합니다.</target>
        </trans-unit>
        <trans-unit id="5bcd7dedd8759fb896822dc89b59d5206ebfcc53" translate="yes" xml:space="preserve">
          <source>This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5745ffae87fdf8e03232a3372f515cd928402ef0" translate="yes" xml:space="preserve">
          <source>This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">이 모델은 손실 함수가 선형 최소 제곱 함수이고 정규화가 l2-norm으로 제공되는 회귀 모델을 해결합니다. 릿지 회귀 또는 Tikhonov 정규화라고도합니다. 이 추정기는 다변량 회귀를 기본적으로 지원합니다 (즉, y가 2 차원 배열 [n_samples, n_targets] 인 경우).</target>
        </trans-unit>
        <trans-unit id="f19c01936c0bc27e43d782c2c60b0838b0f4894d" translate="yes" xml:space="preserve">
          <source>This module contains both distance metrics and kernels. A brief summary is given on the two here.</source>
          <target state="translated">이 모듈에는 거리 측정치와 커널이 모두 포함되어 있습니다. 여기에 두 가지 간단한 요약이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="ea7156035377b3d98062532f66578932992ce326" translate="yes" xml:space="preserve">
          <source>This module contains two loaders. The first one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt;, returns a list of the raw texts that can be fed to text feature extractors such as &lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; with custom parameters so as to extract feature vectors. The second one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt;&lt;/a&gt;, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.</source>
          <target state="translated">이 모듈에는 두 개의 로더가 있습니다. 첫 번째 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt; 는 &lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 와 같은 텍스트 피처 추출기에 공급할 수있는 원시 텍스트의 목록을 반환하여 피처 벡터를 추출합니다. 두 번째 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt; &lt;/a&gt; 는 즉시 사용 가능한 기능을 반환합니다. 즉, 기능 추출기를 사용할 필요가 없습니다.</target>
        </trans-unit>
        <trans-unit id="e5dbda685e3f3b6ebc21c265d6368ea8386c5f03" translate="yes" xml:space="preserve">
          <source>This module implements multiclass learning algorithms:</source>
          <target state="translated">이 모듈은 멀티 클래스 학습 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="40fee252ae7de928b5fc80e9416986beafe64ef4" translate="yes" xml:space="preserve">
          <source>This module implements multioutput regression and classification.</source>
          <target state="translated">이 모듈은 다중 출력 회귀 및 분류를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="f4b5a1fcc345642615c5317116147407ee22511b" translate="yes" xml:space="preserve">
          <source>This module offers support for multi-output problems by implementing this strategy in both &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt;&lt;code&gt;DecisionTreeRegressor&lt;/code&gt;&lt;/a&gt;. If a decision tree is fit on an output array Y of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; then the resulting estimator will:</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt; &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; &lt;/a&gt; 모두에서이 전략을 구현하여 다중 출력 문제를 지원합니다 . 결정 트리가 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 크기의 출력 배열 Y에 맞는 경우 결과 추정기는 다음을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="69b4d83c7d3d58178d932db005c229bef475361e" translate="yes" xml:space="preserve">
          <source>This normalization is implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; class:</source>
          <target state="translated">이 정규화는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 클래스에 의해 구현됩니다 .</target>
        </trans-unit>
        <trans-unit id="36bde3d3011eb8df53f7587f509f261b5588364f" translate="yes" xml:space="preserve">
          <source>This object uses workers to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing or concurrent.futures API are (see examples for details):</source>
          <target state="translated">이 객체는 워커를 사용하여 여러 다른 인수에 대한 함수 적용을 병렬로 계산합니다. 원시 멀티 프로세싱 또는 동시 .futures API 사용과 함께 제공되는 주요 기능은 다음과 같습니다 (자세한 내용은 예제 참조).</target>
        </trans-unit>
        <trans-unit id="43f85d31a1122b5ce913b6ac1587dc97b9fac8c9" translate="yes" xml:space="preserve">
          <source>This package also features helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the &amp;lsquo;real world&amp;rsquo;.</source>
          <target state="translated">이 패키지에는 머신 러닝 커뮤니티에서 일반적으로 사용하는 '대규모'데이터에서 알고리즘을 벤치마킹하기 위해 일반적으로 사용하는 더 큰 데이터 세트를 가져 오는 도우미도 있습니다.</target>
        </trans-unit>
        <trans-unit id="8291d2ecf0ae6621cc55dbf9132ec6413177dbb1" translate="yes" xml:space="preserve">
          <source>This parameter does not have any effect. The components are always normalized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f678792533c8ea573ae326139ccc463721df5e43" translate="yes" xml:space="preserve">
          <source>This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19</source>
          <target state="translated">이 매개 변수는 n_components로 이름이 변경되었으며 버전 0.21에서 제거됩니다. .. 사용되지 않음 :: 0.19</target>
        </trans-unit>
        <trans-unit id="b9bd887348c693f73ff73c188c30554ec63931d0" translate="yes" xml:space="preserve">
          <source>This parameter has no effect on the matplotlib tree visualisation and it is kept here for backward compatibility.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95ab9404db52634d7b16f5cf223dc53c5df9617b" translate="yes" xml:space="preserve">
          <source>This parameter has no effect, is deprecated, and will be removed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3803345bcca4c293ec436fd5df6e0af3703aedc" translate="yes" xml:space="preserve">
          <source>This parameter is deprecated and will be removed in v0.24.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2008376a104f1f0d722babab4554d9900556a331" translate="yes" xml:space="preserve">
          <source>This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="4896edc233b2b945e9bfe76cd7c644f9b147f395" translate="yes" xml:space="preserve">
          <source>This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use &lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;&lt;/a&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . True 인 경우 회귀 X는 평균을 빼고 l2-norm으로 나누어 회귀 전에 정규화됩니다. 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 추정기에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt; &lt;/a&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="a954f8a244020c9f424e9822cfad5459a2f5fec4" translate="yes" xml:space="preserve">
          <source>This parameter is ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74de1b86fb436972ff8b47352593daa33d35779f" translate="yes" xml:space="preserve">
          <source>This parameter is not needed to compute tfidf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18c6fb9267c5496a0a4415bd237b294d4f877c13" translate="yes" xml:space="preserve">
          <source>This parameter is required for multiclass/multilabel targets. If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">이 매개 변수는 멀티 클래스 / 멀티 라벨 대상에 필요합니다. 경우 &lt;code&gt;None&lt;/code&gt; , 각 클래스에 대한 점수가 반환됩니다. 그렇지 않으면 데이터에서 수행되는 평균화 유형을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="4b54c5323e385131687ecd8fe6362000ef5ec12b" translate="yes" xml:space="preserve">
          <source>This parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the product \(y_i \alpha_i\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\) :</source>
          <target state="translated">이 파라미터는 부재를 통해 액세스 할 수 &lt;code&gt;dual_coef_&lt;/code&gt; 제품 \ (y_i \ alpha_i \)를 보유하는 &lt;code&gt;support_vectors_&lt;/code&gt; 지지 벡터를 보유하며 &lt;code&gt;intercept_&lt;/code&gt; 독립적 용어 \ 보유 (\ RHO \)</target>
        </trans-unit>
        <trans-unit id="f0e92a41ff311d2df395e780ee2f06d2a63e9cbc" translate="yes" xml:space="preserve">
          <source>This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.</source>
          <target state="translated">이러한 임의의 나무의 숲에서 평균 된이 경로 길이는 정규성과 측정 기능의 척도입니다.</target>
        </trans-unit>
        <trans-unit id="80dec09fc285dd6f9c561fc2931162bad1982cdb" translate="yes" xml:space="preserve">
          <source>This plot compares the decision surfaces learned by a decision tree classifier (first column), by a random forest classifier (second column), by an extra- trees classifier (third column) and by an AdaBoost classifier (fourth column).</source>
          <target state="translated">이 그림은 의사 결정 트리 분류기 (첫 번째 열), 임의 포리스트 분류기 (두 번째 열), 엑스트라 트리 분류기 (세 번째 열) 및 AdaBoost 분류기 (네 번째 열)에 의해 학습 된 결정 표면을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="3b15144cc63de75145bd9b8c8333a27bef49a738" translate="yes" xml:space="preserve">
          <source>This plot is called a Lorenz curve and can be summarized by the Gini index:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e05dc7c1a0ec44a96abb884d1ce621cca93db4e" translate="yes" xml:space="preserve">
          <source>This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. &lt;strong&gt;For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)&lt;/strong&gt;.</source>
          <target state="translated">표본 수가 1,000 개 이상이고 군집 수가 10 개 미만인 경우에는이 문제를 무시해도됩니다. &lt;strong&gt;표본 크기가 작거나 군집 수가 많은 경우 조정 랜드 지수 ( ARI)&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="432c2ac32bcc01f0466fb33da0cfef7067b741ad" translate="yes" xml:space="preserve">
          <source>This problem stems from two limitations of impurity-based feature importances:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcd7e110cbbcbd004685bcd45cf928dd3da1b5b1" translate="yes" xml:space="preserve">
          <source>This procedure (spectral clustering on an image) is an efficient approximate solution for finding normalized graph cuts.</source>
          <target state="translated">이 절차 (이미지의 스펙트럼 클러스터링)는 정규화 된 그래프 컷을 찾기위한 효율적인 근사 솔루션입니다.</target>
        </trans-unit>
        <trans-unit id="f7d25c0cd20cd8cb8daa937c60ac3edd0b2b2f75" translate="yes" xml:space="preserve">
          <source>This ranking metric yields a high value if true labels are ranked high by &lt;code&gt;y_score&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a8651d966c336f363771d222433438f229cb5c2" translate="yes" xml:space="preserve">
          <source>This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.</source>
          <target state="translated">이 회귀는 다른 (진정한) 회귀와 비교하기위한 간단한 기준으로 유용합니다. 실제 문제에는 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="566769fe300350777b617d7ceed39620171814da" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices by passing &lt;code&gt;with_mean=False&lt;/code&gt; to avoid breaking the sparsity structure of the data.</source>
          <target state="translated">이 스케일러는 &lt;code&gt;with_mean=False&lt;/code&gt; 를 전달 하여 데이터의 희소성 구조가 깨지지 않도록 스파 스 CSR 또는 CSC 행렬에도 적용 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bf350412f5695ebe05a62d114269ff308d8edf9f" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices.</source>
          <target state="translated">이 스케일러는 스파 스 CSR 또는 CSC 매트릭스에도 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="096cae15373bbc176ca80052b34794345347f334" translate="yes" xml:space="preserve">
          <source>This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.</source>
          <target state="translated">이 점수는 X에서 테스트 카이 제곱 통계량에 대해 가장 높은 값을 갖는 n_features 피처를 선택하는 데 사용될 수 있습니다. 여기에는 부울 또는 빈도 (예 : 문서 분류의 용어 수)와 같은 음이 아닌 피처 만 포함해야합니다. 클래스.</target>
        </trans-unit>
        <trans-unit id="bc67f7a4c884a57cb8d65e3051862bc296a2adb5" translate="yes" xml:space="preserve">
          <source>This score is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;'arithmetic'&lt;/code&gt; option for averaging.</source>
          <target state="translated">이 점수는 평균화를위한 &lt;code&gt;'arithmetic'&lt;/code&gt; 옵션 이있는 &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="81b377dd4306b3470c7efb10edcaa8d55b57210b" translate="yes" xml:space="preserve">
          <source>This section illustrates the use of a &lt;code&gt;Pipeline&lt;/code&gt; with &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">이 섹션에서는 &lt;code&gt;GridSearchCV&lt;/code&gt; 와 함께 &lt;code&gt;Pipeline&lt;/code&gt; 을 사용하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="929a45e65dc1a0cd0b685d6194be9ae4708eb857" translate="yes" xml:space="preserve">
          <source>This should make it possible to check that the cross-validation score is in the same range as before.</source>
          <target state="translated">이를 통해 교차 검증 점수가 이전과 동일한 범위에 있는지 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08af461e03baea2ad13f22a738ff3dde29c2a50f" translate="yes" xml:space="preserve">
          <source>This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using a Ball Tree built upon the Haversine distance metric &amp;ndash; i.e. distances over points in latitude/longitude. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">다음은 Haversine 거리 측정법에 기반한 볼 트리 (위도 / 경도 점에서의 거리)를 사용하여 지리 공간 데이터에 대한 이웃 기반 쿼리 (특히 커널 밀도 추정)의 예를 보여줍니다. 데이터 세트는 Phillips et. 알. (2006). 가능한 &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;경우이&lt;/a&gt; 예에서는 베이스 맵 을 사용 하여 남미의 해안선과 국가 경계를 그립니다.</target>
        </trans-unit>
        <trans-unit id="d9d5278cf29981ffe0af23942c116e835acd3587" translate="yes" xml:space="preserve">
          <source>This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using a Ball Tree built upon the Haversine distance metric &amp;ndash; i.e. distances over points in latitude/longitude. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38716491ad409e2f991bc1db8f7b1d944921bf99" translate="yes" xml:space="preserve">
          <source>This sort of preprocessing can be streamlined with the &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline&lt;/a&gt; tools. A single object representing a simple polynomial regression can be created and used as follows:</source>
          <target state="translated">이러한 종류의 전처리는 &lt;a href=&quot;compose#pipeline&quot;&gt;파이프 라인&lt;/a&gt; 도구를 사용하여 간소화 할 수 있습니다 . 간단한 다항식 회귀를 나타내는 단일 객체를 다음과 같이 만들고 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4e6050ab2083fb67a848ffe7f83ae292a8f60f62" translate="yes" xml:space="preserve">
          <source>This strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</source>
          <target state="translated">이 전략은 또한 다중 레이블 학습에 사용될 수 있습니다. 예를 들어, 샘플 i에 레이블 j가 있고 0이 아닌 경우 셀 [i, j]가 1 인 2 차원 행렬에 피팅함으로써 분류자가 여러 레이블을 예측하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8cd6a64f79d725ef1cdd342315685305aab51887" translate="yes" xml:space="preserve">
          <source>This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don&amp;rsquo;t scale well with &lt;code&gt;n_samples&lt;/code&gt;. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used &lt;code&gt;n_classes&lt;/code&gt; times.</source>
          <target state="translated">이 전략은 클래스 쌍당 하나의 분류자를 피팅하는 것으로 구성됩니다. 예측 시간에 가장 많은 표를받은 클래스가 선택됩니다. &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; 분류기 에 맞아야하기 때문에이 방법은 일반적으로 O (n_classes ^ 2) 복잡성으로 인해 1 대 1보다 느립니다. 그러나이 방법은 &lt;code&gt;n_samples&lt;/code&gt; 와 함께 잘 확장되지 않는 커널 알고리즘과 같은 알고리즘에 유리할 수 있습니다 . 각 개별 학습 문제는 데이터의 작은 하위 집합 만 포함하고 나머지는 전체 데이터 세트가 &lt;code&gt;n_classes&lt;/code&gt; 시간 동안 사용 되기 때문 입니다.</target>
        </trans-unit>
        <trans-unit id="de640b51f281cc0046c1a9e652c58d4e96ebef0b" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification</source>
          <target state="translated">이 전략은 대상 당 하나의 분류기를 맞추는 것으로 구성됩니다. 이것은 다중 대상 분류를 기본적으로 지원하지 않는 분류기를 확장하기위한 간단한 전략입니다.</target>
        </trans-unit>
        <trans-unit id="7d6fb6c6a7f2b79b31f4627b09bbb93dcc2f3bc4" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one regressor per target. This is a simple strategy for extending regressors that do not natively support multi-target regression.</source>
          <target state="translated">이 전략은 대상 당 하나의 회귀자를 피팅하는 것으로 구성됩니다. 이것은 다중 대상 회귀를 기본적으로 지원하지 않는 회귀 확장을위한 간단한 전략입니다.</target>
        </trans-unit>
        <trans-unit id="27cbaceed22a6f90d5ae07c700825d27bfca1f78" translate="yes" xml:space="preserve">
          <source>This strategy has several advantages:</source>
          <target state="translated">이 전략에는 몇 가지 장점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="91d49a77db9793bc904e6dbf1c0dda029b6c110b" translate="yes" xml:space="preserve">
          <source>This strategy is illustrated below.</source>
          <target state="translated">이 전략은 아래에 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="02849272fb07ed52ea9b00f7ccc02a748c971372" translate="yes" xml:space="preserve">
          <source>This strategy, also known as &lt;strong&gt;one-vs-all&lt;/strong&gt;, is implemented in &lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;OneVsRestClassifier&lt;/code&gt;&lt;/a&gt;. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.</source>
          <target state="translated">&lt;strong&gt;one-vs-all&lt;/strong&gt; 이라고도하는이 전략 은 &lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 에서 구현됩니다 . 전략은 클래스 당 하나의 분류 기준을 맞추는 것으로 구성됩니다. 각 분류 자에 대해 클래스는 다른 모든 클래스에 적합합니다. 계산 효율성 ( &lt;code&gt;n_classes&lt;/code&gt; 분류 자만 필요) 외에도이 접근 방식의 한 가지 장점은 해석 가능성입니다. 각 클래스는 하나의 분류 자로 만 표시되므로 해당 분류자를 검사하여 클래스에 대한 지식을 얻을 수 있습니다. 이것이 가장 일반적으로 사용되는 전략이며 공정한 기본 선택입니다.</target>
        </trans-unit>
        <trans-unit id="27ac94ab878e8b852843daf6fdc6644656d86a0e" translate="yes" xml:space="preserve">
          <source>This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt;). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.</source>
          <target state="translated">이 서브 모듈들이 지원 벡터 기계의 예를 들어 사용으로 특정 커널에 그 해당 기능 매핑을 근사 함수를 포함합니다 ( &lt;a href=&quot;svm#svm&quot;&gt;지원 벡터 기계&lt;/a&gt; ). 다음 기능 함수는 입력의 비선형 변환을 수행하며 선형 분류 또는 기타 알고리즘의 기초가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fcbae9cfcf0bd03c252edbce0ec54b00175fa149" translate="yes" xml:space="preserve">
          <source>This test can be applied to classes or instances. Classes currently have some additional tests that related to construction, while passing instances allows the testing of multiple options.</source>
          <target state="translated">이 테스트는 클래스 또는 인스턴스에 적용 할 수 있습니다. 클래스에는 현재 건설과 관련된 몇 가지 추가 테스트가 있지만 인스턴스를 통과하면 여러 옵션을 테스트 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1df76b4b8062cdd31b59f7b7a96f694c7a4a84f4" translate="yes" xml:space="preserve">
          <source>This test can be applied to classes or instances. Classes currently have some additional tests that related to construction, while passing instances allows the testing of multiple options. However, support for classes is deprecated since version 0.23 and will be removed in version 0.24 (class checks will still be run on the instances).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe4512e0330ee3f9d4d908b10acadfb35dcb4b46" translate="yes" xml:space="preserve">
          <source>This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.</source>
          <target state="translated">이 텍스트 벡터 라이저 구현은 해싱 트릭을 사용하여 정수 인덱스 맵핑을 특징으로하는 토큰 문자열 이름을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="90704510327932a48fb3c52155398163f97475e2" translate="yes" xml:space="preserve">
          <source>This transformation is often used as an alternative to zero mean, unit variance scaling.</source>
          <target state="translated">이 변환은 종종 평균 제로 평균 단위 분산 스케일링의 대안으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6539f7aec79b7dc39bdc0281525ed4d20f3ca8db" translate="yes" xml:space="preserve">
          <source>This transformation will only be exact if n_components=n_features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f720ce11fea82f150f2e1314efc3b6e74d2aa65" translate="yes" xml:space="preserve">
          <source>This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).</source>
          <target state="translated">이 변환기는 밀도가 높은 numpy 배열과 scipy.sparse 매트릭스 모두에서 작동 할 수 있습니다 (복사 / 변환 부담을 피하려면 CSR 형식을 사용하십시오).</target>
        </trans-unit>
        <trans-unit id="22f51ce5936d304512713b7c2d6420a9339ed07c" translate="yes" xml:space="preserve">
          <source>This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.</source>
          <target state="translated">이 변압기는 잘린 특이 값 분해 (SVD)를 통해 선형 차원 축소를 수행합니다. PCA와 달리이 추정기는 특이 값 분해를 계산하기 전에 데이터를 중앙에 두지 않습니다. 즉, scipy.sparse 행렬을 효율적으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="548ea059334e4b91f9c90aef5db25d9def754336" translate="yes" xml:space="preserve">
          <source>This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbf82199a6ba754f7c0185042f3f4e595a8e8525" translate="yes" xml:space="preserve">
          <source>This transformer should be used to encode target values, &lt;em&gt;i.e.&lt;/em&gt;&lt;code&gt;y&lt;/code&gt;, and not the input &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c058bdf39166834e7b3e02bee2a3dc32df1bea5c" translate="yes" xml:space="preserve">
          <source>This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.</source>
          <target state="translated">이 변환기는 기능 이름과 기능 값의 매핑 (dict-like 객체) 목록을 scikit-learn 추정기와 함께 사용하기 위해 Numpy 배열 또는 scipy.sparse 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="6fa17f9065133747f6dee2c04fd6c387874c04ab" translate="yes" xml:space="preserve">
          <source>This tutorial will explore &lt;em&gt;statistical learning&lt;/em&gt;, the use of machine learning techniques with the goal of &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;statistical inference&lt;/a&gt;: drawing conclusions on the data at hand.</source>
          <target state="translated">이 튜토리얼 에서는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;통계적 추론을&lt;/a&gt; 목표로 머신 러닝 기술을 사용하는 &lt;em&gt;통계 학습&lt;/em&gt; , 즉 데이터에 대한 결론을 도출합니다.</target>
        </trans-unit>
        <trans-unit id="d770164eec068c2686e18d9f67f7678a7fe3d2ab" translate="yes" xml:space="preserve">
          <source>This uses the Benjamini-Hochberg procedure. &lt;code&gt;alpha&lt;/code&gt; is an upper bound on the expected false discovery rate.</source>
          <target state="translated">이것은 Benjamini-Hochberg 절차를 사용합니다. &lt;code&gt;alpha&lt;/code&gt; 는 예상되는 잘못된 발견 비율의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="25630de50e6415b67bb72ea47abf6e457ed32d31" translate="yes" xml:space="preserve">
          <source>This uses the score defined by &lt;code&gt;scoring&lt;/code&gt; where provided, and the &lt;code&gt;best_estimator_.score&lt;/code&gt; method otherwise.</source>
          <target state="translated">제공된 경우 &lt;code&gt;scoring&lt;/code&gt; 정의 된 스코어를 사용하고 그렇지 않으면 &lt;code&gt;best_estimator_.score&lt;/code&gt; 메소드를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="28c90b747be44784ef68312c004db0e8049cba2c" translate="yes" xml:space="preserve">
          <source>This utility is documented, but &lt;strong&gt;private&lt;/strong&gt;. This means that backward compatibility might be broken without any deprecation cycle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6f06f080d161d82167224fa4f5455a90c3bc7e3" translate="yes" xml:space="preserve">
          <source>This utility is meant to be used internally by estimators themselves, typically in their own predict / transform methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57e78bad29e459afb6f7e8e9a46e0b5e6e5f4fa9" translate="yes" xml:space="preserve">
          <source>This value is valid if class_weight parameter in fit() is not set.</source>
          <target state="translated">fit ()의 class_weight 매개 변수가 설정되지 않은 경우이 값이 유효합니다.</target>
        </trans-unit>
        <trans-unit id="0973d55bbbd406d7d050b325e278935eae6a368e" translate="yes" xml:space="preserve">
          <source>This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of &amp;ldquo;mutual information&amp;rdquo; between the label assignments.</source>
          <target state="translated">상호 정보 및 정규화 된 변형의이 값은 우연히 조정되지 않으며 레이블 할당 사이의 &quot;상호 정보&quot;의 실제 양에 관계없이 다른 레이블 (클러스터)의 수가 증가함에 따라 증가하는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec27c204380d1bf1bec671104c4e5cc563667983" translate="yes" xml:space="preserve">
          <source>This visualization is an example of a &lt;em&gt;kernel density estimation&lt;/em&gt;, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.</source>
          <target state="translated">이 시각화는 &lt;em&gt;커널 밀도 추정&lt;/em&gt; 의 예이며 ,이 경우에는 탑햇 커널 (즉, 각 지점에서 정사각형 블록)을 사용합니다. 보다 부드러운 커널을 사용하여보다 원활한 배포를 복구 할 수 있습니다. 오른쪽 아래 그림은 가우스 커널 밀도 추정값을 보여줍니다. 각 포인트는 가우스 곡선을 총계에 기여합니다. 결과는 데이터에서 도출 된 부드러운 밀도 추정치이며 점 분포의 강력한 비모수 적 모델로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="1cad85e71e9b226b43b5778c8058de4fe70516a7" translate="yes" xml:space="preserve">
          <source>This warning is used to notify the user that BLAS was not used for dot operation and hence the efficiency may be affected.</source>
          <target state="translated">이 경고는 BLAS가 도트 작업에 사용되지 않았으므로 효율성에 영향을 줄 수 있음을 사용자에게 알리는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7b4c8162b5298ba9d922a2200274b38ddddf44e8" translate="yes" xml:space="preserve">
          <source>This warning notifies the user that the efficiency may not be optimal due to some reason which may be included as a part of the warning message. This may be subclassed into a more specific Warning class.</source>
          <target state="translated">이 경고는 경고 메시지의 일부로 포함될 수있는 어떤 이유로 인해 효율성이 최적이 아닐 수 있음을 사용자에게 알립니다. 이것은보다 구체적인 경고 클래스로 서브 클래 싱 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec79da6e5e29f4afd0662e82ec298c39ed6dbabd" translate="yes" xml:space="preserve">
          <source>This warning occurs when some input data needs to be converted or interpreted in a way that may not match the user&amp;rsquo;s expectations.</source>
          <target state="translated">이 경고는 일부 입력 데이터를 사용자의 기대에 맞지 않는 방식으로 변환하거나 해석해야 할 때 발생합니다.</target>
        </trans-unit>
        <trans-unit id="14994b75958434504d6803fa4be46a86d6219fc9" translate="yes" xml:space="preserve">
          <source>This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.</source>
          <target state="translated">이는 원래 문서 분류 및 클러스터링에서 유용하게 사용되는 정보 검색 (검색 엔진 결과의 순위 함수로)을 위해 개발 된 가중치 체계입니다.</target>
        </trans-unit>
        <trans-unit id="90d00e9f85af52e63288d2fca3d9f513dce9de12" translate="yes" xml:space="preserve">
          <source>This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.</source>
          <target state="translated">그러나 모집단 공분산이 항등 행렬의 배수 인 경우 Ledoit-Wolf 절차에서는 그렇지 않습니다. 이 경우, 샘플 수가 증가함에 따라 Ledoit-Wolf 수축 추정치는 1에 가까워집니다. 이것은 Ledoit-Wolf 의미에서 공분산 행렬의 최적 추정치가 동일성의 배수임을 나타냅니다. 모집단 공분산은 이미 항등 행렬의 배수이므로 Ledoit-Wolf 솔루션은 실제로 합리적인 추정치입니다.</target>
        </trans-unit>
        <trans-unit id="e911226999d28ae4c4eb95cef049955b008548cf" translate="yes" xml:space="preserve">
          <source>Those 3 metrics are independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score values in any way.</source>
          <target state="translated">이 3 가지 메트릭은 레이블의 절대 값과 무관합니다. 클래스 또는 클러스터 레이블 값의 순열은 점수 값을 변경하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a17151aff3f6e79d6bfb3bc3e7c5d30ff3b77b7d" translate="yes" xml:space="preserve">
          <source>Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given the knowledge of a Ground Truth class labels of the same samples.</source>
          <target state="translated">이러한 메트릭은 클러스터링 레이블링의 정규화 된 조건부 엔트로피 측정 값을 기반으로하여 동일한 샘플의 Ground Truth 클래스 레이블에 대한 지식을 평가합니다.</target>
        </trans-unit>
        <trans-unit id="831e093286e91d34e1415d38600e7c8277f14a07" translate="yes" xml:space="preserve">
          <source>Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'ltsa'&lt;/code&gt;.</source>
          <target state="translated">기술적으로 LLE의 변형은 아니지만 LTSA (Local tangent space alignment)는 LLE과 알고리즘 적으로 유사하여이 범주에 넣을 수 있습니다. LTSA는 LLE에서와 같이 이웃 거리를 유지하는 데 초점을 맞추기보다는 접선 공간을 통해 각 이웃의 로컬 지오메트리를 특성화하고 이러한 접선 공간을 정렬하기 위해 전역 최적화를 수행하여 임베딩을 학습합니다. LTSA는 함수로 수행 될 수 &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 또는 객체 지향 상대 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 키워드와 &lt;code&gt;method = 'ltsa'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3dcfc8b5bdf930c1b66451c5dc30f486901100ee" translate="yes" xml:space="preserve">
          <source>Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable.</source>
          <target state="translated">아래에는 세 가지 유형의 SVM 커널이 표시되어 있습니다. 다항식과 RBF는 데이터 포인트를 선형으로 분리 할 수 ​​없을 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="2eb0d5d5e8d716a06a5bc42a652c1781cf721343" translate="yes" xml:space="preserve">
          <source>Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.</source>
          <target state="translated">샘플 기능의 이진화 (부울 맵핑)에 대한 임계 값입니다. None이면 입력은 이미 이진 벡터로 구성된 것으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="ac359cd376aaf3163dffdc564a92f8f55ebdbfe9" translate="yes" xml:space="preserve">
          <source>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</source>
          <target state="translated">수목의 조기 정지에 대한 임계 값. 불순물이 임계 값을 초과하면 노드가 분리되고, 그렇지 않으면 잎입니다.</target>
        </trans-unit>
        <trans-unit id="d2dde1e4fd07fa9e4ff99bf50a843f5c394281b4" translate="yes" xml:space="preserve">
          <source>Threshold for shrinking centroids to remove features.</source>
          <target state="translated">중심점을 축소하여 피처를 제거하는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="5b50eca69565a6240c9cb586697767c09ac4525e" translate="yes" xml:space="preserve">
          <source>Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Can be an int in Bytes, or a human-readable string, e.g., &amp;lsquo;1M&amp;rsquo; for 1 megabyte. Use None to disable memmapping of large arrays. Only active when backend=&amp;rdquo;loky&amp;rdquo; or &amp;ldquo;multiprocessing&amp;rdquo;.</source>
          <target state="translated">temp_folder에서 자동화 된 메모리 매핑을 트리거하는 작업자에게 전달되는 배열 크기의 임계 값입니다. 바이트 단위의 int 또는 사람이 읽을 수있는 문자열 (예 : 1MB의 경우 '1M') 일 수 있습니다. 대형 배열의 emma 핑을 비활성화하려면 없음을 사용하십시오. 백엔드 =&amp;rdquo;loky&amp;rdquo;또는&amp;ldquo;multiprocessing&amp;rdquo;인 경우에만 활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="3bf722c4ec04176f091be4d50fbd629d5b754a20" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation in SVD solver.</source>
          <target state="translated">SVD 솔버에서 순위 추정에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="0168a115989469a76c56e8c46c0d56b1a01f88c6" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation.</source>
          <target state="translated">순위 추정에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="558232b0add0e7cf1e4001c15b7a509781ecfb59" translate="yes" xml:space="preserve">
          <source>Threshold used in the binary and multi-label cases.</source>
          <target state="translated">이진 및 다중 레이블 경우에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="d260a173cc06214ee3d2352996ea165371c17c29" translate="yes" xml:space="preserve">
          <source>Thresholding</source>
          <target state="translated">Thresholding</target>
        </trans-unit>
        <trans-unit id="8265a18b28c2cb3c5a28ceb45384d9a49c2f7715" translate="yes" xml:space="preserve">
          <source>Thresholding is clearly not useful for denoising, but it is here to show that it can produce a suggestive output with very high speed, and thus be useful for other tasks such as object classification, where performance is not necessarily related to visualisation.</source>
          <target state="translated">임계 값은 노이즈 제거에는 분명히 유용하지 않지만 매우 빠른 속도로 암시적인 출력을 생성 할 수 있으므로 성능과 시각화가 반드시 관련이있는 것은 아닌 객체 분류와 같은 다른 작업에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="c4d29a75003891e7d5c5dbb3dea7166bf19f4ab9" translate="yes" xml:space="preserve">
          <source>Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction.</source>
          <target state="translated">임계 값은 매우 빠르지 만 정확한 재구성을 제공하지는 않습니다. 그것들은 분류 작업에 대한 문헌에서 유용한 것으로 나타났습니다. 이미지 재구성 작업의 경우 직교 매칭 추구는 가장 정확하고 편견없는 재구성을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="3904c870d9e800cc53a98ecb8acef59d010fad3d" translate="yes" xml:space="preserve">
          <source>Throw a ValueError if X contains NaN or infinity.</source>
          <target state="translated">X에 NaN 또는 무한대가 포함 된 경우 ValueError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="8b8612c016401dc529cb09be5ddd6996fe872d9c" translate="yes" xml:space="preserve">
          <source>Thus in binary classification, the count of true negatives is \(C_{0,0}\), false negatives is \(C_{1,0}\), true positives is \(C_{1,1}\) and false positives is \(C_{0,1}\).</source>
          <target state="translated">따라서 이진 분류에서 실제 음수의 개수는 \ (C_ {0,0} \)이고, 위 음수는 \ (C_ {1,0} \)이며, 양수는 \ (C_ {1,1} \)입니다. 오 탐지는 \ (C_ {0,1} \)입니다.</target>
        </trans-unit>
        <trans-unit id="877864e25b035038afd6bbe5a72ca90fb8e0741e" translate="yes" xml:space="preserve">
          <source>Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input&amp;rsquo;s minimum and maximum &amp;mdash; corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively &amp;mdash; do not become infinite under the transformation.</source>
          <target state="translated">따라서 입력의 중앙값은 0을 중심으로하는 출력의 평균이됩니다. 일반 출력은 1e-7 및 1-1e-7 Quantile에 각각 해당하는 입력의 최소값과 최대 값이 무한히되지 않도록 클리핑됩니다. 변형.</target>
        </trans-unit>
        <trans-unit id="911ea2c24698b41ad1167139365e2f911ee7efcc" translate="yes" xml:space="preserve">
          <source>Thus, among the considered estimators, &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; are a-priori better suited for modeling the long tail distribution of the non-negative data as compared to the &lt;code&gt;Ridge&lt;/code&gt; model which makes a wrong assumption on the distribution of the target variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0808b4cdf67452766c8c5389635c6458f9990f5b" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015</source>
          <target state="translated">따라서 대부분의 목표 신호 (34.4ppm)는 장기 상승 추세 (길이 규모 41.8 년)로 설명됩니다. 주기 성분은 3.27ppm의 진폭, 180 년의 붕괴 시간 및 1.44의 길이 스케일을가집니다. 붕괴 시간이 길다는 것은 계절적으로 매우 가까운 계절적 구성 요소가 있음을 나타냅니다. 상관 노이즈는 0.138 년의 길이 스케일과 0.197ppm의 화이트 노이즈 기여로 0.197ppm의 진폭을가집니다. 따라서 전체 노이즈 레벨이 매우 작아서 모델이 데이터를 잘 설명 할 수 있음을 나타냅니다. 또한이 모델은 2015 년 경까지 모델이 매우 자신있게 예측할 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="4ab3c0245825ab663f7197647adadf073e4b3e64" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015.</source>
          <target state="translated">따라서 대부분의 목표 신호 (34.4ppm)는 장기 상승 추세 (길이 규모 41.8 년)로 설명됩니다. 주기 성분은 3.27ppm의 진폭, 180 년의 붕괴 시간 및 1.44의 길이 스케일을가집니다. 붕괴 시간이 길다는 것은 계절적으로 매우 가까운 계절적 구성 요소가 있음을 나타냅니다. 상관 잡음은 0.138ppm의 길이 스케일과 0.197ppm의 화이트 노이즈 기여로 0.197ppm의 진폭을가집니다. 따라서 전체 노이즈 레벨이 매우 작아서 모델이 데이터를 잘 설명 할 수 있음을 나타냅니다. 또한이 모델은 2015 년까지 모델이 매우 자신있게 예측할 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="af16f18f91308907d1dd8226e54112fa0bd29044" translate="yes" xml:space="preserve">
          <source>Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. &lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</source>
          <target state="translated">Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH : 대규모 데이터베이스를위한 효율적인 데이터 클러스터링 방법. &lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d89b88657fcb13ceb20c877ea478d701717a393" translate="yes" xml:space="preserve">
          <source>Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. &lt;a href=&quot;https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="867d68dbf4c080ab9e706507919b510dbb556be3" translate="yes" xml:space="preserve">
          <source>Tianqi Chen, Carlos Guestrin, &lt;a href=&quot;https://arxiv.org/abs/1603.02754&quot;&gt;&amp;ldquo;XGBoost: A Scalable Tree Boosting System&amp;rdquo;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d53cad37906f55db18b858cf86bfeef9ad9688eb" translate="yes" xml:space="preserve">
          <source>Tibshirani, R., Hastie, T., Narasimhan, B., &amp;amp; Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences.</source>
          <target state="translated">Tibshirani, R., Hastie, T., Narasimhan, B., &amp;amp; Chu, G. (2002). 유전자 발현의 중심을 줄임으로써 다중 암 유형의 진단. 미국 국립 과학원 (National Academy of Sciences), 99 (10), 6567-6572의 절차. 국립 과학 아카데미.</target>
        </trans-unit>
        <trans-unit id="54643cfd9d395af8d03ef9fab4df1a2cdb437e0c" translate="yes" xml:space="preserve">
          <source>Tie breaking is costly if &lt;code&gt;decision_function_shape='ovr'&lt;/code&gt;, and therefore it is not enabled by default. This example illustrates the effect of the &lt;code&gt;break_ties&lt;/code&gt; parameter for a multiclass classification problem and &lt;code&gt;decision_function_shape='ovr'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a297f524f28779281bb4e53d7b6af672dcac3672" translate="yes" xml:space="preserve">
          <source>Ties are broken using the secondary method from Leeuw, 1977.</source>
          <target state="translated">1977 년 Leeuw의 2 차 방법을 사용하여 연결을 끊습니다.</target>
        </trans-unit>
        <trans-unit id="59976e05663a4d82c80a3273030c2c2f87094f4d" translate="yes" xml:space="preserve">
          <source>Ties between features with equal scores will be broken in an unspecified way.</source>
          <target state="translated">동일한 점수를 가진 지형지 물 간의 연계는 지정되지 않은 방식으로 중단됩니다.</target>
        </trans-unit>
        <trans-unit id="c41dd9e78b42392c90f4c6ddfb54f7863f5482f1" translate="yes" xml:space="preserve">
          <source>Ties in &lt;code&gt;y_scores&lt;/code&gt; are broken by giving maximal rank that would have been assigned to all tied values.</source>
          <target state="translated">&lt;code&gt;y_scores&lt;/code&gt; 의 동점은 모든 묶인 값에 지정된 최대 순위를 지정하여 끊어집니다.</target>
        </trans-unit>
        <trans-unit id="ba73dffe02601a1abd345b6200b276334877401b" translate="yes" xml:space="preserve">
          <source>Time Series cross-validator</source>
          <target state="translated">시계열 교차 유효성 검사기</target>
        </trans-unit>
        <trans-unit id="bbad16d201e3f82cae87bba42e6286ebcef9d190" translate="yes" xml:space="preserve">
          <source>Time series data is characterised by the correlation between observations that are near in time (&lt;em&gt;autocorrelation&lt;/em&gt;). However, classical cross-validation techniques such as &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the &amp;ldquo;future&amp;rdquo; observations least like those that are used to train the model. To achieve this, one solution is provided by &lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt;&lt;code&gt;TimeSeriesSplit&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">시계열 데이터는 거의 시간에 가까운 관측치 간의 상관 관계를 특징으로합니다 (자가 &lt;em&gt;상관&lt;/em&gt; ). 그러나, 클래식 교차 검증 기술 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 이&lt;/a&gt; 샘플은 독립적이고 동일하게 분포하고, 교육 및 테스트 인스턴스 간의 불합리한 상관 관계를 초래 가정 시계열 데이터에 (일반화 오류의 가난한 추정치를 산출). 따라서 모델 학습에 사용되는 것과 거의 같은 &quot;미래&quot;관측치에 대한 시계열 데이터의 모델을 평가하는 것이 매우 중요합니다. 이를 위해 &lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt; &lt;code&gt;TimeSeriesSplit&lt;/code&gt; &lt;/a&gt; 에서 하나의 솔루션을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="a6268d75d578276d37dec9fce6dea804677e6b49" translate="yes" xml:space="preserve">
          <source>Timeout limit for each task to complete. If any task takes longer a TimeOutError will be raised. Only applied when n_jobs != 1</source>
          <target state="translated">완료 할 각 작업의 시간 초과 제한. 작업이 오래 걸리면 TimeOutError가 발생합니다. n_jobs! = 1 인 경우에만 적용</target>
        </trans-unit>
        <trans-unit id="f98ee87f52adb2c6f3aaf1f01bab51d0b9ae3622" translate="yes" xml:space="preserve">
          <source>Times spent for fitting in seconds. Only present if &lt;code&gt;return_times&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c46532137bb2ae8358c0137ca60928cd3434340" translate="yes" xml:space="preserve">
          <source>Times spent for scoring in seconds. Only present if &lt;code&gt;return_times&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22c6faf6f7a1dbffed43da8c3c0a736a2f22b862" translate="yes" xml:space="preserve">
          <source>Timing and accuracy plots</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="834cbd0fedba36c3380f74ce91ef668820820b53" translate="yes" xml:space="preserve">
          <source>To achieve better accuracy, &lt;code&gt;X_norm_squared&lt;/code&gt; and &lt;code&gt;Y_norm_squared&lt;/code&gt; may be unused if they are passed as &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8879146d32620a1e603bf26a188c9426e79a5ed0" translate="yes" xml:space="preserve">
          <source>To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \(A\) is very distant from point \(B\), and point \(B\) is very close to point \(C\), then we know that points \(A\) and \(C\) are very distant, &lt;em&gt;without having to explicitly calculate their distance&lt;/em&gt;. In this way, the computational cost of a nearest neighbors search can be reduced to \(O[D N \log(N)]\) or better. This is a significant improvement over brute-force for large \(N\).</source>
          <target state="translated">무차별 대입 방식의 계산 비 효율성을 해결하기 위해 다양한 트리 기반 데이터 구조가 발명되었습니다. 일반적으로 이러한 구조는 샘플에 대한 집계 거리 정보를 효율적으로 인코딩하여 필요한 거리 계산 횟수를 줄입니다. 기본 아이디어는 \ (A \) 지점이 \ (B \) 지점에서 멀리 떨어져 있고 \ (B \) 지점이 \ (C \) 지점에 매우 가까운 경우 \ (A \ ) 및 \ (C \)는 &lt;em&gt;거리를 명시 적으로 계산할 필요없이&lt;/em&gt; 매우 먼 &lt;em&gt;거리에 있습니다&lt;/em&gt; . 이러한 방식으로, 가장 가까운 이웃 검색의 계산 비용을 \ (O [DN \ log (N)] \) 이상으로 줄일 수 있습니다. 이것은 큰 \ (N \)에 대한 무차별 대입보다 훨씬 개선 된 것입니다.</target>
        </trans-unit>
        <trans-unit id="81ec528524d941df99755c9bb7fceaf80c6a8752" translate="yes" xml:space="preserve">
          <source>To address the inefficiencies of KD Trees in higher dimensions, the &lt;em&gt;ball tree&lt;/em&gt; data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.</source>
          <target state="translated">더 큰 차원에서 KD 트리의 비 효율성을 해결하기 위해 &lt;em&gt;볼 트리&lt;/em&gt; 데이터 구조가 개발되었습니다. KD 트리가 데카르트 축을 따라 데이터를 분할하는 경우, 볼 트리는 일련의 중첩 하이퍼 스피어에서 데이터를 분할합니다. 이로 인해 트리 구성이 KD 트리보다 비용이 많이 들지만 데이터 구조는 결과적으로 매우 높은 차원에서도 매우 구조화 된 데이터에서 매우 효율적일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a11d5f0b5df4ea3ced24dc7521fb6d9f97740ba3" translate="yes" xml:space="preserve">
          <source>To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an &amp;ldquo;interesting&amp;rdquo; linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data.</source>
          <target state="translated">이 문제를 해결하기 위해 주성분 분석 (PCA), 독립 성분 분석, 선형 판별 분석 등의 여러 감독 및 감독되지 않은 선형 차원 축소 프레임 워크가 설계되었습니다. 이 알고리즘은 데이터의 &quot;관심있는&quot;선형 투영을 선택하기 위해 특정 루 브릭을 정의합니다. 이러한 방법은 강력 할 수 있지만 종종 데이터에서 중요한 비선형 구조를 놓치게됩니다.</target>
        </trans-unit>
        <trans-unit id="c134b5f4c4fa3b034f915a1c4077d9f58401c669" translate="yes" xml:space="preserve">
          <source>To address this issue you can use &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;whiten=True&lt;/code&gt; to further remove the linear correlation across features.</source>
          <target state="translated">이 문제를 해결하기 위해 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt; &lt;/a&gt; 를 whiten &lt;code&gt;whiten=True&lt;/code&gt; 와 함께 사용하여 피처 의 선형 상관 관계를 추가로 제거 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5dfb268e42cc748904256c70f6b80b2da01edce9" translate="yes" xml:space="preserve">
          <source>To also transform a test set \(X\), we multiply it with \(V_k\):</source>
          <target state="translated">테스트 세트 \ (X \)도 변환하려면 \ (V_k \)를 곱하십시오.</target>
        </trans-unit>
        <trans-unit id="6e914d8189fa250ac9b4b7ea3cf2e62431cbcccd" translate="yes" xml:space="preserve">
          <source>To apply an classifier on this data, we need to flatten the image, to turn the data in a (samples, feature) matrix:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ef7600ab8e39fc13b7dc9585804325c8072844d" translate="yes" xml:space="preserve">
          <source>To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge regression) via the &lt;code&gt;ridge_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">시스템이 결정되지 않은 경우 불안정성 문제를 피하기 위해 &lt;code&gt;ridge_alpha&lt;/code&gt; 매개 변수 를 통해 정규화를 적용 할 수 있습니다 (Ridge Regression) .</target>
        </trans-unit>
        <trans-unit id="622754a0a375aafa66f9d8336ffd00fcfc0a1948" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSC matrix.</source>
          <target state="translated">메모리 복사를 피하려면 호출자가 CSC 매트릭스를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="21a05d95ccb73f51d245c302b02e9a8f32df0276" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSR matrix.</source>
          <target state="translated">메모리 복사를 피하려면 호출자가 CSR 매트릭스를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="e5ab0a4f687079cc593610e8d8c0f15b79824d4d" translate="yes" xml:space="preserve">
          <source>To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.</source>
          <target state="translated">메모리 재 할당을 피하려면 해당 형식을 사용하여 메모리에 초기 데이터를 직접 할당하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="ee93c7e2ac06e08c1567b0ca209ad480ea5f1b80" translate="yes" xml:space="preserve">
          <source>To avoid the computation of global clustering, for every call of &lt;code&gt;partial_fit&lt;/code&gt; the user is advised</source>
          <target state="translated">글로벌 클러스터링 계산을 피하기 위해 &lt;code&gt;partial_fit&lt;/code&gt; 을 호출 할 때마다 사용자에게 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="e81cbf7353acbc69eeae43ca8cf143e58e658d10" translate="yes" xml:space="preserve">
          <source>To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called &lt;code&gt;tf&lt;/code&gt; for Term Frequencies.</source>
          <target state="translated">이러한 잠재적 불일치를 피하려면 문서에서 각 단어의 발생 횟수를 문서의 총 단어 수로 나누면 충분합니다. 이러한 새로운 기능 을 용어 빈도의 경우 &lt;code&gt;tf&lt;/code&gt; 라고 합니다.</target>
        </trans-unit>
        <trans-unit id="39f01dfdcdf5847fd1935ba52ba9be2bfc80430b" translate="yes" xml:space="preserve">
          <source>To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop (here executed by &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;), generalization error is estimated by averaging test set scores over several dataset splits.</source>
          <target state="translated">이 문제를 피하기 위해 중첩 된 CV는 일련의 기차 / 검증 / 테스트 세트 스플릿을 효과적으로 사용합니다. 내부 루프 (여기서는 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 에&lt;/a&gt; 의해 실행 됨 )에서, 각 트레이닝 세트에 모델을 맞추면 점수가 대략 최대화 된 다음 검증 세트에서 (하이퍼) 파라미터를 선택하여 직접 최대화됩니다. 외부 루프 (여기에서 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; )에서 일반화 오류는 여러 데이터 세트 분할에 대한 테스트 세트 점수를 평균하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="5ed52a0ba64199519b793ee9dad54a31d6d3eaed" translate="yes" xml:space="preserve">
          <source>To avoid unnecessary memory duplication the X and y arguments of the fit method should be directly passed as Fortran-contiguous numpy arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a37b39aad5fcf98e98548e781cdec5193cfe7b97" translate="yes" xml:space="preserve">
          <source>To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.</source>
          <target state="translated">불필요한 메모리 중복을 피하려면 fit 메소드의 X 인수를 Fortran-contiguous numpy 배열로 직접 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="d9c2f7485084c926a2f68d8587d615406cc01649" translate="yes" xml:space="preserve">
          <source>To be in favorable recovery conditions, we sample the data from a model with a sparse inverse covariance matrix. In addition, we ensure that the data is not too much correlated (limiting the largest coefficient of the precision matrix) and that there a no small coefficients in the precision matrix that cannot be recovered. In addition, with a small number of observations, it is easier to recover a correlation matrix rather than a covariance, thus we scale the time series.</source>
          <target state="translated">유리한 회복 조건에 있도록 희소 역공 분산 행렬을 사용하여 모델에서 데이터를 샘플링합니다. 또한, 데이터가 너무 상관 관계가없고 (정밀도 행렬의 최대 계수 제한) 정밀도 행렬에 복구 할 수없는 작은 계수가 없는지 확인합니다. 또한, 관측치가 적 으면 공분산보다는 상관 행렬을 복구하는 것이 더 쉬워 시계열의 크기를 조정합니다.</target>
        </trans-unit>
        <trans-unit id="f7fd313aae703eaa110952d34fbc2e74f81a873c" translate="yes" xml:space="preserve">
          <source>To be removed in 0.21</source>
          <target state="translated">0.21에서 제거</target>
        </trans-unit>
        <trans-unit id="b656a9f4366f6cbcc5b1e6914e7bc1a8d099ee57" translate="yes" xml:space="preserve">
          <source>To be removed in 0.22</source>
          <target state="translated">0.22에서 제거</target>
        </trans-unit>
        <trans-unit id="b622879f90e0f79392a411b5be4ae9945d5e85aa" translate="yes" xml:space="preserve">
          <source>To be removed in 0.24</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc387423485c5a73576ae6f9089ec34a8b143ae6" translate="yes" xml:space="preserve">
          <source>To begin with, all values for \(r\) and \(a\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \(\lambda\) is introduced to iteration process:</source>
          <target state="translated">우선, \ (r \) 및 \ (a \)의 모든 값은 0으로 설정되며 각 계산은 수렴 될 때까지 반복됩니다. 위에서 설명한 것처럼, 메시지를 업데이트 할 때 수치 진동을 피하기 위해 감쇠 프로세스 \ (\ lambda \)가 반복 프로세스에 도입되었습니다.</target>
        </trans-unit>
        <trans-unit id="ebc5cb56aa5d3da850d595b902c1384fa4142906" translate="yes" xml:space="preserve">
          <source>To begin, we&amp;rsquo;ll visualize our data.</source>
          <target state="translated">먼저 데이터를 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="57e47e513e200b11a216f9768279c1f81e7b3157" translate="yes" xml:space="preserve">
          <source>To benchmark different estimators for your case you can simply change the &lt;code&gt;n_features&lt;/code&gt; parameter in this example: &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt;. This should give you an estimate of the order of magnitude of the prediction latency.</source>
          <target state="translated">사례에 대해 다른 추정량을 벤치마킹하려면 이 예에서 &lt;code&gt;n_features&lt;/code&gt; 매개 변수를 변경하면 됩니다 : &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;예측 지연 시간&lt;/a&gt; . 이를 통해 예측 대기 시간의 크기 순서를 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="892d831a9e807296347eacb2e8474830ca349663" translate="yes" xml:space="preserve">
          <source>To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.</source>
          <target state="translated">발견 된 biclusters 세트와 실제 biclusters 세트를 비교하려면 두 개의 유사성 측정이 필요합니다. 개별 biclusters에 대한 유사성 측정과 이러한 개별 유사성을 전체 점수로 결합하는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="f0ff37a06cd777b22ebe208ab3110388f720b201" translate="yes" xml:space="preserve">
          <source>To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:</source>
          <target state="translated">개별 biclusters를 비교하기 위해 몇 가지 측정이 사용되었습니다. 현재 Jaccard 인덱스 만 구현되었습니다.</target>
        </trans-unit>
        <trans-unit id="658de85a569a63b4d478720bcfaf7adeb72fbb36" translate="yes" xml:space="preserve">
          <source>To compare the 3 models from this perspective, one can plot the cumulative proportion of claims vs the cumulative proportion of exposure for the test samples order by the model predictions, from safest to riskiest according to each model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30a2aa60dabe3d1d8b8497c6228442c6c55454f4" translate="yes" xml:space="preserve">
          <source>To control display of warnings.</source>
          <target state="translated">경고 표시를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="6c3d05eecff544d238db6888c87daeb42794f44b" translate="yes" xml:space="preserve">
          <source>To control the verbosity of the procedure.</source>
          <target state="translated">절차의 상세 정도를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="d66f891ca7bde7537002ad52d27fc9dd62dd5881" translate="yes" xml:space="preserve">
          <source>To convert categorical features to such integer codes, we can use the &lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):</source>
          <target state="translated">범주 형 기능을 이러한 정수 코드로 변환하기 위해 &lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; 를&lt;/a&gt; 사용할 수 있습니다 . 이 추정기는 각 범주 형 피쳐를 정수의 새로운 피쳐 (0-n_categories-1)로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="c4d6b75ae9bd1ab8a2aa45db1ede3ed88ee6cb4c" translate="yes" xml:space="preserve">
          <source>To correct this, the list of labels should be passed in as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="693e8d8ca1982fe1e279b5869b2b710976d06558" translate="yes" xml:space="preserve">
          <source>To counter this effect we can discount the expected RI \(E[\text{RI}]\) of random labelings by defining the adjusted Rand index as follows:</source>
          <target state="translated">이 효과를 방지하기 위해 다음과 같이 조정 된 랜드 인덱스를 정의하여 임의 레이블의 예상 RI \ (E [\ text {RI}] \)를 할인 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2ccfac714af4138a2df70ede11b2ff4e1963a414" translate="yes" xml:space="preserve">
          <source>To create positive examples click the left mouse button; to create negative examples click the right button.</source>
          <target state="translated">긍정적 인 예를 만들려면 마우스 왼쪽 버튼을 클릭하십시오. 부정적인 예를 만들려면 오른쪽 버튼을 클릭하십시오.</target>
        </trans-unit>
        <trans-unit id="3c7bf94a5fb077c325503613ed6e46ecc0fdb413" translate="yes" xml:space="preserve">
          <source>To decide on the importance of the features we are going to use LassoCV estimator. The features with the highest absolute &lt;code&gt;coef_&lt;/code&gt; value are considered the most important</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91f3b8c70c19596fa3422a68b56ef8a0e44f9e91" translate="yes" xml:space="preserve">
          <source>To describe the dataset as a linear model we use a ridge regressor with a very small regularization and to model the logarithm of the WAGE.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a890a65f7673c36b72863dcfff2db0d979bb71bc" translate="yes" xml:space="preserve">
          <source>To design our machine-learning pipeline, we first manually check the type of data that we are dealing with:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0bfc36f728f01d8f998e7e774b5cc731a5652d7" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on inertia, set max_no_improvement to None.</source>
          <target state="translated">관성을 기준으로 수렴 감지를 비활성화하려면 max_no_improvement를 None으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="644ea86209186f0b63818c18611416bf68aa348b" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on normalized center change, set tol to 0.0 (default).</source>
          <target state="translated">정규화 된 중심 변경을 기반으로 수렴 감지를 비활성화하려면 l을 0.0 (기본값)으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="8f49411326bd4f684fb56ae33f90d4fd9150ab8c" translate="yes" xml:space="preserve">
          <source>To do the exercises, copy the content of the &amp;lsquo;skeletons&amp;rsquo; folder as a new folder named &amp;lsquo;workspace&amp;rsquo;:</source>
          <target state="translated">연습을하려면 'skeletons'폴더의 내용을 'workspace'라는 새 폴더로 복사하십시오.</target>
        </trans-unit>
        <trans-unit id="79cf44a84fa8878b10f291a31335b47430451015" translate="yes" xml:space="preserve">
          <source>To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:</source>
          <target state="translated">각 열에 전처리 또는 특정 기능 추출 방법과 같은 다른 변환을 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2e998c39435525bfb05b6223ee051590414cf95b" translate="yes" xml:space="preserve">
          <source>To ensure that estimators yield reasonable predictions for different policyholder types, we can bin test samples according to &lt;code&gt;y_pred&lt;/code&gt; returned by each model. Then for each bin, we compare the mean predicted &lt;code&gt;y_pred&lt;/code&gt;, with the mean observed target:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0a5ce85df1e1aafd2ebf61b7efd7098beb62d7b" translate="yes" xml:space="preserve">
          <source>To estimate a probabilistic model (e.g. a Gaussian model), estimating the precision matrix, that is the inverse covariance matrix, is as important as estimating the covariance matrix. Indeed a Gaussian model is parametrized by the precision matrix.</source>
          <target state="translated">확률 적 모델 (예 : 가우시안 모델)을 추정하려면 역 공분산 행렬 인 정밀 행렬을 추정하는 것이 공분산 행렬을 추정하는 것만 큼 중요합니다. 실제로 가우스 모델은 정밀 행렬로 매개 변수화됩니다.</target>
        </trans-unit>
        <trans-unit id="06985e50b51113b200d13cecad3eedd2a07fa798" translate="yes" xml:space="preserve">
          <source>To evaluate the impact of the scale of the dataset (&lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_features&lt;/code&gt;) while controlling the statistical properties of the data (typically the correlation and informativeness of the features), it is also possible to generate synthetic data.</source>
          <target state="translated">데이터 의 통계적 속성 (일반적으로 피처의 상관 관계 및 정보)을 제어하면서 데이터 세트의 스케일 ( &lt;code&gt;n_samples&lt;/code&gt; 및 &lt;code&gt;n_features&lt;/code&gt; ) 의 영향을 평가하기 위해 합성 데이터를 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="fa6d485f0cac2ad780f34f2a0f500816dbb434b1" translate="yes" xml:space="preserve">
          <source>To evaluate the pertinence of the used metrics, we will consider as a baseline a &amp;ldquo;dummy&amp;rdquo; estimator that constantly predicts the mean frequency of the training sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b81da86f6a51388bf88e8748866dfbc1da10ddb" translate="yes" xml:space="preserve">
          <source>To fully specify a dataset, you need to provide a name and a version, though the version is optional, see &lt;a href=&quot;#openml-versions&quot;&gt;Dataset Versions&lt;/a&gt; below. The dataset contains a total of 1080 examples belonging to 8 different classes:</source>
          <target state="translated">데이터 세트를 완전히 지정하려면 이름과 버전을 제공해야합니다. 버전은 선택 사항이지만 아래의 &lt;a href=&quot;#openml-versions&quot;&gt;데이터 세트 버전을&lt;/a&gt; 참조하십시오 . 데이터 세트에는 8 개의 다른 클래스에 속하는 총 1080 개의 예제가 있습니다.</target>
        </trans-unit>
        <trans-unit id="eddbe44ecc236b178d14592239180b4231c2f462" translate="yes" xml:space="preserve">
          <source>To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in &lt;em&gt;folds&lt;/em&gt; that we use for training and testing:</source>
          <target state="translated">(우리는 모델의 적합도에 대한 프록시로 사용할 수 있습니다) 예측 정확도의 더 나은 측정을 얻으려면, 우리는 연속적으로 데이터 분할 할 수 있습니다 &lt;em&gt;주름을&lt;/em&gt; 우리가 교육 및 테스트에 사용하는 :</target>
        </trans-unit>
        <trans-unit id="8f2c7c86e5d1b0f8592203b4a46517414e54ce05" translate="yes" xml:space="preserve">
          <source>To get identical results for each split, set &lt;code&gt;random_state&lt;/code&gt; to an integer.</source>
          <target state="translated">각 분할에 대해 동일한 결과를 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 정수로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="0228140936e0aced4eaa7c77d90637025c4d0909" translate="yes" xml:space="preserve">
          <source>To get started with this tutorial, you must first install &lt;em&gt;scikit-learn&lt;/em&gt; and all of its required dependencies.</source>
          <target state="translated">이 자습서를 시작하려면 먼저 &lt;em&gt;scikit-learn&lt;/em&gt; 및 모든 필수 종속성을 설치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="8d91bad777aec839541c338ab9f11be081ee54c6" translate="yes" xml:space="preserve">
          <source>To get the signed distance to the hyperplane use &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">초평면까지의 서명 거리를 얻으려면 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt; &lt;code&gt;SGDClassifier.decision_function&lt;/code&gt; 을&lt;/a&gt; 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="507e34b3976bcfaf958e1f0006102fdd2d8713ea" translate="yes" xml:space="preserve">
          <source>To go further we remove one of the 2 features and check what is the impact on the model stability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ae2612052e54b7be6598947088a287fffd01403" translate="yes" xml:space="preserve">
          <source>To illustrate &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt;, first let&amp;rsquo;s create an imbalanced dataset:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt; 를 설명하기 위해 먼저 불균형 데이터 셋을 만들어 봅시다 :</target>
        </trans-unit>
        <trans-unit id="e7a02a8f3e922c68cd6ce64dca33ce54683ffb1b" translate="yes" xml:space="preserve">
          <source>To illustrate this with a simple example, let&amp;rsquo;s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.</source>
          <target state="translated">간단한 예를 통해이를 설명하기 위해 모든 분류 자에 동일한 가중치를 할당하는 3 개의 분류 자 ​​및 3 가지 분류 문제가 있다고 가정 해 봅시다 : w1 = 1, w2 = 1, w3 = 1.</target>
        </trans-unit>
        <trans-unit id="27fe4060cc8aa9166cda2609863b9fdd12999baf" translate="yes" xml:space="preserve">
          <source>To illustrate this, PCA is performed comparing the use of data with &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.</source>
          <target state="translated">이를 설명하기 위해 PCA는 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; 가&lt;/a&gt; 적용된 데이터 사용과 비 스케일 데이터를 비교하여 수행 됩니다. 결과가 시각화되고 명확한 차이가 나타납니다. 스케일링되지 않은 세트의 첫 번째 주성분을 볼 수 있습니다. 특징 # 13이 방향을 지배하는 것을 알 수 있으며, 다른 특징보다 전체적으로 2 배의 크기이다. 이는 스케일링 된 데이터 버전의 주성분을 관찰 할 때 대비됩니다. 확장 버전에서, 크기의 순서는 모든 기능에서 거의 동일합니다.</target>
        </trans-unit>
        <trans-unit id="952f60109f87198cc4767d483a08ad921abb5966" translate="yes" xml:space="preserve">
          <source>To improve the conditioning of the problem (i.e. mitigating the &lt;a href=&quot;#curse-of-dimensionality&quot;&gt;The curse of dimensionality&lt;/a&gt;), it would be interesting to select only the informative features and set non-informative ones, like feature 2 to 0. Ridge regression will decrease their contribution, but not set them to zero. Another penalization approach, called &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; (least absolute shrinkage and selection operator), can set some coefficients to zero. Such methods are called &lt;strong&gt;sparse method&lt;/strong&gt; and sparsity can be seen as an application of Occam&amp;rsquo;s razor: &lt;em&gt;prefer simpler models&lt;/em&gt;.</source>
          <target state="translated">문제의 컨디셔닝을 개선하기 위해 (즉 , &lt;a href=&quot;#curse-of-dimensionality&quot;&gt;차원의 저주&lt;/a&gt; 완화 ), 특징적인 기능 만 선택하고 기능 2에서 0과 같은 비 정보적인 기능을 설정하는 것이 흥미로울 것입니다. 그것들을 0으로 만듭니다. &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; (최소 절대 수축 및 선택 연산자) 라고하는 또 다른 벌칙 접근법 은 일부 계수를 0으로 설정할 수 있습니다. 이러한 방법을 &lt;strong&gt;스파 스 방법&lt;/strong&gt; 이라고 하며 희소성은 Occam의 면도기를 적용한 것으로 볼 수 있습니다 . &lt;em&gt;더 간단한 모델 선호&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="172f2bacd24a45cfb82f466d14d1d58833b9ee69" translate="yes" xml:space="preserve">
          <source>To install the latest version (with pip):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f028c20036ea78694db90136b8cb3004f099e0bf" translate="yes" xml:space="preserve">
          <source>To limit the memory consumption, we queue examples up to a fixed amount before feeding them to the learner.</source>
          <target state="translated">메모리 소비를 제한하기 위해 예제를 학습자에게 공급하기 전에 고정 된 양까지 대기열에 넣습니다.</target>
        </trans-unit>
        <trans-unit id="61f860c325e06c4f97b9f4c7ced3d5279054856d" translate="yes" xml:space="preserve">
          <source>To load from an external dataset, please refer to &lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;loading external datasets&lt;/a&gt;.</source>
          <target state="translated">외부 데이터 세트에서로드하려면 &lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;외부 데이터 세트로드&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d787fd22509da728f07846c2b5d3ecac1d6b4105" translate="yes" xml:space="preserve">
          <source>To load the data and visualize the images:</source>
          <target state="translated">데이터를로드하고 이미지를 시각화하려면</target>
        </trans-unit>
        <trans-unit id="b4be4dde535adc435619c6f0e295e0ce05bad72b" translate="yes" xml:space="preserve">
          <source>To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance.</source>
          <target state="translated">예제를 더 빠르게 실행하기 위해 숨겨진 유닛을 거의 사용하지 않고 짧은 시간 동안 만 훈련합니다. 더 오래 훈련하면 훨씬 더 부드러운 공간 모양의 가중치가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="c413b102ea3791278492eefc26d38700197d19c5" translate="yes" xml:space="preserve">
          <source>To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance. The example will throw a warning because it doesn&amp;rsquo;t converge, in this case this is what we want because of CI&amp;rsquo;s time constraints.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96ba992a3c5af68caa74d107191c5a3c32806c93" translate="yes" xml:space="preserve">
          <source>To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the &lt;code&gt;build_preprocessor&lt;/code&gt;, &lt;code&gt;build_tokenizer&lt;/code&gt; and &lt;code&gt;build_analyzer&lt;/code&gt; factory methods instead of passing custom functions.</source>
          <target state="translated">전 처리기, 토크 나이저 및 분석기가 모델 매개 변수를 인식하도록하기 위해 사용자 정의 함수를 전달하는 대신 클래스에서 파생하여 &lt;code&gt;build_preprocessor&lt;/code&gt; , &lt;code&gt;build_tokenizer&lt;/code&gt; 및 &lt;code&gt;build_analyzer&lt;/code&gt; 팩토리 메소드를 대체 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c0f08b8475e4b67e5147698ce9ccb818f0394d27" translate="yes" xml:space="preserve">
          <source>To make this more explicit, consider the following notation:</source>
          <target state="translated">이를보다 명확하게하려면 다음 표기법을 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="8ada09feb86f8f3751dffbeeaba0e1e4f69156a7" translate="yes" xml:space="preserve">
          <source>To obtain a fully probabilistic model, the output \(y\) is assumed to be Gaussian distributed around \(X w\):</source>
          <target state="translated">완전 확률 모델을 얻기 위해 출력 \ (y \)는 \ (X w \) 주위에 가우시안 분포 된 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="65178eef58b048e690e1c210520e38da80789880" translate="yes" xml:space="preserve">
          <source>To perform classification with generalized linear models, see &lt;a href=&quot;#logistic-regression&quot;&gt;Logistic regression&lt;/a&gt;.</source>
          <target state="translated">일반화 된 선형 모형으로 분류를 수행하려면 &lt;a href=&quot;#logistic-regression&quot;&gt;로지스틱 회귀를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="72dc32b7225454e8d7c0ec26f14d95b55df2c79a" translate="yes" xml:space="preserve">
          <source>To quantify estimation error, we plot the likelihood of unseen data for different values of the shrinkage parameter. We also show the choices by cross-validation, or with the LedoitWolf and OAS estimates.</source>
          <target state="translated">추정 오차를 정량화하기 위해 수축 매개 변수의 다른 값에 대해 보이지 않는 데이터의 가능성을 표시합니다. 또한 교차 검증 또는 LedoitWolf 및 OAS 추정을 통해 선택 사항을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="397393d3de29d9ed57b4f231bd553afc264bafab" translate="yes" xml:space="preserve">
          <source>To return the corresponding classical subsets of kddcup 99. If None, return the entire kddcup 99 dataset.</source>
          <target state="translated">kddcup 99의 해당 클래식 하위 집합을 반환하려면 None 인 경우 전체 kddcup 99 데이터 집합을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b0e502baa68f0434bb574994337b41db58fa07c4" translate="yes" xml:space="preserve">
          <source>To run cross-validation on multiple metrics and also to return train scores, fit times and score times.</source>
          <target state="translated">여러 메트릭에서 교차 유효성 검사를 실행하고 기차 점수, 시간 및 점수 시간을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2d79b95a40b4d1ea2f276f13509aebc984e2d932" translate="yes" xml:space="preserve">
          <source>To see how this generalizes the binary log loss given above, note that in the binary case, \(p_{i,0} = 1 - p_{i,1}\) and \(y_{i,0} = 1 - y_{i,1}\), so expanding the inner sum over \(y_{i,k} \in \{0,1\}\) gives the binary log loss.</source>
          <target state="translated">이것이 위에서 주어진 이진 로그 손실을 일반화하는 방법을 보려면 이진 경우 \ (p_ {i, 0} = 1-p_ {i, 1} \) 및 \ (y_ {i, 0} = 1- y_ {i, 1} \)이므로 \ (y_ {i, k} \ in \ {0,1 \} \)에 대한 내부 합계를 확장하면 이진 로그 손실이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="25684d8b1766d360b665e8498a44a626b2b5bd13" translate="yes" xml:space="preserve">
          <source>To set &lt;code&gt;n_clusters=None&lt;/code&gt; initially</source>
          <target state="translated">&lt;code&gt;n_clusters=None&lt;/code&gt; 을 초기에 설정하려면</target>
        </trans-unit>
        <trans-unit id="78f293aec6a6c458449c6dc3bcd71b696525a449" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds.</source>
          <target state="translated">알고리즘 속도를 높이려면 최소 min_bin_freq 포인트가 시드로 지정된 빈만 허용하십시오.</target>
        </trans-unit>
        <trans-unit id="e1dfcbed698085a7ab462cf760bf24e65a9e8400" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1.</source>
          <target state="translated">알고리즘 속도를 높이려면 최소 min_bin_freq 포인트가 시드로 지정된 빈만 허용하십시오. 정의되지 않은 경우 1로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="d139a616dfe019ba73a251e6419e9ddac4a94c0f" translate="yes" xml:space="preserve">
          <source>To support imputation in inductive mode we store each feature&amp;rsquo;s estimator during the &lt;code&gt;fit&lt;/code&gt; phase, and predict without refitting (in order) during the &lt;code&gt;transform&lt;/code&gt; phase.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d14a37b58107d5112ea5c1494d2809710215a276" translate="yes" xml:space="preserve">
          <source>To train the &lt;code&gt;estimators&lt;/code&gt; and &lt;code&gt;final_estimator&lt;/code&gt;, the &lt;code&gt;fit&lt;/code&gt; method needs to be called on the training data:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd2f04d7c6e080a2cce0d2e7339e598ba17acac5" translate="yes" xml:space="preserve">
          <source>To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call &lt;code&gt;transform&lt;/code&gt; instead of &lt;code&gt;fit_transform&lt;/code&gt; on the transformers, since they have already been fit to the training set:</source>
          <target state="translated">새 문서에서 결과를 예측하려면 이전과 거의 동일한 기능 추출 체인을 사용하여 기능을 추출해야합니다. 차이점은 &lt;code&gt;transform&lt;/code&gt; 에서 &lt;code&gt;fit_transform&lt;/code&gt; 대신 transform 을 호출한다는 것 입니다. 트랜스포머는 이미 훈련 세트에 적합했기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="5da67914dc5314b6125944bb748e1a3d6c08f736" translate="yes" xml:space="preserve">
          <source>To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the LDA classification rule explained above. We write \(K\) for the total number of target classes. Since in LDA we assume that all classes have the same estimated covariance \(\Sigma\), we can rescale the data so that this covariance is the identity:</source>
          <target state="translated">차원 축소에서 LDA의 사용을 이해하려면 위에서 설명한 LDA 분류 규칙의 기하학적 재구성으로 시작하는 것이 유용합니다. 총 대상 클래스 수는 \ (K \)입니다. LDA에서는 모든 클래스의 추정 된 공분산 \ (\ Sigma \)가 동일하다고 가정하므로이 공분산이 항등이되도록 데이터의 크기를 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6df5e0eab0e1e02def9ae99e68c6ddf1a841d6d1" translate="yes" xml:space="preserve">
          <source>To use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator:</source>
          <target state="translated">참신 탐지에 레이블 (예 : 레이블을 예측하거나 새로운 보이지 않는 데이터의 비정상 점수를 계산)에 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 을 사용하려면 추정기를 맞추기 전에 &lt;code&gt;novelty&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정 하여 추정기를 인스턴스화해야합니다 .</target>
        </trans-unit>
        <trans-unit id="011ed6ad19bc2f123579a7e50ff8b4dad33bf360" translate="yes" xml:space="preserve">
          <source>To use joblib.Memory to cache the svmlight file:</source>
          <target state="translated">joblib.Memory를 사용하여 svmlight 파일을 캐시하려면 다음을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="e88dca9a24e84cd773e046a28a4daa4e8217f9da" translate="yes" xml:space="preserve">
          <source>To use text files in a scikit-learn classification or clustering algorithm, you will need to use the :mod`~sklearn.feature_extraction.text` module to build a feature extraction transformer that suits your problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11936ea84de206f18d8b708b6f4eca9fb6c8b59" translate="yes" xml:space="preserve">
          <source>To use text files in a scikit-learn classification or clustering algorithm, you will need to use the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; module to build a feature extraction transformer that suits your problem.</source>
          <target state="translated">scikit-learn 분류 또는 클러스터링 알고리즘에서 텍스트 파일을 사용하려면 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 모듈을 사용하여 문제에 맞는 기능 추출 변환기를 작성해야합니다.</target>
        </trans-unit>
        <trans-unit id="c7aa2d2ef894f356c354736ecb4235681bee95b2" translate="yes" xml:space="preserve">
          <source>To use this dataset with scikit-learn, we transform each 8x8 image into a feature vector of length 64</source>
          <target state="translated">이 데이터 세트를 scikit-learn과 함께 사용하기 위해 각 8x8 이미지를 길이가 64 인 특징 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="f5d271c927cff9ea25de07089e51938b7e86a2a7" translate="yes" xml:space="preserve">
          <source>To use this model as a classifier, we just need to estimate from the training data the class priors \(P(y=k)\) (by the proportion of instances of class \(k\)), the class means \(\mu_k\) (by the empirical sample class means) and the covariance matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on shrinkage below).</source>
          <target state="translated">이 모델을 분류 자로 사용하려면 학습 데이터에서 클래스 우선 순위 \ (P (y = k) \) (클래스 \ (k \)의 인스턴스 비율에 따라)를 추정하면됩니다. \ mu_k \) (경험적 샘플 클래스 수단에 의해) 및 공분산 행렬 (경험적 샘플 클래스 공분산 행렬에 의해 또는 정규화 된 추정기에 의해 : 아래 수축에 대한 섹션 참조).</target>
        </trans-unit>
        <trans-unit id="54c39e1b5f1a8214dbbbffa4ce79accc0474a39a" translate="yes" xml:space="preserve">
          <source>To use this model for classification, one needs to combine a &lt;a href=&quot;generated/sklearn.neighbors.neighborhoodcomponentsanalysis#sklearn.neighbors.NeighborhoodComponentsAnalysis&quot;&gt;&lt;code&gt;NeighborhoodComponentsAnalysis&lt;/code&gt;&lt;/a&gt; instance that learns the optimal transformation with a &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; instance that performs the classification in the projected space. Here is an example using the two classes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e5117e9f756d52dfb6878a7715bd7c9bf590353" translate="yes" xml:space="preserve">
          <source>To validate a model we need a scoring function (see &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Metrics and scoring: quantifying the quality of predictions&lt;/a&gt;), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d015bfb570917361c4b4abaaa59f5e623d8c463" translate="yes" xml:space="preserve">
          <source>To validate a model we need a scoring function (see &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Model evaluation: quantifying the quality of predictions&lt;/a&gt;), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.</source>
          <target state="translated">모델의 유효성을 검사하려면 분류기의 정확도와 같은 스코어링 기능 ( &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;모델 평가 : 예측 품질 정량화&lt;/a&gt; 참조)이 필요합니다 . 추정기의 다중 하이퍼 파라미터를 선택하는 올바른 방법은 검증 세트 또는 다중 검증 세트에서 최대 점수를 갖는 &lt;a href=&quot;grid_search#grid-search&quot;&gt;하이퍼 파라미터&lt;/a&gt; 를 선택하는 그리드 검색 또는 유사한 방법 ( 추정기의 하이퍼 파라미터 조정 참조 )입니다. 유효성 검사 점수를 기준으로 하이퍼 파라미터를 최적화하면 유효성 검사 점수가 편향되어 일반화를 더 이상 잘 평가할 수 없습니다. 일반화의 적절한 추정치를 얻으려면 다른 테스트 세트에서 점수를 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="4f24d3986e58b34af3ea2c4b07af932b987ecc57" translate="yes" xml:space="preserve">
          <source>To verify this interpretation we plot the variability of the AGE and EXPERIENCE coefficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baa10199f999bc30e58b0035ac2f6e51132399ed" translate="yes" xml:space="preserve">
          <source>To visualize the probability weighting, we fit each classifier on the training set and plot the predicted class probabilities for the first sample in this example dataset.</source>
          <target state="translated">확률 가중치를 시각화하기 위해 각 분류자를 학습 세트에 맞추고이 예제 데이터 세트의 첫 번째 샘플에 대해 예측 된 클래스 확률을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="ba234a16bb1a2ae4619585ca04988c1afd574060" translate="yes" xml:space="preserve">
          <source>Tokenize the documents and count the occurrences of token and return them as a sparse matrix</source>
          <target state="translated">문서를 토큰 화하고 토큰 발생을 계산하여 희소 행렬로 반환</target>
        </trans-unit>
        <trans-unit id="e89caeb25fc24a274e225b242d49cc6fb7ddfa72" translate="yes" xml:space="preserve">
          <source>Tokenizing text with &lt;code&gt;scikit-learn&lt;/code&gt;</source>
          <target state="translated">scikit &lt;code&gt;scikit-learn&lt;/code&gt; 텍스트 토큰 화</target>
        </trans-unit>
        <trans-unit id="45d4a0ebe499a5d042ac0f7bc4284501d3667758" translate="yes" xml:space="preserve">
          <source>Tolerance for &amp;lsquo;arpack&amp;rsquo; method Not used if eigen_solver==&amp;rsquo;dense&amp;rsquo;.</source>
          <target state="translated">'arpack'방법에 대한 허용 오차 eigen_solver == 'dense'인 경우 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="318dfc593e0123f93a8fe309f411532f48eea756" translate="yes" xml:space="preserve">
          <source>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.</source>
          <target state="translated">ARPACK에 대한 허용 오차. 0은 기계 정밀도를 의미합니다. 무작위 SVD 솔버에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="13511570864a98fa2d61f43da929b11b4894937f" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if &lt;code&gt;method == 'hessian'&lt;/code&gt;</source>
          <target state="translated">Hessian 고유 매핑 방법에 대한 허용 오차. &lt;code&gt;method == 'hessian'&lt;/code&gt; 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="e4c877ba267607a99e62c8b31f7891feda117cf7" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if method == &amp;lsquo;hessian&amp;rsquo;</source>
          <target state="translated">Hessian 고유 매핑 방법에 대한 허용 오차. method == 'hessian'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="aeb25ea9c0101939a4336136b4e11db71f1bb1be" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if &lt;code&gt;method == 'modified'&lt;/code&gt;</source>
          <target state="translated">수정 된 LLE 방법에 대한 허용 오차. &lt;code&gt;method == 'modified'&lt;/code&gt; 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="b6502cfb6f414093cd5faf0376953824eae5e86f" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if method == &amp;lsquo;modified&amp;rsquo;</source>
          <target state="translated">수정 된 LLE 방법에 대한 허용 오차. method == 'modified'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="40eaf2d9a188116c07595886d4a67c9121557ecf" translate="yes" xml:space="preserve">
          <source>Tolerance for singular values computed by svd_solver == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">svd_solver == 'arpack'에 의해 계산 된 특이 값에 대한 공차.</target>
        </trans-unit>
        <trans-unit id="a495f50d68c5f0d21905244c442ac1ec46831c6d" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criteria.</source>
          <target state="translated">정지 기준에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="1f900b2be351c5e1d6397b25c9a2e6c5e5c36343" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criterion.</source>
          <target state="translated">기준 중지에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="4d73abe23fd3517118aa70ae58840719c14ae6a0" translate="yes" xml:space="preserve">
          <source>Tolerance for the early stopping. When the loss is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations (if set to a number), the training stops.</source>
          <target state="translated">조기 정지에 대한 허용 오차. &lt;code&gt;n_iter_no_change&lt;/code&gt; 반복 (숫자로 설정된 경우)에 대해 손실이 적어도 tol만큼 개선되지 않으면 훈련이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="334a1d6597d473e85cc8725e20828e0c9824ea02" translate="yes" xml:space="preserve">
          <source>Tolerance for the optimization. When the loss or score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive iterations, unless &lt;code&gt;learning_rate&lt;/code&gt; is set to &amp;lsquo;adaptive&amp;rsquo;, convergence is considered to be reached and training stops.</source>
          <target state="translated">최적화에 대한 허용 오차. &lt;code&gt;learning_rate&lt;/code&gt; 가 'adaptive'로 설정되어 있지 않으면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 반복에 대해 손실 또는 점수가 적어도 &lt;code&gt;tol&lt;/code&gt; 만큼 개선 되지 않으면 수렴에 도달 한 것으로 간주되고 교육이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="6938a4dcb29969d15aaa6cafefb8f09b830ed305" translate="yes" xml:space="preserve">
          <source>Tolerance for the stopping condition.</source>
          <target state="translated">정지 조건에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="3a49445cc3e76e8c0deab47f4b10c5bd7dc33960" translate="yes" xml:space="preserve">
          <source>Tolerance of the stopping condition.</source>
          <target state="translated">정지 조건의 허용 오차.</target>
        </trans-unit>
        <trans-unit id="48a48ded1ae1ed29a7ddaed19c15db301472918d" translate="yes" xml:space="preserve">
          <source>Tolerance on update at each iteration.</source>
          <target state="translated">각 반복에서 업데이트에 대한 공차.</target>
        </trans-unit>
        <trans-unit id="a2223ba588ac8a94dc6928512bbe1ae559b46f6b" translate="yes" xml:space="preserve">
          <source>Tolerance used in the iterative algorithm default 1e-06.</source>
          <target state="translated">반복 알고리즘 기본값 1e-06에 사용 된 공차입니다.</target>
        </trans-unit>
        <trans-unit id="20a2955c412dcae35aa2ef964ce8c2d4b1c07dcb" translate="yes" xml:space="preserve">
          <source>Tolerance when calculating spatial median.</source>
          <target state="translated">공간 중앙값을 계산할 때의 공차.</target>
        </trans-unit>
        <trans-unit id="f3d0c54c4b7882f5280f0492c26f2bf33d35d2a2" translate="yes" xml:space="preserve">
          <source>Tony Blair</source>
          <target state="translated">토니 블레어</target>
        </trans-unit>
        <trans-unit id="e1781cb6d03ccb2216639c1d54de7540b9fc2c2b" translate="yes" xml:space="preserve">
          <source>Tools for imputing missing values are discussed at &lt;a href=&quot;impute#impute&quot;&gt;Imputation of missing values&lt;/a&gt;.</source>
          <target state="translated">결 측값 대치 도구는 결 측값 &lt;a href=&quot;impute#impute&quot;&gt;대치&lt;/a&gt; 에서 설명 합니다 .</target>
        </trans-unit>
        <trans-unit id="0d184ce2992ee425b9d4cc3d528da94fb4da399d" translate="yes" xml:space="preserve">
          <source>Tophat kernel (&lt;code&gt;kernel = 'tophat'&lt;/code&gt;)</source>
          <target state="translated">Tophat 커널 ( &lt;code&gt;kernel = 'tophat'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="0954aa60533f43dc3b2b9a9cbdee11a74f79eada" translate="yes" xml:space="preserve">
          <source>Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</source>
          <target state="translated">음이 아닌 행렬 인수 분해 및 잠재 된 Dirichlet 할당을 사용한 주제 추출</target>
        </trans-unit>
        <trans-unit id="97129616afbfcb01d33b44619c8bf267194395ac" translate="yes" xml:space="preserve">
          <source>Total Phenols:</source>
          <target state="translated">총 페놀 :</target>
        </trans-unit>
        <trans-unit id="b9c3723a92a74173bb8adb739559660c0010b476" translate="yes" xml:space="preserve">
          <source>Total impurity of leaves vs effective alphas of pruned tree</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24a9e81269d05c734577a89440230faee238f7b2" translate="yes" xml:space="preserve">
          <source>Total log-likelihood of the data in X.</source>
          <target state="translated">X에있는 데이터의 총 로그 우도</target>
        </trans-unit>
        <trans-unit id="1861e0049c1f17066ecafccd7b29a27b43a45cf0" translate="yes" xml:space="preserve">
          <source>Total log-likelihood of the data in X. This is normalized to be a probability density, so the value will be low for high-dimensional data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0acc1077bbd3872347f5d4223b32bfa85b2dc24b" translate="yes" xml:space="preserve">
          <source>Total number of documents. Only used in the &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4055747cee4593e58e4a6dcbfa7d6ccc61845cf0" translate="yes" xml:space="preserve">
          <source>Total number of documents. Only used in the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="translated">총 문서 수 &lt;code&gt;partial_fit&lt;/code&gt; 메소드 에서만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="aed40ed5719d059f29eba1177189a60b01059871" translate="yes" xml:space="preserve">
          <source>Total phenols</source>
          <target state="translated">총 페놀</target>
        </trans-unit>
        <trans-unit id="babba0bc0e9a3e36ce98f362a62519c8eacb94cb" translate="yes" xml:space="preserve">
          <source>Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</source>
          <target state="translated">최적의 전처리 된 고유 솔버를 향하여 : 로컬 최적의 블록 전처리 된 공액 구배 방법 Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e3ae1e8d052cc2d0c25bbda7e5d0370ec624b1e8" translate="yes" xml:space="preserve">
          <source>Toy example of 1D regression using linear, polynomial and RBF kernels.</source>
          <target state="translated">선형, 다항식 및 RBF 커널을 사용한 1D 회귀 분석의 장난감 예.</target>
        </trans-unit>
        <trans-unit id="264fa08a131d6382d6715d8c951f2b5bea1c373c" translate="yes" xml:space="preserve">
          <source>Traceback example, note how the line of the error is indicated as well as the values of the parameter passed to the function that triggered the exception, even though the traceback happens in the child process:</source>
          <target state="translated">역 추적 예제, 하위 프로세스에서 역 추적이 발생하더라도 오류 행이 표시되는 방식과 예외를 트리거 한 함수에 전달 된 매개 변수의 값을 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="8718fa41b5577d15733c0d074d4e6ea2d5f88486" translate="yes" xml:space="preserve">
          <source>Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.</source>
          <target state="translated">추적, International Computer of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, 2008 년 5 월.</target>
        </trans-unit>
        <trans-unit id="20662c705376209f11480639e3ee11e7bf62f8df" translate="yes" xml:space="preserve">
          <source>Traditional regression metrics such as Mean Squared Error and Mean Absolute Error are hard to meaningfully interpret on count values with many zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e4826fce9da6f5f03d1b11115df13e0bc514c4a" translate="yes" xml:space="preserve">
          <source>Train all data by multiple calls to partial_fit.</source>
          <target state="translated">partial_fit을 여러 번 호출하여 모든 데이터를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="bd98708380c7e60a9c0a687f254abca8478a36f8" translate="yes" xml:space="preserve">
          <source>Train and test sizes may be different in each fold, with a difference of at most &lt;code&gt;n_classes&lt;/code&gt;.</source>
          <target state="translated">최대 &lt;code&gt;n_classes&lt;/code&gt; 의 차이로 각 폴드마다 학습 및 테스트 크기가 다를 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1c08c1bee3835bcafdb50e8cdda68c68d71fa67e" translate="yes" xml:space="preserve">
          <source>Train error vs Test error</source>
          <target state="translated">열차 오차 vs 시험 오차</target>
        </trans-unit>
        <trans-unit id="357c94d50b669e3c60f0758a6140ed17dc81af61" translate="yes" xml:space="preserve">
          <source>Train l1-penalized logistic regression models on a binary classification problem derived from the Iris dataset.</source>
          <target state="translated">Iris 데이터 세트에서 파생 된 이진 분류 문제에 대해 l1 형벌 로지스틱 회귀 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="5099d1b071bb02f5306e84c9c0e29bbe834adc72" translate="yes" xml:space="preserve">
          <source>Train models on the diabetes dataset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cfcb0c276264df865da734aa7faab6c6b43fed6" translate="yes" xml:space="preserve">
          <source>Train the model using libsvm (low-level method)</source>
          <target state="translated">libsvm을 사용하여 모델 학습 (저수준 방법)</target>
        </trans-unit>
        <trans-unit id="b7dd566e0e9177f3a0300b3c2ac1d09a00aaeda2" translate="yes" xml:space="preserve">
          <source>Training a Random Forest and Plotting the ROC Curve</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff5331ad7dc89bf5a9dd23c31ab738af7815c499" translate="yes" xml:space="preserve">
          <source>Training a classifier</source>
          <target state="translated">분류기 훈련</target>
        </trans-unit>
        <trans-unit id="8dcfc4ff7c1f7cc06878c0cf93f4198eb475ff8d" translate="yes" xml:space="preserve">
          <source>Training classifiers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f0630eb2ecfdd0ed6f7defc6642e6c0143bcbf3" translate="yes" xml:space="preserve">
          <source>Training data</source>
          <target state="translated">훈련 데이터</target>
        </trans-unit>
        <trans-unit id="6c7c988c62ce8a65ab6394bf4f62bdef696bbe60" translate="yes" xml:space="preserve">
          <source>Training data, requires length = n_samples</source>
          <target state="translated">훈련 데이터, 길이 = n_samples 필요</target>
        </trans-unit>
        <trans-unit id="c0c6cec2e93954e8c33880af1baef2b15439d9d5" translate="yes" xml:space="preserve">
          <source>Training data, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70fa8e3174eef9a1ccfc0bda8b38c7cfbf09ffd6" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">학습 데이터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="1d999bb02f6364cf15c69e5533af993a3fc0fdd8" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">교육 데이터. 여기서 n_samples는 샘플 수이고 n_features는 기능 수입니다.</target>
        </trans-unit>
        <trans-unit id="c4f931e6893a5565e07f4500ddfb86c154c8b1a3" translate="yes" xml:space="preserve">
          <source>Training data, which is also required for prediction. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead the precomputed training matrix, of shape (n_samples, n_samples).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f12731d4ed32a02266da09997a2bf0e000555cf6" translate="yes" xml:space="preserve">
          <source>Training data, which is also required for prediction. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead the precomputed training matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">예측에 필요한 훈련 데이터. 커널 ==&amp;ldquo;사전 계산 된&amp;rdquo;경우 대신 사전 계산 된 훈련 행렬 모양 = [n_samples, n_samples]입니다.</target>
        </trans-unit>
        <trans-unit id="c5441fed149296831061b9151bd71d563327dc0d" translate="yes" xml:space="preserve">
          <source>Training data.</source>
          <target state="translated">훈련 데이터.</target>
        </trans-unit>
        <trans-unit id="4319dec91a5574f9382b1b679ba9c82bf44c0f15" translate="yes" xml:space="preserve">
          <source>Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">훈련 데이터. 배열 또는 행렬 인 경우 metric = 'precomputed'인 경우 [n_samples, n_features] 또는 [n_samples, n_samples]를 형성하십시오.</target>
        </trans-unit>
        <trans-unit id="744e21c8d62df0575ccae05fe593cde4f20f55b7" translate="yes" xml:space="preserve">
          <source>Training data. If array or matrix, the shape is (n_samples, n_features), or (n_samples, n_samples) if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6ee318ff46063d0c81310a68af0a95508a0a339" translate="yes" xml:space="preserve">
          <source>Training data. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead a precomputed kernel matrix, of shape (n_samples, n_samples).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cc715a75ede17772899f7cc9ab69882475a79bc" translate="yes" xml:space="preserve">
          <source>Training data. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead a precomputed kernel matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">훈련 데이터. 커널 ==&amp;ldquo;사전 계산 된&amp;rdquo;경우 이는 대신 사전 계산 된 커널 행렬입니다. shape = [n_samples, n_samples].</target>
        </trans-unit>
        <trans-unit id="6ea489741914be2912ee247eeaf800f3ba49e6d8" translate="yes" xml:space="preserve">
          <source>Training data. If using GCV, will be cast to float64 if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b12ede4c226e6e2f235813d30bce55744269c03f" translate="yes" xml:space="preserve">
          <source>Training data. Must fulfill input requirements of first step of the pipeline.</source>
          <target state="translated">훈련 데이터. 파이프 라인의 첫 번째 단계에 대한 입력 요구 사항을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="d5044fd4a2ac02d5a0b137f2f3b7fd6b8f65a006" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If &lt;code&gt;y&lt;/code&gt; is mono-output then &lt;code&gt;X&lt;/code&gt; can be sparse.</source>
          <target state="translated">훈련 데이터. 불필요한 메모리 중복을 피하기 위해 포트란 연속 데이터로 직접 전달하십시오. 경우 &lt;code&gt;y&lt;/code&gt; 단일 출력 후 &lt;code&gt;X&lt;/code&gt; 는 희소 수있다.</target>
        </trans-unit>
        <trans-unit id="8ed7855d8da328d2505a0bcd1c3302665b72cb3d" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.</source>
          <target state="translated">훈련 데이터. 불필요한 메모리 중복을 피하기 위해 포트란 연속 데이터로 직접 전달하십시오. y가 단일 출력이면 X가 희박 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4c004afef287030c2dcb4a937f43adb06e1cdf0e" translate="yes" xml:space="preserve">
          <source>Training data. Shape [n_samples, n_features], or [n_samples, n_samples] if affinity==&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">훈련 데이터. affinity == 'precomputed'인 경우 [n_samples, n_features] 또는 [n_samples, n_samples]를 형성하십시오.</target>
        </trans-unit>
        <trans-unit id="30765b444b768ceb7d6bccc7cc4dd80dd79e1fcb" translate="yes" xml:space="preserve">
          <source>Training instances to cluster, or distances between instances if &lt;code&gt;affinity='precomputed'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7fd3fb257a793666a84757ce13a7e7b97d84277" translate="yes" xml:space="preserve">
          <source>Training instances to cluster, or distances between instances if &lt;code&gt;metric='precomputed'&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="106fdf378e3c73e01e49b8eb92780c5d664680e0" translate="yes" xml:space="preserve">
          <source>Training instances to cluster, or similarities / affinities between instances if &lt;code&gt;affinity='precomputed'&lt;/code&gt;. If a sparse feature matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e31b10426b1248f84a44c65b36e8acd0e5e7589c" translate="yes" xml:space="preserve">
          <source>Training instances to cluster, or similarities / affinities between instances if &lt;code&gt;affinity='precomputed'&lt;/code&gt;. If a sparse matrix is provided in a format other than &lt;code&gt;csr_matrix&lt;/code&gt;, &lt;code&gt;csc_matrix&lt;/code&gt;, or &lt;code&gt;coo_matrix&lt;/code&gt;, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8959fb1d08ac2482d5adecb9cc6d42cd3487ff" translate="yes" xml:space="preserve">
          <source>Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">클러스터링 할 교육 인스턴스. 데이터가 C 순서로 변환되므로 주어진 데이터가 C 연속적이지 않은 경우 메모리 복사가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="eed67f947767b8d48d69b1a746024e52c70765e3" translate="yes" xml:space="preserve">
          <source>Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it&amp;rsquo;s not in CSR format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fb973e1446d09c43085a14e14217bfa82f35fac" translate="yes" xml:space="preserve">
          <source>Training set and testing set</source>
          <target state="translated">훈련 세트 및 테스트 세트</target>
        </trans-unit>
        <trans-unit id="ea59a824d416e7ea0dc63df33b1afa59cdb64566" translate="yes" xml:space="preserve">
          <source>Training set.</source>
          <target state="translated">훈련 세트.</target>
        </trans-unit>
        <trans-unit id="3c518c488676e60e90ca53bcc0aa7271b669fe4d" translate="yes" xml:space="preserve">
          <source>Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers.</source>
          <target state="translated">훈련 세트 : 위에서 언급 한 논문에서 참조 된 이론을 바탕으로 최적의 랜덤 매트릭스 치수를 찾기 위해 모양 만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="68d52cda6c0756d21c2527d22eda07d7e45f55d9" translate="yes" xml:space="preserve">
          <source>Training target.</source>
          <target state="translated">훈련 목표.</target>
        </trans-unit>
        <trans-unit id="32e48bd3169f82f98b7879700514da5daba97549" translate="yes" xml:space="preserve">
          <source>Training targets. Must fulfill label requirements for all steps of the pipeline.</source>
          <target state="translated">훈련 목표. 파이프 라인의 모든 단계에 대한 레이블 요구 사항을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="bc89d708a926da60c1e855065f294a150e4844da" translate="yes" xml:space="preserve">
          <source>Training vector, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the total number of features.</source>
          <target state="translated">벡터, 훈련 &lt;code&gt;n_samples&lt;/code&gt; 샘플의 수입니다 &lt;code&gt;n_features&lt;/code&gt; 하는 기능의 총 수입니다.</target>
        </trans-unit>
        <trans-unit id="325dc392b957558d0accbc4c288eabf85d0d476c" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">학습 벡터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="01000b19ae19a1d02ea4ceb374852ca509745c92" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.</source>
          <target state="translated">학습 벡터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다. 중심 수축은 희소 행렬과 함께 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="6a8354ff2f178d04d8fd18ac8502cba6a9d2e53e" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="d6cf7c60af251621aaa911db11caacf9c4de19a4" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples is the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23811d7edf5d74f8278700a3182a9d6e499aa68e" translate="yes" xml:space="preserve">
          <source>Training vectors, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66e0bce9861c05444da85a9795d5edcc3de5cb5e" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="f64b8abd734d5648613b346b4bb3c97a56c66bbf" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples, n_samples).</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다. kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 (n_samples, n_samples)입니다.</target>
        </trans-unit>
        <trans-unit id="f07e6c81521ffea6a563851833abed1de8063cb9" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features. Here, each feature of X is assumed to be from a different categorical distribution. It is further assumed that all categories of each feature are represented by the numbers 0, &amp;hellip;, n - 1, where n refers to the total number of categories for the given feature. This can, for instance, be achieved with the help of OrdinalEncoder.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1d0a9d845fc3b099041b2d0f31be0d050a7001e" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features. When using GCV, will be cast to float64 if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee5e82a19ba6d9a5b4fc8f431028f4e0ae5cae2a" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of predictors.</source>
          <target state="translated">여기에서 n_samples는 샘플 수이고 n_features는 예측 변수 수입니다.</target>
        </trans-unit>
        <trans-unit id="3dbb8cbc3c8d093280069e8e889d1e0c62e1afde" translate="yes" xml:space="preserve">
          <source>Transform X back to its original space.</source>
          <target state="translated">X를 원래 공간으로 되돌립니다.</target>
        </trans-unit>
        <trans-unit id="dbfeebba6e53c937056143e8cf1258378ae1c26d" translate="yes" xml:space="preserve">
          <source>Transform X back to original space.</source>
          <target state="translated">X를 원래 공간으로 되돌립니다.</target>
        </trans-unit>
        <trans-unit id="aad161b5ffd8fb6722cd74d5621a58697eead90e" translate="yes" xml:space="preserve">
          <source>Transform X into a (weighted) graph of k nearest neighbors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="063b3e20cfa017691e75a65bb052691ed38fc79c" translate="yes" xml:space="preserve">
          <source>Transform X into a (weighted) graph of neighbors nearer than a radius</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fcdcd20eec681f04cc400d1e7a3d3a35f46ced9" translate="yes" xml:space="preserve">
          <source>Transform X into subcluster centroids dimension.</source>
          <target state="translated">X를 하위 클러스터 중심 치수로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="da3379264043ea94358e5b4b01ce80967c41f1a5" translate="yes" xml:space="preserve">
          <source>Transform X separately by each transformer, concatenate results.</source>
          <target state="translated">트랜스포머별로 X를 개별적으로 변환하여 결과를 연결합니다.</target>
        </trans-unit>
        <trans-unit id="054e9dc484301382a53ef7807c44414f413c3b43" translate="yes" xml:space="preserve">
          <source>Transform X to a cluster-distance space.</source>
          <target state="translated">X를 군집 거리 공간으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="9340d4e978871cfc2faf3772609beb4370b76837" translate="yes" xml:space="preserve">
          <source>Transform X to ordinal codes.</source>
          <target state="translated">X를 서수로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="d750cda6e828d45a370fec4538601ee99b5443be" translate="yes" xml:space="preserve">
          <source>Transform X using one-hot encoding.</source>
          <target state="translated">one-hot 인코딩을 사용하여 X를 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="f9e88a65d85f54852f98655b3f250fdbf7750c92" translate="yes" xml:space="preserve">
          <source>Transform X using the forward function.</source>
          <target state="translated">앞으로 함수를 사용하여 X를 변형하십시오.</target>
        </trans-unit>
        <trans-unit id="fb06535ce9222390887b51d0862f28eec382f495" translate="yes" xml:space="preserve">
          <source>Transform X using the inverse function.</source>
          <target state="translated">역함수를 사용하여 X를 변형합니다.</target>
        </trans-unit>
        <trans-unit id="55b2dc92fd17631d37a113cafae1257246c63b9f" translate="yes" xml:space="preserve">
          <source>Transform X.</source>
          <target state="translated">X 변환</target>
        </trans-unit>
        <trans-unit id="df5b966033d10ab5ffd4498c25f3563581fac3a4" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a normalized tf or tf-idf representation</source>
          <target state="translated">카운트 행렬을 표준화 된 tf 또는 tf-idf 표현으로 변환</target>
        </trans-unit>
        <trans-unit id="c6579300b554475d257c93a2551d1e7ac8d00f29" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a tf or tf-idf representation</source>
          <target state="translated">카운트 행렬을 tf 또는 tf-idf 표현으로 변환</target>
        </trans-unit>
        <trans-unit id="cfe77beec60d283a1ae2557849fffc568b20c2b6" translate="yes" xml:space="preserve">
          <source>Transform a new matrix using the built clustering</source>
          <target state="translated">내장 된 클러스터링을 사용하여 새로운 매트릭스 변환</target>
        </trans-unit>
        <trans-unit id="eb758f2f9f4d3b4a21a0f5aa711d86b7f433cb44" translate="yes" xml:space="preserve">
          <source>Transform a sequence of documents to a document-term matrix.</source>
          <target state="translated">일련의 문서를 문서 용어 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="90d7961623626a54873e65ae75f5e5aedaf80a7d" translate="yes" xml:space="preserve">
          <source>Transform a sequence of instances to a scipy.sparse matrix.</source>
          <target state="translated">일련의 인스턴스를 scipy.sparse 행렬로 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="482237f55f57c5ab1436ea9ad6e0ca3a5497f2c8" translate="yes" xml:space="preserve">
          <source>Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the &lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt;&lt;/a&gt; estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals.</source>
          <target state="translated">Ricker 웨이블릿의 스파 스 조합으로 신호를 변환합니다. 이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt; &lt;/a&gt; 추정기를 사용하여 다른 희소 코딩 방법을 시각적으로 비교합니다 . Ricker (멕시코 모자 또는 가우시안의 2 차 파생물이라고도 함)는 이와 같은 조각 단위의 일정한 신호를 나타내는 데 특히 좋은 커널이 아닙니다. 따라서 서로 다른 폭의 원자를 추가하는 것이 얼마나 중요한지 알 수 있으므로 신호 유형에 가장 잘 맞는 사전을 배우도록 동기를 부여합니다.</target>
        </trans-unit>
        <trans-unit id="3c3158f9e95a76dac9ab046600d246dc683b1322" translate="yes" xml:space="preserve">
          <source>Transform array or sparse matrix X back to feature mappings.</source>
          <target state="translated">배열 또는 희소 행렬 X를 형상 매핑으로 다시 변환합니다.</target>
        </trans-unit>
        <trans-unit id="43aed443a30ff04a0a7d38cae0c2e3f2c765ad45" translate="yes" xml:space="preserve">
          <source>Transform between iterable of iterables and a multilabel format</source>
          <target state="translated">이터 러블의 이터 러블과 멀티 라벨 형식 간 변환</target>
        </trans-unit>
        <trans-unit id="8428b18b095eb02611727f6a1283e0146f4aea18" translate="yes" xml:space="preserve">
          <source>Transform binary labels back to multi-class labels</source>
          <target state="translated">이진 레이블을 다시 다중 클래스 레이블로 변환</target>
        </trans-unit>
        <trans-unit id="2d5fb2d774241a80b97c22822072a1cd5822cad7" translate="yes" xml:space="preserve">
          <source>Transform data X according to the fitted model.</source>
          <target state="translated">적합 모델에 따라 데이터 X를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="e993947ab9336eb409d6a8eb55c55e2b5b858d46" translate="yes" xml:space="preserve">
          <source>Transform data back to its original space.</source>
          <target state="translated">데이터를 원래 공간으로 다시 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="b922af176e5b4295d0d766cd496f7523d4754428" translate="yes" xml:space="preserve">
          <source>Transform data to polynomial features</source>
          <target state="translated">다항식 피처로 데이터 변환</target>
        </trans-unit>
        <trans-unit id="f1a4a6b05048c3643e26b0d505b52199b7895296" translate="yes" xml:space="preserve">
          <source>Transform dataset.</source>
          <target state="translated">데이터 세트를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="00a7e9f2b3e643cac0fe08c5b1d0d59cfff5c504" translate="yes" xml:space="preserve">
          <source>Transform discretized data back to original feature space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0146265304f248a8c03f040ea5d584981839ec0b" translate="yes" xml:space="preserve">
          <source>Transform documents to document-term matrix.</source>
          <target state="translated">문서를 문서 용어 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="778e7579ae52504e167839efee081ba3167de93f" translate="yes" xml:space="preserve">
          <source>Transform feature-&amp;gt;value dicts to array or sparse matrix.</source>
          <target state="translated">피처-&amp;gt; 값을 배열 또는 희소 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="8fde1456e50a374e1e8877ac2d0ea9941a580f00" translate="yes" xml:space="preserve">
          <source>Transform features by scaling each feature to a given range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8f4f5c3bee4bfd8c782321e0d4eb227c2d3191b" translate="yes" xml:space="preserve">
          <source>Transform features using quantiles information.</source>
          <target state="translated">Quantile 정보를 사용하여 기능을 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="ace4ae2489dd9688eddb3a58e732664d39d28a92" translate="yes" xml:space="preserve">
          <source>Transform labels back to original encoding.</source>
          <target state="translated">레이블을 원래 인코딩으로 다시 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="76c682df30bb4975f2641f2e89f16cc0b5f2d625" translate="yes" xml:space="preserve">
          <source>Transform labels to normalized encoding.</source>
          <target state="translated">레이블을 정규화 된 인코딩으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="e6d8f7568400d53b2f444fa6cbf018c08b09552e" translate="yes" xml:space="preserve">
          <source>Transform multi-class labels to binary labels</source>
          <target state="translated">멀티 클래스 레이블을 이진 레이블로 변환</target>
        </trans-unit>
        <trans-unit id="ec1f3a72d306387b537de1b3b116fbdf51b17550" translate="yes" xml:space="preserve">
          <source>Transform new data by linear interpolation</source>
          <target state="translated">선형 보간으로 새로운 데이터 변환</target>
        </trans-unit>
        <trans-unit id="7e25dbc81754715628745ec728c6c249ac9d1737" translate="yes" xml:space="preserve">
          <source>Transform new points into embedding space.</source>
          <target state="translated">새로운 점을 임베드 공간으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="17e15b65d999776fc7cb047cfc38d87f9b340eec" translate="yes" xml:space="preserve">
          <source>Transform the data X according to the fitted NMF model</source>
          <target state="translated">피팅 된 NMF 모델에 따라 데이터 X 변환</target>
        </trans-unit>
        <trans-unit id="28a4737ac1d13b4e451237dc699b89c49f7fb862" translate="yes" xml:space="preserve">
          <source>Transform the given indicator matrix into label sets</source>
          <target state="translated">주어진 지표 매트릭스를 레이블 세트로 변환</target>
        </trans-unit>
        <trans-unit id="38739bda11e07f48ac023acb8323ed328f115bd5" translate="yes" xml:space="preserve">
          <source>Transform the given label sets</source>
          <target state="translated">주어진 레이블 세트를 변환</target>
        </trans-unit>
        <trans-unit id="92a052e88a019f5aca9bb96a9137d202560617b4" translate="yes" xml:space="preserve">
          <source>Transform the sources back to the mixed data (apply mixing matrix).</source>
          <target state="translated">소스를 혼합 데이터로 다시 변환하십시오 (믹싱 매트릭스 적용).</target>
        </trans-unit>
        <trans-unit id="6414c408546f181e607c3ec28647dd72e64872ea" translate="yes" xml:space="preserve">
          <source>Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.</source>
          <target state="translated">특징을 더 높은 차원의 희소 공간으로 변환하십시오. 그런 다음 이러한 기능에 대해 선형 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="d3709f378c935401f6b259df9cce5a50135da098" translate="yes" xml:space="preserve">
          <source>Transformed array.</source>
          <target state="translated">변형 된 배열.</target>
        </trans-unit>
        <trans-unit id="4a8a97e010ec7ac27b50257ef7ee542c13ba8846" translate="yes" xml:space="preserve">
          <source>Transformed data</source>
          <target state="translated">변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="d460e113769e190612a2b959c1c729d7e8676439" translate="yes" xml:space="preserve">
          <source>Transformed data in the binned space.</source>
          <target state="translated">비닝 된 공간에서 변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="14642329121567cf9f5775d8a6512d3b978fccd2" translate="yes" xml:space="preserve">
          <source>Transformed data matrix</source>
          <target state="translated">변환 된 데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="0d3a338b719647431757293955a1513d13c572f4" translate="yes" xml:space="preserve">
          <source>Transformed data.</source>
          <target state="translated">변환 된 데이터.</target>
        </trans-unit>
        <trans-unit id="08b12f8aaa8632b66a6a22bc4de550a469c3cc9c" translate="yes" xml:space="preserve">
          <source>Transformed dataset.</source>
          <target state="translated">변환 된 데이터 세트.</target>
        </trans-unit>
        <trans-unit id="eeb85e59603c1cea29acb31c92a29204737376ea" translate="yes" xml:space="preserve">
          <source>Transformed input.</source>
          <target state="translated">변환 된 입력.</target>
        </trans-unit>
        <trans-unit id="0a6145f06a4913811002ff339bc5284d2892e790" translate="yes" xml:space="preserve">
          <source>Transformed samples</source>
          <target state="translated">변형 된 샘플</target>
        </trans-unit>
        <trans-unit id="a40fe3e47da3940e6c654b4aca4fd3b4db53b382" translate="yes" xml:space="preserve">
          <source>Transformed values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b6f242e80f2185c5234be3f41a48f40589d58f3" translate="yes" xml:space="preserve">
          <source>Transformer instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd414e8652819a2c899c58cada9c3dd8cc66e071" translate="yes" xml:space="preserve">
          <source>Transformer mixin that performs feature selection given a support mask</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d517e36599e67ec967c7be2afac6d7777579d1a" translate="yes" xml:space="preserve">
          <source>Transformer used in &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 하고 &lt;code&gt;predict&lt;/code&gt; 되는 변압기 .</target>
        </trans-unit>
        <trans-unit id="43471a7ace97310a9577002aa9803e00d83e6192" translate="yes" xml:space="preserve">
          <source>Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a &lt;a href=&quot;#pipeline&quot;&gt;Pipeline&lt;/a&gt;. Pipeline is often used in combination with &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; which concatenates the output of transformers into a composite feature space. &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; deals with transforming the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;target&lt;/a&gt; (i.e. log-transform &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt;). In contrast, Pipelines only transform the observed data (&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt;).</source>
          <target state="translated">변압기는 일반적으로 분류기, 회귀 기 또는 기타 추정기와 결합하여 합성 추정기를 구축합니다. 가장 일반적인 도구는 &lt;a href=&quot;#pipeline&quot;&gt;파이프 라인&lt;/a&gt; 입니다. 파이프 라인은 종종 변압기의 출력을 복합 피처 공간으로 연결하는 &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; 과 함께 사용됩니다 . &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; 는 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;대상&lt;/a&gt; 변환 (즉, 로그 변환 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt; )을 처리합니다. 반대로 파이프 라인은 관측 된 데이터 ( &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt; ) 만 변환합니다 .</target>
        </trans-unit>
        <trans-unit id="021fd2002f82b7c8da7b92d01fe659fdcbdaeb82" translate="yes" xml:space="preserve">
          <source>Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a &lt;a href=&quot;#pipeline&quot;&gt;Pipeline&lt;/a&gt;. Pipeline is often used in combination with &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; which concatenates the output of transformers into a composite feature space. &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; deals with transforming the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-target&quot;&gt;target&lt;/a&gt; (i.e. log-transform &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;y&lt;/a&gt;). In contrast, Pipelines only transform the observed data (&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-x&quot;&gt;X&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b69fe15775501662a569d2ed632ddbe970e558ef" translate="yes" xml:space="preserve">
          <source>Transformers for missing value imputation</source>
          <target state="translated">결 측값 대치 변압기</target>
        </trans-unit>
        <trans-unit id="4804df5ca1652cab2567ab10e41eae2d30b7e99a" translate="yes" xml:space="preserve">
          <source>Transforming Classifier Scores into Accurate Multiclass Probability Estimates, B. Zadrozny &amp;amp; C. Elkan, (KDD 2002)</source>
          <target state="translated">분류기 점수를 정확한 멀티 클래스 확률 추정치로 변환, B. Zadrozny &amp;amp; C. Elkan, (KDD 2002)</target>
        </trans-unit>
        <trans-unit id="74f517360774a680819178109aa2b52b87d4fd99" translate="yes" xml:space="preserve">
          <source>Transforming distance to well-behaved similarities</source>
          <target state="translated">거리를 올바르게 동작하는 유사성으로 변환</target>
        </trans-unit>
        <trans-unit id="87156c340b6aec33fafb3545fc791ff4187014aa" translate="yes" xml:space="preserve">
          <source>Transforms between iterable of iterables and a multilabel format, e.g. a (samples x classes) binary matrix indicating the presence of a class label.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dfe17b4b683a10ef2eafef30897d9c629bf96dd6" translate="yes" xml:space="preserve">
          <source>Transforms discretized data back to original feature space.</source>
          <target state="translated">이산화 된 데이터를 원래 피쳐 공간으로 다시 변환합니다.</target>
        </trans-unit>
        <trans-unit id="5bd7a9a7032f01002afe1d21dc1635e87bd5dbc6" translate="yes" xml:space="preserve">
          <source>Transforms features by scaling each feature to a given range.</source>
          <target state="translated">각 기능을 지정된 범위로 조정하여 기능을 변환합니다.</target>
        </trans-unit>
        <trans-unit id="45675a7235910659531092f94ca2cac1226cb6a9" translate="yes" xml:space="preserve">
          <source>Transforms lists of feature-value mappings to vectors.</source>
          <target state="translated">피처-값 매핑 목록을 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="18a051a7877c1a9e6b194ac68c49193ac689d698" translate="yes" xml:space="preserve">
          <source>Transforms text into a sparse matrix of n-gram counts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff596a653686d4986dda1851c66682d846f4bf4d" translate="yes" xml:space="preserve">
          <source>Transforms the image samples in X into a matrix of patch data.</source>
          <target state="translated">X의 이미지 샘플을 패치 데이터 매트릭스로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="aff42f13a1dfe3735469a5dd26ab12a5bac4a9ad" translate="yes" xml:space="preserve">
          <source>Tree pruning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dda8f4221caceb0e1b9d09350500616b39f0c3c5" translate="yes" xml:space="preserve">
          <source>Tree&amp;rsquo;s Feature Importance from Mean Decrease in Impurity (MDI)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6a25d0e3691aa7e7e60fcc1d61ee6046c7d09b3" translate="yes" xml:space="preserve">
          <source>Tree-based estimators (see the &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module and forest of trees in the &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; meta-transformer):</source>
          <target state="translated">트리 기반 추정량합니다 (참조 &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; 의&lt;/a&gt; 에서 나무의 모듈과 숲 &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 와 결합 할 때 다시 (관련이없는 기능을 삭제하는 데 사용할 수있는 컴퓨팅 기능 importances에 사용할 수있는 모듈) &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt; 메타 트랜스포머) :</target>
        </trans-unit>
        <trans-unit id="ccdf4aec1fccb56109a1e3945469e98a8d62aca9" translate="yes" xml:space="preserve">
          <source>Tree-based estimators (see the &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module and forest of trees in the &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; meta-transformer):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3629b84a13698bd03a810f8a682027a82e4a7e42" translate="yes" xml:space="preserve">
          <source>Tree-based models provide an alternative measure of &lt;a href=&quot;ensemble#random-forest-feature-importance&quot;&gt;feature importances based on the mean decrease in impurity&lt;/a&gt; (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81d8ac0c0739336d0bbd6f8053b2fde8039cdb1c" translate="yes" xml:space="preserve">
          <source>Triangle Inequality: d(x, y) + d(y, z) &amp;gt;= d(x, z)</source>
          <target state="translated">삼각형 부등식 : d (x, y) + d (y, z)&amp;gt; = d (x, z)</target>
        </trans-unit>
        <trans-unit id="331d2c199452ae22aa8941c5bcbe6a7fe41c68b5" translate="yes" xml:space="preserve">
          <source>Tristan Fletcher: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Relevance Vector Machines explained&lt;/a&gt;</source>
          <target state="translated">Tristan Fletcher : &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;관련성 벡터 머신 설명&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3b19d80cd81c13647ace615b9d73da08b4d8c61b" translate="yes" xml:space="preserve">
          <source>True : always precompute distances</source>
          <target state="translated">True : 항상 사전 거리 계산</target>
        </trans-unit>
        <trans-unit id="27f22be4c5a651c1c27cbdc4b85cf77c839d3ddd" translate="yes" xml:space="preserve">
          <source>True : always precompute distances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebac1d7d68472a848a069828ba36b6b2dd6227bb" translate="yes" xml:space="preserve">
          <source>True binary labels in binary indicator format.</source>
          <target state="translated">이진 표시기 형식의 실제 이진 레이블.</target>
        </trans-unit>
        <trans-unit id="94ad072572f1b0d8a6896ff3fd5d5269c3006cce" translate="yes" xml:space="preserve">
          <source>True binary labels or binary label indicators.</source>
          <target state="translated">이진 레이블 또는 이진 레이블 표시기</target>
        </trans-unit>
        <trans-unit id="173029937373f6d16ed7438491b1a8131b2bc4cb" translate="yes" xml:space="preserve">
          <source>True binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given.</source>
          <target state="translated">진정한 이진 레이블. 레이블이 {-1, 1} 또는 {0, 1}이 아닌 경우 pos_label을 명시 적으로 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="394534b1dcaf25753dd90ddcd321d6ef1b441d87" translate="yes" xml:space="preserve">
          <source>True if a fixed vocabulary of term to indices mapping is provided by the user</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cba1cd7ae39f91f6d0e2908d3956200bdf94de07" translate="yes" xml:space="preserve">
          <source>True if estimator is a classifier and False otherwise.</source>
          <target state="translated">추정기가 분류 자이면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="7e34070b9f977933411df9b814860396cade02bb" translate="yes" xml:space="preserve">
          <source>True if estimator is a regressor and False otherwise.</source>
          <target state="translated">추정기가 회귀 변수이면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="d039a95b006860b5b92d23f84015c0458d6fdb1a" translate="yes" xml:space="preserve">
          <source>True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input y is passed in sparse format.</source>
          <target state="translated">predict에서 반환 된 배열이 희소 한 CSC 형식이면 true입니다. 입력 y가 희소 형식으로 전달되면 자동으로 True로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="018f28ffd2c7241c63be51b46bba3e8aa528d907" translate="yes" xml:space="preserve">
          <source>True if the input data to transform is given as a sparse matrix, False otherwise.</source>
          <target state="translated">변환 할 입력 데이터가 희소 행렬로 제공되면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="06236e43536e8bd62b7d950e36ddd9dca022a999" translate="yes" xml:space="preserve">
          <source>True if the output at fit is 2d, else false.</source>
          <target state="translated">맞는 출력이 2d이면 true이고, 그렇지 않으면 false입니다.</target>
        </trans-unit>
        <trans-unit id="abda54d00232aa3c71419926e38966e372a6e200" translate="yes" xml:space="preserve">
          <source>True if the returned array from transform is desired to be in sparse CSR format.</source>
          <target state="translated">변환에서 반환 된 배열이 희소 한 CSR 형식이되도록하려면 True입니다.</target>
        </trans-unit>
        <trans-unit id="b3f5d1c4b9aeea8d97315ada02d3f0f3b6e0dbc5" translate="yes" xml:space="preserve">
          <source>True labels for X.</source>
          <target state="translated">X에 대한 실제 레이블.</target>
        </trans-unit>
        <trans-unit id="d40646e12c6271d621333c3801ded96344098308" translate="yes" xml:space="preserve">
          <source>True labels or binary label indicators. The binary and multiclass cases expect labels with shape (n_samples,) while the multilabel case expects binary label indicators with shape (n_samples, n_classes).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24a7816a0ae25d0715f83e367246b56738200fc8" translate="yes" xml:space="preserve">
          <source>True mutual information can&amp;rsquo;t be negative. If its estimate turns out to be negative, it is replaced by zero.</source>
          <target state="translated">진정한 상호 정보는 부정적 일 수 없습니다. 추정치가 음수로 판명되면 0으로 대체됩니다.</target>
        </trans-unit>
        <trans-unit id="1086f49a3e748a59792d8342e65dffdfa18d8ff5" translate="yes" xml:space="preserve">
          <source>True positive rate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e857c90bead41164c28f265fe201e3c7a69f5d75" translate="yes" xml:space="preserve">
          <source>True target, consisting of integers of two values. The positive label must be greater than the negative label.</source>
          <target state="translated">두 값의 정수로 구성된 진정한 대상. 양수 레이블은 음수 레이블보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="5f6f5563a268706baa91536cfcd1565c453cd8e7" translate="yes" xml:space="preserve">
          <source>True targets of binary classification in range {-1, 1} or {0, 1}.</source>
          <target state="translated">{-1, 1} 또는 {0, 1} 범위의 이진 분류 대상입니다.</target>
        </trans-unit>
        <trans-unit id="308caeb8e2723647ce2ad06a73f50c7b1bd2b781" translate="yes" xml:space="preserve">
          <source>True targets of multilabel classification, or true scores of entities to be ranked.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e2bbfc40bf0e63b31fb5b0351be961b49cc74be" translate="yes" xml:space="preserve">
          <source>True targets.</source>
          <target state="translated">진정한 목표.</target>
        </trans-unit>
        <trans-unit id="18dd5ee40d70767a2f6629e8ff8969a87290115e" translate="yes" xml:space="preserve">
          <source>True values for X</source>
          <target state="translated">X의 참값</target>
        </trans-unit>
        <trans-unit id="81e3774c236b4c61a22388a1a822b3e69fb3e5a6" translate="yes" xml:space="preserve">
          <source>True values for X.</source>
          <target state="translated">X에 대한 참값.</target>
        </trans-unit>
        <trans-unit id="7e4dee4cabccdb0d80fb65794484e70b177b35d1" translate="yes" xml:space="preserve">
          <source>True values of target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d7ce48badf2f0a91a36bec7511768633417749f" translate="yes" xml:space="preserve">
          <source>True when convergence was reached in fit(), False otherwise.</source>
          <target state="translated">fit ()에서 수렴에 도달하면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="d333cd18e174fe06286d776d55b1b9eaf760ce1b" translate="yes" xml:space="preserve">
          <source>True: Force all values of X to be finite.</source>
          <target state="translated">True : X의 모든 값을 유한하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="42eef53fc6823568b56bf4bf254799ea2d7766d8" translate="yes" xml:space="preserve">
          <source>True: Force all values of array to be finite.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="260b9ffda14105c1fd2ffa31d36eae7c83268ddd" translate="yes" xml:space="preserve">
          <source>True: the results is casted to an unsigned int</source>
          <target state="translated">True : 결과는 unsigned int로 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="e67782a583f6700ced58a8f740875b4358d4fb58" translate="yes" xml:space="preserve">
          <source>Trustworthiness of the low-dimensional embedding.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00b6f6ebc7b7070cf35772b16b427811573346a1" translate="yes" xml:space="preserve">
          <source>Try classifying classes 1 and 2 from the iris dataset with SVMs, with the 2 first features. Leave out 10% of each class and test prediction performance on these observations.</source>
          <target state="translated">SVM을 사용하여 홍채 데이터 세트에서 클래스 1과 2를 분류 해보십시오. 각 클래스의 10 %를 제외하고 이러한 관측치에 대한 예측 성능을 테스트하십시오.</target>
        </trans-unit>
        <trans-unit id="5449ae93c54cf3f8e79ab0ee95bb4ee118bd4f84" translate="yes" xml:space="preserve">
          <source>Try classifying the digits dataset with nearest neighbors and a linear model. Leave out the last 10% and test prediction performance on these observations.</source>
          <target state="translated">가장 가까운 이웃과 선형 모델로 숫자 데이터 세트를 분류하십시오. 이 관측치에서 마지막 10 %를 제외하고 예측 성능을 테스트하십시오.</target>
        </trans-unit>
        <trans-unit id="7a783eca4388b1c7b8c830f476caa080328ba3c3" translate="yes" xml:space="preserve">
          <source>Try playing around with the &lt;code&gt;analyzer&lt;/code&gt; and &lt;code&gt;token normalisation&lt;/code&gt; under &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 에서 &lt;code&gt;analyzer&lt;/code&gt; 및 &lt;code&gt;token normalisation&lt;/code&gt; 로 놀아보십시오 .</target>
        </trans-unit>
        <trans-unit id="b7e468fa3f6cfdfb33fa6bf28dcdf3165bf89507" translate="yes" xml:space="preserve">
          <source>Try to differentiate the two first classes of the iris data</source>
          <target state="translated">홍채 데이터의 첫 번째 두 가지 등급을 차별화</target>
        </trans-unit>
        <trans-unit id="1bc09af6523c25787ea3631aac8c3f52f8bc29dc" translate="yes" xml:space="preserve">
          <source>Try using &lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;Truncated SVD&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;latent semantic analysis&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;잠재 의미 체계 분석에 &lt;/a&gt;&lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;잘린 SVD&lt;/a&gt; 를 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="126c548fa4d4a4c65c7759b8f0eb82ce5677dab5" translate="yes" xml:space="preserve">
          <source>Tsoumakas, G., Katakis, I., &amp;amp; Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.</source>
          <target state="translated">Tsoumakas, G., Katakis, I. &amp;amp; Vlahavas, I. (2010). 다중 레이블 데이터 마이닝. 데이터 마이닝 및 지식 검색 핸드북 (pp. 667-685). 스프링거 미국.</target>
        </trans-unit>
        <trans-unit id="0d016a3ee3141a6ebd01b9c31169fb6ec8d37fd6" translate="yes" xml:space="preserve">
          <source>Tuning the hyper-parameters of an estimator</source>
          <target state="translated">추정기의 하이퍼 파라미터 튜닝</target>
        </trans-unit>
        <trans-unit id="2e926727653886b165b872a4b6b2a62bf90f0bd1" translate="yes" xml:space="preserve">
          <source>Tuple of row and column indicators for a set of biclusters.</source>
          <target state="translated">biclusters 세트에 대한 행 및 열 표시기의 튜플.</target>
        </trans-unit>
        <trans-unit id="a813118f879d8793b6336ef94325f8c8d09426be" translate="yes" xml:space="preserve">
          <source>Tuples of the form (transformer, columns) specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c054700312acf34cbacbc98d115120c76da3a6d5" translate="yes" xml:space="preserve">
          <source>Turn seed into a np.random.RandomState instance</source>
          <target state="translated">시드를 np.random.RandomState 인스턴스로 전환</target>
        </trans-unit>
        <trans-unit id="4b18a115e0842d6d0ba3a4ec2ed7b07c665b5f56" translate="yes" xml:space="preserve">
          <source>Tutorial exercises</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="007a671747688cedb01751baca6545483f05de7a" translate="yes" xml:space="preserve">
          <source>Tutorial setup</source>
          <target state="translated">튜토리얼 설정</target>
        </trans-unit>
        <trans-unit id="025f75efad84ed2b985f2818a53e81aa77abca7c" translate="yes" xml:space="preserve">
          <source>Tutorial: A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">학습서 : 과학 데이터 처리를위한 통계 학습에 대한 학습서</target>
        </trans-unit>
        <trans-unit id="e7df3b5ebbaf2fd3b4b4579edbd7ce42f46db699" translate="yes" xml:space="preserve">
          <source>Tutorial: An introduction to machine learning with scikit-learn</source>
          <target state="translated">튜토리얼 : scikit-learn을 이용한 머신 러닝 소개</target>
        </trans-unit>
        <trans-unit id="8682fb6c27e32858369b74e47f829fad6c8d3a68" translate="yes" xml:space="preserve">
          <source>Tutorial: Choosing the right estimator</source>
          <target state="translated">학습서 : 올바른 추정기 선택</target>
        </trans-unit>
        <trans-unit id="2865c0d93493065de0c34da92792e35a845226d3" translate="yes" xml:space="preserve">
          <source>Tutorial: Model selection</source>
          <target state="translated">튜토리얼 : 모델 선택</target>
        </trans-unit>
        <trans-unit id="b7709b919b68974b71489ceb95a00ef31c4eda72" translate="yes" xml:space="preserve">
          <source>Tutorial: Putting it all together</source>
          <target state="translated">튜토리얼 : 모두 모아보기</target>
        </trans-unit>
        <trans-unit id="b0b3bc4bbf4e62230750bf24baeb122d7a994298" translate="yes" xml:space="preserve">
          <source>Tutorial: Statistical learning</source>
          <target state="translated">튜토리얼 : 통계 학습</target>
        </trans-unit>
        <trans-unit id="a149365421f01d98250256737c350c42a4cd4b82" translate="yes" xml:space="preserve">
          <source>Tutorial: Supervised learning</source>
          <target state="translated">튜토리얼 :지도 학습</target>
        </trans-unit>
        <trans-unit id="4353f067a68843e09ae0691ab9f9c44ef2e6db23" translate="yes" xml:space="preserve">
          <source>Tutorial: Unsupervised learning</source>
          <target state="translated">튜토리얼 : 비지도 학습</target>
        </trans-unit>
        <trans-unit id="206fac7baeed5ee14f8990630b6607a7c33e8644" translate="yes" xml:space="preserve">
          <source>Tutorial: Working With Text Data</source>
          <target state="translated">학습서 : 텍스트 데이터 작업</target>
        </trans-unit>
        <trans-unit id="b919de3c63710fd07133db7062fb5a1fbffa0bfe" translate="yes" xml:space="preserve">
          <source>Tutorial: scikit-learn Tutorials</source>
          <target state="translated">튜토리얼 : scikit-learn 튜토리얼</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="b9f0efb9bc5f86b33edfdb893732e46d86a776bd" translate="yes" xml:space="preserve">
          <source>Tweedie deviance is a homogeneous function of degree &lt;code&gt;2-power&lt;/code&gt;. Thus, Gamma distribution with &lt;code&gt;power=2&lt;/code&gt; means that simultaneously scaling &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has no effect on the deviance. For Poisson distribution &lt;code&gt;power=1&lt;/code&gt; the deviance scales linearly, and for Normal distribution (&lt;code&gt;power=0&lt;/code&gt;), quadratically. In general, the higher &lt;code&gt;power&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b534d41a45e4c1bc7da48b9a62fdc9565da97cac" translate="yes" xml:space="preserve">
          <source>Tweedie power parameter. Either power &amp;lt;= 0 or power &amp;gt;= 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25d7747a25958e3f10fc9c93bbab44d13248adbd" translate="yes" xml:space="preserve">
          <source>Tweedie regression on insurance claims</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="995550b74403db560a3a2ea8d3906cd56b901336" translate="yes" xml:space="preserve">
          <source>Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means.</source>
          <target state="translated">일반 k- 평균과보다 확장 가능한 사촌 미니 배치 k- 평균이라는 두 가지 알고리즘이 시연됩니다.</target>
        </trans-unit>
        <trans-unit id="3b934d458351995534fed7d234de7b14c38f4cd4" translate="yes" xml:space="preserve">
          <source>Two approaches for performing calibration of probabilistic predictions are provided: a parametric approach based on Platt&amp;rsquo;s sigmoid model and a non-parametric approach based on isotonic regression (&lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;). Probability calibration should be done on new data not used for model fitting. The class &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged. Already fitted classifiers can be calibrated by &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; via the parameter cv=&amp;rdquo;prefit&amp;rdquo;. In this case, the user has to take care manually that data for model fitting and calibration are disjoint.</source>
          <target state="translated">확률 예측의 교정을 수행하기위한 두 가지 접근법, 즉 Platt의 시그 모이 드 모델을 기반으로하는 파라 메트릭 접근법과 등장 성 회귀를 기반으로하는 비모수 적 접근법 ( &lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt; &lt;code&gt;sklearn.isotonic&lt;/code&gt; &lt;/a&gt; )이 제공됩니다. 모델 피팅에 사용되지 않은 새 데이터에 대해서는 확률 교정을 수행해야합니다. &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt; 클래스 는 교차 검증 생성기를 사용하고 각 분할에 대해 트레인 샘플의 모델 매개 변수와 테스트 샘플의 교정을 추정합니다. 그런 다음 접기에 대해 예측 된 확률을 평균합니다. 이미 장착 된 분류기는 다음을 통해 교정 할 수 있습니다.&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;cv =&amp;rdquo;prefit&amp;rdquo;매개 변수를 통해 CalibratedClassifierCV. 이 경우 사용자는 모델 피팅 및 캘리브레이션 데이터가 분리되도록 수동으로 관리해야합니다.</target>
        </trans-unit>
        <trans-unit id="de7e8d6ad699213a292d0c528c5ddad33bca14ae" translate="yes" xml:space="preserve">
          <source>Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster.</source>
          <target state="translated">연결성을 부과하면 두 가지 결과를 볼 수 있습니다. 연결 매트릭스를 사용한 첫 번째 클러스터링이 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="73ece4ca1e1779fbf5110031e848f2313deac958" translate="yes" xml:space="preserve">
          <source>Two cross-validation loops are performed in parallel: one by the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; estimator to set &lt;code&gt;gamma&lt;/code&gt; and the other one by &lt;code&gt;cross_val_score&lt;/code&gt; to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.</source>
          <target state="translated">두 개의 교차 검증 루프가 병렬로 수행됩니다. 하나는 &lt;code&gt;gamma&lt;/code&gt; 를 설정하기 위해 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 추정기에 의해, 다른 하나 는 추정기의 예측 성능을 측정하기 위해 &lt;code&gt;cross_val_score&lt;/code&gt; 에 의해 다른 것 입니다. 결과 점수는 새 데이터에 대한 예측 점수의 편향되지 않은 추정치입니다.</target>
        </trans-unit>
        <trans-unit id="d467efdd44bf60415fc895b8589d9d81d46d02ec" translate="yes" xml:space="preserve">
          <source>Two families of ensemble methods are usually distinguished:</source>
          <target state="translated">앙상블 방법의 두 제품군은 일반적으로 구별됩니다.</target>
        </trans-unit>
        <trans-unit id="e8ac95de27d48015555c5b981483208d51b8f268" translate="yes" xml:space="preserve">
          <source>Two feature extraction methods can be used in this example:</source>
          <target state="translated">이 예에서는 두 가지 특징 추출 방법을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ebb2ce8305b879c94bfe5ff4f307e7a35d679e99" translate="yes" xml:space="preserve">
          <source>Two plots will be shown for each scaler/normalizer/transformer. The left figure will show a scatter plot of the full data set while the right figure will exclude the extreme values considering only 99 % of the data set, excluding marginal outliers. In addition, the marginal distributions for each feature will be shown on the side of the scatter plot.</source>
          <target state="translated">각 스케일러 / 노멀 라이저 / 트랜스포머에 대해 두 개의 플롯이 표시됩니다. 왼쪽 그림은 전체 데이터 세트의 산점도를 표시하고 오른쪽 그림은 한계 이상 값을 제외하고 데이터 세트의 99 % 만 고려하여 극단 값을 제외합니다. 또한 각 피처에 대한 한계 분포는 산점도 측면에 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="d08300d20f1b41587441de06b29a0ea2e0345c8c" translate="yes" xml:space="preserve">
          <source>Two regions are populated: when the EXPERIENCE coefficient is positive the AGE one is negative and viceversa.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="348f286c4d7d6b276984cd38d102dc527023a237" translate="yes" xml:space="preserve">
          <source>Two separate datasets are used for the two different plots. The reason behind this is the &lt;code&gt;l1&lt;/code&gt; case works better on sparse data, while &lt;code&gt;l2&lt;/code&gt; is better suited to the non-sparse case.</source>
          <target state="translated">두 개의 서로 다른 플롯에 대해 두 개의 개별 데이터 세트가 사용됩니다. 그 이유는 &lt;code&gt;l1&lt;/code&gt; 사례가 희소 데이터에서 더 잘 작동하고 &lt;code&gt;l2&lt;/code&gt; 가 희소가 아닌 경우에 더 적합하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="77c4595f573f3df24ff0cb3827f75f6aed496412" translate="yes" xml:space="preserve">
          <source>Two types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32895c2e5eacd1283051e2d5a4f1fd3f826fb6ed" translate="yes" xml:space="preserve">
          <source>Two-class AdaBoost</source>
          <target state="translated">2 급 AdaBoost</target>
        </trans-unit>
        <trans-unit id="b8fde32df7d701e50fc79883cdf21005c9469e51" translate="yes" xml:space="preserve">
          <source>Type casting</source>
          <target state="translated">타입 캐스팅</target>
        </trans-unit>
        <trans-unit id="7b90464c9a3a0593a486a1facdfd06e25cac2162" translate="yes" xml:space="preserve">
          <source>Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR</source>
          <target state="translated">SVM 유형 : C SVC, nu SVC, 1 클래스, 엡실론 SVR, nu SVR</target>
        </trans-unit>
        <trans-unit id="fb24034e0a15fdb11753c4c561fec377a847eca1" translate="yes" xml:space="preserve">
          <source>Type of SVM: C_SVC, NuSVC, OneClassSVM, EpsilonSVR or NuSVR respectively. 0 by default.</source>
          <target state="translated">SVM 유형 : 각각 C_SVC, NuSVC, OneClassSVM, EpsilonSVR 또는 NuSVR. 기본적으로 0입니다.</target>
        </trans-unit>
        <trans-unit id="05734831eef4f60aabd73eed1535149e1780b49e" translate="yes" xml:space="preserve">
          <source>Type of kernel.</source>
          <target state="translated">커널 유형.</target>
        </trans-unit>
        <trans-unit id="7784bde958a1d323776ea14d0478698cc397c040" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, and &amp;lsquo;distance&amp;rsquo; will return the distances between neighbors according to the given metric.</source>
          <target state="translated">반환 된 매트릭스의 유형 : 'connectivity'는 1과 0으로 연결성 매트릭스를 반환하고 'distance'는 주어진 메트릭에 따라 이웃 사이의 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="029a83801426f186d4049ef92d5f3d3590b1d125" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, in &amp;lsquo;distance&amp;rsquo; the edges are Euclidean distance between points.</source>
          <target state="translated">반환 된 매트릭스의 유형 : '연결성'은 1과 0으로 연결성 매트릭스를 반환합니다. '거리'에서 가장자리는 점 사이의 유클리드 거리입니다.</target>
        </trans-unit>
        <trans-unit id="e2af4c36790c9137ba49cdb815accc60f6f75311" translate="yes" xml:space="preserve">
          <source>Type of store backend for reading/writing cache files. Default: &amp;lsquo;local&amp;rsquo;. The &amp;lsquo;local&amp;rsquo; backend is using regular filesystem operations to manipulate data (open, mv, etc) in the backend.</source>
          <target state="translated">캐시 파일 읽기 / 쓰기를위한 저장소 백엔드 유형 기본값은 'local'입니다. '로컬'백엔드는 일반 파일 시스템 작업을 사용하여 백엔드의 데이터 (open, mv 등)를 조작합니다.</target>
        </trans-unit>
        <trans-unit id="858cba7a97e85950fd69a9661ce88f3dff1bf729" translate="yes" xml:space="preserve">
          <source>Type of the matrix returned by fit_transform() or transform().</source>
          <target state="translated">fit_transform () 또는 transform ()에 의해 반환되는 행렬의 유형입니다.</target>
        </trans-unit>
        <trans-unit id="b6e792a3d08a7bd144dac10e42edb461fd3dd2e3" translate="yes" xml:space="preserve">
          <source>Type to use in computing the mean. For integer inputs, the default is &lt;code&gt;float64&lt;/code&gt;; for floating point inputs, it is the same as the input dtype.</source>
          <target state="translated">평균 계산에 사용할 유형입니다. 정수 입력의 경우 기본값은 &lt;code&gt;float64&lt;/code&gt; 입니다 . 부동 소수점 입력의 경우 입력 dtype과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="9af8f14bd15271db0f113f7c146e7fa9294b1caa" translate="yes" xml:space="preserve">
          <source>TypeError</source>
          <target state="translated">TypeError</target>
        </trans-unit>
        <trans-unit id="363cb5cb9b015bf8fe75ee8f6f3ad675ca5618cc" translate="yes" xml:space="preserve">
          <source>UNION</source>
          <target state="translated">UNION</target>
        </trans-unit>
        <trans-unit id="d609f86a64dc993cf97b5c1696e70d121d69089c" translate="yes" xml:space="preserve">
          <source>UNION_not_member</source>
          <target state="translated">UNION_not_member</target>
        </trans-unit>
        <trans-unit id="5c8cdf8bfe08e7fa632507cd27d7c4593fc32d5d" translate="yes" xml:space="preserve">
          <source>Under the assumption that the data are Gaussian distributed, Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;2&lt;/a&gt; derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf&amp;rsquo;s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f77afa338a167babd59a76b7f498d6550ba586ff" translate="yes" xml:space="preserve">
          <source>Under the assumption that the data are Gaussian distributed, Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt; derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf&amp;rsquo;s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.</source>
          <target state="translated">데이터가 가우스 분포라는 가정하에 Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt; 는 Ledoit와 Wolf의 공식보다 작은 평균 제곱 오차를 산출하는 수축 계수를 선택하기위한 공식을 도출했다. 결과 추정량을 공분산의 Oracle Shrinkage Approximating Estimator라고합니다.</target>
        </trans-unit>
        <trans-unit id="b57ce0246b95ca0ff3d95c499722ea513c24180d" translate="yes" xml:space="preserve">
          <source>Underfitting vs. Overfitting</source>
          <target state="translated">과적 합과 과적 합</target>
        </trans-unit>
        <trans-unit id="10dab5fb240281c20bb10ad043cbba82ec3b0bd6" translate="yes" xml:space="preserve">
          <source>Understanding the decision tree structure</source>
          <target state="translated">의사 결정 트리 구조 이해</target>
        </trans-unit>
        <trans-unit id="a381b476a1bdd0042346ffd8d02655a7131aa344" translate="yes" xml:space="preserve">
          <source>Undo the scaling of X according to feature_range.</source>
          <target state="translated">feature_range에 따라 X의 스케일링을 취소하십시오.</target>
        </trans-unit>
        <trans-unit id="11b4a2e4a2b6531b9e75ad23f03f25fd4f8ecde0" translate="yes" xml:space="preserve">
          <source>Uniform weights are used by default.</source>
          <target state="translated">기본적으로 균일 한 가중치가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9421754583ed6d327fe582bef2d0e2d0f17ba0c4" translate="yes" xml:space="preserve">
          <source>Unique class labels.</source>
          <target state="translated">고유 한 클래스 레이블.</target>
        </trans-unit>
        <trans-unit id="6efd4cf40567c19c24a13e3421a6d1109a47da44" translate="yes" xml:space="preserve">
          <source>Uniquely holds the label for each class.</source>
          <target state="translated">각 클래스의 레이블을 고유하게 보유합니다.</target>
        </trans-unit>
        <trans-unit id="78bd370935302db3cfb593c9af76aeb130eb71c4" translate="yes" xml:space="preserve">
          <source>Unit Deviance \(d(y, \hat{y})\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15f758514c6db2ef9033ee5636e4d09d40ce9747" translate="yes" xml:space="preserve">
          <source>Univariate Feature Selection</source>
          <target state="translated">일 변량 피처 선택</target>
        </trans-unit>
        <trans-unit id="820dda4dd874419c514343cc2737763cc18b33d1" translate="yes" xml:space="preserve">
          <source>Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the &lt;code&gt;transform&lt;/code&gt; method:</source>
          <target state="translated">일 변량 기능 선택은 일 변량 통계 테스트를 기반으로 최상의 기능을 선택하여 작동합니다. 추정기의 전처리 단계로 볼 수 있습니다. Scikit-learn은 기능 선택 루틴을 &lt;code&gt;transform&lt;/code&gt; 메소드 를 구현하는 객체로 노출합니다 .</target>
        </trans-unit>
        <trans-unit id="b943e7c2ae0f248f889b02c7d797d243c0d56e6a" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable mode.</source>
          <target state="translated">구성 가능한 모드가있는 일 변량 기능 선택기.</target>
        </trans-unit>
        <trans-unit id="05b44ce5dcc153b8702db1e0eab0c9af1fb62f9c" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable strategy.</source>
          <target state="translated">구성 가능한 전략을 가진 일 변량 기능 선택기.</target>
        </trans-unit>
        <trans-unit id="049ea86cb7534beee7ca7abb3073edcde3e3d399" translate="yes" xml:space="preserve">
          <source>Univariate imputation of missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9b7ebeeb7d99a7c69a9087085473be1721215ee" translate="yes" xml:space="preserve">
          <source>Univariate linear regression tests.</source>
          <target state="translated">일 변량 선형 회귀 테스트.</target>
        </trans-unit>
        <trans-unit id="833fdc74927caa030c0f5f51bade98d55541b93d" translate="yes" xml:space="preserve">
          <source>Unlabeled entries in &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;y&lt;/code&gt; 에서 레이블이없는 항목</target>
        </trans-unit>
        <trans-unit id="d5f2b47c1710490958929f8f01f5858025c133e1" translate="yes" xml:space="preserve">
          <source>Unless otherwise specified, input will be cast to &lt;code&gt;float64&lt;/code&gt;:</source>
          <target state="translated">달리 지정하지 않으면 입력은 &lt;code&gt;float64&lt;/code&gt; 로 캐스트됩니다 .</target>
        </trans-unit>
        <trans-unit id="6027b38892a9b0df12f36988c98a41a4655ff464" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 와 달리 , 벡터의 표현은 구성 요소를 빼지 않고 중첩하여 부가적인 방식으로 얻습니다. 이러한 추가 모델은 이미지와 텍스트를 표현하는 데 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="5817d589292c98298ab95a877d7595e724495088" translate="yes" xml:space="preserve">
          <source>Unlike SVC (based on LIBSVM), LinearSVC (based on LIBLINEAR) does not provide the support vectors. This example demonstrates how to obtain the support vectors in LinearSVC.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f73d55c7488edaa74f7dc85966062a8a5be2b94b" translate="yes" xml:space="preserve">
          <source>Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).</source>
          <target state="translated">대부분의 다른 점수와 달리 R ^ 2 점수는 음수 일 수 있습니다 (실제로 수량 R의 제곱 일 필요는 없음).</target>
        </trans-unit>
        <trans-unit id="fb0dd07f15380472f302743b85f6d7c3f60efb9d" translate="yes" xml:space="preserve">
          <source>Unlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (see below).</source>
          <target state="translated">이전 스케일러와 달리이 스케일러의 중심 및 스케일링 통계는 백분위 수를 기반으로하므로 소수의 매우 큰 한계 값 이상치의 영향을받지 않습니다. 결과적으로 변환 된 피처 값의 결과 범위는 이전 스케일러보다 크고, 더 중요하게는 거의 비슷합니다. 두 피처 모두 변환 된 값의 대부분은 확대 / 축소에서 볼 수 있듯이 [-2, 3] 범위에 있습니다. 그림에서. 특이 치 자체는 여전히 변환 된 데이터에 존재합니다. 별도의 이상치 클리핑이 필요한 경우 비선형 변환이 필요합니다 (아래 참조).</target>
        </trans-unit>
        <trans-unit id="be4091e1f0941887f57bdebf1b1a9b607356f1f1" translate="yes" xml:space="preserve">
          <source>Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation.</source>
          <target state="translated">이전 변환과 달리 정규화는 기능별 변환 대신 샘플 당 변환을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="1e78af54fb2d86af104fdf7cb3b33ae0e42d69f8" translate="yes" xml:space="preserve">
          <source>Unmarried</source>
          <target state="translated">Unmarried</target>
        </trans-unit>
        <trans-unit id="d6efdeaf0fd8663d7b74628a841a2a21988919e0" translate="yes" xml:space="preserve">
          <source>Unregularized graph based semi-supervised learning</source>
          <target state="translated">정규화되지 않은 그래프 기반의 반 감독 학습</target>
        </trans-unit>
        <trans-unit id="27bee227769f6c4dd6bbb550e3dab104adba94bb" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection using Local Outlier Factor (LOF)</source>
          <target state="translated">LOF (Local Outlier Factor)를 사용한 감독되지 않은 이상치 탐지</target>
        </trans-unit>
        <trans-unit id="3cf71ccf2a88f28c3a0cfcb75adfc8979981d7f4" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection using Local Outlier Factor (LOF).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b5353048e77b9864be0e146d8fe78c3034a09d3" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection.</source>
          <target state="translated">감독되지 않은 이상치 탐지.</target>
        </trans-unit>
        <trans-unit id="a2eb50b9e0078078696dd41ce50d788a5cac282f" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="091dbb252c60dafbec16ee540eb9588c7d736a5b" translate="yes" xml:space="preserve">
          <source>Unsupervised learner for implementing neighbor searches.</source>
          <target state="translated">이웃 검색을 구현하기위한 비지도 학습자.</target>
        </trans-unit>
        <trans-unit id="336bcbb510eed89e13ba021c352635cdc0677016" translate="yes" xml:space="preserve">
          <source>Unsupervised learning: seeking representations of the data</source>
          <target state="translated">비지도 학습 : 데이터의 표현 추구</target>
        </trans-unit>
        <trans-unit id="568c5820f9e5362ca266c9125e695403019435a8" translate="yes" xml:space="preserve">
          <source>Unused parameter.</source>
          <target state="translated">사용하지 않은 매개 변수.</target>
        </trans-unit>
        <trans-unit id="207a5be036cc811b3313bce86d86c7d5b4302176" translate="yes" xml:space="preserve">
          <source>Update k means estimate on a single mini-batch X.</source>
          <target state="translated">업데이트 k는 단일 미니 배치 X에 대한 추정치를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="08230ffcab1953eb1050f05815a03e0d48694ae9" translate="yes" xml:space="preserve">
          <source>Update the model with a single iteration over the given data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="718430f889e80a3494a47ee7bec5ca3329673e5e" translate="yes" xml:space="preserve">
          <source>Updated feature-wise means.</source>
          <target state="translated">기능별 수단이 업데이트되었습니다.</target>
        </trans-unit>
        <trans-unit id="4f10d24907ba29eca99bd941c7ead98539a4f5b4" translate="yes" xml:space="preserve">
          <source>Updated feature-wise variances.</source>
          <target state="translated">기능별 차이가 업데이트되었습니다.</target>
        </trans-unit>
        <trans-unit id="693a7de21c7466734e7ffa03c5ae997209e7997d" translate="yes" xml:space="preserve">
          <source>Updated number of seen samples.</source>
          <target state="translated">본 샘플 수를 업데이트했습니다.</target>
        </trans-unit>
        <trans-unit id="410e0e09369f3d862bca36022b47e478be0933f7" translate="yes" xml:space="preserve">
          <source>Updates the model using the data in X as a mini-batch.</source>
          <target state="translated">X의 데이터를 미니 배치로 사용하여 모델을 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="93ec1aa11f1be18275b029134004fd1ab02c997f" translate="yes" xml:space="preserve">
          <source>Upper bound on a uniform noise parameter to be added to the &lt;code&gt;y&lt;/code&gt; values, to satisfy the model&amp;rsquo;s assumption of one-at-a-time computations. Might help with stability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07333ba49211f5c72e60dfd1b7f4f04d69ceed93" translate="yes" xml:space="preserve">
          <source>Upper bound on the highest predicted value (the maximum may still be lower). If not set, defaults to +inf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48b5dd5eaa54e931a34dd1d6396ec1c9d66da80b" translate="yes" xml:space="preserve">
          <source>Urbanowicz R.J., Moore, J.H. &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0: description and evaluation of a scalable learning classifier system&lt;/a&gt;, Evol. Intel. (2015) 8: 89.</source>
          <target state="translated">Urbanowicz RJ, Moore, JH &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0 : 확장 가능한 학습 분류기 시스템&lt;/a&gt; , Evol. 인텔. (2015) 8:89.</target>
        </trans-unit>
        <trans-unit id="fec43ce445f974147bd0eb223a50147e7fb7202d" translate="yes" xml:space="preserve">
          <source>Usage example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="173610cb31251b28e80fadc258036215d99d7128" translate="yes" xml:space="preserve">
          <source>Usage examples:</source>
          <target state="translated">사용 예 :</target>
        </trans-unit>
        <trans-unit id="272998fc40498f57127bf4e7cf71805cd53c9500" translate="yes" xml:space="preserve">
          <source>Use 0 when &lt;code&gt;Y&lt;/code&gt; contains the output of decision_function (classifier). Use 0.5 when &lt;code&gt;Y&lt;/code&gt; contains the output of predict_proba.</source>
          <target state="translated">&lt;code&gt;Y&lt;/code&gt; 에 decision_function (분류기)의 출력이 포함 된 경우 0을 사용하십시오 . &lt;code&gt;Y&lt;/code&gt; 에 predict_proba의 출력이 포함 된 경우 0.5를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="cf1718d68df5d7e88cb5c515bd9f0d9c1cb19546" translate="yes" xml:space="preserve">
          <source>Use &lt;a href=&quot;#optics&quot;&gt;OPTICS&lt;/a&gt; clustering in conjunction with the &lt;code&gt;extract_dbscan&lt;/code&gt; method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7e4377e83d25be8830adecce4de8c2384ca00b7" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;ColumnTransformer&lt;/code&gt; by selecting column by data types</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e21d1ee32a85ffa963b44a044a8fb65d4d276f52" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;ColumnTransformer&lt;/code&gt; by selecting column by names</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e620712d1a8872ff21d8a3d8ca61cf7867c4f8c8" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;min_samples_split&lt;/code&gt; or &lt;code&gt;min_samples_leaf&lt;/code&gt; to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try &lt;code&gt;min_samples_leaf=5&lt;/code&gt; as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While &lt;code&gt;min_samples_split&lt;/code&gt; can create arbitrarily small leaves, &lt;code&gt;min_samples_leaf&lt;/code&gt; guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, &lt;code&gt;min_samples_leaf=1&lt;/code&gt; is often the best choice.</source>
          <target state="translated">사용 &lt;code&gt;min_samples_split&lt;/code&gt; 또는 &lt;code&gt;min_samples_leaf&lt;/code&gt; 를 여러 샘플 분할이 고려 될 제어하여 트리에서 모든 결정을 통보하도록. 매우 작은 숫자는 일반적으로 나무가 과적 합을 의미하는 반면, 큰 숫자는 나무가 데이터를 학습하지 못하게합니다. &lt;code&gt;min_samples_leaf=5&lt;/code&gt; 를 초기 값으로 사용해보십시오 . 샘플 크기가 크게 변하면 부동 소수점 숫자를이 두 매개 변수에서 백분율로 사용할 수 있습니다. &lt;code&gt;min_samples_split&lt;/code&gt; 은 임의로 작은 잎을 만들 수 있지만 min_samples_leaf 는 각 잎이 최소 크기를 가지도록하여 회귀 문제에서 낮은 분산, &lt;code&gt;min_samples_leaf&lt;/code&gt; 잎 노드를 피합니다. 클래스가 적은 분류의 &lt;code&gt;min_samples_leaf=1&lt;/code&gt; 경우 종종 최선의 선택입니다.</target>
        </trans-unit>
        <trans-unit id="3429b333ce93c8bae91a85fe794f080132090825" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;negative_outlier_factor_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;negative_outlier_factor_&lt;/code&gt; 사용</target>
        </trans-unit>
        <trans-unit id="7760fd58346bed0a2b559e055012bd8a21868552" translate="yes" xml:space="preserve">
          <source>Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.</source>
          <target state="translated">Lasso와 함께 SelectFromModel 메타 변환기를 사용하여 Boston 데이터 세트에서 가장 적합한 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="054f12ffb2d5e13349a9508049b7a257d7468dbe" translate="yes" xml:space="preserve">
          <source>Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the diabetes dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71685d673fcc400f8aa4ec7ef3e4a61bf3bbf5f7" translate="yes" xml:space="preserve">
          <source>Use approximate bound as score.</source>
          <target state="translated">대략적인 한계를 점수로 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="400d526bc775314ded26ebb1519045ccc0979588" translate="yes" xml:space="preserve">
          <source>Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.</source>
          <target state="translated">2001 년 Achlioptas의 결과를 재현하려면 밀도 = 1 / 3.0을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="38fb3c866f165060e0d95ec1a873c702ff2c91dc" translate="yes" xml:space="preserve">
          <source>Use only on new data</source>
          <target state="translated">새로운 데이터에만 사용</target>
        </trans-unit>
        <trans-unit id="0eb6d7f6360fc3b257840e6d0ece909142d961e3" translate="yes" xml:space="preserve">
          <source>Use splitting criteria that compute the average reduction across all n outputs.</source>
          <target state="translated">모든 n 출력에 대한 평균 감소를 계산하는 분할 기준을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="d4a5711bd46bd2a4542a66497bb7f3342ea23a7f" translate="yes" xml:space="preserve">
          <source>Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; estimator.</source>
          <target state="translated">Akaike 정보 기준 (AIC), Bayes 정보 기준 (BIC) 및 교차 검증을 사용하여 &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;올가미&lt;/a&gt; 추정기 의 정규화 매개 변수 알파의 최적 값을 선택하십시오 .</target>
        </trans-unit>
        <trans-unit id="35f6c244b8fd2da4794beb17214246bc1c30610f" translate="yes" xml:space="preserve">
          <source>Usecase</source>
          <target state="translated">Usecase</target>
        </trans-unit>
        <trans-unit id="4ea5661d3bd8912bbf2723a42ab4c0cf1ece7994" translate="yes" xml:space="preserve">
          <source>Used during dictionary learning. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bc010bd367bf6cb9590a9c8e8bd3ff37b7e270c" translate="yes" xml:space="preserve">
          <source>Used during randomized svd. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85ae15952ef1b8cedfa1ed0f0a5091e5742f9b4a" translate="yes" xml:space="preserve">
          <source>Used for NMF initialisation (when &lt;code&gt;init&lt;/code&gt; == &amp;lsquo;nndsvdar&amp;rsquo; or &amp;lsquo;random&amp;rsquo;), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15383d0945f438ddc1d77fc1ae0921de1e717777" translate="yes" xml:space="preserve">
          <source>Used for VotingClassifier</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06c1133b6cf5dee9bc1f27b86d9cca1c046fd5c1" translate="yes" xml:space="preserve">
          <source>Used for initialisation (when &lt;code&gt;init&lt;/code&gt; == &amp;lsquo;nndsvdar&amp;rsquo; or &amp;lsquo;random&amp;rsquo;), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4845a789add1f32681ae6a68b7dba0e6bdbe2af" translate="yes" xml:space="preserve">
          <source>Used for initializing the dictionary when &lt;code&gt;dict_init&lt;/code&gt; is not specified, randomly shuffling the data when &lt;code&gt;shuffle&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, and updating the dictionary. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d6d1bb4bf090f9031a078f80f81b5cfa346c5fb" translate="yes" xml:space="preserve">
          <source>Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">내부 캐싱에 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다.</target>
        </trans-unit>
        <trans-unit id="78a755bf17a9d084d2f5bc3468af8892921b3298" translate="yes" xml:space="preserve">
          <source>Used for random shuffling when &lt;code&gt;shuffle&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, during online dictionary learning. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a659f5ae4cfedf0686976ca2f311fa3c53dbc2ce" translate="yes" xml:space="preserve">
          <source>Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">특이 값 분해 및 k- 평균 초기화를 무작위 화하는 데 사용됩니다. int를 사용하여 임의성을 결정 론적으로 만드십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b3bb1fd38ab7b6b511c90e9e3f961817f7b29fe6" translate="yes" xml:space="preserve">
          <source>Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad6bfb2aef10b93bd4ad8fd792d19a69ba0f50a9" translate="yes" xml:space="preserve">
          <source>Used for randomly initializing the dictionary. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ee2dc12a427741bfbfa29dc3b32d1b1d854da3" translate="yes" xml:space="preserve">
          <source>Used for shuffling the data, when &lt;code&gt;shuffle&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04206996c9eca941c8da97d474cf5a147f4b8713" translate="yes" xml:space="preserve">
          <source>Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute &lt;code&gt;named_steps&lt;/code&gt; or &lt;code&gt;steps&lt;/code&gt; to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.</source>
          <target state="translated">파이프 라인의 장착 된 변압기를 캐시하는 데 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다. 캐싱을 활성화하면 피팅하기 전에 변압기 클론이 트리거됩니다. 따라서 파이프 라인에 제공된 변압기 인스턴스를 직접 검사 할 수 없습니다. 파이프 라인 내 추정기를 검사 하려면 &lt;code&gt;named_steps&lt;/code&gt; 또는 &lt;code&gt;steps&lt;/code&gt; 속성을 사용하십시오 . 피팅에 시간이 오래 걸리면 변압기 캐싱이 유리합니다.</target>
        </trans-unit>
        <trans-unit id="b831b0ccc618367ad6990c063d4f6b68c21b54a0" translate="yes" xml:space="preserve">
          <source>Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">트리 계산의 출력을 캐시하는 데 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다.</target>
        </trans-unit>
        <trans-unit id="5b58434a86ea5888954537c341361956c0e8c395" translate="yes" xml:space="preserve">
          <source>Used to determine when to &amp;ldquo;early stop&amp;rdquo;. The fitting process is stopped when none of the last &lt;code&gt;n_iter_no_change&lt;/code&gt; scores are better than the &lt;code&gt;n_iter_no_change - 1&lt;/code&gt; -th-to-last one, up to some tolerance. Only used if early stopping is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9462556717c7577fff4ef47d9f5dcd20523f1516" translate="yes" xml:space="preserve">
          <source>Used to initialize &lt;code&gt;w_init&lt;/code&gt; when not specified, with a normal distribution. Pass an int, for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f19cda342b4acf0d93861aea53bc9a6ea559de82" translate="yes" xml:space="preserve">
          <source>Used to pick randomly the &lt;code&gt;max_features&lt;/code&gt; used at each split. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="605a4bcc1f5d9a0fe3ef69215d05c4963e05e516" translate="yes" xml:space="preserve">
          <source>Used to shuffle the training data, when &lt;code&gt;shuffle&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec2d1021c796c8396406488bdd5b004de6014166" translate="yes" xml:space="preserve">
          <source>Used to specify the norm used in the penalization. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers support only l2 penalties.</source>
          <target state="translated">벌칙에 사용되는 표준을 지정하는 데 사용됩니다. 'newton-cg', 'sag'및 'lbfgs'솔버는 l2 페널티 만 지원합니다.</target>
        </trans-unit>
        <trans-unit id="5860deefa3fd6b4c16483df037c0ad7cd840b1dc" translate="yes" xml:space="preserve">
          <source>Used to specify the norm used in the penalization. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers support only l2 penalties. &amp;lsquo;elasticnet&amp;rsquo; is only supported by the &amp;lsquo;saga&amp;rsquo; solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2cadba66c38ca766b85abc4b1e03568d1fa6dab" translate="yes" xml:space="preserve">
          <source>Used to specify the norm used in the penalization. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers support only l2 penalties. &amp;lsquo;elasticnet&amp;rsquo; is only supported by the &amp;lsquo;saga&amp;rsquo; solver. If &amp;lsquo;none&amp;rsquo; (not supported by the liblinear solver), no regularization is applied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90efd63a8d51063de92a896b88cf64deaa03a244" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27dcb202bde84d98eaa9b02f62b647ecce32aaba" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;shuffle&lt;/code&gt; is True. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8694b55c04737ba18ec4ec98388e9997e16f39ac" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;saga&amp;rsquo; to shuffle the data. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59bf45c1d50e7570f6107bdd3616f32c93f002ed" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo; to shuffle the data. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6070822f60356161845953fee6c5bfe74cf5d59" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;solver='sag'&lt;/code&gt;, &amp;lsquo;saga&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo; to shuffle the data. Note that this only applies to the solver and not the cross-validation generator. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f152cc191601223884f2e892b0e88e2ae399e550" translate="yes" xml:space="preserve">
          <source>Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a789f5832dbb34972e6ab11f85f2125e32dddce" translate="yes" xml:space="preserve">
          <source>Useful for applying a non-linear transformation in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;.</source>
          <target state="translated">회귀 문제에서 비선형 변환을 적용하는 데 유용합니다. 이 변환은 QuantileTransformer와 같은 Transformer로 제공되거나 &lt;code&gt;log&lt;/code&gt; 및 &lt;code&gt;exp&lt;/code&gt; 와 같은 함수 및 역으로 제공 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="74349d111804f80e1a38097923f8879dfbcca7b0" translate="yes" xml:space="preserve">
          <source>Useful for applying a non-linear transformation to the target &lt;code&gt;y&lt;/code&gt; in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="665be16622d5dce4c265443eced33f5309efed0f" translate="yes" xml:space="preserve">
          <source>Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">newton-cg, sag 및 lbfgs 솔버에만 유용합니다. 솔버가 수렴하는 데 걸린 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="1f7081fc6e8837c157dbcac4dfe150624d77daa3" translate="yes" xml:space="preserve">
          <source>Useful only when the solver &amp;lsquo;liblinear&amp;rsquo; is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt;.</source>
          <target state="translated">솔버 'liblinear'가 사용되고 self.fit_intercept가 True로 설정된 경우에만 유용합니다. 이 경우, x는 [x, self.intercept_scaling]이되고, 즉 intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt; 됩니다.</target>
        </trans-unit>
        <trans-unit id="cb4f9242d2c5bef801309a7b11564e9f2917779f" translate="yes" xml:space="preserve">
          <source>Useful tutorials for developing a feel for some of scikit-learn's applications in the machine learning field.</source>
          <target state="translated">머신 러닝 분야의 일부 scikit-learn 응용 프로그램에 대한 느낌을 개발하는 데 유용한 자습서.</target>
        </trans-unit>
        <trans-unit id="bec249e659662f7d5947bf09a1ea1d4a552885b0" translate="yes" xml:space="preserve">
          <source>User Guide</source>
          <target state="translated">사용자 설명서</target>
        </trans-unit>
        <trans-unit id="221a6dc59fa62390ffe53703a42fa985bbe3d0ea" translate="yes" xml:space="preserve">
          <source>Uses &lt;a href=&quot;#sklearn.model_selection.ParameterGrid&quot;&gt;&lt;code&gt;ParameterGrid&lt;/code&gt;&lt;/a&gt; to perform a full parallelized parameter search.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32b4edc9350cb6d93cc0316d635ce26e35fa63d2" translate="yes" xml:space="preserve">
          <source>Uses BLAS GEMM as replacement for numpy.dot where possible to avoid unnecessary copies.</source>
          <target state="translated">불필요한 복사본을 피하기 위해 BLAS GEMM을 numpy.dot 대신 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ac4bd4f4f631e790604905794abbd6ed09d66803" translate="yes" xml:space="preserve">
          <source>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</source>
          <target state="translated">의사 결정 기능 (지원 벡터라고 함)에 훈련 지점의 하위 집합을 사용하므로 메모리 효율성도 높습니다.</target>
        </trans-unit>
        <trans-unit id="4a4b56e0bea50fff706430a1a94289a4e5e03040" translate="yes" xml:space="preserve">
          <source>Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.</source>
          <target state="translated">화이트 박스 모델을 사용합니다. 주어진 상황이 모델에서 관찰 가능한 경우 조건에 대한 설명은 부울 논리에 의해 쉽게 설명됩니다. 대조적으로, (예를 들어, 인공 신경 네트워크에서) 블랙 박스 모델에서, 결과를 해석하기가 더 어려울 수있다.</target>
        </trans-unit>
        <trans-unit id="ced5d25baa7aaf39837296d764096d52eb67f5ca" translate="yes" xml:space="preserve">
          <source>Uses sampling the fourier transform of the kernel characteristic at regular intervals.</source>
          <target state="translated">정기적으로 커널 특성의 푸리에 변환 샘플링을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2c633fc259072170ae02b4cf8b2266258b09ddc5" translate="yes" xml:space="preserve">
          <source>Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).</source>
          <target state="translated">fit (또는 fit_transform)에서 학습 한 어휘 및 문서 빈도 (df)를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="cdea6e25f0fe24b03bf907dbf26253d4f40a11df" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; or &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; enables the &lt;code&gt;predict_proba&lt;/code&gt; method, which gives a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">사용 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 또는 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 가능 &lt;code&gt;predict_proba&lt;/code&gt; 의 확률 추정치 \ (P (Y | X) \)의 벡터 제공 방법에있어서, 당 샘플 \ (X는 \)</target>
        </trans-unit>
        <trans-unit id="4072d118d17ebbe341d060ec8f347bb287543668" translate="yes" xml:space="preserve">
          <source>Using FunctionTransformer to select columns</source>
          <target state="translated">FunctionTransformer를 사용하여 열 선택</target>
        </trans-unit>
        <trans-unit id="af570039f4e6335c176e5a0ced20f61ade37ec58" translate="yes" xml:space="preserve">
          <source>Using KBinsDiscretizer to discretize continuous features</source>
          <target state="translated">KBinsDiscretizer를 사용하여 연속 기능을 분리</target>
        </trans-unit>
        <trans-unit id="58dd6560cd1dd1ff16a956c31444ffff4530a294" translate="yes" xml:space="preserve">
          <source>Using L1 penalization as provided by &lt;code&gt;LinearSVC(loss='l2', penalty='l1',
dual=False)&lt;/code&gt; yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing &lt;code&gt;C&lt;/code&gt; yields a more complex model (more feature are selected). The &lt;code&gt;C&lt;/code&gt; value that yields a &amp;ldquo;null&amp;rdquo; model (all weights equal to zero) can be calculated using &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;l1_min_c&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;LinearSVC(loss='l2', penalty='l1', dual=False)&lt;/code&gt; 에서 제공하는 L1 불이익을 사용하면 스파 스 솔루션이 생성됩니다. 즉, 기능 가중치의 하위 집합 만 0과 다르고 결정 함수에 기여합니다. &lt;code&gt;C&lt;/code&gt; 를 늘리면 더 복잡한 모델이 생성됩니다 (더 많은 기능이 선택됨). &lt;code&gt;C&lt;/code&gt; 의 은 &quot;널 (null)&quot;모델을 산출 값은 (모든 가중치가 0)을 사용하여 계산 될 수 &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;l1_min_c&lt;/code&gt; 를&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="927a462bb89282ccdc19c70530472008783e5ed5" translate="yes" xml:space="preserve">
          <source>Using L1 penalization as provided by &lt;code&gt;LinearSVC(loss='l2', penalty='l1',
dual=False)&lt;/code&gt; yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing &lt;code&gt;C&lt;/code&gt; yields a more complex model (more features are selected). The &lt;code&gt;C&lt;/code&gt; value that yields a &amp;ldquo;null&amp;rdquo; model (all weights equal to zero) can be calculated using &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;l1_min_c&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="757f15a3dfafa1d1e98a6f1f457c43151b4efe0d" translate="yes" xml:space="preserve">
          <source>Using LDA and QDA requires computing the log-posterior which depends on the class priors \(P(y=k)\), the class means \(\mu_k\), and the covariance matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0033ecf2999fee9c8f71baf5ea186698e26a6aa1" translate="yes" xml:space="preserve">
          <source>Using a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; without cache enabled, it is possible to inspect the original instance such as:</source>
          <target state="translated">캐시를 사용하지 않고 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 을 사용하면 다음 과 같은 원래 인스턴스를 검사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8841876aa584e88fcc31f689448303a79af806c4" translate="yes" xml:space="preserve">
          <source>Using a first-order Taylor approximation, the value of \(l\) can be approximated as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b38a6f3576ed137cc4e9a9f59798fb707d4dde4" translate="yes" xml:space="preserve">
          <source>Using a single underlying feature the model learns both the x and y coordinate as output.</source>
          <target state="translated">단일 기본 기능을 사용하여 모델은 x 및 y 좌표를 출력으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="6d5ea28ea8efb863c08e76177dc50acce9324f64" translate="yes" xml:space="preserve">
          <source>Using a small &lt;code&gt;max_features&lt;/code&gt; value can significantly decrease the runtime.</source>
          <target state="translated">작은 &lt;code&gt;max_features&lt;/code&gt; 값을 사용 하면 런타임이 크게 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="07d5b2f6d48114d6bc5306f53b2bbecb12f93559" translate="yes" xml:space="preserve">
          <source>Using a sub-pipeline, the fitted coefficients can be mapped back into the original feature space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03c1a9468c05dba06aec630e70cb3912379c2cb0" translate="yes" xml:space="preserve">
          <source>Using its &lt;code&gt;partial_fit&lt;/code&gt; method on chunks of data fetched sequentially from the local hard drive or a network database.</source>
          <target state="translated">로컬 하드 드라이브 또는 네트워크 데이터베이스에서 순차적으로 페치 된 데이터 청크에 &lt;code&gt;partial_fit&lt;/code&gt; 메소드를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="ee5e8e298a940fdf98e8e52d2f16edd9327907f7" translate="yes" xml:space="preserve">
          <source>Using kernels</source>
          <target state="translated">커널 사용</target>
        </trans-unit>
        <trans-unit id="9cfba89ca182507cccdcf4094af12bc5a4c62801" translate="yes" xml:space="preserve">
          <source>Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary</source>
          <target state="translated">사전으로 인코딩 된 노이즈 측정에서 희소 신호를 복구하기 위해 직교 매칭 추적 사용</target>
        </trans-unit>
        <trans-unit id="24a0ae926e510d4f01c040899e37f954b1d9b717" translate="yes" xml:space="preserve">
          <source>Using pre_dispatch in a producer/consumer situation, where the data is generated on the fly. Note how the producer is first called 3 times before the parallel loop is initiated, and then called to generate new data on the fly:</source>
          <target state="translated">생산자 / 소비자 상황에서 pre_dispatch를 사용하여 데이터가 즉시 생성됩니다. 병렬 루프가 시작되기 전에 생산자를 처음 3 번 호출 한 다음 새 데이터를 즉시 생성하기 위해 호출되는 방법에 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="efd1182f39233190c4734b5ce34b9cfcf1bbe0bb" translate="yes" xml:space="preserve">
          <source>Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</source>
          <target state="translated">t-SNE 사용. 기계 학습 연구 저널 9 : 2579-2605, 2008.</target>
        </trans-unit>
        <trans-unit id="52758be5ab8f6ab028b2bf9ad6db5d2b69896663" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;TfidfTransformer&lt;/code&gt;&amp;rsquo;s default settings, &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as</source>
          <target state="translated">은 Using &lt;code&gt;TfidfTransformer&lt;/code&gt; 의 기본 설정을 &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; , 용어 빈도, 기간이 주어진 문서의 발생 횟수, IDF 구성 요소에 곱하는 이것은 다음과 같이 계산됩니다</target>
        </trans-unit>
        <trans-unit id="95d62a973e97428e1fb3d7b86f3a392143b00b8c" translate="yes" xml:space="preserve">
          <source>Using the GraphicalLasso estimator to learn a covariance and sparse precision from a small number of samples.</source>
          <target state="translated">GraphicalLasso 추정기를 사용하여 적은 수의 샘플에서 공분산 및 희소 정밀도를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="f9e94a3c6c72e38b7c72fe500879db6b6bb31872" translate="yes" xml:space="preserve">
          <source>Using the Iris dataset, we can construct a tree as follows:</source>
          <target state="translated">Iris 데이터 셋을 사용하여 다음과 같이 트리를 구성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="99fdc1f5bc1cbf237a61e42fe43157752d29d604" translate="yes" xml:space="preserve">
          <source>Using the Poisson loss with a log-link can correct these problems and lead to a well-calibrated linear model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f990799abb31a15673ef02f50b5506399075290a" translate="yes" xml:space="preserve">
          <source>Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:</source>
          <target state="translated">예상 값을 사용하여 조정 된 상호 정보는 조정 된 랜드 인덱스와 유사한 형식을 사용하여 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="525fdffa79943fda791ea83f2bc571cbb29e45c1" translate="yes" xml:space="preserve">
          <source>Using the naive conditional independence assumption that</source>
          <target state="translated">순진한 조건부 독립성 가정을 사용하여</target>
        </trans-unit>
        <trans-unit id="ded2353583b49d229cc248063161251fcd75da59" translate="yes" xml:space="preserve">
          <source>Using the prediction pipeline in a grid search</source>
          <target state="translated">그리드 검색에서 예측 파이프 라인 사용</target>
        </trans-unit>
        <trans-unit id="4b65de55e2dd198ac6e2aecd67614d2f3e1d68d9" translate="yes" xml:space="preserve">
          <source>Using the results of the previous exercises and the &lt;code&gt;cPickle&lt;/code&gt; module of the standard library, write a command line utility that detects the language of some text provided on &lt;code&gt;stdin&lt;/code&gt; and estimate the polarity (positive or negative) if the text is written in English.</source>
          <target state="translated">이전 연습의 결과와 표준 라이브러리 의 &lt;code&gt;cPickle&lt;/code&gt; 모듈을 사용하여 &lt;code&gt;stdin&lt;/code&gt; 에 제공된 일부 텍스트의 언어를 감지 하고 텍스트가 영어로 작성된 경우 극성 (양수 또는 음수)을 추정 하는 명령 행 유틸리티를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="271df6087c1c40487d3dd610dcce2afcb5a005f8" translate="yes" xml:space="preserve">
          <source>Using this modification, the tf-idf of the third term in document 1 changes to 1.8473:</source>
          <target state="translated">이 수정을 사용하면 문서 1의 세 번째 용어 인 tf-idf가 1.8473으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="a81923715d05cfae1b7290021e8d9f8915c03011" translate="yes" xml:space="preserve">
          <source>Usually the Normalized Discounted Cumulative Gain (NDCG, computed by ndcg_score) is preferred.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35033b7b1c0300bd76803da2e755fdbe07a7c28b" translate="yes" xml:space="preserve">
          <source>Utilities from joblib:</source>
          <target state="translated">joblib의 유틸리티 :</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="8d1950c14bc870de437b37f806aaa51c635a9ec3" translate="yes" xml:space="preserve">
          <source>V measure</source>
          <target state="translated">V 측정</target>
        </trans-unit>
        <trans-unit id="a6ed7787c295565530f8c589d9ab12370f5f5b3d" translate="yes" xml:space="preserve">
          <source>V or VI</source>
          <target state="translated">V 또는 VI</target>
        </trans-unit>
        <trans-unit id="e659ac0cd03fda8c2776727471548e055d6ecaa7" translate="yes" xml:space="preserve">
          <source>V-Measure (NMI with arithmetic mean option.)</source>
          <target state="translated">V- 측정 (산술 평균 옵션이있는 NMI)</target>
        </trans-unit>
        <trans-unit id="893c35d89ea6a5c89935fd8eeed462af4410524f" translate="yes" xml:space="preserve">
          <source>V-Measure is furthermore symmetric: swapping &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;label_pred&lt;/code&gt; will give the same score. This does not hold for homogeneity and completeness. V-Measure is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the arithmetic averaging method.</source>
          <target state="translated">또한 V-Measure는 대칭입니다. &lt;code&gt;labels_true&lt;/code&gt; 와 &lt;code&gt;label_pred&lt;/code&gt; 를 바꾸면 동일한 점수가 부여됩니다. 이것은 동질성과 완전성을 유지하지 않습니다. V-Measure는 산술 평균법 으로 &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="47faeee4990a814efec079e593cccd036ad6778a" translate="yes" xml:space="preserve">
          <source>V-measure cluster labeling given a ground truth.</source>
          <target state="translated">확실한 사실을 고려한 V- 측정 클러스터 라벨링.</target>
        </trans-unit>
        <trans-unit id="a4fd517acce42be80ab9791260ae88f5cbdf1a52" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;Spam filtering with Naive Bayes &amp;ndash; Which Naive Bayes?&lt;/a&gt; 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis, I. Androutsopoulos 및 G. Paliouras (2006). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;Naive Bayes를 사용한 스팸 필터링 &amp;ndash; 어떤 Naive Bayes? &lt;/a&gt;3 차 회의 이메일 및 스팸 방지 (CEAS)</target>
        </trans-unit>
        <trans-unit id="942786415758a60976a047b84669d53dbc12ecc2" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with naive Bayes &amp;ndash; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis, I. Androutsopoulos 및 G. Paliouras (2006). 순진한 베이로 스팸 필터링 &amp;ndash; 어떤 순진한 베이? 3 차 회의 이메일 및 스팸 방지 (CEAS)</target>
        </trans-unit>
        <trans-unit id="a4d9f3d16f166bfe390c3f0be94b7a7169e9ff69" translate="yes" xml:space="preserve">
          <source>Valid &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; representations for &lt;code&gt;type_of_target&lt;/code&gt; (&lt;code&gt;y&lt;/code&gt;) are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9c007aafb4123183734854c6075f37be2b0eaac" translate="yes" xml:space="preserve">
          <source>Valid &lt;code&gt;type_of_target&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca4c1a38268237e7698432accf1a9a5a48b6fb08" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_distances.</source>
          <target state="translated">pairwise_distances에 유효한 메트릭입니다.</target>
        </trans-unit>
        <trans-unit id="26700c8a25eea6fd902a0bb4963f14783c982650" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_kernels</source>
          <target state="translated">pairwise_kernels에 유효한 메트릭</target>
        </trans-unit>
        <trans-unit id="850c962c3f063b586578621ee12bbb84d6f38bb7" translate="yes" xml:space="preserve">
          <source>Valid options:</source>
          <target state="translated">유효한 옵션 :</target>
        </trans-unit>
        <trans-unit id="493108de26f61e3b76acfe363b14fa409e1fdc9a" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with &lt;code&gt;get_params()&lt;/code&gt;.</source>
          <target state="translated">유효한 매개 변수 키는 &lt;code&gt;get_params()&lt;/code&gt; 로 나열 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1a791ec45bdf21e7b9592679ed5472b3c5ab8093" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with get_params().</source>
          <target state="translated">유효한 매개 변수 키는 get_params ()로 나열 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90d9fefcb561047bbbd742b13c3608c6fcf1e657" translate="yes" xml:space="preserve">
          <source>Valid values for metric are:</source>
          <target state="translated">메트릭의 유효한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47edea5ff3c24dcb16dc15647447ec5c4a9d77c2" translate="yes" xml:space="preserve">
          <source>Valid values for metric are::</source>
          <target state="translated">메트릭의 유효한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="21601426cfbdf9a26553c2613d8f13b5c8a0699e" translate="yes" xml:space="preserve">
          <source>Validate scalar parameters type and value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec59a2a93f21b1aa3fbc16cd26b3b2dbab6fa78b" translate="yes" xml:space="preserve">
          <source>Validation curve.</source>
          <target state="translated">검증 곡선.</target>
        </trans-unit>
        <trans-unit id="675b8482a7f9f38fa965a1efe10c6a6ee6ec5cdb" translate="yes" xml:space="preserve">
          <source>Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations. This can also prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge.</source>
          <target state="translated">피팅하는 동안 커널 행렬의 대각선에 추가 된 값입니다. 값이 클수록 관측치의 노이즈 수준이 증가합니다. 또한 계산 된 값이 양의 한정 행렬을 형성하도록하여 피팅하는 동안 잠재적 인 수치 문제를 방지 할 수 있습니다. 어레이가 전달되면 피팅에 사용되는 데이터와 동일한 수의 항목이 있어야하며 데이터 포인트 종속 노이즈 레벨로 사용됩니다. 이것은 c = alpha로 WhiteKernel을 추가하는 것과 같습니다. 노이즈 레벨을 매개 변수로 직접 지정할 수 있도록하는 것은 주로 편의성과 Ridge와의 일관성을위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="cdffc29f88adeffd4bcff100fb7aa53a66956d52" translate="yes" xml:space="preserve">
          <source>Value for numerical stability in adam. Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">아담의 수치 안정성 값. solver = 'adam'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="c6350d6ef6528ee88ef12a12465bebefd1891dd8" translate="yes" xml:space="preserve">
          <source>Value of the pseudo-likelihood (proxy for likelihood).</source>
          <target state="translated">유사 가능성의 값 (가능성에 대한 프록시).</target>
        </trans-unit>
        <trans-unit id="7a1b051ea7b4e31aba2ba2519e8c31958f649214" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
