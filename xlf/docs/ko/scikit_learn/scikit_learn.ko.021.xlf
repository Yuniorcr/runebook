<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="aaa6add3e7cf1ec084105f029a9e3050a4d18bde" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. &amp;lsquo;binary_crossentropy&amp;rsquo; (also known as logistic loss) is used for binary classification and generalizes to &amp;lsquo;categorical_crossentropy&amp;rsquo; for multiclass classification. &amp;lsquo;auto&amp;rsquo; will automatically choose either loss depending on the nature of the problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d058a2ad82a18d6b6a9ed5c1f1cacf9ddbfdb363" translate="yes" xml:space="preserve">
          <source>The loss function to use in the boosting process. Note that the &amp;ldquo;least squares&amp;rdquo; and &amp;ldquo;poisson&amp;rdquo; losses actually implement &amp;ldquo;half least squares loss&amp;rdquo; and &amp;ldquo;half poisson deviance&amp;rdquo; to simplify the computation of the gradient. Furthermore, &amp;ldquo;poisson&amp;rdquo; loss internally uses a log-link and requires &lt;code&gt;y &amp;gt;= 0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">각 부스팅 반복 후 가중치를 업데이트 할 때 사용할 손실 함수입니다.</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">사용 된 손실 함수는 이항 이탈입니다. 축소 ( &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ) 를 통한 정규화 는 성능을 크게 향상시킵니다. 수축과 결합하여 확률 적 그라디언트 부스팅 ( &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; )은 배깅을 통해 분산을 줄임으로써보다 정확한 모델을 생성 할 수 있습니다. 수축이없는 서브 샘플링은 일반적으로 좋지 않습니다. 분산을 줄이는 또 다른 전략은 랜덤 포레스트의 랜덤 스플릿과 유사한 기능을 서브 샘플링하는 것입니다 ( &lt;code&gt;max_features&lt;/code&gt; 매개 변수 를 통해 ).</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">프로파일의 하위 부분은 데이터의 구조화 된 신호 부분으로 간주 될 수있는 반면 테일은 적은 수의 선형 구성 요소 (단일 벡터)로 요약 할 수없는 데이터의 노이즈 부분으로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="798401f91692e498e70c4cd9edead7945caa2484" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;alpha&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;alpha&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d36bb18704838afeb9c91e3cf29995f34fe370d6" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;gamma&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;gamma&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05c76bb31dceb610d87492d4b267252a46764fae" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;length_scale&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;length_scale&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="971e4cf6ca3d178a182a1d45d26bbec9249f02e4" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;noise_level&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;noise_level&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f18d5554a9b373b29b2cfd43681990a3c136384e" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;periodicity&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;periodicity&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c4988467c8a93656299039dfe058cd67baf3941" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &amp;lsquo;sigma_0&amp;rsquo;. If set to &amp;ldquo;fixed&amp;rdquo;, &amp;lsquo;sigma_0&amp;rsquo; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb2363bfa3c4e13746c6b4233da08b1f95f83117" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on &lt;code&gt;constant_value&lt;/code&gt;. If set to &amp;ldquo;fixed&amp;rdquo;, &lt;code&gt;constant_value&lt;/code&gt; cannot be changed during hyperparameter tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">알파의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">constant_value의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">감마의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">l의 상한과 하한</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">length_scale의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">noise_level의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">주기성의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="ca7705b5107aa6db64b47bb244d2b0d4033a135a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on the parameter. If n_elements&amp;gt;1, a pair of 1d array with n_elements each may be given alternatively. If the string &amp;ldquo;fixed&amp;rdquo; is passed as bounds, the hyperparameter&amp;rsquo;s value cannot be changed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">추출 될 상이한 n- 그램에 대한 n- 값 범위의 하한 및 상한. min_n &amp;lt;= n &amp;lt;= max_n과 같은 n의 모든 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="1a06e2593892f38e2c7f98d2aaf16f88ee8f121a" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfc6e4d4438c9aec5ef09350cb9a99015ebb32f0" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n &amp;lt;= n &amp;lt;= max_n will be used. For example an &lt;code&gt;ngram_range&lt;/code&gt; of &lt;code&gt;(1, 1)&lt;/code&gt; means only unigrams, &lt;code&gt;(1, 2)&lt;/code&gt; means unigrams and bigrams, and &lt;code&gt;(2, 2)&lt;/code&gt; means only bigrams. Only applies if &lt;code&gt;analyzer is not callable&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">사용 된 하한 및 상한 백분위 수는 &lt;code&gt;grid&lt;/code&gt; 대한 극값을 만듭니다 . &lt;code&gt;X&lt;/code&gt; 가 없음이 아닌 경우에만 .</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">PDP 축의 극한 값을 생성하는 데 사용되는 하한 및 상한 백분위 수입니다.</target>
        </trans-unit>
        <trans-unit id="8e5bebe8622375d11e17f863f40a564811e9afd3" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes. Must be in [0, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9186aa58b7ce14e687d5b2e114470d0ce75e6d07" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">왼쪽 아래 그림은 단일 의사 결정 트리의 예상 평균 제곱 오차의 점별 분해를 나타냅니다. 편차 항 (파란색)이 낮고 분산이 클 때 (녹색) 확인합니다. 또한 예상대로 일정하고 약 &lt;code&gt;0.01&lt;/code&gt; 인 것으로 보이는 오류의 노이즈 부분을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">낮을수록 좋습니다.</target>
        </trans-unit>
        <trans-unit id="71c561a637b01dde1b3c6166ce3bd23db956f72f" translate="yes" xml:space="preserve">
          <source>The machine-learning pipeline</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 잘못된 시스템의 경우이 값을 늘리십시오.</target>
        </trans-unit>
        <trans-unit id="d9100545597c68bda47e361ae3fa5acd8ebd846b" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48832baaa50854fed8a3f45f6240f92c4370d802" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is &lt;code&gt;np.finfo(np.float64).eps&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 잘못된 시스템의 경우이 값을 늘리십시오. 일부 반복 최적화 기반 알고리즘의 'tol'매개 변수와 달리이 매개 변수는 최적화의 허용 오차를 제어하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 좋지 않은 시스템의 경우이를 늘리십시오. 일부 반복 최적화 기반 알고리즘 의 &lt;code&gt;tol&lt;/code&gt; 매개 변수 와 달리이 매개 변수는 최적화의 허용 오차를 제어하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="638a11bb66c553576b68621fdefa92568dded5ac" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a355ae348914d14284e933a2a39b838c82d13102" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, &lt;code&gt;np.finfo(np.float).eps&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 에&lt;/a&gt; 대한 요인 분석의 주요 장점 은 입력 공간의 모든 방향에 대한 분산을 독립적으로 모델링 할 수 있다는 것입니다 (이 분산 잡음).</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">레이블이 지정되지 않은 데이터에서 가우시안 혼합 모델을 학습하는 데있어 가장 어려운 점은 일반적으로 어떤 잠재적 구성 요소에서 어떤 지점이 왔는지 알 수 없다는 것입니다. 포인트들). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;기대 최대화&lt;/a&gt; 는 반복 프로세스를 통해이 문제를 해결하기 위해 잘 알려진 통계 알고리즘입니다. 첫 번째는 랜덤 성분 (데이터 포인트에 무작위로 중심을 두거나 k- 평균에서 배운 것, 또는 원점을 따라 정상적으로 분포 됨)을 가정하고 각 포인트에 대해 모델의 각 컴포넌트에 의해 생성 될 확률을 계산합니다. 그런 다음 할당이 주어진 데이터의 가능성을 최대화하기 위해 매개 변수를 조정합니다. 이 과정을 반복하면 항상 지역 최적으로 수렴됩니다.</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">주요 문서. 여기에는 모든 알고리즘에 대한 자세한 설명과 적용 방법이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">선호도 전파의 주요 단점은 복잡성입니다. 알고리즘은 \ (O (N ^ 2 T) \) 차수의 시간 복잡성을 가지며, 여기서 \ (N \)는 샘플 수이고 \ (T \)는 수렴까지의 반복 횟수입니다. 또한 밀도가 높은 유사성 매트릭스를 사용하는 경우 메모리 복잡도는 \ (O (N ^ 2) \) 순서이지만 스파 스 유사성 매트릭스를 사용하는 경우에는 감소 할 수 있습니다. 따라서 선호도 전파는 중소 규모의 데이터 집합에 가장 적합합니다.</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">예측 대기 시간에 영향을 미치는 주요 요인은</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">주요 관찰 사항은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="36a91e6ee1a9ee310e3b9c8a52ba666f014296cb" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=None&lt;/code&gt; (always considering all features instead of a random subset) for regression problems, and &lt;code&gt;max_features=&quot;sqrt&quot;&lt;/code&gt; (using a random subset of size &lt;code&gt;sqrt(n_features)&lt;/code&gt;) for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">이러한 방법을 사용할 때 조정해야 할 주요 매개 변수는 &lt;code&gt;n_estimators&lt;/code&gt; 및 &lt;code&gt;max_features&lt;/code&gt; 입니다. 전자는 숲의 나무 수입니다. 클수록 클수록 좋지만 계산하는 데 시간이 오래 걸립니다. 또한 중요한 수의 나무를 넘어서는 결과는 훨씬 더 나아지지 않습니다. 후자는 노드를 분할 할 때 고려해야 할 기능의 임의 하위 집합 크기입니다. 낮을수록 분산 감소는 커지지 만 바이어스 증가는 커집니다. 실증 좋은 기본 값은 &lt;code&gt;max_features=n_features&lt;/code&gt; 회귀 문제에 대한, 그리고 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 분류 작업 ( &lt;code&gt;n_features&lt;/code&gt; 데이터의 기능 수입니다). &lt;code&gt;max_depth=None&lt;/code&gt; 을 &lt;code&gt;min_samples_split=2&lt;/code&gt; 와 함께 설정하면 (즉, 트리를 완전히 개발할 때) 좋은 결과를 얻을 수 있습니다 . 이러한 값은 일반적으로 최적이 아니며 많은 RAM을 소비하는 모델이 될 수 있습니다. 최상의 매개 변수 값은 항상 교차 검증되어야합니다. 또한 임의 포리스트에서는 기본적으로 부트 스트랩 샘플이 사용되며 ( &lt;code&gt;bootstrap=True&lt;/code&gt; ) 추가 트리의 기본 전략은 전체 데이터 세트 ( &lt;code&gt;bootstrap=False&lt;/code&gt; ) 를 사용하는 것 입니다. 부트 스트랩 샘플링을 사용할 때 일반화 정확도는 왼쪽 또는 가방 외부 샘플에서 추정 할 수 있습니다. &lt;code&gt;oob_score=True&lt;/code&gt; 를 설정하여 활성화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">t-SNE의 주요 목적은 고차원 데이터의 시각화입니다. 따라서 데이터가 2 차원 또는 3 차원으로 임베드 될 때 가장 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">랜덤 프로젝션의 효율성에 대한 주요 이론적 결과는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (Wikipedia 인용)입니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 의 주요 사용법은 데이터 포인트 간의 GP 공분산을 계산하는 것입니다. 이를 위해 커널의 &lt;code&gt;__call__&lt;/code&gt; 메소드를 호출 할 수 있습니다. 이 방법은 2d 배열 X에있는 모든 데이터 쌍 쌍의 &quot;자동 공분산&quot;을 계산하거나 2d 배열 X에있는 데이터 지점과 2d 배열 Y에있는 데이터 지점의 모든 조합의 &quot;교차 공분산&quot;을 계산하는 데 사용할 수 있습니다. 다음 커널은 모든 커널 k에 적용됩니다 ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; 제외 ). &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4418da68cc8e1590c7a6e820d42fa84b7f1fff49" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; 커널 의 주요 사용 사례 는 신호의 노이즈 구성 요소를 설명하는 합계 커널의 일부입니다. 매개 변수 \ (noise \ _level \)를 조정하면 노이즈 수준을 추정 할 수 있습니다. 다음과 같이 정의됩니다 .e</target>
        </trans-unit>
        <trans-unit id="9e4c246aa3bcb240193dfa5bc5ce95df373e50e0" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed. The parameter noise_level equals the variance of this noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">이 커널의 주요 사용 사례는 신호의 노이즈 구성 요소를 설명하는 합 커널의 일부입니다. 해당 파라미터를 튜닝하면 노이즈 레벨을 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">SGD의 주요 장점은 효율성이며, 이는 훈련 예제 수에서 기본적으로 선형입니다. X가 크기 (n, p)의 행렬이면 \ (O (kn \ bar p) \)의 비용이 듭니다. 여기서 k는 반복 횟수 (에포크)이고 \ (\ bar p \)는 평균입니다. 샘플 당 0이 아닌 속성의 수</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">scikit-learn에서 사용 가능한 매니 폴드 학습 구현은 아래에 요약되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="97fe69cbbfc59d33068b7f1700f458b4fb09dc06" translate="yes" xml:space="preserve">
          <source>The mapping from the value \(F_M(x_i)\) to a class or a probability is loss-dependent. For the deviance (or log-loss), the probability that \(x_i\) belongs to the positive class is modeled as \(p(y_i = 1 | x_i) = \sigma(F_M(x_i))\) where \(\sigma\) is the sigmoid function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">매핑은 커널 값에 대한 Monte Carlo 근사에 의존합니다. &lt;code&gt;fit&lt;/code&gt; (가) 반면 기능 수행 몬테 카를로 샘플링 &lt;code&gt;transform&lt;/code&gt; 방법을 수행하는 데이터의 매핑. 프로세스의 고유 한 무작위성으로 인해 결과는 &lt;code&gt;fit&lt;/code&gt; 함수 에 대한 다른 호출마다 다를 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">선택된 지형지 물의 마스크입니다.</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">수학적 공식은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">매트릭스</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">매트릭스 차원.</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">공분산 행렬의 역행렬은 종종 정밀 행렬이라고도하며 부분 상관 행렬에 비례합니다. 부분적인 독립 관계를 제공합니다. 즉, 두 피처가 다른 피처에 대해 조건부로 독립적 인 경우 정밀 행렬의 해당 계수는 0이됩니다. 이것이 희소 정밀 행렬을 추정하는 것이 합리적입니다. 공분산 행렬의 추정은 데이터로부터 독립 관계를 학습함으로써 더 잘 조절됩니다. 이를 &lt;em&gt;공분산 선택이라고&lt;/em&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">특징의 행렬. 여기서 NP는 입력 조합에서 생성 된 다항식 특징의 수입니다.</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">매트릭스.</target>
        </trans-unit>
        <trans-unit id="9e68c52b3ee0713ce10cfb911b2833fc7e9a1df1" translate="yes" xml:space="preserve">
          <source>The maximal number of iterations for the solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">각 나무의 최대 깊이. None이면 모든 나뭇잎이 순수하거나 모든 나뭇잎에 min_samples_split보다 적은 샘플이 포함될 때까지 노드가 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="826fe580f0140d0a1b7e9876f557d09be8bbd041" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn&amp;rsquo;t constrained by default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">표현의 최대 깊이. None이면 트리가 완전히 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="f5406d9d68ce630037091594fec01e2950e41c42" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">나무의 최대 깊이. None이면 모든 나뭇잎이 순수하거나 모든 나뭇잎에 min_samples_split보다 적은 샘플이 포함될 때까지 노드가 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="a55adc01e4f423a3103fea57c0a2cfa41d8471f9" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. By default it assumes the same value as &lt;code&gt;max_eps&lt;/code&gt;. Used only when &lt;code&gt;cluster_method='dbscan'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2785d22ac4386c9273ddac709630eaca73ae74f0" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. Default value of &lt;code&gt;np.inf&lt;/code&gt; will identify clusters across all scales; reducing &lt;code&gt;max_eps&lt;/code&gt; will result in shorter run times.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86a132d622f603d09919856c8916a271668d069a" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">두 샘플 사이의 최대 거리는 동일한 이웃에서와 같이 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="2d03a3cd1f8dea91e35b793f66d9300ac80fe58d" translate="yes" xml:space="preserve">
          <source>The maximum number of bins to use for non-missing values. Before training, each feature of the input array &lt;code&gt;X&lt;/code&gt; is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than &lt;code&gt;max_bins&lt;/code&gt; bins. In addition to the &lt;code&gt;max_bins&lt;/code&gt; bins, one more bin is always reserved for missing values. Must be no larger than 255.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78f8fd023fb60eb04dc691f81a2e17712a336e66" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axes or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78e415aabcf40a1b66551486961973263113520d" translate="yes" xml:space="preserve">
          <source>The maximum number of columns in the grid plot. Only active when &lt;code&gt;ax&lt;/code&gt; is a single axis or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">백엔드 = &quot;멀티 프로세싱&quot;인 경우 Python 작업자 프로세스 수 또는 백엔드 = &quot;스레딩&quot;인 경우 스레드 풀의 크기와 같이 동시에 실행되는 최대 작업 수 -1이면 모든 CPU가 사용됩니다. 1이 주어지면 병렬 컴퓨팅 코드가 전혀 사용되지 않으므로 디버깅에 유용합니다. -1 미만의 n_jobs에는 (n_cpus + 1 + n_jobs)가 사용됩니다. 따라서 n_jobs = -2의 경우 하나를 제외한 모든 CPU가 사용됩니다. n_jobs에 대한 다른 값을 설정하는 parallel_backend 컨텍스트 관리자에서 호출이 수행되지 않으면 n_jobs = 1 (순차 실행)로 해석되는 'unset'에 대한 마커가 없습니다.</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">부스팅이 종료되는 최대 추정량입니다. 완벽하게 맞는 경우 학습 절차가 일찍 중단됩니다.</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">선택한 점수가 &lt;code&gt;threshold&lt;/code&gt; 하는 최대 기능 수입니다 . &lt;code&gt;threshold&lt;/code&gt; 을 비활성화 하고 &lt;code&gt;max_features&lt;/code&gt; 기반으로 만 선택 하려면 &lt;code&gt;threshold=-np.inf&lt;/code&gt; 를 설정 하십시오 .</target>
        </trans-unit>
        <trans-unit id="305b61ba0832c48af4c31e220047663d92a85155" translate="yes" xml:space="preserve">
          <source>The maximum number of features to select. To only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">최대 반복 횟수</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">예측 동안 후부를 근사화하기위한 뉴턴 방법의 최대 반복 횟수입니다. 값이 작을수록 결과가 나빠질수록 계산 시간이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">최대 반복 횟수는 일반적으로 충분히 높으며 튜닝이 필요하지 않습니다. 최적화는 초기 과장 단계와 최종 최적화의 두 단계로 구성됩니다. 과장 초기에 원래의 공간에서의 합동 확률은 주어진 요인과의 곱셈에 의해 인위적으로 증가합니다. 요인이 클수록 데이터의 자연 군집간에 간격이 더 커집니다. 계수가 너무 높으면이 단계에서 KL 분기가 증가 할 수 있습니다. 일반적으로 튜닝 할 필요는 없습니다. 중요한 매개 변수는 학습 속도입니다. 그래디언트 디센트가 너무 낮 으면 로컬 최소값이 나빠질 수 있습니다. 너무 높으면 최적화 중에 KL 발산이 증가합니다. Laurens van der Maaten의 FAQ에서 더 많은 팁을 찾을 수 있습니다 (참고 자료 참조). 마지막 매개 변수 인 각도는 성능과 정확도 간의 균형입니다.각도가 클수록 단일 지점으로 더 큰 영역을 근사 할 수있어 속도는 향상되지만 정확도는 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="ae7ecafb1fbb8659823714d61be48b0de277df78" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, &lt;code&gt;n_classes&lt;/code&gt; trees per iteration are built.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d7badc19d2ae32c74e6847ba0481766700bc0d4" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations of the boosting process, i.e. the maximum number of trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">실행할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="828c8485a088b2e8fca34b650a5b350287a8d14c" translate="yes" xml:space="preserve">
          <source>The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7623dada54053128bc10f5932064e15f34e6e533" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3d89c5f2bce98870adcd2b6143612a20116ca64" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.Perceptron.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcd0f20b2b6a22d8d1fd2812b58e7d3f318eed78" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDClassifier.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="728ec87f7057b93a5bbf338c5362cf41c1855cde" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;a href=&quot;#sklearn.linear_model.SGDRegressor.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd38afe4aeee3f58ba57bcb27ec36aa15fe40a58" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">훈련 데이터를 통과하는 최대 패스 수 (일명 에포크). &lt;code&gt;partial_fit&lt;/code&gt; 이 아니라 &lt;code&gt;fit&lt;/code&gt; 메소드 의 동작에만 영향을 미칩니다 . 기본값은 5입니다. 0.21에서 또는 tol이 None이 아닌 경우 기본값은 1000입니다.</target>
        </trans-unit>
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="translated">추출 할 이미지 당 최대 패치 수입니다. max_patches가 (0, 1)의 부동 소수점 인 경우 총 패치 수의 비율을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="52d2f42a039085572a1dcc0b5530d4df0b8b6859" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If &lt;code&gt;max_patches&lt;/code&gt; is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="translated">추출 할 최대 패치 수입니다. max_patches가 0과 1 사이의 부동 소수점 인 경우 총 패치 수의 비율로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="translated">교차 검증에서 잔차를 계산하는 데 사용되는 경로의 최대 포인트 수</target>
        </trans-unit>
        <trans-unit id="36c1ffc6cd6501f0a66093a4a420f28ce4311341" translate="yes" xml:space="preserve">
          <source>The maximum valid value the parameter can take. If None (default) it is implied that the parameter does not have an upper bound.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="translated">전체 데이터 세트의 평균 및 경험적 공분산으로, 데이터 세트에 특이 치가있는 즉시 분류됩니다.</target>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="translated">좋은 것으로 알려진 관측치의 평균과 경험적 공분산. 이것은 &quot;완벽한&quot;MCD 추정으로 간주 될 수 있으므로이 경우와 비교하여 구현을 신뢰할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="24b29efa5c5ad7917d67d95c19c0b6440c20e3d8" translate="yes" xml:space="preserve">
          <source>The mean claim amount or severity (&lt;code&gt;AvgClaimAmount&lt;/code&gt;) can be empirically shown to follow approximately a Gamma distribution. We fit a GLM model for the severity with the same features as the frequency model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="translated">각 혼합 성분의 평균.</target>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="translated">다차원 정규 분포의 평균입니다. None 인 경우 원점 (0, 0,&amp;hellip;)을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="bde1358a0967a15325e4e7cc6fcf95bc7ef0f700" translate="yes" xml:space="preserve">
          <source>The mean over features. Only set if &lt;code&gt;self.whiten&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="translated">각 구간의 평균 예측 확률입니다.</target>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="translated">따라서 점수 추정치의 평균 점수와 95 % 신뢰 구간은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fb645170ce6174deae19b9d9f752a59168ce3c91" translate="yes" xml:space="preserve">
          <source>The mean squared error (&lt;code&gt;power=0&lt;/code&gt;) is very sensitive to the prediction difference of the second point,:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="translated">교육 세트의 각 기능에 대한 평균값입니다. 같음 &lt;code&gt;None&lt;/code&gt; &lt;code&gt;with_mean=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="translated">이러한 특징의 평균, 표준 오차 및 &quot;최악의&quot;또는 가장 큰 (3 개의 가장 큰 값의 평균)을 각 이미지에 대해 계산하여 30 개의 특징을 생성했습니다. 예를 들어, 필드 3은 평균 반경, 필드 13은 반경 SE, 필드 23은 최악 반경입니다.</target>
        </trans-unit>
        <trans-unit id="8d0066f714eed86dea1c0b26b34c2da2cae60854" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="translated">나무에 대한 관측치의 정규성 측정은이 관측치를 포함하는 잎의 깊이이며,이 점을 분리하는 데 필요한 분할 수와 같습니다. 리프에 여러 관측치 n_left가있는 경우 n_left 샘플 격리 트리의 평균 경로 길이가 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="translated">손상되지 않은 새로운 데이터에 대한 중앙값 절대 편차는 예측의 품질을 판단하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="translated">교육 세트의 각 기능에 대한 중앙값입니다.</target>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="translated">캐시 numpy 배열에서로드 할 때 사용되는 젬핑 모드입니다. 인수의 의미는 numpy.load를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="translated">캐시 numpy 배열에서로드 할 때 사용되는 젬핑 모드입니다. 인수의 의미는 numpy.load를 참조하십시오. 기본적으로 메모리 객체의 메모리 객체가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="translated">무작위 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 의 메모리 풋 프린트 는 또한 \ (n _ {\ max} \ cdot n _ {\ min} \) 대신 \ (2 \ cdot n _ {\ max} \ cdot n _ {\ mathrm {components}} \)에 비례합니다. 정확한 방법.</target>
        </trans-unit>
        <trans-unit id="ad92e195504975d239e8efd05966b747d9d3adc0" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier, and discretize the [0, 1] interval into bins.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="translated">이 방법은 입력이 이진 분류기에서 나온 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="translated">이 방법은 모형 &lt;code&gt;n_init&lt;/code&gt; 시간을 맞추고 모형이 가장 큰 확률 또는 하한을 갖는 매개 변수를 설정합니다. 각 시도 내에서, 방법 은 우도 또는 하한의 변화가 &lt;code&gt;tol&lt;/code&gt; 보다 작을 때까지 &lt;code&gt;max_iter&lt;/code&gt; 시간 동안 E-step과 M-step 사이를 반복하고 , 그렇지 않으면 &lt;code&gt;ConvergenceWarning&lt;/code&gt; 이 발생합니다. 경우 &lt;code&gt;warm_start&lt;/code&gt; 가 있다 &lt;code&gt;True&lt;/code&gt; , 다음 &lt;code&gt;n_init&lt;/code&gt; 무시되고 하나의 초기화는 최초의 호출에 따라 수행됩니다. 연속적인 통화가 끝나면 중단 된 곳에서 교육이 시작됩니다.</target>
        </trans-unit>
        <trans-unit id="64dcb6bda2581c1c56dbef7b8447b61db8e804dd" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;a href=&quot;sklearn.exceptions.convergencewarning#sklearn.exceptions.ConvergenceWarning&quot;&gt;&lt;code&gt;ConvergenceWarning&lt;/code&gt;&lt;/a&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="translated">이 방법은 모형 n_init 시간을 맞추고 모형이 가장 큰 확률 또는 하한을 갖는 매개 변수를 설정합니다. 각 시도 내에서, 방법 은 우도 또는 하한의 변화가 &lt;code&gt;tol&lt;/code&gt; 보다 작을 때까지 &lt;code&gt;max_iter&lt;/code&gt; 시간 동안 E-step과 M-step 사이를 반복하고 , 그렇지 않으면 &lt;code&gt;ConvergenceWarning&lt;/code&gt; 이 발생합니다. 피팅 후 입력 데이터 포인트에 대해 가장 가능성있는 레이블을 예측합니다.</target>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="translated">이 방법은 독립적 인 RBM의 가중치로 심층 신경망을 초기화하는 데 인기를 얻었습니다. 이 방법을 감독되지 않은 사전 훈련이라고합니다.</target>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="translated">회귀 문제를 해결하기 위해 Support Vector Classification 방법을 확장 할 수 있습니다. 이 방법을 Support Vector Regression이라고합니다.</target>
        </trans-unit>
        <trans-unit id="0ea5095ab7a2f7c4087eef18fc7667a5380ac2de" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method (i.e. a logistic regression model) or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="translated">교정에 사용하는 방법입니다. Platt의 방법에 해당하는 'sigmoid'또는 비모수 적 접근 방식 인 'isotonic'일 수 있습니다. 너무 적합하지 않은 교정 샘플 &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; 로 등장 교정을 사용하지 않는 것이 좋습니다 . 이 경우 S 자형 (플랫 보정)을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="1d0e566bfd264bfe0a9882ca94efb92a9385b867" translate="yes" xml:space="preserve">
          <source>The method used by each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87ca64fde3cf11f4163bb009ef5ad4abbcdc8e98" translate="yes" xml:space="preserve">
          <source>The method used to calculate the averaged predictions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="translated">가중치, 평균 및 공분산을 초기화하는 데 사용되는 방법입니다. 다음 중 하나 여야합니다.</target>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="translated">가중치, 평균 및 정밀도를 초기화하는 데 사용되는 방법입니다. 다음 중 하나 여야합니다.</target>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">이 방법은 파이프 라인과 같은 중첩 된 개체뿐만 아니라 간단한 견적 도구에서도 작동합니다. 후자는 &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; 형식의 매개 변수를 가지 므로 중첩 된 개체의 각 구성 요소를 업데이트 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="translated">이 방법은 간단한 커널과 중첩 커널에서 작동합니다. 후자는 &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; 형식의 매개 변수를 가지 므로 중첩 된 개체의 각 구성 요소를 업데이트 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="translated">F- 검정을 기반으로하는 방법은 두 랜덤 변수 사이의 선형 의존도를 추정합니다. 한편, 상호 정보 방법은 모든 종류의 통계적 의존성을 포착 할 수 있지만 비모수 적이므로 정확한 추정을 위해 더 많은 샘플이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="86d449abad7d30ccc370e439f66e761a4c276339" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9346206e9d06ed06060a101f1e51a8e80f2445f7" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. 메트릭이 문자열이거나 호출 가능한 경우 메트릭 매개 변수에 대해 &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt; &lt;/a&gt; 에서 허용하는 옵션 중 하나 여야합니다 . 메트릭이 &quot;사전 계산 된&quot;경우 X는 거리 행렬로 가정되며 정사각형이어야합니다. X는 희소 행렬 일 수 있으며,이 경우 &quot;0이 아닌&quot;요소 만 DBSCAN의 이웃으로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. 메트릭이 문자열이거나 호출 가능한 경우 메트릭 매개 변수에 대해 metrics.pairwise.pairwise_distances에서 허용하는 옵션 중 하나 여야합니다. 각 클래스에 해당하는 샘플의 중심은 특정 클래스에 속하는 모든 샘플의 거리 (메트릭에 따라)의 합계가 최소화되는 지점입니다. &quot;맨해튼&quot;메트릭이 제공되는 경우이 중심은 중앙값이며 다른 모든 메트릭의 경우 이제 중심이 평균으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. metric이 문자열 인 경우 &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt; 에서 허용되는 옵션 중 하나 여야합니다 . X가 거리 배열 자체 인 경우 &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. metric이 문자열 인 경우 &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt; 에서 허용되는 옵션 중 하나 여야합니다 . X가 거리 배열 자체 인 경우 &quot;사전 계산 된&quot;을 메트릭으로 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="74dc2788261a34d78d8ba4595625c4b4c3a263d6" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric. Precomputed distance matrices must have 0 along the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. 메트릭이 문자열 인 경우 메트릭 매개 변수에 대해 scipy.spatial.distance.pdist에서 허용하는 옵션 중 하나이거나 쌍으로 나열되는 메트릭 중 하나 여야합니다. 메트릭이 &quot;사전 계산 된&quot;경우 X는 거리 행렬로 간주됩니다. 또는 메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 X에서 두 개의 배열을 입력으로 받아 그들 사이의 거리를 나타내는 값을 반환해야합니다.</target>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. 메트릭이 문자열 인 경우 메트릭 매개 변수에 대해 scipy.spatial.distance.pdist에서 허용하는 옵션 중 하나이거나 pairwise.PAIRWISE_DISTANCE_FUNCTIONS에 나열된 메트릭 중 하나 여야합니다. 메트릭이 &quot;사전 계산 된&quot;경우 X는 거리 행렬로 간주됩니다. 또는 메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 X에서 두 개의 배열을 입력으로 받아 그들 사이의 거리를 나타내는 값을 반환해야합니다. 기본값은 &quot;유클리드&quot;이며 이는 제곱 유클리드 거리로 해석됩니다.</target>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">피처 배열에서 인스턴스 간 거리를 계산할 때 사용할 지표입니다. 메트릭이 문자열 인 경우 &quot;euclidean&quot;, &quot;manhattan&quot;또는 &quot;cosine&quot;을 포함하여 PAIRED_DISTANCES에 지정된 옵션 중 하나 여야합니다. 또는 메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 X에서 두 개의 배열을 입력으로 받아 그들 사이의 거리를 나타내는 값을 반환해야합니다.</target>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="translated">기능 배열에서 인스턴스 간 커널을 계산할 때 사용할 지표입니다. 측정 항목이 문자열 인 경우 쌍으로 표시되는 측정 항목 중 하나 여야합니다 .PAIRWISE_KERNEL_FUNCTIONS. 메트릭이 &quot;사전 계산 된&quot;경우 X는 커널 매트릭스 인 것으로 가정합니다. 또는 메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 X에서 두 개의 배열을 입력으로 받아 그들 사이의 거리를 나타내는 값을 반환해야합니다.</target>
        </trans-unit>
        <trans-unit id="fc294d94430f6d524884c3e3bbe7bf2fd080ba19" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="translated">n_samples로 eps 임베딩을 보장 할 수있는 최소 개수의 구성 요소입니다.</target>
        </trans-unit>
        <trans-unit id="bffda997c9866bdeb3072a563a59e0dfd40b41cb" translate="yes" xml:space="preserve">
          <source>The minimization problem becomes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="translated">최소 컨센서스 점수 0은 모든 쌍의 biclusters가 완전히 다른 경우에 발생합니다. 최대 점수 1은 두 세트가 동일 할 때 발생합니다.</target>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="translated">eps 포함을 보장하기위한 최소 구성 요소 수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="translated">eps 포함을 보장하는 최소 구성 요소 수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="translated">선택할 최소 기능 수입니다. 원래 기능 개수와 &lt;code&gt;min_features_to_select&lt;/code&gt; 의 차이 를 &lt;code&gt;step&lt;/code&gt; 로 나눌 수없는 경우 에도이 기능 수는 항상 점수가 매겨 집니다 .</target>
        </trans-unit>
        <trans-unit id="dc2c2ff1c0458b789a8dbc35522b80adbfc0fde6" translate="yes" xml:space="preserve">
          <source>The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="translated">리프 노드에 있어야하는 최소 샘플 수입니다. 임의의 깊이에서 분리 점 은 각 왼쪽 및 오른쪽 분기 에 최소한 &lt;code&gt;min_samples_leaf&lt;/code&gt; 훈련 샘플을 남겨 두는 경우에만 고려됩니다 . 이것은 특히 회귀에서 모형을 부드럽게하는 효과가 있습니다.</target>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="translated">내부 노드를 분할하는 데 필요한 최소 샘플 수 :</target>
        </trans-unit>
        <trans-unit id="28930c1108db91ea501be86c776a39ad33671bd9" translate="yes" xml:space="preserve">
          <source>The minimum score is zero, with lower values indicating better clustering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cad4521e1577b76d3fbf78851e9af149127ba4db" translate="yes" xml:space="preserve">
          <source>The minimum valid value the parameter can take. If None (default) it is implied that the parameter does not have a lower bound.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="translated">리프 노드에 있어야하는 (모든 입력 샘플의) 총 중량의 최소 가중 비율입니다. sample_weight가 제공되지 않은 경우 샘플의 가중치는 동일합니다.</target>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="translated">입력 데이터에 대한 누락 된 표시기 &lt;code&gt;Xt&lt;/code&gt; 의 데이터 유형은 부울입니다.</target>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="translated">알고리즘을 초기화하는 데 사용되는 믹싱 매트릭스입니다.</target>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="translated">믹싱 매트릭스</target>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="translated">모든 클래스가 동일한 공분산 행렬을 공유한다고 가정하면 모델은 가우스 밀도를 각 클래스에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="translated">이 모델은 가우스 밀도를 각 클래스에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="c383334c8db0249de2e6de6266d715cf757d0e4f" translate="yes" xml:space="preserve">
          <source>The model learnt is far from being a good model making accurate predictions: this is obvious when looking at the plot above, where good predictions should lie on the red line.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="translated">모형은 입력 분포에 관한 가정을합니다. 현재 scikit-learn은 &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; 만 제공 합니다.이 입력은 이진 값 또는 0과 1 사이의 값으로 가정하며 각 기능은 특정 기능이 설정 될 확률을 인코딩합니다.</target>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="translated">모형에는 훈련 시간에 계산 된 확률 정보가 있어야합니다 . 속성 &lt;code&gt;probability&lt;/code&gt; 과 일치 하여 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="f6f2922bb9e8bafadd9354168cde2a4c9535aa4f" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; attributes: &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\) and &lt;code&gt;intercept_&lt;/code&gt; holds \(b\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="translated">모델 매개 변수는 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 멤버를 통해 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="translated">지원 벡터 분류 (위에서 설명한대로)로 생성 된 모델은 모델을 빌드하기위한 비용 함수가 여백을 초과하는 트레이닝 포인트에 신경 쓰지 않기 때문에 트레이닝 데이터의 서브 세트에만 의존합니다. 이와 유사하게 Support Vector Regression에서 생성 된 모델은 훈련 데이터의 하위 집합에만 의존합니다. 모델을 구축하기위한 비용 함수는 모델 예측에 가까운 훈련 데이터를 무시하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="14cc80df7b1b8b9f6ebfb922c7e5dfe3e7692df9" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4911f4659c5f4e900454be23c5d2d4b6dba06b2b" translate="yes" xml:space="preserve">
          <source>The model will stay unchanged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="translated">모델은 가장 강력한 정규화에서 최소 정규화로 주문됩니다. 모델의 4 가지 계수가 수집되고 &quot;정규화 경로&quot;로 표시됩니다. 그림의 왼쪽 (강력한 정규화 기)에서 모든 계수는 정확히 0입니다. 정규화가 점점 느슨해지면 계수는 0이 아닙니다. 한 번에 하나씩 값.</target>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 에는 1995 년 Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]에&lt;/a&gt; 의해 도입 된 인기있는 부스팅 알고리즘 인 AdaBoost가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="1ad1575f52119d57fdeed35bbf3eb9cde06a6188" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted decision trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 은 그라디언트 부스트 회귀 트리를 통해 분류 및 회귀에 대한 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; 모듈 은 또한 사실과 예측이 주어지면 예측 오류를 측정하는 간단한 함수 세트를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="translated">&lt;code&gt;partial_dependence&lt;/code&gt; 모듈 은 편의 함수 &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; 를 제공하여 단방향 및 양방향 부분 의존도를 생성합니다. 아래 예제에서는 부분 의존도 그리드를 만드는 방법을 보여줍니다. 지형지 물 &lt;code&gt;0&lt;/code&gt; 및 &lt;code&gt;1&lt;/code&gt; 에 대한 두 개의 단방향 PDP 및 두 지형지 물에 대한 양방향 PDP :</target>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="translated">모듈은 공용 속성 &lt;code&gt;coefs_&lt;/code&gt; 및 &lt;code&gt;intercepts_&lt;/code&gt; 를 포함합니다 . &lt;code&gt;coefs_&lt;/code&gt; 는 가중치 행렬의 목록이며, 인덱스 \ (i \)의 가중치 행렬은 레이어 \ (i \)와 레이어 \ (i + 1 \) 사이의 가중치를 나타냅니다. &lt;code&gt;intercepts_&lt;/code&gt; 는 바이어스 벡터의 목록입니다. 인덱스 \ (i \)의 벡터는 레이어 \ (i + 1 \)에 추가 된 바이어스 값을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="translated">모듈 : &lt;code&gt;random_projection&lt;/code&gt; 은 랜덤 프로젝션에 의한 데이터 축소를위한 여러 도구를 제공합니다. 문서의 관련 섹션 : &lt;a href=&quot;random_projection#random-projection&quot;&gt;임의 투영을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="translated">모니터는 현재 반복, 추정기에 대한 참조 및 &lt;code&gt;_fit_stages&lt;/code&gt; 의 로컬 변수 를 키워드 인수 &lt;code&gt;callable(i, self, locals())&lt;/code&gt; 반복 할 때마다 호출 됩니다. 콜 러블이 &lt;code&gt;True&lt;/code&gt; 를 반환 하면 피팅 절차가 중지됩니다. 이 모니터는 계산 보류 계산, 조기 중지, 모델 내성 및 스냅 샷과 같은 다양한 용도로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="translated">이 전략에 가장 적합한 매개 변수는 정규화 기의 강도를 인코딩하는 매개 변수입니다. 이 경우 추정기 의 &lt;strong&gt;정규화 경로&lt;/strong&gt; 를 계산한다고합니다 .</target>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="translated">가장 직관적 인 방법은 단어 표현을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="translated">이 스케일링을 사용하려는 동기에는 기능의 매우 작은 표준 편차에 대한 견고성 및 희소 데이터의 0 항목 보존이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="translated">멀티 태스킹 올가미를 사용하면 선택한 기능이 여러 작업에서 동일하게 적용되도록하는 여러 회귀 문제를 맞출 수 있습니다. 이 예제는 순차적 측정을 시뮬레이션하고 각 작업은 시간 순간이며 관련 기능은 시간이 지남에 따라 진폭이 다양하지만 동일합니다. 멀티 태스킹 올가미는 한 시점에서 선택된 기능이 모든 시점에서 선택되도록합니다. 이것은 올가미에 의한 기능 선택을보다 안정적으로 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="translated">여기서 다중 클래스 정의는 이진 분류에 사용 된 메트릭의 가장 합리적인 확장으로 보이지만 문헌에는 특정 합의가 없습니다.</target>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="translated">멀티 클래스 지원은 일대일 구성표에 따라 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="559112a07c580b9f5163f4eaab25d31b6f252bc0" translate="yes" xml:space="preserve">
          <source>The multilabel_confusion_matrix calculates class-wise or sample-wise multilabel confusion matrices, and in multiclass tasks, labels are binarized under a one-vs-rest way; while confusion_matrix calculates one confusion matrix for confusion between every two classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="translated">다항식 Naive Bayes 분류기는 개별 기능 (예 : 텍스트 분류를위한 단어 수)으로 분류하는 데 적합합니다. 다항 분포에는 일반적으로 정수 피처 수가 필요합니다. 그러나 실제로는 tf-idf와 같은 소수 계수도 작동 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="translated">여러 메트릭을 목록, 튜플 또는 사전 정의 된 득점자 이름 세트로 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0f55ab2d94390a312af5800dfb617d9ab1f868e3" translate="yes" xml:space="preserve">
          <source>The name of the hyperparameter. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cdcb45ef3965759b13ccc7a57841383638bd06" translate="yes" xml:space="preserve">
          <source>The name of the parameter to be printed in error messages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="translated">로드 된 샘플 이미지의 이름</target>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="translated">이름의 &lt;code&gt;vect&lt;/code&gt; , &lt;code&gt;tfidf&lt;/code&gt; 및 &lt;code&gt;clf&lt;/code&gt; (분류)은 임의이다. 이를 사용하여 아래에서 적합한 하이퍼 파라미터에 대한 그리드 검색을 수행합니다. 이제 단일 명령으로 모델을 학습 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="37788c74066a15702915a906173e5747b775560c" translate="yes" xml:space="preserve">
          <source>The names of features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a6d82313da18df77f363e5439a6b3ce08c6680a" translate="yes" xml:space="preserve">
          <source>The names of target classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="translated">데이터 세트 열의 이름</target>
        </trans-unit>
        <trans-unit id="9c71b3cc12d889ddb9194bd05e135e431b48c18f" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e5fcf3b8aec8853bca895bccf214ac28ac4d27e" translate="yes" xml:space="preserve">
          <source>The names of the target columns</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40ffca0689344b78c3e10b40b262e0c45ea9b50f" translate="yes" xml:space="preserve">
          <source>The names of the target columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1942d0e2fca00caa96a5a7573d350784bbfcadf1" translate="yes" xml:space="preserve">
          <source>The new backend can then be selected by passing its name as the backend argument to the Parallel class. Moreover, the default backend can be overwritten globally by setting make_default=True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="translated">새로운 dtype은 원래 유형에 따라 np.float32 또는 np.float64입니다. 이 함수는 인수 사본에 따라 사본을 작성하거나 인수를 수정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="translated">새로운 항목 \ (d (u, v) \)는 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="translated">다음 그림은 종래 중량 농도의 다른 유형에 대해 얻어진 결과 (파라미터 비교 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; 다른 값) &lt;code&gt;weight_concentration_prior&lt;/code&gt; 를 . 여기서 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 매개 변수 의 값이 획득 한 유효 구성 요소의 유효 수에 큰 영향을주는 것을 볼 수 있습니다. 또한 사전 유형이 'dirichlet_distribution'인 경우 사전 농도 가중치의 값이 클수록 균일 한 가중치가 발생하지만 'dirichlet_process'유형의 경우 (기본적으로 사용됨)는 아닙니다.</target>
        </trans-unit>
        <trans-unit id="c25f5050f6fd3011382e4c50ae76e75315ba1a26" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; scales better. With regard to prediction time, &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="translated">다음 그림은 다양한 크기의 트레이닝 세트에 대한 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 및 &lt;code&gt;SVR&lt;/code&gt; 의 피팅 및 예측 시간을 비교합니다 . 피팅 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 는 중간 규모의 훈련 세트 (1000 개 미만의 샘플)의 경우 &lt;code&gt;SVR&lt;/code&gt; 보다 빠릅니다 . 그러나 더 큰 훈련 세트의 경우 &lt;code&gt;SVR&lt;/code&gt; 이 더 잘 확장됩니다. 예측 시간과 관련하여 &lt;code&gt;SVR&lt;/code&gt; 은 학습 된 스파 스 솔루션으로 인해 모든 규모의 트레이닝 세트에서 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 보다 빠릅니다 . 희소도 및 따라서 예측 시간은 &lt;code&gt;SVR&lt;/code&gt; 의 파라미터 \ (\ epsilon \) 및 \ (C \)에 의존한다는 점에 유의한다 ; \ (\ epsilon = 0 \)은 밀도가 높은 모델에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="translated">다음 그림은 다양한 크기의 트레이닝 세트에 대한 KRR 및 SVR의 피팅 및 예측 시간을 비교합니다. 피팅 KRR은 중간 규모의 훈련 세트 (1000 개 미만의 샘플)의 경우 SVR보다 빠릅니다. 그러나 더 큰 훈련 세트의 경우 SVR이 더 잘 확장됩니다. 예측 시간과 관련하여 SVR은 학습 된 희소 솔루션으로 인해 모든 크기의 트레이닝 세트에서 KRR보다 빠릅니다. 희소도 및 따라서 예측 시간은 SVR의 파라미터 엡실론 및 C에 의존한다는 점에 유의한다.</target>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="translated">다음 이미지는 시그 모이 드 캘리브레이션 변경으로 3 클래스 분류 문제의 예상 확률을 보여줍니다. 3 개의 모서리가 3 개의 클래스에 해당하는 표준 2 심플 렉스가 그림에 나와 있습니다. 화살표는 보정되지 않은 분류기에 의해 예측 된 확률 벡터에서 홀드 아웃 유효성 검사 세트에서 시그 모이 드 보정 후 동일한 분류기에 의해 예측 된 확률 벡터를 가리 킵니다. 색상은 인스턴스의 실제 클래스를 나타냅니다 (빨간색 : 클래스 1, 녹색 : 클래스 2, 파란색 : 클래스 3).</target>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="translated">노드는 상태가 연결된 다른 노드의 상태에 따라 달라지는 임의 변수입니다. 따라서이 모델은 연결 가중치와 각 가시 및 숨겨진 단위에 대한 하나의 절편 (바이어스) 항에 의해 매개 변수화되며, 간략화를 위해 이미지에서 생략됩니다.</target>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="translated">대상의 노이즈 레벨은 매개 변수 &lt;code&gt;alpha&lt;/code&gt; 를 통해 전역 적으로 스칼라 또는 데이터 포인트 별로 전달하여 지정할 수 있습니다 . 적당한 소음 수준은 Tikhonov 정규화, 즉 커널 매트릭스의 대각선에 추가함으로써 효과적으로 구현되므로 피팅 중 숫자 문제를 처리하는 데 도움이 될 수 있습니다. 노이즈 수준을 명시 적으로 지정하는 대안은 커널에서 WhiteKernel 구성 요소를 포함하여 데이터에서 전체 노이즈 수준을 추정 할 수 있습니다 (아래 예 참조).</target>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="translated">커널의 수정되지 않은 로그 변환 하이퍼 파라미터</target>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="translated">비 메트릭 알고리즘은 응력을 계산하기 전에 단조 회귀 단계를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="translated">0이 아닌 각 샘플 (또는 축이 0 인 경우 0이 아닌 각 특징)을 정규화하는 데 사용되는 표준입니다.</target>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="translated">0이 아닌 각 샘플을 정규화하는 데 사용되는 표준입니다.</target>
        </trans-unit>
        <trans-unit id="6d3c61aac7334b6f6f54f73be53f19b464b34cbe" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample. If norm=&amp;rsquo;max&amp;rsquo; is used, values will be rescaled by the maximum of the absolute values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="translated">정규화 된 상호 정보는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="translated">그런 다음 노멀 라이저 인스턴스를 임의의 변환기로 샘플 벡터에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="translated">고려되는 이웃의 수 k (별칭 매개 변수 n_neighbors)는 일반적으로 1) 클러스터에 포함해야하는 최소 개체 수보다 많으므로 다른 개체는이 클러스터에 비해 로컬 특이 치일 수 있고 2) 최대보다 작습니다. 잠재적으로 로컬 이상 치일 수있는 객체에 의한 근접도. 실제로 이러한 정보는 일반적으로 사용할 수 없으며 n_neighbors = 20을 사용하면 일반적으로 잘 작동하는 것으로 보입니다. 특이 치 비율이 높은 경우 (즉, 아래 예에서와 같이 10 % 이상) n_neighbors는 커야합니다 (아래 예에서 n_neighbors = 35).</target>
        </trans-unit>
        <trans-unit id="9650bdc07aaa5281bfc3be8ecd4292573a8743c6" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to compute the partial dependences. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">OVA (멀티 클래스 문제의 경우 하나) 계산을 수행하는 데 사용할 CPU 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="985a6706e191013209fa45542581d0ffa377ae50" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">계산에 사용할 CPU 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="2e29d4d145cd20eb08e9ea5571074f934df7e0d8" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="translated">수행 할 EM 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="df46ce41d5f6eae9d9abc12693e64e5ffe0f042c" translate="yes" xml:space="preserve">
          <source>The number of OpenMP threads to use for the computation. Parallelism is sample-wise on the main cython loop which assigns each sample to its closest center.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="translated">각 작업자에게 한 번에 발송할 원자 작업 수입니다. 개별 평가가 매우 빠르면 오버 헤드로 인해 작업자에게 콜을 디스패치하는 것이 순차적 계산보다 느릴 수 있습니다. 빠른 계산을 함께 배치하면이를 완화 할 수 있습니다. &lt;code&gt;'auto'&lt;/code&gt; 전략은 전체에 배치 걸리는 시간을 추적, 동적 휴리스틱을 사용하여, 0.5 초 정도의 시간을 유지하기 위해 배치 크기를 조정합니다. 초기 배치 크기는 1입니다. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; 은 스레딩 백엔드의 오버 헤드가 거의없고 더 큰 배치 크기를 사용하더라도 그에 따른 이득을 얻지 못하므로 한 번에 단일 작업의 배치를 디스패치합니다. 케이스.</target>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="translated">앙상블의 기본 추정기의 수입니다.</target>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="translated">사전 발송할 배치 (작업) 수입니다. 기본값은 '2 * n_jobs'입니다. batch_size =&amp;rdquo;auto&amp;rdquo;인 경우 이는 합리적인 기본값이며 작업자는 절대 굶지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="translated">찾을 biclusters의 수입니다.</target>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="translated">biclusters의 수입니다.</target>
        </trans-unit>
        <trans-unit id="d76125e41f0deb521acf9ed71af5267a484f2d30" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="translated">생산할 용기 수입니다. 빈의 간격은 입력 데이터의 최소 및 최대에 의해 결정됩니다. &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt; 경우 ValueError를 발생 시킵니다 .</target>
        </trans-unit>
        <trans-unit id="b2f078045a64e3b870ff56d7d5abb73dda7d8a43" translate="yes" xml:space="preserve">
          <source>The number of bins used to bin the data is controlled with the &lt;code&gt;max_bins&lt;/code&gt; parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="translated">수행 할 부스팅 단계 수입니다. 그라디언트 부스팅은 과적 합에 상당히 강하므로 많은 수의 경우 일반적으로 성능이 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="d5d9cefb2fff3dbdd417050b8ce7c297d42aaa9f" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer (0 included). Thus, this target can be modelled by a Poisson distribution. It is then assumed to be the number of discrete events occuring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years). Here we model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt;, which is still a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a186b2a8600030ec87eb172dc6f657942fddfcfb" translate="yes" xml:space="preserve">
          <source>The number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;) is a positive integer that can be modeled as a Poisson distribution. It is then assumed to be the number of discrete events occurring with a constant rate in a given time interval (&lt;code&gt;Exposure&lt;/code&gt;, in units of years).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="translated">수업 수</target>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="translated">클래스 수 (단일 출력 문제의 경우) 또는 각 출력의 클래스 수를 포함하는 목록 (다중 출력 문제의 경우)</target>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="translated">분류 문제의 클래스 (또는 레이블) 수입니다.</target>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="translated">클래스 수 (단일 출력 문제) 또는 각 출력에 대한 클래스 수를 포함하는 목록 (다중 출력 문제)</target>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="translated">훈련 데이터의 클래스 수</target>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="translated">분류 문제의 클래스 수입니다.</target>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="translated">반환 할 클래스 수입니다.</target>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="translated">클래스 수</target>
        </trans-unit>
        <trans-unit id="9298de4c7bbca93c3d80fc1d9af6567cb0b29b0a" translate="yes" xml:space="preserve">
          <source>The number of clusters found by the algorithm. If &lt;code&gt;distance_threshold=None&lt;/code&gt;, it will be equal to the given &lt;code&gt;n_clusters&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="translated">클래스 당 클러스터 수</target>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="translated">찾을 클러스터 수입니다.</target>
        </trans-unit>
        <trans-unit id="703c8a00edcb110a0abc7bd2f2de73653cb3b8bc" translate="yes" xml:space="preserve">
          <source>The number of clusters to find. It must be &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="translated">형성 할 군집 수와 생성 할 중심 수입니다.</target>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="translated">그리드 플롯의 열 수입니다 (기본값 : 3).</target>
        </trans-unit>
        <trans-unit id="1ab9cd0e34ad15344a0623bebcdec3c277a5d196" translate="yes" xml:space="preserve">
          <source>The number of components. It is same as the &lt;code&gt;n_components&lt;/code&gt; parameter if it was given. Otherwise, it will be same as the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="translated">그래프에서 연결된 구성 요소의 수입니다.</target>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="translated">교차 유효성 검사 분할 수 (접기 / 반복).</target>
        </trans-unit>
        <trans-unit id="4e96a6f0802b675664ce18cac028e5815cdfcfc0" translate="yes" xml:space="preserve">
          <source>The number of data features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="translated">모형에서 각 구성 요소의 자유도입니다.</target>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="translated">정보 및 중복 기능에서 임의로 추출 된 중복 기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="81a86627bf928c8685cc6768d089212f83e7a3d6" translate="yes" xml:space="preserve">
          <source>The number of elements of the hyperparameter value. Defaults to 1, which corresponds to a scalar hyperparameter. n_elements &amp;gt; 1 corresponds to a hyperparameter which is vector-valued, such as, e.g., anisotropic length-scales.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 에서 같은 간격으로 점 수입니다 .</target>
        </trans-unit>
        <trans-unit id="1ea7fab9bca4ccb2b6b305c06e165e2e51c05101" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes of the plots, for each target feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="translated">축에서 동일 간격의 포인트 수입니다.</target>
        </trans-unit>
        <trans-unit id="6843d07b6a565b2c9c3cc44410aeae6ef1353131" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the grid, for each target feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">조기 중지에 의해 선택된 추정기 수 ( &lt;code&gt;n_iter_no_change&lt;/code&gt; 가 지정된 경우). 그렇지 않으면 &lt;code&gt;n_estimators&lt;/code&gt; 로 설정됩니다 .</target>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="translated">출력 행렬의 피처 (열) 수입니다. 소수의 피처는 해시 충돌을 일으킬 가능성이 있지만, 많은 수는 선형 학습자에서 더 큰 계수 치수를 유발합니다.</target>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="translated">각 샘플의 기능 수입니다.</target>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="translated">기능 수는&amp;gt; = 5 여야합니다.</target>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="translated">최상의 분할을 찾을 때 고려해야 할 기능 수 :</target>
        </trans-unit>
        <trans-unit id="c3e18b2792aee2f260136dbfb16e08ac8ac5d9d0" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator ( without replacement by default, see &lt;code&gt;bootstrap_features&lt;/code&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="translated">각 기본 추정기를 훈련시키기 위해 X에서 그릴 기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="translated">선택할 기능의 수입니다. 경우 &lt;code&gt;None&lt;/code&gt; , 기능의 절반이 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="translated">사용할 기능의 수입니다. None 인 경우 파일에서 발생하는 최대 열 인덱스에서 유추됩니다.</target>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="translated">사용할 기능의 수입니다. None 인 경우 추론됩니다. 이 인수는 더 큰 슬라이스 데이터 세트의 서브 세트 인 여러 파일을로드하는 데 유용합니다. 각 서브 세트에 모든 기능의 예가 없을 수 있으므로 유추 된 모양이 슬라이스마다 다를 수 있습니다. n_features는 &lt;code&gt;offset&lt;/code&gt; 또는 &lt;code&gt;length&lt;/code&gt; 에 기본값이 아닌 값이 전달 된 경우에만 필요 합니다.</target>
        </trans-unit>
        <trans-unit id="3f0c441872fc889415f948d3a194dda00a747f75" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65e177587b4163893d6713a7981471e039d2ab1a" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 이 수행 될 때 기능의 수입니다 .</target>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="translated">기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="translated">기능의 수입니다. 5 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="translated">유익한 기능의 수, 즉 출력을 생성하는 데 사용되는 선형 모델을 작성하는 데 사용되는 기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="translated">유익한 기능의 수입니다. 각 클래스는 차원 &lt;code&gt;n_informative&lt;/code&gt; 의 하위 공간에서 하이퍼 큐브의 정점 주위에 각각 위치한 여러 가우스 클러스터로 구성 됩니다. 각 군집에 대해 정보 기능은 N (0, 1)과 독립적으로 도출 된 다음 공분산을 추가하기 위해 각 군집 내에서 임의로 선형으로 결합됩니다. 그런 다음 클러스터는 하이퍼 큐브의 정점에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="translated">수행 할 초기화 수입니다. 최상의 결과가 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="translated">수행 할 초기화 수입니다. 가능성에서 가장 낮은 하한값을 가진 결과가 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="translated">샘플링 할 정수의 수입니다.</target>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="translated">partial_fit을 호출하기 전에 수행 된 데이터 배치에 대한 반복 횟수입니다. 이것은 선택 사항입니다. 숫자가 전달되지 않으면 객체의 메모리가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="99c30bd94e39fd4b9ed3f7e2a7d5b85b28ed521a" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86dc5cbf697553ae74ea1f2c0f3574967cc15f4" translate="yes" xml:space="preserve">
          <source>The number of iterations as selected by early stopping, depending on the &lt;code&gt;early_stopping&lt;/code&gt; parameter. Otherwise it corresponds to max_iter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">최고의 스트레스에 해당하는 반복 횟수입니다. &lt;code&gt;return_n_iter&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="translated">각 대상의 알파 격자를 찾기 위해 lars_path가 수행 한 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="translated">좌표 하강 최적화가 각 알파에 대해 지정된 공차에 도달하기 위해 수행 한 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="translated">좌표 하강 최적화가 각 알파에 대해 지정된 공차에 도달하기 위해 수행 한 반복 횟수입니다. &lt;code&gt;return_n_iter&lt;/code&gt; 가 True로 설정 되면 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="translated">솔버가 실행 한 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="28e3bc9f7f876eba064c747240fb8d2ff652b8df" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel all &lt;code&gt;estimators&lt;/code&gt;&lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2412be87a40770d353886ae29c8c16a95b728197" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbb843cd2b1d5904ac077f6964fae31def7713e0" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt; of all &lt;code&gt;estimators&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;code&gt;joblib.parallel_backend&lt;/code&gt; context. -1 means using all processors. See Glossary for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 위해 병렬로 실행할 작업 수입니다 . &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="f140b3227d03089cb6e6210a5db41bedf6b9aa63" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c5c499bb768493660dc4f85003a1f155b88c8c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e98b60a4f63df4b45688240803363e4326e4c3c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.BaggingRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="631e50edfdf734f4505b382652be3db79f846fe9" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;a href=&quot;#sklearn.ensemble.IsolationForest.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.IsolationForest.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 과 &lt;code&gt;predict&lt;/code&gt; 모두 에 대해 병렬로 실행할 작업 수입니다 . &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 과 &lt;code&gt;predict&lt;/code&gt; 모두 에 대해 병렬로 실행할 작업 수입니다 . &lt;code&gt;None`&lt;/code&gt; 방법 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b1734bf8c02c7376c103570d8af07531d90ec5dd" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30ff6a93f265905272a4a92b5eefd941a956fb0c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.ExtraTreesRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8618a7401b1bd61324e6d152687e714d9dfac99" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestClassifier.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c242a688459caf97557bbb76035f6cc722015df4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.predict&quot;&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomForestRegressor.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91298d7ca6e21328dab1603b8e897f672cc46e7d" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel. &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.decision_path&quot;&gt;&lt;code&gt;decision_path&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.RandomTreesEmbedding.apply&quot;&gt;&lt;code&gt;apply&lt;/code&gt;&lt;/a&gt; are all parallelized over the trees. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">계산에 사용할 작업 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="2f8f980d4703b1ec3df64fe85f466c6c6170bcab" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="translated">계산에 사용할 작업 수입니다. 여러 초기화가 사용되는 경우 ( &lt;code&gt;n_init&lt;/code&gt; ) 각 알고리즘 실행이 병렬로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">계산에 사용할 작업 수입니다. y의 각 대상 변수를 병렬로 수행합니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="be06d9a31cbe7a4bf21b6f4456cf1df2e9b8ee45" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">계산에 사용할 작업 수입니다. 이것은 n_targets&amp;gt; 1의 속도 향상과 충분한 큰 문제를 제공합니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ba376dc0fa282138bebfa832fa05fd19bdc385f3" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="translated">계산에 사용할 작업 수입니다. 이것은 짝 행렬을 n_jobs 짝수 조각으로 나누고 병렬로 계산하여 작동합니다.</target>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="translated">계산에 사용할 작업 수입니다. 이것은 각 n_init 실행을 병렬로 계산하여 작동합니다.</target>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">E 단계에서 사용할 작업 수 &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b51be92d99796ca039296711c164e0b4cf278e0a" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="translated">나무의 잎 수</target>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="translated">격자의 경도 (x) 및 위도 (y) 수</target>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="translated">혼합물 성분의 수.</target>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="translated">혼합물 성분의 수. 데이터와 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 의 값에 따라 모델은 일부 구성 요소 &lt;code&gt;weights_&lt;/code&gt; 를 0에 매우 가까운 값 으로 설정하여 모든 구성 요소를 사용하지 않도록 결정할 수 있습니다 . 따라서 유효 구성 요소의 수는 n_components보다 적습니다.</target>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="translated">돌아올 가장 가까운 이웃의 수</target>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">고려되는 이웃의 수 (매개 변수 n_neighbors)는 일반적으로 1) 클러스터에 포함해야하는 최소 샘플 수보다 더 크게 설정되므로 다른 샘플은이 클러스터에 대해 로컬 특이 치가 될 수 있고 2) 최대 닫기 수보다 작습니다. 잠재적으로 지역 특이 치일 수있는 샘플에 의해 실제로 이러한 정보는 일반적으로 사용할 수 없으며 n_neighbors = 20을 사용하면 일반적으로 잘 작동하는 것으로 보입니다.</target>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="translated">고려되는 이웃 수 (매개 변수 n_neighbors)는 일반적으로 1) 클러스터에 포함해야하는 최소 샘플 수보다 더 크게 설정되므로 다른 샘플은이 클러스터에 비해 로컬 특이 치가 될 수 있고 2) 최대 수보다 작습니다. 잠재적으로 지역 특이 치일 수있는 샘플로 닫습니다. 실제로 이러한 정보는 일반적으로 사용할 수 없으며 n_neighbors = 20을 사용하면 일반적으로 잘 작동하는 것으로 보입니다.</target>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 에서 각 레이블의 발생 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="translated">외곽 점의 수는 중요하지만, 또한 이상치입니다.</target>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 를 수행 할 때의 출력 수입니다 .</target>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="translated">출력 수</target>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="translated">이웃 검색을 위해 실행할 병렬 작업 수입니다.</target>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">이웃 검색을 위해 실행할 병렬 작업 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="translated">이웃 검색을 위해 실행할 병렬 작업 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오. &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt; 메소드 에만 영향을줍니다 .</target>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">이웃 검색을 위해 실행할 병렬 작업 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오. &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 방법 에는 영향을 미치지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">이웃 검색을 위해 실행할 병렬 작업 수입니다. &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오. &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 방법 에는 영향을 미치지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="1e03b6a0d6693562cf04d81d449658ec683196ea" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab2a0224fab84a593d45a145e0638371b6911fd8" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecd1b3d7da18bb6dc421488bb596e855ecb80e43" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f8cf8af9be5d62ff8935fbef652326c9adba1d1" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. If &lt;code&gt;-1&lt;/code&gt;, then the number of jobs is set to the number of CPU cores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e352fe55bf07363046d5192af0f663c879039cfb" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. This parameter has no impact when &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt; or (&lt;code&gt;metric=&quot;euclidean&quot;&lt;/code&gt; and &lt;code&gt;method=&quot;exact&quot;&lt;/code&gt;). &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="translated">실행할 병렬 작업 수 &lt;code&gt;None&lt;/code&gt; 수단 1에서 않는 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt; &lt;code&gt;joblib.parallel_backend&lt;/code&gt; 의&lt;/a&gt; 콘텍스트. &lt;code&gt;-1&lt;/code&gt; 은 모든 프로세서를 사용한다는 의미입니다. 자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;용어집&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="d31f80403a0bc612aeee728ef96a0071578ad09f" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="translated">훈련 데이터를 통과 한 횟수 (일명 에포크). 기본값은 없음입니다. 더 이상 사용되지 않으며 0.21에서 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="translated">중복 기능의 수입니다. 이러한 기능은 정보 기능의 임의의 선형 조합으로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="translated">회귀 목표의 수, 즉 샘플과 관련된 y 출력 벡터의 치수입니다. 기본적으로 출력은 스칼라입니다.</target>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="translated">로그-마진 가능성을 최대화하는 커널의 매개 변수를 찾기 위해 옵티 마이저를 다시 시작한 횟수입니다. 옵티마이 저의 첫 번째 실행은 커널의 초기 매개 변수에서 수행되며, 나머지 (있는 경우)는 허용 된 세타-값의 공간에서 무작위로 로그-균일하게 샘플링됩니다. 0보다 크면 모든 경계는 유한해야합니다. n_restarts_optimizer == 0은 한 번의 실행이 수행됨을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="translated">로그-마진 가능성을 최대화하는 커널의 매개 변수를 찾기 위해 옵티 마이저를 다시 시작한 횟수입니다. 옵티마이 저의 첫 번째 실행은 커널의 초기 매개 변수에서 수행되며, 나머지 (있는 경우)는 허용 된 세타-값의 공간에서 무작위로 로그-균일하게 샘플링됩니다. 0보다 크면 모든 경계는 유한해야합니다. n_restarts_optimizer = 0은 한 번의 실행이 수행됨을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="translated">바둑판 구조의 행 및 열 클러스터 수입니다.</target>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="translated">행 및 열 클러스터 수</target>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="translated">S 곡선의 샘플 포인트 수입니다.</target>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="translated">포인트가 핵심 포인트로 간주되는 이웃의 샘플 수 (또는 총 중량)입니다. 여기에는 포인트 자체가 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="translated">가우스 프로세스에서 추출한 샘플 수</target>
        </trans-unit>
        <trans-unit id="c514b9b07551e0f76131a511ab26abfabcd4aa8a" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Also, up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9e5e159d14938dc3482cf1b66194258f9217ba8" translate="yes" xml:space="preserve">
          <source>The number of samples in a neighborhood for a point to be considered as a core point. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">각 기능에 대해 추정자가 처리 한 샘플 수입니다. 누락 된 샘플이 없으면 &lt;code&gt;n_samples_seen&lt;/code&gt; 은 정수가되고 그렇지 않으면 배열이됩니다. 새 통화에 맞게 재설정되지만 &lt;code&gt;partial_fit&lt;/code&gt; 통화에서 증가 합니다.</target>
        </trans-unit>
        <trans-unit id="8b8db8f325de293d35a9c6b4d95ec39fb2cca6ee" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="translated">추정기에서 처리 한 샘플 수입니다. 새 통화에 맞게 재설정되지만 &lt;code&gt;partial_fit&lt;/code&gt; 통화에서 증가 합니다.</target>
        </trans-unit>
        <trans-unit id="baac00734c9f0ef53943bb384323fdc39b43973f" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator (with replacement by default, see &lt;code&gt;bootstrap&lt;/code&gt; for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="translated">각 기본 추정기를 훈련시키기 위해 X에서 추출 할 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="translated">각 배치에서 채취 할 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="translated">각 배치에 사용할 샘플 수입니다. &lt;code&gt;fit&lt;/code&gt; 를 호출 할 때만 사용됩니다 . 경우 &lt;code&gt;batch_size&lt;/code&gt; 없는 &lt;code&gt;None&lt;/code&gt; 다음 &lt;code&gt;batch_size&lt;/code&gt; 행 데이터 세트로부터 유추 &lt;code&gt;5 * n_features&lt;/code&gt; 근사 정확도와 메모리 소비 사이의 균형을 제공하기 위해.</target>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="translated">사용할 샘플 수입니다. 제공하지 않으면 모든 샘플이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="translated">샘플 수</target>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="translated">델타에 포함 된 초 수</target>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="translated">교차 유효성 검사로 선택된 기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="translated">선택된 기능의 수입니다.</target>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="translated">최종 모델의 단계 수는 &lt;code&gt;n_estimators_&lt;/code&gt; 속성에서 사용 가능합니다 .</target>
        </trans-unit>
        <trans-unit id="d8f394636e1a93d63e3434a6391ec5ef2f73741a" translate="yes" xml:space="preserve">
          <source>The number of threads used by the OpenBLAS, MKL or BLIS libraries can be set via the &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, and &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; environment variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="translated">그리드가 수정 된 횟수입니다. 명시적인 알파 값이 전달되면 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="642e04b02615d230b9669f1186145c403a47fbce" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed. Range is [1, inf).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="089ef760cb35bf7968ad0395345a0886662c0be1" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. For regressors, this is always 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20cb5259cf581cee4993b651401d828750438d9c" translate="yes" xml:space="preserve">
          <source>The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to &lt;code&gt;n_classes&lt;/code&gt; for multiclass classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="translated">숲에있는 나무의 수.</target>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="translated">약한 학습자 (즉, 회귀 트리)의 수는 &lt;code&gt;n_estimators&lt;/code&gt; 매개 변수에 의해 제어됩니다 . &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;각 트리의 크기는 &lt;/a&gt; &lt;code&gt;max_depth&lt;/code&gt; 를 통해 트리 깊이를 설정하거나 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 를 통해 리프 노드 수 를 설정하여 제어 할 수 있습니다 . &lt;code&gt;learning_rate&lt;/code&gt; 는 컨트롤로 overfitting되는 범위 (0.0, 1.0]의 하이퍼 파라미터 &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;수축&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="translated">약한 학습자의 수는 &lt;code&gt;n_estimators&lt;/code&gt; 매개 변수에 의해 제어됩니다 . &lt;code&gt;learning_rate&lt;/code&gt; 매개 변수는 최종 조합 약한 학습자의 기여를 제어합니다. 기본적으로 약한 학습자는 의사 결정 그루터기입니다. &lt;code&gt;base_estimator&lt;/code&gt; 매개 변수를 통해 다른 약한 학습자를 지정할 수 있습니다 . 좋은 결과를 얻기 위해 조정하는 주요 매개 변수는 &lt;code&gt;n_estimators&lt;/code&gt; 와 기본 추정기의 복잡성입니다 (예 : split &lt;code&gt;min_samples_split&lt;/code&gt; 을 고려하기위한 깊이 &lt;code&gt;max_depth&lt;/code&gt; 또는 최소 필수 샘플 수 ).</target>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="translated">이 객체는 LassoCV 객체와 동일한 문제를 해결합니다. 그러나 LassoCV와 달리 관련 알파 값 자체를 찾습니다. 일반적 으로이 속성으로 인해 더 안정적입니다. 그러나 다중 공선 데이터 세트를 사용하는 것은 더 취약합니다.</target>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="translated">데이터를 맞추는 데 사용할 객체입니다.</target>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="translated">객체의 &lt;code&gt;best_score_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 속성은 최고 평균 점수 및 해당 점수에 해당하는 매개 변수 설정을 저장합니다.</target>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="translated">목표 함수는 W와 H를 번갈아 최소화하면서 최소화됩니다.</target>
        </trans-unit>
        <trans-unit id="f16ae566357b7a1e9f3616f2cbcfd46a6904b9f5" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="translated">목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="translated">이 경우 최소화 할 목적 함수는</target>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="translated">관측은 더 낮은 차원의 잠재 요인과 가우스 잡음의 선형 변환으로 인해 발생한다고 가정합니다. 일반성의 손실없이 요인은 평균 및 단위 공분산이 0 인 가우시안에 따라 분포됩니다. 잡음도 제로 평균이며 임의의 대각선 공분산 행렬을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">군집 관측. 데이터가 C 순서로 변환되므로 주어진 데이터가 C 연속적이지 않은 경우 메모리 복사가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="translated">관측치, 우리가 계산 한 Mahalanobis 거리. 관측치는 적합하게 사용 된 데이터와 동일한 분포에서 도출 된 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="translated">획득 한 점수는 항상 0보다 크며 최상의 값은 1입니다.</target>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="translated">one-vs-the-rest 메타 분류기는 이러한 방법이 기본 분류기에 의해 구현되는 한 &lt;code&gt;predict_proba&lt;/code&gt; 방법 도 구현합니다 . 이 메소드는 단일 레이블 및 다중 레이블 케이스 모두에서 클래스 멤버쉽의 확률을 리턴합니다. 다중 라벨의 경우 확률은 주어진 샘플이 주어진 클래스에 속하는 한계 확률입니다. 따라서, 다중 라벨 사례에서 주어진 표본에 대해 가능한 모든 라벨에 대한 이러한 확률의 합은 단일 라벨 경우에서와 같이 단일 합산 &lt;em&gt;되지 않습니다&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="translated">훈련 샘플의 반대 LOF. 높을수록 더 정상입니다. 인 라이어의 LOF 점수는 1에 가까우며 ( &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 는 -1에 가까움), 이상 치는 LOF 점수 가 더 큰 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="translated">각 입력 샘플의 로컬 특이 치 계수의 반대입니다. 낮을수록 비정상적입니다.</target>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="translated">주어진 데이터 세트에 대한 최적의 알고리즘은 복잡한 선택이며 여러 요인에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="translated">왜도를 최소화하기위한 최적의 람다 매개 변수는 최대 가능성을 사용하여 각 기능에서 독립적으로 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="translated">Lasso의 최적화 목표는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="translated">MultiTaskElasticNet의 최적화 목표는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="translated">MultiTaskLasso의 최적화 목표는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="translated">case method = 'lasso'의 최적화 목표는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="translated">선택적인 추가 인수가 지원 중단 메시지 및 문서 문자열에 추가됩니다. 참고 : 엑스트라를 기본값으로 사용하려면 빈 괄호를 넣으십시오.</target>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="translated">선택적 매개 변수 &lt;code&gt;whiten=True&lt;/code&gt; 를 사용하면 각 성분을 단위 분산으로 스케일링하면서 단일 공간에 데이터를 투영 할 수 있습니다. 이는 다운 스트림 모델이 신호의 등방성에 대해 강력한 가정을하는 경우에 유용합니다. 예를 들어 RBF 커널 및 K- 평균 클러스터링 알고리즘이있는 Support Vector Machine의 경우입니다.</target>
        </trans-unit>
        <trans-unit id="c77f2b2e59e77aff75fd77a2825914acd9eb1242" translate="yes" xml:space="preserve">
          <source>The order in which the features will be imputed. Possible values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="translated">분류기 체인의 레이블 순서입니다.</target>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="translated">정수 목록을 제공하여 체인의 순서를 명시 적으로 설정할 수 있습니다. 예를 들어 길이가 5 인 체인의 경우 :</target>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="translated">변환 된 피처 매트릭스의 열 순서는 &lt;code&gt;transformers&lt;/code&gt; 목록 에서 열이 지정되는 순서를 따릅니다 . &lt;code&gt;passthrough&lt;/code&gt; 키워드에 지정되지 않은 경우, 지정되지 않은 원래 기능 행렬의 열은 변환 된 결과 기능 행렬에서 삭제됩니다 . &lt;code&gt;passthrough&lt;/code&gt; 로 지정된 열 은 변압기의 출력 오른쪽에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="translated">원본 데이터</target>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="translated">원래 데이터 세트는 92 x 112로 구성되었으며 여기서 사용 가능한 버전은 64x64 이미지로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="translated">Weinberger 등의 해싱 트릭의 원래 공식. 피처의 열 인덱스와 부호를 각각 결정하기 위해 두 개의 개별 해시 함수 \ (h \)와 \ (\ xi \)를 사용했습니다. 본 구현은 MurmurHash3의 부호 비트가 다른 비트와 독립적이라는 가정하에 작동한다.</target>
        </trans-unit>
        <trans-unit id="62507dafeac3a5da5ac8979b39262545fa83f997" translate="yes" xml:space="preserve">
          <source>The original image data. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="translated">원본 이미지는 250 x 250 픽셀이지만 기본 슬라이스 및 크기 조정 인수는 62 x 47로 줄입니다.</target>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="translated">다른 커널</target>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="translated">행 및 열 레이블 벡터의 외부 곱은 바둑판 구조를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">출력 &lt;code&gt;y&lt;/code&gt; 는 다음 공식에 따라 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="translated">&lt;code&gt;n_informative&lt;/code&gt; 를 사용 하여 (잠재적으로 바이어스 된) 임의 선형 회귀 모델을 적용하여 출력이 생성됩니다. nonzero 이전에 생성 된 입력과 일부 가우스 중심 노이즈에 조정 가능한 스케일 .</target>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="translated">특이 값 분해의 출력은 특이 벡터의 부호의 순열까지만 유일합니다. 경우 &lt;code&gt;flip_sign&lt;/code&gt; 가 설정되어 &lt;code&gt;True&lt;/code&gt; , 부호의 모호성은 왼쪽 단수 벡터 양의 각 구성 요소에 대한 가장 큰 부하를함으로써 해결됩니다.</target>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="translated">3 개 모델의 출력은 2D 그래프로 결합되며 여기서 노드는 주식을 나타내고 가장자리는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="translated">변환의 출력은 때때로 1-K 코딩 방식으로 지칭된다.</target>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="translated">변환의 출력은 때때로 일부 작성자에 의해 1-K 코딩 방식으로 지칭됩니다.</target>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="translated">출력값</target>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="translated">Isomap의 전체 복잡도는 \ (O [D \ log (k) N \ log (N)] + O [N ^ 2 (k + \ log (N))] + O [d N ^ 2] \)입니다.</target>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="translated">MLLE의 전체 복잡도는 \ (O [D \ log (k) N \ log (N)] + O [DN k ^ 3] + O [N (kD) k ^ 2] + O [d N ^ 2]입니다. \).</target>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">스펙트럼 임베딩의 전체적인 복잡성은 \ (O [D \ log (k) N \ log (N)] + O [DN k ^ 3] + O [d N ^ 2] \)입니다.</target>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="translated">표준 HLLE의 전체 복잡성은 \ (O [D \ log (k) N \ log (N)] + O [DN k ^ 3] + O [N d ^ 6] + O [d N ^ 2] \)입니다. .</target>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="translated">표준 LLE의 전반적인 복잡성은 \ (O [D \ log (k) N \ log (N)] + O [DN k ^ 3] + O [d N ^ 2] \)입니다.</target>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="translated">표준 LTSA의 전체 복잡도는 \ (O [D \ log (k) N \ log (N)] + O [DN k ^ 3] + O [k ^ 2 d] + O [d N ^ 2] \)입니다. .</target>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="translated">점수가 우연히 획득 될 확률과 비슷한 p- 값입니다. 이것은 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="translated">&lt;code&gt;numpy&lt;/code&gt; 가 &lt;code&gt;Accelerate&lt;/code&gt; Framework를 사용하면 OS X에서 병렬 버전의 K-Means가 손상됩니다 . 이것은 예상 된 동작입니다. 포크 후에 &lt;code&gt;Accelerate&lt;/code&gt; 를 호출 할 수 있지만 다중 프로세스는 posix에서는 수행하지 않는 Python 바이너리로 하위 프로세스를 실행해야합니다.</target>
        </trans-unit>
        <trans-unit id="6d1486e3a09b61694eecc888bffbc6ccf12d8263" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf&quot; id=&quot;id20&quot;&gt;[HTF]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id21&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="translated">파라미터 &lt;code&gt;learning_rate&lt;/code&gt; 이 강하게 파라미터와 상호 작용 &lt;code&gt;n_estimators&lt;/code&gt; 약한 학습자의 수에 맞게. &lt;code&gt;learning_rate&lt;/code&gt; 값이 작을수록 지속적인 학습 오류를 유지하기 위해 많은 수의 약한 학습자가 필요합니다. 경험적 증거는 작은 값의 &lt;code&gt;learning_rate&lt;/code&gt; 가 더 나은 테스트 오류를 ​​선호 한다는 것을 시사합니다 . &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; 는 학습률을 작은 상수 (예 : &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt; )로 설정하고 조기 중지로 &lt;code&gt;n_estimators&lt;/code&gt; 를 선택할 것을 권장 합니다. &lt;code&gt;learning_rate&lt;/code&gt; 와 &lt;code&gt;n_estimators&lt;/code&gt; 간의 상호 작용에 대한 자세한 내용 은 &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="translated">변압기를 캐싱하려면 파라미터 &lt;code&gt;memory&lt;/code&gt; 가 필요합니다. &lt;code&gt;memory&lt;/code&gt; 는 변환기를 캐시 할 디렉토리가 포함 된 문자열이거나 &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; 오브젝트 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="171048c7012440ddda965396141222fe52434637" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;normalize&lt;/code&gt; allows to report ratios instead of counts. The confusion matrix can be normalized in 3 different ways: &lt;code&gt;'pred'&lt;/code&gt;, &lt;code&gt;'true'&lt;/code&gt;, and &lt;code&gt;'all'&lt;/code&gt; which will divide the counts by the sum of each columns, rows, or the entire matrix, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="translated">\ (\ nu \) 매개 변수 는 기울기 하강 절차의 단계 길이를 조정하므로 &lt;strong&gt;학습 속도&lt;/strong&gt; 라고도합니다 . &lt;code&gt;learning_rate&lt;/code&gt; 매개 변수 를 통해 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="translated">매개 변수 epsilon은 특이 값으로 분류되어야하는 샘플 수를 제어합니다. 입실론이 작을수록 특이 치에 대해 더 견고합니다.</target>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="translated">사전에 추정기 매개 변수를 허용 된 값의 시퀀스에 맵핑하는 탐색 할 매개 변수 그리드입니다.</target>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="translated">l1_ratio 매개 변수는 glmnet R 패키지의 알파에 해당하고 alpha는 glmnet의 람다 매개 변수에 해당합니다. 보다 구체적으로 최적화 목표는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="translated">l1_ratio 매개 변수는 glmnet R 패키지의 알파에 해당하고 alpha는 glmnet의 람다 매개 변수에 해당합니다. 특히 l1_ratio = 1은 올가미 페널티입니다. 현재 l1_ratio &amp;lt;= 0.01은 고유 한 알파 시퀀스를 제공하지 않으면 신뢰할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="translated">학습 된 기능의 평활도를 제어하는 ​​매개 변수 nu nu가 작을수록 근사 함수의 부드러움이 줄어 듭니다. nu = inf의 경우 커널은 RBF 커널과 같고 nu = 0.5의 경우 절대 지수 커널과 같습니다. 중요한 중간 값은 nu = 1.5 (한 번 차별화 함수)와 nu = 2.5 (두 번 차별화 함수)입니다. [0.5, 1.5, 2.5, inf]에없는 nu 값은 수정 된 Bessel 함수를 평가해야하기 때문에 계산 비용이 상당히 높습니다 (약 10 배 더 높음). 또한, l과 달리, nu는 초기 값으로 고정되어 있고 최적화되지는 않았다.</target>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="translated">매개 변수 \ (\ sigma_y \) 및 \ (\ mu_y \)는 최대 우도를 사용하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="translated">매개 변수 \ (\ theta_y \)는 최대 가능성의 평활화 된 버전, 즉 상대 주파수 계산에 의해 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="c10d9b59570fee3848efafd20062e78feab2baf1" translate="yes" xml:space="preserve">
          <source>The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model, the regularization parameters \(\alpha\) and \(\lambda\) being estimated by maximizing the &lt;em&gt;log marginal likelihood&lt;/em&gt;. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \(\alpha\) and \(\lambda\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters &lt;code&gt;alpha_init&lt;/code&gt; and &lt;code&gt;lambda_init&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt; 클래스 의 매개 변수 구현은 가중치 분포에 대한 두 가지 유형의 사전을 제안합니다 : Dirichlet 분포를 갖는 유한 혼합물 모델 및 Dirichlet Process를 사용한 무한 혼합 모델. 실제로 Dirichlet Process 추론 알고리즘은 근사치이며 고정 된 최대 개수의 구성 요소 (스틱 분리 표현이라고 함)와 함께 잘린 분포를 사용합니다. 실제로 사용되는 구성 요소의 수는 거의 항상 데이터에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="translated">이러한 방법을 적용하는 데 사용되는 추정기의 모수는 모수 모눈에서 교차 검증 된 그리드 검색에 의해 최적화됩니다.</target>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="translated">이러한 방법을 적용하는 데 사용되는 추정기의 매개 변수는 교차 검증 된 검색 매개 변수 설정에 의해 최적화됩니다.</target>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="translated">선택한 기능에 대한 전력 변환 매개 변수.</target>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="translated">선택된 매개 변수는 스코어링 매개 변수에 따라 보류 된 데이터의 점수를 최대화하는 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="translated">선택된 매개 변수는 명시 적 점수가 전달되지 않는 경우 제외 된 데이터의 점수를 최대화하는 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="translated">평가 된 매개 변수.</target>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="translated">각 노드의 부모 연결 매트릭스가 지정된 경우에만 '없음'이 리턴됩니다.</target>
        </trans-unit>
        <trans-unit id="491910df08517ded9cf6622a10e93f79d8d7b51e" translate="yes" xml:space="preserve">
          <source>The partial depdendence curves can be plotted for the multi-layer perceptron. In this case, &lt;code&gt;line_kw&lt;/code&gt; is passed to &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to change the color of the curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 에서 부분 의존성 함수가 평가되었습니다 . 회귀 및 이진 분류의 경우 &lt;code&gt;n_classes==1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="95b5d0f6fa140b6ee1f70868d5a14424c22e2d4c" translate="yes" xml:space="preserve">
          <source>The partial dependence of the response \(f\) at a point \(x_S\) is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">수동적 공격적 알고리즘은 대규모 학습을위한 알고리즘 계열입니다. 학습 속도가 필요하지 않다는 점에서 퍼셉트론과 유사합니다. 그러나 Perceptron과 달리 정규화 매개 변수 &lt;code&gt;C&lt;/code&gt; 가 포함 됩니다.</target>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="translated">데이터 저장소로 사용할 기본 디렉토리의 경로 또는 없음 None을 지정하면 캐싱이 수행되지 않고 Memory 객체가 완전히 투명합니다. 이 옵션은 버전 0.12 이후의 cachedir을 대체합니다.</target>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="translated">scikit-learn 데이터 디렉토리의 경로입니다.</target>
        </trans-unit>
        <trans-unit id="d92bdbe34c5616d0c8598712ff33d12e5d0daade" translate="yes" xml:space="preserve">
          <source>The path to the location of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2f9a75941863d91af30a0efa7c5bf671f8aabb3" translate="yes" xml:space="preserve">
          <source>The path to the location of the target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e58c61969eaf91a150b8c119238c04ee82b41f7" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">사용될 페널티 (일명 정규화 용어). 선형 SVM 모델의 표준 정규화 기인 'l2'가 기본값입니다. 'l1'과 'elasticnet'은 'l2'로는 달성 할 수없는 모델 (기능 선택)에 희소성을 가져올 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="translated">사용될 페널티 (일명 정규화 용어). 기본값은 없음입니다.</target>
        </trans-unit>
        <trans-unit id="3874ef3081cc70f2fd57e0725f8bff89bf9a04cd" translate="yes" xml:space="preserve">
          <source>The performance is may slightly worse for the randomized search, and is likely due to a noise effect and would not carry over to a held-out test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="translated">무작위 검색의 경우 성능이 약간 저하되지만 노이즈 효과 일 가능성이 높으며 보류 테스트 세트로 넘어 가지 않습니다.</target>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="translated">&lt;em&gt;k-&lt;/em&gt; 겹 교차 검증으로 보고 된 성능 측정 값은 루프에서 계산 된 값의 평균입니다. 이 방법은 계산 비용이 많이 들지만 (임의의 유효성 검사 세트를 수정하는 경우와 같이) 너무 많은 데이터를 낭비하지 않으며, 이는 샘플 수가 매우 적은 역 추론과 같은 문제에서 주요 이점입니다.</target>
        </trans-unit>
        <trans-unit id="773c4b551d63eb1e8cc16b04d1476c48de12db0a" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="translated">SAMME 및 SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; 알고리즘 의 성능 이 비교됩니다. SAMME.R은 확률 추정값을 사용하여 덧셈 모델을 업데이트하지만 SAMME는 분류 만 사용합니다. 예제에서 알 수 있듯이 SAMME.R 알고리즘은 일반적으로 SAMME보다 빠르게 수렴되어 부스팅 반복 횟수가 줄어 테스트 오류가 줄어 듭니다. 각 부스팅 반복 후 테스트 세트의 각 알고리즘 오류는 왼쪽에 표시되고 각 트리의 테스트 세트에 대한 분류 오류는 중간에 표시되며 각 트리의 부스트 가중치는 오른쪽에 표시됩니다. SAMME.R 알고리즘에서 모든 트리의 가중치는 1이므로 표시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="54c9475018dc0386efaf6207cbf2038791834d22" translate="yes" xml:space="preserve">
          <source>The performance of the models can be evaluated by their ability to yield well-calibrated predictions and a good ranking.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="translated">선택한 하이퍼 파라미터 및 훈련 된 모델의 성능은 모델 선택 단계에서 사용되지 않은 전용 평가 세트에서 측정됩니다.</target>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="translated">커널의 주기성</target>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="translated">복잡도는 \ (k = 2 ^ {(S)} \)로 정의됩니다. 여기서 \ (S \)는 조건부 확률 분포의 Shannon 엔트로피입니다. \ (k \)-사이드 다이의 복잡성은 \ (k \)이므로, \ (k \)는 조건부 확률을 생성 할 때 t-SNE가 고려하는 가장 가까운 이웃의 수입니다. 더 큰 난관은 더 가까운 이웃으로 이어지고 작은 구조에 덜 민감합니다. 반대로 낮은 난관은 소수의 이웃을 고려하므로 더 많은 글로벌 정보를 무시하고 지역 이웃에 유리합니다. 데이터 세트 크기가 커짐에 따라 지역 주변의 합리적인 표본을 얻기 위해서는 더 많은 포인트가 필요하므로 더 큰 난이도가 필요할 수 있습니다. 마찬가지로 잡음이 많은 데이터 세트는 배경 잡음을 넘어서 볼 수있는 충분한 로컬 이웃을 포함하기 위해 더 큰 난수 값이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="61a03c034b8bc8a8d92d85436ffe8ab0dc0dda5b" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significanlty different results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="translated">당황은 다른 매니 폴드 학습 알고리즘에 사용되는 가장 가까운 이웃의 수와 관련이 있습니다. 더 큰 데이터 세트는 일반적으로 더 큰 난이도가 필요합니다. 5와 50 사이의 값을 선택하는 것을 고려하십시오. t-SNE는이 매개 변수에 매우 민감하지 않으므로 선택은 매우 중요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="042f3b8255d615a1ffcd846018bae060090b690e" translate="yes" xml:space="preserve">
          <source>The physical location of boston csv dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0881df888192122fb3f0dbde6c8cd7ff6f0357c" translate="yes" xml:space="preserve">
          <source>The pipeline below extracts the subject and body from each post using &lt;code&gt;SubjectBodyExtractor&lt;/code&gt;, producing a (n_samples, 2) array. This array is then used to compute standard bag-of-words features for the subject and body as well as text length and number of sentences on the body, using &lt;code&gt;ColumnTransformer&lt;/code&gt;. We combine them, with weights, then train a classifier on the combined set of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="translated">결 측값 자리 표시 자입니다. &lt;code&gt;missing_values&lt;/code&gt; 의 모든 발생이 대치 됩니다.</target>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="translated">결 측값 자리 표시 자입니다. &lt;code&gt;missing_values&lt;/code&gt; 의 모든 발생이 대치 됩니다. np.nan으로 인코딩 된 결 측값의 경우 문자열 값 &quot;NaN&quot;을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="7d178852ef63f5a60aca24eb2ef7cd366b0501af" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For pandas&amp;rsquo; dataframes with nullable integer dtypes with missing values, &lt;code&gt;missing_values&lt;/code&gt; should be set to &lt;code&gt;np.nan&lt;/code&gt;, since &lt;code&gt;pd.NA&lt;/code&gt; will be converted to &lt;code&gt;np.nan&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="182f79373d80c85b96a18c9f29af644fec99f0de" translate="yes" xml:space="preserve">
          <source>The plot above tells us about dependencies between a specific feature and the target when all other features remain constant, i.e., &lt;strong&gt;conditional dependencies&lt;/strong&gt;. An increase of the AGE will induce a decrease of the WAGE when all other features remain constant. On the contrary, an increase of the EXPERIENCE will induce an increase of the WAGE when all other features remain constant. Also, AGE, EXPERIENCE and EDUCATION are the three variables that most influence the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="translated">도표는 분류기의 학습 곡선을 나타냅니다. 미니 배치 과정에서 분류 정확도의 진화. 정확성은 검증 세트로 유지되는 첫 1000 개의 샘플에서 측정됩니다.</target>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="translated">이 그림은 선형 판별 분석 및 2 차 판별 분석의 결정 경계를 보여줍니다. 맨 아래 행은 선형 판별 분석이 선형 경계 만 학습 할 수있는 반면 2 차 판별 분석은 2 차 경계를 학습 할 수 있으므로 더 유연하다는 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="b078f7cb7ea97d6d1e43a3acd4d110579a0862d7" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="translated">이 그림은 4 개의 단방향 및 1 개의 양방향 부분 의존도를 보여줍니다. 단방향 PDP의 대상 변수는 중간 소득 ( &lt;code&gt;MedInc&lt;/code&gt; ), 평균입니다. 가구당 거주자 ( &lt;code&gt;AvgOccup&lt;/code&gt; ), 평균 주택 연령 ( &lt;code&gt;HouseAge&lt;/code&gt; ) 및 평균. 가구당 객실 수 ( &lt;code&gt;AveRooms&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="translated">플롯은 이산화 된 인코딩이 일정한 영역을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="translated">아래 그림은 매개 변수 &lt;code&gt;C&lt;/code&gt; 가 분리선에 미치는 영향을 보여줍니다 . &lt;code&gt;C&lt;/code&gt; 의 큰 값은 기본적으로 모델에 데이터 분포에 대한 많은 믿음이 없으며 분리 선에 가까운 점만 고려할 것임을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="translated">도표는 먼저 3 개의 군집을 사용하여 K- 평균 알고리즘이 산출하는 것을 표시합니다. 그러면 잘못된 초기화가 분류 프로세스에 미치는 영향을 알 수 있습니다. n_init를 1 (기본값은 10)로 설정하면 알고리즘이 다른 중심 시드로 실행되는 횟수가 줄어 듭니다. 다음 줄거리는 8 개의 클러스터를 사용하여 무엇을 제공 할 것인지, 그리고 궁극적으로 진실을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="translated">도표는 예측 대기 시간의 분포를 상자 그림으로 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="9d34744e2f178eb734dfa28bde9ee7e67cdcc28f" translate="yes" xml:space="preserve">
          <source>The plots show four 1-way and two 1-way partial dependence plots (omitted for &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; due to computation time). The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), average occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and average rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="translated">플롯은 단색으로 훈련 포인트와 반투명 테스트 포인트를 보여줍니다. 오른쪽 하단에는 테스트 세트의 분류 정확도가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="translated">위의 관측치에 걸쳐있는 점 구름은 한 방향으로 매우 평평합니다. 세 가지 일 변량 피쳐 중 하나는 다른 두 가지를 사용하여 거의 정확하게 계산할 수 있습니다. PCA는 데이터가 &lt;em&gt;평평&lt;/em&gt; 하지 않은 방향을 찾습니다.&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="translated">포인트</target>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="translated">다항식 커널은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="translated">각 기능 클러스터에 대한 풀링 된 값입니다.</target>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="translated">가능한 옵션은 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'또는 회귀 손실 ( 'squared_loss', 'huber', 'epsilon_insensitive'또는 'squared_epsilon_insensitive')입니다.</target>
        </trans-unit>
        <trans-unit id="6a0542e47ae26fdcd6f04cfe1be082ea4e8e2319" translate="yes" xml:space="preserve">
          <source>The power determines the underlying target distribution according to the following table:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="translated">점 사이의 거리를 계산하는 데 사용되는 Minkowski 지표의 검정력입니다.</target>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="translated">전력 변환은 균일 성 및 정규성이 요구되는 모델링 문제의 변환으로서 유용하다. 다음은 6 가지 확률 분포에 적용되는 Box-Cox 및 Yeo-Johnwon의 예입니다 : Lognormal, Chi-squared, Weibull, Gaussian, Uniform 및 Bimodal.</target>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="translated">전력 변환 방법. 사용 가능한 방법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="translated">전력 변환 방법. 현재 'box-cox'(Box-Cox 변환)가 사용 가능한 유일한 옵션입니다.</target>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="translated">정밀도는 비율 &lt;code&gt;tp / (tp + fp)&lt;/code&gt; 여기서 &lt;code&gt;tp&lt;/code&gt; 는 진양 수이고 &lt;code&gt;fp&lt;/code&gt; 는 오 탐지 수입니다. 정밀도는 직관적으로 분류자가 음수 인 샘플을 양수로 표시하지 않는 기능입니다.</target>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">혼합물의 각 성분에 대한 정밀 매트릭스. 정밀 행렬은 공분산 행렬의 역수입니다. 공분산 행렬은 대칭 양수로 한정되므로 가우스 혼합은 정밀 행렬에 의해 동등하게 매개 변수화 될 수 있습니다. 공분산 행렬 대신 정밀 행렬을 저장하면 테스트 시간에 새로운 샘플의 로그 가능성을보다 효율적으로 계산할 수 있습니다. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="translated">현재 공분산 객체와 연관된 정밀 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="translated">평균 분포에서 각 구성 요소의 정밀도 (가우시안).</target>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="translated">평균 분포 이전의 정밀도 (가우시안). 수단을 배치 할 수있는 확장을 제어합니다. 값이 작을수록 각 군집의 평균이 &lt;code&gt;mean_prior&lt;/code&gt; 주위에 집중 됩니다.</target>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="translated">평균 분포 이전의 정밀도 (가우시안). 수단을 배치 할 수있는 위치까지 확장을 제어합니다. 값이 작을수록 각 군집의 평균이 &lt;code&gt;mean_prior&lt;/code&gt; 주위에 집중 됩니다. 매개 변수의 값은 0보다 커야합니다. None이면 1로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="99b687233f264a106cecce1d5b60aee20c688782" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. If mean_precision_prior is set to None, &lt;code&gt;mean_precision_prior_&lt;/code&gt; is set to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88b322c6c16cb10f58188170d43f7d8832a9357f" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it is set to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="translated">정밀 리콜 곡선은 서로 다른 임계 값에 대한 정밀도와 리콜 간의 절충점을 보여줍니다. 곡선 아래의 높은 영역은 높은 회수율과 높은 정밀도를 나타내며, 높은 정밀도는 낮은 오 탐지율과 관련이 있으며 높은 리콜은 낮은 오 탐률과 관련이 있습니다. 두 등급 모두에서 높은 점수를 받으면 분류 기가 정확한 결과 (높은 정밀도)를 반환 할뿐 아니라 모든 긍정적 결과 (대부분의 리콜)를 반환한다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="translated">X의 각 샘플에 대해 예측 된 클래스 C가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 된 클래스 로그 확률은 앙상블에서 기본 추정기의 평균 예측 된 클래스 확률의 로그로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="translated">입력 샘플의 예측 된 클래스 로그 확률은 포리스트에있는 나무의 평균 예측 된 클래스 확률의 로그로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 된 클래스 로그 확률은 앙상블에서 분류기의 가중 평균 예측 된 클래스 로그 확률로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="translated">입력 샘플의 예측 된 클래스는 확률 추정값에 따라 가중치가 지정된 포리스트의 나무에 의한 투표입니다. 즉, 예측 클래스는 트리 전체에서 평균 확률이 가장 높은 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="translated">입력 샘플의 예측 된 클래스는 평균 예측 확률이 가장 높은 클래스로 계산됩니다. 기본 견적자가 &lt;code&gt;predict_proba&lt;/code&gt; 메소드를 구현하지 않으면 투표에 의지합니다.</target>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 된 클래스는 앙상블에서 분류기의 가중 평균 예측으로서 계산된다.</target>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">입력 샘플의 예측 된 클래스 확률은 숲에서 나무의 평균 예측 된 클래스 확률로 계산됩니다. 단일 트리의 클래스 확률은 잎에서 동일한 클래스의 샘플 비율입니다.</target>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="translated">입력 샘플의 예측 클래스 확률은 앙상블에서 기본 추정기의 평균 예측 클래스 확률로 계산됩니다. 기본 추정기가 &lt;code&gt;predict_proba&lt;/code&gt; 메소드를 구현하지 않으면 투표에 의존 하고 입력 샘플의 예측 된 클래스 확률은 각 클래스를 예측하는 추정기의 비율을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 클래스 확률은 앙상블에서 분류기의 가중 평균 예측 클래스 확률로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="translated">예측 된 클래스 확률은 리프에서 동일한 클래스의 샘플 비율입니다.</target>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="translated">예측 된 클래스.</target>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="translated">예측 된 클래스 또는 예측 값</target>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="translated">예측 된 클래스.</target>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="translated">&lt;code&gt;self.classes_&lt;/code&gt; 와 같이 클래스가 정렬 된 모델의 각 클래스에 대한 샘플의 예상 로그 확률입니다 . log (predict_proba (X))와 동일</target>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self.classes_&lt;/code&gt; 와 같이 클래스가 정렬 된 모델의 각 클래스에 대한 샘플의 예측 확률입니다 .</target>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="translated">예측 된 프로 바.</target>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 회귀 목표는 앙상블에서 추정기의 평균 예측 회귀 목표로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="translated">입력 샘플의 예측 회귀 목표는 숲에서 나무의 평균 예측 회귀 목표로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="translated">입력 샘플의 예측 된 회귀 값은 앙상블에서 분류기의 가중 평균 예측으로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="translated">예측 된 회귀 값입니다.</target>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="translated">예측 된 목표 값.</target>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="translated">입력 샘플의 예상 값입니다.</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">예측 된 값.</target>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="translated">예측은 관찰을 보간합니다 (적어도 일반 커널의 경우).</target>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="translated">예측은 확률 적 (가우시안)이므로 경험적 신뢰 구간을 계산하고 관심있는 일부 영역에서 예측을 수정 (온라인 피팅, 적응 피팅)해야하는지에 따라 결정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="888b741b5759a3600a521b02ebf591af7172583e" translate="yes" xml:space="preserve">
          <source>The prediction is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92250eb4257ffd9737b8e4be4f65018174b8c07a" translate="yes" xml:space="preserve">
          <source>The predictions for all the points in the grid, averaged over all samples in X (or over the training data if &lt;code&gt;method&lt;/code&gt; is &amp;lsquo;recursion&amp;rsquo;). &lt;code&gt;n_outputs&lt;/code&gt; corresponds to the number of classes in a multi-class setting, or to the number of tasks for multi-output regression. For classical regression and binary classification &lt;code&gt;n_outputs==1&lt;/code&gt;. &lt;code&gt;n_values_feature_j&lt;/code&gt; corresponds to the size &lt;code&gt;values[j]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f4304f9d52554d908e981e82f60b25cc189525a" translate="yes" xml:space="preserve">
          <source>The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="translated">이전에 소개 된 측정 항목은 &lt;strong&gt;무작위 표시와 관련하여 정규화되지 않았습니다&lt;/strong&gt; . 즉, 샘플, 군집 및지면 진실 클래스의 수에 따라 완전히 임의의 표시가 항상 동질성, 완전성 및 v- 측정에 대해 동일한 값을 생성하지는 않습니다. 특히 &lt;strong&gt;무작위 레이블링은 특히 클러스터 수가 많은 경우 0 점을 얻지 않습니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="b298c69bd549b9b894cc39801befe9de3ee0bc57" translate="yes" xml:space="preserve">
          <source>The primal problem can be equivalently formulated as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="translated">가장 가까운 이웃 방법의 기본 원리는 새로운 점에 가장 가까운 사전 정의 된 수의 훈련 샘플을 찾아 이들로부터 라벨을 예측하는 것입니다. 샘플의 수는 사용자 정의 상수 (k- 최근 접 이웃 학습) 일 수 있거나 점의 로컬 밀도 (반경 기반 이웃 학습)에 따라 달라집니다. 일반적으로 거리는 모든 측정 기준이 될 수 있습니다. 표준 유클리드 거리가 가장 일반적인 선택입니다. 이웃 기반 방법은 모든 일반 교육 데이터를 단순히 &quot;기억&quot;하기 때문에 (일반적으로 &lt;a href=&quot;#ball-tree&quot;&gt;볼 트리&lt;/a&gt; 또는 &lt;a href=&quot;#kd-tree&quot;&gt;KD 트리&lt;/a&gt; 와 같은 빠른 색인 구조로 변환되기 때문에) &lt;em&gt;일반화되지 않은&lt;/em&gt; 기계 학습 방법으로 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="translated">GP로 인한 GP의 전후 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt; 커널로는 다음 그림과 같습니다.</target>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="translated">ExpSineSquared 커널로 인한 GP의 앞뒤는 다음 그림과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="translated">공분산 분포에 대한 자유도 (Wishart)의 선행.</target>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="translated">공분산 분포에 대한 자유도 (Wishart)의 선행. None이면 &lt;code&gt;n_features&lt;/code&gt; 로 설정됩니다 . .</target>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">공분산 분포에 대한 사전 (Wishart). None이면 X의 공분산을 사용하여 이전의 반구형 공분산이 초기화됩니다. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">공분산 분포에 대한 사전 (Wishart). 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="translated">평균 분포에 대한 사전 (가우시안).</target>
        </trans-unit>
        <trans-unit id="330376751a07bab7a9ae7af5823384716a02e1f4" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it is set to the mean of X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="translated">평균 분포에 대한 사전 (가우시안). None이면 X의 평균으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="translated">\ (\ alpha \) 및 \ (\ lambda \)에 대한 선행은 가우스 &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;분포 (Gammas distribution)&lt;/a&gt; , 가우시안의 정밀도를위한 선행 컨쥬 게이트 로 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="4f2acfb8ac15388ba72cc76c4859e9942703930f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian. The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="translated">확률 모델은 교차 검증을 사용하여 만들어 지므로 결과는 예측 결과와 약간 다를 수 있습니다. 또한 매우 작은 데이터 세트에서 의미없는 결과를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="a6cf9fb7eddd86de2e07f31468eef83947604765" translate="yes" xml:space="preserve">
          <source>The probability of category \(t\) in feature \(i\) given class \(c\) is estimated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">각 클래스가 그려 질 확률. &lt;code&gt;return_distributions=True&lt;/code&gt; 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="translated">각 등급에 따라 각 지형지 물이 그려 질 확률. &lt;code&gt;return_distributions=True&lt;/code&gt; 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="translated">계수가 0 일 확률 (주 참조). 값이 클수록 희소성이 높아집니다.</target>
        </trans-unit>
        <trans-unit id="f5e975aa46dbf2f661a2bf284eb823b950bfb65a" translate="yes" xml:space="preserve">
          <source>The problem of correlated variables</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="translated">최적의 의사 결정 트리를 학습하는 문제는 최적의 여러 측면에서 단순한 개념으로 NP 완료된 것으로 알려져 있습니다. 결과적으로 실용적인 의사 결정 트리 학습 알고리즘은 각 노드에서 로컬로 최적의 결정을 내리는 욕심 알고리즘과 같은 휴리스틱 알고리즘을 기반으로합니다. 이러한 알고리즘은 최적의 의사 결정 트리를 반환한다고 보장 할 수 없습니다. 이 기능은 앙상블 학습자에서 여러 트리를 학습하여 완화 할 수 있으며, 기능과 샘플은 임의로 교체하여 샘플링됩니다.</target>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="translated">클러스터링에서 해결 된 문제</target>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="translated">지도 학습에서 해결 된 문제</target>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="translated">진행률 표시기 : &lt;code&gt;verbose&lt;/code&gt; 값이 높을수록 더 많은 메시지가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="translated">프로젝트 메일 링리스트</target>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="translated">예상 데이터</target>
        </trans-unit>
        <trans-unit id="8bfacd813c2539118e4e65b3f249a31e3f90cba3" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;(n_sample + n_features + 1) / 2&lt;/code&gt;. The parameter must be in the range (0, 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="translated">원시 MCD 추정값을 지원하는 데 포함될 포인트의 비율입니다. 기본값은 없음입니다. 이는 support_fraction의 최소값이 알고리즘 내에서 사용됨을 의미합니다. [n_sample + n_features + 1] / 2</target>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="translated">원시 MCD 추정값을 지원하는 데 포함될 포인트의 비율입니다. None 인 경우 support_fraction의 최소값이 알고리즘 내에서 사용됩니다 : &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2ecaf585d98315eb2e879bcbfec7a3d49600f7b8" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;. Range is (0, 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd542c4196f5720eadadf90803a23b09f997f270" translate="yes" xml:space="preserve">
          <source>The proportion of samples whose class is the positive class, in each bin (fraction of positives).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6be8a7dea2b475115d00cec72a7cea2ea475dbdd" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;early_stopping&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="translated">조기 중지를위한 검증 세트로 따로 설정할 교육 데이터의 비율입니다. 0과 1 사이 여야합니다. &lt;code&gt;n_iter_no_change&lt;/code&gt; 가 정수로 설정된 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="translated">조기 중지를위한 검증 세트로 따로 설정할 교육 데이터의 비율입니다. 0과 1 사이 여야합니다. early_stopping이 True 인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="translated">조기 중지를위한 검증 세트로 따로 설정할 교육 데이터의 비율입니다. 0과 1 사이 여야합니다. early_stopping이 True 인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="translated">각 클래스에 할당 된 샘플의 비율. 없음 인 경우 클래스 균형이 조정됩니다. 경우 참고 &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt; 후, 마지막 클래스 무게가 자동으로 유추됩니다. &lt;code&gt;weights&lt;/code&gt; 의 합 이 1을 초과하면 &lt;code&gt;n_samples&lt;/code&gt; 이상의 샘플이 반환 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e063e4585132e12e4e3b8bc3e0e5f8f9333735ee" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of &lt;code&gt;components_&lt;/code&gt;. It is the linear operator that maps independent sources to the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b5cae22ba8a7c5a2306b5698bd81974f62f5c35" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to &amp;lsquo;passthrough&amp;rsquo; or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="translated">파이프 라인의 목적은 서로 다른 매개 변수를 설정하면서 함께 교차 검증 될 수있는 여러 단계를 조립하는 것입니다. 이를 위해 아래 예와 같이 이름과 '__'으로 구분 된 매개 변수 이름을 사용하여 다양한 단계의 매개 변수를 설정할 수 있습니다. 단계 추정기는 매개 변수 이름을 다른 추정기로 설정하거나 변압기를 None으로 설정하여 제거함으로써 완전히 대체 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7aabe2d97cf9256a5a6bd2e4c38d95ce8f939f1c" translate="yes" xml:space="preserve">
          <source>The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="translated">모델을 생성하는 데 사용되는 파이썬 소스 코드</target>
        </trans-unit>
        <trans-unit id="a971c90794b3ab81f6079922073a4f8be4eaa91d" translate="yes" xml:space="preserve">
          <source>The qualitative difference between these models can also be visualized by comparing the histogram of observed target values with that of predicted values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="translated">&quot;quantile&quot;전략을 사용하여 예측하는 Quantile입니다. Quantile 0.5는 중앙값에 해당하는 반면 0.0은 최소, 1.0은 최대입니다.</target>
        </trans-unit>
        <trans-unit id="eabfb5a10c049cb3056c46f05114ad3228125c8e" translate="yes" xml:space="preserve">
          <source>The quantity \(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}\) is the derivative of the loss with respect to its second parameter, evaluated at \(F_{m-1}(x)\). It is easy to compute for any given \(F_{m - 1}(x_i)\) in a closed form since the loss is differentiable. We will denote it by \(g_i\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="translated">우리가 사용하는 수량은 견적 가격의 일일 변동입니다. 연결된 견적은 하루 동안 변동하는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="translated">쿼리 포인트 제공하지 않으면 각 인덱스 포인트의 인접 항목이 반환됩니다. 이 경우 쿼리 지점은 자체 이웃으로 간주되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="translated">학습 샘플을 사용하여 로컬 특이 요인을 계산하기위한 쿼리 샘플.</target>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="translated">훈련 샘플에 대한 로컬 이상치 계수를 계산하기위한 쿼리 샘플.</target>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="translated">새 샘플과 가장 가까운 서브 클러스터를 병합하여 얻은 서브 클러스터의 반경은 임계 값보다 작아야합니다. 그렇지 않으면 새 서브 클러스터가 시작됩니다. 이 값을 매우 낮게 설정하면 분할이 촉진되고 그 반대도 가능합니다.</target>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="translated">랜덤 포레스트 회귀 분석은 관측 범위 내에서 또는 각 목표에 대해 0에 가까운 값만 예측합니다. 결과적으로 예측은 원의 중심을 향해 편향됩니다.</target>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="translated">난수 생성기는 난수 순서를 생성하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="translated">랜덤 대칭, 양의 유한 행렬.</target>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="translated">무작위 검색과 그리드 검색은 정확히 같은 공간의 매개 변수를 탐색합니다. 매개 변수 설정의 결과는 매우 유사하지만 무작위 검색의 런타임은 크게 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="translated">원시 (조정되지 않은) 랜드 인덱스는 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="translated">그런 다음 원시 RI 점수는 다음 체계를 사용하여 ARI 점수에&amp;ldquo;기회 조정&amp;rdquo;됩니다.</target>
        </trans-unit>
        <trans-unit id="c2c7c9eff2c5366f7dc8bd26b18e5e786e16eeff" translate="yes" xml:space="preserve">
          <source>The raw image data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31fb3fd5063abf3dc0c0004645bb280ed7e9b6d1" translate="yes" xml:space="preserve">
          <source>The raw predicted values (i.e. the sum of the trees leaves) for each sample. n_trees_per_iteration is equal to the number of classes in multiclass classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="translated">보정 및 재 가중치 이전의 원시 강력한 추정 공분산.</target>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="translated">보정 및 재 가중치 이전의 강력한 예상 추정 위치.</target>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="translated">실제 데이터는 &lt;code&gt;filenames&lt;/code&gt; 과 &lt;code&gt;target&lt;/code&gt; 속성에 있습니다. 대상 속성은 카테고리의 정수 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="translated">리콜은 &lt;code&gt;tp / (tp + fn)&lt;/code&gt; 의 비율입니다. 여기서 &lt;code&gt;tp&lt;/code&gt; 는 양수이고 &lt;code&gt;fn&lt;/code&gt; 은 음수입니다. 리콜은 직관적으로 분류 기가 모든 양성 샘플을 찾을 수있는 능력입니다.</target>
        </trans-unit>
        <trans-unit id="6f99b6066a1cdf81f95a9e89b923c52bb3877c2c" translate="yes" xml:space="preserve">
          <source>The reconstructed image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="translated">메트릭 MDS 및 비 메트릭 MDS를 사용하여 재구성 된 포인트는 겹치지 않도록 약간 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="translated">각 루틴에서 계산 한 재구성 오류를 사용하여 최적의 출력 치수를 선택할 수 있습니다. \ (D \)에 매립 차원 폴드 - - \ (d \)로써 차원 파라미터 공간, 재구성 에러가 감소 &lt;code&gt;n_components&lt;/code&gt; 이 될 때까지 증가된다 &lt;code&gt;n_components == d&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="translated">L1 페널티를 사용한 재구성은 투영에 노이즈가 추가 된 경우에도 제로 오류 (모든 픽셀에 0 또는 1로 레이블이 지정됨)가 발생합니다. 이에 비해 L2 불이익 ( &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; &lt;/a&gt; )은 픽셀에 대해 많은 수의 레이블링 오류를 생성합니다. L1 벌칙에 반하여 재구성 된 이미지에서 중요한 아티팩트가 관찰됩니다. 특히 코너에서 픽셀을 분리하는 원형 아티팩트는 중앙 디스크보다 더 적은 투영에 기여합니다.</target>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="translated">일부 메트릭에 대해 정의 된 감소 된 거리는 실제 거리의 순위를 유지하는 계산적으로 더 효율적인 측정입니다. 예를 들어, 유클리드 거리 메트릭에서 감소 된 거리는 제곱-유클리드 거리입니다.</target>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="translated">감소 된 샘플.</target>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">수정 된 추정기는 &lt;code&gt;best_estimator_&lt;/code&gt; 속성 에서 사용 가능 &lt;code&gt;GridSearchCV&lt;/code&gt; 인스턴스 에서 직접 &lt;code&gt;predict&lt;/code&gt; 사용을 허용 합니다.</target>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="translated">수정 된 추정기는 &lt;code&gt;best_estimator_&lt;/code&gt; 속성 에서 사용 가능 하며이 &lt;code&gt;RandomizedSearchCV&lt;/code&gt; 인스턴스 에서 직접 &lt;code&gt;predict&lt;/code&gt; 사용을 허용 합니다.</target>
        </trans-unit>
        <trans-unit id="d35d9dbef92f31bfc55b28e05410d9f2634f5c0d" translate="yes" xml:space="preserve">
          <source>The regression target for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="translated">해당되는 경우 회귀 목표 또는 분류 레이블. Dtype은 숫자이면 float이고 범주 형이면 object입니다.</target>
        </trans-unit>
        <trans-unit id="f49726f7b2ee9ab1b3928455718be0471fabc15a" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical. If &lt;code&gt;as_frame&lt;/code&gt; is True, &lt;code&gt;target&lt;/code&gt; is a pandas object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3833ac2bfe2b88ee813e1c7e696adf8c3874fe96" translate="yes" xml:space="preserve">
          <source>The regression target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b42b60d45f4e29bdab4ac89a5e2270a5774add83" translate="yes" xml:space="preserve">
          <source>The regression target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c39a60b37913113f18fb812e5aaafb85e77ce6aa" translate="yes" xml:space="preserve">
          <source>The regression targets. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="translated">회귀는 예측에 사용되며 예측 을 반환하기 전에 &lt;code&gt;inverse_func&lt;/code&gt; 또는 &lt;code&gt;inverse_transform&lt;/code&gt; 이 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="edcb36e5cf0943282d84be3c17128a4f09595f3b" translate="yes" xml:space="preserve">
          <source>The regressor that is used for calibration depends on the &lt;code&gt;method&lt;/code&gt; parameter. &lt;code&gt;'sigmoid'&lt;/code&gt; corresponds to a parametric approach based on Platt&amp;rsquo;s logistic model &lt;a href=&quot;#id8&quot; id=&quot;id4&quot;&gt;3&lt;/a&gt;, i.e. \(p(y_i = 1 | f_i)\) is modeled as \(\sigma(A f_i + B)\) where \(\sigma\) is the logistic function, and \(A\) and \(B\) are real numbers to be determined when fitting the regressor via maximum likelihood. &lt;code&gt;'isotonic'&lt;/code&gt; will instead fit a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function (see &lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ab5d35ab966f753794908f22950ec4d3c66e8fc" translate="yes" xml:space="preserve">
          <source>The regressor to stacked the base estimators fitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="translated">정규화 된 공분산은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1 인 정규화 혼합 매개 변수 l1_ratio = 0의 경우 페널티는 요소 별 L2 페널티 (일명 Frobenius Norm)입니다. l1_ratio = 1의 경우 요소 별 L1 페널티입니다. 0 &amp;lt;l1_ratio &amp;lt;1의 경우 패널티는 L1과 L2의 조합입니다.</target>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="translated">LogisticRegression의 정규화 매개 변수 C C가 배열 인 경우 fit은 LogisticRegression에 대해 C의 각 정규화 매개 변수를 하나씩 가져 &lt;code&gt;all_scores_&lt;/code&gt; 에 각각에 대한 결과를 저장합니다 . 여기서 열과 행은 해당 reg_parameters 및 기능을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="translated">올가미의 정규화 매개 변수 알파 매개 변수입니다. 경고 : 이것은 스케일링중인 안정성 선택 아티클의 알파 매개 변수가 아닙니다.</target>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="translated">정규화 매개 변수 : 알파가 높을수록 정규화가 많을수록 역공 분산이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="e124bf83498b6636f4567895eaf37c151f534f9f" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="688891a69c745428fa6d147d1a8f3e33c50a1bf6" translate="yes" xml:space="preserve">
          <source>The regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="translated">정규화 된 (공식) 공분산은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="translated">정규화 (축소) 공분산은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="translated">정규화 된 공분산은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="translated">정규화 기는 손실 함수에 추가되는 페널티로, 제곱 유클리드 규범 L2 또는 절대 규범 L1 또는이 둘의 조합 (Elastic Net)을 사용하여 모델 매개 변수를 제로 벡터로 축소합니다. 정규화로 인해 매개 변수 업데이트가 0.0 값을 초과하는 경우 스파 스 모델을 학습하고 온라인 기능 선택을 수행 할 수 있도록 업데이트가 0.0으로 잘립니다.</target>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="translated">리콜과 정밀도의 관계는 플롯의 계단 영역에서 관찰 할 수 있습니다. 이러한 단계의 가장자리에서 임계 값의 작은 변화는 리콜의 작은 이득만으로 정밀도를 상당히 감소시킵니다.</target>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="translated">&lt;code&gt;effective_rank&lt;/code&gt; 가 None이 아닌 경우 특이 값 프로파일의 지방 시끄러운 꼬리의 상대적 중요성 .</target>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="translated">특이 값 프로파일의 지방 시끄러운 꼬리의 상대적 중요성.</target>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="translated">수렴을 선언하기 전에 결과의 상대적 증가.</target>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="translated">트리에서 결정 노드로서 사용되는 특징의 상대 순위 (즉, 깊이)는 목표 변수의 예측 가능성과 관련하여 그 특징의 상대적 중요성을 평가하기 위해 사용될 수있다. 트리 상단에 사용 된 기능은 입력 샘플의 더 큰 부분에 대한 최종 예측 결정에 기여합니다. 그들이 기여한 &lt;strong&gt;샘플&lt;/strong&gt; 의 &lt;strong&gt;예상 부분은 &lt;/strong&gt;&lt;strong&gt;기능&lt;/strong&gt; 의 &lt;strong&gt;상대적 중요성을&lt;/strong&gt; 추정하는 데 사용될 수 &lt;strong&gt;있습니다&lt;/strong&gt; . scikit-learn에서 피처가 기여하는 샘플의 비율은 해당 피처의 예측력에 대한 표준화 된 추정치를 생성하기 위해 피펫을 분리하는 데 따른 불순물 감소와 결합됩니다.</target>
        </trans-unit>
        <trans-unit id="81bcb12f84ef974910f69bd2abe477a2b6a71621" translate="yes" xml:space="preserve">
          <source>The remaining columns can be used to predict the frequency of claim events. Those columns are very heterogeneous with a mix of categorical and numeric variables with different scales, possibly very unevenly distributed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="translated">나머지 특이 값의 꼬리는 뚱뚱하며 다음과 같이 감소합니다.</target>
        </trans-unit>
        <trans-unit id="ec1c6c51e9847818ba009f4ace94e61e7f83ab56" translate="yes" xml:space="preserve">
          <source>The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), and sample average (only for multilabel classification). Micro average (averaging the total true positives, false negatives and false positives) is only shown for multi-label or multi-class with a subset of classes, because it corresponds to accuracy otherwise. See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="translated">보고 된 평균에는 미시 평균 (총 진양 수 평균, 위음성 및 오 탐지 평균), 매크로 평균 (라벨 당 비가 중 평균 평균), 가중 평균 (라벨 당 평균 가중 평균 평균) 및 샘플 평균 (다중 라벨 만 해당) 분류). 평균에 대한 자세한 내용 은 &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="translated">X (Xk + 1) 블록의 잔차 행렬은 현재 X 점수 (x_score)의 수축으로 구합니다.</target>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="translated">Y (Yk + 1) 블록의 잔차 행렬은 현재 X 점수의 수축으로 구합니다. PLS2로 알려진 PLS 회귀를 수행합니다. 이 모드는 예측 지향적입니다.</target>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="translated">Y (Yk + 1) 블록의 잔차 행렬은 현재 Y 점수에 대한 수축으로 구합니다.</target>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="translated">Y (Yk + 1) 블록의 잔차 행렬은 현재 Y 점수에 대한 수축으로 구합니다. 이것은 표준 대칭 버전의 PLS 회귀를 수행합니다. 그러나 CCA와 약간 다릅니다. 주로 모델링에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f3c685007c5e417136c9d43a236103e7a7414e81" translate="yes" xml:space="preserve">
          <source>The result is quite similar to the non-normalized case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="translated">&lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;최소 각도 회귀&lt;/a&gt; 의 결과 는 훨씬 더 치우친 편향입니다. 차이는 원래 이미지의 로컬 강도 값을 연상시킵니다.</target>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="translated">결과 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; 을&lt;/a&gt; 사용하여 수득 된 것과 상이 할 수 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; 을&lt;/a&gt; 요소들은 상이한 방식으로 분류된다. &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 함수 는 교차 검증 배수에 대한 평균을 취하지 만, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; 는&lt;/a&gt; 단순히 구별되지 않은 여러 가지 모델에서 레이블 (또는 확률)을 반환합니다. 따라서 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; 는 적절한 일반화 오류 측정 값이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;GridSearchCV&lt;/code&gt; 객체 에 &lt;code&gt;fit&lt;/code&gt; 호출의 결과는 다음 과 같이 &lt;code&gt;predict&lt;/code&gt; 사용할 수있는 분류기입니다 .</target>
        </trans-unit>
        <trans-unit id="88a0848975f5bdc5b1a985f762b1245644b8930f" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to &lt;code&gt;np.diag(self(X))&lt;/code&gt;; however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="translated">이 방법의 결과는 np.diag (self (X))와 동일합니다. 그러나 대각선 만 평가되므로보다 효율적으로 평가할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="translated">결과 지점은 쿼리 지점까지의 거리에 따라 정렬 될 필요 는 &lt;em&gt;없습니다&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="97bd456bf5cbdca967559f8f3d5ed5bef7911efc" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabasz score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="translated">결과 Calinski-Harabaz 점수.</target>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="translated">결과 Davies-Bouldin 점수.</target>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="translated">결과적으로 Fowlkes-Mallows 점수가 나옵니다.</target>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="translated">각 행과 각 열이 정확히 하나의 bicluster에 속하기 때문에 결과적인 bicluster 구조는 블록 대각선입니다.</target>
        </trans-unit>
        <trans-unit id="a10cedad8ec8047b7f7badb13dd314c46b9b1d6c" translate="yes" xml:space="preserve">
          <source>The resulting counts are normalized using &lt;a href=&quot;sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt;&lt;/a&gt; unless normalize is set to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">결과 데이터 세트에는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 에서 추가로 사용할 수있는 서수 속성이 포함됩니다 .</target>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="translated">결과 커널은 k_exp (X, Y) = k (X, Y) ** 지수로 정의됩니다</target>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="translated">결과 커널은 k_prod (X, Y) = k1 (X, Y) * k2 (X, Y)로 정의됩니다</target>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="translated">결과 커널은 k_sum (X, Y) = k1 (X, Y) + k2 (X, Y)로 정의됩니다</target>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="translated">결과 모델을 &lt;em&gt;베이지안 릿지 회귀&lt;/em&gt; 라고 하며 클래식 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 와 비슷합니다 . 매개 변수 \ (w \), \ (\ alpha \) 및 \ (\ lambda \)는 모형 적합 동안 함께 추정됩니다. 나머지 하이퍼 파라미터는 \ (\ alpha \) 및 \ (\ lambda \)에 대한 감마 우선 순위의 매개 변수입니다. 이들은 일반적으로 &lt;em&gt;정보&lt;/em&gt; 가 &lt;em&gt;없는&lt;/em&gt; 것으로 선택됩니다 . &lt;em&gt;한계 로그 우도를&lt;/em&gt; 최대화하여 모수를 추정합니다 .</target>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="translated">결과 패치는 전용 배열로 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="translated">결과 트랜스포머는 데이터의 감독되고 드문 고차원 범주 형 임베딩을 배웠습니다.</target>
        </trans-unit>
        <trans-unit id="52f668ebefc79801d78ca36ee52d3c9f5a955a8d" translate="yes" xml:space="preserve">
          <source>The results from OPTICS &lt;code&gt;cluster_optics_dbscan&lt;/code&gt; method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="translated">리턴 값은 &lt;code&gt;split&lt;/code&gt; 방법을 통해 트레인 / 테스트 스플릿을 생성하는 교차 검증기입니다 .</target>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="translated">반환 된 데이터 셋은 &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;입니다. 파이썬 &lt;code&gt;dict&lt;/code&gt; 키 또는 편의를 위해 &lt;code&gt;object&lt;/code&gt; 속성 으로 액세스 할 수있는 필드가있는 간단한 홀더 객체 입니다. 예를 들어 &lt;code&gt;target_names&lt;/code&gt; 에는 요청 된 카테고리 이름 목록이 있습니다.</target>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="translated">모든 클래스에 대해 반환 된 견적은 클래스 레이블별로 정렬됩니다.</target>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="translated">모든 클래스에 대해 반환 된 견적은 클래스 레이블로 정렬됩니다.</target>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">반환 된 개체는 MemorizedFunc 개체로, 함수처럼 동작하지만 캐시 조회 및 관리를위한 추가 메서드를 제공합니다. &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt; &lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt; &lt;/a&gt; 문서를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="translated">오른쪽의 더 풍부한 사전은 크기가 크지 않으며 동일한 크기로 유지하기 위해 더 큰 서브 샘플링이 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="translated">오른쪽 그림은 동일한 플롯에 해당하지만 대신에 결정 트리의 배깅 앙상블을 사용합니다. 두 그림 모두에서, 바이어스 항이 이전 경우보다 더 크다는 것을 알 수 있습니다. 오른쪽 위 그림에서 평균 예측 (청록색)과 최상의 모델 간의 차이가 더 큽니다 (예 : &lt;code&gt;x=2&lt;/code&gt; 주위의 오프셋을 확인하십시오)). 오른쪽 아래 그림에서 바이어스 곡선은 왼쪽 아래 그림보다 약간 높습니다. 그러나 분산 측면에서 예측 빔은 더 좁아서 분산이 더 낮음을 나타냅니다. 실제로 오른쪽 아래 그림에서 확인할 수 있듯이 분산 용어 (녹색)는 단일 의사 결정 트리보다 낮습니다. 따라서 편향 분산 분석은 더 이상 동일하지 않습니다. 데이터 세트의 부트 스트랩 사본에 맞는 여러 의사 결정 트리를 평균하면 바이어스 용어가 약간 증가하지만 분산의 큰 감소를 허용하여 전체 평균 제곱 오차가 낮아집니다 (빨간색 곡선과 낮은 곡선 비교) 그림). 스크립트 출력도이 직관을 확인합니다. 배깅 앙상블의 총 오류는 단일 의사 결정 트리의 총 오류보다 낮습니다.이 차이는 실제로 분산 감소에서 비롯됩니다.</target>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="translated">오른쪽 그림은 모형에서 찾은 계수와 선택한 벡터 w 사이의 평균 제곱 오차를 보여줍니다. 덜 정규화 된 모델은 정확한 계수를 검색하고 (오류는 0과 같음), 더 정규화 된 모델은 오류를 증가시킵니다.</target>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="translated">\ (n_ \ text {samples}&amp;gt; 5n_ \ text {features} \)이면 오류가 적은 강력한 MCD</target>
        </trans-unit>
        <trans-unit id="ba79522dd51eaaef6a47e20f449051b56df7c07d" translate="yes" xml:space="preserve">
          <source>The roc curve requires either the probabilities or the non-thresholded decision values from the estimator. Since the logistic regression provides a decision function, we will use it to plot the roc curve:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="translated">행은 샘플이고 열은 Sepal Length, Sepal Width, Petal Length 및 Petal Width입니다.</target>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="translated">s 매개 변수는 다른 기능의 패널티를 임의로 조정하는 데 사용됩니다. 0과 1 사이 여야합니다.</target>
        </trans-unit>
        <trans-unit id="04a6b9de3c4814c0e0ce533f438ce9705b113ee4" translate="yes" xml:space="preserve">
          <source>The same as the min_samples given to OPTICS. Up and down steep regions can&amp;rsquo;t have more then &lt;code&gt;min_samples&lt;/code&gt; consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="translated">동일한 그룹은 서로 다른 두 개의 접기에 나타나지 않습니다 (개별 그룹의 수는 최소한 접기 수와 같아야합니다).</target>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="translated">그런 다음 변압기의 동일한 인스턴스를 맞춤 호출 중에 보이지 않는 새로운 테스트 데이터에 적용 할 수 있습니다. 기차 데이터에서 수행 된 변환과 일치하도록 동일한 스케일링 및 변속 작업이 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="9b63d107562f8bc53dcee73bc223f241497e945e" translate="yes" xml:space="preserve">
          <source>The same parameter &lt;code&gt;target&lt;/code&gt; is used to specify the target in multi-output regression settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4fc830eaa32e3715aa0bbc8cdaaf20cd8fc44d0" translate="yes" xml:space="preserve">
          <source>The same probability calibration procedure is available for all estimators via the &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;calibration#calibration&quot;&gt;Probability calibration&lt;/a&gt;). In the case of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt;, this procedure is builtin in &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; which is used under the hood, so it does not rely on scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="translated">표시되는 샘플 수에는 존재할 수있는 sample_weights가 가중됩니다.</target>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="translated">샘플 가중치는 C 매개 변수의 크기를 조정하므로 분류자가 이러한 점을 올바르게 얻는 데 더 중점을 둡니다. 효과는 종종 미묘 할 수 있습니다. 여기에서 그 효과를 강조하기 위해 우리는 특히 특이 치를 가중시켜 결정 경계의 변형을 매우 눈에 띄게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="translated">정수의 샘플링 된 부분 집합. 선택된 정수의 부분 집합이 무작위 화되지 않을 수 있습니다. 메소드 인수를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="translated">이 데이터 세트의 샘플은 미국에서 30x30m의 산림 패치에 해당하며 각 패치의 표지 유형, 즉 지배적 인 나무 종을 예측하는 작업을 위해 수집되었습니다. 7 가지의 커버 타입이 있는데, 이것이 멀티 클래스 분류 문제가됩니다. 각 샘플에는 &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;데이터 세트 홈페이지&lt;/a&gt; 에 설명 된 54 개의 기능 이 있습니다 . 일부 기능은 부울 표시기이고 다른 기능은 불연속 측정입니다.</target>
        </trans-unit>
        <trans-unit id="5425b30895b660513d41a7b82e02c9858a9f43c3" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5340a26524629a68efc3f526b09b13eeae9b4043" translate="yes" xml:space="preserve">
          <source>The samples that are used to train the calibrator should not be used to train the target classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="translated">샘플.</target>
        </trans-unit>
        <trans-unit id="7217248266c27250d74fa132fb5cef7e3435697a" translate="yes" xml:space="preserve">
          <source>The scalar parameter to validate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="translated">그런 다음 스케일러 인스턴스를 새로운 데이터에 사용하여 훈련 세트에서와 동일한 방식으로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="translated">scikit-learn 프로젝트는 참신함 또는 이상치 탐지에 모두 사용할 수있는 일련의 기계 학습 도구를 제공합니다. 이 전략은 데이터에서 감독되지 않은 방식으로 학습하는 객체로 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="translated">scikit-learn은 개체 &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt; 을 제공합니다. EllipticEnvelope 는 데이터에 대한 강력한 공분산 추정값을 맞추므로 중앙 모드 외부의 점을 무시하고 중앙 데이터 점에 타원을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="translated">기능을 선택해야하는 점수입니다.</target>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="translated">각 cv 스플릿에서 테스트 점수에 대한 점수 배열.</target>
        </trans-unit>
        <trans-unit id="a1547f496d2e14d7ede805b073598361b82ebf35" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;test_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;test_r2&lt;/code&gt; or &lt;code&gt;test_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e1ecd73a82717f9c044c3e124c2c92104db3c6b" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. Suffix &lt;code&gt;_score&lt;/code&gt; in &lt;code&gt;train_score&lt;/code&gt; changes to a specific metric like &lt;code&gt;train_r2&lt;/code&gt; or &lt;code&gt;train_auc&lt;/code&gt; if there are multiple scoring metrics in the scoring parameter. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">각 이력서 분할에서 열차 점수의 점수 배열입니다. &lt;code&gt;return_train_score&lt;/code&gt; 매개 변수가 &lt;code&gt;True&lt;/code&gt; 인 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="translated">점수는 잘못된 클러스터링의 경우 -1과 고밀도 클러스터링의 경우 +1 사이로 제한됩니다. 0 주위의 점수는 겹치는 클러스터를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="translated">스코어는 클러스터 내 분산액과 클러스터 간 분산액 사이의 비율로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="24504b3de92e08246a270b0dc2b57eafc2174bab" translate="yes" xml:space="preserve">
          <source>The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="translated">점수는 클러스터 내 거리와 클러스터 간 거리의 비율로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="translated">점수는 계산이 빠릅니다</target>
        </trans-unit>
        <trans-unit id="ef88d78d9d28409ef934e001658916bd55503917" translate="yes" xml:space="preserve">
          <source>The score is fast to compute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="translated">클러스터가 밀도가 높고 분리되어 있으면 클러스터의 표준 개념과 관련하여 점수가 높습니다.</target>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="translated">점수 범위는 0에서 1까지이며, &lt;code&gt;adjusted=True&lt;/code&gt; 참을 사용하면 임의의 성능으로 \ (\ frac {1} {1-\ text {n \ _classes}} \)에서 1까지 범위로 조정됩니다. 점수 0.</target>
        </trans-unit>
        <trans-unit id="f66ce641ead1aaeebf85e08eeb401e8169ff494e" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - n\_classes}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="translated">점수의 범위는 0에서 1입니다. 값이 높을수록 두 군집간에 유사성이 우수함을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">스코어러 호출 가능 오브젝트 / 함수에는 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 로 서명이 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="translated">득점자.</target>
        </trans-unit>
        <trans-unit id="97894cb494476d0c2ed229f8ae81e55aa78ff1c3" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. Empty if no early stopping or if &lt;code&gt;validation_fraction&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04400e5c44d656378bb2e0f77c0e1b05794d2b51" translate="yes" xml:space="preserve">
          <source>The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the &lt;code&gt;scoring&lt;/code&gt; parameter. If &lt;code&gt;scoring&lt;/code&gt; is not &amp;lsquo;loss&amp;rsquo;, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="translated">경로를 따라 각 지형지 물의 점수입니다.</target>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="translated">각 순열에 대해 얻은 점수입니다.</target>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="translated">HuberRegressor의 점수는 특이 치를 완전히 필터링하지는 않지만 효과를 줄이기 때문에 TheilSen 및 RANSAC와 직접 비교할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="translated">모든 득점자의 점수는 &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ( &lt;code&gt;'mean_test_precision'&lt;/code&gt; , &lt;code&gt;'rank_test_precision'&lt;/code&gt; 등으로 끝나는 키 의 &lt;code&gt;cv_results_&lt;/code&gt; dict에서 사용할 수 있습니다 .)</target>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="translated">최적의 벌칙 매개 변수 (알파)에 대한 검색은 반복적으로 정제 된 그리드에서 수행됩니다. 먼저 그리드의 교차 검증 된 점수가 계산 된 다음 새로운 정제 된 그리드가 최대 값을 중심으로하는 방식으로 진행됩니다.</target>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="translated">검색된 매개 변수.</target>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="translated">제품 커널의 두 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="translated">합 커널의 두 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="translated">두 번째 예는 데이터 분산의 주요 모드에 집중할 수있는 최소 공분산 결정 성 공분산의 강력한 추정량의 능력을 보여줍니다. 바나나 모양의 분포로 인해 공분산을 추정하기는 어렵지만 위치는 잘 추정되는 것 같습니다. 어쨌든, 우리는 외적인 관찰을 제거 할 수 있습니다. One-Class SVM은 실제 데이터 구조를 캡처 할 수 있지만 데이터 스 캐터 매트릭스의 모양과 데이터가 과적 합 될 위험 사이에서 적절한 절충을 얻기 위해 커널 대역폭 매개 변수를 조정하는 것이 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">두 번째 그림은 선형지지 벡터 분류기 (LinearSVC)의 교정 곡선을 보여줍니다. LinearSVC는 Gaussian naive Bayes와 반대되는 동작을 보여줍니다. 교정 곡선에는 시그 모이 드 곡선이 있으며 이는 과소 분류기에서 일반적입니다. LinearSVC의 경우 힌지 손실의 마진 속성으로 인해 결정 경계 (지원 벡터)에 가까운 하드 샘플에 모델이 초점을 맞출 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="translated">두 번째 그림은 첫 번째 그림에서 사용 된 두 개의 하이퍼 매개 변수 중에서 검은 점으로 강조 표시된 두 가지 하이퍼 하이퍼 매개 변수 선택에 대한 로그 한계 가능성을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="translated">두 번째 로더는 일반적으로 얼굴 확인 작업에 사용됩니다. 각 샘플은 동일한 사람에 속하는지 아닌 두 개의 그림 쌍입니다.</target>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="translated">두 번째 모델은 변형 추론에 적합하기 전에 Dirichlet 프로세스를 갖춘 베이지안 가우스 혼합 모델입니다. 이전 농도의 낮은 값은 모델이 적은 수의 활성 성분을 선호하게합니다. 이 모델은 비대 각 공분산 행렬에 의해 모델링 된 교대 방향을 갖는 점 그룹 인 데이터 세트 구조의 큰 그림에 모델링 능력을 집중시키기 위해 &quot;결정&quot;을 모델링합니다. 이러한 교번 방향은 원래 사인 신호의 교번 특성을 대략적으로 포착합니다.</target>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="translated">두 번째는 더 작은 소음 수준과 더 짧은 길이의 스케일을 가지며, 이는 무 잡음 기능 관계에 의한 대부분의 변동을 설명합니다. 두 번째 모델은 가능성이 높습니다. 그러나 하이퍼 파라미터의 초기 값에 따라 그래디언트 기반 최적화도 고 소음 솔루션으로 수렴 될 수 있습니다. 따라서 다른 초기화에 대해 최적화를 여러 번 반복하는 것이 중요합니다.</target>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="translated">두 번째 그림 은 &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 및 &lt;code&gt;n_init=1&lt;/code&gt; 을 사용하여 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 추정기 의 단일 실행을 보여줍니다 . 이 실행은 추정 센터가 지상 진실 클러스터 사이에 붙어있는 수렴 (로컬 최적)을 초래합니다.</target>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="translated">두 번째 플롯은 &lt;code&gt;C&lt;/code&gt; 및 &lt;code&gt;gamma&lt;/code&gt; 의 함수로서 분류기의 교차 검증 정확도에 대한 히트 맵입니다 . 이 예에서는 설명을 위해 비교적 큰 격자를 살펴 봅니다. 실제로 \ (10 ​​^ {-3} \)에서 \ (10 ​​^ 3 \)까지의 로그 그리드로 충분합니다. 최상의 매개 변수가 그리드 경계에있는 경우 후속 검색에서 해당 방향으로 확장 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">두 번째 플롯은 허용 가능한 왜곡 &lt;code&gt;eps&lt;/code&gt; 의 증가가 주어진 수의 샘플 &lt;code&gt;n_samples&lt;/code&gt; 에 대한 최소 크기 &lt;code&gt;n_components&lt;/code&gt; 를 크게 줄일 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="translated">두 번째 줄거리는 대략적인 커널 맵으로 RBF 커널 SVM과 선형 SVM의 결정 표면을 시각화했습니다. 이 도표는 데이터의 처음 두 주요 구성 요소에 투영 된 분류기의 결정 표면을 보여줍니다. 이 시각화는 결정 표면을 통해 64 차원의 흥미로운 조각이기 때문에 소금 한 알갱이로 가져와야합니다. 특히 데이터 점 (점으로 표시됨)은 처음 두 주요 구성 요소가 포함 된 평면에 놓이지 않기 때문에 데이터 영역이 놓여있는 영역으로 반드시 분류되지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="0b0b4cc48e44d58bc4674ef40664981912a3f333" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span. The usage of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="translated">두 번째 유스 케이스는 &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; 를&lt;/a&gt; 사용하여 간단한 파이썬 함수에서 완전히 사용자 정의 스코어러 오브젝트를 빌드하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">반복되는 값을 제거하기 위해 연속 변수에 작은 노이즈를 추가하기위한 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="translated">업데이트 할 임의의 기능을 선택하는 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . 사용시 &lt;code&gt;selection&lt;/code&gt; == '임의'</target>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="translated">업데이트 할 임의의 기능을 선택하는 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;selection&lt;/code&gt; == 'random'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="fceac838e81f76b0b51f388983ff2416b1824960" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">이중 좌표 하강 ( &lt;code&gt;dual=True&lt;/code&gt; 인 경우)에 대한 데이터를 섞을 때 사용할 의사 난수 생성기의 시드입니다 . 때 &lt;code&gt;dual=False&lt;/code&gt; 의 기본이되는 구현 &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 은&lt;/a&gt; 무작위로하지 않고 &lt;code&gt;random_state&lt;/code&gt; 는 결과에 영향을주지 않습니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="213c180fc69b83cb51964cde95f090b02ff40db1" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data, i.e. getting the random vectors to initialize the algorithm. Pass an int for reproducible results across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">데이터를 섞을 때 사용할 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="translated">데이터를 섞을 때 사용할 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'sag'또는 'liblinear'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="translated">데이터를 섞을 때 사용할 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'sag'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="3a35c2036466dbf3b68131c5150065896eadbbdd" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use. Randomizes selection of estimator features if n_nearest_features is not None, the &lt;code&gt;imputation_order&lt;/code&gt; if &lt;code&gt;random&lt;/code&gt;, and the sampling from posterior if &lt;code&gt;sample_posterior&lt;/code&gt; is True. Use an integer for determinism. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">확률 추정을 위해 데이터를 섞을 때 사용되는 의사 난수 생성기의 시드입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="translated">F 값 세트입니다.</target>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="translated">레이블 세트는 각 출력 변수마다 다를 수 있습니다. 예를 들어, 샘플은&amp;ldquo;pear&amp;rdquo;,&amp;ldquo;apple&amp;rdquo;과 같은 유한 종 세트에서 가능한 값을 취하는 출력 변수에 대해&amp;ldquo;pear&amp;rdquo;로 할당 될 수 있습니다. &amp;ldquo;green&amp;rdquo;,&amp;ldquo;red&amp;rdquo;,&amp;ldquo;blue&amp;rdquo;,&amp;ldquo;yellow&amp;rdquo;와 같은 유한 한 색상 세트로 가능한 값을 취하는 두 번째 출력 변수의 경우&amp;ldquo;blue&amp;rdquo;또는&amp;ldquo;green&amp;rdquo;&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;y[i]&lt;/code&gt; &lt;code&gt;classes_[j]&lt;/code&gt; 가 각 &lt;code&gt;yt[i, j] == 1&lt;/code&gt; 대한 classes_ [j] 로 구성 되도록 각 샘플에 대한 레이블 세트 .</target>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">&lt;code&gt;average != 'binary'&lt;/code&gt; 일 때 포함 할 레이블 세트 와 &lt;code&gt;average is None&lt;/code&gt; 경우 순서 입니다. 예를 들어 다수의 음수 클래스를 무시하는 멀티 클래스 평균을 계산하기 위해 데이터에 존재하는 레이블을 제외 할 수 있지만, 데이터에 존재하지 않는 레이블은 매크로 평균에서 0 개의 구성 요소가됩니다. 다중 레이블 대상의 경우 레이블은 열 인덱스입니다. 기본적으로 &lt;code&gt;y_true&lt;/code&gt; 및 &lt;code&gt;y_pred&lt;/code&gt; 의 모든 레이블 은 정렬 된 순서로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="translated">p- 값 세트입니다.</target>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="translated">순차적으로 테스트 할 회귀 집합입니다.</target>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="translated">X와 Y의 점 사이의 페어 단위 거리의 모양 (Nx, Ny) 배열입니다.</target>
        </trans-unit>
        <trans-unit id="313d9c9325eeb4a6e4291910f1f9fa2e1a31f92e" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;(n_classes-1, n_SV)&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_classes - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_classes - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="translated">형상 &lt;code&gt;dual_coef_&lt;/code&gt; 이 있다 &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; 다소 딱딱 배치를 파악한다. 열은 &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &quot;one-vs-one&quot;분류 자 중 하나에 포함 된 지원 벡터에 해당합니다 . 각 지원 벡터는 &lt;code&gt;n_class - 1&lt;/code&gt; 분류 자 에서 사용 됩니다. 각 행 의 &lt;code&gt;n_class - 1&lt;/code&gt; 항목은 이러한 분류기의 이중 계수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="translated">결과의 모양.</target>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="translated">시프트 오프셋은 이상치에 대한 제로 임계 값을 허용합니다. 참신 탐지에만 사용할 수 있습니다 (참신이 True로 설정된 경우). 인수 X는 &lt;em&gt;새로운 데이터&lt;/em&gt; 를 포함해야한다 . 만약 X가 훈련의 포인트를 포함한다면, 그것은 자신의 이웃에서 나중에 고려한다. 또한 X의 샘플은 어떤 점에서도 고려되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">각 입력 샘플의 Local Outlier Factor의 반대 편이되었습니다. 낮을수록 비정상적입니다. 음수 점수는 특이 치를 나타내고 양수 점수는 특이 치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="432ee6647b81dbc5a7cbb417e7251dacced20941" translate="yes" xml:space="preserve">
          <source>The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="translated">두 세트의 biclusters의 유사성.</target>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="translated">&lt;em&gt;절대 MDS&lt;/em&gt; 라고 하는 가장 간단한 메트릭 &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; 모델 은 \ (\ hat {d} _ {ij} = S_ {ij} \)로 정의됩니다. 절대 MDS를 사용하면 \ (S_ {ij} \) 값은 포함 지점의 \ (i \)와 \ (j \) 사이의 거리와 정확히 일치해야합니다.&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="translated">가능한 가장 간단한 분류기는 &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;가장 가까운 이웃입니다&lt;/a&gt; . 새로운 관측치 &lt;code&gt;X_test&lt;/code&gt; 가 주어지면 훈련 세트 (예 : 추정기를 훈련시키는 데 사용 된 데이터)에서 가장 가까운 특징 벡터를 가진 관측치를 찾습니다. ( 이러한 분류기 유형에 대한 자세한 내용은 온라인 Scikit-learn 설명서 의 &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;가장 가까운 이웃 섹션&lt;/a&gt; 을 참조하십시오 .)</target>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="translated">이러한 차원 축소를 달성하는 가장 간단한 방법은 데이터를 임의로 투영하는 것입니다. 이것은 데이터 구조의 어느 정도의 가시화를 허용하지만, 선택의 임의성은 많이 요구된다. 랜덤 프로젝션에서는 데이터 내에서 더 흥미로운 구조가 손실 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="translated">교차 유효성 검사를 사용하는 가장 간단한 방법 은 추정기와 데이터 집합 에서 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 도우미 함수 를 호출하는 것 입니다.</target>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="translated">특이 값 분해 \ (A_n = U \ Sigma V ^ \ top \)는 \ (A \)의 행과 열의 파티션을 제공합니다. 왼쪽 특이 벡터의 서브 세트는 행 파티션을 제공하고 오른쪽 특이 벡터의 서브 세트는 열 파티션을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="translated">선택한 각 구성 요소에 해당하는 특이 값입니다. 특이 값은 저 차원 공간에서 &lt;code&gt;n_components&lt;/code&gt; 변수 의 2- 노름과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="translated">크기 &lt;code&gt;grid_scores_&lt;/code&gt; 은 동일하다 &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt; 단계 각 반복에서 제거 된 기능의 수이다.</target>
        </trans-unit>
        <trans-unit id="2f08a32d3411460a5643fb826528c94534f8d8f2" translate="yes" xml:space="preserve">
          <source>The size of the image that will be reconstructed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">기본 매개 변수가있는 모델의 크기는 \ (O (M * N * log (N)) \)입니다. 여기서 \ (M \)은 트리 수이고 \ (N \)은 샘플 수입니다. 모델의 크기를 줄이기 위해 &lt;code&gt;min_samples_split&lt;/code&gt; , &lt;code&gt;max_leaf_nodes&lt;/code&gt; , &lt;code&gt;max_depth&lt;/code&gt; 및 &lt;code&gt;min_samples_leaf&lt;/code&gt; 매개 변수를 변경할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="translated">생성 할 랜덤 행렬의 크기입니다.</target>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="translated">회귀 트리 기본 학습자의 크기는 그라디언트 부스팅 모델로 캡처 할 수있는 변수 상호 작용의 수준을 정의합니다. 일반적으로, 깊이 트리 &lt;code&gt;h&lt;/code&gt; 는 차수 &lt;code&gt;h&lt;/code&gt; 의 상호 작용을 캡처 할 수 있습니다 . 개별 회귀 트리의 크기를 제어 할 수있는 두 가지 방법이 있습니다.</target>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="translated">임의의 데이터 하위 집합에서 실루엣 계수를 계산할 때 사용할 샘플의 크기입니다. 경우 &lt;code&gt;sample_size is None&lt;/code&gt; , 어떤 샘플링은 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="translated">샘플링 할 세트의 크기입니다.</target>
        </trans-unit>
        <trans-unit id="bec91be0bb08923b4b038e4efb20c2c584713078" translate="yes" xml:space="preserve">
          <source>The size of the trees can be controlled through the &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt;, and &lt;code&gt;min_samples_leaf&lt;/code&gt; parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="translated">클러스터의 크기, 거리 및 모양은 초기화, 난이도 값에 따라 다를 수 있으며 항상 의미를 전달하지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="translated">비뚤어진 카이 제곱 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="b37812e3809837f255e9952013df77f9cb7868cc" translate="yes" xml:space="preserve">
          <source>The smaller the Brier score, the better, hence the naming with &amp;ldquo;loss&amp;rdquo;. Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). The Brier loss is composed of refinement loss and calibration loss. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently &amp;ldquo;distant&amp;rdquo; from one another). Which label is considered to be the positive label is controlled via the parameter pos_label, which defaults to 1. Read more in the &lt;a href=&quot;../calibration#calibration&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="translated">평활화 우선 순위 \ (\ alpha \ ge 0 \)는 학습 샘플에없는 특징을 설명하고 추가 계산에서 0 확률을 방지합니다. \ (\ alpha = 1 \) 설정을 Laplace smoothing이라고하고 \ (\ alpha &amp;lt;1 \)를 Lidstone smoothing이라고합니다.</target>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="translated">솔버 &quot;liblinear&quot;는 CD (좌표 하강) 알고리즘을 사용 하며 scikit-learn과 함께 제공되는 우수한 C ++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR 라이브러리를 사용&lt;/a&gt; 합니다. 그러나 liblinear로 구현 된 CD 알고리즘은 진정한 다항식 (멀티 클래스) 모델을 학습 할 수 없습니다. 대신 최적화 문제는 &quot;일대일&quot;방식으로 분해되므로 모든 클래스에 대해 별도의 이진 분류 기가 훈련됩니다. 이러한 상황에서 발생 하므로이 솔버를 사용하는 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 인스턴스는 멀티 클래스 분류 자로 작동합니다. L1 불이익의 경우 &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; &lt;/a&gt; 는 &quot;null&quot;이 아닌 (모든 피처 가중치가 0 인) 모델을 얻기 위해 C의 하한을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="26f79d036c2795565ba6a21b798f1790f5f71dab" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For \(\ell_1\) regularization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="translated">무게 최적화를위한 솔버.</target>
        </trans-unit>
        <trans-unit id="7622dc087d4f210bd1e051afc82030dfdfb796a8" translate="yes" xml:space="preserve">
          <source>The solver is selected by a default policy based on &lt;code&gt;X.shape&lt;/code&gt; and &lt;code&gt;n_components&lt;/code&gt;: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient &amp;lsquo;randomized&amp;rsquo; method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 클래스에서 구현 된 솔버 는 &quot;liblinear&quot;, &quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;sag&quot;및 &quot;saga&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="translated">임시 거리 매트릭스 청크에 대한 최대 메모리를 찾았습니다. None (기본값) 인 경우 &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="translated">소스는 &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;Github&lt;/a&gt; 에서도 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="translated">이 튜토리얼의 소스는 scikit-learn 폴더에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="translated">그리드 점 사이의 간격 (도)</target>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="translated">행렬 분해의 희소 코드 계수.</target>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="translated">이 행렬의 각 열이 정확히 n_nonzero_coefs 0이 아닌 항목 (X)을 갖도록하는 희소 코드입니다.</target>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="translated">희소 코드</target>
        </trans-unit>
        <trans-unit id="98979bbd88c557cc69074b91f3080d6fb357dded" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See &lt;a href=&quot;#implementation-details&quot;&gt;Implementation details&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="translated">드문 구현은 인터셉트에 대한 학습 속도가 감소하여 밀도가 높은 구현과 약간 다른 결과를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="translated">희소 벡터</target>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="translated">희소성은 실제로 행렬의 cholesky factor에 부과됩니다. 따라서 알파는 행렬 자체의 채우기 부분으로 직접 변환되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="translated">희소성을 유발하는 \ (\ ell_1 \) 규범은 사용 가능한 교육 샘플이 거의 없을 때 학습 구성 요소에서 소음이 발생하는 것을 방지합니다. 벌칙의 정도 (따라서 희소성)는 하이퍼 파라미터 &lt;code&gt;alpha&lt;/code&gt; 를 통해 조정할 수 있습니다 . 작은 값은 부드럽게 정규화 된 인수 분해로 이어지고 큰 값은 많은 계수를 0으로 줄입니다.</target>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="translated">단일 샘플의 분할 코드는 길이 &lt;code&gt;2 * n_components&lt;/code&gt; 가지며 다음 규칙을 사용하여 구성됩니다. 먼저, 길이 &lt;code&gt;n_components&lt;/code&gt; 의 일반 코드 가 계산됩니다. 그리고, 제 &lt;code&gt;n_components&lt;/code&gt; 에서 의 엔트리 &lt;code&gt;split_code&lt;/code&gt; 가 정규 코드 벡터의 양의 부분으로 채워진다. 분할 코드의 후반은 양의 부호 만있는 코드 벡터의 음수 부분으로 채워집니다. 따라서 split_code는 음이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="c55afca5a7a433d8d42b73a1df4af26f8dc38160" translate="yes" xml:space="preserve">
          <source>The stacked regressor will combine the strengths of the different regressors. However, we also see that training the stacked regressor is much more computationally expensive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="translated">표준 LLE 알고리즘은 세 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="translated">군집의 표준 편차입니다.</target>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="translated">출력에 적용된 가우스 잡음의 표준 편차입니다.</target>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="translated">가우스 노이즈의 표준 편차입니다.</target>
        </trans-unit>
        <trans-unit id="0d973eb25a07dcdadcaeac90be3869c3ac64dae0" translate="yes" xml:space="preserve">
          <source>The standard score of a sample &lt;code&gt;x&lt;/code&gt; is calculated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">입력 도메인 &lt;code&gt;X&lt;/code&gt; 를 포함하는 단계적 보간 함수입니다 .</target>
        </trans-unit>
        <trans-unit id="a2cf8fe1dea678ae31186ca36392deb4d997cfe8" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="translated">정지 기준. None이 아닌 경우 반복은 중지됩니다 (loss&amp;gt; previous_loss-tol). 기본값은 없음입니다. 기본값은 0.21에서 1e-3입니다.</target>
        </trans-unit>
        <trans-unit id="b0435766401c2c671f3d0dbc7668ffa0a2dc3aba" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, training will stop when (loss &amp;gt; best_loss - tol) for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="translated">임베드 공간에 레이블을 지정하는 데 사용되는 전략입니다. 라플라시안 삽입 후 레이블을 지정하는 방법에는 두 가지가 있습니다. k- 평균을 적용 할 수 있으며 인기있는 선택입니다. 그러나 초기화에 민감 할 수도 있습니다. 이산화는 무작위 초기화에 덜 민감한 또 다른 접근법입니다.</target>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="translated">임베드 공간에 레이블을 지정하는 데 사용되는 전략입니다. 라플라시안 삽입 후 레이블을 지정하는 방법에는 두 가지가 있습니다. k- 평균을 적용 할 수 있으며 인기있는 선택입니다. 그러나 초기화에 민감 할 수도 있습니다. 이산화는 무작위 초기화에 덜 민감한 또 다른 접근법입니다. 이산화 접근법에 대한 자세한 내용은 아래 참조 된 '멀티 클래스 스펙트럼 클러스터링'백서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="translated">각 노드에서 분할을 선택하는 데 사용 된 전략입니다. 지원되는 전략은 최상의 분할을 선택하는 데 &quot;최고&quot;, 최상의 무작위 분할을 선택하는 &quot;무작위&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="translated">F 점수에서 리콜의 강도 대 정밀도.</target>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="translated">LOF 알고리즘의 장점은 데이터 집합의 로컬 및 전역 속성을 모두 고려한다는 것입니다. 비정상 샘플의 기본 밀도가 다른 데이터 집합에서도 성능이 우수합니다. 문제는 샘플이 얼마나 고립되어 있는지가 아니라 주변 환경과 얼마나 고립되어 있는지입니다.</target>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="translated">해독 할 문자열</target>
        </trans-unit>
        <trans-unit id="3b937bbc7005ed347df9fefe128290b88cb139ff" translate="yes" xml:space="preserve">
          <source>The string to decode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="translated">문자열 값 &quot;auto&quot;는 Spearman 상관 추정의 부호에 따라 y를 증가 또는 감소시켜야하는지 여부를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="translated">bicluster i에 해당하는 하위 행렬.</target>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="translated">각 기본 추정기에 대해 그려진 기능의 하위 집합입니다.</target>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="translated">각 기본 추정량에 대해 추출 된 표본의 부분 집합입니다.</target>
        </trans-unit>
        <trans-unit id="fce75057d74e03086c8b482c64f0a007dbeb6ebe" translate="yes" xml:space="preserve">
          <source>The sum of all predictions also confirms the calibration issue of the &lt;code&gt;Ridge&lt;/code&gt; model: it under-estimates by more than 3% the total number of claims in the test set while the other three models can approximately recover the total number of claims of the test portfolio.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="translated">피쳐의 합 (문서의 경우 단어 수)이이 예상 값을 가진 포아송 분포에서 도출됩니다.</target>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">지원은 &lt;code&gt;y_true&lt;/code&gt; 에서 각 클래스의 발생 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="translated">의 서포트 벡터 머신 밀도 (모두 지원 scikit는 배우기 &lt;code&gt;numpy.ndarray&lt;/code&gt; 및 의한으로 변환 &lt;code&gt;numpy.asarray&lt;/code&gt; ) 및 스파 스 (모든 &lt;code&gt;scipy.sparse&lt;/code&gt; 입력으로) 샘플 벡터. 그러나 SVM을 사용하여 희소 데이터를 예측하려면 해당 데이터에 적합해야합니다. 최적의 성능을 얻으려면 &lt;code&gt;dtype=float64&lt;/code&gt; 와 함께 C 순서 &lt;code&gt;numpy.ndarray&lt;/code&gt; (밀도) 또는 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse)를 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="translated">부분 의존성을 계산해야하는 대상 기능 (시각적 렌더링의 경우 크기가 3보다 작아야 함).</target>
        </trans-unit>
        <trans-unit id="1676d735c85e09d0307a407a6c8fc0482ea454c1" translate="yes" xml:space="preserve">
          <source>The target features for which to create the PDPs. If features[i] is an int or a string, a one-way PDP is created; if features[i] is a tuple, a two-way PDP is created. Each tuple must be of size 2. if any entry is a string, then it must be in &lt;code&gt;feature_names&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="translated">목표는 훈련 세트에서 가장 가까운 이웃과 관련된 목표의 로컬 보간에 의해 예측된다.</target>
        </trans-unit>
        <trans-unit id="fd8f31b4b903aeb83268043a3b52655fbcfeb540" translate="yes" xml:space="preserve">
          <source>The target labels (integer index).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9040adccd40d7354a58b523dff53abedde8f2d61" translate="yes" xml:space="preserve">
          <source>The target labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="translated">목표 값 (분류의 클래스 레이블, 회귀의 실수).</target>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="translated">대상 값 (클래스 레이블)을 정수 또는 문자열로 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="translated">대상 값 (클래스 레이블)</target>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="translated">목표 값 (분류의 클래스, 회귀의 실수)에 해당하는 정수입니다.</target>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="translated">목표 값 (실수).</target>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">목표 값 (실수). 효율성을 극대화 하려면 &lt;code&gt;dtype=np.float64&lt;/code&gt; 및 &lt;code&gt;order='C'&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="translated">목표 값.</target>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="translated">지도 학습 문제에 대한 목표 변수.</target>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="translated">지도 학습 문제에 대한 목표 변수. 층화는 y 라벨을 기준으로 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="translated">목표 변수는 캘리포니아 지역의 중간 주택 가치입니다.</target>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="translated">지도 학습의 경우 예측하려고하는 대상 변수입니다.</target>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="translated">당면 과제는 생리적 변수로부터 질병 진행을 예측하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="translated">10 가지 기능은 표준 독립 가우시안이며 대상 &lt;code&gt;y&lt;/code&gt; 는 다음 과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="translated">&amp;ldquo;이산 된 특징&amp;rdquo;이라는 용어는 본질을보다 정확하게 설명하기 때문에&amp;ldquo;범주 적&amp;rdquo;이라고 명명하는 대신 사용됩니다. 예를 들어, 이미지의 픽셀 강도는 별개의 기능이지만 범주는 거의 없지만 이미지를 표시하면 더 나은 결과를 얻을 수 있습니다. 또한 연속 변수를 불연속으로 처리하거나 그 반대로 처리하면 일반적으로 잘못된 결과가 발생하므로주의를 기울여야합니다.</target>
        </trans-unit>
        <trans-unit id="2014676021228002e8224c1f06ee12bb4596d24b" translate="yes" xml:space="preserve">
          <source>The term \((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\) corresponds to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mahalanobis_distance&quot;&gt;Mahalanobis Distance&lt;/a&gt; between the sample \(x\) and the mean \(\mu_k\). The Mahalanobis distance tells how close \(x\) is from \(\mu_k\), while also accounting for the variance of each feature. We can thus interpret LDA as assigning \(x\) to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="translated">데이터의 테스트 포인트 훈련 데이터와 동일한 형식입니다.</target>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="translated">테스트는 해당 분할에 대한 인덱스를 설정합니다.</target>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="translated">scikit-learn의 텍스트 기능 추출기는 텍스트 파일을 디코딩하는 방법을 알고 있지만 파일의 인코딩을 알려주는 경우에만 해당됩니다. &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 는이를 위해 &lt;code&gt;encoding&lt;/code&gt; 매개 변수를 사용합니다. 최신 텍스트 파일의 경우 올바른 인코딩은 아마도 UTF-8 일 것이므로 기본값 ( &lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt; )입니다.</target>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="translated">이론은 예측 일관성을 달성하기 위해 페널티 파라미터는 샘플 수가 증가함에 따라 일정하게 유지되어야한다고 말합니다.</target>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="translated">세 번째 그림은 1 차원 100 샘플 분포에 대한 커널 밀도 추정치를 비교합니다. 이 예제는 1D 분포를 사용하지만 커널 밀도 추정은 더 높은 차원으로 쉽고 효율적으로 확장 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="translated">세 번째 모델은 이전에 Dirichlet 공정을 사용한 베이지안 가우시안 혼합 모델이지만 이번에는 이전 농도 값이 높아져 모델의 데이터를 세밀하게 모델링 할 수있는 자유가 더 커졌습니다. 결과적으로 구성 요소 수를 임의로 10으로 고정하기로 결정한 첫 번째 모델과 유사한 활성 구성 요소가 더 많은 혼합물이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="translated">기능 선택에 사용할 임계 값입니다. 중요도가 크거나 같은 기능은 유지되고 다른 기능은 삭제됩니다. &quot;중간 값&quot;(예 : &quot;평균&quot;)이면 &lt;code&gt;threshold&lt;/code&gt; 값은 기능 중요도의 중간 값 (평균)입니다. 스케일링 계수 (예를 들어, &quot;1.25 * 평균&quot;)가 또한 사용될 수있다. None이고 추정기가 매개 변수 패널티를 명시 적으로 또는 암시 적으로 l1로 설정 한 경우 (예 : Lasso) 사용 된 임계 값은 1e-5입니다. 그렇지 않으면 기본적으로 &quot;mean&quot;이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="translated">기능 선택에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="translated">이 구현의 시간 복잡도는 d ~ n_features ~ n_components를 가정하면 &lt;code&gt;O(d ** 2)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="translated">각 이력서 분할에 대해 설정된 열차에 견적자를 맞추는 시간.</target>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">각 cv 스플릿에 대한 테스트 세트에서 추정기를 스코어링하는 시간입니다. ( &lt;code&gt;return_train_score&lt;/code&gt; 가 &lt;code&gt;True&lt;/code&gt; 로 설정 되어 있어도 열차 세트의 득점 시간은 포함되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="translated">하강 방향을 계산하는 데 사용되는 탄성 그물 솔버의 공차입니다. 이 매개 변수는 전체 매개 변수 추정값이 아니라 주어진 열 업데이트에 대한 검색 방향의 정확도를 제어합니다. mode = 'cd'에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="1c748ca3956e12feeccc0881efb2e7b986b52b0e" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="translated">최적화에 대한 허용 오차 : 업데이트가 &lt;code&gt;tol&lt;/code&gt; 보다 작은 경우 , 최적화 코드는 이중 간격이 최적인지 확인하고 &lt;code&gt;tol&lt;/code&gt; 보다 작을 때까지 계속 됩니다.</target>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="translated">수렴 선언 공차 : 이중 간격이이 값 아래로 떨어지면 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="c8679f3dc5795753fd4b3924c7477d6451516504" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="translated">단층 촬영법은 선형 변환입니다. 선형 회귀에 해당하는 데이터 충실도 용어 외에도 이미지의 L1 규범에 벌칙을 적용하여 희소성을 설명합니다. 결과 최적화 문제를 &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; 라고합니다 . 좌표 하강 알고리즘을 사용하는 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt; &lt;/a&gt; 클래스 를 사용합니다. 중요하게도,이 구현은 여기서 사용 된 프로젝션 연산자보다 희소 행렬에서 더 계산적으로 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="translated">총 기능 수입니다.</target>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="translated">총 기능 수입니다. 이들은 &lt;code&gt;n_informative&lt;/code&gt; 정보 정보 기능, &lt;code&gt;n_redundant&lt;/code&gt; 중복 중복 기능, &lt;code&gt;n_repeated&lt;/code&gt; 반복 중복 기능 및 &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; 기능 -n_ 정보 -n_ 중복 -n_ 반복 쓸모없는 기능으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="translated">총 입력 기능 수입니다.</target>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="translated">클래스간에 똑같이 나누어 진 총 포인트 수입니다.</target>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="translated">생성 된 총 포인트 수입니다.</target>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="translated">생성 된 총 포인트 수입니다. 홀수 인 경우 내부 원은 외부 원보다 한 점 더 큽니다.</target>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="translated">다항식 출력 기능의 총 수입니다. 출력 기능의 수는 모든 적절한 크기의 입력 기능 조합을 반복하여 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="translated">주요 고유 벡터를 계산하는 전통적인 방법은 전력 반복 방법을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; 에서 구현 된 훈련 알고리즘 은 확률 적 최대 가능성 (SML) 또는 영구 대비 발산 (PCD)으로 알려져 있습니다. 데이터 가능성의 형태로 인해 최대 가능성을 직접 최적화하는 것은 불가능합니다.</target>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="translated">훈련 데이터</target>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="translated">훈련 데이터에는 다른 것과는 다른 관측치로 정의 된 특이 치가 포함됩니다. 이상치 검출 추정값은 이탈 한 관측 값을 무시하고 훈련 데이터가 가장 집중된 영역에 적합하려고합니다.</target>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="translated">교육 데이터는 특이 치에 의해 오염되지 않으며 &lt;strong&gt;새로운&lt;/strong&gt; 관측치가 특이 치 인지 여부를 감지하는 데 관심 이 있습니다. 이러한 맥락에서 특이점은 참신이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="translated">교육 데이터 (예 : 불변 스냅 샷에 대한 참조)</target>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="translated">훈련 입력 샘플.</target>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">훈련 입력 샘플. 내부적으로 는 희소 행렬이 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 에 제공되는 경우 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="translated">훈련 입력 샘플. 내부적으로 dtype은 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 . 희소 행렬이 제공되면 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="translated">훈련 입력 샘플. 희소 행렬은 기본 추정기가 지원하는 경우에만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="cc9cba55947c2b2da5cd927ed2d1f239e6fce1e8" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="translated">훈련 입력 샘플. 희소 행렬은 CSC, CSR, COO, DOK 또는 LIL 일 수 있습니다. DOK와 LIL은 CSR로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="translated">데이터의 훈련 포인트. 각 포인트에는 세 개의 필드가 있습니다.</target>
        </trans-unit>
        <trans-unit id="768a232e21b799d7ab4a21a202aa20c0c2da04e0" translate="yes" xml:space="preserve">
          <source>The training samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">훈련 세트의 크기는 &lt;code&gt;i * n_samples // (n_splits + 1) + n_samples % (n_splits + 1)&lt;/code&gt; 이며 &lt;code&gt;i``th split, with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt; . 여기서 &lt;code&gt;n_samples&lt;/code&gt; 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="translated">트레이닝은 해당 스플릿에 대한 인덱스를 설정합니다.</target>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="translated">&lt;code&gt;transformer&lt;/code&gt; 또는 &lt;code&gt;func&lt;/code&gt; 및 &lt;code&gt;inverse_func&lt;/code&gt; 함수 쌍을 설정하여 변환을 트리거 할 수 있습니다 . 그러나 두 옵션을 모두 설정하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="cb596196fe310821d43e26f57899432a8af2c024" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="translated">변환은 각 기능에 독립적으로 적용됩니다. 피쳐의 누적 밀도 함수는 원래 값을 투사하는 데 사용됩니다. 적합 범위보다 낮거나 높은 새로운 / 보이지 않는 데이터의 피처 값은 출력 분포의 경계에 매핑됩니다. 이 변환은 비선형입니다. 동일한 척도로 측정 된 변수 간의 선형 상관 관계를 왜곡 할 수 있지만 다른 척도로 측정 된 변수를보다 직접적으로 비교할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="019c47d3f3ad8207e8989b103cc42bdf22c099af" translate="yes" xml:space="preserve">
          <source>The transformation is calculated as (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac658687b2a734f60273ed8ccd4ce7f47f7a43fa" translate="yes" xml:space="preserve">
          <source>The transformation is given by (when &lt;code&gt;axis=0&lt;/code&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="translated">변환은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="translated">변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="f7cd2d51dfcb247e9b3665d7843f0082b5c854bd" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by kneighbors_graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e2ab39a27e391675e94304e8d0476203d9b90e4" translate="yes" xml:space="preserve">
          <source>The transformed data is a sparse graph as returned by radius_neighbors_graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="translated">변환 된 데이터는 순진한 베이 즈 분류기를 훈련시키는 데 사용되며, 예측 정확도의 명확한 차이가 관찰되는데, 여기서 PCA 전에 스케일링 된 데이터 세트는 스케일링되지 않은 버전보다 훨씬 우수하다.</target>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="translated">변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="translated">사용할 트리 알고리즘. 유효한 옵션은 [ 'kd_tree'| 'ball_tree'| 'auto']입니다. 기본값은 '자동'입니다.</target>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="translated">트리 데이터 구조는 각 노드가 여러 하위 클러스터로 구성된 노드로 구성됩니다. 노드의 최대 하위 클러스터 수는 분기 계수에 의해 결정됩니다. 각 하위 클러스터는 선형 합계, 제곱합 및 해당 하위 클러스터의 샘플 수를 유지합니다. 또한 서브 클러스터가 리프 노드의 구성원이 아닌 경우 각 서브 클러스터는 하위 노드를 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e124a1b9cc4902e35946b8c1931bf2b92e9cefc7" translate="yes" xml:space="preserve">
          <source>The tree-based model is significantly better at ranking policyholders by risk while the two linear models perform similarly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="translated">각 빈의 실제 확률 (양의 분수).</target>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="translated">목표를 바꾸지 않고 진정한 점수.</target>
        </trans-unit>
        <trans-unit id="78ab726a9fdf47809b691c3859f0df06c5754377" translate="yes" xml:space="preserve">
          <source>The trustworthiness is within [0, 1]. It is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="translated">튜토리얼 폴더에는 다음과 같은 하위 폴더가 포함되어야합니다.</target>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="translated">아래의 두 그림 은 생성 된 데이터 집합의 여러 가지 분수 에 대해 &lt;code&gt;x-axis&lt;/code&gt; &lt;code&gt;C&lt;/code&gt; 값 과 &lt;code&gt;y-axis&lt;/code&gt; 해당 교차 검증 점수를 나타 냅니다.</target>
        </trans-unit>
        <trans-unit id="726ed01e2c2e9b244f4346dd4c7877757cc3867c" translate="yes" xml:space="preserve">
          <source>The two linear regressors &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt; now support sample weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85a91cdcd45557b2849d30b64bc99a3236c9a910" translate="yes" xml:space="preserve">
          <source>The two plots differ only in the area in the middle where the classes are tied. If &lt;code&gt;break_ties=False&lt;/code&gt;, all input in that area would be classified as one class, whereas if &lt;code&gt;break_ties=True&lt;/code&gt;, the tie-breaking mechanism will create a non-convex decision boundary in that area.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04a77900ed6f82fb8a154b287d08593a39a5648c" translate="yes" xml:space="preserve">
          <source>The two sample image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="translated">두 종은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="translated">사용할 기준의 유형입니다.</target>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="translated">기능 값의 유형입니다. Numpy array / scipy.sparse 행렬 생성자에 dtype 인수로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="translated">기능 값의 유형입니다. scipy.sparse 행렬 생성자에 dtype 인수로 전달됩니다. 이것을 bool, np.boolean 또는 부호없는 정수 유형으로 설정하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="translated">오류를 계산하는 데 사용되는 표준 유형입니다. 사용 가능한 오류 유형 :- 'frobenius'(기본값) : sqrt (tr (A ^ tA))- 'spectral': sqrt (max (eigenvalues ​​(A ^ tA)) 여기서 A는 오류입니다 &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8b60d3555697082fbd56cd614358874d006ccd25" translate="yes" xml:space="preserve">
          <source>The type of the hyperparameter. Currently, only &amp;ldquo;numeric&amp;rdquo; hyperparameters are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="translated">변경되지 않은 사전 원자</target>
        </trans-unit>
        <trans-unit id="17cd05cd0020cc8838dc1f102077fa8c28f81758" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="translated">기본 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 구현은 난수 생성기를 사용하여 모델을 이중 좌표 하강으로 피팅 할 때 (즉, &lt;code&gt;dual&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우) 피처를 선택합니다 . 따라서 동일한 입력 데이터에 대해 약간 다른 결과를 갖는 것은 드문 일이 아닙니다. 이 경우 더 작은 tol 매개 변수로 시도하십시오. 이 임의성은 &lt;code&gt;random_state&lt;/code&gt; 매개 변수 로 제어 할 수도 있습니다 . 때 &lt;code&gt;dual&lt;/code&gt; 설정되어 &lt;code&gt;False&lt;/code&gt; 의 기본이되는 구현 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 은&lt;/a&gt; 무작위로하지 않고 &lt;code&gt;random_state&lt;/code&gt; 는 결과에 영향을주지 않습니다.</target>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="translated">기본 C 구현은 난수 생성기를 사용하여 모델을 피팅 할 때 피처를 선택합니다. 따라서 동일한 입력 데이터에 대해 약간 다른 결과를 갖는 것은 드문 일이 아닙니다. 이 경우 더 작은 &lt;code&gt;tol&lt;/code&gt; 매개 변수로 시도하십시오 .</target>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="translated">기본 C 구현은 난수 생성기를 사용하여 모델을 피팅 할 때 피처를 선택합니다. 따라서 동일한 입력 데이터에 대해 약간 다른 결과를 갖는 것은 드문 일이 아닙니다. 이 경우 더 작은 tol 매개 변수로 시도하십시오.</target>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="translated">기본이되는 Tree 객체 Tree 객체의 속성 및 이러한 속성의 기본 사용법에 대한 &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;의사 결정 트리 구조 이해는 &lt;/a&gt; &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="translated">기본 구현은 조회 테이블, 블룸 필터, 카운트 최소 스케치 또는 기능 해싱을 구현하는 데 적합한 낮은 대기 시간 32 비트 해시를 생성하는 MurmurHash3_x86_32입니다.</target>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="translated">기본 구현 인 liblinear는 메모리 복사본이 발생할 데이터에 대해 희소 한 내부 표현을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="7c5718e72f8aabec82565a4976b3e3b79f8616d0" translate="yes" xml:space="preserve">
          <source>The unique classes labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="translated">매니 폴드에있는 점의 주요 치수에 따른 샘플의 일 변량 위치.</target>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="translated">언 믹싱 매트릭스.</target>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="translated">비 감독 데이터 축소 및 감산 추정기는 한 단계로 연결될 수 있습니다. &lt;a href=&quot;compose#pipeline&quot;&gt;파이프 라인 : 체인 추정기를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="translated">왼쪽 위 그림은 장난감 1d 회귀 문제의 임의 데이터 세트 LS (파란색 점)에 대해 훈련 된 단일 의사 결정 트리의 예측 (진한 빨간색)을 보여줍니다. 또한 문제의 다른 (그리고 다른) 무작위로 그려진 인스턴스 LS에 대해 훈련 된 다른 단일 의사 결정 트리의 예측 (연한 빨간색)을 보여줍니다. 직관적으로, 여기서 분산 항은 개별 추정기의 예측 빔의 폭 (빨간색)에 해당합니다. 분산이 클수록 &lt;code&gt;x&lt;/code&gt; 에 대한 예측이 더 민감 해집니다.훈련 세트의 작은 변화에. 편향 항은 추정기의 평균 예측 (청록색)과 최상의 모델 (진한 파란색)의 차이에 해당합니다. 이 문제에서, 우리는 바이어스가 상당히 낮고 (청록색과 파란색 곡선이 서로 가까이 있음), 분산이 클 때 (빨간색 빔이 다소 넓음) 관찰 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="30828e1dddfa129352e0424777c8d521d86c8855" translate="yes" xml:space="preserve">
          <source>The usage and the parameters of &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are described below. The 2 most important parameters of these estimators are &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;learning_rate&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 의 사용법은 &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt; 에 자세히 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="translated">중심 거리의 사용은 거리 메트릭을 유클리드 공간으로 제한합니다.</target>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="translated">의 사용량 &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt; &lt;code&gt;SkewedChi2Sampler&lt;/code&gt; 는&lt;/a&gt; 대해 상술 사용량과 동일 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; . 유일한 차이는 free 매개 변수에 있습니다. 즉 \ (c \)입니다. 이 매핑에 대한 동기와 수학적 세부 사항은 &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e4fafa67af5e9d647520b3a289f2e58cae35673f" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">회귀 분석에 가장 가까운 다중 출력 이웃을 &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;사용하는 방법은 다중 출력 추정기&lt;/a&gt; 로 얼굴 완성에 설명되어 있습니다. 이 예에서, 입력 X는 얼굴의 상반신의 픽셀이고 출력 Y는 그 얼굴의 하반신의 픽셀입니다.</target>
        </trans-unit>
        <trans-unit id="810478c2cd15f9d623ca524c5ee19e348c649a7e" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/miscellaneous/plot_multioutput_face_completion#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="translated">분류를 위해 다중 출력 트리를 사용하는 방법은 다중 &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;출력 추정기&lt;/a&gt; 로 얼굴 완성에 설명되어 있습니다. 이 예에서, 입력 X는 얼굴의 상반신의 픽셀이고 출력 Y는 그 얼굴의 하반신의 픽셀입니다.</target>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="translated">회귀에 다중 출력 트리를 사용하는 방법은 다중 &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;출력 결정 트리 회귀에서 설명&lt;/a&gt; 합니다. 이 예에서 입력 X는 단일 실수 값이고 출력 Y는 X의 사인 및 코사인입니다.</target>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="translated">사용 된 카테고리는 &lt;code&gt;categories_&lt;/code&gt; 속성 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">사용자가 제공 한 초기 수단의 기본값은 None이며, None 인 경우 &lt;code&gt;init_params&lt;/code&gt; 메소드를 사용하여 초기화됩니다 .</target>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="translated">사용자가 제공 한 초기 정밀도 (공분산 행렬의 역수)는 기본값이 없음입니다. 그렇지 않은 경우 정밀도는 'init_params'메소드를 사용하여 초기화됩니다. 모양은 'covariance_type'에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="translated">사용자가 제공 한 초기 가중치는 기본값이 없음입니다. None이면 가중치는 &lt;code&gt;init_params&lt;/code&gt; 메소드를 사용하여 초기화됩니다 .</target>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="translated">일반적인 공분산 최대 우도 추정치는 수축을 사용하여 정규화 할 수 있습니다. Ledoit와 Wolf는 무증상 최적 수축 매개 변수 (MSE 기준을 최소화 함)를 계산하여 Ledoit-Wolf 공분산 추정치를 산출하는 공식을 제안했습니다.</target>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="translated">일반적인 공분산 최대 우도 추정치는 데이터 세트에 특이 치가 존재하는 데 매우 민감합니다. 이 경우 추정값이 데이터 세트의 &quot;오차적인&quot;관측치에 내성이 있음을 보장하기 위해 강력한 공분산 추정량을 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="b7108164734a4610e8bf46680745d5c565a5fe9c" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set. &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;2&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="translated">유틸리티 함수 &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt; &lt;code&gt;make_pipeline&lt;/code&gt; &lt;/a&gt; 은 파이프 라인 구성을위한 축약 형입니다. 가변 개수의 추정기를 사용하고 파이프 라인을 반환하여 이름을 자동으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="translated">유효한 거리 측정 항목과 이들이 매핑하는 기능은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="translated">값 2는 가장 높은 점수를 갖습니다. 가중치는 1.5와 2로 두 번 나타납니다. 이들의 합은 3입니다.</target>
        </trans-unit>
        <trans-unit id="1614b1f299556b57f6975870b8797f657adfc906" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.5.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="translated">값 4는 세 번 나타납니다. 균일 한 가중치로 결과는 단순히 분포 모드입니다.</target>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="translated">값하는 &lt;code&gt;|y - X'w - c|&lt;/code&gt; 축소됩니다.</target>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="translated">선택한 파티션과 연관된 관성 기준 값 (comput_labels가 True로 설정된 경우). 관성은 가장 가까운 이웃까지의 샘플의 제곱 거리의 합으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="translated">모든 알파에서 정보 기준 ( 'aic', 'bic')의 값입니다. 정보 기준이 가장 작은 알파가 선택됩니다. 이 값은 &lt;code&gt;n_samples&lt;/code&gt; 비교하여 n_samples 만큼 큽니다 . (Zou et al, 2007)에서 2.15 및 2.16.</target>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="translated">가장 큰 계수의 값입니다.</target>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="translated">가장 작은 계수의 값입니다.</target>
        </trans-unit>
        <trans-unit id="77ad2ae374f9b7e58f24db15820679db8e02f6eb" translate="yes" xml:space="preserve">
          <source>The values at which the partial dependence should be evaluated are directly generated from &lt;code&gt;X&lt;/code&gt;. For 2-way partial dependence, a 2D-grid of values is generated. The &lt;code&gt;values&lt;/code&gt; field returned by &lt;a href=&quot;generated/sklearn.inspection.partial_dependence#sklearn.inspection.partial_dependence&quot;&gt;&lt;code&gt;sklearn.inspection.partial_dependence&lt;/code&gt;&lt;/a&gt; gives the actual values used in the grid for each target feature. They also correspond to the axis of the plots.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="translated">참조 Quantile에 해당하는 값입니다.</target>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="translated">ValueError 예외로 나열된 값은 다음 섹션에서 설명하는 예측 정확도를 측정하는 함수에 해당합니다. 해당 함수의 스코어러 오브젝트는 사전 &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt; 에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="translated">평가할 매개 변수의 값입니다.</target>
        </trans-unit>
        <trans-unit id="380d80dd5b496982ade02999d670873884707a35" translate="yes" xml:space="preserve">
          <source>The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, in which case it will be an array of zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="translated">각 샘플 군집에 할당 할 값</target>
        </trans-unit>
        <trans-unit id="15b198c6985fde1d17e7089abe8db26ba2c88b68" translate="yes" xml:space="preserve">
          <source>The values with which the grid has been created. The generated grid is a cartesian product of the arrays in &lt;code&gt;values&lt;/code&gt;. &lt;code&gt;len(values) ==
len(features)&lt;/code&gt;. The size of each array &lt;code&gt;values[j]&lt;/code&gt; is either &lt;code&gt;grid_resolution&lt;/code&gt;, or the number of unique values in &lt;code&gt;X[:, j]&lt;/code&gt;, whichever is smaller.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="translated">교육 세트의 각 기능에 대한 차이입니다. &lt;code&gt;scale_&lt;/code&gt; 을 계산하는 데 사용됩니다 . 같음 &lt;code&gt;None&lt;/code&gt; 때 &lt;code&gt;with_std=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="translated">각 구성 요소에 대한 투영에 의해 변형 된 학습 샘플의 분산입니다.</target>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">경로를 따라 다양한 계수 값. &lt;code&gt;fit_path&lt;/code&gt; 매개 변수가 &lt;code&gt;False&lt;/code&gt; 인 경우 존재하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="translated">\ (h_i \) 벡터는 관찰되지 않기 때문에 &quot;잠재적&quot;이라고합니다. \ (\ epsilon \)은 평균 0과 공분산 \ (\ Psi \) (예 : \ (\ epsilon \ sim \ mathcal {N} (0, \ Psi) \))을 갖는 가우시안에 따라 분포 된 노이즈 항으로 간주됩니다. \ (\ mu \)는 임의의 오프셋 벡터입니다. 이러한 모델은 \ (h_i \)에서 \ (x_i \)가 생성되는 방법을 설명하므로 &quot;생성&quot;이라고합니다. 모든 \ (x_i \)를 열로 사용하여 행렬 \ (\ mathbf {X} \)를 형성하고 모든 \ (h_i \)를 행렬의 열로 사용하면 \ (\ mathbf {H} \ ) 다음으로 작성할 수 있습니다 (적절하게 정의 된 \ (\ mathbf {M} \) 및 \ (\ mathbf {E} \)).</target>
        </trans-unit>
        <trans-unit id="928beebc49e69bf6a914c81639fd968b851e773a" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;rsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;rsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="translated">MiniBatchKMeans의 상세 설정을 통해 부분 맞춤에 대한 연속 호출 중에 일부 클러스터가 재 할당되는 것을 볼 수 있습니다. 패치가 나타내는 패치 수가 너무 낮아서 임의의 새 클러스터를 선택하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="translated">상세 수준</target>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="translated">자세한 수준.</target>
        </trans-unit>
        <trans-unit id="d073e4bf5a007c3a6f9ea54d0f642d54d81c6316" translate="yes" xml:space="preserve">
          <source>The verbosity level. If not zero, print some information about the fitting process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="translated">자세한 수준. 기본값 인 0은 자동 모드를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="translated">상세 레벨 : 0이 아닌 경우 진행 메시지가 인쇄됩니다. 50 이상에서는 출력이 표준 출력으로 전송됩니다. 메시지의 빈도는 상세 레벨에 따라 증가합니다. 10을 초과하면 모든 반복이보고됩니다.</target>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="translated">기능의 상세 모드입니다. 기본적으로 메모리 객체의 메모리 객체가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="translated">scikit-learn의 버전 및 해당 종속성</target>
        </trans-unit>
        <trans-unit id="588dcf117cf46b2a6deec167a78bc5c1266a3688" translate="yes" xml:space="preserve">
          <source>The visualization is fit automatically to the size of the axis. Use the &lt;code&gt;figsize&lt;/code&gt; or &lt;code&gt;dpi&lt;/code&gt; arguments of &lt;code&gt;plt.figure&lt;/code&gt; to control the size of the rendering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="translated">이 벡터 라이저에 의해 추출 된 어휘는 훨씬 더 커서 이제 로컬 위치 패턴으로 인코딩 된 모호성을 해결할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="translated">표본에 대한 가중 평균 확률은 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="translated">가중 불순물 감소 방정식은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="translated">모델의 가중치 \ (w \)에 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dff6c2479ef55aa391ea0314f017ab45501554fd" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="translated">X에서 각 관측치의 가중치입니다. None 인 경우 모든 관측치에 동일한 가중치가 할당됩니다 (기본값 : 없음).</target>
        </trans-unit>
        <trans-unit id="89b091775b7b9350f3e527eec283b3139c887c00" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cdf718f4170abd3f842260826cc1d9da2bb111a" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 메소드 호출로 계산 된 각 피처의 가중치 는 모델 속성에 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="translated">각 혼합물 성분의 무게.</target>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">와인 데이터 셋은 클래식하고 매우 쉬운 멀티 클래스 분류 데이터 셋입니다.</target>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="translated">&quot;기사&quot;라는 단어는 사람들이 다음과 같은 이전 게시물을 인용하는 빈도에 따라 중요한 기능입니다. &quot;기사 [기사 ID], [이름] &amp;lt;[이메일 주소]&amp;gt; 작성 :&quot;</target>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="translated">&lt;em&gt;제한된&lt;/em&gt; 단어 는 모델의 이분자 구조를 나타내며, 숨겨진 단위 간 또는 가시적 단위 간 직접적인 상호 작용을 금지합니다. 이는 다음과 같은 조건부 독립성을 가정합니다.</target>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="translated">경계를 인식하는 변형 &lt;code&gt;char_wb&lt;/code&gt; 는 단어 구분을 위해 공백을 사용하는 언어에서 특히 흥미 롭습니다 .이 경우 원시 &lt;code&gt;char&lt;/code&gt; 변형 보다 노이즈가 적은 기능을 생성 합니다. 이러한 언어의 경우 철자 및 단어 파생과 관련하여 견고성을 유지하면서 이러한 기능을 사용하여 학습 한 분류기의 예측 정확도 및 수렴 속도를 모두 높일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="translated">최악의 복잡도는 n = n_samples, p = n_features 인 O (n ^ (k + 2 / p))로 나타냅니다. (D. Arthur와 S. Vassilvitskii, 'k- 평균 방법은 얼마나 느리 냐?'SoCG2006)</target>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="translated">Theil-Sen Estimator : 강력한 다변량 회귀 모델.</target>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="translated">다중 선형 회귀 모형의 테일 센 추정기, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang 및 Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="translated">테일 센 회귀</target>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="translated">TheilSen은 X와 y 방향으로 작은 특이 치에 적합하지만 중단 점이 OLS보다 성능이 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;V-measure&lt;/strong&gt; 라는 그들의 고조파 평균 은 &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt; &lt;code&gt;v_measure_score&lt;/code&gt; 에&lt;/a&gt; 의해 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="translated">그런 다음 ipython 쉘을 시작하고 다음을 사용하여 진행중인 작업 스크립트를 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="translated">그런 다음 스케일링 후 데이터 포인트를 분류하는 것은 유클리드 거리의 데이터 포인트에 가장 가까운 추정 클래스 평균 \ (\ mu ^ * _ k \)를 찾는 것과 같습니다. 그러나 이것은 모든 클래스에 대해 모든 \ (\ mu ^ * _ k \)에 의해 생성 된 \ (K-1 \) affine subspace \ (H_K \)에 투영 한 후에도 수행 할 수 있습니다. 이는 LDA 분류기에서 암시 적으로 \ (K-1 \) 차원 공간으로 선형 투영함으로써 차원 축소가 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="translated">그런 다음 Davies-Bouldin 지수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="translated">그런 다음 메트릭은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="translated">그런 다음 멀티 클래스 MCC는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="translated">그런 다음 \ (Z \)의 행은 &lt;a href=&quot;clustering#k-means&quot;&gt;k-means를&lt;/a&gt; 사용하여 클러스터됩니다 . 첫 번째 &lt;code&gt;n_rows&lt;/code&gt; 레이블은 행 분할을 제공하고 나머지 &lt;code&gt;n_columns&lt;/code&gt; 레이블은 열 분할을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="35fd57830f62bfb9e9a1c9481c01f75fad3f7e92" translate="yes" xml:space="preserve">
          <source>Then we check the performance of the computed model plotting its predictions on the test set and computing, for example, the median absolute error of the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04ebdc92c95e8a854e544219afce2fe077f07d46" translate="yes" xml:space="preserve">
          <source>Then we check the quality of the predictions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="translated">그런 다음 유클리드 (L2) 규범을 적용하여 문서 1에 대해 다음 tf-idfs를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="translated">그런 다음 &lt;code&gt;raw_X&lt;/code&gt; 에 공급할 &lt;code&gt;FeatureHasher.transform&lt;/code&gt; 는 다음을 사용하여 구성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36e93c1a3cbd0ad36880c0132d911de26dc0ca83" translate="yes" xml:space="preserve">
          <source>Then, we identify features &lt;code&gt;X&lt;/code&gt; and targets &lt;code&gt;y&lt;/code&gt;: the column WAGE is our target variable (i.e., the variable which we want to predict).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2abd121a4ed8d06cf0817807518539ab09e24fe" translate="yes" xml:space="preserve">
          <source>Then, we introspect the information regarding each column data type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="translated">이론적 한계</target>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="translated">모델 예측의 품질을 평가하기위한 3 가지 API가 있습니다.</target>
        </trans-unit>
        <trans-unit id="96e1915039a69b2e8b64f25478a0431f9e517cc9" translate="yes" xml:space="preserve">
          <source>There are \(K\) topics in the corpus.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="translated">거리 측정법과 커널과 같은 유사성 측정 값 사이를 변환하는 방법에는 여러 가지가 있습니다. 하자 &lt;code&gt;D&lt;/code&gt; 는 거리, 그리고 &lt;code&gt;S&lt;/code&gt; 는 커널 수 :</target>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="translated">또한 몇 가지 단점이 있습니다 (메모리 내 어휘와 함께 CountVectorizer 사용).</target>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="translated">의사 결정 트리가 XOR, 패리티 또는 멀티플렉서 문제와 같이 의사 결정 트리를 쉽게 표현하지 못하기 때문에 배우기가 어려운 개념이 있습니다.</target>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="translated">특이 치에 의해 손상된 데이터를 처리 할 때 명심해야 할 사항이 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e7f48473f9869b2775936845c434aa7ef66ad5e" translate="yes" xml:space="preserve">
          <source>There are four more hyperparameters, \(\alpha_1\), \(\alpha_2\), \(\lambda_1\) and \(\lambda_2\) of the gamma prior distributions over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="translated">가장 가까운 이웃을 중심으로하는 많은 학습 루틴이 있습니다. 한 가지 예는 &lt;a href=&quot;density#density-estimation&quot;&gt;밀도 추정&lt;/a&gt; 섹션 에서 논의 된 &lt;a href=&quot;density#kernel-density&quot;&gt;커널 밀도 추정&lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="de49a8d62debf6dd1584d9f929c607a2c0abdd42" translate="yes" xml:space="preserve">
          <source>There are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See &lt;a href=&quot;../auto_examples/impute/plot_iterative_imputer_variants_comparison#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py&quot;&gt;Imputing missing values with variants of IterativeImputer&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afc5c09497df5828bed5e53fb9ba7340fbd12b56" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. It does not aim to be a general, &amp;lsquo;one-size-fits-all&amp;rsquo; solution as some tasks may require a more custom solution. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="translated">제공된 '영어'중지 단어 목록에는 몇 가지 알려진 문제가 있습니다. &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="69da86c48a6fab38a24a0a096ce607f0a2966cc1" translate="yes" xml:space="preserve">
          <source>There are several possibilities to do that, two of which are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="translated">40 개의 개별 피사체 각각에 대해 10 개의 서로 다른 이미지가 있습니다. 일부 피사체의 경우 조명, 얼굴 표정 (열린 눈 / 닫은 눈, 웃는 얼굴 / 웃지 않은 얼굴) 및 얼굴 세부 사항 (안경 / 안경)을 변경하여 이미지가 다른 시간에 촬영되었습니다. 모든 이미지는 대상이 똑바로 정면 위치에있는 어두운 동종 배경에 대해 촬영되었습니다 (일부 측면 움직임에 대한 내성).</target>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="translated">Support Vector Regression에는 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; 의 세 가지 구현이 있습니다. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; 은 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 보다 빠른 구현을 제공 하지만 선형 커널 만 고려하지만 &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;NuSVR&lt;/code&gt; &lt;/a&gt; 은 &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;LinearSVR&lt;/code&gt; &lt;/a&gt; 과 는 약간 다른 공식을 구현합니다 . 자세한 내용은 &lt;a href=&quot;#svm-implementation-details&quot;&gt;구현 세부 사항&lt;/a&gt; 을 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="7c46376033bb18a7479bbb076a0191da5ac66026" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers the linear kernel, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="translated">원하는 데이터 셋 유형에 따라 데이터 셋을 얻는 데 사용할 수있는 세 가지 주요 데이터 셋 인터페이스가 있습니다.</target>
        </trans-unit>
        <trans-unit id="f738d3558005f6c63ce087749d6fa9a898307784" translate="yes" xml:space="preserve">
          <source>There are two main methods to approximate the integral above, namely the &amp;lsquo;brute&amp;rsquo; and &amp;lsquo;recursion&amp;rsquo; methods. The &lt;code&gt;method&lt;/code&gt; parameter controls which method to use.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="translated">레이블을 지정하는 두 가지 옵션이 있습니다.</target>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="translated">바이러스 터링 결과를 평가하는 방법에는 내부 및 외부의 두 가지가 있습니다. 군집 안정성과 같은 내부 측정 값은 데이터와 결과 자체에만 의존합니다. 현재 scikit-learn에는 내부 bicluster 측정이 없습니다. 외부 측정은 실제 솔루션과 같은 외부 정보 소스를 나타냅니다. 실제 데이터로 작업 할 때 실제 솔루션은 일반적으로 알려져 있지 않지만 실제 솔루션을 알고 있기 때문에 인공 데이터를 biclustering하면 알고리즘을 정확하게 평가하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
