<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="7c9f9f5dcfc8aebd4eea110198ededa32c4b1278" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">추정기 피팅에서 오류가 발생할 경우 점수에 지정할 값입니다. '올림'으로 설정하면 오류가 발생합니다. 숫자 값이 제공되면 FitFailedWarning이 발생합니다. 이 매개 변수는 수정 단계에 영향을 미치지 않으므로 항상 오류가 발생합니다. 기본값은 '올림'이지만 버전 0.22부터 np.nan으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="51fe27463b1bbffac6c175b98fff3a09a8ef007a" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If set to &amp;lsquo;raise-deprecating&amp;rsquo;, a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise-deprecating&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">추정기 피팅에서 오류가 발생할 경우 점수에 지정할 값입니다. '올림'으로 설정하면 오류가 발생합니다. '사용 중지'로 설정하면 오류가 발생하기 전에 FutureWarning이 인쇄됩니다. 숫자 값이 제공되면 FitFailedWarning이 발생합니다. 이 매개 변수는 수정 단계에 영향을 미치지 않으므로 항상 오류가 발생합니다. 기본값은 '감추기'이지만 버전 0.22부터는 np.nan으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="402a4cfb84a22d3670431b1576518e6587de93b8" translate="yes" xml:space="preserve">
          <source>Value to use for the dummy feature.</source>
          <target state="translated">더미 기능에 사용할 값입니다.</target>
        </trans-unit>
        <trans-unit id="82321dd8f607145fb8d2875c3e367d82d45dfc71" translate="yes" xml:space="preserve">
          <source>Value with which negative labels must be encoded.</source>
          <target state="translated">음수 레이블을 인코딩해야하는 값입니다.</target>
        </trans-unit>
        <trans-unit id="4e0758fceaa4f106e89501aa7c196eea7d1ad1c2" translate="yes" xml:space="preserve">
          <source>Value with which positive labels must be encoded.</source>
          <target state="translated">양수 레이블을 인코딩해야하는 값입니다.</target>
        </trans-unit>
        <trans-unit id="ca5e1888f7ff9f4679a3377b455596a48d014681" translate="yes" xml:space="preserve">
          <source>ValueError</source>
          <target state="translated">ValueError</target>
        </trans-unit>
        <trans-unit id="6a7ed2e67e56dace630120ac5c7bd01e4d032523" translate="yes" xml:space="preserve">
          <source>Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.</source>
          <target state="translated">임계 값보다 큰 값은 1에 매핑되고 임계 값보다 작거나 같은 값은 0에 매핑됩니다. 기본 임계 값이 0이면 양수 값만 1에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="0a659f48fb09b2c2dd949774cc3bd6b7ffd6fd87" translate="yes" xml:space="preserve">
          <source>Values in each bin have the same nearest center of a 1D k-means cluster.</source>
          <target state="translated">각 구간의 값은 1D k- 평균 군집의 가장 가까운 중심을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="c67d763b94a97115ba7ee9d49115a272cb508cf6" translate="yes" xml:space="preserve">
          <source>Values of n_samples samples drawn from Gaussian process and evaluated at query points.</source>
          <target state="translated">n_samples의 값은 가우스 프로세스에서 추출되어 쿼리 지점에서 평가됩니다.</target>
        </trans-unit>
        <trans-unit id="1cd1b62dfd3b6a63572d1bf631789bd088451d1d" translate="yes" xml:space="preserve">
          <source>Values of the visible layer after one Gibbs step.</source>
          <target state="translated">1 Gibbs 단계 후 표시되는 레이어의 값입니다.</target>
        </trans-unit>
        <trans-unit id="c9500aef779ad4ab4355590ca05b9c0de0aa083a" translate="yes" xml:space="preserve">
          <source>Values of the visible layer to start from.</source>
          <target state="translated">보이는 레이어의 값으로 시작합니다.</target>
        </trans-unit>
        <trans-unit id="d9ca5115511a5b00d878e1de5b9daa8f530b632c" translate="yes" xml:space="preserve">
          <source>Values of the visible layer. Must be all-boolean (not checked).</source>
          <target state="translated">보이는 레이어의 값. 모두 부울이어야합니다 (확인되지 ​​않음).</target>
        </trans-unit>
        <trans-unit id="35e8d31773dfd80568909d00a3100d73e50de4e1" translate="yes" xml:space="preserve">
          <source>Values predicted by each regressor.</source>
          <target state="translated">각 회귀 변수에서 예측 한 값입니다.</target>
        </trans-unit>
        <trans-unit id="445d09e482944669dc8a6e893591f45bda28254b" translate="yes" xml:space="preserve">
          <source>Vanschoren, van Rijn, Bischl and Torgo &lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;OpenML: networked science in machine learning&amp;rdquo;&lt;/a&gt;, ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.</source>
          <target state="translated">Vanschoren, van Rijn, Bischl 및 Torgo &lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;OpenML : 기계 학습의 네트워크 과학&amp;rdquo;&lt;/a&gt; , ACM SIGKDD Explorations Newsletter, 15 (2), 49-60, 2014.</target>
        </trans-unit>
        <trans-unit id="ba47c39bbafea14cc75438e6420272a547d1a3e3" translate="yes" xml:space="preserve">
          <source>Variance explained by each of the selected components.</source>
          <target state="translated">선택한 각 구성 요소에 따라 차이가 설명됩니다.</target>
        </trans-unit>
        <trans-unit id="17b5cb397e6ad9830d59b5fc66bebb45024c7ef4" translate="yes" xml:space="preserve">
          <source>Variances of individual features.</source>
          <target state="translated">개별 기능의 차이.</target>
        </trans-unit>
        <trans-unit id="7933f72bf76c6cfbc1dde87498522f5e833878e6" translate="yes" xml:space="preserve">
          <source>Variational Bayesian estimation of a Gaussian mixture.</source>
          <target state="translated">가우스 혼합의 변형 베이지안 추정.</target>
        </trans-unit>
        <trans-unit id="e459652719d6e0edf38bb3c9f14dba040888c62f" translate="yes" xml:space="preserve">
          <source>Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.</source>
          <target state="translated">변형 추론은 데이터 가능성 대신 모형 증거 (사전 포함)의 하한을 최대화하는 기대 최대화의 확장입니다. 변형 방법의 기본 원리는 기대 최대화와 동일합니다 (둘 다 각 혼합물에 의해 생성 될 각 점에 대한 확률 찾기와 이러한 할당 된 점에 혼합물 맞추기간에 번갈아 가며 반복되는 알고리즘입니다). 이전 배포판의 정보 통합 이를 통해 기대 최대화 솔루션에서 흔히 발견되는 특이점을 피할 수 있지만 모델에 미묘한 편향이 발생합니다. 추론은 종종 속도가 느리지 만 일반적으로 사용을 실용적이지 않게하는 정도는 아닙니다.</target>
        </trans-unit>
        <trans-unit id="009e019794c4b5a288d65b8e0eabe9064e298c81" translate="yes" xml:space="preserve">
          <source>Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the &amp;ldquo;true&amp;rdquo; number of components, affects only algorithmic complexity, not the actual number of components used).</source>
          <target state="translated">Dirichlet 공정에 대한 변형 추론 기법은 여전히이 무한 혼합 모델에 대한 유한 근사법으로 작동하지만 사용하고자하는 성분 수를 사전에 지정하는 대신 농도 매개 변수와 혼합물 수의 상한을 지정합니다. 구성 요소 ( &quot;실제&quot;구성 요소 수보다 높다고 가정하면이 상한은 실제 사용되는 구성 요소 수가 아니라 알고리즘 복잡성에만 영향을 미칩니다).</target>
        </trans-unit>
        <trans-unit id="bfc42eb1bb86ce5f62b086e9ffd3c67a080b0730" translate="yes" xml:space="preserve">
          <source>Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, &lt;code&gt;components_[i, j]&lt;/code&gt; can be viewed as pseudocount that represents the number of times word &lt;code&gt;j&lt;/code&gt; was assigned to topic &lt;code&gt;i&lt;/code&gt;. It can also be viewed as distribution over the words for each topic after normalization: &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt;.</source>
          <target state="translated">주제 단어 분포에 대한 변형 매개 변수. 주제 단어 분포에 대한 완전한 조건은 Dirichlet이므로 &lt;code&gt;components_[i, j]&lt;/code&gt; 는 단어 &lt;code&gt;j&lt;/code&gt; 가 주제 &lt;code&gt;i&lt;/code&gt; 에 할당 된 횟수를 나타내는 의사 수로 볼 수 있습니다 . 정규화 후 각 주제에 대한 단어에 대한 분포로 볼 수도 있습니다. &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="65cb8d21f19957f8f56d7ec3aeccadf0c1efbc6f" translate="yes" xml:space="preserve">
          <source>Various Agglomerative Clustering on a 2D embedding of digits</source>
          <target state="translated">자릿수의 2D 임베딩에 대한 다양한 집계 클러스터링</target>
        </trans-unit>
        <trans-unit id="ddbf21d472ec4b83f538c33f12eb263b69f44ca0" translate="yes" xml:space="preserve">
          <source>Various improvements were made to &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. On top of the Poisson loss mentionned above, these estimators now support &lt;a href=&quot;../../modules/ensemble#sw-hgbdt&quot;&gt;sample weights&lt;/a&gt;. Also, an automatic early-stopping criterion was added: early-stopping is enabled by default when the number of samples exceeds 10k. Finally, users can now define &lt;a href=&quot;../../modules/ensemble#monotonic-cst-gbdt&quot;&gt;monotonic constraints&lt;/a&gt; to constrain the predictions based on the variations of specific features. In the following example, we construct a target that is generally positively correlated with the first feature, with some noise. Applying monotoinc constraints allows the prediction to capture the global effect of the first feature, instead of fitting the noise.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt; &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; 에 대한&lt;/a&gt; 다양한 개선이 이루어 졌습니다 . 위에서 언급 한 포아송 손실 외에도 이러한 추정기는 이제 &lt;a href=&quot;../../modules/ensemble#sw-hgbdt&quot;&gt;샘플 가중치를&lt;/a&gt; 지원 합니다. 또한 자동 조기 중지 기준이 추가되었습니다. 샘플 수가 10k를 초과하면 기본적으로 조기 중지가 활성화됩니다. 마지막으로 사용자는 이제 특정 기능의 변형을 기반으로 예측을 &lt;a href=&quot;../../modules/ensemble#monotonic-cst-gbdt&quot;&gt;제한&lt;/a&gt; 하기 위해 단조로운 제약 조건 을 정의 할 수 있습니다. 다음 예제에서는 노이즈가있는 첫 번째 기능과 일반적으로 양의 상관 관계가있는 대상을 구성합니다. 모노톤 제약 조건을 적용하면 예측에서 노이즈를 맞추는 대신 첫 번째 기능의 전체 효과를 캡처 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b1e276370580ceeca94d9a25d3b75abf89a69938" translate="yes" xml:space="preserve">
          <source>Varying regularization in Multi-layer Perceptron</source>
          <target state="translated">다층 퍼셉트론의 다양한 정규화</target>
        </trans-unit>
        <trans-unit id="1e178759402bc070ae202c1ae1b1666da0dd6a9c" translate="yes" xml:space="preserve">
          <source>Vector Quantization Example</source>
          <target state="translated">벡터 양자화 예</target>
        </trans-unit>
        <trans-unit id="ba8b3829eecac2c5ad85a64a161b0e7f2fae54cf" translate="yes" xml:space="preserve">
          <source>Vector of errors at each iteration.</source>
          <target state="translated">각 반복에서 오류의 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="4e8912fa819c195ed09c153115c88939fa1f2e3c" translate="yes" xml:space="preserve">
          <source>Vector to be scored, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">채점 할 벡터. 여기서 &lt;code&gt;n_samples&lt;/code&gt; 는 샘플 수이고 &lt;code&gt;n_features&lt;/code&gt; 는 특성 수입니다.</target>
        </trans-unit>
        <trans-unit id="cf31e021601b7cdd3aba8ca6f6502b2543cdeb94" translate="yes" xml:space="preserve">
          <source>VehAge</source>
          <target state="translated">VehAge</target>
        </trans-unit>
        <trans-unit id="ea3a474961d1b758addd0978141f5ebc1f6f2b70" translate="yes" xml:space="preserve">
          <source>VehBrand</source>
          <target state="translated">VehBrand</target>
        </trans-unit>
        <trans-unit id="fda6c72b6cabb64878498cd55268cca04e8d770b" translate="yes" xml:space="preserve">
          <source>VehGas</source>
          <target state="translated">VehGas</target>
        </trans-unit>
        <trans-unit id="9e0c38e996b000566fa359d3ed449ba796915a3e" translate="yes" xml:space="preserve">
          <source>VehPower</source>
          <target state="translated">VehPower</target>
        </trans-unit>
        <trans-unit id="93c6f0903309fd539ceb7d4710cf07f7db8cb1d8" translate="yes" xml:space="preserve">
          <source>Verbose mode when fitting the model.</source>
          <target state="translated">모델을 피팅 할 때 상세 모드.</target>
        </trans-unit>
        <trans-unit id="ee5731f921bfe2d1197cd007f006306ad3328112" translate="yes" xml:space="preserve">
          <source>Verbose output during PD computations.</source>
          <target state="translated">PD 계산 중 자세한 출력.</target>
        </trans-unit>
        <trans-unit id="eeb343db47ab9124ef417c41d9bdb92839772dc0" translate="yes" xml:space="preserve">
          <source>Verbose output during PD computations. Defaults to 0.</source>
          <target state="translated">PD 계산 중 자세한 출력. 기본값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="65d3f9d357c8217e4b4161cc31c9983e755e9511" translate="yes" xml:space="preserve">
          <source>Verbosity flag, controls the debug messages that are issued as functions are evaluated.</source>
          <target state="translated">Verbosity 플래그는 함수가 평가 될 때 발행되는 디버그 메시지를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="66d100e92da285b9102a10ebf1825a4d7ce2d91c" translate="yes" xml:space="preserve">
          <source>Verbosity flag, controls the debug messages that are issued as functions are evaluated. The higher, the more verbose. Can be 0, 1, or 2.</source>
          <target state="translated">Verbosity 플래그는 함수가 평가 될 때 발행되는 디버그 메시지를 제어합니다. 높을수록 더 자세한 정보가 표시됩니다. 0, 1 또는 2 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a606460a9db19f2456900341e545d542d386dcd0" translate="yes" xml:space="preserve">
          <source>Verbosity level.</source>
          <target state="translated">상세 수준.</target>
        </trans-unit>
        <trans-unit id="9cadb350a887ea26b759cec53fba259b94be0b70" translate="yes" xml:space="preserve">
          <source>Verbosity level. Setting verbose &amp;gt; 0 will display additional information depending on the solver used.</source>
          <target state="translated">상세 수준. verbose&amp;gt; 0을 설정하면 사용 된 솔버에 따라 추가 정보가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="c09635f4883cd7798a06597eead68509e08bef52" translate="yes" xml:space="preserve">
          <source>Verbosity mode.</source>
          <target state="translated">상세 모드.</target>
        </trans-unit>
        <trans-unit id="4ee9c427e0cc678ca91bc7294708dc43db03512b" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#gp-kernels&quot;&gt;kernels&lt;/a&gt; can be specified. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">다용도 : 다른 &lt;a href=&quot;#gp-kernels&quot;&gt;커널을&lt;/a&gt; 지정할 수 있습니다. 일반적인 커널이 제공되지만 사용자 정의 커널을 지정할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="4b16fac84e3102257e26d97dc6bcc2775a76c275" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">다용도 : 의사 결정 기능에 대해 다른 &lt;a href=&quot;#svm-kernels&quot;&gt;커널 기능을&lt;/a&gt; 지정할 수 있습니다. 일반적인 커널이 제공되지만 사용자 정의 커널을 지정할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="102caab3d934ebf62d9240e41edbdfb96ec830ff" translate="yes" xml:space="preserve">
          <source>Version of the dataset. Can only be provided if also &lt;code&gt;name&lt;/code&gt; is given. If &amp;lsquo;active&amp;rsquo; the oldest version that&amp;rsquo;s still active is used. Since there may be more than one active version of a dataset, and those versions may fundamentally be different from one another, setting an exact version is highly recommended.</source>
          <target state="translated">데이터 세트의 버전입니다. &lt;code&gt;name&lt;/code&gt; 이 지정된 경우에만 제공 할 수 있습니다 . '활성'인 경우 여전히 활성 상태 인 가장 오래된 버전이 사용됩니다. 데이터 세트의 활성 버전이 둘 이상있을 수 있으며 해당 버전이 근본적으로 다를 수 있으므로 정확한 버전을 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="2d0ab9e3d9a896817cdfc684c77fbf9f0c4f1aac" translate="yes" xml:space="preserve">
          <source>Version: RCV1-v2, vectors, full sets, topics multilabels.</source>
          <target state="translated">버전 : RCV1-v2, 벡터, 전체 세트, 주제 다중 레이블.</target>
        </trans-unit>
        <trans-unit id="dfb46f94f7d77a5ddeabfd408748577e450d338b" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, large &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">매우 큰 &lt;code&gt;n_samples&lt;/code&gt; , 큰 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3496b8ff284f2499d5b89bafe815a9579d5a779b" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">매우 큰 &lt;code&gt;n_samples&lt;/code&gt; , 중간 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90edfe52a065a045edbdd25bf2f2316a234658ac" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt; with &lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch code&lt;/a&gt;</source>
          <target state="translated">매우 큰 &lt;code&gt;n_samples&lt;/code&gt; , 매체 &lt;code&gt;n_clusters&lt;/code&gt; 와 &lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch 코드&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="56b71e89fb1079caaadefd0889e9a22e8b0560e3" translate="yes" xml:space="preserve">
          <source>Videos</source>
          <target state="translated">Videos</target>
        </trans-unit>
        <trans-unit id="644cc701dadc9ddd9912f1b11b35ead35737260b" translate="yes" xml:space="preserve">
          <source>Vinh et al. (2010) named variants of NMI and AMI by their averaging method &lt;a href=&quot;#veb2010&quot; id=&quot;id15&quot;&gt;[VEB2010]&lt;/a&gt;. Their &amp;lsquo;sqrt&amp;rsquo; and &amp;lsquo;sum&amp;rsquo; averages are the geometric and arithmetic means; we use these more broadly common names.</source>
          <target state="translated">Vinh et al. (2010)은 평균화 방법으로 NMI 및 AMI의 변종을 명명했습니다 &lt;a href=&quot;#veb2010&quot; id=&quot;id15&quot;&gt;[VEB2010]&lt;/a&gt; . 그들의 'sqrt'및 'sum'평균은 기하학적 및 산술적 수단입니다. 우리는 이러한보다 광범위하게 일반적인 이름을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="f74a22805e87ed187a5bf75da0c74d88f9fc4e05" translate="yes" xml:space="preserve">
          <source>Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their &amp;lsquo;sqrt&amp;rsquo; and &amp;lsquo;sum&amp;rsquo; averages are the geometric and arithmetic means; we use these more broadly common names.</source>
          <target state="translated">Vinh et al. (2010)은 평균화 방법 [VEB2010]으로 NMI 및 AMI의 변형을 명명했습니다. 그들의 'sqrt'와 'sum'평균은 기하 및 산술 수단입니다. 우리는 이러한보다 일반적인 이름을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="3e1bb17ff94c0af1df416cfea07d80b830f06013" translate="yes" xml:space="preserve">
          <source>Vinh, Epps, and Bailey, (2009). &amp;ldquo;Information theoretic measures for clusterings comparison&amp;rdquo;. Proceedings of the 26th Annual International Conference on Machine Learning - ICML &amp;lsquo;09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi:10.1145/1553374.1553511&lt;/a&gt;. ISBN 9781605585161.</source>
          <target state="translated">Vinh, Epps 및 Bailey, (2009). &amp;ldquo;클러스터링 비교를위한 정보 이론적 측정&amp;rdquo;. 기계 학습에 관한 제 26 회 연례 국제 컨퍼런스-ICML '09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;DOI : / 1553374.1553511 10.1145을&lt;/a&gt; . ISBN 9781605585161.</target>
        </trans-unit>
        <trans-unit id="aa05fa0b125b1736672a1d80de8273f7659ae9f5" translate="yes" xml:space="preserve">
          <source>Vinh, Epps, and Bailey, (2010). &amp;ldquo;Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance&amp;rdquo;. JMLR &amp;lt;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt;&amp;gt;</source>
          <target state="translated">Vinh, Epps 및 Bailey, (2010). &quot;클러스터링 비교를위한 정보 이론적 측정 : 변형, 속성, 정규화 및 기회 수정&quot;. JMLR &amp;lt; &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt; &amp;gt;</target>
        </trans-unit>
        <trans-unit id="40fdfddfc4699e8e891d021e6ce4e4252243f9c7" translate="yes" xml:space="preserve">
          <source>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance, JMLR</source>
          <target state="translated">Vinh, Epps, Bailey (2010). 군집화 비교를위한 정보 이론적 측정 : 변형, 특성, 정규화 및 수정, JMLR에 대한 수정</target>
        </trans-unit>
        <trans-unit id="64eeb8915eff8290b0627c8aa96dd46ee4bda6f1" translate="yes" xml:space="preserve">
          <source>Visualise your tree as you are training by using the &lt;code&gt;export&lt;/code&gt; function. Use &lt;code&gt;max_depth=3&lt;/code&gt; as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.</source>
          <target state="translated">&lt;code&gt;export&lt;/code&gt; 기능 을 사용하여 훈련하는 동안 트리를 시각화하십시오 . 사용 &lt;code&gt;max_depth=3&lt;/code&gt; 초기 트리 깊이로는 나무가 데이터에 피팅되는 방법에 대한 느낌을 얻을하고 깊이를 증가시킵니다.</target>
        </trans-unit>
        <trans-unit id="d175985b87dd9f620aa960059c730b4a35e3bcb5" translate="yes" xml:space="preserve">
          <source>Visualization</source>
          <target state="translated">Visualization</target>
        </trans-unit>
        <trans-unit id="38623835e09f3cd243cdf966707dad9de4ecfeda" translate="yes" xml:space="preserve">
          <source>Visualization of MLP weights on MNIST</source>
          <target state="translated">MNIST의 MLP 가중치 시각화</target>
        </trans-unit>
        <trans-unit id="a291ff40a157c9afd67fb01fd3c6b6d368d78978" translate="yes" xml:space="preserve">
          <source>Visualization of predictions obtained from different models.</source>
          <target state="translated">다른 모델에서 얻은 예측 시각화.</target>
        </trans-unit>
        <trans-unit id="94a741e26bd8d9bfdacbab90c67da4753dcabca8" translate="yes" xml:space="preserve">
          <source>Visualizations with Display Objects</source>
          <target state="translated">표시 개체를 사용한 시각화</target>
        </trans-unit>
        <trans-unit id="a6c65fa336dc53edc04e670583358fb288a99997" translate="yes" xml:space="preserve">
          <source>Visualize cross-validation indices for many CV objects</source>
          <target state="translated">많은 CV 객체에 대한 교차 검증 지수 시각화</target>
        </trans-unit>
        <trans-unit id="4a2bddc914855cb9100c33fc5e346e89e1926136" translate="yes" xml:space="preserve">
          <source>Visualize our data</source>
          <target state="translated">데이터 시각화</target>
        </trans-unit>
        <trans-unit id="28ba2de8813583360eb0588f62cb6dc19a1f4072" translate="yes" xml:space="preserve">
          <source>Visualize the resulting regions</source>
          <target state="translated">결과 영역을 시각화</target>
        </trans-unit>
        <trans-unit id="b77da1f257213eacf5971ec98d2f34c8fe910374" translate="yes" xml:space="preserve">
          <source>Visualizing cross-validation behavior in scikit-learn</source>
          <target state="translated">scikit-learn에서 교차 유효성 검사 동작 시각화</target>
        </trans-unit>
        <trans-unit id="e6614e53d3137ea4329f7b88a52f014060c402bb" translate="yes" xml:space="preserve">
          <source>Visualizing the stock market structure</source>
          <target state="translated">주식 시장 구조 시각화</target>
        </trans-unit>
        <trans-unit id="3e1e0ef10e7a115946f530e934380438421c5b34" translate="yes" xml:space="preserve">
          <source>Vocabulary: classification and regression</source>
          <target state="translated">어휘 : 분류 및 회귀</target>
        </trans-unit>
        <trans-unit id="9b3e588521f2631086f0d4ea61248bd80936e042" translate="yes" xml:space="preserve">
          <source>VotingRegressor</source>
          <target state="translated">VotingRegressor</target>
        </trans-unit>
        <trans-unit id="dd7b37acd93acaf11b82a9ebbc3eea819b85e0ef" translate="yes" xml:space="preserve">
          <source>W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171.</source>
          <target state="translated">WH Wolberg, WN Street 및 OL Mangasarian. 미세 바늘 흡 인물로부터 유방암을 진단하는 기계 학습 기술. 암 편지 77 (1994) 163-171.</target>
        </trans-unit>
        <trans-unit id="985d7c09beb3a8622b6c063b009de9296fdb89ed" translate="yes" xml:space="preserve">
          <source>W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&amp;amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.</source>
          <target state="translated">WN Street, WH Wolberg 및 OL Mangasarian. 유방 종양 진단을위한 핵 특징 추출. IS &amp;amp; T / SPIE 1993 전자 이미징에 관한 국제 심포지엄 : 과학 및 기술, 1905 권, 861-870 쪽, 캘리포니아 주 산호세, 1993.</target>
        </trans-unit>
        <trans-unit id="0525374f4c7331dc5d256feba32a265d327160ed" translate="yes" xml:space="preserve">
          <source>WDBC-Benign</source>
          <target state="translated">WDBC-Benign</target>
        </trans-unit>
        <trans-unit id="fd630df285b07b6076d3b38d88445f6031d3902e" translate="yes" xml:space="preserve">
          <source>WDBC-Malignant</source>
          <target state="translated">WDBC-Malignant</target>
        </trans-unit>
        <trans-unit id="9b852a8108b3e892136da9e7da9f0a5bb56540ea" translate="yes" xml:space="preserve">
          <source>WMinkowskiDistance</source>
          <target state="translated">WMinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="c3600a53fe931326fad0ec5abc0f593830220f56" translate="yes" xml:space="preserve">
          <source>Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp;amp; Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)</source>
          <target state="translated">Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp;amp; Liu, TY (2013 년 5 월). NDCG 순위 측정에 대한 이론적 분석. 제 26 차 학습 이론에 관한 연례 컨퍼런스 (COLT 2013)</target>
        </trans-unit>
        <trans-unit id="4e8ee595c7db5dd5f284f8fb603cc66c6c8287ae" translate="yes" xml:space="preserve">
          <source>Ward clustering based on a Feature matrix.</source>
          <target state="translated">기능 매트릭스를 기반으로하는 와드 클러스터링.</target>
        </trans-unit>
        <trans-unit id="1af684513cf70467c9307765e01677f8970cff6e" translate="yes" xml:space="preserve">
          <source>Ward hierarchical clustering</source>
          <target state="translated">와드 계층 적 클러스터링</target>
        </trans-unit>
        <trans-unit id="d2173ac976f5809b703436c0de33dab1598b674c" translate="yes" xml:space="preserve">
          <source>Ward is the most effective method for noisy data.</source>
          <target state="translated">Ward는 시끄러운 데이터에 가장 효과적인 방법입니다.</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="44d79cfceaac3d6c963709a9f443a7578dfdd3fa" translate="yes" xml:space="preserve">
          <source>Warning class used if there is an error while fitting the estimator.</source>
          <target state="translated">추정기를 피팅하는 동안 오류가 발생하면 경고 클래스가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c56f46b7e83e71f0bcaf54dacd8cfc15c71ef274" translate="yes" xml:space="preserve">
          <source>Warning class used to notify the user of any change in the behavior.</source>
          <target state="translated">동작 변경 사항을 사용자에게 알리는 데 사용되는 경고 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="43a2ad1c4144ef567b3006486da55db4df5bcce7" translate="yes" xml:space="preserve">
          <source>Warning used to notify implicit data conversions happening in the code.</source>
          <target state="translated">코드에서 발생하는 암시 적 데이터 변환을 알리는 데 사용되는 경고입니다.</target>
        </trans-unit>
        <trans-unit id="69bb37448a64e3ca58327c210b042b57b19259a7" translate="yes" xml:space="preserve">
          <source>Warning used to notify the user of inefficient computation.</source>
          <target state="translated">비효율적 인 계산을 사용자에게 알리는 경고.</target>
        </trans-unit>
        <trans-unit id="95fde5bcc048210bdd2da0e9628c10dadee1ce1e" translate="yes" xml:space="preserve">
          <source>Warning used when the dot operation does not use BLAS.</source>
          <target state="translated">도트 작업에서 BLAS를 사용하지 않을 때 사용되는 경고입니다.</target>
        </trans-unit>
        <trans-unit id="77d4a9d6a0a46436c153d66828a62d378a7e1f5d" translate="yes" xml:space="preserve">
          <source>Warning used when the metric is invalid</source>
          <target state="translated">메트릭이 유효하지 않은 경우 사용되는 경고</target>
        </trans-unit>
        <trans-unit id="d0aaba5d13d0d7235d440530a46ccfa6343b56ff" translate="yes" xml:space="preserve">
          <source>Warning: Extra-trees should only be used within ensemble methods.</source>
          <target state="translated">경고 : 여분의 트리는 앙상블 방법 내에서만 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="c083bc8548c03dc08f53c7e8a611e5699830557a" translate="yes" xml:space="preserve">
          <source>Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;sklearn.inspection.permutation_importance&lt;/code&gt;&lt;/a&gt; as an alternative.</source>
          <target state="translated">경고 : 높은 카디널리티 기능 (많은 고유 값)의 경우 불순물 기반 기능 중요도가 잘못 될 수 있습니다. 대안으로 &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt; &lt;code&gt;sklearn.inspection.permutation_importance&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="a70a0040ebdf5db3cf497e998d5844dc921e921b" translate="yes" xml:space="preserve">
          <source>Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See &lt;a href=&quot;sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;sklearn.inspection.permutation_importance&lt;/code&gt;&lt;/a&gt; as an alternative.</source>
          <target state="translated">경고 : 높은 카디널리티 기능 (많은 고유 값)의 경우 불순물 기반 기능 중요도가 잘못 될 수 있습니다. 대안으로 &lt;a href=&quot;sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt; &lt;code&gt;sklearn.inspection.permutation_importance&lt;/code&gt; &lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="c2eb6fdf9d13ab34884681ef0c66f41e16b52aad" translate="yes" xml:space="preserve">
          <source>Warning: this function is experimental and subject to change in a future version of joblib.</source>
          <target state="translated">경고 :이 기능은 실험용이며 이후 버전의 joblib에서 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b33d3bb4e4bfe5e80c2407f4ead429c7fa466ef0" translate="yes" xml:space="preserve">
          <source>We achieved 83.5% accuracy. Let&amp;rsquo;s see if we can do better with a linear &lt;a href=&quot;../../modules/svm#svm&quot;&gt;support vector machine (SVM)&lt;/a&gt;, which is widely regarded as one of the best text classification algorithms (although it&amp;rsquo;s also a bit slower than na&amp;iuml;ve Bayes). We can change the learner by simply plugging a different classifier object into our pipeline:</source>
          <target state="translated">83.5 %의 정확도를 달성했습니다. 최고의 텍스트 분류 알고리즘 중 하나로 널리 알려진 선형 &lt;a href=&quot;../../modules/svm#svm&quot;&gt;지원 벡터 머신 (SVM)을&lt;/a&gt; 사용하여 더 나은 작업을 수행 할 수 있는지 살펴 보겠습니다 ( Naive Bayes보다 약간 느리지 만). 다른 분류 자 ​​객체를 파이프 라인에 간단히 연결하여 학습자를 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5e4234559a6f8eb7829d45ed1528d4d2b014e858" translate="yes" xml:space="preserve">
          <source>We achieved 91.3% accuracy using the SVM. &lt;code&gt;scikit-learn&lt;/code&gt; provides further utilities for more detailed performance analysis of the results:</source>
          <target state="translated">SVM을 사용하여 91.3 %의 정확도를 달성했습니다. &lt;code&gt;scikit-learn&lt;/code&gt; 은 결과의보다 자세한 성능 분석을위한 추가 유틸리티를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="4073bf855ec3741a8d17116f66549a3127ee1b52" translate="yes" xml:space="preserve">
          <source>We add observation noise to these waveforms. We generate very sparse noise: only 6% of the time points contain noise. As a result, the l1 norm of this noise (ie &amp;ldquo;cityblock&amp;rdquo; distance) is much smaller than it&amp;rsquo;s l2 norm (&amp;ldquo;euclidean&amp;rdquo; distance). This can be seen on the inter-class distance matrices: the values on the diagonal, that characterize the spread of the class, are much bigger for the Euclidean distance than for the cityblock distance.</source>
          <target state="translated">이 파형에 관측 노이즈를 추가합니다. 우리는 매우 드문 노이즈를 생성합니다. 6 %의 포인트 만 노이즈를 포함합니다. 결과적으로이 잡음의 l1 규범 (즉,&amp;ldquo;cityblock&amp;rdquo;거리)은 l2 규범 (&amp;ldquo;유클리드&amp;rdquo;거리)보다 훨씬 작습니다. 이것은 클래스 간 거리 행렬에서 볼 수 있습니다. 클래스의 확산을 특징으로하는 대각선의 값은 도시 블록 거리보다 유클리드 거리에서 훨씬 큽니다.</target>
        </trans-unit>
        <trans-unit id="0408d5d268f82423b1624837fe33bcd8dd7f81b2" translate="yes" xml:space="preserve">
          <source>We also observe that &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; has much smoother predictions than &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. For the plots to be comparable, it is necessary to subtract the average value of the target &lt;code&gt;y&lt;/code&gt;: The &amp;lsquo;recursion&amp;rsquo; method, used by default for &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, does not account for the initial predictor (in our case the average target). Setting the target average to 0 avoids this bias.</source>
          <target state="translated">우리는 또한 &lt;a href=&quot;../../modules/generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt; &lt;code&gt;MLPRegressor&lt;/code&gt; &lt;/a&gt; 가 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 보다 훨씬 더 부드러운 예측을 가지고 있음을 관찰합니다 . 플롯을 비교할 수 있으려면 대상 &lt;code&gt;y&lt;/code&gt; 의 평균 값을 빼야합니다 . &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 에 대해 기본적으로 사용되는 '재귀'방법 은 초기 예측 변수 (이 경우 평균 대상)를 고려하지 않습니다. 목표 평균을 0으로 설정하면 이러한 편향을 피할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5fc281c3e8b120a8bce90f392d5367731bdfa8e6" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">또한 다항식 확장을 사용하여 1 차원 회귀에 대한 ARD의 예측 및 불확실성을 플로팅합니다. 플롯의 오른쪽에서 불확실성이 증가하기 시작합니다. 이러한 테스트 샘플이 교육 샘플 범위를 벗어나기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="bda5a1ba58eab4f9acd36ef763104051f247918e" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">또한 다항식 확장을 사용하여 1 차원 회귀에 대한 베이지안 릿지 회귀에 대한 예측 및 불확실성을 플로팅합니다. 플롯의 오른쪽에서 불확실성이 증가하기 시작합니다. 이러한 테스트 샘플이 교육 샘플 범위를 벗어나기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="796db2c0bddee3a97c873bf6deae5d571e22b022" translate="yes" xml:space="preserve">
          <source>We also show the tree structure of a model built on all of the features.</source>
          <target state="translated">또한 모든 기능을 기반으로 구축 된 모델의 트리 구조도 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="b29f4bff482ccd99c1af96e146d78b516ea318aa" translate="yes" xml:space="preserve">
          <source>We also use warm_start=True which means that the coefficients of the models are reused to initialize the next model fit to speed-up the computation of the full-path.</source>
          <target state="translated">또한 warm_start = True를 사용합니다. 이는 전체 경로 계산 속도를 높이기 위해 다음 모델 적합을 초기화하기 위해 모델의 계수가 재사용됨을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="58db9129e28b6367a4c4b1cf6dbdf62e7e4b259e" translate="yes" xml:space="preserve">
          <source>We are pleased to announce the release of scikit-learn 0.22, which comes with many bug fixes and new features! We detail below a few of the major features of this release. For an exhaustive list of all the changes, please refer to the &lt;a href=&quot;https://scikit-learn.org/0.23/whats_new/v0.22.html#changes-0-22&quot;&gt;release notes&lt;/a&gt;.</source>
          <target state="translated">많은 버그 수정과 새로운 기능이 포함 된 scikit-learn 0.22의 출시를 발표하게되어 기쁘게 생각합니다! 이 릴리스의 몇 가지 주요 기능을 아래에서 자세히 설명합니다. 모든 변경 사항의 전체 목록은 &lt;a href=&quot;https://scikit-learn.org/0.23/whats_new/v0.22.html#changes-0-22&quot;&gt;출시 노트&lt;/a&gt; 를 참조하세요 .</target>
        </trans-unit>
        <trans-unit id="85e7dd24e85e9c4275de753975b0d57e773f0b77" translate="yes" xml:space="preserve">
          <source>We are pleased to announce the release of scikit-learn 0.23! Many bug fixes and improvements were added, as well as some new key features. We detail below a few of the major features of this release. &lt;strong&gt;For an exhaustive list of all the changes&lt;/strong&gt;, please refer to the &lt;a href=&quot;https://scikit-learn.org/0.23/whats_new/v0.23.html#changes-0-23&quot;&gt;release notes&lt;/a&gt;.</source>
          <target state="translated">scikit-learn 0.23의 출시를 발표하게되어 기쁘게 생각합니다! 많은 버그 수정 및 개선 사항과 몇 가지 새로운 주요 기능이 추가되었습니다. 이 릴리스의 몇 가지 주요 기능을 아래에서 자세히 설명합니다. &lt;strong&gt;모든 변경 사항의 전체 목록은 &lt;/strong&gt;&lt;a href=&quot;https://scikit-learn.org/0.23/whats_new/v0.23.html#changes-0-23&quot;&gt;릴리스 정보&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0f24896ee5efd7713cddd30b90821fa31665d4c0" translate="yes" xml:space="preserve">
          <source>We assume that the observations are independent and identically distributed (i.i.d.).</source>
          <target state="translated">관측치가 독립적이고 동일하게 분포되어 있다고 가정합니다 (iid).</target>
        </trans-unit>
        <trans-unit id="1552999210b3502aaea38a27415ef28fc1fbf44a" translate="yes" xml:space="preserve">
          <source>We build an artificial dataset where the target value is in general positively correlated with the first feature (with some random and non-random variations), and in general negatively correlated with the second feature.</source>
          <target state="translated">목표 값이 일반적으로 첫 번째 특성 (무작위 및 비 무작위 변형 포함)과 양의 상관 관계가 있고 일반적으로 두 번째 특성과 음의 상관 관계가있는 인공 데이터 세트를 구축합니다.</target>
        </trans-unit>
        <trans-unit id="b19a456f973e90e2b4e21cdba9f00b15173fd0ef" translate="yes" xml:space="preserve">
          <source>We call &lt;strong&gt;vectorization&lt;/strong&gt; the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the &lt;strong&gt;Bag of Words&lt;/strong&gt; or &amp;ldquo;Bag of n-grams&amp;rdquo; representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.</source>
          <target state="translated">&lt;strong&gt;벡터화&lt;/strong&gt; 를 텍스트 문서 모음을 숫자 특징 벡터로 변환하는 일반적인 프로세스 라고 합니다. 이 특정 전략 (토큰 화, 카운팅 및 정규화) &lt;strong&gt;을 단어&lt;/strong&gt; 의 &lt;strong&gt;백&lt;/strong&gt; (Bas &lt;strong&gt;of Words)&lt;/strong&gt; 또는 &quot;N- 그램의 백&quot;표현이라고합니다. 문서는 단어에서 단어의 상대 위치 정보를 완전히 무시하면서 단어 발생으로 설명됩니다.</target>
        </trans-unit>
        <trans-unit id="c42f4b562ec41f92f6f2792dcf47013aca7ffcc1" translate="yes" xml:space="preserve">
          <source>We can additionally validate these models by comparing observed and predicted total claim amount over the test and train subsets. We see that, on average, both model tend to underestimate the total claim (but this behavior depends on the amount of regularization).</source>
          <target state="translated">테스트 및 학습 하위 집합에 대해 관찰 및 예측 된 총 청구 금액을 비교하여 이러한 모델을 추가로 검증 할 수 있습니다. 평균적으로 두 모델 모두 총 청구를 과소 평가하는 경향이 있습니다 (그러나이 동작은 정규화 정도에 따라 다릅니다).</target>
        </trans-unit>
        <trans-unit id="63503e35b14a81c8aca7c86c612044cbc5b560d3" translate="yes" xml:space="preserve">
          <source>We can also export the tree in &lt;a href=&quot;https://www.graphviz.org/&quot;&gt;Graphviz&lt;/a&gt; format using the &lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt;&lt;code&gt;export_graphviz&lt;/code&gt;&lt;/a&gt; exporter. If you use the &lt;a href=&quot;https://conda.io&quot;&gt;conda&lt;/a&gt; package manager, the graphviz binaries and the python package can be installed with &lt;code&gt;conda install python-graphviz&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt; &lt;code&gt;export_graphviz&lt;/code&gt; &lt;/a&gt; 내보내기를 사용하여 &lt;a href=&quot;https://www.graphviz.org/&quot;&gt;Graphviz&lt;/a&gt; 형식으로 트리를 내보낼 수도 있습니다 . &lt;a href=&quot;https://conda.io&quot;&gt;conda&lt;/a&gt; 패키지 관리자 를 사용하는 경우 &lt;code&gt;conda install python-graphviz&lt;/code&gt; 로 graphviz 바이너리 및 python 패키지를 설치할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="096029ae48dbb08bf7baa8066a510ed0e5cf55ab" translate="yes" xml:space="preserve">
          <source>We can also predict based on an unfitted model by using the GP prior. In addition to the mean of the predictive distribution, also its standard deviation (return_std=True) or covariance (return_cov=True). Note that at most one of the two can be requested.</source>
          <target state="translated">GP를 사용하여 적합하지 않은 모델을 기반으로 예측할 수도 있습니다. 예측 분포의 평균 외에도 표준 편차 (return_std = True) 또는 공분산 (return_cov = True)입니다. 둘 중 하나만 요청할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c4674ccfad638384c25bbb5d0cf26d42218171fa" translate="yes" xml:space="preserve">
          <source>We can check the coefficient variability through cross-validation: it is a form of data perturbation (related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Resampling_(statistics)&quot;&gt;resampling&lt;/a&gt;).</source>
          <target state="translated">교차 검증을 통해 계수 변동성을 확인할 수 있습니다. 이것은 데이터 섭동의 한 형태입니다 ( &lt;a href=&quot;https://en.wikipedia.org/wiki/Resampling_(statistics)&quot;&gt;리샘플링&lt;/a&gt; 관련 ).</target>
        </trans-unit>
        <trans-unit id="7f60df7f123136fa11f4ddf222fab31a6a72d17d" translate="yes" xml:space="preserve">
          <source>We can choose &lt;code&gt;alpha&lt;/code&gt; to minimize left out error, this time using the diabetes dataset rather than our synthetic data:</source>
          <target state="translated">이번에는 합성 데이터 대신 당뇨병 데이터 세트를 사용하여 오류를 최소화하기 위해 &lt;code&gt;alpha&lt;/code&gt; 를 선택할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a7e4da902c37585f80154c204f27a006746ba8cb" translate="yes" xml:space="preserve">
          <source>We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the average occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data.</source>
          <target state="translated">주택 가격 중앙값은 소득 중앙값 (왼쪽 상단)과 선형 관계를 보이고 있으며, 가구당 평균 거주자가 증가하면 주택 가격이 하락하는 것을 알 수 있습니다 (상단 중앙). 오른쪽 상단 플롯은 한 구역의 주택 연령이 (중앙값) 주택 가격에 큰 영향을 미치지 않음을 보여줍니다. 가구당 평균 방도 마찬가지입니다. x 축의 눈금은 훈련 데이터의 특성 값의 십진수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="469d4e6eabf44c56eec1620cbb6087744c30d3f6" translate="yes" xml:space="preserve">
          <source>We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data.</source>
          <target state="translated">우리는 평균 주택 가격이 평균 소득 (왼쪽 상단)과 선형 관계를 나타내고 평균 가격이 하락할 때 주택 가격이 하락 함을 분명히 알 수 있습니다. 가구당 거주자 증가 (중간 상단). 오른쪽 맨 위 줄거리는 지구의 주택 연령이 (중간) 주택 가격에 큰 영향을 미치지 않음을 보여줍니다. 가구당 평균 방도 마찬가지입니다. x 축의 눈금은 교육 데이터에있는 특성 값의 십진수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="c32b4a200d58f731852c3f4a8eefb32ef18f31ed" translate="yes" xml:space="preserve">
          <source>We can keep the remaining rating columns by setting &lt;code&gt;remainder='passthrough'&lt;/code&gt;. The values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder='passthrough'&lt;/code&gt; 설정하여 나머지 등급 열을 유지할 수 있습니다 . 변환의 끝에 값이 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="a59ff6e4288992e31a9513b51da5a036927e8ec8" translate="yes" xml:space="preserve">
          <source>We can now load the list of files matching those categories as follows:</source>
          <target state="translated">이제 다음과 같이 해당 범주와 일치하는 파일 목록을로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f1f864cfbdff004081b5667459fd9c49d8fcf703" translate="yes" xml:space="preserve">
          <source>We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:</source>
          <target state="translated">이제 분류기를 테스트 (평가) 할 데이터의 40 %를 유지하면서 학습 세트를 신속하게 샘플링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6d11b97d462683ffa27a678189fc90fddb4a8ee5" translate="yes" xml:space="preserve">
          <source>We can observe that the &lt;code&gt;embarked&lt;/code&gt; and &lt;code&gt;sex&lt;/code&gt; columns were tagged as &lt;code&gt;category&lt;/code&gt; columns when loading the data with &lt;code&gt;fetch_openml&lt;/code&gt;. Therefore, we can use this information to dispatch the categorical columns to the &lt;code&gt;categorical_transformer&lt;/code&gt; and the remaining columns to the &lt;code&gt;numerical_transformer&lt;/code&gt;.</source>
          <target state="translated">우리는 관찰 할 수 있다는 &lt;code&gt;embarked&lt;/code&gt; 및 &lt;code&gt;sex&lt;/code&gt; 열이로 태그 된 &lt;code&gt;category&lt;/code&gt; 로 데이터를로드 할 때 열 &lt;code&gt;fetch_openml&lt;/code&gt; 을 . 따라서, 우리는에 범주 열을 전달하기 위해이 정보를 사용할 수 있습니다 &lt;code&gt;categorical_transformer&lt;/code&gt; 받는 나머지 열을 &lt;code&gt;numerical_transformer&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="850dcea5aea59348ecbdc161cc86fe56ad9b64b5" translate="yes" xml:space="preserve">
          <source>We can reduce the dimension even more, to a chosen \(L\), by projecting onto the linear subspace \(H_L\) which maximizes the variance of the \(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the transformed class means \(\mu^*_k\)). This \(L\) corresponds to the &lt;code&gt;n_components&lt;/code&gt; parameter used in the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt; method. See &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">투영 후 \ (\ mu ^ * _ k \)의 분산을 최대화하는 선형 부분 공간 \ (H_L \)에 투영하여 선택한 \ (L \)로 치수를 훨씬 더 줄일 수 있습니다. 변환 된 클래스 수단 \ (\ mu ^ * _ k \))에 대해 PCA 양식을 수행하고 있습니다. 이 \ (L \) 는 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; &lt;/a&gt; 메소드에 사용 된 &lt;code&gt;n_components&lt;/code&gt; 매개 변수에 해당합니다 . 자세한 내용은 &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="92db19023b735fef317e25e1a918af44df264854" translate="yes" xml:space="preserve">
          <source>We can reduce the dimension even more, to a chosen \(L\), by projecting onto the linear subspace \(H_L\) which maximizes the variance of the \(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the transformed class means \(\mu^*_k\)). This \(L\) corresponds to the &lt;code&gt;n_components&lt;/code&gt; parameter used in the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt; method. See &lt;a href=&quot;#id5&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt; for more details.</source>
          <target state="translated">투영 후 \ (\ mu ^ * _ k \)의 분산을 최대화하는 선형 부분 공간 \ (H_L \)에 투영하여 선택한 \ (L \)로 차원을 더 줄일 수 있습니다 (실제로는 변환 된 클래스에 대해 PCA 형식을 수행하는 것은 \ (\ mu ^ * _ k \))를 의미합니다. 이 \ (L \) 은 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt; 메소드 에서 사용되는 &lt;code&gt;n_components&lt;/code&gt; 매개 변수에 해당합니다 . 자세한 내용은 &lt;a href=&quot;#id5&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="32e64fece84cec61516540c9bc9ea581152251c6" translate="yes" xml:space="preserve">
          <source>We can see that &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; preserves the class ratios (approximately 1 / 10) in both train and test dataset.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; 가&lt;/a&gt; 훈련 및 테스트 데이터 세트 모두에서 클래스 비율 (약 1/10)을 유지 한다는 것을 알 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bee45302ee9842b9e6d2e72404b6369d8b7e97cc" translate="yes" xml:space="preserve">
          <source>We can see that for low values of &lt;code&gt;n_components&lt;/code&gt; the distribution is wide with many distorted pairs and a skewed distribution (due to the hard limit of zero ratio on the left as distances are always positives) while for larger values of n_components the distortion is controlled and the distances are well preserved by the random projection.</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; 의 낮은 값에 대해서는 분포가 왜곡 된 많은 쌍과 비대칭 분포 (거리가 항상 양수이므로 왼쪽의 0 비율의 하드 한계로 인해)가 넓고 n_components의 값이 클수록 왜곡이 제어되고 분포가 넓다는 것을 알 수 있습니다. 거리는 랜덤 투영에 의해 잘 보존됩니다.</target>
        </trans-unit>
        <trans-unit id="d51b2a4e76fb7eb99807202c5234812c15585e4c" translate="yes" xml:space="preserve">
          <source>We can see that if the maximum depth of the tree (controlled by the &lt;code&gt;max_depth&lt;/code&gt; parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.</source>
          <target state="translated">트리의 최대 깊이 ( &lt;code&gt;max_depth&lt;/code&gt; 매개 변수로 제어 )가 너무 높게 설정되어 있으면 의사 결정 트리가 훈련 데이터에 대해 너무 세밀한 정보를 배우고 소음으로부터 배우는 것, 즉 과적 합을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5bc27bfd8c709ab5505734b2124db3dbd09e7af4" translate="yes" xml:space="preserve">
          <source>We can see that, although feature 2 has a strong coefficient on the full model, it conveys little information on &lt;code&gt;y&lt;/code&gt; when considered with feature 1.</source>
          <target state="translated">피처 2는 전체 모델에 대해 강한 계수를 갖지만 피처 1로 간주 될 때 &lt;code&gt;y&lt;/code&gt; 에 대한 정보를 거의 전달하지 않음 을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="208e5f1f40787dd5dcd62a253128bd90d5a3414b" translate="yes" xml:space="preserve">
          <source>We can turn those concept as scores &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt;. Both are bounded below by 0.0 and above by 1.0 (higher is better):</source>
          <target state="translated">이러한 개념을 &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; 점수로 전환 할 수 있습니다 . 둘 다 0.0과 1.0 이상으로 제한됩니다 (높을수록 좋음).</target>
        </trans-unit>
        <trans-unit id="592289b1a6fdac7a6256bbd4a62b88701d16e505" translate="yes" xml:space="preserve">
          <source>We can use the function &lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt;&lt;code&gt;learning_curve&lt;/code&gt;&lt;/a&gt; to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt; &lt;code&gt;learning_curve&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 이러한 학습 곡선 (사용 된 샘플 수, 훈련 세트의 평균 점수 및 유효성 검사 세트의 평균 점수)을 표시하는 데 필요한 값을 생성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="36de20f1638d2356805015f502415f93be57efc5" translate="yes" xml:space="preserve">
          <source>We can visually compare observed and predicted values, aggregated by the drivers age (&lt;code&gt;DrivAge&lt;/code&gt;), vehicle age (&lt;code&gt;VehAge&lt;/code&gt;) and the insurance bonus/malus (&lt;code&gt;BonusMalus&lt;/code&gt;).</source>
          <target state="translated">우리는 육안으로 관찰 및 드라이버 연령 (집계 값을 예측 비교할 수 &lt;code&gt;DrivAge&lt;/code&gt; ), 자동차 세 ( &lt;code&gt;VehAge&lt;/code&gt; ) 및 보험 보너스 / 아 ( &lt;code&gt;BonusMalus&lt;/code&gt; 을 ).</target>
        </trans-unit>
        <trans-unit id="b0507ecbdf9b58dd0986ef6194b95f4c79d58f52" translate="yes" xml:space="preserve">
          <source>We can visually compare observed and predicted values, aggregated for the drivers age (&lt;code&gt;DrivAge&lt;/code&gt;).</source>
          <target state="translated">운전자 연령 ( &lt;code&gt;DrivAge&lt;/code&gt; )에 대해 집계 된 관측 값과 예측값을 시각적으로 비교할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="3c55a99ecafce9d11edefa5f7981438b525c546b" translate="yes" xml:space="preserve">
          <source>We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C.</source>
          <target state="translated">우리는 숫자의 8x8 이미지를 두 가지 클래스로 분류합니다 : 0-4 대 5-9. 시각화는 다양한 C에 대한 모델의 계수를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="523600239bb64c3dc717f02d73255329247ddc08" translate="yes" xml:space="preserve">
          <source>We configured a pipeline to scale the numerical input features and tuned the neural network size and learning rate to get a reasonable compromise between training time and predictive performance on a test set.</source>
          <target state="translated">수치 입력 기능을 확장하기위한 파이프 라인을 구성하고 테스트 세트에서 훈련 시간과 예측 성능간에 합리적인 절충안을 얻기 위해 신경망 크기와 학습률을 조정했습니다.</target>
        </trans-unit>
        <trans-unit id="4c2ceff57e3b51d317fe78664f4ea8312ed35966" translate="yes" xml:space="preserve">
          <source>We consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the target depends on them as follows:</source>
          <target state="translated">[0, 1]에 균일하게 분포 된 3 가지 기능 x_1, x_2, x_3을 고려합니다. 대상은 다음과 같이 기능에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="f5f452641b4c7cde04f8f2f3146b6f807ae80e50" translate="yes" xml:space="preserve">
          <source>We construct the freMTPL2 dataset by joining the freMTPL2freq table, containing the number of claims (&lt;code&gt;ClaimNb&lt;/code&gt;), with the freMTPL2sev table, containing the claim amount (&lt;code&gt;ClaimAmount&lt;/code&gt;) for the same policy ids (&lt;code&gt;IDpol&lt;/code&gt;).</source>
          <target state="translated">클레임 수 ( &lt;code&gt;ClaimNb&lt;/code&gt; )가 포함 된 freMTPL2freq 테이블과 동일한 정책 ID ( &lt;code&gt;IDpol&lt;/code&gt; )에 대한 클레임 금액 ( &lt;code&gt;ClaimAmount&lt;/code&gt; )이 포함 된 freMTPL2sev 테이블을 결합하여 freMTPL2 데이터 세트를 구성합니다 .</target>
        </trans-unit>
        <trans-unit id="8b738423194ebf355d81c34c5848e446f6f25c86" translate="yes" xml:space="preserve">
          <source>We create a multi-label dataset, to illustrate the precision-recall in multi-label settings</source>
          <target state="translated">우리는 다중 레이블 설정에서 정밀도 기억을 설명하기 위해 다중 레이블 데이터 세트를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="2b472a5c2bccac31b45ff227b2c9930cbabaf64f" translate="yes" xml:space="preserve">
          <source>We create the preprocessing pipelines for both numeric and categorical data.</source>
          <target state="translated">숫자 및 범주 데이터 모두에 대한 전처리 파이프 라인을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="6395abe83d6b6a4aa4bc2a531d4a8a4bd65d8620" translate="yes" xml:space="preserve">
          <source>We describe here the mathematical details of the SGD procedure. A good overview with convergence rates can be found in &lt;a href=&quot;#id16&quot; id=&quot;id4&quot;&gt;12&lt;/a&gt;.</source>
          <target state="translated">여기에서는 SGD 절차의 수학적 세부 사항을 설명합니다. 수렴 률에 대한 좋은 개요는 &lt;a href=&quot;#id16&quot; id=&quot;id4&quot;&gt;12&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5f9ff80fbe7d3d20607821a1978395dad8f7243a" translate="yes" xml:space="preserve">
          <source>We describe these 3 scenarios in the following subsections.</source>
          <target state="translated">다음 하위 섹션에서 이러한 3 가지 시나리오를 설명합니다.</target>
        </trans-unit>
        <trans-unit id="7df3a30efca0a13e3a362147c9be1fa02a3e02fd" translate="yes" xml:space="preserve">
          <source>We don&amp;rsquo;t allow:</source>
          <target state="translated">우리는 허용하지 않습니다 :</target>
        </trans-unit>
        <trans-unit id="484dbca389e5beb12ba95d531ed59ac647119de0" translate="yes" xml:space="preserve">
          <source>We fetch the data from &lt;a href=&quot;http://openml.org/&quot;&gt;OpenML&lt;/a&gt;. Note that setting the parameter &lt;code&gt;as_frame&lt;/code&gt; to True will retrieve the data as a pandas dataframe.</source>
          <target state="translated">&lt;a href=&quot;http://openml.org/&quot;&gt;OpenML&lt;/a&gt; 에서 데이터를 가져옵니다 . 매개 변수 &lt;code&gt;as_frame&lt;/code&gt; 을 True로 설정하면 데이터가 pandas 데이터 프레임으로 검색됩니다.</target>
        </trans-unit>
        <trans-unit id="43167795aabd351e0317ac3702c4dd940441044a" translate="yes" xml:space="preserve">
          <source>We filter out &lt;code&gt;ClaimAmount == 0&lt;/code&gt; as the Gamma distribution has support on \((0, \infty)\), not \([0, \infty)\).</source>
          <target state="translated">감마 분포가 \ ([0, \ infty) \)가 아니라 \ ((0, \ infty) \)를 지원하므로 &lt;code&gt;ClaimAmount == 0&lt;/code&gt; 을 필터링합니다 .</target>
        </trans-unit>
        <trans-unit id="c17cba12ad6e5c2f931d1de0fcadd712e7bb0a0a" translate="yes" xml:space="preserve">
          <source>We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes.</source>
          <target state="translated">먼저 평범한 SVC로 분리 평면을 찾은 다음 불균형 클래스를 자동으로 수정하여 분리 초평면을 플로팅 (대시)합니다.</target>
        </trans-unit>
        <trans-unit id="1b6887ebea04d51a67698037588b0795f4c76e3a" translate="yes" xml:space="preserve">
          <source>We first present GBRT for regression, and then detail the classification case.</source>
          <target state="translated">먼저 회귀를위한 GBRT를 제시 한 다음 분류 사례를 자세히 설명합니다.</target>
        </trans-unit>
        <trans-unit id="916afde7457af1caed530318ff87e1a7a139918e" translate="yes" xml:space="preserve">
          <source>We found that &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; gives comparable results to &lt;code&gt;max_depth=k-1&lt;/code&gt; but is significantly faster to train at the expense of a slightly higher training error. The parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt; corresponds to the variable &lt;code&gt;J&lt;/code&gt; in the chapter on gradient boosting in &lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]&lt;/a&gt; and is related to the parameter &lt;code&gt;interaction.depth&lt;/code&gt; in R&amp;rsquo;s gbm package where &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</source>
          <target state="translated">우리는 그 발견 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 비교 결과를 제공 &lt;code&gt;max_depth=k-1&lt;/code&gt; 하지만, 약간 높은 훈련 오류의 비용으로 훨씬 빠르게 양성하는 것입니다. 파라미터 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 의 변수에 대응하는 &lt;code&gt;J&lt;/code&gt; 의 증대 구배의 장 &lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]&lt;/a&gt; 과는 파라미터 관련된 &lt;code&gt;interaction.depth&lt;/code&gt; R의 GBM 패키지 여기서 &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9a42cbe6e58382c593ff38ee7701bfc70bcd66e4" translate="yes" xml:space="preserve">
          <source>We found that &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; gives comparable results to &lt;code&gt;max_depth=k-1&lt;/code&gt; but is significantly faster to train at the expense of a slightly higher training error. The parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt; corresponds to the variable &lt;code&gt;J&lt;/code&gt; in the chapter on gradient boosting in &lt;a href=&quot;model_evaluation#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; and is related to the parameter &lt;code&gt;interaction.depth&lt;/code&gt; in R&amp;rsquo;s gbm package where &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</source>
          <target state="translated">우리는 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 가 &lt;code&gt;max_depth=k-1&lt;/code&gt; 과 비슷한 결과를 제공 하지만 약간 더 높은 훈련 오류를 희생시키면서 훈련하는 것이 훨씬 빠르다 는 것을 발견했습니다 . &lt;code&gt;max_leaf_nodes&lt;/code&gt; 매개 변수 는 &lt;a href=&quot;model_evaluation#f2001&quot; id=&quot;id15&quot;&gt;[F2001]의&lt;/a&gt; 그래디언트 부스팅 장에 있는 변수 &lt;code&gt;J&lt;/code&gt; 에 해당 하며 &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; R의 gbm 패키지에있는 &lt;code&gt;interaction.depth&lt;/code&gt; 매개 변수와 관련이 있습니다.</target>
        </trans-unit>
        <trans-unit id="72f7189dc223c470430f67b7332d9ced78363339" translate="yes" xml:space="preserve">
          <source>We found that Averaged SGD works best with a larger number of features and a higher eta0</source>
          <target state="translated">평균 SGD가 더 많은 수의 기능과 더 높은 eta0에서 가장 잘 작동한다는 것을 알았습니다.</target>
        </trans-unit>
        <trans-unit id="2147828a070acc191399f3a8049f6a914fdacd22" translate="yes" xml:space="preserve">
          <source>We further include two random variables that are not correlated in any way with the target variable (&lt;code&gt;survived&lt;/code&gt;):</source>
          <target state="translated">대상 변수와 어떤 방식 으로든 상관되지 않는 두 개의 랜덤 변수를 추가로 포함합니다 ( &lt;code&gt;survived&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="ecc641e4f0c3be544f8ba1b6978d12b64c778e22" translate="yes" xml:space="preserve">
          <source>We generate data from three groups of waveforms. Two of the waveforms (waveform 1 and waveform 2) are proportional one to the other. The cosine distance is invariant to a scaling of the data, as a result, it cannot distinguish these two waveforms. Thus even with no noise, clustering using this distance will not separate out waveform 1 and 2.</source>
          <target state="translated">세 그룹의 파형에서 데이터를 생성합니다. 두 파형 (파형 1 및 파형 2)은 서로 비례합니다. 코사인 거리는 데이터의 스케일링에 불변하므로 결과적으로이 두 파형을 구별 할 수 없습니다. 따라서 노이즈가 없어도이 거리를 사용하는 클러스터링은 파형 1과 2를 분리하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="03835cacd7901d07f747c14f209b80543758c4fd" translate="yes" xml:space="preserve">
          <source>We have seen that some estimators can transform data and that some estimators can predict variables. We can also create combined estimators:</source>
          <target state="translated">우리는 일부 추정자가 데이터를 변환 할 수 있고 일부 추정기는 변수를 예측할 수 있음을 확인했습니다. 결합 된 추정량을 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="996013b3c282443801da622c65fee1deca3cef01" translate="yes" xml:space="preserve">
          <source>We have seen that sparsity could be used to mitigate the curse of dimensionality, &lt;em&gt;i.e&lt;/em&gt; an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: &lt;strong&gt;feature agglomeration&lt;/strong&gt;. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data.</source>
          <target state="translated">우리는 희소성이 차원의 저주, &lt;em&gt;즉&lt;/em&gt; 특징의 수에 비해 불충분 한 관측치 의 저주를 완화하는 데 사용될 수 있음을 보았습니다 . 또 다른 접근법은 비슷한 기능을 통합하는 것입니다 : &lt;strong&gt;기능 응집&lt;/strong&gt; . 이 접근법은 특징 방향으로 클러스터링, 즉, 전치 된 데이터를 클러스터링함으로써 구현 될 수있다.</target>
        </trans-unit>
        <trans-unit id="f70b46435230cf70b0b3d749ceef620b9bcdee9d" translate="yes" xml:space="preserve">
          <source>We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt;. This technique has been omitted because it is advantageous only when decomposing a matrix with &lt;code&gt;n_samples&lt;/code&gt; (rows) &amp;gt;= 5/3 * &lt;code&gt;n_features&lt;/code&gt; (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary.</source>
          <target state="translated">SVD의 알고리즘 복잡성을 줄이기 위해 특정 상황에서 사용되는 QR 분해는 두 논문의 저자가 사용하는 최적화를 구체적으로 생략했습니다. 이 기술의 출처는 &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt; . 이 기법은 &lt;code&gt;n_samples&lt;/code&gt; (행)&amp;gt; = 5/3 * &lt;code&gt;n_features&lt;/code&gt; (열) 로 행렬을 분해 할 때만 유리 하고 구현 된 알고리즘의 가독성을 손상시키기 때문에 생략되었습니다 . 필요할 경우 향후 최적화를위한 좋은 기회가 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="ac54e4e32d1eea16ad8753f857ea27f0962cb421" translate="yes" xml:space="preserve">
          <source>We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is &lt;em&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/em&gt;. This technique has been omitted because it is advantageous only when decomposing a matrix with &lt;code&gt;n_samples&lt;/code&gt; (rows) &amp;gt;= 5/3 * &lt;code&gt;n_features&lt;/code&gt; (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary.</source>
          <target state="translated">우리는 SVD의 알고리즘 복잡성을 줄이기 위해 특정 상황에서 사용되는 QR 분해 인 두 논문의 저자가 사용하는 최적화를 특별히 기권했습니다. 이 기술의 출처는 &lt;em&gt;Matrix Computations, Third Edition, G. Holub 및 C. Van Loan, 5 장, 섹션 5.4.4, pp 252-253입니다.&lt;/em&gt; . 이 기법은 &lt;code&gt;n_samples&lt;/code&gt; (rows)&amp;gt; = 5/3 * &lt;code&gt;n_features&lt;/code&gt; (column) 로 행렬을 분해 할 때만 유리 하고 구현 된 알고리즘의 가독성을 떨어 뜨리기 때문에 생략되었습니다 . 필요하다고 판단되는 경우 향후 최적화를위한 좋은 기회가 될 것입니다. ㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="db298eced453f06f8efed43be73a03415c98ca01" translate="yes" xml:space="preserve">
          <source>We have to reconstruct model and parameters to make sure we stay in sync with the python object.</source>
          <target state="translated">파이썬 객체와 동기화되도록 모델과 파라미터를 재구성해야합니다.</target>
        </trans-unit>
        <trans-unit id="ca5f08fcacf0548e5e0c637b82c4414e44704050" translate="yes" xml:space="preserve">
          <source>We introduce a new parameter \(\nu\) (instead of \(C\)) which controls the number of support vectors and &lt;em&gt;margin errors&lt;/em&gt;: \(\nu \in (0, 1]\) is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin.</source>
          <target state="translated">지원 벡터 및 &lt;em&gt;여백 오류&lt;/em&gt; 수를 제어하는 ​​새 매개 변수 \ (\ nu \) (\ (C \) 대신)를 도입했습니다 . \ (\ nu \ in (0, 1] \)은 여백 오류의 비율 및지지 벡터 비율의 하한. 여백 오류는 여백 경계의 잘못된쪽에있는 샘플에 해당합니다. 잘못 분류되었거나 올바르게 분류되었지만 여백을 벗어나지 않습니다. .</target>
        </trans-unit>
        <trans-unit id="f2683785e1f3e3ccb40d3941fed7d5fa67a33cfc" translate="yes" xml:space="preserve">
          <source>We introduce a new parameter \(\nu\) which controls the number of support vectors and training errors. The parameter \(\nu \in (0, 1]\) is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.</source>
          <target state="translated">지원 벡터 수와 학습 오류를 제어하는 ​​새로운 매개 변수 \ (\ nu \)를 소개합니다. 매개 변수 \ (\ nu \ in (0, 1] \)는 훈련 오류의 부분에 대한 상한과 지원 벡터의 부분에 대한 하한입니다.</target>
        </trans-unit>
        <trans-unit id="60ccb05e0b7446f76670a424f0fabb01d1ca71b3" translate="yes" xml:space="preserve">
          <source>We need a vectorized version of the image. &lt;code&gt;'rescaled_coins'&lt;/code&gt; is a down-scaled version of the coins image to speed up the process:</source>
          <target state="translated">벡터화 된 이미지 버전이 필요합니다. &lt;code&gt;'rescaled_coins'&lt;/code&gt; 는 프로세스 속도를 높이기 위해 동전 이미지를 축소 한 버전입니다.</target>
        </trans-unit>
        <trans-unit id="284f00654c2880797bacd24e85e3bc2f5ec8be8d" translate="yes" xml:space="preserve">
          <source>We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other.</source>
          <target state="translated">우리는 더 이상 충돌을 일으키지 않지만, 이는 출력 공간의 훨씬 더 큰 차원 성을 희생시킵니다. 물론 여기서 사용 된 19 이외의 용어는 여전히 서로 충돌 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a19ee5584b95e367f6c768452e1c513e3bb433ea" translate="yes" xml:space="preserve">
          <source>We now inspect the coefficients across several cross-validation folds.</source>
          <target state="translated">이제 여러 교차 검증 접기에 대한 계수를 검사합니다.</target>
        </trans-unit>
        <trans-unit id="9d37726e46e38fa3806c6050ccef5f0b5ccde0ad" translate="yes" xml:space="preserve">
          <source>We now provide a &lt;code&gt;pytest&lt;/code&gt; specific decorator which allows &lt;code&gt;pytest&lt;/code&gt; to run all checks independently and report the checks that are failing.</source>
          <target state="translated">이제 &lt;code&gt;pytest&lt;/code&gt; 가 모든 검사를 독립적으로 실행하고 실패한 검사를보고 할 수 있는 &lt;code&gt;pytest&lt;/code&gt; 전용 데코레이터를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="b22fff2428986dd4d9e25659e5099f6fa799f31a" translate="yes" xml:space="preserve">
          <source>We now support imputation for completing missing values using k-Nearest Neighbors.</source>
          <target state="translated">이제 k-Nearest Neighbors를 사용하여 결 측값을 완성하기위한 대치를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="3b146f434972f2182b845909391decf90277ad41" translate="yes" xml:space="preserve">
          <source>We observe a tendency towards clearer shapes as the perplexity value increases.</source>
          <target state="translated">우리는 난해함 값이 증가함에 따라 더 선명한 모양을 향한 경향을 관찰합니다.</target>
        </trans-unit>
        <trans-unit id="0e7a6d4d2e128e719f76bf1799c4515162612bfb" translate="yes" xml:space="preserve">
          <source>We observe a tendency towards clearer shapes as the preplexity value increases.</source>
          <target state="translated">우리는 preplexity 값이 증가함에 따라 더 명확한 모양으로 향하는 경향을 관찰합니다.</target>
        </trans-unit>
        <trans-unit id="3705c31d0fb66694929007b0cdad834fbbaf06c2" translate="yes" xml:space="preserve">
          <source>We plot partial dependence curves for features &amp;ldquo;age&amp;rdquo; and &amp;ldquo;bmi&amp;rdquo; (body mass index) for the decision tree. With two features, &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; expects to plot two curves. Here the plot function place a grid of two plots using the space defined by &lt;code&gt;ax&lt;/code&gt; .</source>
          <target state="translated">의사 결정 트리에 대한 기능 &quot;연령&quot;및 &quot;bmi&quot;(체질량 지수)에 대한 부분 의존성 곡선을 플로팅합니다. 두 가지 기능이있는 &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; 는 두 개의 곡선을 그릴 것으로 예상합니다. 여기서 plot 함수는 &lt;code&gt;ax&lt;/code&gt; 로 정의 된 공간을 사용하여 두 플롯의 그리드를 배치합니다 .</target>
        </trans-unit>
        <trans-unit id="b4e7d2188aa62b346706f6bd2a1ae94095756883" translate="yes" xml:space="preserve">
          <source>We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.</source>
          <target state="translated">우리는 훈련에 예측 된 레이블을 표시하고 홍채 데이터 세트에서 다양한 GMM 공분산 유형을 사용하여 테스트 데이터를 실시했습니다. GMM을 구형, 대각선, 전체 및 묶인 공분산 행렬과 비교하여 성능 순서를 향상시킵니다. 전체 공분산이 일반적으로 최고 성능을 기대할 수 있지만 소규모 데이터 세트에 과적 합하기 쉽고 테스트 데이터를 잘 나타내기에는 일반화되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7ead9de71a0f24e08ddaae8aaa6dd6470443d3a2" translate="yes" xml:space="preserve">
          <source>We recommend &lt;a href=&quot;#id15&quot; id=&quot;id6&quot;&gt;13&lt;/a&gt; and &lt;a href=&quot;#id16&quot; id=&quot;id7&quot;&gt;14&lt;/a&gt; as good references for the theory and practicalities of SVMs.</source>
          <target state="translated">SVM의 이론과 실용성에 대한 좋은 참고 자료로 &lt;a href=&quot;#id15&quot; id=&quot;id6&quot;&gt;13&lt;/a&gt; 과 &lt;a href=&quot;#id16&quot; id=&quot;id7&quot;&gt;14&lt;/a&gt; 를 권장 합니다.</target>
        </trans-unit>
        <trans-unit id="8a444360e57dc0870f14b3c959a7b626922e7571" translate="yes" xml:space="preserve">
          <source>We see that &lt;code&gt;SVC&lt;/code&gt; doesn&amp;rsquo;t do much better than a dummy classifier. Now, let&amp;rsquo;s change the kernel:</source>
          <target state="translated">우리는 &lt;code&gt;SVC&lt;/code&gt; 가 더미 분류기보다 낫지 않다는 것을 알았습니다. 이제 커널을 바꾸자 :</target>
        </trans-unit>
        <trans-unit id="9619f66671fbeb88bbee2c1b3d850c22bdb6ab25" translate="yes" xml:space="preserve">
          <source>We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt; section for details.</source>
          <target state="translated">정확도가 거의 100 %로 향상되었습니다. CPU가 너무 비싸지 않은 경우 정확도를 더 잘 추정하려면 교차 검증 전략이 권장됩니다. 자세한 정보는 &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;교차 유효성 검증 : 추정기 성능 평가&lt;/a&gt; 섹션을 참조하십시오. 또한 매개 변수 공간을 최적화하려면 적절한 방법을 사용하는 것이 좋습니다. 자세한 내용은 &lt;a href=&quot;grid_search#grid-search&quot;&gt;추정기의 하이퍼 파라미터 튜닝&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b05b900d7824680be8b5fe4195a1e87507494a4c" translate="yes" xml:space="preserve">
          <source>We see that the resulting &lt;em&gt;polynomial regression&lt;/em&gt; is in the same class of linear models we considered above (i.e. the model is linear in \(w\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.</source>
          <target state="translated">결과 &lt;em&gt;다항식 회귀&lt;/em&gt; 는 위에서 고려한 동일한 클래스의 선형 모델 (즉, 모델이 \ (w \)에서 선형 임)에 있으며 동일한 기술로 해결할 수 있음을 알 수 있습니다. 이러한 기본 함수로 구축 된 고차원 공간 내에서 선형 적합을 고려함으로써 모델은 훨씬 더 광범위한 데이터에 적합 할 수있는 유연성을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="d680bb4e5101fe3a9df93911d26cfa982767e679" translate="yes" xml:space="preserve">
          <source>We see that the resulting &lt;em&gt;polynomial regression&lt;/em&gt; is in the same class of linear models we&amp;rsquo;d considered above (i.e. the model is linear in \(w\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.</source>
          <target state="translated">결과 &lt;em&gt;다항식 회귀 분석&lt;/em&gt; 은 위에서 고려한 것과 같은 클래스의 선형 모델에 있으며 (즉, 모델은 \ (w \)에서 선형 임) 동일한 기술로 해결할 수 있습니다. 이러한 기본 기능으로 구축 된 고차원 공간 내에서 선형 적합을 고려함으로써 모델은 훨씬 더 넓은 범위의 데이터에 적합하게 유연성을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="1a0e7af8231b7a460dcfd9acf44c6323ad1ea844" translate="yes" xml:space="preserve">
          <source>We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.</source>
          <target state="translated">몇 가지 이상치 탐지 도구를 사용하여 어떤 종류의 분석을 수행 할 수 있는지 설명하기 위해 Boston Housing 데이터 세트에서 두 가지 변수의 두 세트를 선택했습니다. 시각화의 목적으로, 우리는 2 차원 예제로 작업하고 있지만, 지적 된 바와 같이, 사물이 고차원에서는 그리 사소한 것이 아님을 알아야합니다.</target>
        </trans-unit>
        <trans-unit id="940e7a9289cb117779f9b0e089fc6f41ec456057" translate="yes" xml:space="preserve">
          <source>We should also note that small differences in scores results from the random splits of the cross-validation procedure. Those spurious variations can be smoothed out by increasing the number of CV iterations &lt;code&gt;n_splits&lt;/code&gt; at the expense of compute time. Increasing the value number of &lt;code&gt;C_range&lt;/code&gt; and &lt;code&gt;gamma_range&lt;/code&gt; steps will increase the resolution of the hyper-parameter heat map.</source>
          <target state="translated">또한 점수의 작은 차이는 교차 유효성 검사 절차의 무작위 분할로 인해 발생합니다. 계산 시간을 희생하면서 CV 반복 횟수 &lt;code&gt;n_splits&lt;/code&gt; 의 수를 늘려 이러한 스퓨리어스 변형을 완화 할 수 있습니다 . &lt;code&gt;C_range&lt;/code&gt; 및 &lt;code&gt;gamma_range&lt;/code&gt; 단계 의 값 수를 늘리면 하이퍼 파라미터 히트 맵의 해상도가 높아집니다.</target>
        </trans-unit>
        <trans-unit id="1cbc5d306e08afa1d68b1045fc6a567611cf26d2" translate="yes" xml:space="preserve">
          <source>We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved.</source>
          <target state="translated">linear_model.Lasso는 밀도가 높고 희소 데이터에 대해 동일한 결과를 제공하며 희소 데이터의 경우 속도가 향상됨을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="971fe5258bb92dd314d2f061a23d12526523f329" translate="yes" xml:space="preserve">
          <source>We split the sample into a train and a test dataset. Only the train dataset will be used in the following exploratory analysis. This is a way to emulate a real situation where predictions are performed on an unknown target, and we don&amp;rsquo;t want our analysis and decisions to be biased by our knowledge of the test data.</source>
          <target state="translated">샘플을 기차와 테스트 데이터 세트로 분할했습니다. 다음 탐색 분석에서는 기차 데이터 세트 만 사용됩니다. 이는 알 수없는 대상에 대해 예측이 수행되는 실제 상황을 모방하는 방법이며, 테스트 데이터에 대한 지식에 의해 분석 및 결정이 편향되는 것을 원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9729e48da1dbe46bc3dec3ab22ad1f98a7a56bb9" translate="yes" xml:space="preserve">
          <source>We start by modeling the target variable with the (l2 penalized) least squares linear regression model, more comonly known as Ridge regression. We use a low penalization &lt;code&gt;alpha&lt;/code&gt;, as we expect such a linear model to under-fit on such a large dataset.</source>
          <target state="translated">우리는 Ridge 회귀로 더 잘 알려진 (l2 패널티) 최소 제곱 선형 회귀 모델을 사용하여 대상 변수를 모델링하는 것으로 시작합니다. 이러한 선형 모델이 대규모 데이터 세트에 적합하지 않을 것으로 예상 하므로 낮은 페널티 &lt;code&gt;alpha&lt;/code&gt; 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="efbf964638e7ca0fbdc3e1bd2a5029a9cbed6b8c" translate="yes" xml:space="preserve">
          <source>We start by training a label propagation model with only 10 labeled points, then we select the top five most uncertain points to label. Next, we train with 15 labeled points (original 10 + 5 new ones). We repeat this process four times to have a model trained with 30 labeled examples. Note you can increase this to label more than 30 by changing &lt;code&gt;max_iterations&lt;/code&gt;. Labeling more than 30 can be useful to get a sense for the speed of convergence of this active learning technique.</source>
          <target state="translated">레이블이 지정된 10 개의 포인트만으로 레이블 전파 모델을 학습 한 다음 레이블을 지정할 가장 불확실한 상위 5 개 포인트를 선택합니다. 다음으로 15 개의 레이블이있는 포인트 (원래 10 + 5 개의 새로운 포인트)로 훈련합니다. 이 프로세스를 4 번 반복하여 30 개의 레이블이있는 예제로 훈련 된 모델을 만듭니다. &lt;code&gt;max_iterations&lt;/code&gt; 를 변경 하여이 값을 30 이상으로 늘리도록 늘릴 수 있습니다 . 30 개가 넘는 라벨링은이 능동 학습 기술의 수렴 속도를 이해하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b926aa4a7c89ca684b4dc714781356e70fac60ed" translate="yes" xml:space="preserve">
          <source>We thus transform the KDD Data set into two different data sets: SA and SF.</source>
          <target state="translated">따라서 KDD 데이터 세트를 SA와 SF의 두 가지 데이터 세트로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="c5856ddc1b1774be0ac4aef6a043703c4e71a274" translate="yes" xml:space="preserve">
          <source>We train a random forest classifier and create a plot comparing it to the SVC ROC curve. Notice how &lt;code&gt;svc_disp&lt;/code&gt; uses &lt;a href=&quot;../../modules/generated/sklearn.metrics.roccurvedisplay#sklearn.metrics.RocCurveDisplay.plot&quot;&gt;&lt;code&gt;plot&lt;/code&gt;&lt;/a&gt; to plot the SVC ROC curve without recomputing the values of the roc curve itself. Furthermore, we pass &lt;code&gt;alpha=0.8&lt;/code&gt; to the plot functions to adjust the alpha values of the curves.</source>
          <target state="translated">랜덤 포레스트 분류기를 훈련하고 SVC ROC 곡선과 비교하는 플롯을 만듭니다. &lt;code&gt;svc_disp&lt;/code&gt; 가 roc 곡선 자체의 값을 다시 계산하지 않고 &lt;a href=&quot;../../modules/generated/sklearn.metrics.roccurvedisplay#sklearn.metrics.RocCurveDisplay.plot&quot;&gt; &lt;code&gt;plot&lt;/code&gt; &lt;/a&gt; 을 사용 하여 SVC ROC 곡선을 그리는 방법에 주목하십시오 . 또한 곡선의 알파 값을 조정하기 위해 플롯 함수에 &lt;code&gt;alpha=0.8&lt;/code&gt; 을 전달 합니다.</target>
        </trans-unit>
        <trans-unit id="38d99d5f10e1040b13651031a8f4b3d74d91ef9a" translate="yes" xml:space="preserve">
          <source>We train and test the datasets with 15 different classification models and get performance results for each model.</source>
          <target state="translated">15 가지 분류 모델을 사용하여 데이터 세트를 훈련 및 테스트하고 각 모델에 대한 성능 결과를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="f695efb4e75bb85159c9142762a64409b0330558" translate="yes" xml:space="preserve">
          <source>We use &lt;a href=&quot;../../modules/generated/sklearn.neighbors.neighborhoodcomponentsanalysis#sklearn.neighbors.NeighborhoodComponentsAnalysis&quot;&gt;&lt;code&gt;NeighborhoodComponentsAnalysis&lt;/code&gt;&lt;/a&gt; to learn an embedding and plot the points after the transformation. We then take the embedding and find the nearest neighbors.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.neighbors.neighborhoodcomponentsanalysis#sklearn.neighbors.NeighborhoodComponentsAnalysis&quot;&gt; &lt;code&gt;NeighborhoodComponentsAnalysis&lt;/code&gt; &lt;/a&gt; 를 사용 하여 임베딩을 학습하고 변환 후 점을 플로팅합니다. 그런 다음 임베딩을 취하고 가장 가까운 이웃을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="0b8cd078987f149b454cdac8f89f7fb050617be5" translate="yes" xml:space="preserve">
          <source>We use &lt;code&gt;ClaimNb&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt; to account for policies that contain more than one claim.</source>
          <target state="translated">둘 이상의 클레임을 포함하는 정책을 설명하기 위해 &lt;code&gt;sample_weight&lt;/code&gt; 를 &lt;code&gt;ClaimNb&lt;/code&gt; 로 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="13122bf873819e99a4ec7d32c926bbee95474f9b" translate="yes" xml:space="preserve">
          <source>We use a GridSearchCV to set the dimensionality of the PCA</source>
          <target state="translated">PCA의 차원을 설정하기 위해 GridSearchCV를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="61efd119ac2fcb7bb96489a56e2c262d1e964f20" translate="yes" xml:space="preserve">
          <source>We use a biased estimator for the standard deviation, equivalent to &lt;code&gt;numpy.std(x, ddof=0)&lt;/code&gt;. Note that the choice of &lt;code&gt;ddof&lt;/code&gt; is unlikely to affect model performance.</source>
          <target state="translated">&lt;code&gt;numpy.std(x, ddof=0)&lt;/code&gt; 와 같은 표준 편차에 대한 편향 추정기를 사용합니다 . &lt;code&gt;ddof&lt;/code&gt; 의 선택은 모델 성능에 영향을주지 않습니다.</target>
        </trans-unit>
        <trans-unit id="83c28baded6fcaa48849ef740f04075945b33344" translate="yes" xml:space="preserve">
          <source>We use clustering to group together quotes that behave similarly. Here, amongst the &lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;various clustering techniques&lt;/a&gt; available in the scikit-learn, we use &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;Affinity Propagation&lt;/a&gt; as it does not enforce equal-size clusters, and it can choose automatically the number of clusters from the data.</source>
          <target state="translated">클러스터링을 사용하여 유사하게 동작하는 따옴표를 그룹화합니다. 여기서 scikit-learn에서 사용할 수 있는 &lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;다양한 클러스터링 기술&lt;/a&gt; 중에서 &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;동질&lt;/a&gt; 크기의 클러스터를 적용하지 않으므로 Affinity Propagation 을 사용 하며 데이터에서 클러스터 수를 자동으로 선택할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="78d7bc0e66fad92fa188a5bbaa0a7aaa581101c6" translate="yes" xml:space="preserve">
          <source>We use sparse inverse covariance estimation to find which quotes are correlated conditionally on the others. Specifically, sparse inverse covariance gives us a graph, that is a list of connection. For each symbol, the symbols that it is connected too are those useful to explain its fluctuations.</source>
          <target state="translated">희소 역공 분산 추정을 사용하여 어떤 따옴표가 다른 따옴표와 조건 적으로 상관되어 있는지 찾습니다. 특히 희소 역공 분산은 그래프를 보여줍니다. 이는 연결 목록입니다. 각 심볼에 대해 연결된 심볼도 변동을 설명하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="0bc034dd2aa1865c002d495e95115e5b8c709a84" translate="yes" xml:space="preserve">
          <source>We validate the above bounds on the 20 newsgroups text document (TF-IDF word frequencies) dataset or on the digits dataset:</source>
          <target state="translated">20 개의 뉴스 그룹 텍스트 문서 (TF-IDF 단어 빈도) 데이터 세트 또는 숫자 데이터 세트에서 위의 범위를 확인합니다.</target>
        </trans-unit>
        <trans-unit id="4e3f49993eefd3c100d45584ffc552355d0d52e7" translate="yes" xml:space="preserve">
          <source>We validate the above bounds on the digits dataset or on the 20 newsgroups text document (TF-IDF word frequencies) dataset:</source>
          <target state="translated">숫자 데이터 세트 또는 20 개의 뉴스 그룹 텍스트 문서 (TF-IDF 단어 빈도) 데이터 세트에서 위의 경계를 검증합니다.</target>
        </trans-unit>
        <trans-unit id="63b1da5381eb76c29e4f139b0a4a0f6fa6f76bca" translate="yes" xml:space="preserve">
          <source>We want to calculate the distance between the Ezeiza Airport (Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris, France)</source>
          <target state="translated">에세이 사 공항 (아르헨티나 부에노스 아이레스)과 샤를 드골 공항 (프랑스 파리) 사이의 거리를 계산하려고합니다.</target>
        </trans-unit>
        <trans-unit id="4bd2e73735072e414ab7c853666f3b851cc359c1" translate="yes" xml:space="preserve">
          <source>We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see &lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means&lt;/a&gt;).</source>
          <target state="translated">MiniBatchKMeans와 KMeans의 성능을 비교하려고합니다. MiniBatchKMeans는 빠르지 만 약간 다른 결과를 제공합니다 ( &lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="76b3c7c02e156a1d558e413ab95dd6c124236bdc" translate="yes" xml:space="preserve">
          <source>We will also create a transformer that extracts the length of the text and the number of sentences.</source>
          <target state="translated">또한 텍스트의 길이와 문장 수를 추출하는 변환기를 만들 것입니다.</target>
        </trans-unit>
        <trans-unit id="b8ae1ec8b6dd301757e7e59be0a9aeff645f7f18" translate="yes" xml:space="preserve">
          <source>We will cluster a set of data, first with KMeans and then with MiniBatchKMeans, and plot the results. We will also plot the points that are labelled differently between the two algorithms.</source>
          <target state="translated">먼저 KMeans와 MiniBatchKMeans를 사용하여 일련의 데이터를 클러스터링하고 결과를 플로팅합니다. 또한 두 알고리즘간에 다르게 레이블이 지정된 점을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="de730c276ad64150c605a83740871002db6683ff" translate="yes" xml:space="preserve">
          <source>We will compare the performance of both approaches. To quantify the performance of both models, one can compute the mean deviance of the train and test data assuming a Compound Poisson-Gamma distribution of the total claim amount. This is equivalent to a Tweedie distribution with a &lt;code&gt;power&lt;/code&gt; parameter between 1 and 2.</source>
          <target state="translated">두 접근 방식의 성능을 비교할 것입니다. 두 모델의 성능을 정량화하기 위해 총 청구 금액의 복합 포아송-감마 분포를 가정하여 열차 및 테스트 데이터의 평균 편차를 계산할 수 있습니다. 이는 &lt;code&gt;power&lt;/code&gt; 매개 변수가 1과 2 사이 인 Tweedie 분포와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="b4cfcf9a2f9d6095c0707be11ca996fe9017bf9d" translate="yes" xml:space="preserve">
          <source>We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.</source>
          <target state="translated">더 복잡한 개념을 배울 수있는 (예를 들어, 편향이 더 낮은) 추정기 또는 현재 추정기의 매개 변수를 사용해야 할 것입니다. 훈련 점수가 최대 훈련 샘플 수에 대한 검증 점수보다 훨씬 큰 경우, 훈련 샘플을 더 추가하면 일반화가 증가 할 가능성이 높습니다. 다음 그림에서 SVM이 더 많은 교육 예제를 통해 이점을 얻을 수 있음을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3f09acb611dde4d0822059b7dd910a6bf0d4be22" translate="yes" xml:space="preserve">
          <source>We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.</source>
          <target state="translated">여기에서는 다양한 상황에서 여러 가지 scikit-learn 추정기에서 기대할 수있는 정도의 순서를 검토하고 성능 병목 현상을 극복하기위한 몇 가지 팁과 요령을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="7c06ec0897b21dbb460bd6d56256bf3c61c9d6ea" translate="yes" xml:space="preserve">
          <source>We will train our classifier with the following features:</source>
          <target state="translated">다음과 같은 기능으로 분류기를 훈련 할 것입니다.</target>
        </trans-unit>
        <trans-unit id="3be1dd4cfac1d63b4ce361f8740d86e98582c7b5" translate="yes" xml:space="preserve">
          <source>We will use &lt;a href=&quot;http://jse.amstat.org/v19n3/decock.pdf&quot;&gt;Ames Housing&lt;/a&gt; dataset which was first compiled by Dean De Cock and became better known after it was used in Kaggle challenge. It is a set of 1460 residential homes in Ames, Iowa, each described by 80 features. We will use it to predict the final logarithmic price of the houses. In this example we will use only 20 most interesting features chosen using GradientBoostingRegressor() and limit number of entries (here we won&amp;rsquo;t go into the details on how to select the most interesting features).</source>
          <target state="translated">우리는 사용 &lt;a href=&quot;http://jse.amstat.org/v19n3/decock.pdf&quot;&gt;에임스 주택&lt;/a&gt; 첫째 딘 드 콕에 의해 컴파일과는 Kaggle 도전에 사용 된 후 더 잘 알려지게되었다 데이터 집합을. 아이오와 주 에임스에있는 1,460 채의 주거용 주택으로 각각 80 채의 특징으로 설명되어 있습니다. 이를 사용하여 주택의 최종 대수 가격을 예측합니다. 이 예에서는 GradientBoostingRegressor ()를 사용하여 선택한 가장 흥미로운 기능 20 개만 사용하고 항목 수를 제한합니다 (여기서는 가장 흥미로운 기능을 선택하는 방법에 대한 자세한 내용은 다루지 않습니다).</target>
        </trans-unit>
        <trans-unit id="037b81bcdb33690797b74b53d8be309b975609a6" translate="yes" xml:space="preserve">
          <source>We will use data from the &lt;a href=&quot;https://www.openml.org/d/534&quot;&gt;&amp;ldquo;Current Population Survey&amp;rdquo;&lt;/a&gt; from 1985 to predict wage as a function of various features such as experience, age, or education.</source>
          <target state="translated">우리는 1985 년 의 &lt;a href=&quot;https://www.openml.org/d/534&quot;&gt;&amp;ldquo;현 인구 조사&amp;rdquo;&lt;/a&gt; 데이터를 사용 하여 경험, 연령 또는 교육과 같은 다양한 기능의 함수로 임금을 예측할 것입니다.</target>
        </trans-unit>
        <trans-unit id="66b44f9dbf66aa535fbabfa2d804ce4636c53d36" translate="yes" xml:space="preserve">
          <source>We will use the &lt;a href=&quot;../../datasets/index#newsgroups-dataset&quot;&gt;20 newsgroups dataset&lt;/a&gt;, which comprises posts from newsgroups on 20 topics. This dataset is split into train and test subsets based on messages posted before and after a specific date. We will only use posts from 2 categories to speed up running time.</source>
          <target state="translated">20 개의 주제에 대한 뉴스 그룹의 게시물로 구성된 &lt;a href=&quot;../../datasets/index#newsgroups-dataset&quot;&gt;20 개의 뉴스 그룹 데이터 세트를&lt;/a&gt; 사용합니다 . 이 데이터 세트는 특정 날짜 전후에 게시 된 메시지를 기반으로 학습 및 테스트 하위 집합으로 분할됩니다. 우리는 실행 시간을 단축하기 위해 2 개 카테고리의 게시물 만 사용할 것입니다. ㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ</target>
        </trans-unit>
        <trans-unit id="889d3befa932d46595e6941bb656f25678ccbe94" translate="yes" xml:space="preserve">
          <source>We will use two datasets: Diabetes dataset which consists of 10 feature variables collected from diabetes patients with an aim to predict disease progression and California Housing dataset for which the target is the median house value for California districts.</source>
          <target state="translated">우리는 2 개의 데이터 세트를 사용할 것입니다 : 질병 진행을 예측하기 위해 당뇨병 환자로부터 수집 된 10 개의 특징 변수로 구성된 당뇨병 데이터 세트와 목표가 캘리포니아 지역의 중앙값 인 캘리포니아 주택 데이터 세트입니다.</target>
        </trans-unit>
        <trans-unit id="7968cd3ef765f8c808b9f8985bbceb6fcebbccfc" translate="yes" xml:space="preserve">
          <source>We will work with the diabetes dataset which consists of 10 features collected from a cohort of diabetes patients. The target is a quantitative measure of disease progression one year after baseline.</source>
          <target state="translated">우리는 당뇨병 환자 집단에서 수집 한 10 가지 특징으로 구성된 당뇨병 데이터 세트로 작업 할 것입니다. 목표는 기준선 1 년 후 질병 진행을 정량적으로 측정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="5ab95440492dec088ab7d086a3622b86acadc7e7" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ll define a function that lets us visualize the behavior of each cross-validation object. We&amp;rsquo;ll perform 4 splits of the data. On each split, we&amp;rsquo;ll visualize the indices chosen for the training set (in blue) and the test set (in red).</source>
          <target state="translated">각 교차 유효성 검사 개체의 동작을 시각화 할 수있는 함수를 정의하겠습니다. 우리는 4 개의 데이터 분할을 수행 할 것입니다. 각 스플릿마다 트레이닝 세트 (파란색)와 테스트 세트 (빨간색)에 선택된 인덱스를 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="70c023ae490fc71ae664cdd80527ab708a5db4b9" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ve already encountered some parameters such as &lt;code&gt;use_idf&lt;/code&gt; in the &lt;code&gt;TfidfTransformer&lt;/code&gt;. Classifiers tend to have many parameters as well; e.g., &lt;code&gt;MultinomialNB&lt;/code&gt; includes a smoothing parameter &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;SGDClassifier&lt;/code&gt; has a penalty parameter &lt;code&gt;alpha&lt;/code&gt; and configurable loss and penalty terms in the objective function (see the module documentation, or use the Python &lt;code&gt;help&lt;/code&gt; function to get a description of these).</source>
          <target state="translated">우리는 이미 같은 일부 매개 변수가 발생했습니다 &lt;code&gt;use_idf&lt;/code&gt; 에서 &lt;code&gt;TfidfTransformer&lt;/code&gt; . 분류 기에도 많은 매개 변수가 있습니다. 예를 들면, &lt;code&gt;MultinomialNB&lt;/code&gt; 는 스무딩 파라미터 포함 &lt;code&gt;alpha&lt;/code&gt; 와 &lt;code&gt;SGDClassifier&lt;/code&gt; 는 페널티 매개 변수가 &lt;code&gt;alpha&lt;/code&gt; 목적 함수와 구성 손실과 처벌 조건 (모듈 설명서를 참조하거나 파이썬 사용 &lt;code&gt;help&lt;/code&gt; 이들의 설명을 얻을 기능).</target>
        </trans-unit>
        <trans-unit id="e255008aad692a93735d4b63680bd4a96fdd74f7" translate="yes" xml:space="preserve">
          <source>Weight function used in prediction. Possible values:</source>
          <target state="translated">예측에 사용되는 가중치 함수입니다. 가능한 값 :</target>
        </trans-unit>
        <trans-unit id="96df76d7fa199e301349be570d5ef4d0bb6a7f3d" translate="yes" xml:space="preserve">
          <source>Weight given to each sample.</source>
          <target state="translated">각 샘플에 주어진 무게.</target>
        </trans-unit>
        <trans-unit id="a7a3ce3e7a16ef99378bfef64690454462da25e9" translate="yes" xml:space="preserve">
          <source>Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units.</source>
          <target state="translated">가중치 행렬 (여기서 표시 단위 수의 n_features 및 n_components는 숨겨진 단위 수)입니다.</target>
        </trans-unit>
        <trans-unit id="77c7b393c2f516d7fd42969c3e5ce51aceb4c82c" translate="yes" xml:space="preserve">
          <source>Weight of each sample, such that a sample with a weight of at least &lt;code&gt;min_samples&lt;/code&gt; is by itself a core sample; a sample with a negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.</source>
          <target state="translated">최소 &lt;code&gt;min_samples&lt;/code&gt; 의 가중치를 가진 샘플이 그 자체로 코어 샘플이되도록 각 샘플의 가중치 ; 무게가 음수 인 샘플은 eps 이웃이 코어가되는 것을 방해 할 수 있습니다. 가중치는 절대적이며 기본값은 1입니다.</target>
        </trans-unit>
        <trans-unit id="d0664e46a183d0a2a2e3afcbc3b6c5ba30a9d4ef" translate="yes" xml:space="preserve">
          <source>Weight of each sample, such that a sample with a weight of at least &lt;code&gt;min_samples&lt;/code&gt; is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.</source>
          <target state="translated">적어도 &lt;code&gt;min_samples&lt;/code&gt; 의 중량을 갖는 샘플이 그 자체가 핵심 샘플이되도록 각 샘플의 중량 ; 중량이 음수 인 샘플은 EPS 이웃이 코어가되지 않을 수 있습니다. 가중치는 절대 값이며 기본값은 1입니다.</target>
        </trans-unit>
        <trans-unit id="5cc536fd8cf249ec4c2e295665e0070f1b9cec67" translate="yes" xml:space="preserve">
          <source>Weight of precision in harmonic mean.</source>
          <target state="translated">조화 평균의 정밀도 가중치.</target>
        </trans-unit>
        <trans-unit id="6cd90e03c276712f974984f65620519bcea49500" translate="yes" xml:space="preserve">
          <source>Weight vector(s).</source>
          <target state="translated">무게 벡터.</target>
        </trans-unit>
        <trans-unit id="ac0d2c9a738f9c54a5d208ddac8f22019ff9c627" translate="yes" xml:space="preserve">
          <source>Weight, Waist and Pulse.</source>
          <target state="translated">체중, 허리 및 맥박.</target>
        </trans-unit>
        <trans-unit id="c74e4e7c5caf95682fb65872b5814741f06c7fac" translate="yes" xml:space="preserve">
          <source>Weighted average</source>
          <target state="translated">가중 평균</target>
        </trans-unit>
        <trans-unit id="39392047d0260b6fc1e042825fdae9116d630e28" translate="yes" xml:space="preserve">
          <source>Weighted average probability for each class per sample.</source>
          <target state="translated">샘플 당 각 클래스의 가중 평균 확률.</target>
        </trans-unit>
        <trans-unit id="62deefeab258040887cdf6743a94e11a29168c6f" translate="yes" xml:space="preserve">
          <source>Weighted within-class covariance matrix. It corresponds to &lt;code&gt;sum_k prior_k * C_k&lt;/code&gt; where &lt;code&gt;C_k&lt;/code&gt; is the covariance matrix of the samples in class &lt;code&gt;k&lt;/code&gt;. The &lt;code&gt;C_k&lt;/code&gt; are estimated using the (potentially shrunk) biased estimator of covariance. If solver is &amp;lsquo;svd&amp;rsquo;, only exists when &lt;code&gt;store_covariance&lt;/code&gt; is True.</source>
          <target state="translated">가중 클래스 내 공분산 행렬. 이는 &lt;code&gt;sum_k prior_k * C_k&lt;/code&gt; 해당합니다. 여기서 &lt;code&gt;C_k&lt;/code&gt; 는 클래스 &lt;code&gt;k&lt;/code&gt; 에있는 샘플의 공분산 행렬입니다 . &lt;code&gt;C_k&lt;/code&gt; 공분산의 (잠재적으로 축소) 편향 추정기를 사용하여 추정된다. 솔버가 'svd'이면 &lt;code&gt;store_covariance&lt;/code&gt; 가 True 인 경우에만 존재합니다 .</target>
        </trans-unit>
        <trans-unit id="ec852c96538aadaa4b0b959b23f492c546aedd45" translate="yes" xml:space="preserve">
          <source>Weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">점수를 계산하기위한 가중치 유형입니다. 없음은 가중치가 없음을 의미합니다. &quot;선형&quot;은 선형 가중치를 의미합니다. &quot;2 차&quot;는 2 차 가중치를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="9a702ae7f12a23bf0cde9786ae821e6b340d4991" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples (1. for unweighted).</source>
          <target state="translated">개별 샘플에 적용되는 분동 (무가 중의 경우 1.).</target>
        </trans-unit>
        <trans-unit id="0c68217fe30f051f7b997da07ad569653cfdb104" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed.</source>
          <target state="translated">개별 샘플에 적용된 분동. 제공하지 않으면 균일 한 가중치가 가정됩니다.</target>
        </trans-unit>
        <trans-unit id="3070fe087be2b6fbe15f65d4db644297c1112686" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified</source>
          <target state="translated">개별 샘플에 적용된 분동. 제공하지 않으면 균일 한 가중치가 가정됩니다. class_weight를 지정하면이 가중치에 class_weight (생성자를 통해 전달됨)가 곱해집니다.</target>
        </trans-unit>
        <trans-unit id="479d03c6533edea4de55367d8593a7974391f281" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified.</source>
          <target state="translated">개별 샘플에 적용되는 가중치. 제공되지 않으면 균일 한 가중치가 가정됩니다. 이러한 가중치는 class_weight가 지정된 경우 class_weight (생성자를 통해 전달됨)와 곱해집니다.</target>
        </trans-unit>
        <trans-unit id="86b1d5826d4836f8e129b6346c9e9e60976aafee" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.</source>
          <target state="translated">피쳐에 지정된 가중치 (원초 문제의 계수). 이것은 선형 커널의 경우에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4b149f5e057b5bff95048ebeff46bfac0e7a368d" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features.</source>
          <target state="translated">기능에 할당 된 가중치.</target>
        </trans-unit>
        <trans-unit id="4094f309127a892ba2564eecc3f5264343e18998" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</source>
          <target state="translated">&lt;code&gt;{class_label: weight}&lt;/code&gt; 형식의 클래스와 관련된 가중치 . None이면 모든 클래스의 가중치가 1이어야합니다. 다중 출력 문제의 경우 dict 목록이 y의 열과 동일한 순서로 제공 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8064bf8d5d6b0c15f7ed813823cc0da43a8bc7e6" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">&lt;code&gt;{class_label: weight}&lt;/code&gt; 형식의 클래스와 연관된 가중치 입니다. 주어지지 않으면 모든 수업은 1을가집니다.</target>
        </trans-unit>
        <trans-unit id="96f3238c530c2df09403ab213fee9515feb36c99" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</source>
          <target state="translated">&lt;code&gt;{class_label: weight}&lt;/code&gt; 형식의 클래스와 연관된 가중치 입니다. 주어지지 않으면 모든 수업은 1을가집니다. 다중 출력 문제의 경우, y 열과 동일한 순서로 dict 목록을 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4a87f3dd4dfb432aa1a51752552e165c9cc23301" translate="yes" xml:space="preserve">
          <source>Weights associated with classes. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">클래스와 관련된 가중치. 주어지지 않으면 모든 수업은 1을가집니다.</target>
        </trans-unit>
        <trans-unit id="4837ab63e8195a91fca82bbd82590df1bbe7fcc4" translate="yes" xml:space="preserve">
          <source>Weights for each estimator in the boosted ensemble.</source>
          <target state="translated">부스트 앙상블의 각 추정기에 대한 가중치.</target>
        </trans-unit>
        <trans-unit id="5f0089227653fec68913be1b7a199c273e750b44" translate="yes" xml:space="preserve">
          <source>Weights of training data.</source>
          <target state="translated">훈련 데이터의 가중치.</target>
        </trans-unit>
        <trans-unit id="55ddc90a49d39d4b40d722b720afa82441019829" translate="yes" xml:space="preserve">
          <source>Weights on each point of the regression. If None, weight is set to 1 (equal weights).</source>
          <target state="translated">회귀의 각 지점에 대한 가중치. 없음 인 경우 가중치는 1 (동일 가중치)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="0946f675292deb36a2ff9f4a33806ccd9e9e1833" translate="yes" xml:space="preserve">
          <source>Weights. If set to None, all weights will be set to 1 (equal weights).</source>
          <target state="translated">무게. 없음으로 설정하면 모든 가중치가 1 (동일 가중치)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="c1f521c553b00dab458900dd1fb3f949a6ba99ab" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class.</source>
          <target state="translated">잘 교정 된 분류기는 확률 적 분류기이며 predict_proba 메소드의 출력을 신뢰 수준으로 직접 해석 할 수 있습니다. 예를 들어, 잘 보정 된 (이진) 분류기는 샘플에 대해 predict_proba 값을 제공 한 샘플 중 대략 0.8에 가깝도록 샘플을 분류해야합니다. 실제로 80 %는 긍정적 인 계층에 속합니다.</target>
        </trans-unit>
        <trans-unit id="8b629519ceaa09dd1767dbc350008d2ef28e9249" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class.</source>
          <target state="translated">잘 보정 된 분류기는 predict_proba 메서드의 출력을 신뢰 수준으로 직접 해석 할 수있는 확률 적 분류기입니다. 예를 들어, 잘 보정 된 (이진) 분류기는 predict_proba 값이 0.8에 가까운 값을 제공 한 샘플 중에서 약 80 %가 실제로 포지티브 클래스에 속하도록 샘플을 분류해야합니다.</target>
        </trans-unit>
        <trans-unit id="6db2509535d857954c618d97c17994b5d42574ae" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class. The following plot compares how well the probabilistic predictions of different classifiers are calibrated:</source>
          <target state="translated">잘 교정 된 분류기는 확률 적 분류기이며 predict_proba 메소드의 출력을 신뢰 수준으로 직접 해석 할 수 있습니다. 예를 들어, 잘 보정 된 (이진) 분류기는 샘플에 대해 predict_proba 값이 0.8에 가까운 샘플 중 약 80 %가 실제로 양의 클래스에 속하도록 샘플을 분류해야합니다. 다음 그림은 여러 분류기의 확률 론적 예측이 얼마나 잘 교정되었는지 비교합니다.</target>
        </trans-unit>
        <trans-unit id="830bc34728ca0799f710b4883d58631c6dfded76" translate="yes" xml:space="preserve">
          <source>Wether to include meta-estimators that are somehow special and can not be default-constructed sensibly. These are currently Pipeline, FeatureUnion and GridSearchCV</source>
          <target state="translated">어쨌든 특별하고 현명하게 기본 구성이 불가능한 메타 추정기를 포함시켜야합니다. 이들은 현재 파이프 라인, FeatureUnion 및 GridSearchCV입니다.</target>
        </trans-unit>
        <trans-unit id="d4f157bc9962e4b0dc2a197ed14e50902555d749" translate="yes" xml:space="preserve">
          <source>What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?</source>
          <target state="translated">다양한 의사 결정 트리 알고리즘은 무엇이며 서로 어떻게 다른가요? scikit-learn에서 어느 것이 구현됩니까?</target>
        </trans-unit>
        <trans-unit id="a1f5f9cd3d06157b8582b1ca4000a5bc395e765a" translate="yes" xml:space="preserve">
          <source>What this example shows us is the behavior &amp;ldquo;rich getting richer&amp;rdquo; of agglomerative clustering that tends to create uneven cluster sizes. This behavior is pronounced for the average linkage strategy, that ends up with a couple of singleton clusters, while in the case of single linkage we get a single central cluster with all other clusters being drawn from noise points around the fringes.</source>
          <target state="translated">이 예에서 우리가 보여주는 것은 고르지 않은 클러스터 크기를 만드는 경향이있는 응집 클러스터링의 &quot;풍부 해짐&quot;동작입니다. 이 동작은 평균 연결 전략에서 두드러지며, 이는 단일 결합 클러스터로 끝나는 반면 단일 결합의 경우 프린지 주변의 노이즈 지점에서 다른 모든 클러스터가 그려지는 단일 중앙 클러스터를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="32580ac608fcdadc1c2050695a6c24cae1fcef1f" translate="yes" xml:space="preserve">
          <source>What we can see that:</source>
          <target state="translated">우리가 볼 수있는 것 :</target>
        </trans-unit>
        <trans-unit id="bd10ebd98b3e733be938ffeb72efbd3793589a2c" translate="yes" xml:space="preserve">
          <source>When &lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; is applied on a &amp;ldquo;document-term&amp;rdquo; matrix, the matrix will be decomposed into a &amp;ldquo;topic-term&amp;rdquo; matrix and a &amp;ldquo;document-topic&amp;rdquo; matrix. While &amp;ldquo;topic-term&amp;rdquo; matrix is stored as &lt;code&gt;components_&lt;/code&gt; in the model, &amp;ldquo;document-topic&amp;rdquo; matrix can be calculated from &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">되면 &lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt; 는 &quot;문서 용어&quot;행렬에 도포하고, 매트릭스는 &quot;항목 용어&quot;행렬 및 &quot;문서 주제&quot;행렬로 분해한다. &quot;토픽 용어&quot;매트릭스는 모델에서 &lt;code&gt;components_&lt;/code&gt; 로 저장되지만 &quot;문서 토픽&quot;매트릭스는 &lt;code&gt;transform&lt;/code&gt; 방법 에서 계산할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0c7daae89d35601e80a373ec7022d580692d615d" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;False&lt;/code&gt;, checks are evaluated when &lt;code&gt;check_estimator&lt;/code&gt; is called. When &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;check_estimator&lt;/code&gt; returns a generator that yields (estimator, check) tuples. The check is run by calling &lt;code&gt;check(estimator)&lt;/code&gt;.</source>
          <target state="translated">때 &lt;code&gt;False&lt;/code&gt; , 때 검사를 평가 &lt;code&gt;check_estimator&lt;/code&gt; 가 호출된다. 때 &lt;code&gt;True&lt;/code&gt; , &lt;code&gt;check_estimator&lt;/code&gt; 는 발전기 즉 수익률 (추정, 검사) 튜플을 반환합니다. 검사는 &lt;code&gt;check(estimator)&lt;/code&gt; 호출하여 실행됩니다 .</target>
        </trans-unit>
        <trans-unit id="d35c02f0322e905eb57e225a27ca83f7c5f6cc47" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=0&lt;/code&gt;, columns which only contained missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;axis=0&lt;/code&gt; 단에서 누락 값을 포함 컬럼 &lt;code&gt;fit&lt;/code&gt; 에 폐기 &lt;code&gt;transform&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7cf98e7188203ecb0d30e3564b161d5393433799" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=1&lt;/code&gt;, an exception is raised if there are rows for which it is not possible to fill in the missing values (e.g., because they only contain missing values).</source>
          <target state="translated">경우 &lt;code&gt;axis=1&lt;/code&gt; , 예외가 발생된다 (그들은 단지 누락 값을 포함하기 때문에, 예)는 누락 값을 입력 할 수되지 않은 행이 있다면.</target>
        </trans-unit>
        <trans-unit id="efffb9d033635c3e5b63e6f6f527201ab0e206e9" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;ccp_alpha&lt;/code&gt; is set to zero and keeping the other default parameters of &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt;, the tree overfits, leading to a 100% training accuracy and 88% testing accuracy. As alpha increases, more of the tree is pruned, thus creating a decision tree that generalizes better. In this example, setting &lt;code&gt;ccp_alpha=0.015&lt;/code&gt; maximizes the testing accuracy.</source>
          <target state="translated">때 &lt;code&gt;ccp_alpha&lt;/code&gt; 는 제로로 설정하고 다른 기본 매개 변수를 유지하고있다 &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; 100 % 훈련 정확도와 88 %의 테스트 정확도로 이어지는, 트리 overfits을. 알파가 증가하면 더 많은 트리가 잘려서 더 잘 일반화되는 의사 결정 트리가 생성됩니다. 이 예에서 &lt;code&gt;ccp_alpha=0.015&lt;/code&gt; 를 설정 하면 테스트 정확도가 최대화됩니다.</target>
        </trans-unit>
        <trans-unit id="4bce6b6fbd4b9fe79bfc7468f7df9f938d3ce841" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;fit&lt;/code&gt; does not converge, &lt;code&gt;cluster_centers_&lt;/code&gt; becomes an empty array and all training samples will be labelled as &lt;code&gt;-1&lt;/code&gt;. In addition, &lt;code&gt;predict&lt;/code&gt; will then label every sample as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">때 &lt;code&gt;fit&lt;/code&gt; 수렴하지 않는, &lt;code&gt;cluster_centers_&lt;/code&gt; 는 하늘의 배열이되고 모든 교육 샘플로 표시됩니다 &lt;code&gt;-1&lt;/code&gt; . 또한 &lt;code&gt;predict&lt;/code&gt; 는 모든 샘플에 &lt;code&gt;-1&lt;/code&gt; 레이블을 붙 입니다.</target>
        </trans-unit>
        <trans-unit id="f7c77c5939a6b7b3a7ed9ce47827d49725522a57" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;gamma&lt;/code&gt; is very small, the model is too constrained and cannot capture the complexity or &amp;ldquo;shape&amp;rdquo; of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.</source>
          <target state="translated">때 &lt;code&gt;gamma&lt;/code&gt; 매우 작고, 모델도 제한되고 복잡하거나 데이터의 &quot;형태&quot;를 캡처 할 수 없습니다. 선택된 서포트 벡터의 영향 영역에는 전체 트레이닝 세트가 포함됩니다. 결과 모델은 두 클래스의 모든 쌍의 고밀도 중심을 분리하는 초평면 세트가있는 선형 모델과 유사하게 작동합니다.</target>
        </trans-unit>
        <trans-unit id="6579a89bba02e2aa5596acf0a356de3285b5831f" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;learning_method&lt;/code&gt; is &amp;lsquo;online&amp;rsquo;, use mini-batch update. Otherwise, use batch update.</source>
          <target state="translated">때 &lt;code&gt;learning_method&lt;/code&gt; 가 '온라인'이다, 미니 일괄 업데이트를 사용합니다. 그렇지 않으면 배치 업데이트를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="1789662661fe322dc45828ec3c3eb62749643034" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;novelty&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; be aware that you must only use &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute.</source>
          <target state="translated">때 &lt;code&gt;novelty&lt;/code&gt; 설정되어 &lt;code&gt;True&lt;/code&gt; 만 사용해야한다는 인식 &lt;code&gt;predict&lt;/code&gt; , &lt;code&gt;decision_function&lt;/code&gt; 을 하고 &lt;code&gt;score_samples&lt;/code&gt; 새로운 보이지 않는 데이터가 아니라이 잘못된 결과로 이어질 것으로 훈련 샘플에. 훈련 샘플의 비정상 점수는 항상 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6539967754248da8802fa11e92b0d1b2532b93b8" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;predict_proba&lt;/code&gt; is used by each estimator (i.e. most of the time for &lt;code&gt;stack_method='auto'&lt;/code&gt; or specifically for &lt;code&gt;stack_method='predict_proba'&lt;/code&gt;), The first column predicted by each estimator will be dropped in the case of a binary classification problem. Indeed, both feature will be perfectly collinear.</source>
          <target state="translated">경우 &lt;code&gt;predict_proba&lt;/code&gt; 가 (위한 즉, 대부분의 시간을 각각 추정하여 사용 &lt;code&gt;stack_method='auto'&lt;/code&gt; 특별히 또는 &lt;code&gt;stack_method='predict_proba'&lt;/code&gt; ), 각 추정기에 의해 예측 된 첫번째 열은 이진 분류 문제의 경우에 감소한다. 실제로 두 기능은 완벽하게 동일 선상에있을 것입니다.</target>
        </trans-unit>
        <trans-unit id="380c86407d2b15e6e735d8a020999525510d4000" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;shuffle&lt;/code&gt; is True, &lt;code&gt;random_state&lt;/code&gt; affects the ordering of the indices, which controls the randomness of each fold for each class. Otherwise, leave &lt;code&gt;random_state&lt;/code&gt; as &lt;code&gt;None&lt;/code&gt;. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;shuffle&lt;/code&gt; 사실입니다 &lt;code&gt;random_state&lt;/code&gt; 는 각 각 클래스에 대한 배의 임의성을 제어하는 인덱스의 순서에 영향을 미친다. 그렇지 않으면 &lt;code&gt;random_state&lt;/code&gt; 를 &lt;code&gt;None&lt;/code&gt; 으로 둡니다 . 여러 함수 호출에서 재현 가능한 출력을 위해 int를 전달합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="dba0ae97777bf9e3aca974d6ebb1591964b94b4e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;shuffle&lt;/code&gt; is True, &lt;code&gt;random_state&lt;/code&gt; affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;shuffle&lt;/code&gt; 사실입니다 &lt;code&gt;random_state&lt;/code&gt; 는 각 배의 임의성을 제어하는 인덱스의 순서에 영향을 미친다. 그렇지 않으면이 매개 변수가 적용되지 않습니다. 여러 함수 호출에서 재현 가능한 출력을 위해 int를 전달합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2f36c5dece71799d4edbabcfff5ea8ed2f825b4d" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;true positive + false negative == 0&lt;/code&gt;, recall returns 0 and raises &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;. This behavior can be modified with &lt;code&gt;zero_division&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;true positive + false negative == 0&lt;/code&gt; 일 때 , 회상은 0을 반환하고 &lt;code&gt;UndefinedMetricWarning&lt;/code&gt; 을 발생 시킵니다. 이 동작은 &lt;code&gt;zero_division&lt;/code&gt; 으로 수정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="43540f9f914266a4b6caeeeeef53fc90c0648c6e" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;true positive + false positive == 0&lt;/code&gt; or &lt;code&gt;true positive + false negative == 0&lt;/code&gt;, f-score returns 0 and raises &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;. This behavior can be modified with &lt;code&gt;zero_division&lt;/code&gt;.</source>
          <target state="translated">때 &lt;code&gt;true positive + false positive == 0&lt;/code&gt; 또는 &lt;code&gt;true positive + false negative == 0&lt;/code&gt; , F-점수 0을 반환하고 제기 &lt;code&gt;UndefinedMetricWarning&lt;/code&gt; 을 . 이 동작은 &lt;code&gt;zero_division&lt;/code&gt; 으로 수정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ba4ca3b8ef57a04587026d0caa7a41ba81b60463" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;true positive + false positive == 0&lt;/code&gt;, precision is undefined; When &lt;code&gt;true positive + false negative == 0&lt;/code&gt;, recall is undefined. In such cases, by default the metric will be set to 0, as will f-score, and &lt;code&gt;UndefinedMetricWarning&lt;/code&gt; will be raised. This behavior can be modified with &lt;code&gt;zero_division&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;true positive + false positive == 0&lt;/code&gt; , 정밀도는 정의되고; 경우 &lt;code&gt;true positive + false negative == 0&lt;/code&gt; , 리콜은 정의되지 않는다. 이러한 경우 기본적으로 메트릭은 f 점수와 마찬가지로 0으로 설정되고 &lt;code&gt;UndefinedMetricWarning&lt;/code&gt; 이 발생합니다. 이 동작은 &lt;code&gt;zero_division&lt;/code&gt; 으로 수정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c54bb2862f6516891f7056fa35777ac8bd097188" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;true positive + false positive == 0&lt;/code&gt;, precision returns 0 and raises &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;. This behavior can be modified with &lt;code&gt;zero_division&lt;/code&gt;.</source>
          <target state="translated">때 &lt;code&gt;true positive + false positive == 0&lt;/code&gt; , 정밀 0을 반환하고 제기 &lt;code&gt;UndefinedMetricWarning&lt;/code&gt; 을 . 이 동작은 &lt;code&gt;zero_division&lt;/code&gt; 으로 수정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="70168d4d772dfaf9b58be440f7660f3adf7f21be" translate="yes" xml:space="preserve">
          <source>When False, &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; both being sparse will yield sparse output. When True, output will always be a dense array.</source>
          <target state="translated">False이면 &lt;code&gt;a&lt;/code&gt; 와 &lt;code&gt;b&lt;/code&gt; 가 모두 희소하면 희소 출력이 생성됩니다. True이면 출력은 항상 조밀 한 배열입니다.</target>
        </trans-unit>
        <trans-unit id="f3329fe0cead2a208482794593628ff4dce53789" translate="yes" xml:space="preserve">
          <source>When False, either &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;b&lt;/code&gt; being sparse will yield sparse output. When True, output will always be an array.</source>
          <target state="translated">False 인 경우 &lt;code&gt;a&lt;/code&gt; 또는 &lt;code&gt;b&lt;/code&gt; 가 드문 드문 드문 드문 출력됩니다. True 인 경우 출력은 항상 배열입니다.</target>
        </trans-unit>
        <trans-unit id="1bb4bf21b8d21fdceb61000bf23d6624de5a3ca6" translate="yes" xml:space="preserve">
          <source>When False, only the predictions of estimators will be used as training data for &lt;code&gt;final_estimator&lt;/code&gt;. When True, the &lt;code&gt;final_estimator&lt;/code&gt; is trained on the predictions as well as the original training data.</source>
          <target state="translated">False이면 추정 자의 예측 만 &lt;code&gt;final_estimator&lt;/code&gt; 의 학습 데이터로 사용됩니다 . True 인 경우 &lt;code&gt;final_estimator&lt;/code&gt; 는 원래 학습 데이터뿐만 아니라 예측에 대해서도 학습됩니다.</target>
        </trans-unit>
        <trans-unit id="0baad85c0e4d647371baa109869739d44cb0f600" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are divided by &lt;code&gt;n_samples&lt;/code&gt; times &lt;code&gt;components_&lt;/code&gt; to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">경우 트루 (기본적으로 false) &lt;code&gt;components_&lt;/code&gt; 의 벡터에 의해 분할된다 &lt;code&gt;n_samples&lt;/code&gt; 시간 &lt;code&gt;components_&lt;/code&gt; 부 성분 와이즈 차이와 상관 출력을 보장하기 위해.</target>
        </trans-unit>
        <trans-unit id="3eaa2881688032030c765cb871d4c787aff03fe7" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">True (기본적으로 False) 인 경우 &lt;code&gt;components_&lt;/code&gt; vector 는 n_samples의 제곱근을 곱한 다음 특이 값으로 나눠 단위 성분 별 분산으로 상관되지 않은 출력을 보장합니다.</target>
        </trans-unit>
        <trans-unit id="afb8087ad0f4a499dd14f72be9807792c351ecd8" translate="yes" xml:space="preserve">
          <source>When True, an absolute value is applied to the features matrix prior to returning it. When used in conjunction with alternate_sign=True, this significantly reduces the inner product preservation property.</source>
          <target state="translated">True 인 경우 반환하기 전에 기능 매트릭스에 절대 값이 적용됩니다. alternate_sign = True와 함께 사용하면 내부 제품 보존 속성이 크게 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="204d5c48d22bd9cd74d36182840231c3ecac4f55" translate="yes" xml:space="preserve">
          <source>When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.</source>
          <target state="translated">True 인 경우 작은 n_features의 경우에도 해시 된 공간에서 내부 제품을 대략 보존하기 위해 대체 부호가 기능에 추가됩니다. 이 방법은 희소 랜덤 프로젝션과 유사합니다.</target>
        </trans-unit>
        <trans-unit id="e4283276989a341cb6ca6bb94564d0f53f92318b" translate="yes" xml:space="preserve">
          <source>When X and/or Y are CSR sparse matrices and they are not already in canonical format, this function modifies them in-place to make them canonical.</source>
          <target state="translated">X 및 / 또는 Y가 CSR 희소 행렬이고 아직 표준 형식이 아닌 경우이 함수는 해당 행렬을 제자리에서 수정하여 표준으로 만듭니다.</target>
        </trans-unit>
        <trans-unit id="e08ec276d8bba6d4be8a6e677715ca1a35d4dd15" translate="yes" xml:space="preserve">
          <source>When a grouped cross-validator is used, the group labels are also passed on to the &lt;code&gt;split&lt;/code&gt; method of the cross-validator. The cross-validator uses them for grouping the samples while splitting the dataset into train/test set.</source>
          <target state="translated">그룹화 된 교차 유효성 검사기를 사용하면 그룹 레이블도 교차 유효성 검사기 의 &lt;code&gt;split&lt;/code&gt; 방법으로 전달됩니다 . 교차 검증기는 데이터 세트를 트레인 / 테스트 세트로 분할하면서 샘플을 그룹화하기 위해이 샘플을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="96fd8c38f0f9978d89774efb70a7d42aee5d30ee" translate="yes" xml:space="preserve">
          <source>When a specific number of neighbors is queried (using &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;KNeighborsTransformer&lt;/code&gt;&lt;/a&gt;), the definition of &lt;code&gt;n_neighbors&lt;/code&gt; is ambiguous since it can either include each training point as its own neighbor, or exclude them. Neither choice is perfect, since including them leads to a different number of non-self neighbors during training and testing, while excluding them leads to a difference between &lt;code&gt;fit(X).transform(X)&lt;/code&gt; and &lt;code&gt;fit_transform(X)&lt;/code&gt;, which is against scikit-learn API. In &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; we use the definition which includes each training point as its own neighbor in the count of &lt;code&gt;n_neighbors&lt;/code&gt;. However, for compatibility reasons with other estimators which use the other definition, one extra neighbor will be computed when &lt;code&gt;mode == 'distance'&lt;/code&gt;. To maximise compatibility with all estimators, a safe choice is to always include one extra neighbor in a custom nearest neighbors estimator, since unnecessary neighbors will be filtered by following estimators.</source>
          <target state="translated">이웃의 특정 번호를 (사용하여 쿼리하면 &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;KNeighborsTransformer&lt;/code&gt; 을&lt;/a&gt; )의 정의 &lt;code&gt;n_neighbors&lt;/code&gt; 이 중 하나를 자신의 이웃 각 교육 포인트를 포함하거나 제외 할 수 있기 때문에 모호합니다. 이들을 포함하면 훈련 및 테스트 중에 다른 수의 비 자기 이웃이 발생하고이를 제외하면 &lt;code&gt;fit(X).transform(X)&lt;/code&gt; 및 &lt;code&gt;fit_transform(X)&lt;/code&gt; 간에 차이 가 생기므로 scikit에 반대하므로 두 선택 모두 완벽하지 않습니다. -API를 배우십시오. 에서 &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;KNeighborsTransformer&lt;/code&gt; &lt;/a&gt; 우리의 수에 자신의 이웃 각 훈련 포인트가 포함 된 정의를 사용 &lt;code&gt;n_neighbors&lt;/code&gt; . 그러나 다른 정의를 사용하는 다른 추정 기와의 호환성 이유로 &lt;code&gt;mode == 'distance'&lt;/code&gt; 때 하나의 추가 이웃이 계산됩니다 . 모든 추정 기와의 호환성을 최대화하려면 불필요한 이웃이 다음 추정기에 의해 필터링되므로 사용자 지정 최근 접 이웃 추정기에 항상 하나의 추가 이웃을 포함하는 것이 안전한 선택입니다.</target>
        </trans-unit>
        <trans-unit id="6c0f90cdc44694d62adb6ac9bccd5513d6bf148f" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, &lt;code&gt;fit&lt;/code&gt; will result in a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">모든 트레이닝 샘플이 동일한 유사성과 동일한 환경 설정을 갖는 경우, 클러스터 센터 및 레이블의 할당은 환경 설정에 따라 다릅니다. 선호도가 유사성보다 작은 경우 &lt;code&gt;fit&lt;/code&gt; 하면 모든 표본 에 대해 단일 군집 중심과 레이블 &lt;code&gt;0&lt;/code&gt; 이 생성됩니다. 그렇지 않으면 모든 교육 샘플이 자체 클러스터 센터가되고 고유 한 레이블이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="c440a8e563c9cc7b9752f14072284d81697bdfcd" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample will be returned. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">모든 트레이닝 샘플이 동일한 유사성과 동일한 환경 설정을 갖는 경우, 클러스터 센터 및 레이블의 할당은 환경 설정에 따라 다릅니다. 기본 설정이 유사성보다 작은 경우 모든 샘플에 대해 단일 군집 중심과 레이블 &lt;code&gt;0&lt;/code&gt; 이 반환됩니다. 그렇지 않으면 모든 교육 샘플이 자체 클러스터 센터가되고 고유 한 레이블이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="2494d11b03713922afdad3fa1bc52bb7064577b4" translate="yes" xml:space="preserve">
          <source>When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both.</source>
          <target state="translated">알파가 매우 크면 정규화 효과가 제곱 손실 함수를 지배하며 계수는 0 인 경향이 있습니다. 경로의 끝에서 알파는 0을 향하고 솔루션은 일반적인 최소 제곱을 향함에 따라 계수는 큰 진동을 나타냅니다. 실제로, 둘 사이의 균형이 유지되도록 알파를 조정해야합니다.</target>
        </trans-unit>
        <trans-unit id="e93a4fdf68d407dc869190eca6cd7dfaf57ad986" translate="yes" xml:space="preserve">
          <source>When applying LOF for outlier detection, there are no &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; methods but only a &lt;code&gt;fit_predict&lt;/code&gt; method. The scores of abnormality of the training samples are accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute. Note that &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; can be used on new unseen data when LOF is applied for novelty detection, i.e. when the &lt;code&gt;novelty&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;. See &lt;a href=&quot;#novelty-with-lof&quot;&gt;Novelty detection with Local Outlier Factor&lt;/a&gt;.</source>
          <target state="translated">이상치 검출 LOF을 적용 할 때, 어떤이없는 &lt;code&gt;predict&lt;/code&gt; , &lt;code&gt;decision_function&lt;/code&gt; 및 &lt;code&gt;score_samples&lt;/code&gt; 방법 만 만 &lt;code&gt;fit_predict&lt;/code&gt; 의 방법을. 훈련 샘플의 비정상 점수는 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 . LOF가 참신 탐지에 적용되는 경우, 즉 &lt;code&gt;novelty&lt;/code&gt; 매개 변수가 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우 &lt;code&gt;predict&lt;/code&gt; 않은 새로운 데이터에 대해 predict , &lt;code&gt;decision_function&lt;/code&gt; 및 &lt;code&gt;score_samples&lt;/code&gt; 를 사용할 수 있습니다 . &lt;a href=&quot;#novelty-with-lof&quot;&gt;로컬 이상치 요인을 사용한 참신 탐지를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2ec6d5a6d0d8178f2ec5cc1ea59ce167ddd0a98a" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 작성할 때 문서 빈도가 주어진 임계 값 (말뭉치 특정 불용어)보다 엄격하게 높은 용어를 무시하십시오. [0.0, 1.0] 범위에서 부동 인 경우 매개 변수는 문서의 비율, 정수 절대 개수를 나타냅니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="869141a5ff3e1444543cdaec8c9968684d76b66e" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 만들 때 주어진 문턱 값보다 엄격하게 높은 문서 빈도를 가진 용어는 무시합니다 (corpus-specific stop words). float 인 경우 매개 변수는 문서의 비율을 나타내며 정수 절대 계수입니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="d83f4236f3f25a9891417b446059c6cca3f6ec76" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 작성할 때 문서 빈도가 주어진 임계 값보다 엄격하게 낮은 용어를 무시하십시오. 이 값은 문헌에서 컷오프라고도합니다. [0.0, 1.0] 범위에서 부동 인 경우 매개 변수는 문서의 비율, 정수 절대 개수를 나타냅니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="1f13ad80586b74ad6fdc521fde036c051cf6e0e7" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 작성할 때 문서 빈도가 주어진 임계 값보다 엄격하게 낮은 용어는 무시하십시오. 이 값은 문헌에서 차단이라고도합니다. float 인 경우 매개 변수는 문서의 비율을 나타내며 정수 절대 계수입니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="757013c910e6b4628ced11d689b0016d8c17e540" translate="yes" xml:space="preserve">
          <source>When calculating class-wise multilabel confusion matrix \(C\), the count of true negatives for class \(i\) is \(C_{i,0,0}\), false negatives is \(C_{i,1,0}\), true positives is \(C_{i,1,1}\) and false positives is \(C_{i,0,1}\).</source>
          <target state="translated">클래스 별 다중 레이블 혼동 행렬 \ (C \)를 계산할 때 클래스 \ (i \)에 대한 참 음성의 개수는 \ (C_ {i, 0,0} \)이고 거짓 음성은 \ (C_ {i, 1)입니다. , 0} \), 참 양성은 \ (C_ {i, 1,1} \)이고 거짓 양성은 \ (C_ {i, 0,1} \)입니다.</target>
        </trans-unit>
        <trans-unit id="8ed788b8f95069d89dcb15b1bea5b5e7da9a9648" translate="yes" xml:space="preserve">
          <source>When calling &lt;code&gt;fit&lt;/code&gt;, an affinity matrix is constructed using either kernel function such the Gaussian (aka RBF) kernel of the euclidean distanced &lt;code&gt;d(X, X)&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 을 호출 할 때 선호도 행렬은 유클리드 거리 &lt;code&gt;d(X, X)&lt;/code&gt; 의 가우스 (일명 RBF) 커널과 같은 커널 함수를 사용하여 구성됩니다 .</target>
        </trans-unit>
        <trans-unit id="5c5941b79e3bd733fe2a806a66f459a87c19af32" translate="yes" xml:space="preserve">
          <source>When dealing with a cleaned dataset, the preprocessing can be automatic by using the data types of the column to decide whether to treat a column as a numerical or categorical feature. &lt;a href=&quot;../../modules/generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;sklearn.compose.make_column_selector&lt;/code&gt;&lt;/a&gt; gives this possibility. First, let&amp;rsquo;s only select a subset of columns to simplify our example.</source>
          <target state="translated">정리 된 데이터 세트를 처리 할 때 열의 데이터 유형을 사용하여 열을 숫자 또는 범주 특성으로 처리할지 여부를 결정하여 사전 처리를 자동으로 수행 할 수 있습니다. &lt;a href=&quot;../../modules/generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt; &lt;code&gt;sklearn.compose.make_column_selector&lt;/code&gt; &lt;/a&gt; 는 이러한 가능성을 제공합니다. 먼저 예제를 단순화하기 위해 열의 하위 집합 만 선택하겠습니다.</target>
        </trans-unit>
        <trans-unit id="da9c4333516559f145c3dcae1d1fd9f5e0da891d" translate="yes" xml:space="preserve">
          <source>When doing classification in scikit-learn, &lt;code&gt;y&lt;/code&gt; is a vector of integers or strings.</source>
          <target state="translated">scikit-learn에서 분류를 수행 할 때 &lt;code&gt;y&lt;/code&gt; 는 정수 또는 문자열로 구성된 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="fb08dbb1ad477236e66b72b0cef9bccfd1ceec61" translate="yes" xml:space="preserve">
          <source>When doing supervised learning, a simple sanity check consists of comparing one&amp;rsquo;s estimator against simple rules of thumb. &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt; implements several such simple strategies for classification:</source>
          <target state="translated">지도 학습을 수행 할 때 간단한 위생 검사는 추정기를 간단한 경험 규칙과 비교하여 구성됩니다. &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt; 는 분류를위한 몇 가지 간단한 전략을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="3bf9748662d1dbe365a15ba24b002d016f11408b" translate="yes" xml:space="preserve">
          <source>When evaluating different settings (&amp;ldquo;hyperparameters&amp;rdquo;) for estimators, such as the &lt;code&gt;C&lt;/code&gt; setting that must be manually set for an SVM, there is still a risk of overfitting &lt;em&gt;on the test set&lt;/em&gt; because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can &amp;ldquo;leak&amp;rdquo; into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called &amp;ldquo;validation set&amp;rdquo;: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</source>
          <target state="translated">SVM에 대해 수동으로 설정해야하는 &lt;code&gt;C&lt;/code&gt; 설정 과 같은 추정기의 다른 설정 (&amp;ldquo;하이 파라미터&amp;rdquo;)을 평가할 때 추정기 가 최적으로 수행 될 때까지 매개 변수를 조정할 수 있으므로 &lt;em&gt;테스트 세트&lt;/em&gt; 에 과적 합의 위험이 여전히 있습니다. 이런 식으로 테스트 세트에 대한 지식이 모델에 &quot;누설&quot;될 수 있으며 평가 지표는 더 이상 일반화 성능에 대해보고하지 않습니다. 이 문제를 해결하기 위해 데이터 집합의 또 다른 부분을 소위 &quot;유효성 검사 세트&quot;라고 할 수 있습니다. 훈련은 훈련 세트에서 진행된 후 유효성 검사 세트에서 평가가 수행 된 후 실험이 성공한 것으로 보입니다 테스트 세트에서 최종 평가를 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6d52e3adfb55f9048c5ef57545d77d640ae7db4" translate="yes" xml:space="preserve">
          <source>When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt;. The F-score will be lower because it is more realistic.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터에서 텍스트 분류자를 평가할 때 뉴스 그룹 관련 메타 데이터를 제거해야합니다. scikit-learn에서 &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt; 를 설정하여이를 수행 할 수 있습니다 . F- 점수가 더 현실적이기 때문에 더 낮아집니다.</target>
        </trans-unit>
        <trans-unit id="b44ace677df67ec762b5f7d213266d4181953b1c" translate="yes" xml:space="preserve">
          <source>When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a &lt;strong&gt;development set&lt;/strong&gt; (to be fed to the &lt;code&gt;GridSearchCV&lt;/code&gt; instance) and an &lt;strong&gt;evaluation set&lt;/strong&gt; to compute performance metrics.</source>
          <target state="translated">결과 모델을 평가할 때 그리드 검색 프로세스 중에 보이지 않은 보류 된 샘플에서 모델을 수행하는 것이 중요합니다. 데이터를 &lt;strong&gt;개발 세트&lt;/strong&gt; ( &lt;code&gt;GridSearchCV&lt;/code&gt; 인스턴스에 제공) 및 &lt;strong&gt;평가 세트&lt;/strong&gt; 로 분할하는 것이 좋습니다. 성능 지표를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1a86b5b9ee94c593acad1b7968fd00ab3b487df9" translate="yes" xml:space="preserve">
          <source>When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature &amp;ldquo;f&amp;rdquo; that can take on the values &amp;ldquo;ham&amp;rdquo; and &amp;ldquo;spam&amp;rdquo; will become two features in the output, one signifying &amp;ldquo;f=ham&amp;rdquo;, the other &amp;ldquo;f=spam&amp;rdquo;.</source>
          <target state="translated">기능 값이 문자열 인 경우이 변환기는 이진 one-hot (일명 K) 코딩을 수행합니다. 하나의 부울 값 기능은 기능이 취할 수있는 가능한 문자열 값 각각에 대해 구성됩니다. 예를 들어, &quot;ham&quot;및 &quot;spam&quot;값을 사용할 수있는 기능 &quot;f&quot;는 출력에서 ​​&quot;f = ham&quot;을 나타내는 두 가지 기능, 다른 하나는 &quot;f = spam&quot;이됩니다.</target>
        </trans-unit>
        <trans-unit id="81d685e408d1bc940e50f235a77cb7283ea2909d" translate="yes" xml:space="preserve">
          <source>When features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster. First, we plot a heatmap of the correlated features:</source>
          <target state="translated">피쳐가 동일 선상에있을 때 하나의 피쳐를 순열해도 상관 피쳐에서 동일한 정보를 얻을 수 있으므로 모델 성능에 거의 영향을주지 않습니다. 다중 공 선적 기능을 처리하는 한 가지 방법은 Spearman 순위 상관 관계에서 계층 적 클러스터링을 수행하고 임계 값을 선택하고 각 클러스터에서 단일 기능을 유지하는 것입니다. 먼저 상호 관련된 기능의 히트 맵을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="e3d8c3576d6baaf6f77dc4e34223e91a48a8c662" translate="yes" xml:space="preserve">
          <source>When fitting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case if you load the files individually with load_svmlight_file.</source>
          <target state="translated">모델을 행렬 X_train에 피팅하고 행렬 X_test에 대해 평가할 때 X_train과 X_test는 동일한 수의 기능 (X_train.shape [1] == X_test.shape [1])을 가져야합니다. load_svmlight_file을 사용하여 파일을 개별적으로로드하는 경우에는 해당되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="618912ddd6add9bc398f8cf8343e255bb5f0ac59" translate="yes" xml:space="preserve">
          <source>When in doubt, use &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;</source>
          <target state="translated">의심 &lt;a href=&quot;#ransac-regression&quot;&gt;스러운&lt;/a&gt; 경우 RANSAC을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="4150d19b2bd7fadd0941f6ca06455157d317e492" translate="yes" xml:space="preserve">
          <source>When in doubt, use &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#ransac-regression&quot;&gt;확실하지 않은&lt;/a&gt; 경우 RANSAC를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="7ebc317e66e94c9c3ce7d1b975021eefa97b880b" translate="yes" xml:space="preserve">
          <source>When individual estimators are fast to train or predict using &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; can result in slower performance due to the overhead of spawning processes.</source>
          <target state="translated">개별 견적자가 &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 을 사용하여 빠르게 훈련하거나 예측하는 경우 생성 프로세스 오버 헤드로 인해 성능이 저하 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5223d409a8ec7650bd8559a52e22ddb32a950ada" translate="yes" xml:space="preserve">
          <source>When loss=&amp;rdquo;modified_huber&amp;rdquo;, probability estimates may be hard zeros and ones, so taking the logarithm is not possible.</source>
          <target state="translated">loss =&amp;rdquo;modified_huber&amp;rdquo;인 경우 확률 추정치는 0과 1 일 수 있으므로 로그를 취할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="3169e6d4a043a16249e2b124b1d248f9aac1b1e2" translate="yes" xml:space="preserve">
          <source>When modeling text corpora, the model assumes the following generative process for a corpus with \(D\) documents and \(K\) topics, with \(K\) corresponding to &lt;code&gt;n_components&lt;/code&gt; in the API:</source>
          <target state="translated">텍스트 말뭉치를 모델링 할 때 모델은 \ (D \) 문서 및 \ (K \) 주제가있는 말뭉치에 대해 다음 생성 프로세스를 가정하고 \ (K \) 는 API의 &lt;code&gt;n_components&lt;/code&gt; 에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="63fb46936ec9f0c71e69b897d1f9d91941d422e3" translate="yes" xml:space="preserve">
          <source>When modeling text corpora, the model assumes the following generative process for a corpus with \(D\) documents and \(K\) topics:</source>
          <target state="translated">텍스트 코포 라를 모델링 할 때이 모델은 \ (D \) 문서와 \ (K \) 토픽을 가진 모음에 대해 다음과 같은 생성 프로세스를 가정합니다.</target>
        </trans-unit>
        <trans-unit id="744347c999c68da3080dd50ae1b985e4b97e385e" translate="yes" xml:space="preserve">
          <source>When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.</source>
          <target state="translated">혼합물 당 점이 충분하지 않으면 공분산 행렬을 추정하기가 어려워지며, 알고리즘은 공분산을 인위적으로 정규화하지 않는 한 무한한 가능성으로 솔루션을 분기하고 찾는 것으로 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="e24f09e860711fd3a63a21415134601f90e4efc0" translate="yes" xml:space="preserve">
          <source>When parametrized by error using the parameter &lt;code&gt;tol&lt;/code&gt;: argmin ||gamma||_0 subject to ||y - Xgamma||^2 &amp;lt;= tol</source>
          <target state="translated">매개 변수 &lt;code&gt;tol&lt;/code&gt; 을 사용하여 오류로 매개 변수화되는 경우 : argmin || gamma || _0 || y-Xgamma || ^ 2 &amp;lt;= tol</target>
        </trans-unit>
        <trans-unit id="70b3334d846e26366c45a04fb1a4a39af8e3ec45" translate="yes" xml:space="preserve">
          <source>When parametrized by the number of non-zero coefficients using &lt;code&gt;n_nonzero_coefs&lt;/code&gt;: argmin ||y - Xgamma||^2 subject to ||gamma||_0 &amp;lt;= n_{nonzero coefs}</source>
          <target state="translated">&lt;code&gt;n_nonzero_coefs&lt;/code&gt; 를 사용하여 0이 아닌 계수의 수로 매개 변수화되는 경우 : argmin || y-Xgamma || ^ 2 || gamma || _0 &amp;lt;= n_ {nonzero coefs}</target>
        </trans-unit>
        <trans-unit id="10d326cee514d3b967129b254954126bcda90793" translate="yes" xml:space="preserve">
          <source>When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to display how well calibrated the predicted probabilities are and how to calibrate an uncalibrated classifier.</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블뿐만 아니라 관련 확률도 예측하려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 이 예제는 예측 된 확률이 얼마나 잘 교정되었는지와 교정되지 않은 분류기를 교정하는 방법을 표시하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="533cc79868c5585705042f97bd64dbf9b1c6133f" translate="yes" xml:space="preserve">
          <source>When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블을 예측할뿐만 아니라 각 레이블의 확률을 얻으려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 일부 모델은 클래스 확률에 대한 잘못된 추정치를 제공 할 수 있으며 일부는 확률 예측을 지원하지 않습니다. 보정 모듈을 사용하면 주어진 모델의 확률을보다 잘 보정하거나 확률 예측에 대한 지원을 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ede14543224daf4bd7da97ebebdbcb4bdb8fe514" translate="yes" xml:space="preserve">
          <source>When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier&amp;rsquo;s score (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score&lt;/a&gt;).</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블뿐만 아니라 관련 확률도 예측하려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 그러나 모든 분류자가 교정이 잘 된 확률을 제공하는 것은 아니며 일부는 지나치게 확신하고 다른 일부는 확신이 부족합니다. 따라서, 예측 된 확률의 개별 교정은 종종 후 처리로서 바람직하다. 이 예제는이 교정에 대한 두 가지 다른 방법을 보여주고 Brier의 점수를 사용하여 반환 된 확률의 품질을 평가합니다 ( &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="47397daaf8d579706c38eeae4f614a41b7464779" translate="yes" xml:space="preserve">
          <source>When performing cross-validation for the &lt;code&gt;power&lt;/code&gt; parameter of &lt;code&gt;TweedieRegressor&lt;/code&gt;, it is advisable to specify an explicit &lt;code&gt;scoring&lt;/code&gt; function, because the default scorer &lt;a href=&quot;generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor.score&quot;&gt;&lt;code&gt;TweedieRegressor.score&lt;/code&gt;&lt;/a&gt; is a function of &lt;code&gt;power&lt;/code&gt; itself.</source>
          <target state="translated">에 대한 교차 검증을 수행 할 때 &lt;code&gt;power&lt;/code&gt; 의 매개 변수 &lt;code&gt;TweedieRegressor&lt;/code&gt; , 명시 적으로 지정하는 것이 좋습니다 &lt;code&gt;scoring&lt;/code&gt; 득점 기본 있기 때문에, 기능을 &lt;a href=&quot;generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor.score&quot;&gt; &lt;code&gt;TweedieRegressor.score&lt;/code&gt; 이&lt;/a&gt; 의 함수 &lt;code&gt;power&lt;/code&gt; 자체가.</target>
        </trans-unit>
        <trans-unit id="e99cb78745b2020137c83400ee3d8d22f37293c0" translate="yes" xml:space="preserve">
          <source>When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown.</source>
          <target state="translated">사전 계산 거리의 경우 데이터를 먼저 중앙에 배치하는 것이 더 정확합니다. copy_x가 True (기본값)이면 원본 데이터는 수정되지 않고 X가 C 연속적입니다. False 인 경우 원래 데이터가 수정되고 함수가 리턴되기 전에 되돌려 지지만 데이터 평균을 빼고 추가하여 작은 숫자 차이가 발생할 수 있습니다.이 경우 데이터가 C 연속적임을 보장하지 않습니다. 상당한 속도 저하.</target>
        </trans-unit>
        <trans-unit id="785dafae54d31c04c4a974bc00da768d9cfae46f" translate="yes" xml:space="preserve">
          <source>When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False.</source>
          <target state="translated">거리를 미리 계산할 때 먼저 데이터를 중앙에 배치하는 것이 수치 적으로 더 정확합니다. copy_x가 True (기본값)이면 원본 데이터가 수정되지 않습니다. False 인 경우 원래 데이터가 수정되고 함수가 반환되기 전에 되돌려 지지만 데이터 평균을 뺀 다음 더하면 작은 수치 차이가 발생할 수 있습니다. 원본 데이터가 C- 연속이 아닌 경우 copy_x가 False 인 경우에도 복사본이 만들어집니다. 원본 데이터가 희소하지만 CSR 형식이 아닌 경우 copy_x가 False 인 경우에도 복사본이 만들어집니다.</target>
        </trans-unit>
        <trans-unit id="55ac178846c6596090a6f8d0133ba62fcd26aebb" translate="yes" xml:space="preserve">
          <source>When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.</source>
          <target state="translated">예측할 때 실제 레이블을 사용할 수 없습니다. 대신 각 모델의 예측이 체인의 후속 모델로 전달되어 피처로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="441463f4c28b96b4ca0739417ce251a6d1637336" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt;.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합이 특징의 임의의 부분 집합으로 그려 질 때, 방법은 무작위 부분 공간 &lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt; 으로 알려져있다 .</target>
        </trans-unit>
        <trans-unit id="ed3a388f4c5d9809937b1b2fceecd5d5d41437fc" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]&lt;/a&gt;.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합이 샘플의 임의의 부분 집합으로 그려 질 때이 알고리즘을 붙여 넣기 &lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="f494cf947867cf3181ff2c5eaa996adc8d0ce783" translate="yes" xml:space="preserve">
          <source>When requesting a dataset with a name that is in mock_datasets, this object creates a fake dataset in a StringIO object and returns it. Otherwise, it raises an HTTPError.</source>
          <target state="translated">이름이 mock_datasets 인 데이터 집합을 요청하면이 개체는 StringIO 개체에 가짜 데이터 집합을 만들어 반환합니다. 그렇지 않으면 HTTPError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="f6f6725ccb736a3cf59a107563029dd1f92b7eb9" translate="yes" xml:space="preserve">
          <source>When sample_weight is provided, the selected hyperparameter may depend on whether we use generalized cross-validation (cv=None or cv=&amp;rsquo;auto&amp;rsquo;) or another form of cross-validation, because only generalized cross-validation takes the sample weights into account when computing the validation score.</source>
          <target state="translated">sample_weight가 제공되면 일반화 된 교차 검증 (cv = None 또는 cv = 'auto')을 사용하는지 아니면 다른 형태의 교차 검증을 사용하는지에 따라 선택한 하이퍼 파라미터가 달라질 수 있습니다. 검증 점수 계산.</target>
        </trans-unit>
        <trans-unit id="d66c9681351cfa02a7cc3145d8e8cc7e7a3877b3" translate="yes" xml:space="preserve">
          <source>When samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]&lt;/a&gt;.</source>
          <target state="translated">샘플을 교체하여 채취 할 때이 방법을 배깅 (Bagging) &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]이라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="b67d588266cee585b7c0608db5b116728771921f" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt;, i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">self.fit_intercept가 True 인 경우 인스턴스 벡터 x는 &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt; . 즉, intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 intercept_scaling * 합성 피처 중량이됩니다. 참고! 합성 피처 중량은 다른 모든 피처와 마찬가지로 l1 / l2 정규화에 적용됩니다. 합성 피처 가중치 (따라서 인터셉트)에 대한 정규화의 영향을 줄이려면 intercept_scaling을 늘려야합니다.</target>
        </trans-unit>
        <trans-unit id="339fba0d20ce2052ad9daa9e3c6cc55589d832bc" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">self.fit_intercept가 True 인 경우 인스턴스 벡터 x는 [x, self.intercept_scaling]이됩니다. 즉, intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 intercept_scaling * 합성 피처 중량이됩니다. 참고! 합성 피처 중량은 다른 모든 피처와 마찬가지로 l1 / l2 정규화에 적용됩니다. 합성 피처 가중치 (따라서 인터셉트)에 대한 정규화의 영향을 줄이려면 intercept_scaling을 늘려야합니다.</target>
        </trans-unit>
        <trans-unit id="ad06c0ce7c1fc91387cf888768a953ddfc43c4b6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;False&lt;/code&gt;, ignore special characters for PostScript compatibility.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 로 설정되면 PostScript 호환성을 위해 특수 문자를 무시하십시오.</target>
        </trans-unit>
        <trans-unit id="d1b93df55570608a785000a0ebdde0c1a478b680" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, change the display of &amp;lsquo;values&amp;rsquo; and/or &amp;lsquo;samples&amp;rsquo; to be proportions and percentages respectively.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 '값'및 / 또는 '샘플'표시가 각각 비율 및 백분율로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="3843d20a53a1ef3b59460be1fa7cb77d3c2ee1f6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw all leaf nodes at the bottom of the tree.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 트리의 맨 아래에 모든 리프 노드를 그립니다.</target>
        </trans-unit>
        <trans-unit id="1dd8441b2d9ea649f69d55eb070005c43dff3ea1" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정하면 모서리가 둥근 노드 상자를 그리고 Times-Roman 대신 Helvetica 글꼴을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="37e5e7c90dcc2881b20280bd58a636a2601aa0c6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, forces the coefficients to be positive.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 계수가 양수가됩니다.</target>
        </trans-unit>
        <trans-unit id="a556178389cf10e9a8f97ab94b5cbc137168d877" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, orient tree left to right rather than top-down.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 트리가 하향식이 아닌 왼쪽에서 오른쪽으로 향합니다.</target>
        </trans-unit>
        <trans-unit id="db2d0a81092e78238cdb916143e482a964d5a789" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정하면 분류를위한 대다수 클래스, 회귀 값의 극단 또는 다중 출력에 대한 노드 순도를 나타내도록 노드를 페인트합니다.</target>
        </trans-unit>
        <trans-unit id="387704ecb7f5fc4e8c941c08ae18f72e58104663" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="42b61336d9e93453cd22782ddee266aae253af56" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 다시 사용하여 앙상블에 더 많은 추정자를 맞추고 추가합니다. 그렇지 않으면 이전 솔루션을 지 웁니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e08becb1d7f3dabb059c61e3163efbfc5e9d6bd5" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하고 그렇지 않으면 완전히 새로운 포리스트에 맞 춥니 다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="4ca94f8cc22fb05a614eaf938146928a9cad01e1" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 다시 사용하여 앙상블에 더 많은 추정자를 맞추고 추가합니다. 그렇지 않으면 완전히 새로운 포리스트에 적합합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b034ef6cd843a9c71bbf2f16594203b3e34675bd" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble. For results to be valid, the estimator should be re-trained on the same data only. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 다시 사용하여 앙상블에 더 많은 추정량을 추가합니다. 결과가 유효하려면 동일한 데이터에 대해서만 추정기를 재 학습해야합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1ddb18c96e1fca0312edb00f224f2db160c80a30" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="06f93e3e87621fa903e7cdd20131cea02e9878ba" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 다시 사용하여 초기화에 맞 춥니 다. 그렇지 않으면 이전 솔루션을 지 웁니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="303e5cc9af6656c2592224b864d50a2e7f014b54" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the ID number on each node.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 각 노드에서 ID 번호를 표시하십시오.</target>
        </trans-unit>
        <trans-unit id="d19057d05dc608969ed0862b9054b2defc3fb482" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the impurity at each node.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 각 노드에서 불순물을 표시하십시오.</target>
        </trans-unit>
        <trans-unit id="ee8efaddcdb7929ada76230c032dd700897cd47f" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights accross all updates and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches &lt;code&gt;average&lt;/code&gt;. So &lt;code&gt;average=10&lt;/code&gt; will begin averaging after seeing 10 samples.</source>
          <target state="translated">True로 설정하면 모든 업데이트에 대한 평균 SGD 가중치를 계산하고 결과를 &lt;code&gt;coef_&lt;/code&gt; 속성 에 저장 합니다. 1보다 큰 int로 설정하면 총 샘플 수가 &lt;code&gt;average&lt;/code&gt; 에 도달하면 평균화가 시작됩니다 . 따라서 &lt;code&gt;average=10&lt;/code&gt; 은 10 개의 샘플을 본 후 평균화를 시작합니다.</target>
        </trans-unit>
        <trans-unit id="0199409f1eeb1aa7581c717d5c23e182af166761" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So &lt;code&gt;average=10&lt;/code&gt; will begin averaging after seeing 10 samples.</source>
          <target state="translated">True로 설정하면 평균 SGD 가중치를 계산하고 결과를 &lt;code&gt;coef_&lt;/code&gt; 속성 에 저장 합니다. int를 1보다 큰 값으로 설정하면 표시된 총 샘플 수가 평균에 도달하면 평균화가 시작됩니다. 따라서 &lt;code&gt;average=10&lt;/code&gt; 은 10 개의 샘플을 본 후 평균화되기 시작합니다.</target>
        </trans-unit>
        <trans-unit id="19c4ca669b90d48df2f3aa161b89103ea08c0cbd" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.</source>
          <target state="translated">True로 설정하면 평균 SGD 가중치를 계산하고 결과를 &lt;code&gt;coef_&lt;/code&gt; 속성 에 저장 합니다. int를 1보다 큰 값으로 설정하면 표시된 총 샘플 수가 평균에 도달하면 평균화가 시작됩니다. 따라서 평균 = 10은 10 개의 샘플을 본 후 평균화되기 시작합니다.</target>
        </trans-unit>
        <trans-unit id="e60b70114bd43f63c876204c608aaaaca2e5c6d2" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하고 그렇지 않으면 완전히 새로운 앙상블에 맞 춥니 다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="fb1187119affc46cd36d1e1403f76ef562249b84" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 다시 사용하여 앙상블에 더 많은 추정자를 맞추고 추가합니다. 그렇지 않으면 완전히 새로운 앙상블에 적합합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0615ecfccfe9c12eea86adfdd92570d0b3a6f9cf" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b9ddbc0f60da434a4d403c500c89b585a3aeef7d" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 다시 사용하여 초기화에 맞 춥니 다. 그렇지 않으면 이전 솔루션을 지 웁니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="869683b3c71be56286e995de01f21c989fc735d8" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. liblinear 솔버에는 쓸모가 없습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="90df9a75b07a1010dc74b460f30a26d769910e81" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 다시 사용하여 초기화에 맞 춥니 다. 그렇지 않으면 이전 솔루션을 지 웁니다. liblinear 솔버에는 쓸모가 없습니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="61362ce0a02977970071e88e9dc71d066251fcde" translate="yes" xml:space="preserve">
          <source>When specifying multiple metrics, the &lt;code&gt;refit&lt;/code&gt; parameter must be set to the metric (string) for which the &lt;code&gt;best_params_&lt;/code&gt; will be found and used to build the &lt;code&gt;best_estimator_&lt;/code&gt; on the whole dataset. If the search should not be refit, set &lt;code&gt;refit=False&lt;/code&gt;. Leaving refit to the default value &lt;code&gt;None&lt;/code&gt; will result in an error when using multiple metrics.</source>
          <target state="translated">여러 통계를 지정하는 경우 &lt;code&gt;refit&lt;/code&gt; 매개 변수가있는 메트릭 (문자열)로 설정해야합니다 &lt;code&gt;best_params_&lt;/code&gt; 가 발견하고 구축하는 데 사용됩니다 &lt;code&gt;best_estimator_&lt;/code&gt; 전체 데이터 세트에 있습니다. 검색을 다시 작성하지 않으려면 &lt;code&gt;refit=False&lt;/code&gt; 로 설정하십시오 . 여러 값을 사용하는 경우 기본값을 '재설정 &lt;code&gt;None&lt;/code&gt; '으로두면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="8303fa1cd1689a65bba6fc9b59c3502d4f370e6f" translate="yes" xml:space="preserve">
          <source>When starting from the default values (alpha_init = 1.90, lambda_init = 1.), the bias of the resulting curve is large, and the variance is small. So, lambda_init should be relatively small (1.e-3) so as to reduce the bias.</source>
          <target state="translated">기본값 (alpha_init = 1.90, lambda_init = 1)에서 시작할 때 결과 곡선의 치우침이 크고 분산이 작습니다. 따라서 lambda_init는 편향을 줄이기 위해 상대적으로 작아야합니다 (1.e-3).</target>
        </trans-unit>
        <trans-unit id="765386eefc1dd2e9ed203024ee007c99e07f6618" translate="yes" xml:space="preserve">
          <source>When strategy == &amp;ldquo;constant&amp;rdquo;, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and &amp;ldquo;missing_value&amp;rdquo; for strings or object data types.</source>
          <target state="translated">strategy ==&amp;ldquo;constant&amp;rdquo;인 경우 fill_value를 사용하여 missing_value의 모든 발생을 대체합니다. 기본값을 그대로두면 숫자 데이터를 대치 할 때 fill_value는 0이되고 문자열 또는 객체 데이터 형식은 &quot;missing_value&quot;가됩니다.</target>
        </trans-unit>
        <trans-unit id="234c27f3e17ce7a8cf496679483a69e65bcabb05" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;Pipeline&lt;/code&gt; is printed out in a jupyter notebook an HTML representation of the estimator is displayed as follows:</source>
          <target state="translated">때 &lt;code&gt;Pipeline&lt;/code&gt; jupyter 노트북에서 인쇄 다음과 같이 추정의 HTML 표현이 표시됩니다 :</target>
        </trans-unit>
        <trans-unit id="b3973bbeeefced2bd7eb8aaa05fcc0fadc5e600b" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;cv&lt;/code&gt; argument is an integer, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; uses the &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; strategies by default, the latter being used if the estimator derives from &lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt;&lt;code&gt;ClassifierMixin&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;cv&lt;/code&gt; 인수가 정수, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 용도 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; , 기본적으로부터 추정의 도출 경우 사용되는 후자 전략을 &lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt; &lt;code&gt;ClassifierMixin&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c7d238875ff93bafa2620f7a2d7675b937a2183b" translate="yes" xml:space="preserve">
          <source>When the algorithm does not converge, it returns an empty array as &lt;code&gt;cluster_center_indices&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; as label for each training sample.</source>
          <target state="translated">알고리즘이 수렴하지 않으면 빈 배열을 &lt;code&gt;cluster_center_indices&lt;/code&gt; 로 , &lt;code&gt;-1&lt;/code&gt; 을 각 학습 샘플의 레이블 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b4fdfb364ee084bf57bd04a0f7c8f0c41739edf4" translate="yes" xml:space="preserve">
          <source>When the data is not initially in the &lt;code&gt;(n_samples, n_features)&lt;/code&gt; shape, it needs to be preprocessed in order to be used by scikit-learn.</source>
          <target state="translated">데이터가 처음에 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 모양이 아닌 경우 scikit-learn에서 사용하려면 데이터를 사전 처리해야합니다.</target>
        </trans-unit>
        <trans-unit id="4b1639b675f158aa2d42c71150242e159f59e266" translate="yes" xml:space="preserve">
          <source>When the missingness pattern is predictive, the splits can be done on whether the feature value is missing or not:</source>
          <target state="translated">누락 패턴이 예측 적이면 특성 값이 누락되었는지 여부에 대해 분할을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90e4401cde0f238342524a25385b7071cd57b4a0" translate="yes" xml:space="preserve">
          <source>When the underlying implementation uses joblib, the number of workers (threads or processes) that are spawned in parallel can be controlled via the &lt;code&gt;n_jobs&lt;/code&gt; parameter.</source>
          <target state="translated">기본 구현이 joblib를 사용하는 경우 병렬로 생성되는 작업자 (스레드 또는 프로세스) 수는 &lt;code&gt;n_jobs&lt;/code&gt; 매개 변수 를 통해 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="aaeec02a52e8579f06c37808c9e18fa50d637a08" translate="yes" xml:space="preserve">
          <source>When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1.</source>
          <target state="translated">라벨이 두 개 이상인 경우 MCC 값은 더 이상 -1과 +1 사이가 아닙니다. 대신 최소값은 실제 라벨의 수와 분포에 따라 -1과 0 사이입니다. 최대 값은 항상 +1입니다.</target>
        </trans-unit>
        <trans-unit id="5c69ffd1e0dc71befa67c00726bc6370582b5df5" translate="yes" xml:space="preserve">
          <source>When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased.</source>
          <target state="translated">출력간에 상관 관계가없는 경우 이러한 종류의 문제를 해결하는 매우 간단한 방법은 n 개의 독립적 인 모델, 즉 각 출력에 대한 모델을 만든 다음 해당 모델을 사용하여 n 개의 출력 각각을 독립적으로 예측하는 것입니다. 그러나 동일한 입력과 관련된 출력 값이 서로 관련되어있을 가능성이 높기 때문에 n 개의 모든 출력을 동시에 예측할 수있는 단일 모델을 구축하는 것이 더 좋은 방법입니다. 첫째, 하나의 추정기만 구축되므로 교육 시간이 단축됩니다. 둘째, 결과 추정기의 일반화 정확도가 종종 증가 될 수있다.</target>
        </trans-unit>
        <trans-unit id="ebcc7b732996f47559f9fb2cca7527f873584d5f" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikit-learn and joblib need to be compatible. Currently, joblib 0.11+ is supported. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets.</source>
          <target state="translated">이 환경 변수가 0이 아닌 값으로 설정되면 scikit-learn은 벤더 버전이 아닌 사이트 joblib를 사용합니다. 따라서 scikit-learn을 실행하려면 joblib를 설치해야합니다. 사이트 joblib를 사용하는 것은 사용자의 책임입니다. scikit-learn 및 joblib의 버전이 호환되어야합니다. 현재 joblib 0.11+가 지원됩니다. 또한 joblib.Memory의 덤프가 호환되지 않을 수 있으며 일부 캐시가 손실되어 일부 데이터 세트를 다시 다운로드해야 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c7fa59e2b8c1ce3abb542d1d5f76caddd1816f1" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikt-learn and joblib need to be compatible. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets.</source>
          <target state="translated">이 환경 변수가 0이 아닌 값으로 설정되면 scikit-learn은 공급 업체 버전이 아닌 사이트 joblib를 사용합니다. 따라서 scikit-learn을 실행하려면 joblib이 설치되어 있어야합니다. 사이트 joblib를 사용하는 것은 사용자 자신의 책임입니다. scikt-learn 및 joblib의 버전은 호환 가능해야합니다. 또한 joblib.Memory의 덤프가 호환되지 않을 수 있으며 일부 캐시를 느슨하게하고 일부 데이터 세트를 다시 다운로드해야 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="743d4762d28a3d61488ddf12d10bce762b152657" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, the tests that need network access are skipped.</source>
          <target state="translated">이 환경 변수가 0이 아닌 값으로 설정되면 네트워크 액세스가 필요한 테스트를 건너 뜁니다.</target>
        </trans-unit>
        <trans-unit id="beb3490e0a85d3bcf9f4888cb75a6b1ea2e1e6a8" translate="yes" xml:space="preserve">
          <source>When training an SVM with the &lt;em&gt;Radial Basis Function&lt;/em&gt; (RBF) kernel, two parameters must be considered: &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. The parameter &lt;code&gt;C&lt;/code&gt;, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low &lt;code&gt;C&lt;/code&gt; makes the decision surface smooth, while a high &lt;code&gt;C&lt;/code&gt; aims at classifying all training examples correctly. &lt;code&gt;gamma&lt;/code&gt; defines how much influence a single training example has. The larger &lt;code&gt;gamma&lt;/code&gt; is, the closer other examples must be to be affected.</source>
          <target state="translated">RBF ( &lt;em&gt;Radial Basis Function&lt;/em&gt; ) 커널을 사용 하여 SVM을 학습 할 때는 두 개의 매개 변수 &lt;code&gt;C&lt;/code&gt; 와 &lt;code&gt;gamma&lt;/code&gt; 를 고려해야합니다 . 모든 SVM 커널에 공통적 인 매개 변수 &lt;code&gt;C&lt;/code&gt; 는 의사 결정 표면의 단순성에 대한 교육 예제의 오 분류를 제거합니다. &lt;code&gt;C&lt;/code&gt; 가 낮 으면 결정 표면이 매끄럽고 &lt;code&gt;C&lt;/code&gt; 가 높으면 모든 교육 예제를 올바르게 분류하는 것입니다. &lt;code&gt;gamma&lt;/code&gt; 는 단일 훈련 예제가 얼마나 많은 영향을 미치는지를 정의합니다. &lt;code&gt;gamma&lt;/code&gt; 가 클수록 다른 예제에 더 가깝게 영향을 주어야합니다.</target>
        </trans-unit>
        <trans-unit id="4f5d7a3d8a7ab119798fbec5ead8471db219e63d" translate="yes" xml:space="preserve">
          <source>When true, the result is adjusted for chance, so that random performance would score 0, and perfect performance scores 1.</source>
          <target state="translated">true 인 경우 결과는 확률에 따라 조정되므로 임의의 성능은 0, 완벽한 성능은 1입니다.</target>
        </trans-unit>
        <trans-unit id="7dbd1175230450ef2444fe8de9e5557001f2473c" translate="yes" xml:space="preserve">
          <source>When truncated SVD is applied to term-document matrices (as returned by &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt;), this transformation is known as &lt;a href=&quot;https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;latent semantic analysis&lt;/a&gt; (LSA), because it transforms such matrices to a &amp;ldquo;semantic&amp;rdquo; space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.</source>
          <target state="translated">잘린 SVD가 용어 문서 행렬 ( &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 에서&lt;/a&gt; 반환 됨 )에 적용되는 경우이 변환을 LSA ( &lt;a href=&quot;https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;잠재 의미 분석&lt;/a&gt; )라고합니다. 이러한 행렬을 낮은 차원의 &quot;의미 적&quot;공간으로 변환하기 때문입니다. 특히 LSA는 동의어와 다의어 (둘 다 단어 당 여러 의미가 있음을 대략적으로 의미 함)의 효과와 싸우는 것으로 알려져 있으며, 이는 용어 문서 행렬이 지나치게 희박하고 코사인 유사성과 같은 측정에서 낮은 유사성을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="a756308318fb23f07be0498c8c83a0d088f9279a" translate="yes" xml:space="preserve">
          <source>When truncated SVD is applied to term-document matrices (as returned by &lt;code&gt;CountVectorizer&lt;/code&gt; or &lt;code&gt;TfidfVectorizer&lt;/code&gt;), this transformation is known as &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;latent semantic analysis&lt;/a&gt; (LSA), because it transforms such matrices to a &amp;ldquo;semantic&amp;rdquo; space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.</source>
          <target state="translated">잘린 SVD가 용어 문서 행렬에 적용되면 ( &lt;code&gt;CountVectorizer&lt;/code&gt; 또는 &lt;code&gt;TfidfVectorizer&lt;/code&gt; 에 의해 반환 됨 )이 변환은 이러한 행렬을 낮은 차원의 &quot;의미 적&quot;공간으로 변환하기 때문에 LSA ( &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;잠재적 의미 분석&lt;/a&gt; )라고합니다. 특히, LSA는 동의어와 다 가공의 효과 (단어 단어 당 여러 의미가 있음을 의미 함)와 싸우는 것으로 알려져 있으며, 이로 인해 용어 문서 행렬이 지나치게 희박 해지고 코사인 유사성 등의 측정에서 유사성이 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="38dbd2ff8901fd68988a77a08c59409f80468575" translate="yes" xml:space="preserve">
          <source>When two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might &lt;em&gt;actually&lt;/em&gt; be important.</source>
          <target state="translated">두 기능이 상호 연관되고 기능 중 하나가 치환 된 경우 모델은 상관 된 기능을 통해 기능에 계속 액세스 할 수 있습니다. 이로 인해 &lt;em&gt;실제로&lt;/em&gt; 중요 할 수있는 두 기능의 중요도 값이 낮아집니다 .</target>
        </trans-unit>
        <trans-unit id="726430099b436b4edbed2772b4e46aec59296fe0" translate="yes" xml:space="preserve">
          <source>When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier.</source>
          <target state="translated">tf-idf 벡터로 텍스트 분류에 사용될 때이 분류기는 Rocchio 분류기로도 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="b1373e66b4937bbac8defef2fa925bed61a8d596" translate="yes" xml:space="preserve">
          <source>When used to &lt;em&gt;transform&lt;/em&gt; data, PCA can reduce the dimensionality of the data by projecting on a principal subspace.</source>
          <target state="translated">데이터 를 &lt;em&gt;변환&lt;/em&gt; 하는 데 사용될 때 PCA는 기본 서브 스페이스에 투영하여 데이터의 차원을 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="93429c223efcd8d6622a4c8e5f0e71f13358e226" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;multiclass classifiers&lt;/code&gt;&lt;/a&gt;, the learning and prediction task that is performed is dependent on the format of the target data fit upon:</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;multiclass classifiers&lt;/code&gt; &lt;/a&gt; 사용할 때 수행되는 학습 및 예측 작업은 다음에 맞는 대상 데이터의 형식에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="bf430655a414d830d0ddbbfedf78ab7c208919a9" translate="yes" xml:space="preserve">
          <source>When using Averaged SGD (with the &lt;code&gt;average&lt;/code&gt; parameter), &lt;code&gt;coef_&lt;/code&gt; is set to the average weight across all updates: &lt;code&gt;coef_&lt;/code&gt;\(= \frac{1}{T} \sum_{t=0}^{T-1} w^{(t)}\), where \(T\) is the total number of updates, found in the &lt;code&gt;t_&lt;/code&gt; attribute.</source>
          <target state="translated">Averaged SGD ( &lt;code&gt;average&lt;/code&gt; 매개 변수 포함)를 사용하는 경우 &lt;code&gt;coef_&lt;/code&gt; 는 모든 업데이트의 평균 가중치로 설정됩니다. &lt;code&gt;coef_&lt;/code&gt; \ (= \ frac {1} {T} \ sum_ {t = 0} ^ {T-1} w ^ { (t)} \), 여기서 \ (T \)는 &lt;code&gt;t_&lt;/code&gt; 속성 에서 찾은 총 업데이트 수입니다 .</target>
        </trans-unit>
        <trans-unit id="cb56aa339cbb48c9a75d42d2c7bf7e7858b3170b" translate="yes" xml:space="preserve">
          <source>When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.</source>
          <target state="translated">배깅을 기반으로 앙상블 방법을 사용할 때, 즉 교체와 함께 샘플링을 사용하여 새 훈련 세트를 생성하는 경우 훈련 세트의 일부는 사용되지 않습니다. 앙상블의 각 분류 자에 대해 훈련 세트의 다른 부분이 제외됩니다.</target>
        </trans-unit>
        <trans-unit id="7a60d247990a32a468cae00d5ed7b0c825a81e35" translate="yes" xml:space="preserve">
          <source>When using the &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; in a &lt;code&gt;Pipeline&lt;/code&gt;, be sure to use the &lt;code&gt;FeatureUnion&lt;/code&gt; or &lt;code&gt;ColumnTransformer&lt;/code&gt; to add the indicator features to the regular features. First we obtain the &lt;code&gt;iris&lt;/code&gt; dataset, and add some missing values to it.</source>
          <target state="translated">사용하는 경우 &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;MissingIndicator&lt;/code&gt; &lt;/a&gt; A의 &lt;code&gt;Pipeline&lt;/code&gt; 의 사용하십시오 &lt;code&gt;FeatureUnion&lt;/code&gt; 또는 &lt;code&gt;ColumnTransformer&lt;/code&gt; 을 표시등이 일반 기능에 기능을 추가 할 수 있습니다. 먼저 &lt;code&gt;iris&lt;/code&gt; 데이터 셋 을 얻고 여기에 누락 된 값을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="a9696826aefafecc5b32845bc270f3c2cabe6664" translate="yes" xml:space="preserve">
          <source>When using these images, please give credit to AT&amp;amp;T Laboratories Cambridge.</source>
          <target state="translated">이 이미지를 사용할 때는 AT &amp;amp; T Laboratories Cambridge에 감사의 뜻을 전하십시오.</target>
        </trans-unit>
        <trans-unit id="5a6f9e7437ff7d6762455e24a46c4771dc2e0a37" translate="yes" xml:space="preserve">
          <source>When using, for example, &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;, to set the amount of regularization with &lt;code&gt;C&lt;/code&gt;, there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation.</source>
          <target state="translated">예를 들어, &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 을 사용하여 &lt;code&gt;C&lt;/code&gt; 로 정규화 양을 설정 하면 교차 검증의 접힘 내에서 주요 문제와 작은 문제 사이에 다른 양의 샘플이 있습니다.</target>
        </trans-unit>
        <trans-unit id="5dae0cb2a4d8c33bb8e68ee9ffd452db73e27eb2" translate="yes" xml:space="preserve">
          <source>When we apply clustering to the data, we find that the clustering reflects what was in the distance matrices. Indeed, for the Euclidean distance, the classes are ill-separated because of the noise, and thus the clustering does not separate the waveforms. For the cityblock distance, the separation is good and the waveform classes are recovered. Finally, the cosine distance does not separate at all waveform 1 and 2, thus the clustering puts them in the same cluster.</source>
          <target state="translated">데이터에 군집화를 적용하면 군집화에 거리 행렬의 내용이 반영됩니다. 실제로 유클리드 거리의 경우 노이즈로 인해 클래스가 잘못 분리되므로 클러스터링이 파형을 분리하지 않습니다. 도시 블록 거리의 경우 분리가 양호하고 파형 클래스가 복구됩니다. 마지막으로 코사인 거리는 모든 파형 1과 2에서 분리되지 않으므로 클러스터링은 동일한 클러스터에 배치합니다.</target>
        </trans-unit>
        <trans-unit id="8975b7dd097c19a6af3c2b4b090f357f96aab52d" translate="yes" xml:space="preserve">
          <source>When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the &lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt;&lt;/a&gt;. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in &lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;Shrunk Covariance&lt;/a&gt; estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.</source>
          <target state="translated">공분산 추정으로 작업 할 때 일반적인 접근 방식은 &lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 와 같은 최대 가능성 추정기를 사용하는 것 입니다. 그것은 편향되지 않으며, 즉 많은 관측치가 주어지면 실제 (인구) 공분산으로 수렴합니다. 그러나 분산을 줄이려면 정규화하는 것이 좋습니다. 이것은 차례로 약간의 편견을 가져옵니다. 이 예는 &lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;Shrunk Covariance&lt;/a&gt; Estimators 에서 사용되는 간단한 정규화를 보여줍니다 . 특히, 정규화의 양을 설정하는 방법, 즉 바이어스-분산 트레이드 오프를 선택하는 방법에 중점을 둡니다.</target>
        </trans-unit>
        <trans-unit id="17b704aa73a46ef6f9edddecff620d33c0b705d7" translate="yes" xml:space="preserve">
          <source>When you want to apply different transformations to each field of the data, see the related class &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;#column-transformer&quot;&gt;user guide&lt;/a&gt;).</source>
          <target state="translated">데이터의 각 필드에 다른 변환을 적용하려면 관련 클래스 &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 ( &lt;a href=&quot;#column-transformer&quot;&gt;사용자 안내서&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="8d5450715de6c511faca4e5034a6c5d189b2299d" translate="yes" xml:space="preserve">
          <source>Where (and how) parallelization happens in the estimators is currently poorly documented. Please help us by improving our docs and tackle &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/issues/14228&quot;&gt;issue 14228&lt;/a&gt;!</source>
          <target state="translated">추정기에서 병렬화가 발생하는 위치 (및 방법)는 현재 제대로 문서화되어 있지 않습니다. 문서를 개선하고 &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/issues/14228&quot;&gt;문제 14228을&lt;/a&gt; 해결하여 도와주세요 !</target>
        </trans-unit>
        <trans-unit id="ffd889b6ef09600260c419603787a00c67ae551d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).</source>
          <target state="translated">어디 &lt;code&gt;TP&lt;/code&gt; 가 의 수입니다 &lt;strong&gt;진정한 포지티브&lt;/strong&gt; (즉, 진정한 레이블과 예측 라벨 모두 동일한 클러스터에 속하는 점 쌍의 수), &lt;code&gt;FP&lt;/code&gt; 는 의 수 &lt;strong&gt;거짓 양성&lt;/strong&gt; (속하는 점의 쌍, 즉 수 진정한 라벨에서가 아니라 예측 라벨에서)와 같은 클러스터 &lt;code&gt;FN&lt;/code&gt; 의 수 &lt;strong&gt;위음성&lt;/strong&gt; (예측 된 라벨에서가 아니라) 실제 라벨에서 동일한 클러스터에 속하는 점의 쌍, 즉 수.</target>
        </trans-unit>
        <trans-unit id="276f699fa82da6208d110c0a23b40d61550922dd" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in both &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt;), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_true&lt;/code&gt; and not in &lt;code&gt;labels_pred&lt;/code&gt;) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_pred&lt;/code&gt; and not in &lt;code&gt;labels_True&lt;/code&gt;).</source>
          <target state="translated">어디 &lt;code&gt;TP&lt;/code&gt; 가 의 수입니다 &lt;strong&gt;진정한 양성&lt;/strong&gt; (모두 같은 클러스터에 속하는 점의 쌍, 즉 수 &lt;code&gt;labels_true&lt;/code&gt; 및 &lt;code&gt;labels_pred&lt;/code&gt; 가 ), &lt;code&gt;FP&lt;/code&gt; 는 의 수를 &lt;strong&gt;거짓 긍정적&lt;/strong&gt; 동일한 클러스터에 속하는 (점의 쌍, 즉 수 에서 &lt;code&gt;labels_true&lt;/code&gt; 및하지에서 &lt;code&gt;labels_pred&lt;/code&gt; )와 &lt;code&gt;FN&lt;/code&gt; 은 의 수 &lt;strong&gt;위음성&lt;/strong&gt; (동일한 클러스터에 속하는 점의 쌍, 즉 수 &lt;code&gt;labels_pred&lt;/code&gt; 및하지에서 &lt;code&gt;labels_True&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e78195e2eb2711f3bb8a0d7f4e3c56eea492c48d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;delta&lt;/code&gt; is a free parameter representing the width of the Gaussian kernel.</source>
          <target state="translated">여기서 &lt;code&gt;delta&lt;/code&gt; 는 가우스 커널의 너비를 나타내는 자유 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="6d88fb8179777bb060d39d8e880a1a6ec89efb59" translate="yes" xml:space="preserve">
          <source>Where C is the number of permutations whose score &amp;gt;= the true score.</source>
          <target state="translated">여기서 C는 점수&amp;gt; = 실제 점수 인 순열 수입니다.</target>
        </trans-unit>
        <trans-unit id="0426d1b8d26623c0079356962f68cf2595f6d67a" translate="yes" xml:space="preserve">
          <source>Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel:</source>
          <target state="translated">여기서 D는 입력 데이터 X에 대한 거리의 행렬이고, D_fit은 X_fit을 포함하는 출력에 대한 거리의 행렬이며, K는 아이소 맵 커널입니다.</target>
        </trans-unit>
        <trans-unit id="77844a8258430d31f41a5c27fd5c3c817a4c46f5" translate="yes" xml:space="preserve">
          <source>Where \(C_2^{n_{samples}}\) is the total number of possible pairs in the dataset (without ordering).</source>
          <target state="translated">여기서 \ (C_2 ^ {n_ {samples}} \)는 데이터 세트에서 가능한 순서의 총 수입니다 (순서없이).</target>
        </trans-unit>
        <trans-unit id="45f9706f8e40bfee3b8f4082279b7c1694d8aead" translate="yes" xml:space="preserve">
          <source>Where \(K\) is the precision matrix to be estimated, and \(S\) is the sample covariance matrix. \(\|K\|_1\) is the sum of the absolute values of off-diagonal coefficients of \(K\). The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">여기서 \ (K \)는 추정 할 정밀 행렬이고 \ (S \)는 샘플 공분산 행렬입니다. \ (\ | K \ | _1 \)는 \ (K \)의 대각 외 계수의 절대 값의 합입니다. 이 문제를 해결하기 위해 사용 된 알고리즘은 Friedman 2008 Biostatistics 논문의 GLasso 알고리즘입니다. R &lt;code&gt;glasso&lt;/code&gt; 패키지 와 동일한 알고리즘 입니다.</target>
        </trans-unit>
        <trans-unit id="23038cc6fb25ab648004d5485267f6db76cb9eda" translate="yes" xml:space="preserve">
          <source>Where \(N(x_i)\) is the neighborhood of samples within a given distance around \(x_i\) and \(m\) is the &lt;em&gt;mean shift&lt;/em&gt; vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:</source>
          <target state="translated">여기서 \ (N (x_i) \)는 \ (x_i \) 주위의 주어진 거리 내의 샘플의 이웃이고 \ (m \)은 최대 증가 영역을 가리키는 각 중심에 대해 계산 된 &lt;em&gt;평균 이동&lt;/em&gt; 벡터입니다. 점의 밀도. 이것은 다음 방정식을 사용하여 계산되며, 중심 근처를 효과적으로 주변의 샘플 평균으로 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="c4be16a40b65a723b122e1219966e4db066c1f56" translate="yes" xml:space="preserve">
          <source>Where \(R\) is the diagonal matrix with entry \(i\) equal to \(\sum_{j} A_{ij}\) and \(C\) is the diagonal matrix with entry \(j\) equal to \(\sum_{i} A_{ij}\).</source>
          <target state="translated">여기서 \ (R \)은 \ (i \) 항목이 \ (\ sum_ {j} A_ {ij} \)와 같은 대각 행렬이고 \ (C \)는 \ (j \) 항목이 같은 대각 행렬입니다. \ (\ sum_ {i} A_ {ij} \)까지</target>
        </trans-unit>
        <trans-unit id="06bd15907339a321f45a7707ee037e3bf4bb294c" translate="yes" xml:space="preserve">
          <source>Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the Hilbert space.</source>
          <target state="translated">여기서 \ (\ langle \ cdot, \ cdot \ rangle \)은 힐버트 공간의 내부 제품을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="b505305e3f68e0055136674f6671623549265da7" translate="yes" xml:space="preserve">
          <source>Where \(\log_e (x)\) means the natural logarithm of \(x\). This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.</source>
          <target state="translated">여기서 \ (\ log_e (x) \)는 \ (x \)의 자연 로그를 의미합니다. 이 지표는 인구 수, 수년에 걸친 상품의 평균 판매 등과 같이 지수 성장이있는 대상이 사용될 때 사용하는 것이 가장 좋습니다.이 지표는 예측치가 과대 평가 된 것보다 크게 예측하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="dc652afd01d2a671ac597240d27fcb8fb6f2cb88" translate="yes" xml:space="preserve">
          <source>Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\). The availability of sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">여기서 \ (s (i, k) \)는 샘플 \ (i \)와 \ (k \)의 유사성입니다. 샘플 \ (i \)의 예가 될 수있는 샘플 \ (k \)의 가용성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9fa1e5b532b4ed4720d23061371103281dda3d83" translate="yes" xml:space="preserve">
          <source>Where r is defined per sample, we need to make use of &lt;code&gt;start&lt;/code&gt;:</source>
          <target state="translated">r이 샘플마다 정의되는 경우 &lt;code&gt;start&lt;/code&gt; 를 사용해야 합니다 .</target>
        </trans-unit>
        <trans-unit id="c16b06fa7e959786262fbf5823a1d1a66514be0c" translate="yes" xml:space="preserve">
          <source>Where the step length \(\gamma_m\) is chosen using line search:</source>
          <target state="translated">행 길이를 사용하여 스텝 길이 \ (\ gamma_m \)를 선택한 경우 :</target>
        </trans-unit>
        <trans-unit id="096f899281fbd2d32d659004bce326784eacf79e" translate="yes" xml:space="preserve">
          <source>Where there are considerations other than maximum score in choosing a best estimator, &lt;code&gt;refit&lt;/code&gt; can be set to a function which returns the selected &lt;code&gt;best_index_&lt;/code&gt; given &lt;code&gt;cv_results_&lt;/code&gt;. In that case, the &lt;code&gt;best_estimator_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; will be set according to the returned &lt;code&gt;best_index_&lt;/code&gt; while the &lt;code&gt;best_score_&lt;/code&gt; attribute will not be available.</source>
          <target state="translated">최선의 추정 선택에 최대 점수 이외의 다른 고려 사항이있는 경우, &lt;code&gt;refit&lt;/code&gt; 선택한 반환하는 함수로 설정 될 수있다 &lt;code&gt;best_index_&lt;/code&gt; 주어진 &lt;code&gt;cv_results_&lt;/code&gt; . 이 경우 &lt;code&gt;best_estimator_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 는 반환 된 &lt;code&gt;best_index_&lt;/code&gt; 에 따라 설정 되지만 &lt;code&gt;best_score_&lt;/code&gt; 속성은 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="be446fd5f2cab9d9132f73b7df79724fa271f7f8" translate="yes" xml:space="preserve">
          <source>Where there are considerations other than maximum score in choosing a best estimator, &lt;code&gt;refit&lt;/code&gt; can be set to a function which returns the selected &lt;code&gt;best_index_&lt;/code&gt; given the &lt;code&gt;cv_results&lt;/code&gt;. In that case, the &lt;code&gt;best_estimator_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; will be set according to the returned &lt;code&gt;best_index_&lt;/code&gt; while the &lt;code&gt;best_score_&lt;/code&gt; attribute will not be available.</source>
          <target state="translated">최선의 추정 선택에 최대 점수 이외의 다른 고려 사항이있는 경우, &lt;code&gt;refit&lt;/code&gt; 선택한 반환하는 함수로 설정 될 수있다 &lt;code&gt;best_index_&lt;/code&gt; 주어진 &lt;code&gt;cv_results&lt;/code&gt; 을 . 이 경우 &lt;code&gt;best_estimator_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 는 반환 된 &lt;code&gt;best_index_&lt;/code&gt; 에 따라 설정 되지만 &lt;code&gt;best_score_&lt;/code&gt; 속성은 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="1e4c7785f80a06d28d2e2b965c377764abc1c026" translate="yes" xml:space="preserve">
          <source>Where to from here</source>
          <target state="translated">여기서 어디로</target>
        </trans-unit>
        <trans-unit id="17eb390ca1dec9880beb722610077dafb8edc9ac" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features] and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">여기서 u와 v는 모양 [n_samples, n_features]의 데이터 집합에서 가져온 행이고 p는 모양이 [n_components, n_features] (또는 희소 한 Achlioptas 행렬) 인 임의 가우스 N (0, 1) 행렬에 의한 투영입니다.</target>
        </trans-unit>
        <trans-unit id="c5759c4abe89df832c23e0269eba38e4f1ad0ad7" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features], eps is in ]0, 1[ and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">u와 v는 모양 [n_samples, n_features]의 데이터 세트에서 가져온 행이며, eps는] 0, 1 [에 있으며 p는 [n_components, n_features] 모양의 임의 가우스 N (0, 1) 행렬에 의한 투영입니다. (또는 희소 Achlioptas 매트릭스).</target>
        </trans-unit>
        <trans-unit id="7e741bc3dcef0123eeda11543758853be2aac149" translate="yes" xml:space="preserve">
          <source>Where:</source>
          <target state="translated">Where:</target>
        </trans-unit>
        <trans-unit id="16270a9447d061ffaba6de4c25d0370619ae5579" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;feature_names_&lt;/code&gt; and &lt;code&gt;vocabulary_&lt;/code&gt; should be sorted when fitting.</source>
          <target state="translated">피팅시 &lt;code&gt;feature_names_&lt;/code&gt; 및 &lt;code&gt;vocabulary_&lt;/code&gt; 를 정렬해야하는지 여부 .</target>
        </trans-unit>
        <trans-unit id="07f1abf8acdb3dcd49bde3ee8a201c3421831b31" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;feature_names_&lt;/code&gt; and &lt;code&gt;vocabulary_&lt;/code&gt; should be sorted when fitting. True by default.</source>
          <target state="translated">피팅 할 때 &lt;code&gt;feature_names_&lt;/code&gt; 및 &lt;code&gt;vocabulary_&lt;/code&gt; 를 정렬해야하는지 여부 입니다. 기본적으로 true입니다.</target>
        </trans-unit>
        <trans-unit id="e61b5eefa6a0d8caaa65c0cf06600523e6eded8c" translate="yes" xml:space="preserve">
          <source>Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion.</source>
          <target state="translated">강제 복사가 트리거되는지 여부 copy = False이면 변환으로 인해 복사가 트리거 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6817dee267fec06b200988a35092c9fafdb28df4" translate="yes" xml:space="preserve">
          <source>Whether a prefit model is expected to be passed into the constructor directly or not. If True, &lt;code&gt;transform&lt;/code&gt; must be called directly and SelectFromModel cannot be used with &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;GridSearchCV&lt;/code&gt; and similar utilities that clone the estimator. Otherwise train the model using &lt;code&gt;fit&lt;/code&gt; and then &lt;code&gt;transform&lt;/code&gt; to do feature selection.</source>
          <target state="translated">프리 피트 모델이 생성자로 직접 전달 될지 여부입니다. True 인 경우 &lt;code&gt;transform&lt;/code&gt; 직접 호출해야하며 SelectFromModel을 &lt;code&gt;cross_val_score&lt;/code&gt; , &lt;code&gt;GridSearchCV&lt;/code&gt; 및 추정기를 복제하는 유사한 유틸리티와 함께 사용할 수 없습니다 . 그렇지 않으면 &lt;code&gt;fit&lt;/code&gt; 사용하여 모델을 학습 한 다음 형상 선택을 수행하도록 &lt;code&gt;transform&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e67f675f1639113224879739e0229eb2671df0d3" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style.</source>
          <target state="translated">배열을 강제 또는 c 스타일로 강제할지 여부</target>
        </trans-unit>
        <trans-unit id="57d1f88164d54372295bc307285273118c662089" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style. When order is None (default), then if copy=False, nothing is ensured about the memory layout of the output array; otherwise (copy=True) the memory layout of the returned array is kept as close as possible to the original array.</source>
          <target state="translated">배열을 강제 또는 c 스타일로 강제할지 여부 order가 None (기본값) 인 경우 copy = False이면 출력 배열의 메모리 레이아웃에 대해 아무 것도 보장되지 않습니다. 그렇지 않으면 (copy = True) 반환 된 배열의 메모리 레이아웃이 원래 배열에 최대한 가깝게 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="f6541283616583040ce3e73f070cbb436dbc5da9" translate="yes" xml:space="preserve">
          <source>Whether bootstrap samples are used when building trees.</source>
          <target state="translated">나무를 만들 때 부트 스트랩 샘플을 사용할지 여부</target>
        </trans-unit>
        <trans-unit id="1349327857b1cdc30e7f508422cffd4b74a570bf" translate="yes" xml:space="preserve">
          <source>Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.</source>
          <target state="translated">나무를 만들 때 부트 스트랩 샘플이 사용되는지 여부입니다. False이면 전체 데이터 세트가 각 트리를 작성하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="344e324f858395af07d0534305d3cd485a522869" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; is passed. If &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to &lt;code&gt;zero_based=True&lt;/code&gt; to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f의 열 인덱스가 0부터 시작하는지 (True) 또는 1에서 시작인지 (False). 열 인덱스가 1부터 시작하면 Python / NumPy 규칙과 일치하도록 0부터 시작합니다. &quot;auto&quot;로 설정하면 휴리스틱 검사가 적용되어 파일 내용에서이를 확인합니다. 두 종류의 파일 모두&amp;ldquo;와일드하게&amp;rdquo;발생하지만 불행히도 자체 식별은 아닙니다. &lt;code&gt;offset&lt;/code&gt; 이나 &lt;code&gt;length&lt;/code&gt; 가 전달 되지 않으면 &quot;auto&quot;또는 True를 사용하는 것이 항상 안전해야합니다 . 경우 &lt;code&gt;offset&lt;/code&gt; 또는 &lt;code&gt;length&lt;/code&gt; 전달, &quot;자동&quot;모드는 다시 하락한다 &lt;code&gt;zero_based=True&lt;/code&gt; 파일의 다른 세그먼트에 휴리스틱 검사 수율 일치하지 않는 결과를 피하기 위해.</target>
        </trans-unit>
        <trans-unit id="f00bfe1387e4927051573cb3d574287560206c66" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no offset or length is passed. If offset or length are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to zero_based=True to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f의 열 인덱스가 0부터 시작하는지 (True) 또는 1에서 시작인지 (False). 열 인덱스가 1부터 시작하면 Python / NumPy 규칙과 일치하도록 0부터 시작합니다. &quot;auto&quot;로 설정하면 휴리스틱 검사가 적용되어 파일 내용에서이를 확인합니다. 두 종류의 파일 모두&amp;ldquo;와일드하게&amp;rdquo;발생하지만 불행히도 자체 식별은 아닙니다. 오프셋이나 길이가 전달되지 않으면 &quot;auto&quot;또는 True를 사용하는 것이 항상 안전해야합니다. 오프셋 또는 길이가 전달되면 &quot;자동&quot;모드는 파일의 다른 세그먼트에서 휴리스틱 검사 결과가 일치하지 않는 결과를 피하기 위해 zero_based = True로 돌아갑니다.</target>
        </trans-unit>
        <trans-unit id="be05ee9a303aba9073e602f5af4606db8dba467c" translate="yes" xml:space="preserve">
          <source>Whether column indices should be written zero-based (True) or one-based (False).</source>
          <target state="translated">열 인덱스를 0부터 시작하는지 (True) 또는 1부터 시작해야하는지 (False).</target>
        </trans-unit>
        <trans-unit id="426c392bb98aa08b186e867710abfa024378fa01" translate="yes" xml:space="preserve">
          <source>Whether features are drawn with replacement.</source>
          <target state="translated">기능을 교체하여 그릴 지 여부</target>
        </trans-unit>
        <trans-unit id="b1153e9c8e50c0d286811aaf218a31fa047ae93d" translate="yes" xml:space="preserve">
          <source>Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.</source>
          <target state="translated">가중치의 두 번째 정규화 수행 여부입니다. 기본 동작은 Mahout 및 Weka에있는 구현을 반영하며,이 문서의 표 9에 설명 된 전체 알고리즘을 따르지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1500a013d74d8899d6c67c8489e35470d425474d" translate="yes" xml:space="preserve">
          <source>Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter &lt;code&gt;fit_intercept&lt;/code&gt;.</source>
          <target state="translated">모델이 인터셉트 (즉, 바이어스 된 초평면)를 사용해야하는지 여부는 &lt;code&gt;fit_intercept&lt;/code&gt; 매개 변수로 제어됩니다 .</target>
        </trans-unit>
        <trans-unit id="c17699d69c93a1c9674665b22c836f689e7a71a8" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch.</source>
          <target state="translated">각 에포크 후에 트레이닝 데이터를 섞어 야하는지의 여부.</target>
        </trans-unit>
        <trans-unit id="2f96d4cd24a907c94c91a597b369db5ae9d183fe" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch. Defaults to True.</source>
          <target state="translated">각 에포크 후에 트레이닝 데이터를 섞어 야하는지의 여부. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="21e287c040da0ab9f7612eda4a9df397d21447be" translate="yes" xml:space="preserve">
          <source>Whether or not to compute labels for each fit.</source>
          <target state="translated">각 적합치에 대한 레이블을 계산할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="ae7627e3aed47d9187a8dde354d4bd8908f66e18" translate="yes" xml:space="preserve">
          <source>Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools.</source>
          <target state="translated">원시 Mahalanobis 거리를 결정 기능으로 고려할지 여부. 다른 이상치 탐지 도구와의 호환성을 위해 False (기본값) 여야합니다.</target>
        </trans-unit>
        <trans-unit id="d7d36162415efc2dece0359749393df764e8f212" translate="yes" xml:space="preserve">
          <source>Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin.</source>
          <target state="translated">절편에 맞는지 여부. 데이터가 이미 원점을 중심으로하는 경우 False로 설정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="99ecd63bc30b233e173e08d6a58d2b609112b08a" translate="yes" xml:space="preserve">
          <source>Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">주어진 데이터의 사본을 만들지 여부. False로 설정하면 초기 데이터를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="95950f270865da71d578c03fd7275708de2f1edb" translate="yes" xml:space="preserve">
          <source>Whether or not to mark each sample as the first nearest neighbor to itself. If &amp;lsquo;auto&amp;rsquo;, then True is used for mode=&amp;rsquo;connectivity&amp;rsquo; and False for mode=&amp;rsquo;distance&amp;rsquo;.</source>
          <target state="translated">각 샘플을 자신에 가장 가까운 첫 번째 이웃으로 표시할지 여부입니다. 'auto'인 경우 모드 = 'connectivity'에 True가 사용되고 mode = 'distance'에 False가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0d8aa347bdbdfa6dd79051c8fee2f95fc5666522" translate="yes" xml:space="preserve">
          <source>Whether or not to mark each sample as the first nearest neighbor to itself. If &lt;code&gt;None&lt;/code&gt;, then True is used for mode=&amp;rsquo;connectivity&amp;rsquo; and False for mode=&amp;rsquo;distance&amp;rsquo; as this will preserve backwards compatibility.</source>
          <target state="translated">각 샘플을 가장 가까운 이웃으로 표시할지 여부입니다. 경우 &lt;code&gt;None&lt;/code&gt; , 다음 진정한 모드 = '연결'을 위해 사용하지 않고 모드 False입니다 =이 같은 '거리'는 이전 버전과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="a1d6eb056f01f6a77d40d6f787612d8008f1be4b" translate="yes" xml:space="preserve">
          <source>Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.</source>
          <target state="translated">스파 스 CSR 매트릭스를 기본 동작으로 반환할지 또는 밀도가 높은 파이프 라인 연산자와 호환되는 밀도 배열을 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="8f592bf838896fb605ecc15c063970bf58250eab" translate="yes" xml:space="preserve">
          <source>Whether or not to return the number of iterations.</source>
          <target state="translated">반복 횟수를 리턴할지 여부.</target>
        </trans-unit>
        <trans-unit id="3433b032133d6a741db0a6ccce9ce8005a84877d" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.</source>
          <target state="translated">분할하기 전에 데이터를 셔플 링할지 여부입니다. shuffle = False 인 경우 계층화는 없음이어야합니다.</target>
        </trans-unit>
        <trans-unit id="d797771ee8d7ac8a664344b3d1655c54bf4a7c5a" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent.</source>
          <target state="translated">데이터 셔플 링 여부 : 표본이 확률 적 경사 하강과 같이 독립적이고 동일하게 분포되어 있다고 가정하는 모델에 중요 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="169aa17090e9b82766c818a4a09acd1cb51fcb24" translate="yes" xml:space="preserve">
          <source>Whether samples are drawn with replacement.</source>
          <target state="translated">교체로 샘플을 채취할지 여부</target>
        </trans-unit>
        <trans-unit id="11994582207268a83009eceee1185d13dd22bd86" translate="yes" xml:space="preserve">
          <source>Whether samples are drawn with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">샘플이 대체로 그려 졌는지 여부. False이면 교체하지 않고 샘플링을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="b61c81ce74fc75ce6a90c790bfafe984d14c7994" translate="yes" xml:space="preserve">
          <source>Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.</source>
          <target state="translated">score_func가 점수 함수 (기본값)인지, 높음을 의미하거나 손실 함수, 낮음을 의미합니다. 후자의 경우, 득점자 개체는 score_func의 결과를 부호 넘기 게됩니다.</target>
        </trans-unit>
        <trans-unit id="01c7a3b4947bb7aed2272a78dd753a14b9e24fdc" translate="yes" xml:space="preserve">
          <source>Whether score_func requires predict_proba to get probability estimates out of a classifier.</source>
          <target state="translated">score_func가 분류기에서 확률 추정치를 얻기 위해 predict_proba가 필요한지 여부</target>
        </trans-unit>
        <trans-unit id="e86601bb1c2db478564381eb9b1a33fc72f3ad69" translate="yes" xml:space="preserve">
          <source>Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method.</source>
          <target state="translated">score_func가 지속적인 의사 결정을 수행하는지 여부 이는 decision_function 또는 predict_proba 메소드가있는 추정기를 사용하는 2 진 분류에만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="62e384e32324c7d8c05ac06534cda98d3cbc0def" translate="yes" xml:space="preserve">
          <source>Whether support is a list of indices.</source>
          <target state="translated">지원이 지수 목록인지 여부.</target>
        </trans-unit>
        <trans-unit id="970ea0e030e6e4be6469b0282edfb558e0e9066d" translate="yes" xml:space="preserve">
          <source>Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The &amp;lsquo;auto&amp;rsquo; mode will trigger the transposition if M.shape[1] &amp;gt; M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.</source>
          <target state="translated">알고리즘이 M 대신 MT에 적용되어야하는지 여부. 결과는 대략 동일해야합니다. 이 경우 무작위 SVD의 구현이 약간 더 빠르기 때문에 M.shape [1]&amp;gt; M.shape [0] 인 경우 '자동'모드는 조옮김을 트리거합니다.</target>
        </trans-unit>
        <trans-unit id="3226951d2ead1297a694651602f8538f5e93757c" translate="yes" xml:space="preserve">
          <source>Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten.</source>
          <target state="translated">공분산 벡터 Xy를 알고리즘으로 복사해야하는지 여부 False이면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d2b667c3b7746e3e678455d8b2d703997aeb5e60" translate="yes" xml:space="preserve">
          <source>Whether the deflation be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effects</source>
          <target state="translated">사본에서 수축이 수행되는지 여부. 부작용에 신경 쓰지 않는 한 기본값을 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="1cba92780e42c1ebe55ada467260206516898de8" translate="yes" xml:space="preserve">
          <source>Whether the deflation should be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effect</source>
          <target state="translated">디플레이션이 사본에서 수행되어야하는지 여부 부작용에 신경 쓰지 않는 한 기본값을 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="99f0df0508624fcfafef99671905c951f197a398" translate="yes" xml:space="preserve">
          <source>Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">알고리즘으로 설계 행렬 X를 복사해야하는지 여부 잘못된 값은 X가 이미 주문 된 경우에만 유용합니다. 그렇지 않으면 복사가 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="6f829dc8dcc41b94f325536e7999d7feb34a89f1" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word n-gram or character n-grams. Option &amp;lsquo;char_wb&amp;rsquo; creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</source>
          <target state="translated">기능이 단어 n-gram 또는 문자 n-gram으로 구성되어야하는지 여부입니다. 옵션 'char_wb'는 단어 경계 내의 텍스트에서만 문자 n- 그램을 생성합니다. 단어 가장자리에있는 n-gram은 공백으로 채워집니다.</target>
        </trans-unit>
        <trans-unit id="f62ef19aafedabcbf9aa2a6c1cbcccaa900e840f" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams.</source>
          <target state="translated">기능이 단어 또는 문자 n- 그램으로 이루어져야하는지 여부</target>
        </trans-unit>
        <trans-unit id="9c1354d44a66effd212e821b1aba74fe08612248" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams. Option &amp;lsquo;char_wb&amp;rsquo; creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</source>
          <target state="translated">기능이 단어 또는 문자 n- 그램으로 이루어져야하는지 여부 옵션 'char_wb'는 단어 경계 안의 텍스트에서만 문자 n- 그램을 만듭니다. 단어 가장자리의 n- 그램은 공백으로 채워집니다.</target>
        </trans-unit>
        <trans-unit id="824ad07968fefbc8aa1fba0ade7c849f0d099562" translate="yes" xml:space="preserve">
          <source>Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">알고리즘으로 그램 행렬을 복사해야하는지 여부 잘못된 값은 이미 주문한 경우에만 유용합니다. 그렇지 않으면 어쨌든 복사됩니다.</target>
        </trans-unit>
        <trans-unit id="23571fa5c9f0e8d5b2ce0915807aa480e23639d3" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask format should be sparse or dense.</source>
          <target state="translated">임 피터 마스크 형식이 희박해야하는지, 아니면 조밀해야하는지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b70758cdff0513f8822d9fb7393f16dda3f13af9" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask should represent all or a subset of features.</source>
          <target state="translated">임 피터 마스크가 모든 기능 또는 일부 기능을 나타내는 지 여부</target>
        </trans-unit>
        <trans-unit id="11709e37813efd2480c1d891188cf0d8201c8a69" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If &lt;code&gt;False&lt;/code&gt;, the data is assumed to be already centered.</source>
          <target state="translated">절편을 추정해야하는지 여부 경우 &lt;code&gt;False&lt;/code&gt; 데이터가 이미 중심을 가정한다.</target>
        </trans-unit>
        <trans-unit id="0ccd5f6458ed970eecf03c787120d2831e50f140" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.</source>
          <target state="translated">절편을 추정해야하는지 여부 False이면 데이터가 이미 중앙에있는 것으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="2fafec857734808cd372cb104d91a568d25acbf6" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.</source>
          <target state="translated">절편을 추정해야하는지 여부 False이면 데이터가 이미 중앙에있는 것으로 간주됩니다. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="1369ea0f90c1ab8c31f4e5d5e2b327950f322e67" translate="yes" xml:space="preserve">
          <source>Whether the kernel works only on fixed-length feature vectors.</source>
          <target state="translated">커널이 고정 길이 특징 벡터에서만 작동하는지 여부.</target>
        </trans-unit>
        <trans-unit id="efe887a2120302f3ddf508ce39115dacb905298c" translate="yes" xml:space="preserve">
          <source>Whether the parameter was found to be a named parameter of the estimator&amp;rsquo;s fit method.</source>
          <target state="translated">매개 변수가 추정기의 적합 방법의 명명 된 매개 변수인지 여부</target>
        </trans-unit>
        <trans-unit id="e3e6070e7b1bf06bd46c63ade906ee83f84569ff" translate="yes" xml:space="preserve">
          <source>Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), &amp;lsquo;none&amp;rsquo; (the fastest but numerically unstable when &lt;code&gt;n_iter&lt;/code&gt; is large, e.g. typically 5 or larger), or &amp;lsquo;LU&amp;rsquo; factorization (numerically stable but can lose slightly in accuracy). The &amp;lsquo;auto&amp;rsquo; mode applies no normalization if &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2 and switches to LU otherwise.</source>
          <target state="translated">단계별 QR 분해 (가장 느리지 만 가장 정확한)로 전력 반복이 정규화되는지, '없음'( &lt;code&gt;n_iter&lt;/code&gt; 가 클 때 가장 빠르지 만 수치 적으로 불안정 함 ( 예 : 일반적으로 5 이상)) 또는 'LU'분해 (수적으로) 안정적이지만 정확도가 약간 떨어질 수 있습니다). '자동'모드는 &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2 인 경우 정규화를 적용하지 않으면 LU로 전환됩니다.</target>
        </trans-unit>
        <trans-unit id="00c2bc5048e0182a905dad0c4dec40740dd521b8" translate="yes" xml:space="preserve">
          <source>Whether the relationship is increasing or decreasing.</source>
          <target state="translated">관계가 증가 또는 감소하는지 여부</target>
        </trans-unit>
        <trans-unit id="dc0314689b038e45038d5534e1499b2766f8b916" translate="yes" xml:space="preserve">
          <source>Whether the return value is an array of sparse matrix depends on the type of the input X.</source>
          <target state="translated">반환 값이 희소 행렬의 배열인지 여부는 입력 X의 유형에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="ed44b1dc96dab239fb6ac2dc375f2ee5fe3ae793" translate="yes" xml:space="preserve">
          <source>Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values&amp;rsquo; mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GP&amp;rsquo;s prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default.</source>
          <target state="translated">목표 값 y가 정규화되는지 여부, 즉 관찰 된 목표 값의 평균은 0이된다. 목표 값의 평균이 0과 크게 다를 것으로 예상되는 경우이 매개 변수를 True로 설정해야합니다. 활성화되면 정규화는 데이터를 기반으로 GP의 사전을 효과적으로 수정하므로 가능성 원리와 모순됩니다. 따라서 표준화는 기본적으로 비활성화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="cb8d2af32f6e15f6d8381b5609652e77d17356c4" translate="yes" xml:space="preserve">
          <source>Whether the target values y are normalized, the mean and variance of the target values are set equal to 0 and 1 respectively. This is recommended for cases where zero-mean, unit-variance priors are used. Note that, in this implementation, the normalisation is reversed before the GP predictions are reported.</source>
          <target state="translated">목표 값 y가 정규화되었는지 여부에 관계없이 목표 값의 평균과 분산은 각각 0과 1로 설정됩니다. 이는 평균이 0 인 단위 분산 사전이 사용되는 경우에 권장됩니다. 이 구현에서는 GP 예측이보고되기 전에 정규화가 반전됩니다.</target>
        </trans-unit>
        <trans-unit id="4d56de522c2f2c8fa83698a0d6921644d35a5428" translate="yes" xml:space="preserve">
          <source>Whether the task is a classification task, in which case stratified KFold will be used.</source>
          <target state="translated">작업이 분류 작업인지 여부에 따라 계층화 된 KFold가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="667f96156fffe3e4c7c8342ead32f8ae00d3d84f" translate="yes" xml:space="preserve">
          <source>Whether the value of this hyperparameter is fixed, i.e., cannot be changed during hyperparameter tuning. If None is passed, the &amp;ldquo;fixed&amp;rdquo; is derived based on the given bounds.</source>
          <target state="translated">이 하이퍼 파라미터의 값이 고정되어 있는지 여부, 즉, 하이퍼 파라미터 튜닝 중에는 변경할 수 없습니다. None이 전달되면 주어진 경계를 기반으로 &quot;fixed&quot;가 파생됩니다.</target>
        </trans-unit>
        <trans-unit id="6cb255093bce0eff775d5414b35fcd6fbd07931b" translate="yes" xml:space="preserve">
          <source>Whether this is a multilabel classifier</source>
          <target state="translated">이것이 다중 레이블 분류기인지 여부</target>
        </trans-unit>
        <trans-unit id="b1be5efdade94da8e67722b4ba2f983efa0225c5" translate="yes" xml:space="preserve">
          <source>Whether to allow 2-d y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True.</source>
          <target state="translated">2-dy (배열 또는 희소 행렬) 허용 여부입니다. False이면 y는 벡터로 검증됩니다. multi_output = True 인 경우 np.nan 또는 np.inf 값을 가질 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="110cd422886570c5a4fe2834aa70ff221b2cdcaf" translate="yes" xml:space="preserve">
          <source>Whether to allow 2D y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True.</source>
          <target state="translated">2D y (배열 또는 희소 행렬) 허용 여부입니다. 거짓이면 y가 벡터로 검증됩니다. multi_output = True 인 경우 y는 np.nan 또는 np.inf 값을 가질 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="8ecc2d4e014d3f2172c25597969963bd34e18f05" translate="yes" xml:space="preserve">
          <source>Whether to allow X.ndim &amp;gt; 2.</source>
          <target state="translated">X.ndim 허용 여부&amp;gt; 2</target>
        </trans-unit>
        <trans-unit id="7a4a847db3917eeca38ca476c1ad3e57930cf992" translate="yes" xml:space="preserve">
          <source>Whether to allow array.ndim &amp;gt; 2.</source>
          <target state="translated">array.ndim&amp;gt; 2. 허용 여부</target>
        </trans-unit>
        <trans-unit id="d5dcbf9253f384309cfbc4a594ef7b057c1cb7e6" translate="yes" xml:space="preserve">
          <source>Whether to also return the code U or just the dictionary V.</source>
          <target state="translated">코드 U 또는 사전 V 만 반환할지 여부</target>
        </trans-unit>
        <trans-unit id="c6289192e1f815e2a760fe289e8841e73f51c42c" translate="yes" xml:space="preserve">
          <source>Whether to be verbose.</source>
          <target state="translated">장황한 지 여부</target>
        </trans-unit>
        <trans-unit id="ccada94fe77cfa7bf4094a2aaa2265ce9f9f4e5f" translate="yes" xml:space="preserve">
          <source>Whether to cache downloaded datasets using joblib.</source>
          <target state="translated">joblib을 사용하여 다운로드 한 데이터 세트를 캐시할지 여부.</target>
        </trans-unit>
        <trans-unit id="86614eccba121d18979d29b5e6a1aceed9dc9343" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. False로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 이미 중앙에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="7285fa12a56fc9f408db5bd27934d6f0654e6093" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</source>
          <target state="translated">이 모델에 대한 절편을 계산할지 여부입니다. False로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 중앙에 배치 될 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="943768cddf06aea5cdead26cd3dff69cb74196ef" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 이미 중심에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="27f8a383f138130dd1f45baee7b1d68ad4ce59f4" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다 (즉, 데이터가 이미 중앙에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="34dd726eec26b92f0d7e507bd65e8b16cfaec4c8" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).</source>
          <target state="translated">이 모델에 대한 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 중앙에 배치 될 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="0336ff17d311b84566800e5cf35b415ab2b27f38" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7c201504f17506a2291f5d939998458e72428d3b" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).</source>
          <target state="translated">이 모델에 대한 절편을 계산할지 여부입니다. 절편은 확률 매개 변수로 처리되지 않으므로 관련 분산이 없습니다. False로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 중앙에 배치 될 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="471ed2aff4494fad57e380482ee0a58232333b20" translate="yes" xml:space="preserve">
          <source>Whether to check that &lt;code&gt;transform&lt;/code&gt; followed by &lt;code&gt;inverse_transform&lt;/code&gt; or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original targets.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 후 &lt;code&gt;inverse_transform&lt;/code&gt; 또는 &lt;code&gt;func&lt;/code&gt; 다음에 &lt;code&gt;inverse_func&lt;/code&gt; 가 있는지 여부를 확인 하면 원래 대상이됩니다.</target>
        </trans-unit>
        <trans-unit id="3c61d748141856e990caff79c0b01fd8a4086321" translate="yes" xml:space="preserve">
          <source>Whether to check that or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original inputs. It can be used for a sanity check, raising a warning when the condition is not fulfilled.</source>
          <target state="translated">확인 여부 또는 &lt;code&gt;func&lt;/code&gt; 다음에 &lt;code&gt;inverse_func&lt;/code&gt; 가 있는지 여부 는 원래 입력으로 이어집니다. 상태 점검에 사용되어 조건이 충족되지 않으면 경고를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="50850a0df7fa2328560b0d7ebe31ee1142901517" translate="yes" xml:space="preserve">
          <source>Whether to compute &lt;code&gt;y_&lt;/code&gt; is increasing (if set to True) or decreasing (if set to False)</source>
          <target state="translated">&lt;code&gt;y_&lt;/code&gt; 계산 증가 (True로 설정된 경우) 또는 감소 (False로 설정된 경우)</target>
        </trans-unit>
        <trans-unit id="5fbe7e9ef9385d70942ace9201de11e6cead8f8d" translate="yes" xml:space="preserve">
          <source>Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.</source>
          <target state="translated">제곱 오차 표준 또는 오류 표준을 계산할지 여부 True (기본값)이면 제곱 오류 표준이 반환됩니다. False이면 오류 규범이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="01086dba4779bb032a62d90a9f9152dc1b833baf" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place computations.</source>
          <target state="translated">X와 Y를 복사 할 것인지 또는 내부 계산을 수행 할 것인지.</target>
        </trans-unit>
        <trans-unit id="725302de0fd34aabdbfc0e0affbd4f4fb754127c" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place normalization.</source>
          <target state="translated">X와 Y를 복사 할 것인지 또는 내부 정규화를 수행 할 것인지.</target>
        </trans-unit>
        <trans-unit id="7462b314a4fe2fff7847f10014abb6b1183fa84f" translate="yes" xml:space="preserve">
          <source>Whether to copy X and operate on the copy or perform in-place operations.</source>
          <target state="translated">X를 복사하여 복사 작업을 수행 할 것인지 아니면 내부 작업을 수행 할 것인지를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="3692936737114d51a6bb410cb3531454c9ad1d68" translate="yes" xml:space="preserve">
          <source>Whether to copy the precomputed covariance matrix; if False, it may be overwritten.</source>
          <target state="translated">미리 계산 된 공분산 행렬을 복사할지 여부; False이면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="324d6fe1307968790ddbb142ded331e1979517da" translate="yes" xml:space="preserve">
          <source>Whether to create a copy of X and operate on it or to perform inplace computation (default behaviour).</source>
          <target state="translated">X의 사본을 작성하여 작업 할 것인지 또는 내부 계산을 수행 할 것인지 (기본 동작).</target>
        </trans-unit>
        <trans-unit id="62527ff1b5ed50e080e833b6d4fd94a862b78590" translate="yes" xml:space="preserve">
          <source>Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.</source>
          <target state="translated">플로팅 된 ROC 곡선에 나타나지 않는 일부 차선의 임계 값을 제거할지 여부입니다. 더 가벼운 ROC 곡선을 만드는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="9851e7fdf1748b99ff7c24c940b7ce5f334a6a27" translate="yes" xml:space="preserve">
          <source>Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector.</source>
          <target state="translated">첫 번째 고유 벡터를 제거할지 여부입니다. 스펙트럼 임베딩의 경우 첫 번째 고유 벡터가 연결된 그래프의 상수 벡터 여야하므로 True 여야하지만 스펙트럼 클러스터링의 경우 첫 번째 고유 벡터를 유지하려면 False로 유지해야합니다.</target>
        </trans-unit>
        <trans-unit id="5f99ffcc1bc3b15acf95363130c37cd5b381a91e" translate="yes" xml:space="preserve">
          <source>Whether to enable probability estimates. This must be enabled prior to calling &lt;code&gt;fit&lt;/code&gt;, and will slow down that method.</source>
          <target state="translated">확률 추정을 활성화할지 여부. &lt;code&gt;fit&lt;/code&gt; 호출하기 전에 활성화해야하며 해당 메소드의 속도가 느려집니다.</target>
        </trans-unit>
        <trans-unit id="1a58ce6c20ca7e5e36327e21b19b4ffc278a0174" translate="yes" xml:space="preserve">
          <source>Whether to enable probability estimates. This must be enabled prior to calling &lt;code&gt;fit&lt;/code&gt;, will slow down that method as it internally uses 5-fold cross-validation, and &lt;code&gt;predict_proba&lt;/code&gt; may be inconsistent with &lt;code&gt;predict&lt;/code&gt;. Read more in the &lt;a href=&quot;../svm#scores-probabilities&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">확률 추정 사용 여부입니다. &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 활성화해야하며 , 내부적으로 5 겹 교차 검증을 사용하므로 해당 메서드의 속도가 느려지고 &lt;code&gt;predict_proba&lt;/code&gt; 가 &lt;code&gt;predict&lt;/code&gt; 와 일치하지 않을 수 있습니다 . 자세한 내용은 &lt;a href=&quot;../svm#scores-probabilities&quot;&gt;사용자 가이드를 참조하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4a4497ffca40d00ae7a4f4bda08e5dece2337700" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the code.</source>
          <target state="translated">코드를 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="2920a1115fbc090ce8de93fb299e0d53a98e0404" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary</source>
          <target state="translated">사전을 찾을 때 양성을 시행할지 여부</target>
        </trans-unit>
        <trans-unit id="cc323427a6369f3d4c345df0c7eab9b613a4b184" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary.</source>
          <target state="translated">사전을 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="0c86815e9d15a8d79f49100b7de99ab0894ffb4b" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the encoding.</source>
          <target state="translated">인코딩을 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="67962e408072673f64867d41053d2df5bd8ffd92" translate="yes" xml:space="preserve">
          <source>Whether to ensure that y has a numeric type. If dtype of y is object, it is converted to float64. Should only be used for regression algorithms.</source>
          <target state="translated">y에 숫자 유형이 있는지 여부입니다. y의 dtype이 object이면 float64로 변환됩니다. 회귀 알고리즘에만 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="a89a89f1fbc0cff73f883e4748e4c43034f0361e" translate="yes" xml:space="preserve">
          <source>Whether to filter invalid parameters or not.</source>
          <target state="translated">잘못된 매개 변수를 필터링할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="8bd7eb051fe8793acf6505d7ff57ac8a40be3e89" translate="yes" xml:space="preserve">
          <source>Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1).</source>
          <target state="translated">모형에 절편을 맞출 지 여부입니다. 이 경우 반환 된 배열의 모양은 (n_cs, n_features + 1)입니다.</target>
        </trans-unit>
        <trans-unit id="86604814c1d6f6dc79ece6e1aa0e214e981e58a5" translate="yes" xml:space="preserve">
          <source>Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are expected to be centered).</source>
          <target state="translated">이 모델에 절편을 맞출 지 여부입니다. false로 설정하면 계산에 절편이 사용되지 않습니다 (예 : &lt;code&gt;X&lt;/code&gt; 와 &lt;code&gt;y&lt;/code&gt; 가 중앙에 위치 할 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="8e39ad37924100e174516b8fe05585ae0c11a660" translate="yes" xml:space="preserve">
          <source>Whether to include &amp;ldquo;special&amp;rdquo; label estimator or test processors.</source>
          <target state="translated">&amp;ldquo;특별한&amp;rdquo;라벨 추정기 또는 테스트 프로세서 포함 여부.</target>
        </trans-unit>
        <trans-unit id="84818ba086581abd044063f9dd3c5b2beb12eda7" translate="yes" xml:space="preserve">
          <source>Whether to include meta-estimators that can be constructed using an estimator as their first argument. These are currently BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier, OneVsRestClassifier, RFE, RFECV.</source>
          <target state="translated">추정기를 첫 번째 인수로 사용하여 구성 할 수있는 메타 추정기를 포함할지 여부입니다. 이들은 현재 BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier, OneVsRestClassifier, RFE, RFECV입니다.</target>
        </trans-unit>
        <trans-unit id="74d7014a87bc3e04b7cb4d5881013af41aaf9d5f" translate="yes" xml:space="preserve">
          <source>Whether to include train scores.</source>
          <target state="translated">열차 점수 포함 여부</target>
        </trans-unit>
        <trans-unit id="5a33027c8dd2a3a26ddc387706fe4e97d00f5eae" translate="yes" xml:space="preserve">
          <source>Whether to include train scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.</source>
          <target state="translated">열차 점수를 포함할지 여부입니다. 훈련 점수 계산은 다양한 매개 변수 설정이 과적 합 / 과소 적합 트레이드 오프에 어떤 영향을 미치는지에 대한 통찰력을 얻는 데 사용됩니다. 그러나 훈련 세트의 점수를 계산하는 것은 계산 비용이 많이들 수 있으며 최상의 일반화 성능을 제공하는 매개 변수를 선택하는 데 엄격하게 요구되지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="b770fc1f2ccfc02ef3107a2ecac741b2276c37e8" translate="yes" xml:space="preserve">
          <source>Whether to learn class prior probabilities or not. If false, a uniform prior will be used.</source>
          <target state="translated">수업 사전 확률을 배울 지 여부. False이면 균일 한 사전이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8606bf192804810fba78c6bd2b8805fda4483156" translate="yes" xml:space="preserve">
          <source>Whether to load only 10 percent of the data.</source>
          <target state="translated">데이터의 10 % 만로드할지 여부</target>
        </trans-unit>
        <trans-unit id="3156faf4c491e77c08d3500dd2b8c76947137632" translate="yes" xml:space="preserve">
          <source>Whether to load or not the content of the different files. If true a &amp;lsquo;data&amp;rsquo; attribute containing the text information is present in the data structure returned. If not, a filenames attribute gives the path to the files.</source>
          <target state="translated">다른 파일의 내용을로드할지 여부 true 인 경우 텍스트 정보를 포함하는 'data'속성이 반환 된 데이터 구조에 있습니다. 그렇지 않은 경우 파일 이름 속성은 파일 경로를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="81f8d6a01f7cffb7f53496e9cfd84f1ce27de740" translate="yes" xml:space="preserve">
          <source>Whether to make X at least 2d.</source>
          <target state="translated">X를 2d 이상으로 만들지 여부</target>
        </trans-unit>
        <trans-unit id="2a578215f5e1bceb4eddd518c894532f84a10916" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of X. If &lt;code&gt;False&lt;/code&gt;, the input X gets overwritten during fitting.</source>
          <target state="translated">X의 사본을 만들지 여부. &lt;code&gt;False&lt;/code&gt; 인 경우 피팅 중 입력 X를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="e2abdc017941ef25090c0789fe34541086dc5677" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">주어진 데이터의 복사본을 만들지 여부 False로 설정하면 초기 데이터를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="df62a350cab30808585d28e267100de2daf97811" translate="yes" xml:space="preserve">
          <source>Whether to normalize the output matrix to make the leading diagonal elements all 1</source>
          <target state="translated">선행 대각선 요소를 모두 만들기 위해 출력 행렬을 정규화할지 여부 1</target>
        </trans-unit>
        <trans-unit id="ba6415e4db38e5cea33cf7fab1a514fcf5285867" translate="yes" xml:space="preserve">
          <source>Whether to perform precomputations. Improves performance when n_targets or n_samples is very large.</source>
          <target state="translated">사전 계산 수행 여부 n_targets 또는 n_samples가 매우 큰 경우 성능을 향상시킵니다.</target>
        </trans-unit>
        <trans-unit id="4010b2bff9133aaf08486f7fb645e8e39634583e" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.</source>
          <target state="translated">피팅에서 최상의 분할을 찾는 속도를 높이기 위해 데이터를 미리 정렬할지 여부입니다. 자동 모드는 기본적으로 밀도가 높은 데이터에 대해 사전 분류를 사용하고 희소 데이터에 대한 기본 정렬을 기본으로 사용합니다. 스파 스 데이터에서 presort를 true로 설정하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="29f75c6794195c888ce8399680c96d5b14e65048" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.</source>
          <target state="translated">피팅에서 최상의 분할을 찾는 속도를 높이기 위해 데이터를 미리 정렬할지 여부입니다. 큰 데이터 세트에서 의사 결정 트리의 기본 설정의 경우이를 true로 설정하면 훈련 프로세스가 느려질 수 있습니다. 더 작은 데이터 세트 또는 제한된 깊이를 사용하면 훈련 속도가 빨라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cc4d3f96c9bc487e50b4fc4701212f323c65bca6" translate="yes" xml:space="preserve">
          <source>Whether to print progress messages to stdout.</source>
          <target state="translated">진행 메시지를 stdout에 인쇄할지 여부</target>
        </trans-unit>
        <trans-unit id="1ef1345441f84cbc6b06cbfc2b69b730789a6079" translate="yes" xml:space="preserve">
          <source>Whether to raise a value error if X is not 2D.</source>
          <target state="translated">X가 2D가 아닌 경우 값 오류를 발생 시킬지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b44ea19ee67df3ef30c54f3be25f24e67c4f3a60" translate="yes" xml:space="preserve">
          <source>Whether to raise a value error if X is not 2d.</source>
          <target state="translated">X가 2d가 아닌 경우 값 오류를 발생 시킬지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="64b755e00dbefa679127012ca837c554b0c217d7" translate="yes" xml:space="preserve">
          <source>Whether to raise a value error if array is not 2D.</source>
          <target state="translated">배열이 2D가 아닌 경우 값 오류를 발생 시킬지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b5d06b3c83946e2cd2c05192b834ad6e01c6a1d5" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. The possibilities are:</source>
          <target state="translated">X에서 np.inf 및 np.nan에 오류를 발생 시킬지 여부입니다. 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="eaa329f6263ead71ba810671bba1e6a99379040d" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. This parameter does not influence whether y can have np.inf or np.nan values. The possibilities are:</source>
          <target state="translated">X에서 np.inf 및 np.nan에 오류를 발생 시킬지 여부 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec8573b1162223970fd6725eca010725203428e3" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf, np.nan, pd.NA in X. The possibilities are:</source>
          <target state="translated">X의 np.inf, np.nan, pd.NA에서 오류를 발생 시킬지 여부. 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="68ace26fa0c8880f45dfa331c844e7939329fec1" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter does not influence whether y can have np.inf, np.nan, pd.NA values. The possibilities are:</source>
          <target state="translated">X의 np.inf, np.nan, pd.NA에서 오류를 발생 시킬지 여부.이 매개 변수는 y가 np.inf, np.nan, pd.NA 값을 가질 수 있는지 여부에 영향을주지 않습니다. 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="3bbeec18546bd2f3b8cccf2897d9a3a9aee68173" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf, np.nan, pd.NA in array. The possibilities are:</source>
          <target state="translated">배열의 np.inf, np.nan, pd.NA에서 오류를 발생 시킬지 여부입니다. 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c551edd4edb4cf1081c3e3d3eb3a36ad8f839a51" translate="yes" xml:space="preserve">
          <source>Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to &amp;lsquo;ignore&amp;rsquo; and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.</source>
          <target state="translated">변환 중에 알 수없는 범주 형 피처가 존재하는 경우 오류를 발생 시킬지 또는 무시할지 여부 (기본값은 올림). 이 매개 변수가 '무시'로 설정되고 변환 중에 알 수없는 카테고리가 발견되면이 기능에 대한 1- 인코딩 된 열이 모두 0이됩니다. 역변환에서 알 수없는 범주는 없음으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="55b5cf4021bca4319afc6cff07e1e1c5a8e21c80" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2).</source>
          <target state="translated">다른 모든 분류기로 모양 (n_samples, n_classes)에 대한 1 대 1 나머지 ( 'ovr') 결정 함수를 반환할지 또는 모양 (n_samples)이있는 libsvm의 원래 1 대 1 ( 'ovo') 결정 함수를 반환할지 여부 , n_classes * (n_classes-1) / 2).</target>
        </trans-unit>
        <trans-unit id="3e008ca3901c2f4656b69b334cd2bbf88df838cc" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (&amp;lsquo;ovo&amp;rsquo;) is always used as multi-class strategy.</source>
          <target state="translated">다른 모든 분류기로 모양 (n_samples, n_classes)에 대한 1 대 1 나머지 ( 'ovr') 결정 함수를 반환할지 또는 모양 (n_samples)이있는 libsvm의 원래 1 대 1 ( 'ovo') 결정 함수를 반환할지 여부 , n_classes * (n_classes-1) / 2). 그러나 일대일 ( 'ovo')은 항상 다중 클래스 전략으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7af50893420dd3e6c393c168dece6ee3ddea82a0" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (&amp;lsquo;ovo&amp;rsquo;) is always used as multi-class strategy. The parameter is ignored for binary classification.</source>
          <target state="translated">모양 (n_samples, n_classes)의 one-vs-rest ( 'ovr') 결정 함수를 다른 모든 분류 자로 반환할지 아니면 shape (n_samples)를 가진 libsvm의 원래 one-vs-one ( 'ovo') 결정 함수를 반환할지 여부 , n_classes * (n_classes-1) / 2). 그러나 일대일 ( 'ovo')은 항상 다중 클래스 전략으로 사용됩니다. 이진 분류에서는 매개 변수가 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="ebccc28d7f21db5d9325894eec5fc9e6d02d15c3" translate="yes" xml:space="preserve">
          <source>Whether to return dense output even when the input is sparse. If &lt;code&gt;False&lt;/code&gt;, the output is sparse if both input arrays are sparse.</source>
          <target state="translated">입력이 드문 경우에도 조밀 한 출력을 반환할지 여부입니다. 경우 &lt;code&gt;False&lt;/code&gt; 를 모두 입력 배열이 부족한 경우, 출력은 스파 스입니다.</target>
        </trans-unit>
        <trans-unit id="12da7a3ff0421b885caca94ef2caa65b1b016c5f" translate="yes" xml:space="preserve">
          <source>Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation.</source>
          <target state="translated">순방향 경로를 따라 0이 아닌 계수의 모든 값을 반환할지 여부입니다. 교차 유효성 검사에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="6a16a084b2e44866fb8e9c04ea892f46ef9407d0" translate="yes" xml:space="preserve">
          <source>Whether to return the estimators fitted on each split.</source>
          <target state="translated">각 분할에 맞는 견적서를 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="5e04c1a3b3a800f3b607834e4028e06fab1a7b16" translate="yes" xml:space="preserve">
          <source>Whether to return the fit and score times.</source>
          <target state="translated">적합도 및 점수 시간을 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="a0296def7d7feccd7f063d12d572b0bb6de2b0cd" translate="yes" xml:space="preserve">
          <source>Whether to return the number of iterations or not.</source>
          <target state="translated">반복 횟수를 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="642c6fa9e4316671c7770b59f9d72b6641ef628f" translate="yes" xml:space="preserve">
          <source>Whether to return the number of iterations.</source>
          <target state="translated">반복 횟수를 반환할지 여부.</target>
        </trans-unit>
        <trans-unit id="b549a24da790409f4ec3623e7b38a8b83191c416" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction.</source>
          <target state="translated">사후 예측의 표준 편차를 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b33ff2a27948731af021590a907c614a500fc29d" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction. All zeros in this case.</source>
          <target state="translated">사후 예측의 표준 편차를 반환할지 여부입니다. 이 경우 모든 0입니다.</target>
        </trans-unit>
        <trans-unit id="7f1c62ca9183e80a5d93cc97ae0f6b09aa7ca43f" translate="yes" xml:space="preserve">
          <source>Whether to sample from the (Gaussian) predictive posterior of the fitted estimator for each imputation. Estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method if set to &lt;code&gt;True&lt;/code&gt;. Set to &lt;code&gt;True&lt;/code&gt; if using &lt;code&gt;IterativeImputer&lt;/code&gt; for multiple imputations.</source>
          <target state="translated">각 대치에 대해 적합 된 추정량의 (가우스) 예측 사후에서 샘플링할지 여부입니다. Estimator는 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우 &lt;code&gt;predict&lt;/code&gt; 메서드 에서 &lt;code&gt;return_std&lt;/code&gt; 를 지원해야합니다 . 다중 대치에 &lt;code&gt;IterativeImputer&lt;/code&gt; 를 사용하는 경우 &lt;code&gt;True&lt;/code&gt; 로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="ef9a1d5fe208dc604ffba3d0e60394e32f61bdb0" translate="yes" xml:space="preserve">
          <source>Whether to scale X and Y.</source>
          <target state="translated">X와 Y를 스케일할지 여부</target>
        </trans-unit>
        <trans-unit id="06ccde346dfb7273af2d0810793b1fb5e47df0dc" translate="yes" xml:space="preserve">
          <source>Whether to show informative labels for impurity, etc. Options include &amp;lsquo;all&amp;rsquo; to show at every node, &amp;lsquo;root&amp;rsquo; to show only at the top root node, or &amp;lsquo;none&amp;rsquo; to not show at any node.</source>
          <target state="translated">불순물에 대한 정보 레이블을 표시할지 여부 등의 옵션에는 모든 노드에 표시되는 'all', 최상위 루트 노드에만 표시되는 'root'또는 노드에 표시되지 않는 'none'이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="bcd035c2f558018eebfd4e5534f5df5bef89069a" translate="yes" xml:space="preserve">
          <source>Whether to shuffle dataset.</source>
          <target state="translated">데이터 세트를 섞을 지 여부.</target>
        </trans-unit>
        <trans-unit id="9ebdad6142b719150be2c8f67618dba47007c5fd" translate="yes" xml:space="preserve">
          <source>Whether to shuffle each class&amp;rsquo;s samples before splitting into batches. Note that the samples within each split will not be shuffled.</source>
          <target state="translated">배치로 분할하기 전에 각 클래스의 샘플을 섞을 지 여부입니다. 각 분할 내의 샘플은 섞이지 않습니다.</target>
        </trans-unit>
        <trans-unit id="8704d580bb26c2f6617363a0297f26abfb9fda30" translate="yes" xml:space="preserve">
          <source>Whether to shuffle each stratification of the data before splitting into batches.</source>
          <target state="translated">배치로 분할하기 전에 데이터의 각 계층화를 섞을 지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="5de7b9303caa771da78304a93ebeac224ba77f9b" translate="yes" xml:space="preserve">
          <source>Whether to shuffle samples in each iteration. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">각 반복에서 샘플을 섞을 지 여부. solver = 'sgd'또는 'adam'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="3558a0c3a77b9ce3c242cc621a2d776c46a133af" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting into batches.</source>
          <target state="translated">배치로 분할하기 전에 데이터를 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="2b23f04bb0d66dad8fab9e868398f1780b26b6ed" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled.</source>
          <target state="translated">배치로 분할하기 전에 데이터를 섞을 지 여부입니다. 각 분할 내의 샘플은 섞이지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ddc8f26baf73b311e3fd82ce49af7a5449330660" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting it in batches.</source>
          <target state="translated">데이터를 일괄 적으로 분할하기 전에 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="f23a63355a4e388bf6ee14cbad5c7c9c8b8c8007" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the samples.</source>
          <target state="translated">샘플을 섞을 지 여부.</target>
        </trans-unit>
        <trans-unit id="a5c75421672ae29d738aa02687f5d9f3ec0cf20c" translate="yes" xml:space="preserve">
          <source>Whether to shuffle training data before taking prefixes of it based on``train_sizes``.</source>
          <target state="translated">``train_sizes ''를 기반으로 접두사를 사용하기 전에 교육 데이터를 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="ce33fd98a79a5db67d420efb6fcabed70acb96c4" translate="yes" xml:space="preserve">
          <source>Whether to sort x before computing. If False, assume that x must be either monotonic increasing or monotonic decreasing. If True, y is used to break ties when sorting x. Make sure that y has a monotonic relation to x when setting reorder to True.</source>
          <target state="translated">계산하기 전에 x를 정렬할지 여부 False이면 x가 단조 증가 또는 단조 감소 여야한다고 가정하십시오. True이면 x를 정렬 할 때 연결을 끊기 위해 y가 사용됩니다. 재정렬을 True로 설정할 때 y가 x와 단조로운 관계인지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="5091491cac888f3972a1197cfef1256978c18b5f" translate="yes" xml:space="preserve">
          <source>Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.</source>
          <target state="translated">희소 피쳐 벡터를 음수 부분과 양수 부분의 연결로 분할할지 여부입니다. 이는 다운 스트림 분류기의 성능을 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8b58e41338c55eaf50346244bd6082e4f3c1a628" translate="yes" xml:space="preserve">
          <source>Whether to use Nesterov&amp;rsquo;s momentum. Only used when solver=&amp;rsquo;sgd&amp;rsquo; and momentum &amp;gt; 0.</source>
          <target state="translated">Nesterov의 운동량 사용 여부. solver = 'sgd'이고 운동량이&amp;gt; 0 일 때만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6cb6c1fd9950933ec2b1bd309d3cbc04da69eccd" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-targets&quot;&gt;n_targets&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-samples&quot;&gt;n_samples&lt;/a&gt; is very large. Note that if you already have such matrices, you can pass them directly to the fit method.</source>
          <target state="translated">계산 속도를 높이기 위해 미리 계산 된 Gram 및 Xy 행렬을 사용할지 여부입니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-targets&quot;&gt;n_targets&lt;/a&gt; 또는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-n-samples&quot;&gt;n_samples&lt;/a&gt; 가 매우 클 때 성능을 향상시킵니다 . 이러한 행렬이 이미있는 경우 해당 행렬을 fit 메서드에 직접 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d97545296a16ed9ee24e38ded3551b9344d66c64" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when &lt;code&gt;n_targets&lt;/code&gt; or &lt;code&gt;n_samples&lt;/code&gt; is very large. Note that if you already have such matrices, you can pass them directly to the fit method.</source>
          <target state="translated">계산 속도를 높이기 위해 사전 계산 된 Gram 및 Xy 매트릭스 사용 여부 &lt;code&gt;n_targets&lt;/code&gt; 또는 &lt;code&gt;n_samples&lt;/code&gt; 가 매우 큰 경우 성능을 향상시킵니다 . 이러한 행렬이 이미있는 경우 직접 적합한 방법으로 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7dc600168d6ae4c500452eb14badcdfddafb11ff" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &amp;lsquo;auto&amp;rsquo; let us decide. The Gram matrix can also be passed as argument, but it will be used only for the selection of parameter alpha, if alpha is &amp;lsquo;aic&amp;rsquo; or &amp;lsquo;bic&amp;rsquo;.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 'auto'로 설정하면 결정하겠습니다. 그람 행렬은 인수로 전달 될 수도 있지만, 알파가 'aic'또는 'bic'인 경우 매개 변수 alpha를 선택하는 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0057a82fcc5e3ed086a2d1961e27e45b3f58597f" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 그램 행렬은 인수로 전달 될 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="419f7e60c7c4f4f84360a1078ab4b36ce5bf208a" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 그램 행렬은 인수로 전달 될 수도 있습니다. 희소 입력의 경우이 옵션은 희소성을 유지하기 위해 항상 &lt;code&gt;True&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="18cfe378dd4d6390fca460de4e70af73f097cdf4" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 우리는 X의 부분 집합 만 사용할 것이기 때문에 그람 행렬은 인수로 전달 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="2bc24bd08e69b99e2a7b63ee3e5ac92e16a75bc1" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 그램 행렬은 인수로 전달 될 수도 있습니다. 희소 입력의 경우이 옵션은 희소성을 유지하기 위해 항상 &lt;code&gt;True&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="1efc80d653c0b8715eb15bdf1b19f3becd74a957" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 True로 설정하면 n_iter_no_change 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 일부를 유효성 검사로 자동으로 설정하고 훈련을 종료합니다.</target>
        </trans-unit>
        <trans-unit id="46750b1ddce70e8be53fc930ce5b9a753b67b8f6" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score returned by the &lt;code&gt;score&lt;/code&gt; method is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs.</source>
          <target state="translated">유효성 검사 점수가 개선되지 않을 때 조기 중지를 사용하여 훈련을 종료할지 여부입니다. True로 설정하면 학습 데이터의 일부를 유효성 검사로 자동 설정하고 &lt;code&gt;score&lt;/code&gt; 메서드에서 반환 된 유효성 검사 점수 가 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 최소 &lt;code&gt;tol&lt;/code&gt; 만큼 향상되지 않으면 교육을 종료 합니다.</target>
        </trans-unit>
        <trans-unit id="febff6e36a4dd65c498e48c75789d34dc34067ef" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score returned by the &lt;code&gt;score&lt;/code&gt; method is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사 점수가 개선되지 않을 때 조기 중지를 사용하여 훈련을 종료할지 여부입니다. True로 설정하면 학습 데이터의 계층화 된 부분을 유효성 검사로 자동 설정하고 &lt;code&gt;score&lt;/code&gt; 메서드에서 반환 된 유효성 검사 점수 가 n_iter_no_change 연속 epoch에 대해 최소 tol만큼 향상되지 않을 때 교육을 종료 합니다.</target>
        </trans-unit>
        <trans-unit id="9af303ba091050935df1b1843777cae63d69ca4c" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 true로 설정하면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 유효성 검사 점수가 최소한 &lt;code&gt;tol&lt;/code&gt; 만큼 개선되지 않으면 훈련 데이터의 10 %를 유효성 검사로 자동 설정하고 교육을 종료 합니다. solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="998484dc55c21823abb36e2b54f5b8f623a0a41f" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 true로 설정하면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 10 %를 유효성 검사로 자동 설정하고 교육을 종료 합니다. solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="87617bcbcc672722844a1d3a57de3b252ff02aa0" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. The split is stratified, except in a multilabel setting. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">유효성 검사 점수가 개선되지 않을 때 조기 중지를 사용하여 훈련을 종료할지 여부입니다. true로 설정하면 학습 데이터의 10 %를 검증으로 자동 설정하고 검증 점수가 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 최소 tol만큼 향상되지 않을 때 훈련을 종료 합니다. 다중 레이블 설정을 제외하고 분할은 계층화됩니다. solver = 'sgd'또는 'adam'인 경우에만 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="cec695b146ca339e70be6934dce1a5005aa741f4" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사시 조기 중지를 사용하여 교육을 종료할지 여부 점수가 향상되지 않습니다. True로 설정하면 n_iter_no_change 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 일부를 유효성 검사로 자동으로 설정하고 훈련을 종료합니다.</target>
        </trans-unit>
        <trans-unit id="09b2b6c8cb36dc98a800b35af81d1805bc50fee1" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사시 조기 중지를 사용하여 훈련을 종료할지 여부입니다. 점수가 향상되지 않습니다. True로 설정하면 훈련 데이터의 계층화 된 부분을 검증으로 자동 설정하고 검증 점수가 n_iter_no_change 연속 epoch에 대해 최소 tol만큼 향상되지 않으면 훈련을 종료합니다.</target>
        </trans-unit>
        <trans-unit id="aad0324e5e5b11eda436b08ef9513e34456be6fd" translate="yes" xml:space="preserve">
          <source>Whether to use mini-batch k-means, which is faster but may get different results.</source>
          <target state="translated">미니 배치 k- 평균을 사용할지 여부는 더 빠르지 만 다른 결과를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="098e9f05a9707c21daca2709c375c5f67a6fc326" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the R^2 on unseen data.</source>
          <target state="translated">보이지 않는 데이터에 대한 R ^ 2를 추정하기 위해 가방 외부 샘플 사용 여부입니다.</target>
        </trans-unit>
        <trans-unit id="cf1378e2f07c46392b05b68a8d3fb4a1b87de256" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization accuracy.</source>
          <target state="translated">일반화 정확도를 평가하기 위해 비 가방 샘플을 사용할지 여부.</target>
        </trans-unit>
        <trans-unit id="b9d7a8d80bd713aecc3b75a6342d626d4ac28b69" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization error.</source>
          <target state="translated">일반화 오류를 추정하기 위해 가방 외부 샘플 사용 여부입니다.</target>
        </trans-unit>
        <trans-unit id="3aa24f38e2caae33363ee03e7cecc2915d7b4a6e" translate="yes" xml:space="preserve">
          <source>Whether to use the shrinking heuristic.</source>
          <target state="translated">축소 휴리스틱 사용 여부</target>
        </trans-unit>
        <trans-unit id="f030250afefd9a8186bdcbf55c79f9c30e8d6a17" translate="yes" xml:space="preserve">
          <source>Whether to use the shrinking heuristic. See the &lt;a href=&quot;../svm#shrinking-svm&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">축소 휴리스틱을 사용할지 여부입니다. &lt;a href=&quot;../svm#shrinking-svm&quot;&gt;사용자 안내서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0fcee6cb3493ee9e39e4cf5b70ffc63bc5b7fc72" translate="yes" xml:space="preserve">
          <source>Whether to zip the stored data on disk. If an integer is given, it should be between 1 and 9, and sets the amount of compression. Note that compressed arrays cannot be read by memmapping.</source>
          <target state="translated">저장된 데이터를 디스크에 압축할지 여부 정수가 제공되면 1과 9 사이 여야하며 압축 량을 설정합니다. 압축 배열은 memmapping으로 읽을 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="67c666eda0eb6c9f89cf06ca0fe8ba3d19260935" translate="yes" xml:space="preserve">
          <source>Whether transform should produce scipy.sparse matrices.</source>
          <target state="translated">변환이 scipy.sparse 행렬을 생성해야하는지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="a8c18609d5573425cb13e22e1562611896bad931" translate="yes" xml:space="preserve">
          <source>Whether transform should produce scipy.sparse matrices. True by default.</source>
          <target state="translated">변환이 scipy.sparse 행렬을 생성해야하는지 여부 기본적으로 true입니다.</target>
        </trans-unit>
        <trans-unit id="a2a2f7408a4ab356f12406498e7b656415a8f8e4" translate="yes" xml:space="preserve">
          <source>Whether y_prob needs to be normalized into the [0, 1] interval, i.e. is not a proper probability. If True, the smallest value in y_prob is linearly mapped onto 0 and the largest one onto 1.</source>
          <target state="translated">y_prob를 [0, 1] 간격으로 정규화해야하는지 여부, 즉 적절한 확률이 아닙니다. True이면 y_prob의 가장 작은 값이 0에 선형으로 매핑되고 가장 큰 값이 1에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="96e3c213b0373954a6f42c2953a288bf58e9c8e1" translate="yes" xml:space="preserve">
          <source>Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper probability. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto 1.</source>
          <target state="translated">y_prob를 bin [0, 1]로 정규화해야하는지 여부, 즉 적절한 확률이 아닙니다. True이면 y_prob에서 가장 작은 값은 0에 매핑되고 가장 큰 값은 1에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="673ccf9c3156ee120e948d124a09a8f79d0986df" translate="yes" xml:space="preserve">
          <source>Which SVD method to use. If &amp;lsquo;lapack&amp;rsquo; use standard SVD from scipy.linalg, if &amp;lsquo;randomized&amp;rsquo; use fast &lt;code&gt;randomized_svd&lt;/code&gt; function. Defaults to &amp;lsquo;randomized&amp;rsquo;. For most applications &amp;lsquo;randomized&amp;rsquo; will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for &lt;code&gt;iterated_power&lt;/code&gt;. If this is not sufficient, for maximum precision you should choose &amp;lsquo;lapack&amp;rsquo;.</source>
          <target state="translated">사용할 SVD 방법. 'lapack'이 scipy.linalg의 표준 SVD를 사용하는 경우 'randomized'는 빠른 &lt;code&gt;randomized_svd&lt;/code&gt; 기능을 사용하십시오. 기본값은 '무작위 화'입니다. 대부분의 응용 분야에서 '무작위 화'는 상당한 속도 향상을 제공하면서 충분히 정밀합니다. &lt;code&gt;iterated_power&lt;/code&gt; 에 대해 더 높은 값을 설정하여 정확도를 향상시킬 수도 있습니다 . 이것이 충분하지 않으면, 최대 정밀도를 위해 'lapack'을 선택해야합니다.</target>
        </trans-unit>
        <trans-unit id="3f4bd36722594cddb9c4f04fa1feac290cb2c703" translate="yes" xml:space="preserve">
          <source>Which affinity to use. At the moment &amp;lsquo;precomputed&amp;rsquo; and &lt;code&gt;euclidean&lt;/code&gt; are supported. &amp;lsquo;euclidean&amp;rsquo; uses the negative squared euclidean distance between points.</source>
          <target state="translated">사용할 선호도. 현재 '미리 계산'과 &lt;code&gt;euclidean&lt;/code&gt; 가 지원됩니다. '유클리드'는 점 사이의 음의 제곱 유클리드 거리를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="f51286fa5438dabdb2fdc9e6a35c79244bd208d2" translate="yes" xml:space="preserve">
          <source>Which affinity to use. At the moment &lt;code&gt;precomputed&lt;/code&gt; and &lt;code&gt;euclidean&lt;/code&gt; are supported. &lt;code&gt;euclidean&lt;/code&gt; uses the negative squared euclidean distance between points.</source>
          <target state="translated">사용할 친화력 현재 &lt;code&gt;precomputed&lt;/code&gt; 및 &lt;code&gt;euclidean&lt;/code&gt; 가 지원됩니다. &lt;code&gt;euclidean&lt;/code&gt; 점 사이의 음의 제곱 유클리드 거리를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="0a291d7f5d694ce4dae77d370f938e385f2f41cf" translate="yes" xml:space="preserve">
          <source>Which kind of estimators should be returned. If None, no filter is applied and all estimators are returned. Possible values are &amp;lsquo;classifier&amp;rsquo;, &amp;lsquo;regressor&amp;rsquo;, &amp;lsquo;cluster&amp;rsquo; and &amp;lsquo;transformer&amp;rsquo; to get estimators only of these specific types, or a list of these to get the estimators that fit at least one of the types.</source>
          <target state="translated">어떤 종류의 추정기가 리턴되어야합니까. None이면 필터가 적용되지 않고 모든 추정기가 반환됩니다. 가능한 값은 이러한 특정 유형의 추정기만 가져 오는 '분류기', '회귀 기', '클러스터'및 '변환기'또는 하나 이상의 유형에 맞는 추정기를 얻기위한 이들 목록입니다.</target>
        </trans-unit>
        <trans-unit id="9ab873abc046d5e79c6628d1e73d15a9a8c8a011" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">사용할 연계 기준입니다. 연계 기준에 따라 피쳐 세트간에 사용할 거리가 결정됩니다. 알고리즘은이 기준을 최소화하는 클러스터 쌍을 병합합니다.</target>
        </trans-unit>
        <trans-unit id="17ba6fc895d15866ea1e60a4db791726755dc58f" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">사용할 연계 기준입니다. 연계 기준에 따라 관측 세트간에 사용할 거리가 결정됩니다. 알고리즘은이 기준을 최소화하는 클러스터 쌍을 병합합니다.</target>
        </trans-unit>
        <trans-unit id="7c032065ee1e199d65a12de581955d2510001155" translate="yes" xml:space="preserve">
          <source>Which metric to use for computing pairwise distances between samples from the original input space. If metric is &amp;lsquo;precomputed&amp;rsquo;, X must be a matrix of pairwise distances or squared distances. Otherwise, see the documentation of argument metric in sklearn.pairwise.pairwise_distances for a list of available metrics.</source>
          <target state="translated">원래 입력 공간의 샘플 간 쌍별 거리를 계산하는 데 사용할 메트릭입니다. 메트릭이 '미리 계산 된'경우 X는 쌍 거리 또는 제곱 거리의 행렬이어야합니다. 그렇지 않은 경우 사용 가능한 메트릭 목록은 sklearn.pairwise.pairwise_distances의 인수 메트릭 문서를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="7ca40022d1c5dcd68056f855d9cb29fbb48c9062" translate="yes" xml:space="preserve">
          <source>Which model is the best is a matter of subjective judgement: do we want to favor models that only capture the big picture to summarize and explain most of the structure of the data while ignoring the details or do we prefer models that closely follow the high density regions of the signal?</source>
          <target state="translated">어떤 모델이 가장 좋은지는 주관적 판단의 문제입니다. 우리는 세부 사항을 무시하면서 데이터 구조의 대부분을 요약하고 설명하기 위해 큰 그림 만 캡처하는 모델을 선호하고 싶습니까? 신호의 영역?</target>
        </trans-unit>
        <trans-unit id="3bad9460d63c46d91cb72ce5e70c801fdf7d8e5b" translate="yes" xml:space="preserve">
          <source>Which model is the best is a matter of subjective judgment: do we want to favor models that only capture the big picture to summarize and explain most of the structure of the data while ignoring the details or do we prefer models that closely follow the high density regions of the signal?</source>
          <target state="translated">어떤 모델이 가장 좋은지는 주관적인 판단의 문제입니다. 세부 사항을 무시하고 대부분의 데이터 구조를 요약하고 설명하기 위해 큰 그림 만 캡처하는 모델을 선호합니까 아니면 고밀도를 밀접하게 따르는 모델을 선호합니까? 신호의 영역?</target>
        </trans-unit>
        <trans-unit id="f9b07db46ccf2aab5a3f471f9311e28ace0c3658" translate="yes" xml:space="preserve">
          <source>Which strategy to use to initialize the missing values. Same as the &lt;code&gt;strategy&lt;/code&gt; parameter in &lt;a href=&quot;sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt; Valid values: {&amp;ldquo;mean&amp;rdquo;, &amp;ldquo;median&amp;rdquo;, &amp;ldquo;most_frequent&amp;rdquo;, or &amp;ldquo;constant&amp;rdquo;}.</source>
          <target state="translated">누락 된 값을 초기화하는 데 사용할 전략입니다. &lt;a href=&quot;sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;strategy&lt;/code&gt; 매개 변수 와 동일 합니다. 유효한 값 : { &quot;mean&quot;, &quot;median&quot;, &quot;most_frequent&quot;또는 &quot;constant&quot;}.</target>
        </trans-unit>
        <trans-unit id="e8cddae54ed0b1b16ee030ff58543e83ff547ded" translate="yes" xml:space="preserve">
          <source>While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.</source>
          <target state="translated">Isomap, LLE 및 변형은 단일 연속 저 차원 매니 폴드를 전개하는 데 가장 적합하지만 t-SNE는 데이터의 로컬 구조에 중점을두고 S- 곡선 예제에서 강조된대로 클러스터 된 로컬 샘플 그룹을 추출하는 경향이 있습니다. 로컬 구조를 기반으로 샘플을 그룹화하는이 기능은 숫자 데이터 세트에서와 같이 한 번에 여러 매니 폴드를 포함하는 데이터 세트를 시각적으로 얽히는 데 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d80a7676bafb31f5f9bcb99f7aed7e1817a9d535" translate="yes" xml:space="preserve">
          <source>While SVM models derived from &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; use &lt;code&gt;C&lt;/code&gt; as regularization parameter, most other estimators use &lt;code&gt;alpha&lt;/code&gt;. The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; regression, the relation between them is given as \(C = \frac{1}{alpha}\).</source>
          <target state="translated">&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 및 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 에서 파생 된 SVM 모델 은 &lt;code&gt;C&lt;/code&gt; 를 정규화 매개 변수로 사용하지만 대부분의 다른 추정기는 &lt;code&gt;alpha&lt;/code&gt; 를 사용 합니다. 두 모델의 정규화 양 사이의 정확한 동등성은 모델에 의해 최적화 된 정확한 목적 함수에 달려 있습니다. 예를 들어, 사용 된 추정값이 &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; 회귀 분석 인 경우 이들 사이의 관계는 \ (C = \ frac {1} {alpha} \)로 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="e19b90ac75c89776c8025a4fe5ef49998061f2d2" translate="yes" xml:space="preserve">
          <source>While SVM models derived from &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; use &lt;code&gt;C&lt;/code&gt; as regularization parameter, most other estimators use &lt;code&gt;alpha&lt;/code&gt;. The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; regression, the relation between them is given as \(C = \frac{1}{alpha}\).</source>
          <target state="translated">&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 및 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 에서 파생 된 SVM 모델 은 정규화 매개 변수로 &lt;code&gt;C&lt;/code&gt; 를 사용 하지만 대부분의 다른 추정기는 &lt;code&gt;alpha&lt;/code&gt; 를 사용 합니다. 두 모델의 정규화 양 간의 정확한 동등성은 모델에 의해 최적화 된 정확한 목적 함수에 따라 다릅니다. 예를 들어 사용 된 추정기가 &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; 회귀 인 경우 이들 간의 관계는 \ (C = \ frac {1} {alpha} \)로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="b688dd0a76a7a11c780bc8304b263ca13e9c529b" translate="yes" xml:space="preserve">
          <source>While both methods should be close in general, they might differ in some specific settings. The &amp;lsquo;brute&amp;rsquo; method assumes the existence of the data points \((x_S, x_C^{(i)})\). When the features are correlated, such artificial samples may have a very low probability mass. The &amp;lsquo;brute&amp;rsquo; and &amp;lsquo;recursion&amp;rsquo; methods will likely disagree regarding the value of the partial dependence, because they will treat these unlikely samples differently. Remember, however, that the primary assumption for interpreting PDPs is that the features should be independent.</source>
          <target state="translated">두 방법 모두 일반적으로 비슷해야하지만 일부 특정 설정에서 다를 수 있습니다. 'brute'방법은 데이터 포인트 \ ((x_S, x_C ^ {(i)}) \)의 존재를 가정합니다. 특성이 상관 될 때 이러한 인공 샘플은 확률 질량이 매우 낮을 수 있습니다. 'brute'및 'recursion'방법은 이러한 가능성이 낮은 샘플을 다르게 취급하기 때문에 부분 의존성의 값에 대해 동의하지 않을 수 있습니다. 그러나 PDP 해석의 주요 가정은 기능이 독립적이어야한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="fb6c7d447c33735b14bfab3f939085b49d50d551" translate="yes" xml:space="preserve">
          <source>While defining the custom scoring function alongside the calling function should work out of the box with the default joblib backend (loky), importing it from another module will be a more robust approach and work independently of the joblib backend.</source>
          <target state="translated">호출 함수와 함께 사용자 지정 스코어링 함수를 정의하는 동안 기본 joblib 백엔드 (loky)를 사용하여 즉시 작동해야하지만 다른 모듈에서 가져 오는 것이 더 강력한 접근 방식이며 joblib 백엔드와 독립적으로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="c20daeb41107431c48223b43ad4fe138aa4845d5" translate="yes" xml:space="preserve">
          <source>While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the data used to fit the estimator as this would not be evaluating the performance of the estimator on &lt;strong&gt;new data&lt;/strong&gt;. This is why datasets are often split into &lt;em&gt;train&lt;/em&gt; and &lt;em&gt;test&lt;/em&gt; data.</source>
          <target state="translated">학습 알고리즘을 실험하는 동안 &lt;strong&gt;새 데이터&lt;/strong&gt; 에 대한 추정기의 성능을 평가하지 않으므로 추정기에 맞는 데 사용 된 데이터에 대한 추정기의 예측을 테스트하지 않는 것이 중요 &lt;strong&gt;합니다&lt;/strong&gt; . 그렇기 때문에 데이터 세트가 종종 &lt;em&gt;열차&lt;/em&gt; 데이터 와 &lt;em&gt;테스트&lt;/em&gt; 데이터 로 분리됩니다 .</target>
        </trans-unit>
        <trans-unit id="f8808630553e3f4de197fcbd8d96757a9a769400" translate="yes" xml:space="preserve">
          <source>While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it is safer to use a &lt;a href=&quot;#timeseries-cv&quot;&gt;time-series aware cross-validation scheme&lt;/a&gt;. Similarly, if we know that the generative process has a group structure (samples collected from different subjects, experiments, measurement devices), it is safer to use &lt;a href=&quot;#group-cv&quot;&gt;group-wise cross-validation&lt;/a&gt;.</source>
          <target state="translated">iid 데이터는 기계 학습 이론에서 일반적인 가정이지만 실제로는 거의 적용되지 않습니다. 샘플이 시간 종속 프로세스를 사용하여 생성되었다는 것을 알고 있다면 &lt;a href=&quot;#timeseries-cv&quot;&gt;시계열 인식 교차 검증 체계&lt;/a&gt; 를 사용하는 것이 더 안전합니다 . 마찬가지로 생성 프로세스가 그룹 구조 (다른 주제, 실험, 측정 장치에서 수집 된 샘플)를 가지고 있다는 것을 알고 있다면 &lt;a href=&quot;#group-cv&quot;&gt;그룹 별 교차 검증&lt;/a&gt; 을 사용하는 것이 더 안전합니다 .</target>
        </trans-unit>
        <trans-unit id="d04cd5b9d0463d9bc89f841d571873fec4953569" translate="yes" xml:space="preserve">
          <source>While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it&amp;rsquo;s safer to use a &lt;a href=&quot;#timeseries-cv&quot;&gt;time-series aware cross-validation scheme&lt;/a&gt; Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use &lt;a href=&quot;#group-cv&quot;&gt;group-wise cross-validation&lt;/a&gt;.</source>
          <target state="translated">iid 데이터는 머신 러닝 이론에서 일반적인 가정이지만 실제로는 거의 유지되지 않습니다. 샘플이 시간 종속 프로세스를 사용 하여 생성 된 것을 알고 있다면 &lt;a href=&quot;#timeseries-cv&quot;&gt;시계열 인식 교차 검증 체계&lt;/a&gt; 를 사용하는 것이 더 안전합니다. 마찬가지로 생성 프로세스에 그룹 구조 (다른 주제에서 수집 한 샘플, 실험)가 있음을 알고있는 경우 &lt;a href=&quot;#group-cv&quot;&gt;그룹 단위 교차 검증&lt;/a&gt; 을 사용하는 것이 더 안전합니다 .</target>
        </trans-unit>
        <trans-unit id="0526a7a936d1a8c691dcdaa23691304fc8ebc7ef" translate="yes" xml:space="preserve">
          <source>While in the spirit of an online algorithm, the class &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt; does not implement &lt;code&gt;partial_fit&lt;/code&gt; because the algorithm is online along the features direction, not the samples direction.</source>
          <target state="translated">온라인 알고리즘의 정신에 있지만 &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt; 클래스 는 알고리즘이 샘플 방향이 아닌 기능 방향을 따라 온라인이기 때문에 &lt;code&gt;partial_fit&lt;/code&gt; 을 구현하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="c8d4a37562dd2bb8e9910fd82f43df80f682c63a" translate="yes" xml:space="preserve">
          <source>While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the &amp;lsquo;weight&amp;rsquo; axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.</source>
          <target state="translated">많은 알고리즘 (예 : SVM, K- 최근 접 이웃 및 로지스틱 회귀)과 같이 기능을 정규화해야하지만 직관적으로 PCA (Principle Component Analysis)는 정규화가 중요한 경우의 주요 예라고 생각할 수 있습니다. PCA에서는 분산을 최대화하는 구성 요소에 관심이 있습니다. 하나의 구성 요소 (예 : 사람의 키)가 각각의 스케일 (미터 대 킬로)로 인해 다른 구성 요소 (예 : 무게)보다 덜 변하면, PCA는 최대 편차의 방향이 '무게'축과 더 밀접하게 일치한다고 판단 할 수 있습니다. 스케일되지 않습니다. 1 미터의 높이 변화는 1 킬로그램의 무게 변화보다 훨씬 더 중요하다고 간주 될 수 있기 때문에, 이것은 분명히 부정확합니다.</target>
        </trans-unit>
        <trans-unit id="3e272f5577ff59f34cc0835d41b4f8bd0e7fa207" translate="yes" xml:space="preserve">
          <source>While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.</source>
          <target state="translated">한 버전의 scikit-learn을 사용하여 저장된 모델은 다른 버전에서로드 될 수 있지만 이는 완전히 지원되지 않으며 바람직하지 않습니다. 또한 이러한 데이터에 대해 수행 된 작업은 예상치 못한 결과를 초래할 수 있음을 명심해야합니다.</target>
        </trans-unit>
        <trans-unit id="5cdd1c0f02e03a5a1b156076678017404f8561ab" translate="yes" xml:space="preserve">
          <source>While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell &lt;code&gt;[i, j]&lt;/code&gt; has value 1 if sample &lt;code&gt;i&lt;/code&gt; has label &lt;code&gt;j&lt;/code&gt; and value 0 otherwise.</source>
          <target state="translated">이진 대상과 같은 멀티 클래스 데이터가 클래스 레이블의 배열로 메트릭에 제공되는 반면 멀티 라벨 데이터는 표시기 행렬로 지정됩니다. &lt;code&gt;[i, j]&lt;/code&gt; 샘플 &lt;code&gt;i&lt;/code&gt; 에 레이블 &lt;code&gt;j&lt;/code&gt; 가 있고 값이 0이면 셀 [i, j] 에 값 1이 있습니다. .</target>
        </trans-unit>
        <trans-unit id="ed422f68a65d31027201e13d55d1a7c59ba06f45" translate="yes" xml:space="preserve">
          <source>While not particularly fast to process, Python&amp;rsquo;s &lt;code&gt;dict&lt;/code&gt; has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.</source>
          <target state="translated">처리 속도가 빠르지는 않지만 Python의 &lt;code&gt;dict&lt;/code&gt; 에는 사용하기 편리하고, 희박하고 (없는 기능은 저장할 필요가 없음) 값 외에도 기능 이름을 저장하는 이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="6575e62afd8d9d86f5e5759f0c1f4bf12add49e4" translate="yes" xml:space="preserve">
          <source>While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure.</source>
          <target state="translated">개별 단어 대신 n- 그램을 추출하여 일부 로컬 위치 정보를 보존 할 수 있지만, bag of words 및 n-gram은 문서의 내부 구조의 대부분을 파괴하므로 해당 내부 구조가 의미하는 대부분의 의미를 파괴합니다.</target>
        </trans-unit>
        <trans-unit id="f9eb71a899b2efe02a5e86463ae919f1b60ed0f7" translate="yes" xml:space="preserve">
          <source>While the &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;TruncatedSVD&lt;/code&gt;&lt;/a&gt; transformer works with any (sparse) feature matrix, using it on tf&amp;ndash;idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (&lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt;) to bring the feature values closer to a Gaussian distribution, compensating for LSA&amp;rsquo;s erroneous assumptions about textual data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;TruncatedSVD&lt;/code&gt; &lt;/a&gt; 변환기는 모든 (가급적) 피처 매트릭스와 함께 작동 하지만 LSA / 문서 처리 설정에서 원시 주파수 카운트보다 tf-idf 매트릭스에서이를 사용하는 것이 좋습니다. 특히, 피처 값을 가우시안 분포에 가깝게 가져 와서 텍스트 데이터에 대한 LSA의 잘못된 가정을 보상하기 위해 &lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt; 스케일링 및 역 문서 빈도를 켜야합니다 ( sublinear_tf = True, use_idf = True ).</target>
        </trans-unit>
        <trans-unit id="f9e8e04743afddc659f8d58efbc124813976ac9e" translate="yes" xml:space="preserve">
          <source>While the &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;TruncatedSVD&lt;/code&gt;&lt;/a&gt; transformer works with any feature matrix, using it on tf&amp;ndash;idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (&lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt;) to bring the feature values closer to a Gaussian distribution, compensating for LSA&amp;rsquo;s erroneous assumptions about textual data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;TruncatedSVD&lt;/code&gt; &lt;/a&gt; 변환기는 모든 기능 매트릭스에서 작동 하지만 LSA / 문서 처리 설정의 원시 주파수 수보다 tf-idf 매트릭스에서 사용하는 것이 좋습니다. 특히, 텍스트 데이터에 대한 LSA의 잘못된 가정을 보상하기 위해 서브 선형 스케일링 및 역 문서 빈도를 &lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt; ( sublinear_tf = True, use_idf = True ).</target>
        </trans-unit>
        <trans-unit id="29a2650e25fbd5181744282ce886a2b96e4ac93d" translate="yes" xml:space="preserve">
          <source>While the above example sets the &lt;code&gt;standardize&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt;&lt;code&gt;PowerTransformer&lt;/code&gt;&lt;/a&gt; will apply zero-mean, unit-variance normalization to the transformed output by default.</source>
          <target state="translated">상기 예는 설정 동안 &lt;code&gt;standardize&lt;/code&gt; 옵션을 ' &lt;code&gt;False&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt; &lt;code&gt;PowerTransformer&lt;/code&gt; 는&lt;/a&gt; 기본적으로 변환 출력하는 제로 - 평균 단위 - 분산 정규화를 적용한다.</target>
        </trans-unit>
        <trans-unit id="e50cabfff84e84e9590f53c8299406f023e4e46d" translate="yes" xml:space="preserve">
          <source>While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.</source>
          <target state="translated">LML을 최적화하여 선택한 하이퍼 파라미터는 상당히 큰 LML을 갖지만 테스트 데이터의 로그 손실에 따라 약간 성능이 떨어집니다. 이 그림은 클래스 경계에서 클래스 확률의 급격한 변화를 나타 내기 때문에 (좋은) 클래스 클래스 경계에서 0.5에 가까운 확률로 예측했기 때문에 (나쁜)이 바람직하지 않은 효과는 GPC에서 내부적으로 사용하는 라플라스 근사치.</target>
        </trans-unit>
        <trans-unit id="2c9458b61e67bda12a1df752c0cc6921b6ff30dd" translate="yes" xml:space="preserve">
          <source>While the parameter &lt;code&gt;min_samples&lt;/code&gt; primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter &lt;code&gt;eps&lt;/code&gt; is &lt;em&gt;crucial to choose appropriately&lt;/em&gt; for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as &lt;code&gt;-1&lt;/code&gt; for &amp;ldquo;noise&amp;rdquo;). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below).</source>
          <target state="translated">매개 변수 &lt;code&gt;min_samples&lt;/code&gt; 는 주로 알고리즘이 노이즈에 대해 얼마나 내성이 있는지를 제어 하지만 (노이즈가 많은 데이터 세트에서이 매개 변수를 늘리는 것이 바람직 할 수 있음), 매개 변수 &lt;code&gt;eps&lt;/code&gt; 는 데이터 세트 및 거리 함수에 대해 &lt;em&gt;적절하게 선택하는 데 중요&lt;/em&gt; 하며 일반적으로 남겨 둘 수 없습니다. 기본값으로. 포인트의 로컬 이웃을 제어합니다. 너무 작게 선택하면 대부분의 데이터가 전혀 클러스터되지 않으며 &lt;code&gt;-1&lt;/code&gt; 로 레이블이 지정됩니다.&quot;소음&quot;). 너무 크게 선택하면 가까운 클러스터가 하나의 클러스터로 병합되고 결국 전체 데이터 세트가 단일 클러스터로 반환됩니다. 이 매개 변수를 선택하기위한 몇 가지 휴리스틱이 문헌에서 논의되었습니다. 예를 들어 가장 가까운 이웃 거리 플롯의 무릎을 기반으로합니다 (아래 참조에서 논의 됨).</target>
        </trans-unit>
        <trans-unit id="17e6df329175ba9fee759ed839a4afe469b184fd" translate="yes" xml:space="preserve">
          <source>While the tf&amp;ndash;idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the &lt;code&gt;binary&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. In particular, some estimators such as &lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes&lt;/a&gt; explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf&amp;ndash;idf values while the binary occurrence info is more stable.</source>
          <target state="translated">tf-idf 정규화는 종종 매우 유용하지만 이진 발생 마커가 더 나은 기능을 제공 할 수 있습니다. &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;binary&lt;/code&gt; 매개 변수를 사용하면 됩니다. 특히, &lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes&lt;/a&gt; 와 같은 일부 추정량에서는 불연속 부울 랜덤 변수를 명시 적으로 모델링합니다. 또한 매우 짧은 텍스트에는 노이즈가 많은 tf-idf 값이있을 수 있지만 이진 발생 정보는 더 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="ed181b0221327c005991b804ef6da84808fa6b75" translate="yes" xml:space="preserve">
          <source>While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.</source>
          <target state="translated">이 예제는 알고리즘에 대한 직관을 제공하지만이 직감은 매우 높은 차원의 데이터에는 적용되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="20e141fdf1f5d5d16051f029790d3b9e841c723d" translate="yes" xml:space="preserve">
          <source>While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:</source>
          <target state="translated">매개 변수 설정 그리드를 사용하는 것이 현재 가장 널리 사용되는 매개 변수 최적화 방법이지만 다른 검색 방법에는 더 유리한 특성이 있습니다. &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt; 는 매개 변수에 대한 무작위 검색을 구현하며, 여기서 각 설정은 가능한 매개 변수 값에 대한 분포에서 샘플링됩니다. 이는 철저한 검색에 비해 두 가지 주요 이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="37619fc13053f82b7cb7da3d24ceb1598ab6d05c" translate="yes" xml:space="preserve">
          <source>White</source>
          <target state="translated">White</target>
        </trans-unit>
        <trans-unit id="ae7c1638fd1917cb535ca7c68b4bd5f19a47ea30" translate="yes" xml:space="preserve">
          <source>White kernel.</source>
          <target state="translated">화이트 커널.</target>
        </trans-unit>
        <trans-unit id="6eaf9e8193566018ccba0d72a95d7647c23f2585" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.</source>
          <target state="translated">화이트닝은 변환 된 신호 (구성 요소의 상대 분산 스케일)에서 일부 정보를 제거하지만 데이터를 고정 배선 가정을 존중하여 다운 스트림 추정기의 예측 정확도를 언젠가 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="07aaa00ad7b994406ce70b8bd7598f7e15e6859a" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions.</source>
          <target state="translated">화이트닝은 변환 된 신호 (구성 요소의 상대 분산 스케일)에서 일부 정보를 제거하지만 데이터를 일부 고정 배선 가정을 존중하도록하여 다운 스트림 추정기의 예측 정확도를 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c6caecec2578a0de52910be667f4fa7e322f3d31" translate="yes" xml:space="preserve">
          <source>Why does the plot above suggest that an increase in age leads to a decrease in wage? Why the &lt;a href=&quot;#marginal-dependencies&quot;&gt;initial pairplot&lt;/a&gt; is telling the opposite?</source>
          <target state="translated">위의 그림에서 연령이 증가하면 임금이 감소하는 이유는 무엇입니까? &lt;a href=&quot;#marginal-dependencies&quot;&gt;초기 쌍 그림&lt;/a&gt; 이 반대를 말하는 이유는 무엇 입니까?</target>
        </trans-unit>
        <trans-unit id="b5ed864ec9d16ad31c6639d1d4c3bf64e3372001" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for Davies-Bouldin index.</source>
          <target state="translated">Davies-Bouldin 지수에 대한 Wikipedia 항목.</target>
        </trans-unit>
        <trans-unit id="3ce8e9b9f756deae78c09a314c4cf49a1aacdb66" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for Discounted Cumulative Gain</source>
          <target state="translated">할인 된 누적 이득에 대한 위키 백과 항목</target>
        </trans-unit>
        <trans-unit id="a0184957526e21d06d99d8f077fe30eb4aaec4f9" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for contingency matrix</source>
          <target state="translated">우발성 매트릭스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="55a3b17abc1268c1d436ba897c97b456b993b4ea" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the (normalized) Mutual Information</source>
          <target state="translated">(정규화 된) 상호 정보에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="1f069c9fec7504cb4f8a493de2e1b54ffc547081" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Adjusted Mutual Information</source>
          <target state="translated">조정 된 상호 정보에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="8030a2f6eb81271b3b56dfad08af7aaea7fcfc10" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Average precision</source>
          <target state="translated">평균 정밀도에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="ca2fe3eff096e2c0ff94d3c0f6ce61af74cc646f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Brier score.</source>
          <target state="translated">Brier 점수를위한 Wikipedia 항목.</target>
        </trans-unit>
        <trans-unit id="ffd655e9eb3a21416da69aac696bc5ce043a000f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Cohen&amp;rsquo;s kappa.</source>
          <target state="translated">코헨의 카파 위키 백과.</target>
        </trans-unit>
        <trans-unit id="8d8ae14fc3bcf00321ca2d4b9c37c609195c6275" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the F1-score</source>
          <target state="translated">F1- 점수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="0d85777073541b6f8aecb3488f1962f6903fd77c" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Fowlkes-Mallows Index</source>
          <target state="translated">Fowlkes-Mallows Index에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="738fb31d9583a6207339f58c0335e89437aa096f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Jaccard index</source>
          <target state="translated">Jaccard 인덱스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="d69dce297a7e32abae3549494346594b424875bc" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Matthews Correlation Coefficient</source>
          <target state="translated">Matthews 상관 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="d1c0692994293b3fef98ac5de7dd74e23175c8d1" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Precision and recall</source>
          <target state="translated">정밀도 및 리콜을위한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="6c2dd7ccbd3afed766d1ee6ce92b068445c27bbb" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Receiver operating characteristic</source>
          <target state="translated">수신기 작동 특성에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="caae1d529b64ebeb0d4804273e9107122a389ac6" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the adjusted Rand index</source>
          <target state="translated">조정 랜드 인덱스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="ccc412d2bb1bb2397fbce7363889e5816eda01a2" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on Neighborhood Components Analysis</source>
          <target state="translated">Neighborhood Components Analysis에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="3b36309de0386ff9491f5f72624bcd77d6a05e19" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on Neighborhood Components Analysis &lt;a href=&quot;https://en.wikipedia.org/wiki/Neighbourhood_components_analysis&quot;&gt;https://en.wikipedia.org/wiki/Neighbourhood_components_analysis&lt;/a&gt;</source>
          <target state="translated">Neighbourhood Components Analysis에 대한 Wikipedia 항목 &lt;a href=&quot;https://en.wikipedia.org/wiki/Neighbourhood_components_analysis&quot;&gt;https://en.wikipedia.org/wiki/Neighbourhood_components_analysis&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="af0472efa729237e92d89bb05e9ca0c8e7f37b5f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Coefficient of determination</source>
          <target state="translated">결정 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="e345be5719f19335870d8d3a8cdd20b6bd307aa0" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hamming distance</source>
          <target state="translated">해밍 거리의 위키 백과 항목</target>
        </trans-unit>
        <trans-unit id="1857fa6b095ad66d104ea60f4be3df45f12529a3" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hinge loss</source>
          <target state="translated">힌지 손실에 대한 위키 백과 항목</target>
        </trans-unit>
        <trans-unit id="d4ccd1b47442c7552ebe73794cdd38515c5ffdef" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Lasso</source>
          <target state="translated">올가미의 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="8751f23b19110bb289e70c6d8c900548f6c9b761" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Least-angle regression</source>
          <target state="translated">최소 각 회귀에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="ed8f4a303fe71f9ad0ec1e1b74ef6fe644dad80d" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Silhouette Coefficient</source>
          <target state="translated">실루엣 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="0b665174747365aef367583fb0c32fb021d06a22" translate="yes" xml:space="preserve">
          <source>Wikipedia principal eigenvector</source>
          <target state="translated">Wikipedia 주요 고유 벡터</target>
        </trans-unit>
        <trans-unit id="713348b23d025b202ea7f033591c046a82a1973b" translate="yes" xml:space="preserve">
          <source>Will be ignored when &lt;code&gt;y_true&lt;/code&gt; is binary.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 가 이진 이면 무시됩니다 .</target>
        </trans-unit>
        <trans-unit id="af498f4dd6f24dbc1f93745e77fe6ed29d0b9d0c" translate="yes" xml:space="preserve">
          <source>Will return sparse matrix if set True else will return an array.</source>
          <target state="translated">True로 설정하면 희소 행렬을 반환하고 그렇지 않으면 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="f02c359862a5df44abc185413e06bdb77cfc5770" translate="yes" xml:space="preserve">
          <source>Williams, C.K.I. and Seeger, M. &amp;ldquo;Using the Nystroem method to speed up kernel machines&amp;rdquo;, Advances in neural information processing systems 2001</source>
          <target state="translated">Williams, CKI 및 Seeger, M.&amp;ldquo;Nystroem 방법을 사용하여 커널 시스템 속도 향상&amp;rdquo;, 신경 정보 처리 시스템의 발전 2001</target>
        </trans-unit>
        <trans-unit id="a20af0cf6ba0496377888d152bfba536fcfdefc1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;adjusted=True&lt;/code&gt;, balanced accuracy reports the relative increase from \(\texttt{balanced-accuracy}(y, \mathbf{0}, w) = \frac{1}{\text{n\_classes}}\). In the binary case, this is also known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;*Youden&amp;rsquo;s J statistic*&lt;/a&gt;, or &lt;em&gt;informedness&lt;/em&gt;.</source>
          <target state="translated">로 &lt;code&gt;adjusted=True&lt;/code&gt; 균형 정밀도 \에서 상대적 증가 (\ texttt {균형 정밀도} (Y \ mathbf {0}, w) = \ FRAC {1} {\ 텍스트 {n \ _classes를}} \)보고. 이진 경우에는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;* Youden 's J statistic *&lt;/a&gt; 또는 &lt;em&gt;informedness&lt;/em&gt; 라고도 합니다.</target>
        </trans-unit>
        <trans-unit id="ad0e52061072794be72972cbf40b994abb34f953" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;adjusted=True&lt;/code&gt;, balanced accuracy reports the relative increase from \(\texttt{balanced-accuracy}(y, \mathbf{0}, w) = \frac{1}{n\_classes}\). In the binary case, this is also known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;*Youden&amp;rsquo;s J statistic*&lt;/a&gt;, or &lt;em&gt;informedness&lt;/em&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=True&lt;/code&gt; 사용하면 균형 잡힌 정확도가 \ (\ texttt {balanced-accuracy} (y, \ mathbf {0}, w) = \ frac {1} {n \ _classes} \)에서 상대적 증가를보고합니다. 이진법의 경우 &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;* Youden의 J 통계량 *&lt;/a&gt; 또는 &lt;em&gt;정보 성 &lt;/em&gt;이라고도 합니다.</target>
        </trans-unit>
        <trans-unit id="8a7d860e7dc8979710329f97e747eaff0d3415d3" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=False&lt;/code&gt;, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the input data.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=False&lt;/code&gt; 모델은 전체 입력 데이터에 장착되고, 정지 기준은 상기 입력 데이터에 대해 계산 된 목적 함수에 기초한다.</target>
        </trans-unit>
        <trans-unit id="3fe735414475494b49457f76f31eea3026d08560" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=False&lt;/code&gt;, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=False&lt;/code&gt; 모델은 전체 입력 데이터에 장착되고, 정지 기준은 트레이닝 데이터에 대해 계산 된 목적 함수에 기초한다.</target>
        </trans-unit>
        <trans-unit id="5515c693f110557bb04dd8dc133e8927dc9c68e0" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=True&lt;/code&gt;, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the &lt;code&gt;score&lt;/code&gt; method) computed on the validation set. The size of the validation set can be changed with the parameter &lt;code&gt;validation_fraction&lt;/code&gt;.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=True&lt;/code&gt; , 입력 데이터는 트레이닝 세트와 검증 집합으로 분할된다. 그런 다음 모델이 학습 세트에 맞춰지고 중지 기준은 검증 세트에서 계산 된 예측 점수 ( &lt;code&gt;score&lt;/code&gt; 방법 사용)를 기반으로합니다 . 검증 세트의 크기는 파라미터로 변경할 수 있습니다 &lt;code&gt;validation_fraction&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4ceb9e226f3e04a8a66252e3801eed93f740afd9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=True&lt;/code&gt;, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score computed on the validation set. The size of the validation set can be changed with the parameter &lt;code&gt;validation_fraction&lt;/code&gt;.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=True&lt;/code&gt; , 입력 데이터는 트레이닝 세트와 검증 집합으로 분할된다. 그런 다음 모델은 학습 세트에 맞춰지고 중지 기준은 유효성 검사 세트에서 계산 된 예측 점수를 기반으로합니다. 검증 세트의 크기는 파라미터로 변경할 수 있습니다 &lt;code&gt;validation_fraction&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="318aced6d4dfc924ad223bd54e79ede301143b06" translate="yes" xml:space="preserve">
          <source>With SGD or Adam, training supports online and mini-batch learning.</source>
          <target state="translated">SGD 또는 Adam을 통해 교육은 온라인 및 미니 배치 학습을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="bebfaf6a5f7ee4311c7425773ef87a0b1b61dcc0" translate="yes" xml:space="preserve">
          <source>With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.</source>
          <target state="translated">SVM 및 로지스틱 회귀 분석을 통해 매개 변수 C는 희소성을 제어합니다. C가 작을수록 선택된 기능이 줄어 듭니다. 알파 매개 변수가 높을수록 기능이 적게 선택된 올가미를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2e07775067fbbb8cee792ed1d4b4b0282fd223be" translate="yes" xml:space="preserve">
          <source>With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\) and \(V\) is calculated by:</source>
          <target state="translated">\ (P '(j) = | V_j | / N \)로. \ (U \)와 \ (V \) 사이의 상호 정보 (MI)는 다음에 의해 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="2042997590ba0f656467c3f6df43427a875d6409" translate="yes" xml:space="preserve">
          <source>With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in scikit-learn are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image.</source>
          <target state="translated">응집 클러스터링을 사용하면 연결 그래프를 제공하여 함께 클러스터링 할 수있는 샘플을 지정할 수 있습니다. scikit-learn의 그래프는 인접 행렬로 표시됩니다. 종종 희소 행렬이 사용됩니다. 예를 들어 이미지를 클러스터링 할 때 연결된 영역 (연결된 구성 요소라고도 함)을 검색하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5313dd287c9c493fc21b86c81282cea7d0608304" translate="yes" xml:space="preserve">
          <source>With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in scikit-learn are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:</source>
          <target state="translated">응집 클러스터링을 사용하면 연결 그래프를 제공하여 함께 클러스터링 할 수있는 샘플을 지정할 수 있습니다. Scikit-learn의 그래프는 인접 행렬로 표시됩니다. 종종 희소 행렬이 사용됩니다. 예를 들어 이미지를 클러스터링 할 때 연결된 영역 (때로는 연결된 구성 요소라고도 함)을 검색하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ebae629f7af13ae26b867ab75161458173d10bdc" translate="yes" xml:space="preserve">
          <source>With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:</source>
          <target state="translated">의사 결정 트리와 관련하여이 전략을 사용하여 다중 출력 문제를 쉽게 지원할 수 있습니다. 다음과 같은 변경이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="d6eab2b8513179355ba20cab88473d0665849027" translate="yes" xml:space="preserve">
          <source>With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.</source>
          <target state="translated">뉴스 그룹을 구별 할 수있는 풍부한 단서가 있기 때문에 분류자는 텍스트에서 주제를 거의 식별하지 않아도되며 모두 동일한 수준에서 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="ba25a12704b8225df22eb5cee35ebe73afb76c8b" translate="yes" xml:space="preserve">
          <source>With sum_over_features equal to False it returns the componentwise distances.</source>
          <target state="translated">sum_over_features가 False 인 경우 구성 요소 별 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="54cc29b6387240d37c6717d5c94b33d650c1152c" translate="yes" xml:space="preserve">
          <source>With the &amp;lsquo;brute&amp;rsquo; method, the parameter &lt;code&gt;X&lt;/code&gt; is used both for generating the grid of values \(x_S\) and the complement feature values \(x_C\). However with the &amp;lsquo;recursion&amp;rsquo; method, &lt;code&gt;X&lt;/code&gt; is only used for the grid values: implicitly, the \(x_C\) values are those of the training data.</source>
          <target state="translated">'brute'방법을 사용하면 매개 변수 &lt;code&gt;X&lt;/code&gt; 가 값 그리드 \ (x_S \) 및 보완 특성 값 \ (x_C \)을 생성하는 데 사용됩니다. 그러나 '재귀'방법을 사용하면 &lt;code&gt;X&lt;/code&gt; 는 그리드 값에만 사용됩니다. 암시 적으로 \ (x_C \) 값은 훈련 데이터의 값입니다.</target>
        </trans-unit>
        <trans-unit id="67616371d2f2a0ec7fd27c8038627dc9ef441900" translate="yes" xml:space="preserve">
          <source>With the fitted model, we compute the predictions of the model on the test dataset. These predictions are used to compute the confustion matrix which is plotted with the &lt;a href=&quot;../../modules/generated/sklearn.metrics.confusionmatrixdisplay#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">적합 모델을 사용하여 테스트 데이터 세트에서 모델의 예측을 계산합니다. 이러한 예측은 &lt;a href=&quot;../../modules/generated/sklearn.metrics.confusionmatrixdisplay#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt; &lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt; &lt;/a&gt; 로 표시되는 혼동 행렬을 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="03d84c3da120d3c6633bcd8017a1f00e1bb6dad8" translate="yes" xml:space="preserve">
          <source>With this class, the base_estimator is fit on the train set of the cross-validation generator and the test set is used for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv=&amp;rdquo;prefit&amp;rdquo; is passed to __init__, it is assumed that base_estimator has been fitted already and all data is used for calibration. Note that data for fitting the classifier and for calibrating it must be disjoint.</source>
          <target state="translated">이 클래스를 사용하면 base_estimator가 교차 검증 생성기의 트레인 세트에 적합하고 테스트 세트가 교정에 사용됩니다. 각각의 접힘에 대한 확률은 예측을 위해 평균화된다. cv =&amp;rdquo;prefit&amp;rdquo;이 __init__에 전달 된 경우 base_estimator가 이미 장착 된 것으로 가정하고 모든 데이터가 교정에 사용됩니다. 분류기를 장착하고 교정하기위한 데이터는 분리되어 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="07ec442186310e3d4d8de1ef730f033183a12d2a" translate="yes" xml:space="preserve">
          <source>With this re-labeling of the data, our problem can be written</source>
          <target state="translated">이 데이터의 레이블을 다시 지정하면 문제를 작성할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bb9dc2936468de0109f9958206c68ba68552df6f" translate="yes" xml:space="preserve">
          <source>With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a &lt;em&gt;KD-tree&lt;/em&gt; in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt;&lt;/a&gt;. Alternatively, the user can work with the &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; class directly.</source>
          <target state="translated">이 설정을 사용하면 테스트 포인트와 중심 사이의 단일 거리 계산으로 노드 내의 모든 포인트까지의 거리에 대한 하한 및 상한을 결정할 수 있습니다. 볼 트리 노드의 구형 형상으로 인해 실제 성능은 훈련 데이터의 구조에 크게 의존하지만 &lt;em&gt;KD 트리&lt;/em&gt; 보다 높은 차원에서 성능을 발휘할 수 있습니다. 공 - 트리 기반의 이웃 검색 키워드 사용하여 지정, scikit - 학습 &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt; , 및 클래스 사용하여 계산된다 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt; 을&lt;/a&gt; . 또는 사용자는 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 클래스를 직접 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c5b891d6db4b2b53f2c329c62187e665d9759f8a" translate="yes" xml:space="preserve">
          <source>Without any prior information on the sample, the number of projections required to reconstruct the image is of the order of the linear size &lt;code&gt;l&lt;/code&gt; of the image (in pixels). For simplicity we consider here a sparse image, where only pixels on the boundary of objects have a non-zero value. Such data could correspond for example to a cellular material. Note however that most images are sparse in a different basis, such as the Haar wavelets. Only &lt;code&gt;l/7&lt;/code&gt; projections are acquired, therefore it is necessary to use prior information available on the sample (its sparsity): this is an example of &lt;strong&gt;compressive sensing&lt;/strong&gt;.</source>
          <target state="translated">샘플에 대한 사전 정보가 없으면 이미지를 재구성하는 데 필요한 투영 수는 이미지의 선형 크기 &lt;code&gt;l&lt;/code&gt; 정도 입니다 (픽셀). 단순화를 위해 여기서는 객체 경계의 픽셀 만 0이 아닌 희소 이미지로 간주합니다. 이러한 데이터는 예를 들어 셀룰러 물질에 해당 할 수있다. 그러나 대부분의 이미지는 Haar 웨이블릿과 같이 다른 기준으로 희소합니다. 단지 &lt;code&gt;l/7&lt;/code&gt; 돌기 따라서 샘플 (그 희소성)에서 사용할 사전 정보를 사용하는 것이 필요하고, 취득한 :이 예이다 &lt;strong&gt;압축 센싱&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="087783a9ac4373b41b03a4f66d5eaf61d7d47ff1" translate="yes" xml:space="preserve">
          <source>Without reduce_func:</source>
          <target state="translated">reduce_func없이 :</target>
        </trans-unit>
        <trans-unit id="e6002e635270be50830b0534ba0aafc304922d8b" translate="yes" xml:space="preserve">
          <source>Without shuffling, &lt;code&gt;X&lt;/code&gt; horizontally stacks features in the following order: the primary &lt;code&gt;n_informative&lt;/code&gt; features, followed by &lt;code&gt;n_redundant&lt;/code&gt; linear combinations of the informative features, followed by &lt;code&gt;n_repeated&lt;/code&gt; duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt;.</source>
          <target state="translated">셔플 링없이 &lt;code&gt;X&lt;/code&gt; 는 기본 &lt;code&gt;n_informative&lt;/code&gt; 기능, 정보 기능의 &lt;code&gt;n_redundant&lt;/code&gt; 선형 조합, &lt;code&gt;n_repeated&lt;/code&gt; 복제본, 정보 및 중복 기능의 대체로 무작위로 추출 된 순서로 기능을 가로 로 쌓습니다 . 나머지 기능은 임의 노이즈로 채워집니다. 따라서 셔플 링없이 모든 유용한 기능이 &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt; 열에 포함됩니다 .</target>
        </trans-unit>
        <trans-unit id="6943d5826611d6c30be0d7d32e8882ddb2f3e560" translate="yes" xml:space="preserve">
          <source>Wolpert, David H. &amp;ldquo;Stacked generalization.&amp;rdquo; Neural networks 5.2 (1992): 241-259.</source>
          <target state="translated">Wolpert, David H. &quot;스택 일반화.&quot; 신경망 5.2 (1992) : 241-259.</target>
        </trans-unit>
        <trans-unit id="d2a146386973596d64e5c0f348ec45ab36bab658" translate="yes" xml:space="preserve">
          <source>Working With Text Data</source>
          <target state="translated">텍스트 데이터 작업</target>
        </trans-unit>
        <trans-unit id="61f49f0587c5992cc8f414bbf22889f09b8f3976" translate="yes" xml:space="preserve">
          <source>Working with text documents</source>
          <target state="translated">텍스트 문서 작업</target>
        </trans-unit>
        <trans-unit id="5b5ef6667bd92ea247084ea267c265251f4aa7de" translate="yes" xml:space="preserve">
          <source>Works with sparse matrices. Only works if &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes exist.</source>
          <target state="translated">희소 행렬과 함께 작동합니다. &lt;code&gt;rows_&lt;/code&gt; 및 &lt;code&gt;columns_&lt;/code&gt; 속성이 존재하는 경우에만 작동 합니다.</target>
        </trans-unit>
        <trans-unit id="926da419b9cc98b9060a6d00fb8d48cd55be86f9" translate="yes" xml:space="preserve">
          <source>Wrapper for kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwise의 커널 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="f986c2ac1f7dce99239d5b1ba2c2c97de265f3fa" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline to classify movie reviews as either positive or negative.</source>
          <target state="translated">영화 리뷰를 긍정적 또는 부정적으로 분류하기 위해 텍스트 분류 파이프 라인을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="c1b32a0493a32a44864910f6b6b9c9398af4b20e" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline using a custom preprocessor and &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; using data from Wikipedia articles as training set.</source>
          <target state="translated">Wikipedia 기사의 데이터를 학습 세트로 사용하여 사용자 지정 전처리 &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; 사용 하여 텍스트 분류 파이프 라인을 작성하십시오 .</target>
        </trans-unit>
        <trans-unit id="c72e193d2469d6cfb2918ba7a00dbc8ed1d451d6" translate="yes" xml:space="preserve">
          <source>Wu, Lin and Weng, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;Probability estimates for multi-class classification by pairwise coupling&amp;rdquo;&lt;/a&gt;, JMLR 5:975-1005, 2004.</source>
          <target state="translated">Wu, Lin 및 Weng, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;페어 와이즈 커플 링에 의한 멀티 클래스 분류의 확률 추정치&amp;rdquo;&lt;/a&gt; , JMLR 5 : 975-1005, 2004.</target>
        </trans-unit>
        <trans-unit id="11ab439af4c255f5f8d3594c0dad27bd31f9055a" translate="yes" xml:space="preserve">
          <source>Wu, Lin and Weng, &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;Probability estimates for multi-class classification by pairwise coupling&amp;rdquo;&lt;/a&gt;, JMLR 5:975-1005, 2004.</source>
          <target state="translated">Wu, Lin 및 Weng, &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&quot;쌍 결합에 의한 다중 클래스 분류에 대한 확률 추정&quot;&lt;/a&gt; , JMLR 5 : 975-1005, 2004.</target>
        </trans-unit>
        <trans-unit id="c8c1574205d07b839af62817660ba2b78f320cd0" translate="yes" xml:space="preserve">
          <source>X block loadings vectors.</source>
          <target state="translated">X 블록 로딩 벡터.</target>
        </trans-unit>
        <trans-unit id="b8076ad410e1a569012d16107ff003e5d358439f" translate="yes" xml:space="preserve">
          <source>X block to latents rotations.</source>
          <target state="translated">잠복 회전에 대한 X 블록.</target>
        </trans-unit>
        <trans-unit id="a6b8640132f42899bc713ee5acc307439a5b7049" translate="yes" xml:space="preserve">
          <source>X block weights vectors.</source>
          <target state="translated">X 블록 가중치 벡터.</target>
        </trans-unit>
        <trans-unit id="51ee8a5ebc2ecb45284445a844d1c86426452ff9" translate="yes" xml:space="preserve">
          <source>X is projected on the first principal components previously extracted from a training set, using minibatches of size batch_size if X is sparse.</source>
          <target state="translated">X가 희소 인 경우 batch_size 크기의 미니 배치를 사용하여 학습 세트에서 이전에 추출 된 첫 번째 주성분에 X가 투영됩니다.</target>
        </trans-unit>
        <trans-unit id="e6bc2e58339df2a473a9897261f25e31780f738c" translate="yes" xml:space="preserve">
          <source>X is projected on the first principal components previously extracted from a training set.</source>
          <target state="translated">X는 이전에 훈련 세트에서 추출한 첫 번째 주요 구성 요소에 투영됩니다.</target>
        </trans-unit>
        <trans-unit id="81850902f59e77236b068c4b29af3fcf5c8ba36f" translate="yes" xml:space="preserve">
          <source>X is stored for future use, as &lt;a href=&quot;#sklearn.isotonic.IsotonicRegression.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt; needs X to interpolate new input data.</source>
          <target state="translated">X는 향후 사용을 위해 저장되는 &lt;a href=&quot;#sklearn.isotonic.IsotonicRegression.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt; 보간 새로운 입력 데이터 X의 요구한다.</target>
        </trans-unit>
        <trans-unit id="7228d382c859d348525ccb5bce51e5752c38bc04" translate="yes" xml:space="preserve">
          <source>X is stored for future use, as &lt;code&gt;transform&lt;/code&gt; needs X to interpolate new input data.</source>
          <target state="translated">X는 향후 사용을 위해 저장되는 &lt;code&gt;transform&lt;/code&gt; 보간 새로운 입력 데이터 X의 요구한다.</target>
        </trans-unit>
        <trans-unit id="3074bef8d8da5f206ce501f5438e3d5abb038064" translate="yes" xml:space="preserve">
          <source>X must have been produced by this DictVectorizer&amp;rsquo;s transform or fit_transform method; it may only have passed through transformers that preserve the number of features and their order.</source>
          <target state="translated">X는이 DictVectorizer의 transform 또는 fit_transform 메소드에 의해 생성되어야합니다. 기능의 수와 순서를 유지하는 변압기 만 통과했을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d0ad8e13f68af13dec8ad59c4f3a6a0df7a4de08" translate="yes" xml:space="preserve">
          <source>X scores.</source>
          <target state="translated">X 점수.</target>
        </trans-unit>
        <trans-unit id="28a3e4c54c0fde2f1aaa67a11fe405d430c2fe41" translate="yes" xml:space="preserve">
          <source>X transformed in the new space.</source>
          <target state="translated">새로운 공간에서 X가 변형되었습니다.</target>
        </trans-unit>
        <trans-unit id="ae3643384dc9ac54889b85ea1da357f34b173e6e" translate="yes" xml:space="preserve">
          <source>X_embedded: ndarray of shape (n_samples, n_components)</source>
          <target state="translated">X_embedded: ndarray of shape (n_samples, n_components)</target>
        </trans-unit>
        <trans-unit id="d925cac037a82cae915e8a0893b59ac5a4590490" translate="yes" xml:space="preserve">
          <source>X_new array, shape (n_samples, n_components)</source>
          <target state="translated">X_new 배열, 모양 (n_samples, n_components)</target>
        </trans-unit>
        <trans-unit id="b0e804c93132e5ef73393b841ae090d872aa2191" translate="yes" xml:space="preserve">
          <source>X_original array-like, shape (n_samples, n_features)</source>
          <target state="translated">X_original array-like, shape (n_samples, n_features)</target>
        </trans-unit>
        <trans-unit id="feedfda54d7431e28acb98075b2c2bd9cf8331f2" translate="yes" xml:space="preserve">
          <source>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</source>
          <target state="translated">Xiaojin Zhu와 Zoubin Ghahramani. 레이블 전파를 통해 레이블이있는 데이터와 레이블이없는 데이터로부터 학습 기술 보고서 ​​CMU-CALD-02-107, 2002 년 카네기 멜론 대학교 &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5c986ee528dd9bdf7a6c8d0101f77976c47ae9d2" translate="yes" xml:space="preserve">
          <source>Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;Theil-Sen Estimators in a Multiple Linear Regression Model.&lt;/a&gt;</source>
          <target state="translated">Xin Dang, Hanxiang Peng, Xueqin Wang 및 Heping Zhang : &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;다중 선형 회귀 모델의 테일 센 추정기&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3674e6b71697b8f4a5e14e694db26daa53371e84" translate="yes" xml:space="preserve">
          <source>Xt[i, j] is assigned the weight of edge that connects i to j. Only the neighbors have an explicit value. The diagonal is always explicit. The matrix is of CSR format.</source>
          <target state="translated">Xt [i, j]에는 i를 j에 연결하는 모서리의 가중치가 할당됩니다. 이웃 만 명시적인 값을 갖습니다. 대각선은 항상 명시 적입니다. 매트릭스는 CSR 형식입니다.</target>
        </trans-unit>
        <trans-unit id="c93217dd923de34853280b8058e56203ef9ee737" translate="yes" xml:space="preserve">
          <source>Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.</source>
          <target state="translated">Xy = 사전 계산할 수있는 np.dot (XT, y) 그램 행렬이 사전 계산 된 경우에만 유용합니다.</target>
        </trans-unit>
        <trans-unit id="700502aa71883c1784c7b4df71f7526cabc3833b" translate="yes" xml:space="preserve">
          <source>Xy = np.dot(X.T, y).</source>
          <target state="translated">Xy = np.dot(X.T, y).</target>
        </trans-unit>
        <trans-unit id="23eb4d3f4155395a74e9d534f97ff4c1908f5aac" translate="yes" xml:space="preserve">
          <source>Y</source>
          <target state="translated">Y</target>
        </trans-unit>
        <trans-unit id="aac13ced89d2b311880e53ba16f36f4513402a98" translate="yes" xml:space="preserve">
          <source>Y block loadings vectors.</source>
          <target state="translated">Y 블록 로딩 벡터.</target>
        </trans-unit>
        <trans-unit id="148708c0aec99251158277d2fc4d038d62f32551" translate="yes" xml:space="preserve">
          <source>Y block to latents rotations.</source>
          <target state="translated">잠복 회전에 Y 블록.</target>
        </trans-unit>
        <trans-unit id="8f4ded8aca1a84f4452774f8bc622751045ade48" translate="yes" xml:space="preserve">
          <source>Y block weights vectors.</source>
          <target state="translated">Y 블록 가중치 벡터.</target>
        </trans-unit>
        <trans-unit id="780dd8f1641062cfc0af001d2fcfedba3262be26" translate="yes" xml:space="preserve">
          <source>Y scores.</source>
          <target state="translated">Y 점수.</target>
        </trans-unit>
        <trans-unit id="93b5936ef31b077aecad8b961838c412166e9fd0" translate="yes" xml:space="preserve">
          <source>Y. Freund, R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting&amp;rdquo;, 1995.</source>
          <target state="translated">Y. Freund, R. Schapire,&amp;ldquo;온라인 학습의 의사 결정 이론 일반화 및 부스팅 적용&amp;rdquo;, 1995.</target>
        </trans-unit>
        <trans-unit id="bf931371fe813e68af145bc28f1f0c59ead42876" translate="yes" xml:space="preserve">
          <source>Y. Freund, and R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting&amp;rdquo;, 1997.</source>
          <target state="translated">Y. Freund, R. Schapire,&amp;ldquo;온라인 학습의 의사 결정 이론적 일반화 및 응용 프로그램 향상&amp;rdquo;, 1997.</target>
        </trans-unit>
        <trans-unit id="982b5c305af507a5853864a0280fe0330e5fb9d9" translate="yes" xml:space="preserve">
          <source>Y[argmin[i], :] is the row in Y that is closest to X[i, :].</source>
          <target state="translated">Y [argmin [i], :]는 X [i, :]에 가장 가까운 Y의 행입니다.</target>
        </trans-unit>
        <trans-unit id="6cc2acee87fd2b4d368d7294a14e1666de3c66f0" translate="yes" xml:space="preserve">
          <source>Yang, Algesheimer, and Tessone, (2016). &amp;ldquo;A comparative analysis of community detection algorithms on artificial networks&amp;rdquo;. Scientific Reports 6: 30750. &lt;a href=&quot;https://www.nature.com/articles/srep30750&quot;&gt;doi:10.1038/srep30750&lt;/a&gt;.</source>
          <target state="translated">Yang, Algesheimer 및 Tessone, (2016). &quot;인공 네트워크에서 커뮤니티 탐지 알고리즘의 비교 분석&quot;. 과학 보고서 6 : 30750. &lt;a href=&quot;https://www.nature.com/articles/srep30750&quot;&gt;doi : 10.1038 / srep30750&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="3526f607bcd4f51ad0bc05f814579a42c2c0ba57" translate="yes" xml:space="preserve">
          <source>Yellow</source>
          <target state="translated">Yellow</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="44e848b37858df8125129ee3d3911783b05fb21f" translate="yes" xml:space="preserve">
          <source>Yields indices to split data into training and test sets.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c970e3f1e790a2a4cd28b40401902501b9bc2d74" translate="yes" xml:space="preserve">
          <source>Yields:</source>
          <target state="translated">Yields:</target>
        </trans-unit>
        <trans-unit id="e285a8203a02b899c727c909bb971f8a5290d1a9" translate="yes" xml:space="preserve">
          <source>You can &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; over parameters of all estimators in the pipeline at once.</source>
          <target state="translated">파이프 라인에서 모든 추정기의 매개 변수를 한 번에 &lt;a href=&quot;grid_search#grid-search&quot;&gt;그리드 검색&lt;/a&gt; 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="afc09f41b74c7e769c70223fd4fc6ea043be4f9a" translate="yes" xml:space="preserve">
          <source>You can access the newly created figure and Axes objects using &lt;code&gt;plt.gcf()&lt;/code&gt; and &lt;code&gt;plt.gca()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;plt.gcf()&lt;/code&gt; 및 &lt;code&gt;plt.gca()&lt;/code&gt; 사용하여 새로 생성 된 Figure 및 Axes 객체에 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e4f0eb08d1e594cb4ba39dda583b2124c29e8d3e" translate="yes" xml:space="preserve">
          <source>You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them.</source>
          <target state="translated">데이터 세트 로더에 이름을 제공하거나 카테고리를 없음으로 설정하여 카테고리 수를 조정하여 카테고리 수를 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d6a91645b832623d5d5110588ef04aebdc451880" translate="yes" xml:space="preserve">
          <source>You can already copy the skeletons into a new folder somewhere on your hard-drive named &lt;code&gt;sklearn_tut_workspace&lt;/code&gt; where you will edit your own files for the exercises while keeping the original skeletons intact:</source>
          <target state="translated">&lt;code&gt;sklearn_tut_workspace&lt;/code&gt; 라는 하드 드라이브의 어딘가에 골격을 이미 복사 할 수 있습니다. 여기서 원래 골격을 그대로 유지하면서 연습을 위해 자신의 파일을 편집 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1cf0d94595a131d36f8236532aeafb049eaa6dbc" translate="yes" xml:space="preserve">
          <source>You can also specify both the name and the version, which also uniquely identifies the dataset:</source>
          <target state="translated">이름과 버전을 모두 지정하여 데이터 세트를 고유하게 식별 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="ae5f0da778f6ff8c0d5d1c2ccd6f86bebea3d9e0" translate="yes" xml:space="preserve">
          <source>You can also use your own defined kernels by passing a function to the keyword &lt;code&gt;kernel&lt;/code&gt; in the constructor.</source>
          <target state="translated">생성자 의 키워드 &lt;code&gt;kernel&lt;/code&gt; 에 함수를 전달하여 자체 정의 된 커널을 사용할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c90fd70bc9584ad72350fadce865213cd8c472be" translate="yes" xml:space="preserve">
          <source>You can combine &lt;code&gt;KBinsDiscretizer&lt;/code&gt; with &lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; if you only want to preprocess part of the features.</source>
          <target state="translated">당신은 결합 할 수 있습니다 &lt;code&gt;KBinsDiscretizer&lt;/code&gt; 을 함께 &lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt; 당신은 단지 기능의 전처리 부분에 원하는 경우.</target>
        </trans-unit>
        <trans-unit id="b37a62ca838923df55452889a68156ed8900e3e5" translate="yes" xml:space="preserve">
          <source>You can control the exact number of threads that are used via the &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; environment variable:</source>
          <target state="translated">&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; 환경 변수 를 통해 사용되는 정확한 스레드 수를 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="579522a3d8d5bf3b4958e7c2d681266388521dce" translate="yes" xml:space="preserve">
          <source>You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.</source>
          <target state="translated">커널을 파이썬 함수로 제공하거나 Gram 매트릭스를 사전 계산하여 자신의 커널을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5a14b3f83e3bd81fa3adef5b677dc7c591d450db" translate="yes" xml:space="preserve">
          <source>You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:</source>
          <target state="translated">다음 명령을 사용하여 NumPy / SciPy / scikit-learn 설치에 사용 된 BLAS / LAPACK 구현을 표시 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="929abd63168ac2d721d4708b8ef8be3cd51b08a0" translate="yes" xml:space="preserve">
          <source>You can ensure that &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt; are the inverse of each other by setting &lt;code&gt;check_inverse=True&lt;/code&gt; and calling &lt;code&gt;fit&lt;/code&gt; before &lt;code&gt;transform&lt;/code&gt;. Please note that a warning is raised and can be turned into an error with a &lt;code&gt;filterwarnings&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;inverse_func&lt;/code&gt; &lt;code&gt;check_inverse=True&lt;/code&gt; 를 설정 하고 &lt;code&gt;transform&lt;/code&gt; before &lt;code&gt;fit&lt;/code&gt; 을 호출 하여 &lt;code&gt;func&lt;/code&gt; 과 inverse_func 가 서로 역함을 보장 할 수 있습니다 . 경고가 발생하고 &lt;code&gt;filterwarnings&lt;/code&gt; 경고로 인해 오류가 발생할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8fe8cd91261eb27750ae7b2f82fa15c24077f346" translate="yes" xml:space="preserve">
          <source>You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt; factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; 팩토리 를 사용하지 않고 자체 스코어링 오브젝트를 처음부터 구성하여보다 유연한 모델 스코어러를 생성 할 수 있습니다 . 콜 러블이 스코어러가 되려면 다음 두 규칙에 지정된 프로토콜을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="cb07d258c61c328d902779de990b642f82ba2beb" translate="yes" xml:space="preserve">
          <source>You can get more information on the dataset by looking at the &lt;code&gt;DESCR&lt;/code&gt; and &lt;code&gt;details&lt;/code&gt; attributes:</source>
          <target state="translated">&lt;code&gt;DESCR&lt;/code&gt; 및 &lt;code&gt;details&lt;/code&gt; 속성 을보고 데이터 세트에 대한 자세한 정보를 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="316dc294ff0e2890db335b30189c691f1a723809" translate="yes" xml:space="preserve">
          <source>You can now see many things that these features have overfit to:</source>
          <target state="translated">이제이 기능들이 과적 합한 많은 것들을 볼 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="e2cf6b0ce385c4e9fb3f4b5189c83feb905d92cf" translate="yes" xml:space="preserve">
          <source>You can pass pre-computed kernels by using the &lt;code&gt;kernel='precomputed'&lt;/code&gt; option. You should then pass Gram matrix instead of X to the &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; methods. The kernel values between &lt;em&gt;all&lt;/em&gt; training vectors and the test vectors must be provided:</source>
          <target state="translated">&lt;code&gt;kernel='precomputed'&lt;/code&gt; 옵션 을 사용하여 미리 계산 된 커널을 전달할 수 있습니다 . 그런 다음 X 대신 그람 행렬을 &lt;code&gt;fit&lt;/code&gt; 및 &lt;code&gt;predict&lt;/code&gt; 방법에 전달해야 합니다. &lt;em&gt;모든&lt;/em&gt; 훈련 벡터와 테스트 벡터 사이의 커널 값을 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="ee86b8b814976ee239ecfc8e86907133be9d3afc" translate="yes" xml:space="preserve">
          <source>You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the &lt;code&gt;n_features&lt;/code&gt; parameter.</source>
          <target state="translated">벡터 출력에서 ​​0이 아닌 기능 토큰 16 개가 추출 된 것을 볼 수 있습니다. 이는 동일한 장난감 코퍼스 에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 에 의해 이전에 추출 된 0이 아닌 0보다 작습니다 . 불일치는 &lt;code&gt;n_features&lt;/code&gt; 매개 변수 의 낮은 값으로 인해 해시 함수 충돌에서 비롯 됩니다.</target>
        </trans-unit>
        <trans-unit id="6b3885cd10ec182cb98af7964ed686cdd6c77762" translate="yes" xml:space="preserve">
          <source>You can specify a monotonic constraint on each feature using the &lt;code&gt;monotonic_cst&lt;/code&gt; parameter. For each feature, a value of 0 indicates no constraint, while -1 and 1 indicate a negative and positive constraint, respectively:</source>
          <target state="translated">&lt;code&gt;monotonic_cst&lt;/code&gt; 매개 변수를 사용하여 각 피쳐에 대해 단조로운 제약 조건을 지정할 수 있습니다 . 각 기능에 대해 값 0은 제약 조건이 없음을 나타내고 -1 및 1은 각각 음수 및 양수 제약을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="c31033fd31d22147ea7534b97a7d63f646fe3e48" translate="yes" xml:space="preserve">
          <source>You can then edit the content of the workspace without fear of losing the original exercise instructions.</source>
          <target state="translated">그런 다음 원래 연습 지침을 잃을 염려없이 작업 공간의 내용을 편집 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bc5b6b055370779ded34375c059372bbad8d8da3" translate="yes" xml:space="preserve">
          <source>You can use your own defined kernels by passing a function to the &lt;code&gt;kernel&lt;/code&gt; parameter.</source>
          <target state="translated">&lt;code&gt;kernel&lt;/code&gt; 매개 변수에 함수를 전달하여 사용자가 정의한 커널을 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e21dcf7dc4d8353d8949b3e6ddc2c35c364a9b4a" translate="yes" xml:space="preserve">
          <source>You cannot nest objects with parallel computing (&lt;code&gt;n_jobs&lt;/code&gt; different than 1).</source>
          <target state="translated">병렬 컴퓨팅 ( 1과 다른 &lt;code&gt;n_jobs&lt;/code&gt; )으로 오브젝트를 중첩 할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="d086a1b811e8ad563a3cd7d98758c535aff811c7" translate="yes" xml:space="preserve">
          <source>You could try UTF-8 and disregard the errors. You can decode byte strings with &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; to replace all decoding errors with a meaningless character, or set &lt;code&gt;decode_error='replace'&lt;/code&gt; in the vectorizer. This may damage the usefulness of your features.</source>
          <target state="translated">UTF-8을 시도하고 오류를 무시할 수 있습니다. 바이트 문자열을 &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; 로 디코딩하여 모든 디코딩 오류를 의미없는 문자로 &lt;code&gt;decode_error='replace'&lt;/code&gt; 거나 벡터 화기에서 decode_error = 'replace' 를 설정할 수 있습니다. 기능의 유용성을 손상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c0d5a5afa92ed6aa301d13299623473f530c94ba" translate="yes" xml:space="preserve">
          <source>You may also load two (or more) datasets at once:</source>
          <target state="translated">한 번에 두 개 이상의 데이터 세트를로드 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="a822ec525f0ce269b9b885feec474e1f8b512e04" translate="yes" xml:space="preserve">
          <source>You may also retain the estimator fitted on each training set by setting &lt;code&gt;return_estimator=True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;return_estimator=True&lt;/code&gt; 를 설정하여 각 트레이닝 세트에 맞는 추정량을 유지할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c9bcd37e9efb4d07a927274f7ad017afc3f094c8" translate="yes" xml:space="preserve">
          <source>You may be able to find out what kind of encoding it is in general using the UNIX command &lt;code&gt;file&lt;/code&gt;. The Python &lt;code&gt;chardet&lt;/code&gt; module comes with a script called &lt;code&gt;chardetect.py&lt;/code&gt; that will guess the specific encoding, though you cannot rely on its guess being correct.</source>
          <target state="translated">UNIX 명령 &lt;code&gt;file&lt;/code&gt; 사용하여 일반적으로 어떤 종류의 인코딩인지 확인할 수 있습니다 . 파이썬 &lt;code&gt;chardet&lt;/code&gt; 의 모듈라는 스크립트와 함께 제공 &lt;code&gt;chardetect.py&lt;/code&gt; 당신의 추측이 정확 인에 의존하지 수 있지만, 특정 인코딩을 추측됩니다.</target>
        </trans-unit>
        <trans-unit id="04405b99190799597dab92e2ef615219d1447404" translate="yes" xml:space="preserve">
          <source>You may load a dataset like as follows:</source>
          <target state="translated">다음과 같이 데이터 세트를로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f9f71500b978e09c529098f893f05269c79caaff" translate="yes" xml:space="preserve">
          <source>You may want to include the parameters of the preprocessors in a &lt;a href=&quot;grid_search#grid-search&quot;&gt;parameter search&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;grid_search#grid-search&quot;&gt;매개 변수 검색에&lt;/a&gt; 전 처리기의 매개 변수를 포함시킬 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4cbe0908270a3a4effe7f03ed10c6fc1b573bdb1" translate="yes" xml:space="preserve">
          <source>You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR which penalizes the intercept.</source>
          <target state="translated">솔버 liblinear를 사용하면 인터리브를 사용하는 경우 인터셉트에 페널티를주는 LIBLINEAR을 사용하므로 결과가 약간 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eacc5e93bbce61c3d762f60af9c0b85d6ab90006" translate="yes" xml:space="preserve">
          <source>You might have noticed that the samples were shuffled randomly when we called &lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt;: this is useful if you wish to select only a subset of samples to quickly train a model and get a first idea of the results before re-training on the complete dataset later.</source>
          <target state="translated">&lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt; 호출 할 때 샘플이 무작위로 섞여 있음을 알았을 것입니다 . 이는 모델을 빠르게 훈련시키고 첫 번째를 얻기 위해 샘플의 하위 집합 만 선택하려는 경우에 유용합니다 나중에 전체 데이터 세트를 다시 학습하기 전에 결과에 대한 아이디어.</target>
        </trans-unit>
        <trans-unit id="c8c03561e45bae9c49c8b8f096417e119a986f38" translate="yes" xml:space="preserve">
          <source>You only have to call &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt; once on your data to fit a whole sequence of estimators.</source>
          <target state="translated">당신은 전화로이 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합&lt;/a&gt; 하고 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;예측&lt;/a&gt; 추정량의 전체 순서에 맞게 데이터를 한 번.</target>
        </trans-unit>
        <trans-unit id="1c0c1bb33d891f47deca1c516d19aa08bd8443b9" translate="yes" xml:space="preserve">
          <source>You only have to call &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; once on your data to fit a whole sequence of estimators.</source>
          <target state="translated">당신은 전화로이 &lt;code&gt;fit&lt;/code&gt; 하고 &lt;code&gt;predict&lt;/code&gt; 추정량의 전체 순서에 맞게 데이터를 한 번.</target>
        </trans-unit>
        <trans-unit id="8f00f0e599f4c3a7b71dcab47e41c43fc685f526" translate="yes" xml:space="preserve">
          <source>You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is split into &lt;em&gt;we&lt;/em&gt; and &lt;em&gt;ve&lt;/em&gt; by CountVectorizer&amp;rsquo;s default tokenizer, so if &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is in &lt;code&gt;stop_words&lt;/code&gt;, but &lt;em&gt;ve&lt;/em&gt; is not, &lt;em&gt;ve&lt;/em&gt; will be retained from &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.</source>
          <target state="translated">또한 중지 단어 목록에 벡터 라이저에서 사용 된 것과 동일한 사전 처리 및 토큰 화가 적용되어 있는지 확인해야합니다. &lt;em&gt;우리가 한&lt;/em&gt; 단어 &lt;em&gt;는 &lt;/em&gt;&lt;em&gt;우리&lt;/em&gt; 와 &lt;em&gt;ve&lt;/em&gt; 로 나뉩니다&lt;em&gt;&lt;/em&gt; 그렇다면, CountVectorizer의 기본 토크 나이로 &lt;em&gt;우리가했습니다&lt;/em&gt; 에 &lt;code&gt;stop_words&lt;/code&gt; 하지만 &lt;em&gt;적이&lt;/em&gt; 아니라, &lt;em&gt;적이&lt;/em&gt; 에서 유지됩니다 &lt;em&gt;우리가했습니다&lt;/em&gt; 변형 텍스트. 벡터 라이저는 일부 불일치에 대해 식별하고 경고합니다.</target>
        </trans-unit>
        <trans-unit id="c05eee82bc79ecab0104c0f165ab54dde97d70eb" translate="yes" xml:space="preserve">
          <source>You will find additional details about joblib mitigation of oversubscription in &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-ressources&quot;&gt;joblib documentation&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-ressources&quot;&gt;joblib 문서&lt;/a&gt; 에서 초과 구독의 joblib 완화에 대한 추가 세부 정보를 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="03e4dbf3891b38cb2bcd04772f91e994b5e9c01b" translate="yes" xml:space="preserve">
          <source>Your dataset consists of heterogeneous data types (e.g. raster images and text captions)</source>
          <target state="translated">데이터 세트는 이기종 데이터 유형 (예 : 래스터 이미지 및 텍스트 캡션)으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="7b0a68e70dc900bed821b4c342a07edfa667e451" translate="yes" xml:space="preserve">
          <source>Your dataset is stored in a Pandas DataFrame and different columns require different processing pipelines.</source>
          <target state="translated">데이터 세트는 Pandas DataFrame에 저장되며 열마다 다른 처리 파이프 라인이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="cf246e4fd612425ede440737acf49a3220f42916" translate="yes" xml:space="preserve">
          <source>Your kernel must take as arguments two matrices of shape &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt;, &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; and return a kernel matrix of shape &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt;.</source>
          <target state="translated">커널 인수 모양의 두 개의 행렬로 수행해야합니다 &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt; , &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; 모양의 커널 매트릭스 반환 &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a97cec9f16597107c699027a6e02cf1c0426b74a" translate="yes" xml:space="preserve">
          <source>ZN proportion of residential land zoned for lots over 25,000 sq.ft.</source>
          <target state="translated">주거 용지의 ZN 비율은 25,000 평방 피트가 넘는 부지에 구역화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="712d097b167e76a6e9d59b3e5e274cb4dc4edfe4" translate="yes" xml:space="preserve">
          <source>Zadrozny and Elkan, &amp;ldquo;Transforming classifier scores into multiclass probability estimates&amp;rdquo;, SIGKDD&amp;lsquo;02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</source>
          <target state="translated">Zadrozny와 Elkan,&amp;ldquo;분류 자 점수를 멀티 클래스 확률 추정값으로 변환&amp;rdquo;, SIGKDD'02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2a3a7f6ae69d75cdee1953a2daa7e044766fcafc" translate="yes" xml:space="preserve">
          <source>Zadrozny and Elkan, &amp;ldquo;Transforming classifier scores into multiclass probability estimates&amp;rdquo;, SIGKDD&amp;rsquo;02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</source>
          <target state="translated">Zadrozny 및 Elkan, &quot;분류 자 점수를 다중 클래스 확률 추정치로 변환&quot;, SIGKDD'02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f05a65af97509516c00dcac126500e3f1415b5be" translate="yes" xml:space="preserve">
          <source>Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">다항식과 S 자형 커널에 대한 영 계수. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="e35caa5ca631cf4323249c1e10ca37b600a29376" translate="yes" xml:space="preserve">
          <source>Zero is the lowest possible score. Values closer to zero indicate a better partition.</source>
          <target state="translated">최저 점수는 0입니다. 0에 가까울수록 더 나은 파티션을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4196df3003bc4705f3359c145eca39ac9042a13b" translate="yes" xml:space="preserve">
          <source>Zero-one classification loss.</source>
          <target state="translated">제로원 분류 손실.</target>
        </trans-unit>
        <trans-unit id="ee137211a128584365e4b492f8f1e31e317a831d" translate="yes" xml:space="preserve">
          <source>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&lt;/a&gt;</source>
          <target state="translated">Zhang, J. 및 Marszalek, M. 및 Lazebnik, S. 및 Schmid, C. 텍스처 및 객체 범주의 분류를위한 로컬 기능 및 커널 : 종합 연구 International Computer of 2007 Vision &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/ ko-kr / um / people / manik / projects / trade-off / papers / ZhangIJCV06.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="78f8c38e7e05f7b4f27cc97ecbaeea34135ce110" translate="yes" xml:space="preserve">
          <source>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 &lt;a href=&quot;https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&lt;/a&gt;</source>
          <target state="translated">Zhang, J. 및 Marszalek, M. 및 Lazebnik, S. 및 Schmid, C. 텍스처 및 개체 범주 분류를위한 로컬 기능 및 커널 : 포괄적 인 연구 International Journal of Computer Vision 2007 &lt;a href=&quot;https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;https://research.microsoft.com/ ko-us / um / people / manik / projects / trade-off / papers / ZhangIJCV06.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0c6fefb0786b59f4ca28434a9adf73a14f912b16" translate="yes" xml:space="preserve">
          <source>Zhang, Z. &amp;amp; Wang, J. MLLE: Modified Locally Linear Embedding Using Multiple Weights. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382&lt;/a&gt;</source>
          <target state="translated">Zhang, Z. &amp;amp; Wang, J. MLLE : 다중 가중치를 사용하여 수정 된 로컬 선형 임베딩. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e40887e48b748aeda9c07d81d6206e919ed1f726" translate="yes" xml:space="preserve">
          <source>Zhang, Z. &amp;amp; Zha, H. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. Journal of Shanghai Univ. 8:406 (2004)</source>
          <target state="translated">Zhang, Z. &amp;amp; Zha, H. 접선 공간 정렬을 통한 주요 매니 폴드 및 비선형 차원 감소. 상하이 대학교 저널 8 : 406 (2004 년)</target>
        </trans-unit>
        <trans-unit id="2f2ef1b5180fd57b17245a5c505519733d35270d" translate="yes" xml:space="preserve">
          <source>Zhu, H. Zou, S. Rosset, T. Hastie, &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">Zhu, H. Zou, S. Rosset, T. Hastie,&amp;ldquo;멀티 클래스 AdaBoost&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="8ce45cc584babf565a133f667c041638840fdfd3" translate="yes" xml:space="preserve">
          <source>Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust estimation in signal processing: A tutorial-style treatment of fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.</source>
          <target state="translated">Zoubir A., ​​Koivunen V., Chakhchoukh Y. 및 Muma M. (2012). 신호 처리에서의 강력한 평가 : 기본 개념에 대한 튜토리얼 스타일 처리. IEEE 신호 처리 매거진 29 (4), 61-80.</target>
        </trans-unit>
        <trans-unit id="e2b89a96fcf50192e8235c2260c291f63c7f4fa9" translate="yes" xml:space="preserve">
          <source>[&amp;lsquo;additive_chi2&amp;rsquo;, &amp;lsquo;chi2&amp;rsquo;, &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;polynomial&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;laplacian&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;]</source>
          <target state="translated">[ 'additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf', 'laplacian', 'sigmoid', 'cosine']</target>
        </trans-unit>
        <trans-unit id="cd3417b4282b09dc45879fe7c77bee8859983780" translate="yes" xml:space="preserve">
          <source>[&amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;polynomial&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;]</source>
          <target state="translated">[ 'rbf', 'sigmoid', 'polynomial', 'poly', 'linear', 'cosine']</target>
        </trans-unit>
        <trans-unit id="7c0453b88eaf6a5b1a0ac2faa1dec6c20e0dda6a" translate="yes" xml:space="preserve">
          <source>[1, x_2, x_2 ** 2, x_2 ** 3, &amp;hellip;], &amp;hellip;]</source>
          <target state="translated">[1, x_2, x_2 ** 2, x_2 ** 3,&amp;hellip;],&amp;hellip;]</target>
        </trans-unit>
        <trans-unit id="af237073ca841ce40d3b1c3f9ec3b84ba9e8c1ce" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Online Learning for Latent Dirichlet Allocation&amp;rdquo;, Matthew D. Hoffman,</source>
          <target state="translated">[1] &quot;잠재적 인 Dirichlet 할당을위한 온라인 학습&quot;, Matthew D. Hoffman,</target>
        </trans-unit>
        <trans-unit id="a5828c16246e11e0eda2596d27fdd402ee57d009" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Shrinkage Algorithms for MMSE Covariance Estimation&amp;rdquo; Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.</source>
          <target state="translated">[1] &quot;MMSE 공분산 추정을위한 수축 알고리즘&quot;Chen et al., IEEE Trans. 서명. Proc., Volume 58, Issue 10, 2010 년 10 월.</target>
        </trans-unit>
        <trans-unit id="c5ae55965c66d78c700f954c5d28c9832964e702" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning&amp;rdquo; by A. Rahimi and Benjamin Recht. (&lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt;)</source>
          <target state="translated">[1] A. Rahimi와 Benjamin Recht의&amp;ldquo;무작위 주방 싱크의 가중치 합계 : 학습에서 최소화를 무작위로 대체&amp;rdquo;. ( &lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="4bdc516c4c0b901726d527e6df8200cdc4a8acc8" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning&amp;rdquo; by A. Rahimi and Benjamin Recht. (&lt;a href=&quot;https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt;)</source>
          <target state="translated">[1] A. Rahimi와 Benjamin Recht의&amp;ldquo;무작위 주방 싱크의 가중치 합계 : 학습에서 최소화를 무작위 화로 대체&amp;rdquo;. ( &lt;a href=&quot;https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="066cf0134c1716b5ce53bcaba6c6b8d7e86263f5" translate="yes" xml:space="preserve">
          <source>[1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and Selection. The Elements of Statistical Learning (pp. 219-260). New York, NY, USA: Springer New York Inc..</source>
          <target state="translated">Hastie, T., Tibshirani, R., Friedman, J. (2001). 모델 평가 및 선택. 통계적 학습의 요소 (pp. 219-260). 미국 뉴욕 주 뉴욕 : Springer New York Inc.</target>
        </trans-unit>
        <trans-unit id="851ede0920efe80a8308115ddfdc22058d99b224" translate="yes" xml:space="preserve">
          <source>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</source>
          <target state="translated">[1] Hinton, GE, Osindero, S. 및 Teh, Y. 빠른 학습 알고리즘</target>
        </trans-unit>
        <trans-unit id="3208128766e3298f0714b3eacb4e2ecfc88475e9" translate="yes" xml:space="preserve">
          <source>[1] L. Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32,</source>
          <target state="translated">[1] L. Breiman,&amp;ldquo;Random Forests&amp;rdquo;, 기계 학습, 45 (1), 5-32,</target>
        </trans-unit>
        <trans-unit id="c4ab6918e1971671fbb440a7f8e61bfcc4315791" translate="yes" xml:space="preserve">
          <source>[1] P. J. Rousseeuw. Least median of squares regression. J. Am</source>
          <target state="translated">[1] PJ Rousseeuw. 최소 제곱 회귀 분석 암</target>
        </trans-unit>
        <trans-unit id="4becf43125cdaf0ec29f63ac1f954b679ab8e6bc" translate="yes" xml:space="preserve">
          <source>[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216</source>
          <target state="translated">[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. 반 감독 학습 (2006), pp. 193-216</target>
        </trans-unit>
        <trans-unit id="9a201577697a06c9ac689a946ae70d44d48c0e7c" translate="yes" xml:space="preserve">
          <source>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data</source>
          <target state="translated">[1] van der Maaten, LJP; Hinton, GE, 고차원 데이터 시각화</target>
        </trans-unit>
        <trans-unit id="8eeff125eef3cfca1ff3f8b3157054b95e0b3509" translate="yes" xml:space="preserve">
          <source>[2] &amp;ldquo;Stochastic Variational Inference&amp;rdquo;, Matthew D. Hoffman, David M. Blei,</source>
          <target state="translated">[2] &quot;확률 론적 변이 추론&quot;, Matthew D. Hoffman, David M. Blei,</target>
        </trans-unit>
        <trans-unit id="ea3c887d7b7624a41f686043b166f388d3617ff3" translate="yes" xml:space="preserve">
          <source>[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</source>
          <target state="translated">[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Semi-Supervised Learning에서 효율적인 비모수 적 기능 유도. AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="73434047889c8334ffffe648547654fa2862e107" translate="yes" xml:space="preserve">
          <source>[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 &lt;a href=&quot;https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</source>
          <target state="translated">[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. 준지도 학습에서 효율적인 비모수 적 함수 유도. AISTAT 2005 &lt;a href=&quot;https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;https://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="390f7912993134abee39b500fe8ff987c558bcc7" translate="yes" xml:space="preserve">
          <source>[2] Tieleman, T. Training Restricted Boltzmann Machines using</source>
          <target state="translated">[2] Tieleman, T. 교육 제한 볼츠만 머신</target>
        </trans-unit>
        <trans-unit id="936e8131576c3002ff436671f77d81f6c95e71d7" translate="yes" xml:space="preserve">
          <source>[2] Wilson, E. B., &amp;amp; Hilferty, M. M. (1931). The distribution of chi-square.</source>
          <target state="translated">[2] Wilson, EB, &amp;amp; Hilferty, MM (1931). 카이-제곱의 분포.</target>
        </trans-unit>
        <trans-unit id="5cccbf6c7fe7c1f50410b68e37c12f00b67f9330" translate="yes" xml:space="preserve">
          <source>[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding</source>
          <target state="translated">[2] van der Maaten, LJP t- 분산 스토캐스틱 이웃 임베딩</target>
        </trans-unit>
        <trans-unit id="740947d1c8302c56dc8a9209234aab173f75acad" translate="yes" xml:space="preserve">
          <source>[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.</source>
          <target state="translated">LJP van der Maaten. 트리 기반 알고리즘을 사용하여 t-SNE 가속화</target>
        </trans-unit>
        <trans-unit id="a16dde9090b3c419b1ba6d8027b90785ddf73263" translate="yes" xml:space="preserve">
          <source>[3] Matthew D. Hoffman&amp;rsquo;s onlineldavb code. Link:</source>
          <target state="translated">[3] Matthew D. Hoffman의 onlineldavb 코드. 링크:</target>
        </trans-unit>
        <trans-unit id="2091fb37b7afd77ae2e8e60855d4cad2538ba378" translate="yes" xml:space="preserve">
          <source>[B1996]</source>
          <target state="translated">[B1996]</target>
        </trans-unit>
        <trans-unit id="66fa89cedf249bba6f8bbd7ca59a6edba1eb2520" translate="yes" xml:space="preserve">
          <source>[B1998]</source>
          <target state="translated">[B1998]</target>
        </trans-unit>
        <trans-unit id="7665023d9511c3ea5a7a3e7baca06057eba900d6" translate="yes" xml:space="preserve">
          <source>[B1999]</source>
          <target state="translated">[B1999]</target>
        </trans-unit>
        <trans-unit id="5c075f95c1e65a7e49dec5ad30ee36d1fb13b2b6" translate="yes" xml:space="preserve">
          <source>[B2001]</source>
          <target state="translated">[B2001]</target>
        </trans-unit>
        <trans-unit id="04bec92cc809290da2bf608da574e1877293633f" translate="yes" xml:space="preserve">
          <source>[B2011]</source>
          <target state="translated">[B2011]</target>
        </trans-unit>
        <trans-unit id="2b6c9f7f2623b948c281da789073abc37bb7f8fa" translate="yes" xml:space="preserve">
          <source>[ButlerDavies]</source>
          <target state="translated">[ButlerDavies]</target>
        </trans-unit>
        <trans-unit id="1d442f2d1661e89f1bc869a582a8d649d24881e4" translate="yes" xml:space="preserve">
          <source>[D1997]</source>
          <target state="translated">[D1997]</target>
        </trans-unit>
        <trans-unit id="20e70caf2f764d35f0c5f51eb6c2cc52a6f947f2" translate="yes" xml:space="preserve">
          <source>[Davis2006]</source>
          <target state="translated">[Davis2006]</target>
        </trans-unit>
        <trans-unit id="3b0f17e8250c1e7b54512123b77c31bb0899ae7a" translate="yes" xml:space="preserve">
          <source>[Everingham2010]</source>
          <target state="translated">[Everingham2010]</target>
        </trans-unit>
        <trans-unit id="e5ff04dac92d8d5710e2d4ade54a4899d9a01662" translate="yes" xml:space="preserve">
          <source>[F1999]</source>
          <target state="translated">[F1999]</target>
        </trans-unit>
        <trans-unit id="ddfaba8b68f822d387eefcc49dbcf89bee83fcbe" translate="yes" xml:space="preserve">
          <source>[F2001]</source>
          <target state="translated">[F2001]</target>
        </trans-unit>
        <trans-unit id="5712ff07224dea2f09976ad1b5f9060cea098ee8" translate="yes" xml:space="preserve">
          <source>[FS1995]</source>
          <target state="translated">[FS1995]</target>
        </trans-unit>
        <trans-unit id="111b120f6e2f3a7d9723c16133fcfb1ce7b7557b" translate="yes" xml:space="preserve">
          <source>[Flach2015]</source>
          <target state="translated">[Flach2015]</target>
        </trans-unit>
        <trans-unit id="0be1b91bf292e6f295e73f8ba1626aa315ca1709" translate="yes" xml:space="preserve">
          <source>[Guyon2015]</source>
          <target state="translated">[Guyon2015]</target>
        </trans-unit>
        <trans-unit id="16deff704a4f867ca18d98b22e63afcb70ec96d2" translate="yes" xml:space="preserve">
          <source>[H1998]</source>
          <target state="translated">[H1998]</target>
        </trans-unit>
        <trans-unit id="346ddc10d1a23f1ff0ba05ea1da881f4666595c5" translate="yes" xml:space="preserve">
          <source>[HTF2009]</source>
          <target state="translated">[HTF2009]</target>
        </trans-unit>
        <trans-unit id="2b6bce181ae06e6796d39628b4dc12ca65767e20" translate="yes" xml:space="preserve">
          <source>[HTF]</source>
          <target state="translated">[HTF]</target>
        </trans-unit>
        <trans-unit id="8f4756ba18c793a637ad7568fd4450479f47b718" translate="yes" xml:space="preserve">
          <source>[Hubert1985]</source>
          <target state="translated">[Hubert1985]</target>
        </trans-unit>
        <trans-unit id="1c2acae56920363d695dc395b30aa0dac0656aa7" translate="yes" xml:space="preserve">
          <source>[Jen09]</source>
          <target state="translated">[Jen09]</target>
        </trans-unit>
        <trans-unit id="52833f723be6af6645b6622da4d471b65e89d942" translate="yes" xml:space="preserve">
          <source>[Kelleher2015]</source>
          <target state="translated">[Kelleher2015]</target>
        </trans-unit>
        <trans-unit id="8d63f432cd9715591fb04662784fbed01426124f" translate="yes" xml:space="preserve">
          <source>[L2014]</source>
          <target state="translated">[L2014]</target>
        </trans-unit>
        <trans-unit id="e6f810474b9d9bf1966e66d024bfd2d0d9af7983" translate="yes" xml:space="preserve">
          <source>[LG2012]</source>
          <target state="translated">[LG2012]</target>
        </trans-unit>
        <trans-unit id="4108a351bec333bc41371d8be81bbf1f0bd216a0" translate="yes" xml:space="preserve">
          <source>[LS2010]</source>
          <target state="translated">[LS2010]</target>
        </trans-unit>
        <trans-unit id="36c1f9470816b8da81a8ed80471ace8df2456c31" translate="yes" xml:space="preserve">
          <source>[M2012]</source>
          <target state="translated">[M2012]</target>
        </trans-unit>
        <trans-unit id="536dc6f43e65eedbc7dcd92d64ef850b0a7951d3" translate="yes" xml:space="preserve">
          <source>[MRS2008]</source>
          <target state="translated">[MRS2008]</target>
        </trans-unit>
        <trans-unit id="350f14f810397ce0cd7520f35f0fae7d54d72023" translate="yes" xml:space="preserve">
          <source>[Manning2008]</source>
          <target state="translated">[Manning2008]</target>
        </trans-unit>
        <trans-unit id="0cc214a1564fbbf90b4daa0b0972b6f8408e3afc" translate="yes" xml:space="preserve">
          <source>[Mosley2013]</source>
          <target state="translated">[Mosley2013]</target>
        </trans-unit>
        <trans-unit id="73be0b37b87d3c88f49438b3a7ca251dc3d3c1d2" translate="yes" xml:space="preserve">
          <source>[Mrl09]</source>
          <target state="translated">[Mrl09]</target>
        </trans-unit>
        <trans-unit id="690e639d5d468ec30ab9e54cb03287a1d07d286f" translate="yes" xml:space="preserve">
          <source>[NQY18]</source>
          <target state="translated">[NQY18]</target>
        </trans-unit>
        <trans-unit id="95849b59dfe0de62fa4f930bc19ca3b64ad51c5b" translate="yes" xml:space="preserve">
          <source>[R2007]</source>
          <target state="translated">[R2007]</target>
        </trans-unit>
        <trans-unit id="d27f89b25dd2806ef3b69d37ac341ea761b1f775" translate="yes" xml:space="preserve">
          <source>[RR2007]</source>
          <target state="translated">[RR2007]</target>
        </trans-unit>
        <trans-unit id="99aae3a9e5135b3c3c9e6f81c5583f53ea54b161" translate="yes" xml:space="preserve">
          <source>[RVD]</source>
          <target state="translated">[RVD]</target>
        </trans-unit>
        <trans-unit id="7fc4fd0834c6c78f63966f47416f1e672a05b032" translate="yes" xml:space="preserve">
          <source>[RVDriessen]</source>
          <target state="translated">[RVDriessen]</target>
        </trans-unit>
        <trans-unit id="9a3d290ec7e0cf466e2e530b732c430fd93742e5" translate="yes" xml:space="preserve">
          <source>[RW2006]</source>
          <target state="translated">[RW2006]</target>
        </trans-unit>
        <trans-unit id="ab40883d6ce3b576febad86ad20a220ae60fc722" translate="yes" xml:space="preserve">
          <source>[Rouseeuw1984]</source>
          <target state="translated">[Rouseeuw1984]</target>
        </trans-unit>
        <trans-unit id="f4ff5aad65e1461e94ef70a659337011d0c58126" translate="yes" xml:space="preserve">
          <source>[Rousseeuw]</source>
          <target state="translated">[Rousseeuw]</target>
        </trans-unit>
        <trans-unit id="74f2d2c5b044f5dc96afc1e0482d2495c57d5370" translate="yes" xml:space="preserve">
          <source>[Urbanowicz2015]</source>
          <target state="translated">[Urbanowicz2015]</target>
        </trans-unit>
        <trans-unit id="34cb3f1593778c84c25ed6665ee9a323140a68d9" translate="yes" xml:space="preserve">
          <source>[VEB2009] Vinh, Epps, and Bailey, (2009). &amp;ldquo;Information theoretic measures for clusterings comparison&amp;rdquo;. Proceedings of the 26th Annual International Conference on Machine Learning - ICML &amp;lsquo;09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi:10.1145/1553374.1553511&lt;/a&gt;. ISBN 9781605585161.</source>
          <target state="translated">[VEB2009] Vinh, Epps 및 Bailey (2009). &amp;ldquo;클러스터링 비교를위한 정보 이론적 조치&amp;rdquo;. 기계 학습에 관한 제 26 회 연례 국제 회의 절차-ICML '09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi : 10.1145 / 1553374.1553511&lt;/a&gt; . ISBN 9781605585161.</target>
        </trans-unit>
        <trans-unit id="48a5d15f42b24744d500812bf01a83ad55ecae83" translate="yes" xml:space="preserve">
          <source>[VEB2010] Vinh, Epps, and Bailey, (2010). &amp;ldquo;Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance&amp;rdquo;. JMLR &amp;lt;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt;&amp;gt;</source>
          <target state="translated">[VEB2010] Vinh, Epps 및 Bailey (2010). &amp;ldquo;클러스터링 비교를위한 정보 이론적 측정 : 변형, 속성, 정규화 및 수정에 대한 수정&amp;rdquo;. JMLR &amp;lt; &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt; &amp;gt;</target>
        </trans-unit>
        <trans-unit id="0cb878c6eb08bc32d947d6ddc4baaa42377cefd9" translate="yes" xml:space="preserve">
          <source>[VVZ2010]</source>
          <target state="translated">[VVZ2010]</target>
        </trans-unit>
        <trans-unit id="3db6db2379703ce194c034f1bb123b847f702832" translate="yes" xml:space="preserve">
          <source>[VZ2010]</source>
          <target state="translated">[VZ2010]</target>
        </trans-unit>
        <trans-unit id="4f385f25921c7c64a43d9b5e0a8904686fbad4ed" translate="yes" xml:space="preserve">
          <source>[X1, y1, &amp;hellip;, Xn, yn]</source>
          <target state="translated">[X1, y1, &amp;hellip;, Xn, yn]</target>
        </trans-unit>
        <trans-unit id="70b15e6dab0cb6917fd158eac29a9ddf08fcef21" translate="yes" xml:space="preserve">
          <source>[YAT2016] Yang, Algesheimer, and Tessone, (2016). &amp;ldquo;A comparative analysis of community detection algorithms on artificial networks&amp;rdquo;. Scientific Reports 6: 30750. &lt;a href=&quot;https://www.nature.com/articles/srep30750&quot;&gt;doi:10.1038/srep30750&lt;/a&gt;.</source>
          <target state="translated">[YAT2016] Yang, Algesheimer 및 Tessone (2016). &amp;ldquo;인공 네트워크에서 커뮤니티 감지 알고리즘의 비교 분석&amp;rdquo;. 과학 보고서 6 : 30750. &lt;a href=&quot;https://www.nature.com/articles/srep30750&quot;&gt;doi : 10.1038 / srep30750&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="652e235b7dc2f3d75c7f909152b3819c6c97a6ec" translate="yes" xml:space="preserve">
          <source>[Yates2011]</source>
          <target state="translated">[Yates2011]</target>
        </trans-unit>
        <trans-unit id="e3b9ec6790a1b2a9d7bcca67037f67b282ccc4ee" translate="yes" xml:space="preserve">
          <source>[ZZRH2009]</source>
          <target state="translated">[ZZRH2009]</target>
        </trans-unit>
        <trans-unit id="96b76160e0ad993ba5f0d18a67c96f386933b35a" translate="yes" xml:space="preserve">
          <source>[[1, x_1, x_1 ** 2, x_1 ** 3, &amp;hellip;],</source>
          <target state="translated">[[1, x_1, x_1 ** 2, x_1 ** 3,&amp;hellip;],</target>
        </trans-unit>
        <trans-unit id="2eab98aedd6e4787cc1b6ed7c5288a5a2279237c" translate="yes" xml:space="preserve">
          <source>[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.</source>
          <target state="translated">[callable] : 거리 배열을 허용하고 가중치를 포함하는 동일한 모양의 배열을 반환하는 사용자 정의 함수입니다.</target>
        </trans-unit>
        <trans-unit id="b622702bcf4592f09f68d731f0a4b9f486da53eb" translate="yes" xml:space="preserve">
          <source>[n_samples_a, n_features] otherwise Array of pairwise distances between samples, or a feature array.</source>
          <target state="translated">[n_samples_a, n_features] 그렇지 않으면 샘플 사이의 쌍별 거리 배열 또는 피처 배열입니다.</target>
        </trans-unit>
        <trans-unit id="361169bda90a02e4e54f39ff838115b39a75d83d" translate="yes" xml:space="preserve">
          <source>[wk]</source>
          <target state="translated">[wk]</target>
        </trans-unit>
        <trans-unit id="a4fcb5a87f4e322ea14fa74b5213a00a6d8c1551" translate="yes" xml:space="preserve">
          <source>\((y-\hat{y})^2\)</source>
          <target state="translated">\((y-\hat{y})^2\)</target>
        </trans-unit>
        <trans-unit id="b66b6613ed1a2db616d592d66bc160b7abcc7139" translate="yes" xml:space="preserve">
          <source>\(2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)\)</source>
          <target state="translated">\(2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)\)</target>
        </trans-unit>
        <trans-unit id="764bb49b144ee69e8d51ad0d7a5cedd15acf75d9" translate="yes" xml:space="preserve">
          <source>\(2(y\log\frac{y}{\hat{y}}-y+\hat{y})\)</source>
          <target state="translated">\(2(y\log\frac{y}{\hat{y}}-y+\hat{y})\)</target>
        </trans-unit>
        <trans-unit id="cade30b6baa03b8bc431f6cb3586bc5b1d642a6f" translate="yes" xml:space="preserve">
          <source>\(C\) is used to set the amount of regularization</source>
          <target state="translated">\ (C \)는 정규화 양을 설정하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ad93e4d23ea1b4d81a2b3f3c3d29dff7d49b5d27" translate="yes" xml:space="preserve">
          <source>\(D\) : input dimension</source>
          <target state="translated">\ (D \) : 입력 치수</target>
        </trans-unit>
        <trans-unit id="a32e93d880aaba8a860e59edd1cb39f288e59537" translate="yes" xml:space="preserve">
          <source>\(F1 = 2\frac{P \times R}{P+R}\)</source>
          <target state="translated">\ (F1 = 2 \ frac {P \ 시간 R} {P + R} \)</target>
        </trans-unit>
        <trans-unit id="6e8b3587e80a2131c2360cdeb81572614017e523" translate="yes" xml:space="preserve">
          <source>\(F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}\)</source>
          <target state="translated">\ (F_ \ beta (A, B) : = \ left (1 + \ beta ^ 2 \ right) \ frac {P (A, B) \ times R (A, B)} {\ beta ^ 2 P (A , B) + R (A, B)} \)</target>
        </trans-unit>
        <trans-unit id="9a350d9475f7eafeffa8a9835bca28d431bee93e" translate="yes" xml:space="preserve">
          <source>\(F_\beta(y, \hat{y})\)</source>
          <target state="translated">\ (F_ \ beta (y, \ hat {y}) \)</target>
        </trans-unit>
        <trans-unit id="09eb5f139bd0c207766706ffe3b5d171a556d4ec" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto 1 - \frac{x^2}{h^2}\)</source>
          <target state="translated">\ (K (x; h) \ propto 1-\ frac {x ^ 2} {h ^ 2} \)</target>
        </trans-unit>
        <trans-unit id="95dfc8e81899d8cb940cb0df0bfaa1052ffb36d2" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto 1 - x/h\) if \(x &amp;lt; h\)</source>
          <target state="translated">\ (x &amp;lt;h \) 인 경우 \ (K (x; h) \ propto 1-x / h \)</target>
        </trans-unit>
        <trans-unit id="2a910118bdbf495d82747952ba6971367b93dfac" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto 1\) if \(x &amp;lt; h\)</source>
          <target state="translated">\ (x &amp;lt;h \) 인 경우 \ (K (x; h) \ propto 1 \)</target>
        </trans-unit>
        <trans-unit id="4b298d5bd2fdf7182db10bf076925db3745527a8" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto \cos(\frac{\pi x}{2h})\) if \(x &amp;lt; h\)</source>
          <target state="translated">\ (x &amp;lt;h \) 경우 \ (K (x; h) \ propto \ cos (\ frac {\ pi x} {2h}) \)</target>
        </trans-unit>
        <trans-unit id="a4443829e2c48ab72daedb9b74f9dc8debc9681a" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto \exp(- \frac{x^2}{2h^2} )\)</source>
          <target state="translated">\ (K (x; h) \ propto \ exp (-\ frac {x ^ 2} {2h ^ 2}) \)</target>
        </trans-unit>
        <trans-unit id="c70853c06adf6986de8dcbc5a8f801fc0502ae1b" translate="yes" xml:space="preserve">
          <source>\(K(x; h) \propto \exp(-x/h)\)</source>
          <target state="translated">\ (K (x; h) \ propto \ exp (-x / h) \)</target>
        </trans-unit>
        <trans-unit id="7d9776eac061dd7f197e8155c6784a047082de92" translate="yes" xml:space="preserve">
          <source>\(L\) the set of labels</source>
          <target state="translated">라벨 세트 \ (L \)</target>
        </trans-unit>
        <trans-unit id="f0f78908f9cef66c5c39e39b004aba794a24d223" translate="yes" xml:space="preserve">
          <source>\(N\) : number of training data points</source>
          <target state="translated">\ (N \) : 훈련 데이터 포인트 수</target>
        </trans-unit>
        <trans-unit id="85d811223bbd2564d928a5184adabae9693a9903" translate="yes" xml:space="preserve">
          <source>\(P = \frac{T_p}{T_p+F_p}\)</source>
          <target state="translated">\ (P = \ fra {{T_p} {T_p + F_p} \)</target>
        </trans-unit>
        <trans-unit id="5c7b5d8ee367861eab9617a9df8debede099fe4a" translate="yes" xml:space="preserve">
          <source>\(P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\)</source>
          <target state="translated">\ (P (A, B) : = \ fra {{left | A \ cap B \ right |} {\ left | A \ right |} \)</target>
        </trans-unit>
        <trans-unit id="b2e10949fc710189ed8a464185eb0fbad995ab8d" translate="yes" xml:space="preserve">
          <source>\(P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\) for some sets \(A\) and \(B\)</source>
          <target state="translated">\ (P (A, B) : = \ frac {\ left | A \ cap B \ right |} {\ left | A \ right |} \) 일부 세트 \ (A \) 및 \ (B \)</target>
        </trans-unit>
        <trans-unit id="c180cdb6205e02ec243b8a0c392cb71991d0a3d0" translate="yes" xml:space="preserve">
          <source>\(P(y, \hat{y})\)</source>
          <target state="translated">\ (P (y, \ hat {y}) \)</target>
        </trans-unit>
        <trans-unit id="c753c8fc287915fbc16c3d512099716031e38eb4" translate="yes" xml:space="preserve">
          <source>\(R = \frac{T_p}{T_p + F_n}\)</source>
          <target state="translated">\ (R = \ fra {{T_p} {T_p + F_n} \)</target>
        </trans-unit>
        <trans-unit id="2d1e6161821a7c78dd1e8cc86ac7329f67bfcf0c" translate="yes" xml:space="preserve">
          <source>\(R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}\) (Conventions vary on handling \(B = \emptyset\); this implementation uses \(R(A, B):=0\), and similar for \(P\).)</source>
          <target state="translated">\ (R (A, B) : = \ fra {{left | A \ cap B \ right |} {\ left | B \ right |} \) (협약은 \ (B = \ emptyset \) 처리에 따라 다릅니다. 구현은 \ (R (A, B) : = 0 \)를 사용하고 \ (P \)와 유사합니다.)</target>
        </trans-unit>
        <trans-unit id="76e12b53dc747a452508648de37b3bbf1292886a" translate="yes" xml:space="preserve">
          <source>\(R(y, \hat{y})\)</source>
          <target state="translated">\ (R (y, \ hat {y}) \)</target>
        </trans-unit>
        <trans-unit id="fe30e7d28d145393d116de8ccfdb95f1930cc647" translate="yes" xml:space="preserve">
          <source>\(S\) the set of samples</source>
          <target state="translated">\ (S \) 샘플 세트</target>
        </trans-unit>
        <trans-unit id="4d975b6c5632f1c81d6cf8cc9f674c8e0d143548" translate="yes" xml:space="preserve">
          <source>\(X\): data</source>
          <target state="translated">\ (X \) : 데이터</target>
        </trans-unit>
        <trans-unit id="59b5b0f07758a431bbb7dbf6ebe63bc98b0cd7dd" translate="yes" xml:space="preserve">
          <source>\(\Omega\) is a &lt;code&gt;penalty&lt;/code&gt; function of our model parameters</source>
          <target state="translated">\ (\ Omega \)는 모델 매개 변수 의 &lt;code&gt;penalty&lt;/code&gt; 함수입니다.</target>
        </trans-unit>
        <trans-unit id="d82693345c0acee22512b8f82f5807d2eafe4d72" translate="yes" xml:space="preserve">
          <source>\(\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)\): This model is called &lt;a href=&quot;generated/sklearn.decomposition.factoranalysis#sklearn.decomposition.FactorAnalysis&quot;&gt;&lt;code&gt;FactorAnalysis&lt;/code&gt;&lt;/a&gt;, a classical statistical model. The matrix W is sometimes called the &amp;ldquo;factor loading matrix&amp;rdquo;.</source>
          <target state="translated">\ (\ Psi = \ mathrm {diag} (\ psi_1, \ psi_2, \ dots, \ psi_n) \) :이 모델을 클래식 통계 모델 인 &lt;a href=&quot;generated/sklearn.decomposition.factoranalysis#sklearn.decomposition.FactorAnalysis&quot;&gt; &lt;code&gt;FactorAnalysis&lt;/code&gt; &lt;/a&gt; 라고 합니다. 매트릭스 W는 때때로 &quot;인자 로딩 매트릭스&quot;로 불린다.</target>
        </trans-unit>
        <trans-unit id="fbd9ea497a59df8b22d9aee98766b15298f6f7b0" translate="yes" xml:space="preserve">
          <source>\(\Psi = \sigma^2 \mathbf{I}\): This assumption leads to the probabilistic model of &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">\ (\ Psi = \ sigma ^ 2 \ mathbf {I} \) :이 가정은 확률 론적 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 모델로 이어집니다 .</target>
        </trans-unit>
        <trans-unit id="01f8f0e3a7f60922aad934db70b43f4626579a4f" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{0,1}\)</source>
          <target state="translated">\(\alpha^{0}_{0,1}\)</target>
        </trans-unit>
        <trans-unit id="51f4c7f6d4e230692f7f7ca95d3a951f2ccb6900" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{0,2}\)</source>
          <target state="translated">\(\alpha^{0}_{0,2}\)</target>
        </trans-unit>
        <trans-unit id="fc59eea930c0650810949c61616237e10d7608e7" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{1,0}\)</source>
          <target state="translated">\(\alpha^{0}_{1,0}\)</target>
        </trans-unit>
        <trans-unit id="b7a1987ec2f1ae255445b9203ba76dab46f8180e" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{1,2}\)</source>
          <target state="translated">\(\alpha^{0}_{1,2}\)</target>
        </trans-unit>
        <trans-unit id="ddda7bd4dda2cccb6220d08cd96b97d2564703fc" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{2,0}\)</source>
          <target state="translated">\(\alpha^{0}_{2,0}\)</target>
        </trans-unit>
        <trans-unit id="cd76d3a6ac27cf4608fdba8c1bcefea98e40a082" translate="yes" xml:space="preserve">
          <source>\(\alpha^{0}_{2,1}\)</source>
          <target state="translated">\(\alpha^{0}_{2,1}\)</target>
        </trans-unit>
        <trans-unit id="670a0c506ecd80ad22241edc90899b4ca5a65063" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{0,1}\)</source>
          <target state="translated">\(\alpha^{1}_{0,1}\)</target>
        </trans-unit>
        <trans-unit id="715b85177f34a3aec41f1d6aca5ea943c045452d" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{0,2}\)</source>
          <target state="translated">\(\alpha^{1}_{0,2}\)</target>
        </trans-unit>
        <trans-unit id="9bafd2e8e2343917301e41ef344db246b60c40d3" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{1,0}\)</source>
          <target state="translated">\(\alpha^{1}_{1,0}\)</target>
        </trans-unit>
        <trans-unit id="bd6409137f75a05bb280bc5000840c3e16f9d850" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{1,2}\)</source>
          <target state="translated">\(\alpha^{1}_{1,2}\)</target>
        </trans-unit>
        <trans-unit id="e31f66e6ca63dbde84a383e5d61ebb97f39d2fa6" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{2,0}\)</source>
          <target state="translated">\(\alpha^{1}_{2,0}\)</target>
        </trans-unit>
        <trans-unit id="603e33877eb24d9878fa7efb61a9c926fb80dd71" translate="yes" xml:space="preserve">
          <source>\(\alpha^{1}_{2,1}\)</source>
          <target state="translated">\(\alpha^{1}_{2,1}\)</target>
        </trans-unit>
        <trans-unit id="9ffe8c9fb6dee2771ab0ca39575e1df479c8a1ca" translate="yes" xml:space="preserve">
          <source>\(\alpha^{2}_{0,1}\)</source>
          <target state="translated">\(\alpha^{2}_{0,1}\)</target>
        </trans-unit>
        <trans-unit id="7f0fc3efbc7900034a8626635972b65d78e7a257" translate="yes" xml:space="preserve">
          <source>\(\alpha^{2}_{0,2}\)</source>
          <target state="translated">\(\alpha^{2}_{0,2}\)</target>
        </trans-unit>
        <trans-unit id="69166f5380836f7b04cceea34acce5263efbceff" translate="yes" xml:space="preserve">
          <source>\(\beta\): Coefficients</source>
          <target state="translated">\ (\ beta \) : 계수</target>
        </trans-unit>
        <trans-unit id="fec074bbf7d22c3b3d841bc5e89270a0a2f32779" translate="yes" xml:space="preserve">
          <source>\(\epsilon\): Observation noise</source>
          <target state="translated">\ (\ epsilon \) : 관측 소음</target>
        </trans-unit>
        <trans-unit id="12791ef36495d018e1daf82140fd4c3db8180c28" translate="yes" xml:space="preserve">
          <source>\(\frac{(y-\hat{y})^2}{y\hat{y}^2}\)</source>
          <target state="translated">\(\frac{(y-\hat{y})^2}{y\hat{y}^2}\)</target>
        </trans-unit>
        <trans-unit id="08cc56772d86f4dfece03ec8003aba4f776d2aab" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | L \ right |} \ sum_ {l \ in L} F_ \ beta (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="8c4693b66d3ba7a40d8a9abfe000616818f55aaf" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | L \ right |} \ sum_ {l \ in L} P (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="52e6ce52e9b8ae2877c124e769de40084f5638d9" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | L \ right |} \ sum_ {l \ in L} R (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="7f2d1bc16a43e60170cc4d3e4429f1eaefb3b9b2" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | S \ right |} \ sum_ {s \ in S} F_ \ beta (y_s, \ hat {y} _s) \)</target>
        </trans-unit>
        <trans-unit id="301209259a59426be923af21027651f698a1adbb" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | S \ right |} \ sum_ {s \ in S} P (y_s, \ hat {y} _s) \)</target>
        </trans-unit>
        <trans-unit id="76d3b6c7bb7266c85c29df970c85d63d00762934" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)\)</source>
          <target state="translated">\ (\ frac {1} {\ left | S \ right |} \ sum_ {s \ in S} R (y_s, \ hat {y} _s) \)</target>
        </trans-unit>
        <trans-unit id="771a76dcfeccc7d1c5c81729270313804a1579cd" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| F_\beta(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ sum_ {l \ in L} \ left | \ hat {y} _l \ right |} \ sum_ {l \ in L} \ left | \ hat {y} _l \ right | F_ \ beta (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="8c17ed8e7ab856a74c9c5ce2466ffe22f7bfd45b" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| P(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ sum_ {l \ in L} \ left | \ hat {y} _l \ right |} \ sum_ {l \ in L} \ left | \ hat {y} _l \ right | P (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="084995f248284e6186e09f61b6402476e717eb20" translate="yes" xml:space="preserve">
          <source>\(\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| R(y_l, \hat{y}_l)\)</source>
          <target state="translated">\ (\ frac {1} {\ sum_ {l \ in L} \ left | \ hat {y} _l \ right |} \ sum_ {l \ in L} \ left | \ hat {y} _l \ right | R (y_l, \ hat {y} _l) \)</target>
        </trans-unit>
        <trans-unit id="6e89e30113ce1a3053ff34b9257e3d282abe2f88" translate="yes" xml:space="preserve">
          <source>\(\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}} = [0.8515, 0, 0.5243]\):</source>
          <target state="translated">\ (\ frac {[3, 0, 1.8473]} {\ sqrt {\ big (3 ^ 2 + 0 ^ 2 + 1.8473 ^ 2 \ big)}} = [0.8515, 0, 0.5243] \) :</target>
        </trans-unit>
        <trans-unit id="673551b6d125b3cd94d7674f49f994c6afdd6f65" translate="yes" xml:space="preserve">
          <source>\(\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}} = [ 0.819, 0, 0.573].\)</source>
          <target state="translated">\ (\ frac {[3, 0, 2.0986]} {\ sqrt {\ big (3 ^ 2 + 0 ^ 2 + 2.0986 ^ 2 \ big)}} = [0.819, 0, 0.573]. \)</target>
        </trans-unit>
        <trans-unit id="1a5082b9d05f10e66dae95fed7849c72d0a8cfc1" translate="yes" xml:space="preserve">
          <source>\(\gamma\) is known as slope</source>
          <target state="translated">\ (\ gamma \)는 기울기로 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="126898e7757cd77e4cefb87e42f683763bf54571" translate="yes" xml:space="preserve">
          <source>\(\hat{y}\) the set of &lt;em&gt;true&lt;/em&gt;\((sample, label)\) pairs</source>
          <target state="translated">\ (\ 모자 {Y} \)의 세트 &lt;em&gt;에 해당&lt;/em&gt; \ ((샘플 라벨) \) 쌍</target>
        </trans-unit>
        <trans-unit id="9f39f60cd1f2428420af5f2dec2780245432c521" translate="yes" xml:space="preserve">
          <source>\(\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle\)</source>
          <target state="translated">\ (\ langle F_ \ beta (y_l, \ hat {y} _l) | l \ in L \ rangle \)</target>
        </trans-unit>
        <trans-unit id="f3dc3b8f8243a37f6cfc63694bfee10bbc8dbb16" translate="yes" xml:space="preserve">
          <source>\(\langle P(y_l, \hat{y}_l) | l \in L \rangle\)</source>
          <target state="translated">\ (\ langle P (y_l, \ hat {y} _l) | l \ in L \ rangle \)</target>
        </trans-unit>
        <trans-unit id="1c6337bdf58558dfa45a337181a8da415d8b489c" translate="yes" xml:space="preserve">
          <source>\(\langle R(y_l, \hat{y}_l) | l \in L \rangle\)</source>
          <target state="translated">\ (\ langle R (y_l, \ hat {y} _l) | l \ in L \ rangle \)</target>
        </trans-unit>
        <trans-unit id="825ca6208fb0c99a89fb31d3479b162477b3c1b6" translate="yes" xml:space="preserve">
          <source>\(\mathcal{L}\) is a &lt;code&gt;loss&lt;/code&gt; function of our samples and our model parameters.</source>
          <target state="translated">\ (\ mathcal {L} \)은 샘플과 모델 파라미터 의 &lt;code&gt;loss&lt;/code&gt; 함수입니다.</target>
        </trans-unit>
        <trans-unit id="2a02f90f2b2001d6562373a7ae2a2e1dfb4565e2" translate="yes" xml:space="preserve">
          <source>\(\text{AP} = \sum_n (R_n - R_{n-1}) P_n\)</source>
          <target state="translated">\ (\ text {AP} = \ sum_n (R_n-R_ {n-1}) P_n \)</target>
        </trans-unit>
        <trans-unit id="db505c609ecce8b6164765bd7848f3b41ea35f0d" translate="yes" xml:space="preserve">
          <source>\(\text{df}(d, t)_{\text{term1}} = 6\)</source>
          <target state="translated">\ (\ text {df} (d, t) _ {\ text {term1}} = 6 \)</target>
        </trans-unit>
        <trans-unit id="e0997db48a8d87bf8ee1c7a8fcaa36916a54e707" translate="yes" xml:space="preserve">
          <source>\(\text{df}(t)_{\text{term1}} = 6\)</source>
          <target state="translated">\(\text{df}(t)_{\text{term1}} = 6\)</target>
        </trans-unit>
        <trans-unit id="3761c41a5e61f125696abaab3d6ab5e6c9467a9a" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(d, t)_{\text{term1}} = log \frac{n_d}{\text{df}(d, t)} + 1 = log(1)+1 = 1\)</source>
          <target state="translated">\ (\ text {idf} (d, t) _ {\ text {term1}} = log \ frac {n_d} {\ text {df} (d, t)} + 1 = log (1) +1 = 1 \)</target>
        </trans-unit>
        <trans-unit id="aa5a67b1ed0374bd72cf6f0e10b3fe3a74615260" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\)</source>
          <target state="translated">\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\)</target>
        </trans-unit>
        <trans-unit id="528e4287e895ebbf7910677616d1f6077d104040" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\),</source>
          <target state="translated">\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\),</target>
        </trans-unit>
        <trans-unit id="61843849cf83ac9cc60022c827bcec315d11ec03" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = \log{\frac{n}{1+\text{df}(t)}}.\)</source>
          <target state="translated">\(\text{idf}(t) = \log{\frac{n}{1+\text{df}(t)}}.\)</target>
        </trans-unit>
        <trans-unit id="3943a22b1a933b8cec2e621f9da837d0f8e0bccb" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = \log{\frac{n}{\text{df}(t)}} + 1\)</source>
          <target state="translated">\(\text{idf}(t) = \log{\frac{n}{\text{df}(t)}} + 1\)</target>
        </trans-unit>
        <trans-unit id="418ed87bb3a9ff83d64cef6c0afed152e2ce9063" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1\)</source>
          <target state="translated">\ (\ text {idf} (t) = log {\ frac {1 + n_d} {1+ \ text {df} (d, t)}} + 1 \)</target>
        </trans-unit>
        <trans-unit id="cbf2fa82ddd1456d42b5d1023dd8e97cfb8e3a89" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1\),</source>
          <target state="translated">\ (\ text {idf} (t) = log {\ frac {1 + n_d} {1+ \ text {df} (d, t)}} + 1 \),</target>
        </trans-unit>
        <trans-unit id="4b7b756caf347e71181dfa1202016ab8356f261a" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = log{\frac{n_d}{1+\text{df}(d,t)}}.\)</source>
          <target state="translated">\ (\ text {idf} (t) = log {\ frac {n_d} {1+ \ text {df} (d, t)}}. \)</target>
        </trans-unit>
        <trans-unit id="4034a1550b8c7d630dbd8eba4c06d1a4f1d30dc5" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t) = log{\frac{n_d}{\text{df}(d,t)}} + 1\)</source>
          <target state="translated">\ (\ text {idf} (t) = log {\ frac {n_d} {\ text {df} (d, t)}} + 1 \)</target>
        </trans-unit>
        <trans-unit id="96edf4cb5b96d644ba339f4099a39ef286ca85fb" translate="yes" xml:space="preserve">
          <source>\(\text{idf}(t)_{\text{term1}} = \log \frac{n}{\text{df}(t)} + 1 = \log(1)+1 = 1\)</source>
          <target state="translated">\(\text{idf}(t)_{\text{term1}} = \log \frac{n}{\text{df}(t)} + 1 = \log(1)+1 = 1\)</target>
        </trans-unit>
        <trans-unit id="f18c6689111b29d4354e7fffcee7cc0802b481a5" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{raw}} = [3, 0, 2.0986].\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {raw}} = [3, 0, 2.0986]. \)</target>
        </trans-unit>
        <trans-unit id="80aba8c317a078e4609ae8bd36660d65edad7083" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {term1}} = \ text {tf} \ times \ text {idf} = 3 \ times 1 = 3 \)</target>
        </trans-unit>
        <trans-unit id="1d70946637e51aace1378ca1a204825107f21cdf" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term2}} = 0 \times (\log(6/1)+1) = 0\)</source>
          <target state="translated">\(\text{tf-idf}_{\text{term2}} = 0 \times (\log(6/1)+1) = 0\)</target>
        </trans-unit>
        <trans-unit id="7f8322f95350f60ce08c7cdd3032f22e074bd2b1" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term2}} = 0 \times (log(6/1)+1) = 0\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {term2}} = 0 \ 회 (log (6/1) +1) = 0 \)</target>
        </trans-unit>
        <trans-unit id="a326be18c218e6683afc2f56f1612584e40eaa79" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term3}} = 1 \times (\log(6/2)+1) \approx 2.0986\)</source>
          <target state="translated">\(\text{tf-idf}_{\text{term3}} = 1 \times (\log(6/2)+1) \approx 2.0986\)</target>
        </trans-unit>
        <trans-unit id="dcd228feb463195495d0b530544c87a8f5f36307" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term3}} = 1 \times (log(6/2)+1) \approx 2.0986\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {term3}} = 1 \ times (log (6/2) +1) \ 약 2.0986 \)</target>
        </trans-unit>
        <trans-unit id="9c4af8e7df634860beafeba835d8777813396030" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term3}} = 1 \times \log(7/3)+1 \approx 1.8473\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {term3}} = 1 \ times \ log (7/3) +1 \ 약 1.8473 \)</target>
        </trans-unit>
        <trans-unit id="a8c4b99bd2ff2af3e5f2b8ace2008c6e0a9ef2e7" translate="yes" xml:space="preserve">
          <source>\(\text{tf-idf}_{\text{term3}} = 1 \times log(7/3)+1 \approx 1.8473\)</source>
          <target state="translated">\ (\ text {tf-idf} _ {\ text {term3}} = 1 \ times log (7/3) +1 \ 약 1.8473 \)</target>
        </trans-unit>
        <trans-unit id="b1e424267dd17efa4321c87fe24219eaad1c3249" translate="yes" xml:space="preserve">
          <source>\(a\), the number of pairs of elements that are in the same set in C and in the same set in K</source>
          <target state="translated">\ (a \), C에서 같은 세트에 있고 K에서 같은 세트에있는 요소 쌍의 수</target>
        </trans-unit>
        <trans-unit id="ffa07cceebbfadc52e2be9cdf6b9599e34083b82" translate="yes" xml:space="preserve">
          <source>\(b\), the number of pairs of elements that are in different sets in C and in different sets in K</source>
          <target state="translated">\ (b \), C에서 다른 세트에 있고 K에서 다른 세트에있는 요소 쌍의 수</target>
        </trans-unit>
        <trans-unit id="9634b82eb2f3a1a53f10d0105fa96b96683012b1" translate="yes" xml:space="preserve">
          <source>\(c=\sum_{k}^{K} C_{kk}\) the total number of samples correctly predicted,</source>
          <target state="translated">정확하게 예측 된 총 샘플 수 \ (c = \ sum_ {k} ^ {K} C_ {kk} \)</target>
        </trans-unit>
        <trans-unit id="e0b006e0f953d2967208fad0e101db9a95b1773e" translate="yes" xml:space="preserve">
          <source>\(c_0\) is known as intercept</source>
          <target state="translated">\ (c_0 \)는 가로 채기라고합니다</target>
        </trans-unit>
        <trans-unit id="20f463b5231561db0d1529b308e8cd1a67579869" translate="yes" xml:space="preserve">
          <source>\(d\) : output dimension</source>
          <target state="translated">\ (d \) : 출력 치수</target>
        </trans-unit>
        <trans-unit id="3c2644dcb6baee3a2f0954ec1f4febb35c2c968e" translate="yes" xml:space="preserve">
          <source>\(d_{ij}\), the distance between cluster centroids \(i\) and \(j\).</source>
          <target state="translated">\ (d_ {ij} \), 군집 중심 \ (i \)와 \ (j \) 사이의 거리.</target>
        </trans-unit>
        <trans-unit id="464de6af9c88cdb5c8c4a2137401febf7a5e41c7" translate="yes" xml:space="preserve">
          <source>\(k\) : number of nearest neighbors</source>
          <target state="translated">\ (k \) : 가장 가까운 이웃 수</target>
        </trans-unit>
        <trans-unit id="ded29692d567c68006b5e22274ae992999727d77" translate="yes" xml:space="preserve">
          <source>\(n = 6\)</source>
          <target state="translated">\(n = 6\)</target>
        </trans-unit>
        <trans-unit id="8f8783323f752c7593ada8acf8c4d67282a83607" translate="yes" xml:space="preserve">
          <source>\(n_{d} = 6\)</source>
          <target state="translated">\ (n_ {d} = 6 \)</target>
        </trans-unit>
        <trans-unit id="13e265f7db3a9509d50f014f7170db211a73f394" translate="yes" xml:space="preserve">
          <source>\(p_k=\sum_{i}^{K} C_{ki}\) the number of times class \(k\) was predicted,</source>
          <target state="translated">\ (p_k = \ sum_ {i} ^ {K} C_ {ki} \) 클래스 \ (k \)가 예측 된 횟수,</target>
        </trans-unit>
        <trans-unit id="cc4e2c50ba94767cc02eead7002753560f0219c2" translate="yes" xml:space="preserve">
          <source>\(s=\sum_{i}^{K} \sum_{j}^{K} C_{ij}\) the total number of samples.</source>
          <target state="translated">총 샘플 수 \ (s = \ sum_ {i} ^ {K} \ sum_ {j} ^ {K} C_ {ij} \)</target>
        </trans-unit>
        <trans-unit id="6eb93fac640cf512d1e63922116e7722c92fabe0" translate="yes" xml:space="preserve">
          <source>\(s_i\), the average distance between each point of cluster \(i\) and the centroid of that cluster &amp;ndash; also know as cluster diameter.</source>
          <target state="translated">\ (s_i \), 클러스터의 각 점 \ (i \)과 해당 클러스터의 중심 사이의 평균 거리 &amp;ndash; 클러스터 직경이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="2904d711734ca668dfddae3cff9c273e4d482636" translate="yes" xml:space="preserve">
          <source>\(t_k=\sum_{i}^{K} C_{ik}\) the number of times class \(k\) truly occurred,</source>
          <target state="translated">\ (k \) 클래스가 실제로 발생한 횟수 \ (t_k = \ sum_ {i} ^ {K} C_ {ik} \)</target>
        </trans-unit>
        <trans-unit id="0a59d79c97bec565e1c47b5aab7109c37d7376fa" translate="yes" xml:space="preserve">
          <source>\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 + v{_2}^2 + \dots + v{_n}^2}}\)</source>
          <target state="translated">\ (v_ {norm} = \ frac {v} {|| v || _2} = \ frac {v} {\ sqrt {v {_1} ^ 2 + v {_2} ^ 2 + \ dots + v {_n } ^ 2}} \)</target>
        </trans-unit>
        <trans-unit id="ef3a9e22b14c74ec22385c615853f9e45898420d" translate="yes" xml:space="preserve">
          <source>\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 + v{_2}^2 + \dots + v{_n}^2}}\).</source>
          <target state="translated">\ (v_ {norm} = \ frac {v} {|| v || _2} = \ frac {v} {\ sqrt {v {_1} ^ 2 + v {_2} ^ 2 + \ dots + v {_n } ^ 2}} \).</target>
        </trans-unit>
        <trans-unit id="e9fe36499695707a4fa25e6312e0decc4b5ce11a" translate="yes" xml:space="preserve">
          <source>\(x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)\).</source>
          <target state="translated">\ (x_1 \ leq x_1 '\은 F (x_1, x_2) \ geq F (x_1', x_2) \)를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="12ab6bd6cdfcad92f2c8d9cd45d8531fb50b8225" translate="yes" xml:space="preserve">
          <source>\(x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\), where \(F\) is the predictor with two features.</source>
          <target state="translated">\ (x_1 \ leq x_1 '\ implies F (x_1, x_2) \ leq F (x_1', x_2) \), 여기서 \ (F \)는 두 특성을 가진 예측 변수입니다.</target>
        </trans-unit>
        <trans-unit id="d224f9dd671e9669fa16b0ba657459625c42e63b" translate="yes" xml:space="preserve">
          <source>\(y \in (-\infty, \infty)\)</source>
          <target state="translated">\(y \in (-\infty, \infty)\)</target>
        </trans-unit>
        <trans-unit id="7fb9264ce64cd7d5e1a33f2fcf663917cc9226ab" translate="yes" xml:space="preserve">
          <source>\(y \in (0, \infty)\)</source>
          <target state="translated">\(y \in (0, \infty)\)</target>
        </trans-unit>
        <trans-unit id="6ffefe23f049210303238c3770c5a3a8cbf9ad47" translate="yes" xml:space="preserve">
          <source>\(y \in [0, \infty)\)</source>
          <target state="translated">\(y \in [0, \infty)\)</target>
        </trans-unit>
        <trans-unit id="2f32f66d76e16029cdb54470231bf86f45c56290" translate="yes" xml:space="preserve">
          <source>\(y\) the set of &lt;em&gt;predicted&lt;/em&gt;\((sample, label)\) pairs</source>
          <target state="translated">\ (y \) &lt;em&gt;예측 된&lt;/em&gt; \ ((sample, label) \) 쌍 세트</target>
        </trans-unit>
        <trans-unit id="84dbf0f25232d7453b4b36e6d3fffb9c5594408b" translate="yes" xml:space="preserve">
          <source>\(y\): target variable</source>
          <target state="translated">\ (y \) : 대상 변수</target>
        </trans-unit>
        <trans-unit id="29655c66149656107eee3658832dbefda0f10f88" translate="yes" xml:space="preserve">
          <source>\(y_l\) the subset of \(y\) with label \(l\)</source>
          <target state="translated">\ (y_l \) 레이블이 \ (l \) 인 \ (y \)의 서브 세트</target>
        </trans-unit>
        <trans-unit id="2e082165475a8fcee912e7d794c4b87072c02854" translate="yes" xml:space="preserve">
          <source>\(y_s\) the subset of \(y\) with sample \(s\), i.e. \(y_s := \left\{(s', l) \in y | s' = s\right\}\)</source>
          <target state="translated">\ (y_s \) 샘플 \ (s \)를 가진 \ (y \)의 서브 세트, 즉 \ (y_s : = \ left \ {(s ', l) \ in y | s'= s \ right \} \ )</target>
        </trans-unit>
        <trans-unit id="83a2c7fcc781f513174d7284ba894e2de571456a" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\\\Downarrow\\\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} P (y \ mid x_1, \ dots, x_n) \ propto P (y) \ prod_ {i = 1} ^ {n} P (x_i \ mid y) \\ \ Downarrow \\\ hat {y} = \ arg \ max_y P (y) \ prod_ {i = 1} ^ {n} P (x_i \ mid y), \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="c500aca02db178fd925a59974dd45ddee8face02" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}Q_{left}(\theta) = {(x, y) | x_j &amp;lt;= t_m}\\Q_{right}(\theta) = Q \setminus Q_{left}(\theta)\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} Q_ {left} (\ theta) = {(x, y) | x_j &amp;lt;= t_m} \\ Q_ {right} (\ theta) = Q \ setminus Q_ {left} (\ theta) \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="6d63d132be650c44cbd8191fea374d6099c68415" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\bar{y}_m = \frac{1}{N_m} \sum_{i \in N_m} y_i\\H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - \bar{y}_m)^2\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ bar {y} _m = \ frac {1} {N_m} \ sum_ {i \ in N_m} y_i \\ H (X_m) = \ frac {1} {N_m } \ sum_ {i \ in N_m} (y_i-\ bar {y} _m) ^ 2 \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="09657abb97474afe531f716bc21393139a923381" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\bar{y}_m = \frac{1}{N_m} \sum_{i \in N_m} y_i\\H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - \bar{y}_m|\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ bar {y} _m = \ frac {1} {N_m} \ sum_ {i \ in N_m} y_i \\ H (X_m) = \ frac {1} {N_m } \ sum_ {i \ in N_m} | y_i-\ bar {y} _m | \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="a6a013161b26d9345009bc657a0fa52bacc545a7" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\hat{\theta}_{ci} = \frac{\alpha_i + \sum_{j:y_j \neq c} d_{ij}} {\alpha + \sum_{j:y_j \neq c} \sum_{k} d_{kj}}\\w_{ci} = \log \hat{\theta}_{ci}\\w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ hat {\ theta} _ {ci} = \ fra {{alpha_i + \ sum_ {j : y_j \ neq c} d_ {ij}} {\ alpha + \ sum_ {j : y_j \ neq c} \ sum_ {k} d_ {kj}} \\ w_ {ci} = \ log \ hat {\ theta} _ {ci} \\ w_ {ci} = \ frac {w_ {ci }} {\ sum_ {j} | w_ {cj} |} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="3d5da4b9043141ddac28de63232838abbbe5a18b" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\log\left(\frac{P(y=k|X)}{P(y=l|X)}\right)= \log\left(\frac{P(X|y=k)P(y=k)}{P(X|y=l)P(y=l)}\right)=0 \Leftrightarrow\\(\mu_k-\mu_l)^t\Sigma^{-1} X = \frac{1}{2} (\mu_k^t \Sigma^{-1} \mu_k - \mu_l^t \Sigma^{-1} \mu_l) - \log\frac{P(y=k)}{P(y=l)}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ log \ left (\ frac {P (y = k | X)} ​​{P (y = l | X)} ​​\ right) = \ log \ left (\ frac {P (X | y = k) P (y = k)} {P (X | y = l) P (y = l)} \ right) = 0 \ 왼쪽 화살표 \\ (\ mu_k- \ mu_l) ^ t \ Sigma ^ {-1} X = \ fra {1} {2} (\ mu_k ^ t \ Sigma ^ {-1} \ mu_k-\ mu_l ^ t \ Sigma ^ {-1} \ mu_l)-\ log \ frac {P (y = k)} {P (y = l)} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="970228f13e58c5000f67243636530c2e6768a476" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)\\\begin{split}\textrm {subject to } &amp;amp; y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\ &amp;amp; w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\ &amp;amp; \zeta_i, \zeta_i^* \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ min_ {w, b, \ zeta, \ zeta ^ *} \ frac {1} {2} w ^ T w + C \ sum_ {i = 1} ^ { n} (\ zeta_i + \ zeta_i ^ *) \\\ begin {split} \ textrm {}에 따라 &amp;amp; y_i-w ^ T \ phi (x_i)-b \ leq \ varepsilon + \ zeta_i, \\ &amp;amp; w ^ T \ phi (x_i) + b-y_i \ leq \ varepsilon + \ zeta_i ^ *, \\ &amp;amp; \ zeta_i, \ zeta_i ^ * \ geq 0, i = 1, ..., n \ end {split} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="e7cf54257feefa3a1cdfa65288e5d1230916a9ce" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\\\begin{split}\textrm {subject to } &amp;amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\ &amp;amp; \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ min_ {w, b, \ zeta} \ fra {1} {2} w ^ T w + C \ sum_ {i = 1} ^ {n} \ zeta_i \ \\ 시작 {split} \ textrm {}에 따라 &amp;amp; y_i (w ^ T \ phi (x_i) + b) \ geq 1-\ zeta_i, \\ &amp;amp; \ zeta_i \ geq 0, i = 1, ..., n \ end {split} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="80291068f9bc632282f639938cf37593076f7e40" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)\\\begin{split} \textrm {subject to } &amp;amp; e^T (\alpha - \alpha^*) = 0\\ &amp;amp; 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n\end{split}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ min _ {\ alpha, \ alpha ^ *} \ frac {1} {2} (\ alpha-\ alpha ^ *) ^ TQ (\ alpha-\ alpha ^ * ) + \ varepsilon e ^ T (\ alpha + \ alpha ^ *)-y ^ T (\ alpha-\ alpha ^ *) \\\ begin {split} \ textrm {}에 따라} &amp;amp; e ^ T (\ alpha- \ alpha ^ *) = 0 \\ &amp;amp; 0 \ leq \ alpha_i, \ alpha_i ^ * \ leq C, i = 1, ..., n \ end {split} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="6c7b70ef69da1a978d3d70eb6e72d1cfe185904a" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\\\begin{split} \textrm {subject to } &amp;amp; y^T \alpha = 0\\ &amp;amp; 0 \leq \alpha_i \leq C, i=1, ..., n\end{split}\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} \ min _ {\ alpha} \ frac {1} {2} \ alpha ^ TQ \ alpha-e ^ T \ alpha \\\ begin {split} \ textrm {대상에 따라 } &amp;amp; y ^ T \ alpha = 0 \\ &amp;amp; 0 \ leq \ alpha_i \ leq C, i = 1, ..., n \ end {split} \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="b61ecb24510d037b2f103f5394b305a25a58f23a" translate="yes" xml:space="preserve">
          <source>\[ \begin{align}\begin{aligned}median(y)_m = \underset{i \in N_m}{\mathrm{median}}(y_i)\\H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - median(y)_m|\end{aligned}\end{align} \]</source>
          <target state="translated">\ [\ begin {align} \ begin {aligned} median (y) _m = \ underset {i \ in N_m} {\ mathrm {median}} (y_i) \\ H (X_m) = \ frac {1} {N_m } \ sum_ {i \ in N_m} | y_i-중앙값 (y) _m | \ end {aligned} \ end {align} \]</target>
        </trans-unit>
        <trans-unit id="d6188a7c021d3288e439ea7246a74358b63a884c" translate="yes" xml:space="preserve">
          <source>\[(1 - eps) \|u - v\|^2 &amp;lt; \|p(u) - p(v)\|^2 &amp;lt; (1 + eps) \|u - v\|^2\]</source>
          <target state="translated">\ [(1-eps) \ | u-v \ | ^ 2 &amp;lt;\ | p (u)-p (v) \ | ^ 2 &amp;lt;(1 + eps) \ | u-v \ | ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="c4407dbb151e2710b04032ff6939f68ed3c7179c" translate="yes" xml:space="preserve">
          <source>\[A_n = R^{-1/2} A C^{-1/2}\]</source>
          <target state="translated">\ [A_n = R ^ {-1/2} AC ^ {-1/2} \]</target>
        </trans-unit>
        <trans-unit id="f6b0d9bcf51ca0db910abd2831694b2e6df3d3e4" translate="yes" xml:space="preserve">
          <source>\[BS = \frac{1}{N} \sum_{t=1}^{N}(f_t - o_t)^2\]</source>
          <target state="translated">\ [BS = \ fra {1} {N} \ sum_ {t = 1} ^ {N} (f_t-o_t) ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="c92a9c50030567e1c914fdb60f6f36c624d7857a" translate="yes" xml:space="preserve">
          <source>\[B_k = \sum_q n_q (c_q - c) (c_q - c)^T\]</source>
          <target state="translated">\ [B_k = \ sum_q n_q (c_q-c) (c_q-c) ^ T \]</target>
        </trans-unit>
        <trans-unit id="ba2a7d3bd989e0c77672451c8ebf6d95f02e79e3" translate="yes" xml:space="preserve">
          <source>\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]</source>
          <target state="translated">\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]</target>
        </trans-unit>
        <trans-unit id="f7004892a8adf03dbfdffcdcf3787b7ebb419e77" translate="yes" xml:space="preserve">
          <source>\[C \sum_{i=1, n} \mathcal{L} (f(x_i), y_i) + \Omega (w)\]</source>
          <target state="translated">\ [C \ sum_ {i = 1, n} \ mathcal {L} (f (x_i), y_i) + \ Omega (w) \]</target>
        </trans-unit>
        <trans-unit id="cdd0300688de915acfc1a115df5a69adf7a6197d" translate="yes" xml:space="preserve">
          <source>\[D(x, y) = 2\arcsin[\sqrt{\sin^2((x1 - y1) / 2) + \cos(x1)\cos(y1)\sin^2((x2 - y2) / 2)}]\]</source>
          <target state="translated">\[D(x, y) = 2\arcsin[\sqrt{\sin^2((x1 - y1) / 2) + \cos(x1)\cos(y1)\sin^2((x2 - y2) / 2)}]\]</target>
        </trans-unit>
        <trans-unit id="24e7ca264d5b1b5ecf81452a44a55176b2008b8c" translate="yes" xml:space="preserve">
          <source>\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]</source>
          <target state="translated">\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]</target>
        </trans-unit>
        <trans-unit id="69d401f524d7e967afe2dcb64dc99487200dfcd8" translate="yes" xml:space="preserve">
          <source>\[DB = \frac{1}{k} \sum{i=1}^k \max_{i \neq j} R_{ij}\]</source>
          <target state="translated">\ [DB = \ fra {1} {k} \ sum {i = 1} ^ k \ max_ {i \ neq j} R_ {ij} \]</target>
        </trans-unit>
        <trans-unit id="0e40c0ff9b9201b3e6cc8ee7f729672fe6ce4c93" translate="yes" xml:space="preserve">
          <source>\[E(\mathbf{v}, \mathbf{h}) = -\sum_i \sum_j w_{ij}v_ih_j - \sum_i b_iv_i - \sum_j c_jh_j\]</source>
          <target state="translated">\ [E (\ mathbf {v}, \ mathbf {h}) =-\ sum_i \ sum_j w_ {ij} v_ih_j-\ sum_i b_iv_i-\ sum_j c_jh_j \]</target>
        </trans-unit>
        <trans-unit id="783de3e797180d6f4ba6c3e9379606ecef2ead2a" translate="yes" xml:space="preserve">
          <source>\[E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)\]</source>
          <target state="translated">\ [E (w, b) = \ frac {1} {n} \ sum_ {i = 1} ^ {n} L (y_i, f (x_i)) + \ alpha R (w) \]</target>
        </trans-unit>
        <trans-unit id="365f54944565346973bba6038073efb7e313ed11" translate="yes" xml:space="preserve">
          <source>\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+ }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right) \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})! (N-a_i-b_j+n_{ij})!}\]</source>
          <target state="translated">\ [E [\ text {MI} (U, V)] = \ sum_ {i = 1} ^ {| U |} \ sum_ {j = 1} ^ {| V |} \ sum_ {n_ {ij} = (a_i + b_j-N) ^ +} ^ {\ min (a_i, b_j)} \ frac {n_ {ij}} {N} \ log \ left (\ frac {N.n_ {ij}} {a_i b_j} \ right) \ fra {a_i! b_j! (N-a_i)! (N-b_j)!} {N! n_ {ij}! (a_i-n_ {ij})! (b_j-n_ {ij})! (N-a_i-b_j + n_ {ij})!} \]</target>
        </trans-unit>
        <trans-unit id="fa1ff8faa81d9e66252f62a095f5cf70ca1078c1" translate="yes" xml:space="preserve">
          <source>\[F(x) = \sum_{m=1}^{M} \gamma_m h_m(x)\]</source>
          <target state="translated">\ [F (x) = \ sum_ {m = 1} ^ {M} \ gamma_m h_m (x) \]</target>
        </trans-unit>
        <trans-unit id="8b0368e365c0b4ab2c5226a46b2b8384821b7abd" translate="yes" xml:space="preserve">
          <source>\[F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}.\]</source>
          <target state="translated">\ [F_ \ beta = (1 + \ beta ^ 2) \ frac {\ text {precision} \ times \ text {recall}} {\ beta ^ 2 \ text {precision} + \ text {recall}}. \]</target>
        </trans-unit>
        <trans-unit id="c9eeb87b260c2954980548a507a8b1393c039854" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) + \arg\min_{h} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + h(x))\]</source>
          <target state="translated">\ [F_m (x) = F_ {m-1} (x) + \ arg \ min_ {h} \ sum_ {i = 1} ^ {n} L (y_i, F_ {m-1} (x_i) + h (엑스))\]</target>
        </trans-unit>
        <trans-unit id="c3b959c536d9b51ce676e93f30895d0c6d681eae" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)\]</source>
          <target state="translated">\ [F_m (x) = F_ {m-1} (x) + \ gamma_m h_m (x) \]</target>
        </trans-unit>
        <trans-unit id="9b119698c7b1cbf47db5678415973bc69f053f0c" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)\]</source>
          <target state="translated">\ [F_m (x) = F_ {m-1} (x) + \ nu \ gamma_m h_m (x) \]</target>
        </trans-unit>
        <trans-unit id="f67ac5f779f99408bbddee5aabe3119d85a325aa" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]</source>
          <target state="translated">\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]</target>
        </trans-unit>
        <trans-unit id="53cc4ef6b492c65482e59fea0173cd005acf6ca7" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) + h_m(x),\]</source>
          <target state="translated">\[F_m(x) = F_{m-1}(x) + h_m(x),\]</target>
        </trans-unit>
        <trans-unit id="79620d3f64f3ecc0a5f216eb6a57691d801c15cd" translate="yes" xml:space="preserve">
          <source>\[F_m(x) = F_{m-1}(x) - \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i, F_{m-1}(x_i))\]</source>
          <target state="translated">\ [F_m (x) = F_ {m-1} (x)-\ gamma_m \ sum_ {i = 1} ^ {n} \ nabla_F L (y_i, F_ {m-1} (x_i)) \]</target>
        </trans-unit>
        <trans-unit id="90cafad385d21b4d4db9860dff7480af31e6cc3a" translate="yes" xml:space="preserve">
          <source>\[G(Q, \theta) = \frac{n_{left}}{N_m} H(Q_{left}(\theta)) + \frac{n_{right}}{N_m} H(Q_{right}(\theta))\]</source>
          <target state="translated">\ [G (Q, \ theta) = \ frac {n_ {left}} {N_m} H (Q_ {left} (\ theta)) + \ frac {n_ {right}} {N_m} H (Q_ {right} (\ theta)) \]</target>
        </trans-unit>
        <trans-unit id="a3b30c3461532826adfe7425f51daed9aaf98e97" translate="yes" xml:space="preserve">
          <source>\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]</source>
          <target state="translated">\ [H (C) =-\ sum_ {c = 1} ^ {| C |} \ fra {n_c} {n} \ cdot \ log \ left (\ frac {n_c} {n} \ right) \]</target>
        </trans-unit>
        <trans-unit id="672f9873cccc012ff842c8c7fce069ac5cd92675" translate="yes" xml:space="preserve">
          <source>\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n} \cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]</source>
          <target state="translated">\ [H (C | K) =-\ sum_ {c = 1} ^ {| C |} \ sum_ {k = 1} ^ {| K |} \ frac {n_ {c, k}} {n} \ cdot \ log \ left (\ frac {n_ {c, k}} {n_k} \ right) \]</target>
        </trans-unit>
        <trans-unit id="9dcc776906a998f47112457373d3636e82b5c382" translate="yes" xml:space="preserve">
          <source>\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]</source>
          <target state="translated">\ [H (U) =-\ sum_ {i = 1} ^ {| U |} P (i) \ log (P (i)) \]</target>
        </trans-unit>
        <trans-unit id="bf873d01bafa66fa00d4a9abe244e71c3ad5c483" translate="yes" xml:space="preserve">
          <source>\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]</source>
          <target state="translated">\ [H (V) =-\ sum_ {j = 1} ^ {| V |} P '(j) \ log (P'(j)) \]</target>
        </trans-unit>
        <trans-unit id="825bd8051092a0ff4da11fbb8a018da499318440" translate="yes" xml:space="preserve">
          <source>\[H(X_m) = - \sum_k p_{mk} \log(p_{mk})\]</source>
          <target state="translated">\ [H (X_m) =-\ sum_k p_ {mk} \ log (p_ {mk}) \]</target>
        </trans-unit>
        <trans-unit id="86fb9fbe52343d6db8c412062ca26e8b3a61d7f8" translate="yes" xml:space="preserve">
          <source>\[H(X_m) = 1 - \max(p_{mk})\]</source>
          <target state="translated">\ [H (X_m) = 1-\ max (p_ {mk}) \]</target>
        </trans-unit>
        <trans-unit id="471e679a9a1baa18ae5b83f786849ce5561bcf62" translate="yes" xml:space="preserve">
          <source>\[H(X_m) = \sum_k p_{mk} (1 - p_{mk})\]</source>
          <target state="translated">\ [H (X_m) = \ sum_k p_ {mk} (1-p_ {mk}) \]</target>
        </trans-unit>
        <trans-unit id="d07612141f923e53a5d822ae265800fc511ebfa2" translate="yes" xml:space="preserve">
          <source>\[J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}\]</source>
          <target state="translated">\ [J (A, B) = \ frac {| A \ cap B |} {| A | + | B | -| A \ cap B |} \]</target>
        </trans-unit>
        <trans-unit id="b38d8ccb5f6105408ebebb5c4de384f0bcc0244c" translate="yes" xml:space="preserve">
          <source>\[J(y_i, \hat{y}_i) = \frac{|y_i \cap \hat{y}_i|}{|y_i \cup \hat{y}_i|}.\]</source>
          <target state="translated">\ [J (y_i, \ hat {y} _i) = \ frac {| y_i \ cap \ hat {y} _i |} {| y_i \ cup \ hat {y} _i |}. \]</target>
        </trans-unit>
        <trans-unit id="d318f9ea1800cd74dd7a27a0e7aa26e39a217efa" translate="yes" xml:space="preserve">
          <source>\[K_{ij} = L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot j}} + \overline{L_{\cdot \cdot}}\]</source>
          <target state="translated">\ [K_ {ij} = L_ {ij}-\ overline {L_ {i \ cdot}}-\ overline {L _ {\ cdot j}} + \ overline {L _ {\ cdot \ cdot}} \]</target>
        </trans-unit>
        <trans-unit id="4f142a1b3f1a25b7844f5bd577802840d9086424" translate="yes" xml:space="preserve">
          <source>\[LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0} \sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}\]</source>
          <target state="translated">\ [LRAP (y, \ hat {f}) = \ frac {1} {n _ {\ text {samples}}} \ sum_ {i = 0} ^ {n _ {\ text {samples}}-1} \ frac {1} {|| y_i || _0} \ sum_ {j : y_ {ij} = 1} \ frac {| \ mathcal {L} _ {ij} |} {\ text {rank} _ {ij}} \ ]</target>
        </trans-unit>
        <trans-unit id="ce318e5aa62c88285e6dbc3450ddc43884867d96" translate="yes" xml:space="preserve">
          <source>\[L_\text{Hinge}(y, w) = \max\left\{1 - wy, 0\right\} = \left|1 - wy\right|_+\]</source>
          <target state="translated">\ [L_ \ text {Hinge} (y, w) = \ max \ left \ {1-wy, 0 \ right \} = \ left | 1-wy \ right | _ + \]</target>
        </trans-unit>
        <trans-unit id="ba42e2e6c73e49cc69262971d98739da8f3de127" translate="yes" xml:space="preserve">
          <source>\[L_\text{Hinge}(y_w, y_t) = \max\left\{1 + y_t - y_w, 0\right\}\]</source>
          <target state="translated">\ [L_ \ text {Hinge} (y_w, y_t) = \ max \ left \ {1 + y_t-y_w, 0 \ right \} \]</target>
        </trans-unit>
        <trans-unit id="7d42c3e71601b07673f2ff67be6498a9b9b5b660" translate="yes" xml:space="preserve">
          <source>\[L_{0-1}(y_i, \hat{y}_i) = 1(\hat{y}_i \not= y_i)\]</source>
          <target state="translated">\ [L_ {0-1} (y_i, \ hat {y} _i) = 1 (\ hat {y} _i \ not = y_i) \]</target>
        </trans-unit>
        <trans-unit id="2d55be0d74f5ef605272bd6c32e6867b2bf3e8e6" translate="yes" xml:space="preserve">
          <source>\[L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{labels}} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_j \not= y_j)\]</source>
          <target state="translated">\ [L_ {Hamming} (y, \ hat {y}) = \ fra {1} {n_ \ text {labels}} \ sum_ {j = 0} ^ {n_ \ text {labels}-1} 1 (\ 모자 {y} _j \ not = y_j) \]</target>
        </trans-unit>
        <trans-unit id="b3092c91dac051243dcdc2b0e51b2cc71f3f1851" translate="yes" xml:space="preserve">
          <source>\[L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}^{N-1} \sum_{k=0}^{K-1} y_{i,k} \log p_{i,k}\]</source>
          <target state="translated">\ [L _ {\ log} (Y, P) =-\ log \ 연산자 이름 {Pr} (Y | P) =-\ frac {1} {N} \ sum_ {i = 0} ^ {N-1} \ sum_ {k = 0} ^ {K-1} y_ {i, k} \ log p_ {i, k} \]</target>
        </trans-unit>
        <trans-unit id="253a9ad7d14147f9b4d52f808d7adc29f4f1d369" translate="yes" xml:space="preserve">
          <source>\[L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))\]</source>
          <target state="translated">\ [L _ {\ log} (y, p) =-\ log \ 연산자 이름 {Pr} (y | p) =-(y \ log (p) + (1-y) \ log (1-p)) \ ]</target>
        </trans-unit>
        <trans-unit id="2efbeda9bdd157cc2d7a3c7780634a18e9cf252c" translate="yes" xml:space="preserve">
          <source>\[Loss(\hat{y},y,W) = -y \ln {\hat{y}} - (1-y) \ln{(1-\hat{y})} + \alpha ||W||_2^2\]</source>
          <target state="translated">\ [손실 (\ hat {y}, y, W) = -y \ ln {\ hat {y}}-(1-y) \ ln {(1- \ hat {y})} + \ alpha || W || _2 ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="ad216be468be053e061c385b7f1c21d710fe84a9" translate="yes" xml:space="preserve">
          <source>\[Loss(\hat{y},y,W) = \frac{1}{2}||\hat{y} - y ||_2^2 + \frac{\alpha}{2} ||W||_2^2\]</source>
          <target state="translated">\ [손실 (\ hat {y}, y, W) = \ frac {1} {2} || \ hat {y}-y || _2 ^ 2 + \ fra {{alpha} {2} || W || _2 ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="3d4d43e0aa8af2572cd4c7afaa27772d600c52ba" translate="yes" xml:space="preserve">
          <source>\[MCC = \frac{ c \times s - \sum_{k}^{K} p_k \times t_k }{\sqrt{ (s^2 - \sum_{k}^{K} p_k^2) \times (s^2 - \sum_{k}^{K} t_k^2) }}\]</source>
          <target state="translated">\ [MCC = \ fra {{c \ times s-\ sum_ {k} ^ {K} p_k \ times t_k} {\ sqrt {(s ^ 2-\ sum_ {k} ^ {K} p_k ^ 2) \ times (s ^ 2-\ sum_ {k} ^ {K} t_k ^ 2)}} \]</target>
        </trans-unit>
        <trans-unit id="f5497e39840904c9cb7d2472c1ab3621a66cd485" translate="yes" xml:space="preserve">
          <source>\[MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\]</source>
          <target state="translated">\ [MCC = \ fra {{tp \ times tn-fp \ times fn} {\ sqrt {(tp + fp) (tp + fn) (tn + fp) (tn + fn)}}. \]</target>
        </trans-unit>
        <trans-unit id="a3da3c85336ea30ff991df5f7886c792d671d3d1" translate="yes" xml:space="preserve">
          <source>\[MI(U,V)=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i\cap V_j|}{N} \log\frac{N|U_i \cap V_j|}{|U_i||V_j|}\]</source>
          <target state="translated">\ [MI (U, V) = \ sum_ {i = 1} ^ {| U |} \ sum_ {j = 1} ^ {| V |} \ frac {| U_i \ cap V_j |} {N} \ log \ frac {N | U_i \ cap V_j |} {| U_i || V_j |} \]</target>
        </trans-unit>
        <trans-unit id="c94cfec038c55c60afa9fef476e0a1aad2563170" translate="yes" xml:space="preserve">
          <source>\[P(X | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)\]</source>
          <target state="translated">\ [P (X | y = k) = \ frac {1} {(2 \ pi) ^ {d / 2} | \ Sigma_k | ^ {1/2}} \ exp \ left (-\ frac {1} {2} (X- \ mu_k) ^ t \ Sigma_k ^ {-1} (X- \ mu_k) \ 오른쪽) \]</target>
        </trans-unit>
        <trans-unit id="cd893dbc741157abd91341aed411a740724f85fa" translate="yes" xml:space="preserve">
          <source>\[P(\mathbf{v}, \mathbf{h}) = \frac{e^{-E(\mathbf{v}, \mathbf{h})}}{Z}\]</source>
          <target state="translated">\ [P (\ mathbf {v}, \ mathbf {h}) = \ frac {e ^ {-E (\ mathbf {v}, \ mathbf {h})}} {Z} \]</target>
        </trans-unit>
        <trans-unit id="7e8396a93e6bd3272f4718e7de218023882d7d31" translate="yes" xml:space="preserve">
          <source>\[P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)\]</source>
          <target state="translated">\ [P (x | y = k) = \ frac {1} {(2 \ pi) ^ {d / 2} | \ Sigma_k | ^ {1/2}} \ exp \ left (-\ frac {1} {2} (x- \ mu_k) ^ t \ Sigma_k ^ {-1} (x- \ mu_k) \ 오른쪽) \]</target>
        </trans-unit>
        <trans-unit id="f84acbf7827b429f5edf701fe90ef79259a19956" translate="yes" xml:space="preserve">
          <source>\[P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} + \alpha n_i},\]</source>
          <target state="translated">\[P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} + \alpha n_i},\]</target>
        </trans-unit>
        <trans-unit id="0a8ab66c6ea03a89651facccbd08d5f0c9be8c6d" translate="yes" xml:space="preserve">
          <source>\[P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)\]</source>
          <target state="translated">\ [P (x_i \ mid y) = P (i \ mid y) x_i + (1-P (i \ mid y)) (1-x_i) \]</target>
        </trans-unit>
        <trans-unit id="33b093ce0e5d6e386417ef609cc2f6a855343cc3" translate="yes" xml:space="preserve">
          <source>\[P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)\]</source>
          <target state="translated">\ [P (x_i \ mid y) = \ frac {1} {\ sqrt {2 \ pi \ sigma ^ 2_y}} \ exp \ left (-\ frac {(x_i-\ mu_y) ^ 2} {2 \ sigma ^ 2_y} \ 오른쪽) \]</target>
        </trans-unit>
        <trans-unit id="1f013d2ae1dd46efa06019f68c7849dc72ce7f4e" translate="yes" xml:space="preserve">
          <source>\[P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),\]</source>
          <target state="translated">\ [P (x_i | y, x_1, \ dots, x_ {i-1}, x_ {i + 1}, \ dots, x_n) = P (x_i | y), \]</target>
        </trans-unit>
        <trans-unit id="c732ad1752bec3f2371482df4bfec3ce784d1ec4" translate="yes" xml:space="preserve">
          <source>\[P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)} {P(x_1, \dots, x_n)}\]</source>
          <target state="translated">\ [P (y \ mid x_1, \ dots, x_n) = \ frac {P (y) P (x_1, \ dots x_n \ mid y)} {P (x_1, \ dots, x_n)} \]</target>
        </trans-unit>
        <trans-unit id="facd455c5147b9177420e6f465438d2ec052942b" translate="yes" xml:space="preserve">
          <source>\[P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)} {P(x_1, \dots, x_n)}\]</source>
          <target state="translated">\ [P (y \ 중간 x_1, \ 도트, x_n) = \ frac {P (y) P (x_1, \ 도트, x_n \ 중간 y)} {P (x_1, \ 도트, x_n)} \]</target>
        </trans-unit>
        <trans-unit id="84c8d40000ffaabc15011af6c72fc5efaa03e40c" translate="yes" xml:space="preserve">
          <source>\[P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)} {P(x_1, \dots, x_n)}\]</source>
          <target state="translated">\ [P (y \ mid x_1, \ dots, x_n) = \ frac {P (y) \ prod_ {i = 1} ^ {n} P (x_i \ mid y)} {P (x_1, \ dots, x_n )} \]</target>
        </trans-unit>
        <trans-unit id="edcd72a09735a7f6adceea22c0b30c20fbfbd80a" translate="yes" xml:space="preserve">
          <source>\[P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} = \frac{P(X | y=k) P(y = k)}{ \sum_{l} P(X | y=l) \cdot P(y=l)}\]</source>
          <target state="translated">\ [P (y = k | X) = \ frac {P (X | y = k) P (y = k)} {P (X)} = \ frac {P (X | y = k) P (y = k)} {\ sum_ {l} P (X | y = l) \ cdot P (y = l)} \]</target>
        </trans-unit>
        <trans-unit id="d0d051ebe8e5f14c78017b6beb24cfde6b45cc0c" translate="yes" xml:space="preserve">
          <source>\[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]</source>
          <target state="translated">\[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]</target>
        </trans-unit>
        <trans-unit id="b5a07827e49a88167851d8970eaa9edda3b99d65" translate="yes" xml:space="preserve">
          <source>\[R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^{n_{\text{samples}} - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n_\text{samples} - 1} (y_i - \bar{y})^2}\]</source>
          <target state="translated">\ [R ^ 2 (y, \ hat {y}) = 1-\ frac {\ sum_ {i = 0} ^ {n _ {\ text {samples}}-1} (y_i-\ hat {y} _i) ^ 2} {\ sum_ {i = 0} ^ {n_ \ text {samples}-1} (y_i-\ bar {y}) ^ 2} \]</target>
        </trans-unit>
        <trans-unit id="75a581e81c894ca3d5a39bbe361df977932542f2" translate="yes" xml:space="preserve">
          <source>\[R^2(y, \hat{y}) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\]</source>
          <target state="translated">\[R^2(y, \hat{y}) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\]</target>
        </trans-unit>
        <trans-unit id="e9b98e0db2ea75d1dfbeda81f631985feb91e53a" translate="yes" xml:space="preserve">
          <source>\[R_\alpha(T) = R(T) + \alpha|T|\]</source>
          <target state="translated">\[R_\alpha(T) = R(T) + \alpha|T|\]</target>
        </trans-unit>
        <trans-unit id="c4a8061b9eb9628b02ec75d4e5fc5a21d09cd880" translate="yes" xml:space="preserve">
          <source>\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]</source>
          <target state="translated">\ [R_ {ij} = \ frac {s_i + s_j} {d_ {ij}} \]</target>
        </trans-unit>
        <trans-unit id="a0fa8b8fb90971dc259d470e1011abda5608da98" translate="yes" xml:space="preserve">
          <source>\[T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1} \sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k))\]</source>
          <target state="translated">\[T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1} \sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k))\]</target>
        </trans-unit>
        <trans-unit id="7faa52dada966b566ea53656a8bd94425e2b50a6" translate="yes" xml:space="preserve">
          <source>\[W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}\]</source>
          <target state="translated">\ [W ^ {i + 1} = W ^ i-\ epsilon \ nabla {손실} _ {W} ^ {i} \]</target>
        </trans-unit>
        <trans-unit id="e598814b9bd2e3db200ccff196ebf646bf738ec7" translate="yes" xml:space="preserve">
          <source>\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]</source>
          <target state="translated">\ [W_k = \ sum_ {q = 1} ^ k \ sum_ {x \ in C_q} (x-c_q) (x-c_q) ^ T \]</target>
        </trans-unit>
        <trans-unit id="7eb30be4ba7f05878cf003c46dfa54b35d57c946" translate="yes" xml:space="preserve">
          <source>\[X \approx X_k = U_k \Sigma_k V_k^\top\]</source>
          <target state="translated">\ [X \ 약 X_k = U_k \ Sigma_k V_k ^ \ top \]</target>
        </trans-unit>
        <trans-unit id="c7c03179d6deebc1c581ff68076935c59051f655" translate="yes" xml:space="preserve">
          <source>\[X' = X V_k\]</source>
          <target state="translated">\ [X '= X V_k \]</target>
        </trans-unit>
        <trans-unit id="1b12fce875e4a7610479762a7e27b3b69634b6af" translate="yes" xml:space="preserve">
          <source>\[X^* = D^{-1/2}U^t X\text{ with }\Sigma = UDU^t\]</source>
          <target state="translated">\ [X ^ * = D ^ {-1/2} U ^ t X \ text {with} \ Sigma = UDU ^ t \]</target>
        </trans-unit>
        <trans-unit id="15463a9b27ce80c407246ad66ee390a5bc003068" translate="yes" xml:space="preserve">
          <source>\[\alpha \rho ||W||_1 + \alpha \rho ||H||_1 + \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2 + \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\]</source>
          <target state="translated">\ [\ alpha \ rho || W || _1 + \ alpha \ rho || H || _1 + \ frac {\ alpha (1- \ rho)} {2} || W || _ {\ mathrm {Fro }} ^ 2 + \ fra {{alpha (1- \ rho)} {2} || H || _ {\ mathrm {Fro}} ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="ecb4b9fe8fc24f73a20eeac8e7781ad331ab3cc6" translate="yes" xml:space="preserve">
          <source>\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp;amp; \frac{1}{2} ||X-UV||_2^2+\alpha||U||_1 \\ \text{subject to } &amp;amp; ||V_k||_2 = 1 \text{ for all } 0 \leq k &amp;lt; n_{\mathrm{atoms}}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} (U ^ *, V ^ *) = \ underset {U, V} {\ operatorname {arg \, min \,}} &amp;amp; \ frac {1} {2} || X-UV || _2 ^ 2 + \ alpha || U || _1 \\ \ text {}에 따라} &amp;amp; || V_k || _2 = 1 \ text {}} \ leq k &amp;lt;n _ {\ mathrm {atoms}} \ end {분할} \]</target>
        </trans-unit>
        <trans-unit id="113192b0908b5ad7f78568d56440ab2d30ef92e9" translate="yes" xml:space="preserve">
          <source>\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp;amp; \frac{1}{2} ||X-UV||_2^2+\alpha||V||_1 \\ \text{subject to } &amp;amp; ||U_k||_2 = 1 \text{ for all } 0 \leq k &amp;lt; n_{components}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} (U ^ *, V ^ *) = \ underset {U, V} {\ operatorname {arg \, min \,}} &amp;amp; \ frac {1} {2} || X-UV || _2 ^ 2 + \ alpha || V || _1 \\ \ text {}에 따라} &amp;amp; || U_k || _2 = 1 \ text {모두} 0 \ leq k &amp;lt;n_ {components} \ end {split } \]</target>
        </trans-unit>
        <trans-unit id="3136579a6b2a85a0be46315eb9cbc3ba1e261551" translate="yes" xml:space="preserve">
          <source>\[\begin{split}H_{\epsilon}(z) = \begin{cases} z^2, &amp;amp; \text {if } |z| &amp;lt; \epsilon, \\ 2\epsilon|z| - \epsilon^2, &amp;amp; \text{otherwise} \end{cases}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} H _ {\ epsilon} (z) = \ begin {cases} z ^ 2 및 \ text {if} | z | &amp;lt;\ epsilon, \\ 2 \ epsilon | z | -\ epsilon ^ 2 및 \ text {otherwise} \ end {cases} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="b46cf81ba2e913dc611a985b07c8255c093eef0a" translate="yes" xml:space="preserve">
          <source>\[\begin{split}P(v_i=1|\mathbf{h}) = \sigma(\sum_j w_{ij}h_j + b_i) \\ P(h_i=1|\mathbf{v}) = \sigma(\sum_i w_{ij}v_i + c_j)\end{split}\]</source>
          <target state="translated">\ [\ begin {split} P (v_i = 1 | \ mathbf {h}) = \ sigma (\ sum_j w_ {ij} h_j + b_i) \\ P (h_i = 1 | \ mathbf {v}) = \ sigma (\ sum_i w_ {ij} v_i + c_j) \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="2a61de83e06a531ec76671fed3f61a04aabc9bf7" translate="yes" xml:space="preserve">
          <source>\[\begin{split}Z = \begin{bmatrix} R^{-1/2} U \\\\ C^{-1/2} V \end{bmatrix}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} Z = \ begin {bmatrix} R ^ {-1/2} U \\\\ C ^ {-1/2} V \ end {bmatrix} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="8fd5d1a7323dcc269879eaa3da3604856f3eb3a0" translate="yes" xml:space="preserve">
          <source>\[\begin{split}\left\{ \begin{array}{c c l} -\sqrt{\frac{s}{n_{\text{components}}}} &amp;amp; &amp;amp; 1 / 2s\\ 0 &amp;amp;\text{with probability} &amp;amp; 1 - 1 / s \\ +\sqrt{\frac{s}{n_{\text{components}}}} &amp;amp; &amp;amp; 1 / 2s\\ \end{array} \right.\end{split}\]</source>
          <target state="translated">\ [\ begin {split} \ left \ {\ begin {array} {ccl}-\ sqrt {\ frac {s} {n _ {\ text {components}}}} &amp;amp; &amp;amp; 1 / 2s \\ 0 &amp;amp; \ text {확률} &amp;amp; 1-1 / s \\ + \ sqrt {\ frac {s} {n _ {\ text {components}}}} &amp;amp; &amp;amp; 1 / 2s \\ \ end {array} \ right. \ end { 스플릿}\]</target>
        </trans-unit>
        <trans-unit id="240e6564ced61b1bc331dcf97a462834371b6641" translate="yes" xml:space="preserve">
          <source>\[\begin{split}\log P(y=k | x) &amp;amp;= \log P(x | y=k) + \log P(y = k) + Cst \\ &amp;amp;= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,\end{split}\]</source>
          <target state="translated">\[\begin{split}\log P(y=k | x) &amp;amp;= \log P(x | y=k) + \log P(y = k) + Cst \\ &amp;amp;= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,\end{split}\]</target>
        </trans-unit>
        <trans-unit id="cce5812b4c3daf727144c68d463e8caf50c7bfbb" translate="yes" xml:space="preserve">
          <source>\[\begin{split}\text{D}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} \begin{cases} (y_i-\hat{y}_i)^2, &amp;amp; \text{for }p=0\text{ (Normal)}\\ 2(y_i \log(y/\hat{y}_i) + \hat{y}_i - y_i), &amp;amp; \text{for}p=1\text{ (Poisson)}\\ 2(\log(\hat{y}_i/y_i) + y_i/\hat{y}_i - 1), &amp;amp; \text{for}p=2\text{ (Gamma)}\\ 2\left(\frac{\max(y_i,0)^{2-p}}{(1-p)(2-p)}- \frac{y\,\hat{y}^{1-p}_i}{1-p}+\frac{\hat{y}^{2-p}_i}{2-p}\right), &amp;amp; \text{otherwise} \end{cases}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} \ text {D} (y, \ hat {y}) = \ frac {1} {n_ \ text {samples}} \ sum_ {i = 0} ^ {n_ \ text {samples} -1} \ begin {cases} (y_i- \ hat {y} _i) ^ 2, &amp;amp; \ text {for} p = 0 \ text {(일반)} \\ 2 (y_i \ log (y / \ hat { y} _i) + \ hat {y} _i-y_i), &amp;amp; \ text {for} p = 1 \ text {(푸 아송)} \\ 2 (\ log (\ hat {y} _i / y_i) + y_i / \ hat {y} _i-1), &amp;amp; \ text {for} p = 2 \ text {(감마)} \\ 2 \ left (\ frac {\ max (y_i, 0) ^ {2-p}} { (1-p) (2-p)}-\ frac {y \, \ hat {y} ^ {1-p} _i} {1-p} + \ frac {\ hat {y} ^ {2-p } _i} {2-p} \ right) 및 \ text {otherwise} \ end {cases} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="64d0ec223c44ca16fb23a31d32e7c757a5e38ba8" translate="yes" xml:space="preserve">
          <source>\[\begin{split}h_i \bot h_j | \mathbf{v} \\ v_i \bot v_j | \mathbf{h}\end{split}\]</source>
          <target state="translated">\ [\ 시작 {split} h_i \ bot h_j | \ mathbf {v} \\ v_i \ bot v_j | \ mathbf {h} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="3a84b3b0085920a5dafc19e08421419f997fff14" translate="yes" xml:space="preserve">
          <source>\[\begin{split}pd_{X_S}(x_S) &amp;amp;\overset{def}{=} \mathbb{E}_{X_C}\left[ f(x_S, X_C) \right]\\ &amp;amp;= \int f(x_S, x_C) p(x_C) dx_C,\end{split}\]</source>
          <target state="translated">\[\begin{split}pd_{X_S}(x_S) &amp;amp;\overset{def}{=} \mathbb{E}_{X_C}\left[ f(x_S, X_C) \right]\\ &amp;amp;= \int f(x_S, x_C) p(x_C) dx_C,\end{split}\]</target>
        </trans-unit>
        <trans-unit id="6675ac3debfec98638ba1f9b136883988a6a0c0b" translate="yes" xml:space="preserve">
          <source>\[\begin{split}x_i^{(\lambda)} = \begin{cases} [(x_i + 1)^\lambda - 1] / \lambda &amp;amp; \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt] \ln{(x_i + 1)} &amp;amp; \text{if } \lambda = 0, x_i \geq 0 \\[8pt] -[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) &amp;amp; \text{if } \lambda \neq 2, x_i &amp;lt; 0, \\[8pt] - \ln (- x_i + 1) &amp;amp; \text{if } \lambda = 2, x_i &amp;lt; 0 \end{cases}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} x_i ^ {(\ lambda)} = \ begin {cases} [(x_i + 1) ^ \ lambda-1] / \ lambda &amp;amp; \ text {if} \ lambda \ neq 0, x_i \ geq 0, \\ [8pt] \ ln {(x_i + 1)} &amp;amp; \ text {if} \ lambda = 0, x_i \ geq 0 \\ [8pt]-[(-x_i + 1) ^ {2-\ 람다}-1] / (2-\ lambda) &amp;amp; \ text {if} \ lambda \ neq 2, x_i &amp;lt;0, \\ [8pt]-\ ln (-x_i + 1) &amp;amp; \ text {if} \ lambda = 2, x_i &amp;lt;0 \ end {cases} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="3198bb38407bb1554dc0d1959561349fcfcad349" translate="yes" xml:space="preserve">
          <source>\[\begin{split}x_i^{(\lambda)} = \begin{cases} [(x_i + 1)^\lambda - 1] / \lambda &amp;amp; \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt] \ln{(x_i) + 1} &amp;amp; \text{if } \lambda = 0, x_i \geq 0 \\[8pt] -[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) &amp;amp; \text{if } \lambda \neq 2, x_i &amp;lt; 0, \\[8pt] - \ln (- x_i + 1) &amp;amp; \text{if } \lambda = 2, x_i &amp;lt; 0 \end{cases}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} x_i ^ {(\ lambda)} = \ begin {cases} [(x_i + 1) ^ \ lambda-1] / \ lambda &amp;amp; \ text {if} \ lambda \ neq 0, x_i \ geq 0, \\ [8pt] \ ln {(x_i) + 1} &amp;amp; \ text {if} \ lambda = 0, x_i \ geq 0 \\ [8pt]-[(-x_i + 1) ^ {2-\ 람다}-1] / (2-\ lambda) &amp;amp; \ text {if} \ lambda \ neq 2, x_i &amp;lt;0, \\ [8pt]-\ ln (-x_i + 1) &amp;amp; \ text {if} \ lambda = 2, x_i &amp;lt;0 \ end {cases} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="dc87ecfd4801e13673bdd8bb567f720368f1d00c" translate="yes" xml:space="preserve">
          <source>\[\begin{split}x_i^{(\lambda)} = \begin{cases} \dfrac{x_i^\lambda - 1}{\lambda} &amp;amp; \text{if } \lambda \neq 0, \\[8pt] \ln{(x_i)} &amp;amp; \text{if } \lambda = 0, \end{cases}\end{split}\]</source>
          <target state="translated">\ [\ begin {split} x_i ^ {(\ lambda)} = \ begin {cases} \ dfrac {x_i ^ \ lambda-1} {\ lambda} &amp;amp; \ text {if} \ lambda \ neq 0, \\ [ 8pt] \ ln {(x_i)} &amp;amp; \ text {if} \ lambda = 0, \ end {cases} \ end {split} \]</target>
        </trans-unit>
        <trans-unit id="4537e01f403cf1db6a1704e87011ad05c2900083" translate="yes" xml:space="preserve">
          <source>\[\binom{n_{\text{samples}}}{n_{\text{subsamples}}}\]</source>
          <target state="translated">\[\binom{n_{\text{samples}}}{n_{\text{subsamples}}}\]</target>
        </trans-unit>
        <trans-unit id="a73d653ad8356da32b64d5107d59910b4574da8f" translate="yes" xml:space="preserve">
          <source>\[\binom{n_{samples}}{n_{subsamples}}\]</source>
          <target state="translated">\[\binom{n_{samples}}{n_{subsamples}}\]</target>
        </trans-unit>
        <trans-unit id="ac1ff82d2bb9be7a891279ff19e58ef9606ac1e2" translate="yes" xml:space="preserve">
          <source>\[\eta^{(t)} = \frac {1}{\alpha (t_0 + t)}\]</source>
          <target state="translated">\ [\ eta ^ {(t)} = \ frac {1} {\ alpha (t_0 + t)} \]</target>
        </trans-unit>
        <trans-unit id="94fa32af9a54520431e5e0db6110f5f4be3adb89" translate="yes" xml:space="preserve">
          <source>\[\eta^{(t)} = \frac{eta_0}{t^{power\_t}}\]</source>
          <target state="translated">\ [\ eta ^ {(t)} = \ frac {eta_0} {t ^ {power \ _t}} \]</target>
        </trans-unit>
        <trans-unit id="5914d3325cfa1135b23fa0bc1a30756771d61bd1" translate="yes" xml:space="preserve">
          <source>\[\frac{2}{c(c-1)}\sum_{j=1}^{c}\sum_{k &amp;gt; j}^c (\text{AUC}(j | k) + \text{AUC}(k | j))\]</source>
          <target state="translated">\[\frac{2}{c(c-1)}\sum_{j=1}^{c}\sum_{k &amp;gt; j}^c (\text{AUC}(j | k) + \text{AUC}(k | j))\]</target>
        </trans-unit>
        <trans-unit id="23f7d8421e9f7d86dc62315499303963443744dd" translate="yes" xml:space="preserve">
          <source>\[\frac{2}{c(c-1)}\sum_{j=1}^{c}\sum_{k &amp;gt; j}^c p(j \cup k)( \text{AUC}(j | k) + \text{AUC}(k | j))\]</source>
          <target state="translated">\[\frac{2}{c(c-1)}\sum_{j=1}^{c}\sum_{k &amp;gt; j}^c p(j \cup k)( \text{AUC}(j | k) + \text{AUC}(k | j))\]</target>
        </trans-unit>
        <trans-unit id="0ab74fa9a47831af7bc26708f2eb4bb36eb7c478" translate="yes" xml:space="preserve">
          <source>\[\gamma_m = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) - \gamma \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)})\]</source>
          <target state="translated">\ [\ gamma_m = \ arg \ min _ {\ gamma} \ sum_ {i = 1} ^ {n} L (y_i, F_ {m-1} (x_i)-\ gamma \ frac {\ partial L (y_i, F_ {m-1} (x_i))} {\ 부분 F_ {m-1} (x_i)}) \]</target>
        </trans-unit>
        <trans-unit id="03488d7aa371e338e3a792b1ee0314e7750d1c23" translate="yes" xml:space="preserve">
          <source>\[\hat{K} = \mathrm{argmin}_K \big( \mathrm{tr} S K - \mathrm{log} \mathrm{det} K + \alpha \|K\|_1 \big)\]</source>
          <target state="translated">\ [\ hat {K} = \ mathrm {argmin} _K \ big (\ mathrm {tr} SK-\ mathrm {log} \ mathrm {det} K + \ alpha \ | K \ | _1 \ big) \]</target>
        </trans-unit>
        <trans-unit id="9d564db2d54a230e005c20037b5a37d139fd79dd" translate="yes" xml:space="preserve">
          <source>\[\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}\]</source>
          <target state="translated">\ [\ hat {\ theta} _ {yi} = \ frac {N_ {yi} + \ alpha} {N_y + \ alpha n} \]</target>
        </trans-unit>
        <trans-unit id="c11c6e06d183bd2451c8cb2b3112bec0b1bc1ee8" translate="yes" xml:space="preserve">
          <source>\[\hat{c} = \arg\min_c \sum_{i} t_i w_{ci}\]</source>
          <target state="translated">\ [\ hat {c} = \ arg \ min_c \ sum_ {i} t_i w_ {ci} \]</target>
        </trans-unit>
        <trans-unit id="0d0878697ede61cc2d8e8a17d350fedc4b3570ca" translate="yes" xml:space="preserve">
          <source>\[\hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}\]</source>
          <target state="translated">\ [\ hat {w} _i = \ frac {w_i} {\ sum_j {1 (y_j = y_i) w_j}} \]</target>
        </trans-unit>
        <trans-unit id="2a9f64bd46b89b26f87a73f822e103df8215fc20" translate="yes" xml:space="preserve">
          <source>\[\hat{y_i} = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]</source>
          <target state="translated">\[\hat{y_i} = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]</target>
        </trans-unit>
        <trans-unit id="c4cf751319dcee1f0c7d13ac626ea9ed7a5bb6a9" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, X) = h(Xw).\]</source>
          <target state="translated">\[\hat{y}(w, X) = h(Xw).\]</target>
        </trans-unit>
        <trans-unit id="463125ace329bbcbe2634beaad2a87856ea8f696" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\]</source>
          <target state="translated">\ [\ hat {y} (w, x) = w_0 + w_1 x_1 + ... + w_p x_p \]</target>
        </trans-unit>
        <trans-unit id="d7104cc52e967d53cc9bb7db2f89101bc8dd84a8" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\]</source>
          <target state="translated">\ [\ hat {y} (w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1 ^ 2 + w_5 x_2 ^ 2 \]</target>
        </trans-unit>
        <trans-unit id="658832cb765efda37a6b6c541d565e8e7abeaf9e" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\]</source>
          <target state="translated">\ [\ hat {y} (w, x) = w_0 + w_1 x_1 + w_2 x_2 \]</target>
        </trans-unit>
        <trans-unit id="e16dea6416d3f9e94f0e5c1b50c7914fc96699e9" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]</source>
          <target state="translated">\ [\ hat {y} (w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5 \]</target>
        </trans-unit>
        <trans-unit id="1ef6664dd57ac85f53b0ac082bd4ac87b3d1bc48" translate="yes" xml:space="preserve">
          <source>\[\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]</source>
          <target state="translated">\[\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]</target>
        </trans-unit>
        <trans-unit id="7ee9d5e754305261d56bc754281eac2d4bc47e43" translate="yes" xml:space="preserve">
          <source>\[\kappa = (p_o - p_e) / (1 - p_e)\]</source>
          <target state="translated">\ [\ kappa = (p_o-p_e) / (1-p_e) \]</target>
        </trans-unit>
        <trans-unit id="167a98e04ab5b59774a90feac289ea3a97af6579" translate="yes" xml:space="preserve">
          <source>\[\log P(v) = \log \sum_h e^{-E(v, h)} - \log \sum_{x, y} e^{-E(x, y)}\]</source>
          <target state="translated">\ [\ log P (v) = \ log \ sum_h e ^ {-E (v, h)}-\ log \ sum_ {x, y} e ^ {-E (x, y)} \]</target>
        </trans-unit>
        <trans-unit id="3615a8591ff789e7148f8d4eaedd232d6f832187" translate="yes" xml:space="preserve">
          <source>\[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.\]</source>
          <target state="translated">\[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.\]</target>
        </trans-unit>
        <trans-unit id="a818beecd93bff11790e50acc933a2d8090bf837" translate="yes" xml:space="preserve">
          <source>\[\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.\]</source>
          <target state="translated">\[\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.\]</target>
        </trans-unit>
        <trans-unit id="ad74f3245329845209a6a11327e3e01ab638c922" translate="yes" xml:space="preserve">
          <source>\[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=} E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\]</source>
          <target state="translated">\ [\ log \ : P (w | \ alpha, \ eta) \ geq L (w, \ phi, \ gamma, \ lambda) \ overset {\ triangle} {=} E_ {q} [\ log \ : p (w, z, \ theta, \ beta | \ alpha, \ eta)]-E_ {q} [\ log \ : q (z, \ theta, \ beta)] \]</target>
        </trans-unit>
        <trans-unit id="acd2ba041ece952cccd5e4312456e983a53c6177" translate="yes" xml:space="preserve">
          <source>\[\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}\]</source>
          <target state="translated">\ [\ mathbf {X} = W \ mathbf {H} + \ mathbf {M} + \ mathbf {E} \]</target>
        </trans-unit>
        <trans-unit id="3fc441ca9a3b802fffa9f5232b4a01983592e3f6" translate="yes" xml:space="preserve">
          <source>\[\mathrm{Var}[X] = p(1 - p)\]</source>
          <target state="translated">\ [\ mathrm {Var} [X] = p (1-p) \]</target>
        </trans-unit>
        <trans-unit id="115415b8df2e0037567cec45ba374bf6acae476a" translate="yes" xml:space="preserve">
          <source>\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, y_i (w^T \phi(x_i) + b)),\]</source>
          <target state="translated">\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, y_i (w^T \phi(x_i) + b)),\]</target>
        </trans-unit>
        <trans-unit id="ff67bbd323e7ac7f1f2a65cc0812c3d22a62f042" translate="yes" xml:space="preserve">
          <source>\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),\]</source>
          <target state="translated">\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),\]</target>
        </trans-unit>
        <trans-unit id="1fa465ae3dc6f6cd2a645126f7fc3c5985939961" translate="yes" xml:space="preserve">
          <source>\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}}^2 + \alpha \rho ||W||_{2 1} + \frac{\alpha(1-\rho)}{2} ||W||_{\text{Fro}}^2}\]</source>
          <target state="translated">\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}}^2 + \alpha \rho ||W||_{2 1} + \frac{\alpha(1-\rho)}{2} ||W||_{\text{Fro}}^2}\]</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
