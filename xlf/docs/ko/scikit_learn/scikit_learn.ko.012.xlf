<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="b70db829e86f8b0b87ee4b4ea9165e2800cb135e" translate="yes" xml:space="preserve">
          <source>In particular the interrogative form &amp;ldquo;Is this&amp;rdquo; is only present in the last document:</source>
          <target state="translated">특히&amp;ldquo;이것입니까?&amp;rdquo;라는 의문 양식은 마지막 문서에만 있습니다.</target>
        </trans-unit>
        <trans-unit id="48a72aaef5f57348c3c02ddfbd84f34663c56133" translate="yes" xml:space="preserve">
          <source>In particular we name:</source>
          <target state="translated">특히 우리는</target>
        </trans-unit>
        <trans-unit id="32bae48b70c29501503578b88b61dfab45b0637c" translate="yes" xml:space="preserve">
          <source>In particular, \(\nu = 3/2\):</source>
          <target state="translated">특히 \ (\ nu = 3/2 \) :</target>
        </trans-unit>
        <trans-unit id="ff34c019cd4067dd0b8a1a6d1536db31ff351b58" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="translated">특히 잘린 SVD는 sklearn.feature_extraction.text의 벡터 라이저가 반환하는 count / tf-idf 행렬에 대해 작동합니다. 이러한 맥락에서,이를 잠재적 의미 분석 (LSA)이라고합니다.</target>
        </trans-unit>
        <trans-unit id="74d856933005d819af2a4b7d2ce24317b5186453" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.</source>
          <target state="translated">실제로 Spectral Clustering은 개별 클러스터의 구조가 볼록하지 않거나 볼록한 경우 또는 클러스터의 중심과 확산이 전체 클러스터에 대한 적절한 설명이 아닐 때 매우 유용합니다. 예를 들어, 클러스터가 2D 계획에 중첩 된 원일 경우.</target>
        </trans-unit>
        <trans-unit id="5662b7bb73c6d21ae298513382716bd3fe526ba2" translate="yes" xml:space="preserve">
          <source>In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.</source>
          <target state="translated">실제로, 국부 밀도는 k- 최근 접 이웃으로부터 얻어진다. 관측치의 LOF 점수는 k- 최근 접 이웃의 평균 국부 밀도와 자체 국부 밀도의 비율과 동일합니다. 정상 인스턴스는 이웃의 것과 유사한 국부 밀도를 가질 것으로 예상되지만 비정상 데이터는 국부 밀도가 훨씬 더 작을 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="7ab811a62d63edef5c9695fca6cafd8cc266404c" translate="yes" xml:space="preserve">
          <source>In practice those estimates are stored as an attribute named &lt;code&gt;feature_importances_&lt;/code&gt; on the fitted model. This is an array with shape &lt;code&gt;(n_features,)&lt;/code&gt; whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</source>
          <target state="translated">실제로 이러한 추정치는 적합 모형에 &lt;code&gt;feature_importances_&lt;/code&gt; 라는 속성으로 저장됩니다 . 값이 양수이고 합계가 1.0 인 형태 &lt;code&gt;(n_features,)&lt;/code&gt; 의 배열입니다 . 값이 클수록 예측 기능에 대한 일치 기능의 기여가 더 중요합니다.</target>
        </trans-unit>
        <trans-unit id="2f91c327e74b19930bc0b8d2d9c2f5d99fe44af7" translate="yes" xml:space="preserve">
          <source>In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.</source>
          <target state="translated">실제로 우리는 종종 분포의 형태를 무시하고 각 피쳐의 평균값을 제거하여 데이터를 중앙에 위치하도록 변환 한 다음 일정하지 않은 피쳐를 표준 편차로 나누어 스케일을 조정합니다.</target>
        </trans-unit>
        <trans-unit id="4c632dd9d37d8e850afe2fbbbdbddfedb108d119" translate="yes" xml:space="preserve">
          <source>In practice, \(\mu\) and \(\Sigma\) are replaced by some estimates. The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set and therefor, the corresponding Mahalanobis distances are. One would better have to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set and that the associated Mahalanobis distances accurately reflect the true organisation of the observations.</source>
          <target state="translated">실제로 \ (\ mu \) 및 \ (\ Sigma \)는 몇 가지 추정치로 대체됩니다. 일반적인 공분산 최대 우도 추정치는 데이터 세트에 특이 치의 존재에 매우 민감하므로 해당 Mahalanobis 거리가 있습니다. 추정값이 데이터 세트의 &quot;오차적인&quot;관측치에 저항성이 있고 관련 Mahalanobis 거리가 관측치의 실제 구성을 정확하게 반영하도록 보장하기 위해 강력한 공분산 추정기를 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="7f19bfe5f66f3783151b8147191d95599d9b587d" translate="yes" xml:space="preserve">
          <source>In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That&amp;rsquo;s why it can be useful to restart it several times.</source>
          <target state="translated">실제로 k- 평균 알고리즘은 매우 빠르지 만 (사용 가능한 가장 빠른 클러스터링 알고리즘 중 하나) 로컬 최소값에 해당합니다. 따라서 여러 번 다시 시작하는 것이 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a9809378b0338436bd7bbe8f2e2070a6272b570d" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples keywords &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="translated">특정 클래스 또는 특정 개별 샘플에 더 중요한 것을 요구하는 문제에서 키워드 &lt;code&gt;class_weight&lt;/code&gt; 및 &lt;code&gt;sample_weight&lt;/code&gt; 를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cae7f41bfdb577edc832687cbceee9865d3220c5" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.</source>
          <target state="translated">랜덤 포레스트 ( &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt; 클래스 참조 )에서 앙상블의 각 트리는 트레이닝 세트에서 대체 (즉, 부트 스트랩 샘플)로 그린 샘플에서 빌드됩니다. 또한 트리를 구성하는 동안 노드를 분할 할 때 선택한 분할이 더 이상 모든 기능 중에서 가장 잘 분할되지 않습니다. 대신 선택되는 분할이 기능의 임의의 하위 집합 중에서 가장 잘 분할됩니다. 이 무작위성의 결과로, 숲의 편견은 보통 (비 무작위 트리의 편견과 관련하여) 약간 증가하지만 평균화로 인해 편차가 감소합니다. 보통 편향의 증가를 보상하는 것 이상으로, 따라서 전체적으로 더 나은 모델을 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eee63fdeeb00f1867cdf7e3f336a4276e1d12ae2" translate="yes" xml:space="preserve">
          <source>In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.</source>
          <target state="translated">회귀 분석에서 추정기의 예상 평균 제곱 오차는 치우침, 분산 및 노이즈 측면에서 분해 될 수 있습니다. 회귀 문제의 평균 데이터 집합에 대해 편향 항은 추정기의 예측이 문제에 대한 가능한 최상의 추정기의 예측과 다른 평균 량을 측정합니다 (예 : 베이 즈 모델). 분산 항은 문제의 다른 사례 LS에 적합 할 때 추정기 예측의 변동성을 측정합니다. 마지막으로 노이즈는 데이터의 변동성으로 인한 오차의 돌이킬 수없는 부분을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="3eae88b0a075df5d4090eee16e911849fedcc7b3" translate="yes" xml:space="preserve">
          <source>In regression, the output remains as \(f(x)\); therefore, output activation function is just the identity function.</source>
          <target state="translated">회귀에서 출력은 \ (f (x) \)로 유지됩니다. 따라서 출력 활성화 기능은 ID 기능 일뿐입니다.</target>
        </trans-unit>
        <trans-unit id="253be6f032627aec5a3b2c4240659e2190b9fba2" translate="yes" xml:space="preserve">
          <source>In scikit-learn a random split into training and test sets can be quickly computed with the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; helper function. Let&amp;rsquo;s load the iris data set to fit a linear support vector machine on it:</source>
          <target state="translated">scikit-learn에서는 &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt; 도우미 함수를 사용하여 훈련 및 테스트 세트로 무작위 분할을 신속하게 계산할 수 있습니다 . 선형 서포트 벡터 머신에 맞게 홍채 데이터 세트를로드합시다 :</target>
        </trans-unit>
        <trans-unit id="bdcdb5bf0b220e10633a04e3b3d7b23fb832fcf9" translate="yes" xml:space="preserve">
          <source>In scikit-learn, an estimator for classification is a Python object that implements the methods &lt;code&gt;fit(X, y)&lt;/code&gt; and &lt;code&gt;predict(T)&lt;/code&gt;.</source>
          <target state="translated">scikit-learn에서 분류 추정기는 &lt;code&gt;fit(X, y)&lt;/code&gt; 및 &lt;code&gt;predict(T)&lt;/code&gt; 메소드를 구현하는 Python 객체입니다 .</target>
        </trans-unit>
        <trans-unit id="a15700735758e7e1606b13cd4f276e5fe1e0ef96" translate="yes" xml:space="preserve">
          <source>In scikit-learn, bagging methods are offered as a unified &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt;&lt;code&gt;BaggingClassifier&lt;/code&gt;&lt;/a&gt; meta-estimator (resp. &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt;&lt;code&gt;BaggingRegressor&lt;/code&gt;&lt;/a&gt;), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, &lt;code&gt;max_samples&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt; control the size of the subsets (in terms of samples and features), while &lt;code&gt;bootstrap&lt;/code&gt; and &lt;code&gt;bootstrap_features&lt;/code&gt; control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting &lt;code&gt;oob_score=True&lt;/code&gt;. As an example, the snippet below illustrates how to instantiate a bagging ensemble of &lt;code&gt;KNeighborsClassifier&lt;/code&gt; base estimators, each built on random subsets of 50% of the samples and 50% of the features.</source>
          <target state="translated">scikit-learn에서 배깅 방법은 통합 된 &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt; &lt;code&gt;BaggingClassifier&lt;/code&gt; &lt;/a&gt; 메타 추정기 ( &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt; &lt;code&gt;BaggingRegressor&lt;/code&gt; &lt;/a&gt; . BaggingRegressor ) 로 제공되며, 임의의 하위 집합을 그리는 전략을 지정하는 매개 변수와 함께 사용자 지정 기준 추정기를 입력으로 사용합니다. 특히 &lt;code&gt;max_samples&lt;/code&gt; 및 &lt;code&gt;max_features&lt;/code&gt; 는 하위 집합의 크기 (샘플 및 기능 측면)를 제어 하는 반면, &lt;code&gt;bootstrap&lt;/code&gt; 및 &lt;code&gt;bootstrap_features&lt;/code&gt; 는 샘플과 기능을 교체 여부와 상관없이 제어합니다. 사용 가능한 샘플의 서브 세트를 사용할 때 일반화 정확도는 설정을 통해 가방 외부 샘플로 추정 할 수 있습니다 &lt;code&gt;oob_score=True&lt;/code&gt; . 예를 들어, 아래 스 니펫은 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 기본 추정기 의 배깅 앙상블을 인스턴스화하는 방법을 보여줍니다 . 각각은 샘플의 50 %와 기능의 50 %의 임의의 하위 집합을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="0848ab34fddbc88dcbfdd395d0519986e8d182eb" translate="yes" xml:space="preserve">
          <source>In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt;&lt;code&gt;shrunk_covariance&lt;/code&gt;&lt;/a&gt; method. Also, a shrunk estimator of the covariance can be fitted to data with a &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt;&lt;code&gt;ShrunkCovariance&lt;/code&gt;&lt;/a&gt; object and its &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt;&lt;code&gt;ShrunkCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Again, results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately.</source>
          <target state="translated">scikit-learn에서이 변형 (사용자 정의 수축 계수 포함)은 &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt; &lt;code&gt;shrunk_covariance&lt;/code&gt; &lt;/a&gt; 방법 을 사용하여 미리 계산 된 공분산에 직접 적용 할 수 있습니다 . 또한 공분산의 축소 추정값을 &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt; &lt;code&gt;ShrunkCovariance&lt;/code&gt; &lt;/a&gt; 객체와 해당 &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt; &lt;code&gt;ShrunkCovariance.fit&lt;/code&gt; &lt;/a&gt; 메서드를 사용 하여 데이터에 피팅 할 수 있습니다 . 다시 결과는 데이터가 중심에 있는지 여부에 따라 달라 &lt;code&gt;assume_centered&lt;/code&gt; 매개 변수를 정확하게 사용하려고 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="77c9e3634b9d256acc307751b68c900b0b833a29" translate="yes" xml:space="preserve">
          <source>In single precision, &lt;code&gt;mean&lt;/code&gt; can be inaccurate:</source>
          <target state="translated">단 정밀도에서는 &lt;code&gt;mean&lt;/code&gt; 이 부정확 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="640c5c337251b299605fe0c1704cd43d50fcda84" translate="yes" xml:space="preserve">
          <source>In some cases it&amp;rsquo;s not necessary to include higher powers of any single feature, but only the so-called &lt;em&gt;interaction features&lt;/em&gt; that multiply together at most \(d\) distinct features. These can be gotten from &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;.</source>
          <target state="translated">경우에 따라 단일 기능의 강력한 기능을 모두 포함 할 필요는 없지만 최대 \ (d \) 개의 고유 한 기능을 함께 곱하는 소위 &lt;em&gt;상호 작용 기능&lt;/em&gt; 만 포함 할 수 있습니다. &lt;code&gt;interaction_only=True&lt;/code&gt; 설정으로 &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt; 에서 가져올 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4976a8ab8df4ced9a6ceb9c8368b484dbc953550" translate="yes" xml:space="preserve">
          <source>In some cases, only interaction terms among features are required, and it can be gotten with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;:</source>
          <target state="translated">경우에 따라 기능 간의 상호 작용 용어 만 필요하며 &lt;code&gt;interaction_only=True&lt;/code&gt; 설정으로 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1b40a5cc0ca592bd5c1306138f251a81fe534579" translate="yes" xml:space="preserve">
          <source>In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)</source>
          <target state="translated">지나치게 단순화 된 가정에도 불구하고 순진한 Bayes 분류기는 문서 분류 및 스팸 필터링과 같은 많은 실제 상황에서 상당히 잘 작동했습니다. 필요한 매개 변수를 추정하려면 소량의 교육 데이터가 필요합니다. (순진한 Bayes가 잘 작동하는 이론적 인 이유와 어떤 유형의 데이터에 대해서는 아래 참조를 참조하십시오.)</target>
        </trans-unit>
        <trans-unit id="389828ed3005eb646a2fbe827835555cbdc74437" translate="yes" xml:space="preserve">
          <source>In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \(n - 1\) of the \(n\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.</source>
          <target state="translated">정확도 측면에서 LOO는 종종 테스트 오류의 추정값으로 높은 분산을 초래합니다. 직관적으로, \ (n \) 샘플의 \ (n-1 \)는 각 모델을 빌드하는 데 사용되므로 접기로 구성된 모델은 서로 거의 동일하며 전체 교육 세트에서 작성된 모델과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="36132aafc76511f4279f2a1765dcbaeb9d7a44b1" translate="yes" xml:space="preserve">
          <source>In terms of time and space complexity, Theil-Sen scales according to</source>
          <target state="translated">시간과 공간의 복잡성 측면에서 Theil-Sen은</target>
        </trans-unit>
        <trans-unit id="8cac8320893acecd46013a1cd740f5237cabb213" translate="yes" xml:space="preserve">
          <source>In that case, the model with 2 components and full covariance (which corresponds to the true generative model) is selected.</source>
          <target state="translated">이 경우 성분이 2 개인 모델과 완전 공분산 (실제 생성 모델에 해당)이 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="df23be83a828beae97b01644c9cebfd6ec568f81" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;smooth_idf=False&lt;/code&gt;, the &amp;ldquo;1&amp;rdquo; count is added to the idf instead of the idf&amp;rsquo;s denominator:</source>
          <target state="translated">에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;smooth_idf=False&lt;/code&gt; 의 &quot;1&quot;카운트가 대신 IDF의 분모의 IDF에 추가됩니다</target>
        </trans-unit>
        <trans-unit id="5bd53e1aa867b99daf4cb138797f33e24d9cfda1" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren&amp;rsquo;t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.</source>
          <target state="translated">에서 &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; , 모든 단위는 바이너리 확률 단위입니다. 이는 입력 데이터가 이진이거나 가시적 단위가 켜지거나 꺼질 가능성을 나타내는 0과 1 사이의 실수 값이어야 함을 의미합니다. 이것은 어떤 픽셀이 활성화되어 있고 어떤 픽셀이 활성화되지 않았는 지에 대한 문자 인식을위한 좋은 모델입니다. 자연스러운 장면의 이미지의 경우 배경, 깊이 및 인접 픽셀의 값이 같기 때문에 더 이상 적합하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="729860dcb0b5963ed7d873f5d8718ecbded39d56" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the &lt;code&gt;l1&lt;/code&gt;. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling &lt;code&gt;C1&lt;/code&gt;.</source>
          <target state="translated">에서 &lt;code&gt;l1&lt;/code&gt; 경우, 이론은 예측의 일관성 말한다 (즉, 주어진 가설 아래, 배운 추정이 실제 분포를 아는 모델뿐만 아니라 예측 것을) 때문에의 편견 수 없습니다 &lt;code&gt;l1&lt;/code&gt; . 그러나 &lt;code&gt;C1&lt;/code&gt; 을 스케일링하여 0이 아닌 매개 변수의 올바른 세트와 그 부호를 찾는 관점에서 모델 일관성을 달성 할 수 있다고합니다. 합니다.</target>
        </trans-unit>
        <trans-unit id="3ac113ba1d5ea8579f40e00fa885b5d980c4ba43" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; penalty case, the cross-validation-error correlates best with the test-error, when scaling our &lt;code&gt;C&lt;/code&gt; with the number of samples, &lt;code&gt;n&lt;/code&gt;, which can be seen in the first figure.</source>
          <target state="translated">에서 &lt;code&gt;l1&lt;/code&gt; 페널티 경우, 교차 검증 오류의 상관 관계는 시험 오류와 최고의 우리의 확장 &lt;code&gt;C&lt;/code&gt; 를 샘플 수, &lt;code&gt;n&lt;/code&gt; 은 제 도면에서 알 수있다.</target>
        </trans-unit>
        <trans-unit id="4495b7082e60ec7cb3a7d3cf1946536697a0e6b3" translate="yes" xml:space="preserve">
          <source>In the above case, the classifier is fit on a 1d array of multiclass labels and the &lt;code&gt;predict()&lt;/code&gt; method therefore provides corresponding multiclass predictions. It is also possible to fit upon a 2d array of binary label indicators:</source>
          <target state="translated">위의 경우 분류기는 다중 클래스 레이블의 1d 배열과 &lt;code&gt;predict()&lt;/code&gt; 하므로 메서드는 해당 다중 클래스 예측을 제공합니다. 이진 레이블 표시기의 2D 배열에 맞출 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="41d2181ce120b72b14a943b5e6f5608fe64d404d" translate="yes" xml:space="preserve">
          <source>In the above example, &lt;code&gt;char_wb&lt;/code&gt; analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The &lt;code&gt;char&lt;/code&gt; analyzer, alternatively, creates n-grams that span across words:</source>
          <target state="translated">위의 예에서, &lt;code&gt;char_wb&lt;/code&gt; 분석기가 사용되며 단어 경계 안에있는 문자 ( 각면에 공백으로 채워짐) 에서만 n- 그램을 만듭니다. &lt;code&gt;char&lt;/code&gt; 분석기, 대안 단어에서 그 범위를 N-그램 만듭니다 :</target>
        </trans-unit>
        <trans-unit id="02dd6b844a6f6c7bb7b63318a0172b18e25d4984" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'city'&lt;/code&gt;). However, other transformers generally expect 2D data, and in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="translated">위의 예에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 는 1D 배열을 입력으로 예상하므로 열이 문자열 ( &lt;code&gt;'city'&lt;/code&gt; ) 로 지정되었습니다 . 그러나 다른 변환기에는 일반적으로 2D 데이터가 필요하므로이 경우 열을 문자열 목록으로 지정해야합니다 ( &lt;code&gt;['city']&lt;/code&gt; ) .</target>
        </trans-unit>
        <trans-unit id="8be3ffb93abc4e32a08a3f61b709f6ed3e6124c4" translate="yes" xml:space="preserve">
          <source>In the above example-code, we firstly use the &lt;code&gt;fit(..)&lt;/code&gt; method to fit our estimator to the data and secondly the &lt;code&gt;transform(..)&lt;/code&gt; method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the &lt;code&gt;fit_transform(..)&lt;/code&gt; method as shown below, and as mentioned in the note in the previous section:</source>
          <target state="translated">위의 예제 코드에서, 우리는 먼저 &lt;code&gt;fit(..)&lt;/code&gt; 메소드를 사용하여 추정기에 데이터를 맞추고 두번째로 &lt;code&gt;transform(..)&lt;/code&gt; 메소드를 사용하여 count-matrix를 tf-idf 표현으로 변환합니다. 이 두 단계를 중복 처리하여 중복 처리를 건너 뛰어 동일한 최종 결과를 더 빠르게 달성 할 수 있습니다. 아래에 표시된대로 이전 섹션의 참고에서 언급 한대로 &lt;code&gt;fit_transform(..)&lt;/code&gt; 메소드 를 사용하여 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="fc1fc9feffcc3ab7062a5968abf403f994be456d" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is more than 2, and that the document length is never zero. Likewise, we reject classes which have already been chosen. The documents that are assigned to both classes are plotted surrounded by two colored circles.</source>
          <target state="translated">위의 프로세스에서 거부 샘플링은 n이 2보다 크고 문서 길이가 0이 아닌지 확인하는 데 사용됩니다. 마찬가지로, 우리는 이미 선택된 수업을 거부합니다. 두 클래스에 할당 된 문서는 두 개의 컬러 원으로 둘러싸여 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="35ac86e1f976c024d854c94dc1a07d9250df373b" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is never zero or more than &lt;code&gt;n_classes&lt;/code&gt;, and that the document length is never zero. Likewise, we reject classes which have already been chosen.</source>
          <target state="translated">위의 프로세스에서 거부 샘플링은 n이 절대 0 또는 &lt;code&gt;n_classes&lt;/code&gt; 이상이고 문서 길이가 절대 0이 아닌지 확인하는 데 사용 됩니다. 마찬가지로, 우리는 이미 선택된 수업을 거부합니다.</target>
        </trans-unit>
        <trans-unit id="35b3eed71c5956697e4e941c9abda7fa7875d908" translate="yes" xml:space="preserve">
          <source>In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and \(fn\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as</source>
          <target state="translated">이항 (2 클래스)의 경우, \ ​​(tp \), \ (tn \), \ (fp \) 및 \ (fn \)은 각각 참 긍정, 참 부정, 거짓 긍정 및 거짓 부정의 수입니다. MCC는</target>
        </trans-unit>
        <trans-unit id="8cf754386b9e93bff61012cc7eebd891fe098125" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores.</source>
          <target state="translated">이진 경우 균형 잡힌 정확도는 산술 평균 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;감도&lt;/a&gt; (진 양성 비율) 및 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;특이성&lt;/a&gt; 과 같습니다. (진 음성 비율) 또는 점수가 아닌 이진 예측이있는 ROC 곡선 아래 영역과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a998ef5238ae7921fe9486295ae64f00deab3566" translate="yes" xml:space="preserve">
          <source>In the binary case, we can extract true positives, etc as follows:</source>
          <target state="translated">이진 경우 다음과 같이 진양 수 등을 추출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2482e7f51309cef70ec85538824011f95f0813b1" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of having a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; and &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="translated">&quot;일대일&quot; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 의 경우, 속성의 레이아웃이 약간 더 복잡합니다. 선형 커널이있는 경우 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 는 각각 &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; 및 &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; . 이것은 위에서 설명한 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 의 레이아웃과 유사 하며 각 행은 이제 이진 분류기에 해당합니다. 0에서 n까지의 클래스 순서는 &quot;0 vs 1&quot;, &quot;0 vs 2&quot;,&amp;hellip; &quot;0 vs n&quot;, &quot;1 vs 2&quot;, &quot;1 vs 3&quot;, &quot;1 vs n&quot;입니다. . . &quot;n-1 대 n&quot;.</target>
        </trans-unit>
        <trans-unit id="7befa9fe69dc29e17ce8c14ce1f24dcd596f25dc" translate="yes" xml:space="preserve">
          <source>In the case of Gaussian process classification, &amp;ldquo;one_vs_one&amp;rdquo; might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that &amp;ldquo;one_vs_one&amp;rdquo; does not support predicting probability estimates but only plain predictions. Moreover, note that &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.</source>
          <target state="translated">가우시안 프로세스 분류의 경우,&amp;ldquo;one_vs_one&amp;rdquo;은 전체 데이터 세트의 문제를 줄이는 것이 아니라 전체 훈련 세트의 서브 세트 만 포함하는 많은 문제를 해결해야하기 때문에 계산 비용이 저렴할 수 있습니다. 가우스 프로세스 분류는 데이터 세트의 크기에 따라 입방체로 확장되므로 상당히 빠릅니다. 그러나&amp;ldquo;one_vs_one&amp;rdquo;은 예측 확률 예측을 지원하지 않으며 일반 예측 만 지원합니다. 또한 &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; &lt;/a&gt; 는 내부적으로 진정한 멀티 클래스 라플라스 근사치를 구현하지는 않지만 위에서 논의한 바와 같이 내부적으로 몇 가지 이진 분류 작업을 해결하는 데 기반을두고 있습니다.</target>
        </trans-unit>
        <trans-unit id="35a5ada3d16c18d2778299b423e5960182d45740" translate="yes" xml:space="preserve">
          <source>In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios \(\log[P(y=k | X) / P(y=l | X)]\):</source>
          <target state="translated">LDA의 경우 각 클래스의 가우시안은 \ (k \)에 대해 동일한 공분산 행렬 (\ (\ Sigma_k = \ Sigma \))을 공유한다고 가정합니다. 이것은 선형 결정 표면으로 이어지며, 이는 로그 확률 비율 \ (\ log [P (y = k | X) / P (y = l | X)] \)을 비교하여 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9191afe7181654f4c123a5274615786e30f48b2b" translate="yes" xml:space="preserve">
          <source>In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">QDA의 경우 가우시안의 공분산 행렬 \ (\ Sigma_k \)에 대한 가정이 없으므로 2 차 결정 표면이됩니다. 보다&lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt; [3]&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="be50ffb98df62eb79c354d934aa76c5587bf8cba" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of &lt;code&gt;shape=[n_classes]&lt;/code&gt;. The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="translated">다중 클래스 분류의 경우 &lt;code&gt;coef_&lt;/code&gt; 는 &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; 의 2 차원 배열 이고 &lt;code&gt;intercept_&lt;/code&gt; 는 &lt;code&gt;shape=[n_classes]&lt;/code&gt; 의 1 차원 배열입니다 . &lt;code&gt;coef_&lt;/code&gt; 의 i 번째 행은 i 번째 클래스에 대한 OVA 분류기의 가중치 벡터를 보유하고; 클래스는 오름차순으로 색인됩니다 ( &lt;code&gt;classes_&lt;/code&gt; 속성 참조 ). 원칙적으로 확률 모델을 작성할 수 있으므로 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 및 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 가 일대일 분류에 더 적합합니다.</target>
        </trans-unit>
        <trans-unit id="e1959581346965192dc91c946ef9926bceb1c51a" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.</source>
          <target state="translated">멀티 클래스 분류의 경우 1 대 나머지 분류기의 평균 로그-마진 가능성이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="a45d03e82085c1e194b2b19d0700767439a7ac42" translate="yes" xml:space="preserve">
          <source>In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than the original ones.</source>
          <target state="translated">one-hot / one-of-K 코딩의 경우, 생성 된 피처 이름과 값이 원래 것보다 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="108b9e0576d5a78538771fb415a46ae76d1a6e26" translate="yes" xml:space="preserve">
          <source>In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. &lt;code&gt;BernoulliNB&lt;/code&gt; might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.</source>
          <target state="translated">텍스트 분류의 경우, 단어 분류 벡터가 아닌 단어 발생 벡터를 사용하여이 분류기를 학습하고 사용할 수 있습니다. &lt;code&gt;BernoulliNB&lt;/code&gt; 는 일부 데이터 세트, 특히 문서가 짧은 데이터 세트에서 성능이 향상 될 수 있습니다. 시간이 허락하면 두 모델을 모두 평가하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1ee37ddaa2e2b7fb0103fdddd162d9ad76a8f2dd" translate="yes" xml:space="preserve">
          <source>In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits zero through nine) on which we &lt;em&gt;fit&lt;/em&gt; an &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimator&lt;/a&gt; to be able to &lt;em&gt;predict&lt;/em&gt; the classes to which unseen samples belong.</source>
          <target state="translated">숫자 데이터 세트의 경우, 작업은 이미지가 주어진 숫자를 예측하여 예측하는 것입니다. 우리는 &lt;em&gt;예측할&lt;/em&gt; 수 있는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;추정기&lt;/a&gt; 에 &lt;em&gt;맞는&lt;/em&gt; 10 개의 가능한 클래스 (0에서 9까지의 숫자) 각각의 샘플을받습니다.&lt;em&gt;&lt;/em&gt; 보이지 않는 샘플이 속한되는 클래스를.</target>
        </trans-unit>
        <trans-unit id="0484e6facaecfeba6a4ff8e0552fa9a5ac1c52dd" translate="yes" xml:space="preserve">
          <source>In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if &lt;code&gt;method&lt;/code&gt; produces columns per class, as in {&amp;lsquo;decision_function&amp;rsquo;, &amp;lsquo;predict_proba&amp;rsquo;, &amp;lsquo;predict_log_proba&amp;rsquo;}. For &lt;code&gt;predict_proba&lt;/code&gt; this value is 0. In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases.</source>
          <target state="translated">훈련 부분에 하나 이상의 클래스가없는 경우 { 'decision_function', 'predict_proba', 'predict_log_proba'}와 같이 &lt;code&gt;method&lt;/code&gt; 가 클래스 당 열을 생성하는 경우 기본 점수를 해당 클래스의 모든 인스턴스에 할당해야합니다. . 에 대한 &lt;code&gt;predict_proba&lt;/code&gt; 이 값은 제한된 출력, 다른 경우에 대한 DTYPE 최소 유한 플로트 값만큼 우리 대략 마이너스 무한대을 보장하기 위해 0이다.</target>
        </trans-unit>
        <trans-unit id="74ff5bfdda6b3e93f59169f7c22fc2d68fa3fcf3" translate="yes" xml:space="preserve">
          <source>In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear model&amp;rsquo;s decision_function method directly as the input of inverse_transform.</source>
          <target state="translated">이진 레이블이 소수 (확률) 인 경우 inverse_transform은 가장 큰 값을 가진 클래스를 선택합니다. 일반적으로 선형 모델의 decision_function 메소드 출력을 inverse_transform의 입력으로 직접 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2d852af4ac330c07eec96a31a9559a88b3b70655" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;code&gt;VotingClassifier&lt;/code&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="translated">동점 인 경우 &lt;code&gt;VotingClassifier&lt;/code&gt; 는 오름차순 정렬 순서에 따라 클래스를 선택합니다. 예를 들어 다음 시나리오에서</target>
        </trans-unit>
        <trans-unit id="84352a0fa93e8d3c09e63ba562819b09bff22e0e" translate="yes" xml:space="preserve">
          <source>In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:</source>
          <target state="translated">바둑판의 경우 각 행은 모든 열 클러스터에 속하고 각 열은 모든 행 클러스터에 속합니다. 각 bicluster 내의 값의 분산이 작은이 구조의 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6a06bacf0f95acd504bad8493dc228833ae72576" translate="yes" xml:space="preserve">
          <source>In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised.</source>
          <target state="translated">Fisher 변환을 기반으로하는 95 % 신뢰 구간이 0에 이르면 경고가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2da0fe066fd806cee05903c8f41b8c38bb726d66" translate="yes" xml:space="preserve">
          <source>In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.</source>
          <target state="translated">아래 예에서 작은 수축 임계 값을 사용하면 모델의 정확도가 0.81에서 0.82로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="ae3527cc8009043f3459062f8b3ac5f4c7cdc080" translate="yes" xml:space="preserve">
          <source>In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.</source>
          <target state="translated">아래 그림에서 색상은 클러스터 구성원을 나타내며 큰 원은 알고리즘에서 찾은 핵심 샘플을 나타냅니다. 작은 원은 여전히 ​​클러스터의 일부인 비 핵심 샘플입니다. 또한 특이 치는 아래에 검은 점으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="a915ebedbc9fc712ae336eb7409727fc370425dc" translate="yes" xml:space="preserve">
          <source>In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only.</source>
          <target state="translated">첫 번째 행에서 분류기는 sepal 너비와 sepal 길이 피처 만 사용하고 두 번째 행은 꽃잎 길이와 sepal 길이 만 사용하고 세 번째 행은 꽃잎 너비와 꽃잎 길이 만 사용하여 작성합니다.</target>
        </trans-unit>
        <trans-unit id="bd9366b471cf174a5dc26260b1ee3e4775bd8a95" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1, 1, 1]:</source>
          <target state="translated">다음 예제에서는 데이터 세트를 나타내는 배열에서 NeighborsClassifier 클래스를 구성하고 [1, 1, 1]에 가장 가까운 사람을 묻습니다.</target>
        </trans-unit>
        <trans-unit id="f873541d5e32ccd97b454877a7265b9e862eeb9a" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="translated">다음 예제에서는 데이터 세트를 나타내는 배열에서 NeighborsClassifier 클래스를 구성하고 [1,1,1]에 가장 가까운 사람을 묻습니다.</target>
        </trans-unit>
        <trans-unit id="65f9f2f58e8b0fd298381aa88835a40b1607c17f" translate="yes" xml:space="preserve">
          <source>In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:</source>
          <target state="translated">다음 그림에서는 바이 모달 분포에서 100 개의 점을 가져오고 세 가지 커널 선택에 대한 커널 밀도 추정값이 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="24263a7351535bc4435386a661d4637b721eb5a0" translate="yes" xml:space="preserve">
          <source>In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).</source>
          <target state="translated">다음 그림에서 \ (f (x) = \ cos (\ frac {3} {2} \ pi x) \) 함수와 해당 함수의 일부 노이즈 샘플을 볼 수 있습니다. 함수를 맞추기 위해 세 가지 다른 추정값을 사용합니다. 1, 4, 15 차의 다항식 특징을 갖는 선형 회귀 분석. 첫 번째 추정기는 표본에 적합하지 않은 적합도를 제공 할 수 있으며 실제 함수는 너무 단순하기 때문에 ( 두 번째 추정기는 거의 완벽하게 근사하고 마지막 추정기는 훈련 데이터와 완벽하게 근사하지만 실제 기능에 잘 맞지 않습니다. 즉, 다양한 훈련 데이터에 매우 민감합니다 (높은 분산).</target>
        </trans-unit>
        <trans-unit id="de54ef0bf525631d31eb0623dcd55f8a9ddc120b" translate="yes" xml:space="preserve">
          <source>In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.</source>
          <target state="translated">다음 하위 섹션에서는 이러한 각 기능에 대해 설명하고 공통 API 및 메트릭 정의에 대한 몇 가지 참고 사항을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="7549668dfe247eb9e0e172cc89f620a63604992b" translate="yes" xml:space="preserve">
          <source>In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; function by pointing it to the &lt;code&gt;20news-bydate-train&lt;/code&gt; sub-folder of the uncompressed archive folder.</source>
          <target state="translated">다음에서는 scikit-learn의 20 개 뉴스 그룹에 내장 데이터 세트 로더를 사용합니다. 또는 웹 사이트에서 수동으로 데이터 세트를 다운로드 하고 &lt;code&gt;20news-bydate-train&lt;/code&gt; 을 지정 하여 &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; &lt;/a&gt; 함수를 사용할 수 있습니다 하고 압축되지 않은 아카이브 폴더 하위 폴더를 .</target>
        </trans-unit>
        <trans-unit id="2cb0b9817ecf09ea4893bb9df9d328ce75ef2d1b" translate="yes" xml:space="preserve">
          <source>In the following, &amp;ldquo;city&amp;rdquo; is a categorical attribute while &amp;ldquo;temperature&amp;rdquo; is a traditional numerical feature:</source>
          <target state="translated">다음에서 &quot;도시&quot;는 범주 적 속성 인 반면 &quot;온도&quot;는 일반적인 숫자 기능입니다.</target>
        </trans-unit>
        <trans-unit id="3e019b4cbe3ce7f1554fa08ce08898c560cb8a3b" translate="yes" xml:space="preserve">
          <source>In the following, we start a Python interpreter from our shell and then load the &lt;code&gt;iris&lt;/code&gt; and &lt;code&gt;digits&lt;/code&gt; datasets. Our notational convention is that &lt;code&gt;$&lt;/code&gt; denotes the shell prompt while &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; denotes the Python interpreter prompt:</source>
          <target state="translated">다음에서는 쉘에서 Python 인터프리터를 시작한 다음 &lt;code&gt;iris&lt;/code&gt; 및 &lt;code&gt;digits&lt;/code&gt; 데이터 세트 를로드합니다 . 우리의 표기법은 &lt;code&gt;$&lt;/code&gt; 가 쉘 프롬프트를 나타내고 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 나타내고 는 파이썬 인터프리터 프롬프트를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="fe30ab8cfcc104ee94b5fb01793274570a377034" translate="yes" xml:space="preserve">
          <source>In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:</source>
          <target state="translated">위의 공식에서 \ (\ mathbf {b} \) 및 \ (\ mathbf {c} \)는 각각 보이는 레이어와 숨겨진 레이어의 인터셉트 벡터입니다. 모형의 결합 확률은 에너지 측면에서 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="66070e8da21856ec34fc0b507e5d723e9475a567" translate="yes" xml:space="preserve">
          <source>In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 사례에서 이는 &lt;code&gt;average&lt;/code&gt; 매개 변수 에 따라 가중치를 적용한 각 클래스의 F1 점수 평균 입니다.</target>
        </trans-unit>
        <trans-unit id="47604d7ff8f733fb868ecbca5a77b859a57fe5aa" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the Matthews correlation coefficient can be &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;defined&lt;/a&gt; in terms of a &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt;\(C\) for \(K\) classes. To simplify the definition consider the following intermediate variables:</source>
          <target state="translated">다중 클래스의 경우, Matthews 상관 계수는 \ (K \) 클래스 에 대한 &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; &lt;/a&gt; \ (C \)의 관점에서 &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;정의&lt;/a&gt; 될 수 있습니다 . 정의를 단순화하려면 다음 중간 변수를 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="b8d01a57cb617acafda7dfb6fdd570d7cb46be7c" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross- entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="translated">멀티 클래스의 경우 훈련 알고리즘은 'multi_class'옵션이 'ovr'로 설정된 경우 OvR (one-vs-rest) 방식을 사용하고 'multi_class'옵션이 '다항식으로 설정된 경우 교차 엔트로피 손실을 사용합니다. '. (현재 '다항식'옵션은 'lbfgs', 'sag'및 'newton-cg'솔버에서만 지원됩니다.)</target>
        </trans-unit>
        <trans-unit id="0fc004eb44e3b89b80f2b6796f35d04551e921cc" translate="yes" xml:space="preserve">
          <source>In the multiclass case:</source>
          <target state="translated">멀티 클래스의 경우 :</target>
        </trans-unit>
        <trans-unit id="d77598124fa8bad46b51e89decd127c4c83bc1e3" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators, where the first label set [0,1] has an error:</source>
          <target state="translated">이진 레이블 표시기가있는 다중 레이블 경우 첫 번째 레이블 세트 [0,1]에 오류가 있습니다.</target>
        </trans-unit>
        <trans-unit id="4557110f167a92f3d0af48823270c458eb95839b" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators:</source>
          <target state="translated">이진 레이블 표시기가있는 다중 레이블 경우 :</target>
        </trans-unit>
        <trans-unit id="3dbb497f0422701e359a6bab16a477019a81aa92" translate="yes" xml:space="preserve">
          <source>In the multilabel learning literature, OvR is also known as the binary relevance method.</source>
          <target state="translated">다중 레이블 학습 문헌에서 OvR은 이진 관련성 방법이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="b098a0ed402e179dee6b5de05c6210f893507735" translate="yes" xml:space="preserve">
          <source>In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by &lt;code&gt;transform&lt;/code&gt; will typically be dense.</source>
          <target state="translated">새 공간에서 각 차원은 클러스터 중심까지의 거리입니다. X가 희소하더라도 &lt;code&gt;transform&lt;/code&gt; 의해 반환되는 배열 은 일반적으로 밀도가 높습니다.</target>
        </trans-unit>
        <trans-unit id="a90aa674a36fbb7c4f65ac18f5e6a5a0eb4c7bc3" translate="yes" xml:space="preserve">
          <source>In the official &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt&lt;/a&gt; this task is described as the &amp;ldquo;Restricted&amp;rdquo; task. As I am not sure as to implement the &amp;ldquo;Unrestricted&amp;rdquo; variant correctly, I left it as unsupported for now.</source>
          <target state="translated">공식 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt에서&lt;/a&gt; 에서이 작업은 &quot;제한된&quot;작업으로 설명됩니다. &amp;ldquo;제한되지 않은&amp;rdquo;변형을 올바르게 구현할지 확실하지 않으므로 지금은 지원되지 않는 것으로 남겨 두었습니다.</target>
        </trans-unit>
        <trans-unit id="901c760a214576d30b4ced7bbcce771984b42233" translate="yes" xml:space="preserve">
          <source>In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.</source>
          <target state="translated">예제에서 보았던 간단한 1 차원 문제에서 추정기가 편향 또는 분산으로 고통 받는지 쉽게 알 수 있습니다. 그러나 고차원 공간에서는 모델을 시각화하기가 매우 어려워 질 수 있습니다. 이러한 이유로 아래 설명 된 도구를 사용하는 것이 종종 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="bada0a0c8458a65354b2c23e7134e865cc4bf85c" translate="yes" xml:space="preserve">
          <source>In the single label multiclass case, the rows of the returned matrix sum to 1.</source>
          <target state="translated">단일 레이블 멀티 클래스의 경우 반환 된 행렬의 행은 1이됩니다.</target>
        </trans-unit>
        <trans-unit id="696912c12d134eed0fdc2e472302634288905dc5" translate="yes" xml:space="preserve">
          <source>In the small-samples situation, in which &lt;code&gt;n_samples&lt;/code&gt; is on the order of &lt;code&gt;n_features&lt;/code&gt; or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.</source>
          <target state="translated">&lt;code&gt;n_samples&lt;/code&gt; 가 n_features 이하인 작은 표본 상황에서는 희소 &lt;code&gt;n_features&lt;/code&gt; 분산 추정기가 수축 공분산 추정기보다 더 잘 작동하는 경향이 있습니다. 그러나 반대 상황이나 상관 관계가 높은 데이터의 경우 수치 적으로 불안정 할 수 있습니다. 또한 수축 추정기와 달리 희소 추정기는 비 대각선 구조를 복구 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d5f33dda5b96ece3c041650e9fa8db02e62bfd46" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="translated">scikit-learn의 특정 경우에는 joblib의 pickle 대체 작업 ( &lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt; )을 사용하는 것이 좋습니다 . 이는 종종 장착 된 scikit- 추정기를 배우지 만 문자열이 아닌 디스크로만 피클 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d7c6f4a78fb7d42f5b6b076760e4c0fb27e052c" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be more interesting to use joblib&amp;rsquo;s replacement for pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on big data but it can only pickle to the disk and not to a string:</source>
          <target state="translated">scikit-learn의 특정 경우에는 빅 데이터에서 더 효율적이지만 문자열이 아닌 디스크로만 피클 할 수있는 pickle에 대한 joblib의 대체 ( &lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt; )를 사용하는 것이 더 흥미로울 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="4e8940d2e745f9a24bd23b0a1547dcf715a870bb" translate="yes" xml:space="preserve">
          <source>In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification.</source>
          <target state="translated">전체 기능 세트에서 처음 4 개의 기능 만 중요합니다. 일 변량 피처 선택에서 점수가 가장 높다는 것을 알 수 있습니다. SVM은 이러한 기능 중 하나에 큰 가중치를 할당하지만 정보가 아닌 많은 기능도 선택합니다. SVM 전에 일 변량 기능 선택을 적용하면 중요한 기능으로 인한 SVM 가중치가 증가하므로 분류가 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="a3dd53135ff49dbe7e421a248bf614a3ea8f0d5e" translate="yes" xml:space="preserve">
          <source>In the vector quantization literature, &lt;code&gt;cluster_centers_&lt;/code&gt; is called the code book and each value returned by &lt;code&gt;predict&lt;/code&gt; is the index of the closest code in the code book.</source>
          <target state="translated">벡터 양자화 문헌에서 &lt;code&gt;cluster_centers_&lt;/code&gt; 를 코드북 이라고하며 &lt;code&gt;predict&lt;/code&gt; 에 의해 반환되는 각 값은 코드북 에서 가장 가까운 코드의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="9a63d5086bf9614df56a7612405271e0122a8145" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="translated">2004 년 논문 &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; 에서 O. Ledoit와 M. Wolf는 추정 된 공분산 행렬과 실제 공분산 행렬 사이의 평균 제곱 오차를 최소화하는 최적 수축 계수 \ (\ alpha \)를 계산하는 공식을 제안합니다.</target>
        </trans-unit>
        <trans-unit id="ee9767309b3df05ebf7c392a0bb4e8915eee3d46" translate="yes" xml:space="preserve">
          <source>In these settings, the &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; approach solves the problem know as &amp;lsquo;normalized graph cuts&amp;rsquo;: the image is seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts defining regions while minimizing the ratio of the gradient along the cut, and the volume of the region.</source>
          <target state="translated">이 설정에서 &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;스펙트럼 클러스터링&lt;/a&gt; 접근 방식은 '정규화 된 그래프 컷'으로 알려진 문제를 해결합니다. 이미지는 연결된 복셀의 그래프로 표시되며 스펙트럼 클러스터링 알고리즘은 그래디언트 비율을 최소화하면서 영역을 정의하는 그래프 컷을 선택하는 것과 같습니다. 컷 및 영역의 볼륨.</target>
        </trans-unit>
        <trans-unit id="e2e4475ec0999dd975ba681e19179d817d6681ae" translate="yes" xml:space="preserve">
          <source>In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.</source>
          <target state="translated">이 경우 특정 그룹 집합에 대해 훈련 된 모델이 보이지 않는 그룹으로 일반화되는지 알고 싶습니다. 이를 측정하기 위해 검증 폴드의 모든 샘플이 짝을 이루는 트레이닝 폴드에서 전혀 표현되지 않은 그룹에서 나온 것인지 확인해야합니다.</target>
        </trans-unit>
        <trans-unit id="8e6586aaac37d3a887b7276aee6797fdd6471b11" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;X_test&lt;/code&gt; are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:</source>
          <target state="translated">이 경우 &lt;code&gt;X_train&lt;/code&gt; 과 &lt;code&gt;X_test&lt;/code&gt; 는 동일한 수의 기능을 갖습니다. 동일한 결과를 얻는 또 다른 방법은 기능 수를 수정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c94c921d6a6a8582b29da8ef5a3a44fe1ea80a89" translate="yes" xml:space="preserve">
          <source>In this case, the classifier is fit upon instances each assigned multiple labels. The &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt; is used to binarize the 2d array of multilabels to &lt;code&gt;fit&lt;/code&gt; upon. As a result, &lt;code&gt;predict()&lt;/code&gt; returns a 2d array with multiple predicted labels for each instance.</source>
          <target state="translated">이 경우 분류기는 여러 레이블이 할당 된 인스턴스에 적합합니다. &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; 는&lt;/a&gt; 에 multilabels의 2 차원 배열을 이진화하는 데 사용됩니다 &lt;code&gt;fit&lt;/code&gt; 시. 결과적으로 &lt;code&gt;predict()&lt;/code&gt; 는 각 인스턴스에 대해 여러 개의 예측 레이블이있는 2 차원 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="4367a423584d0b6ba622189fa17b21bb3e206f2c" translate="yes" xml:space="preserve">
          <source>In this case, the cross-validation retained the same ratio of classes across each CV split. Next we&amp;rsquo;ll visualize this behavior for a number of CV iterators.</source>
          <target state="translated">이 경우 교차 검증은 각 CV 분할에서 동일한 비율의 클래스를 유지했습니다. 다음으로 여러 CV 반복자에 대해이 동작을 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="d121f450bc55250670235f93c8cd2083eb40a561" translate="yes" xml:space="preserve">
          <source>In this context, we can define the notions of precision, recall and F-measure:</source>
          <target state="translated">이러한 맥락에서 우리는 정밀도, 리콜 및 F 측정의 개념을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6b7e5d4a758a26d1b659ba54387246d5cebcf12f" translate="yes" xml:space="preserve">
          <source>In this example the dependent variable Y is set as a function of the input features: y = X*w + c. The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant.</source>
          <target state="translated">이 예에서 종속 변수 Y는 입력 기능의 함수로 설정됩니다 : y = X * w + c. 계수 벡터 w는 정규 분포에서 무작위로 샘플링되는 반면, 바이어스 항 c는 상수로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="5d530c717737ac885c81ddc70c9c4fe51f2f2e42" translate="yes" xml:space="preserve">
          <source>In this example the silhouette analysis is used to choose an optimal value for &lt;code&gt;n_clusters&lt;/code&gt;. The silhouette plot shows that the &lt;code&gt;n_clusters&lt;/code&gt; value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.</source>
          <target state="translated">이 예에서, 실루엣 분석은 &lt;code&gt;n_clusters&lt;/code&gt; 에 대한 최적의 값을 선택하는 데 사용됩니다 . 실루엣 플롯은 평균 실루엣 점수가 낮은 클러스터의 존재와 실루엣 플롯의 크기의 변동으로 인해 주어진 데이터에 대해 &lt;code&gt;n_clusters&lt;/code&gt; 값 3, 5 및 6이 잘못된 선택 임을 보여줍니다 . 실루엣 분석은 2와 4 사이에서 더 모호합니다.</target>
        </trans-unit>
        <trans-unit id="550c896ceafe31d2e76547c4031642097a79581f" translate="yes" xml:space="preserve">
          <source>In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.</source>
          <target state="translated">이 예에서는 런타임 및 결과 품질 측면에서 K- 평균에 대한 다양한 초기화 전략을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="d9bed364e1f96090d42e72d8ad4e31b8f81dfc1d" translate="yes" xml:space="preserve">
          <source>In this example we prefer the &lt;code&gt;elasticnet&lt;/code&gt; penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the &lt;code&gt;l1_ratio&lt;/code&gt; parameter (in combination with the regularization strength &lt;code&gt;alpha&lt;/code&gt;) to control this tradeoff.</source>
          <target state="translated">이 예제에서 우리 는 모델 소형 성과 예측력 사이에서 좋은 절충안이되기 때문에 &lt;code&gt;elasticnet&lt;/code&gt; 패널티를 선호합니다 . 이 상충 관계를 제어하기 위해 &lt;code&gt;l1_ratio&lt;/code&gt; 파라미터를 (정규화 강도 &lt;code&gt;alpha&lt;/code&gt; 와 함께) 추가로 조정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5f4ca84332e1fc2168e90c43c86e1d784ebd5f8c" translate="yes" xml:space="preserve">
          <source>In this example we see how to robustly fit a linear model to faulty data using the RANSAC algorithm.</source>
          <target state="translated">이 예에서는 RANSAC 알고리즘을 사용하여 선형 모델을 결함이있는 데이터에 강력하게 맞추는 방법을 살펴 봅니다.</target>
        </trans-unit>
        <trans-unit id="38fc37287fc51222e73dd7c83e6c92e563107ff6" translate="yes" xml:space="preserve">
          <source>In this example you might try to:</source>
          <target state="translated">이 예에서는 다음을 시도 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8a61e0fa0737723bbfe9d0174ce3aad285419f4d" translate="yes" xml:space="preserve">
          <source>In this example, &lt;code&gt;X&lt;/code&gt; is &lt;code&gt;float32&lt;/code&gt;, which is cast to &lt;code&gt;float64&lt;/code&gt; by &lt;code&gt;fit_transform(X)&lt;/code&gt;.</source>
          <target state="translated">이 예에서 &lt;code&gt;X&lt;/code&gt; 는 &lt;code&gt;float32&lt;/code&gt; 이며 &lt;code&gt;fit_transform(X)&lt;/code&gt; 의해 &lt;code&gt;float64&lt;/code&gt; 로 캐스트됩니다 .</target>
        </trans-unit>
        <trans-unit id="2812da8873763c11175cae962f9ab9000ab381c4" translate="yes" xml:space="preserve">
          <source>In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.</source>
          <target state="translated">이 예에서는 연결된 원이있는 이미지가 생성되고 스펙트럼 클러스터링을 사용하여 원을 구분합니다.</target>
        </trans-unit>
        <trans-unit id="69af0b849be70a0524a821dde21a609feb16811a" translate="yes" xml:space="preserve">
          <source>In this example, pixels are represented in a 3D-space and K-means is used to find 64 color clusters. In the image processing literature, the codebook obtained from K-means (the cluster centers) is called the color palette. Using a single byte, up to 256 colors can be addressed, whereas an RGB encoding requires 3 bytes per pixel. The GIF file format, for example, uses such a palette.</source>
          <target state="translated">이 예제에서 픽셀은 3D 공간으로 표현되며 K- 평균은 64 개의 색상 군집을 찾는 데 사용됩니다. 화상 처리 문헌에서, K- 평균 (클러스터 중심)으로부터 얻은 코드북을 컬러 팔레트라고한다. 단일 바이트를 사용하면 최대 256 개의 색상을 처리 할 수 ​​있지만 RGB 인코딩에는 픽셀 당 3 바이트가 필요합니다. 예를 들어 GIF 파일 형식은 이러한 팔레트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2f0fb947da0f2bfc5faf32b771a3cb10ff049eda" translate="yes" xml:space="preserve">
          <source>In this example, the numeric data is standard-scaled after mean-imputation, while the categorical data is one-hot encoded after imputing missing values with a new category (&lt;code&gt;'missing'&lt;/code&gt;).</source>
          <target state="translated">이 예에서 숫자 데이터는 평균 측정 후 표준 배율이며, 범주 형 데이터는 새 범주 ( &lt;code&gt;'missing'&lt;/code&gt; )로 결 측값 을 대치 한 후 원 핫 인코딩 됩니다.</target>
        </trans-unit>
        <trans-unit id="d88656bc2e040320cf9595554acac12be98f916c" translate="yes" xml:space="preserve">
          <source>In this example, we compare the estimation errors that are made when using various types of location and covariance estimates on contaminated Gaussian distributed data sets:</source>
          <target state="translated">이 예에서는 오염 된 가우시안 분산 데이터 세트에서 다양한 유형의 위치 및 공분산 추정을 사용할 때 발생하는 추정 오차를 비교합니다.</target>
        </trans-unit>
        <trans-unit id="2b7bcaf87ef3b0730f7083836942b0b038810927" translate="yes" xml:space="preserve">
          <source>In this example, we give an overview of the &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt;. Two examples illustrate the benefit of transforming the targets before learning a linear regression model. The first example uses synthetic data while the second example is based on the Boston housing data set.</source>
          <target state="translated">이 예에서는 &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt; 의 개요를 제공합니다 . 두 가지 예는 선형 회귀 모델을 학습하기 전에 대상을 변환하는 이점을 보여줍니다. 첫 번째 예는 합성 데이터를 사용하고 두 번째 예는 보스턴 주택 데이터 세트를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="fd9410f53a0f1d1aa2f5ff77c7bafaf9751d4c08" translate="yes" xml:space="preserve">
          <source>In this example, we set the value of &lt;code&gt;gamma&lt;/code&gt; manually. To find good values for these parameters, we can use tools such as &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; and &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;.</source>
          <target state="translated">이 예에서는 &lt;code&gt;gamma&lt;/code&gt; 값을 수동으로 설정합니다 . 이러한 매개 변수에 적합한 값을 찾기 위해 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;그리드 검색&lt;/a&gt; 및 &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 과 같은 도구를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9e0782ea6d7859077c07d60aaa0a30b1c4373f50" translate="yes" xml:space="preserve">
          <source>In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.</source>
          <target state="translated">이 그림에서 커널 매개 변수 감마의 다른 값에 대한 SVM의 교육 점수 및 유효성 검사 점수를 볼 수 있습니다. 감마 값이 매우 낮은 경우 훈련 점수와 유효성 검사 점수가 모두 낮음을 알 수 있습니다. 이것을 언더 피팅이라고합니다. 감마의 중간 값은 두 점수 모두에서 높은 값을 초래합니다. 즉 분류 기가 상당히 잘 수행됩니다. 감마가 너무 높으면 분류 기가 과적 합되므로 훈련 점수는 양호하지만 유효성 검사 점수는 좋지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2c39a03080473177a8509645110953edafebbd76" translate="yes" xml:space="preserve">
          <source>In this scheme, features and samples are defined as follows:</source>
          <target state="translated">이 체계에서 기능과 샘플은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="5941fbb58c226f551ff80660bcd51a84bcc2bae1" translate="yes" xml:space="preserve">
          <source>In this section we will see how to:</source>
          <target state="translated">이 섹션에서는 다음을 수행하는 방법을 살펴 봅니다.</target>
        </trans-unit>
        <trans-unit id="6ce5845b6414a0cfccffc603f3efdd4b47c7ce4b" translate="yes" xml:space="preserve">
          <source>In this section, we introduce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; vocabulary that we use throughout scikit-learn and give a simple learning example.</source>
          <target state="translated">이 섹션에서는 scikit-learn 전체에서 사용 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;기계 학습&lt;/a&gt; 어휘를 소개하고 간단한 학습 예를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="e7b54ae8e73f20fa370a273bbb52814367b82582" translate="yes" xml:space="preserve">
          <source>In this snippet we make use of a &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; coupled with &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; to evaluate feature importances and select the most relevant features. Then, a &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt;&lt;/a&gt; is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt; examples for more details.</source>
          <target state="translated">이 스 니펫에서는 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt; 결합 된 &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt; 를 사용하여 기능의 중요도를 평가하고 가장 관련성이 높은 기능을 선택합니다. 그런 다음 &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 는 변환 된 출력, 즉 관련 기능 만 사용하여 학습 됩니다. 다른 기능 선택 방법 및 기능 중요도를 평가하는 방법을 제공하는 분류기로 유사한 작업을 수행 할 수 있습니다. 자세한 내용은 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 예제를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="87aee0924ee2a28e2d573efd8e050c2a5c1632a3" translate="yes" xml:space="preserve">
          <source>In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n \}\). How can this dataset be described mathematically? A very simple &lt;code&gt;continuous latent variable&lt;/code&gt; model for \(X\) is</source>
          <target state="translated">비지도 학습에는 데이터 세트 \ (X = \ {x_1, x_2, \ dots, x_n \} \) 만 있습니다. 이 데이터 세트를 수학적으로 어떻게 설명 할 수 있습니까? 매우 간단한 &lt;code&gt;continuous latent variable&lt;/code&gt; \ (X \)에 대한 모델은</target>
        </trans-unit>
        <trans-unit id="c89a6ca6f29b888687c7afd577a282b5b95a2be5" translate="yes" xml:space="preserve">
          <source>Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as &lt;em&gt;data leakage&lt;/em&gt;), for example in the case of scalers or imputing missing values.</source>
          <target state="translated">예를 들어 스케일러 또는 결 측값을 대치하는 경우와 같이 테스트 데이터의 통계를 전처리기에 통합하면 교차 검증 점수를 신뢰할 수 없게합니다 ( &lt;em&gt;데이터 누출&lt;/em&gt; 이라고 함 ).</target>
        </trans-unit>
        <trans-unit id="4c33f1e1286c254eaae3fbe03197d5578f08f56a" translate="yes" xml:space="preserve">
          <source>Increasing &lt;code&gt;max_depth&lt;/code&gt; for AdaBoost lowers the standard deviation of the scores (but the average score does not improve).</source>
          <target state="translated">AdaBoost에 대해 &lt;code&gt;max_depth&lt;/code&gt; 를 늘리면 점수의 표준 편차가 낮아 지지만 평균 점수는 향상되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e79b1358981354168a853701629e2643ba45bf93" translate="yes" xml:space="preserve">
          <source>Increasing false positive rates such that element i is the false positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">요소 i가 스코어&amp;gt; = 임계 값 [i]을 갖는 가양 성 예측율이되도록 가양 성 비율을 증가시키는 것.</target>
        </trans-unit>
        <trans-unit id="3ca08d3a2216068596512fa76cc1f85e2464a3a8" translate="yes" xml:space="preserve">
          <source>Increasing thresholds on the decision function used to compute precision and recall.</source>
          <target state="translated">정밀도를 계산하고 호출하는 데 사용되는 의사 결정 기능의 임계 값을 증가시킵니다.</target>
        </trans-unit>
        <trans-unit id="7ae5f53b337e575381bac1d47d2d4a4d2e4839b6" translate="yes" xml:space="preserve">
          <source>Increasing true positive rates such that element i is the true positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">요소 i가 스코어&amp;gt; = 임계 값 [i]을 갖는 실제 양의 예측 비율이되도록 진 양성 비율을 증가시키는 것.</target>
        </trans-unit>
        <trans-unit id="54206634ab03f8962d59d7c24e12c85ebd45b5e1" translate="yes" xml:space="preserve">
          <source>Incremental PCA</source>
          <target state="translated">증분 PCA</target>
        </trans-unit>
        <trans-unit id="acaf3165fc4e0ddec759b9648ee12eed48089691" translate="yes" xml:space="preserve">
          <source>Incremental fit on a batch of samples.</source>
          <target state="translated">샘플 배치에 증분 적합.</target>
        </trans-unit>
        <trans-unit id="a79a34aec33f8c8b084e4316cf8e243a4d6601e6" translate="yes" xml:space="preserve">
          <source>Incremental fit with X.</source>
          <target state="translated">X에 증분 적합합니다.</target>
        </trans-unit>
        <trans-unit id="87210470540ea5af2ee40f330fdeea4017f1c0aa" translate="yes" xml:space="preserve">
          <source>Incremental fit with X. All of X is processed as a single batch.</source>
          <target state="translated">X에 증분 적합. 모든 X는 단일 배치로 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="5b9d567927b0a80924b0a28fdea6cf19b23d2e57" translate="yes" xml:space="preserve">
          <source>Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.</source>
          <target state="translated">IPCA (증분 주성분 분석)는 일반적으로 분해 할 데이터 세트가 메모리에 맞지 않을 때 주성분 분석 (PCA)을 대체하는 데 사용됩니다. IPCA는 입력 데이터 샘플 수와 무관 한 메모리 양을 사용하여 입력 데이터에 대해 낮은 순위의 근사값을 작성합니다. 여전히 입력 데이터 기능에 의존하지만 배치 크기를 변경하면 메모리 사용을 제어 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="66088e706ece2d903ed2071fa2d42be9315774b6" translate="yes" xml:space="preserve">
          <source>Incremental principal components analysis (IPCA).</source>
          <target state="translated">증분 주성분 분석 (IPCA).</target>
        </trans-unit>
        <trans-unit id="ef7722207a6c2343d08e45f401cd00ccd19381c7" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data.</source>
          <target state="translated">모델을 데이터에 점차적으로 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="505bf67b8aa7cec37d64a9ce9b03d73f70b38b8b" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">모델을 데이터에 점차적으로 맞 춥니 다. 각 출력 변수에 대해 별도의 모델을 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="9e12e704fa3eb16be83d58c2167c9c4f83379a1d" translate="yes" xml:space="preserve">
          <source>Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.</source>
          <target state="translated">실제로 많은 추정값은 각 기능의 값이 거의 0에 가까워 지거나 모든 기능이 비슷한 규모로 변한다는 가정하에 설계되었습니다. 특히, 메트릭 기반 및 그라디언트 기반 추정기는 대략 표준화 된 데이터 (단위 차이가있는 중심 형상)를 가정합니다. 데이터의 임의 스케일링에 강력한 의사 결정 트리 기반 추정기는 예외입니다.</target>
        </trans-unit>
        <trans-unit id="7933c6d72de999f40e22d3286d9c782fd636305b" translate="yes" xml:space="preserve">
          <source>Independent Component Analysis: ICA</source>
          <target state="translated">독립 성분 분석 : ICA</target>
        </trans-unit>
        <trans-unit id="d170598045cdc9e2df037718a96d1706ed03e640" translate="yes" xml:space="preserve">
          <source>Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;Fast ICA&lt;/code&gt;&lt;/a&gt; algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.</source>
          <target state="translated">독립 성분 분석은 다변량 신호를 최대한 독립적 인 부가 성분으로 분리합니다. &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;Fast ICA&lt;/code&gt; 를&lt;/a&gt; 사용하여 scikit-learn에서 구현됩니다. 알고리즘을 . 일반적으로 ICA는 차원을 줄이는 데 사용되지 않고 중첩 된 신호를 분리하는 데 사용됩니다. ICA 모델에는 노이즈 항이 포함되어 있지 않으므로 모델이 정확하려면 미백을 적용해야합니다. 이는 whiten 인수를 사용하여 내부적으로 수행하거나 PCA 변형 중 하나를 사용하여 수동으로 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="420bdf88d0c37e0c49c684e7be83be0c3065749b" translate="yes" xml:space="preserve">
          <source>Independent component analysis, a latent variable model with non-Gaussian latent variables.</source>
          <target state="translated">독립 성분 분석, 비 가우시안 잠재 변수가있는 잠재 변수 모델.</target>
        </trans-unit>
        <trans-unit id="e81fd2ba1ed4351b51becec6b3e044be03272d29" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel.</source>
          <target state="translated">폴리 / 시그 모이 드 커널의 독립 매개 변수.</target>
        </trans-unit>
        <trans-unit id="75b2e172573134b992419919380eaa4d379125c8" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel. 0 by default.</source>
          <target state="translated">폴리 / 시그 모이 드 커널의 독립 매개 변수. 기본적으로 0입니다.</target>
        </trans-unit>
        <trans-unit id="93ccb8f475af3ead0828a4d704d80fd7c1bcca2a" translate="yes" xml:space="preserve">
          <source>Independent term in decision function.</source>
          <target state="translated">의사 결정 기능에서 독립적 인 용어.</target>
        </trans-unit>
        <trans-unit id="d60b4ce63cb13d9b546e71bb468305b122e1ff12" translate="yes" xml:space="preserve">
          <source>Independent term in decision function. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="translated">의사 결정 기능에서 독립적 인 용어. &lt;code&gt;fit_intercept = False&lt;/code&gt; 경우 0.0으로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="62ca36e367478a373b265bf6692ac1dd0b87c736" translate="yes" xml:space="preserve">
          <source>Independent term in kernel function. It is only significant in &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">커널 함수에서 독립적 인 용어. 'poly'와 'sigmoid'에서만 중요합니다.</target>
        </trans-unit>
        <trans-unit id="498514c5789196d4e7f38be6d2ccefad9084f660" translate="yes" xml:space="preserve">
          <source>Independent term in poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">폴리 및 시그 모이 드 커널에서 독립적 인 용어. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="c818388cffefe0c1449b9099e6b8c434f2466b05" translate="yes" xml:space="preserve">
          <source>Independent term in the decision function.</source>
          <target state="translated">의사 결정 기능에서 독립 용어.</target>
        </trans-unit>
        <trans-unit id="b8069da00c91cf6e966b739b57f9cbe347e859d2" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model.</source>
          <target state="translated">선형 모형에서 독립항.</target>
        </trans-unit>
        <trans-unit id="5c78017fad7dc4be13e21b61b07a09cb5931df33" translate="yes" xml:space="preserve">
          <source>Index of the cluster each sample belongs to.</source>
          <target state="translated">각 샘플이 속한 클러스터의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="79bbc01c7afbd569e88078c06011f6a142dc18ba" translate="yes" xml:space="preserve">
          <source>Index of the column of X to be swapped.</source>
          <target state="translated">교환 될 X의 열 색인.</target>
        </trans-unit>
        <trans-unit id="ed1d58c02de7a13d74564b832a9effc7dd7512f7" translate="yes" xml:space="preserve">
          <source>Index of the row of X to be swapped.</source>
          <target state="translated">교환 될 X 행의 인덱스.</target>
        </trans-unit>
        <trans-unit id="ec76f2d92b2be403363a104dc4a87849c9c331a9" translate="yes" xml:space="preserve">
          <source>Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.</source>
          <target state="translated">인덱싱 가능한 데이터 구조는 일관된 첫 번째 차원을 가진 배열, 목록, 데이터 프레임 또는 scipy 희소 행렬 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="436737ead6b730ec05aa4979d3ab186eb46b0b4a" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above.</source>
          <target state="translated">두 번째 축의 데이터를 인덱싱합니다. 정수는 위치 열로 해석되는 반면 문자열은 이름별로 DataFrame 열을 참조 할 수 있습니다. &lt;code&gt;transformer&lt;/code&gt; X를 1d 배열과 같은 (벡터) 로 예상 하는 경우 스칼라 문자열 또는 int를 사용해야합니다 . 그렇지 않으면 2d 배열이 변환기로 전달됩니다. 콜 러블에 입력 데이터 &lt;code&gt;X&lt;/code&gt; 가 전달 되고 위의 값 중 하나를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="33324894ea0a98c1ef1f880dc97967e00c913318" translate="yes" xml:space="preserve">
          <source>Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised.</source>
          <target state="translated">func이 희소 행렬을 입력으로 받아 들인다는 것을 나타냅니다. validate가 False이면 효과가 없습니다. 그렇지 않으면 accept_sparse가 false이면 희소 행렬 입력으로 인해 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="eb7cd0d8cb7fae1e80a3e28d3771772ae8884b48" translate="yes" xml:space="preserve">
          <source>Indicate that the input X array should be checked before calling &lt;code&gt;func&lt;/code&gt;. The possibilities are:</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 호출하기 전에 입력 X 배열을 확인해야 함을 나타냅니다 . 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ae286e88bfa268243cfd38132ccb40e58428bfed" translate="yes" xml:space="preserve">
          <source>Indicate that transform should forward the y argument to the inner callable.</source>
          <target state="translated">변환이 y 인수를 내부 호출 가능으로 전달해야 함을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="9d241566a403a6506d3449cf17d407da2b6e2613" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels</source>
          <target state="translated">클래스 레이블의 순서를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="9f992c130abda366b807271662ae2ae17c7305e7" translate="yes" xml:space="preserve">
          <source>Indices according to which X will be subsampled.</source>
          <target state="translated">X가 서브 샘플링 될 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="13d9e5d82e7cab8325fb7843fa12831d259a69f1" translate="yes" xml:space="preserve">
          <source>Indices of &lt;code&gt;components_&lt;/code&gt; in the training set.</source>
          <target state="translated">훈련 세트 의 &lt;code&gt;components_&lt;/code&gt; 지수 .</target>
        </trans-unit>
        <trans-unit id="eeb5984b85169d88759ac10f7fe9d10f5246bc74" translate="yes" xml:space="preserve">
          <source>Indices of active variables at the end of the path.</source>
          <target state="translated">경로 끝에서 활성 변수의 인덱스.</target>
        </trans-unit>
        <trans-unit id="fff8cc57ffbca371ebcb1bd938597e8d339ff509" translate="yes" xml:space="preserve">
          <source>Indices of cluster centers</source>
          <target state="translated">클러스터 센터의 지표</target>
        </trans-unit>
        <trans-unit id="2c68ceeb78b6311d290c266259420efe85138435" translate="yes" xml:space="preserve">
          <source>Indices of columns in the dataset that belong to the bicluster.</source>
          <target state="translated">데이터 세트에서 bicluster에 속하는 열의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="d4d2ad6637f8190da67b396b7e452baac4f7558f" translate="yes" xml:space="preserve">
          <source>Indices of core samples.</source>
          <target state="translated">핵심 샘플의 지표.</target>
        </trans-unit>
        <trans-unit id="c71c298055da26ecac09942b9115f5fc73f7640d" translate="yes" xml:space="preserve">
          <source>Indices of rows in the dataset that belong to the bicluster.</source>
          <target state="translated">데이터 세트에서 bicluster에 속하는 행의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="a5ecd973c55633f90c97a971f74a80fe11af3310" translate="yes" xml:space="preserve">
          <source>Indices of support vectors.</source>
          <target state="translated">지지 벡터의 지표.</target>
        </trans-unit>
        <trans-unit id="9ccd80ce2c5e0529264583d000f7c9651892271d" translate="yes" xml:space="preserve">
          <source>Indices of the approximate nearest points in the population matrix.</source>
          <target state="translated">모집단 행렬에서 가장 가까운 가장 가까운 점의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="94147abfa5127a12fe3c3b0c7153a32b171d401a" translate="yes" xml:space="preserve">
          <source>Indices of the nearest points in the population matrix.</source>
          <target state="translated">모집단 행렬에서 가장 가까운 점의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="3db97f5586a1b88a52d6998bdcb68ce6424bd44e" translate="yes" xml:space="preserve">
          <source>Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.</source>
          <target state="translated">트리 구조를 시각화하여 개별 의사 결정 트리를 쉽게 해석 할 수 있습니다. 그러나 그라디언트 부스팅 모델은 수백 개의 회귀 트리를 포함하므로 개별 트리의 육안 검사로 쉽게 해석 할 수 없습니다. 다행히도, 기울기 부스팅 모델을 요약하고 해석하기 위해 많은 기술이 제안되었습니다.</target>
        </trans-unit>
        <trans-unit id="dda28c621b6ebb6a75808a25ba823af15c148423" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="translated">개별 의사 결정 트리는 본질적으로 적절한 분리 점을 선택하여 기능 선택을 수행합니다. 이 정보는 각 기능의 중요성을 측정하는 데 사용될 수 있습니다. 기본 아이디어는 : 기능이 트리의 분할 지점에서 자주 사용되는 경우 해당 기능이 더 중요하다는 것입니다. 이 중요성 개념은 각 트리의 기능 중요도를 평균화하여 의사 결정 트리 앙상블까지 확장 할 수 있습니다 (자세한 내용은 &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;기능 중요도 평가&lt;/a&gt; 참조).</target>
        </trans-unit>
        <trans-unit id="e3bee3019e098ad6b65e2ff793a0e712b529b8f1" translate="yes" xml:space="preserve">
          <source>Individual samples are assumed to be files stored a two levels folder structure such as the following:</source>
          <target state="translated">개별 샘플은 다음과 같은 2 단계 폴더 구조로 저장된 파일 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="c0fc01010c6e24d626cecdd62fd33ec75c0c929c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="translated">개별 단계는 매개 변수로 대체 될 수 있으며, 비 최종 단계는 &lt;code&gt;None&lt;/code&gt; 으로 설정하여 무시할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1005c12f11beba0f3b6f9dd6a7112c31b922588d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample</source>
          <target state="translated">각 샘플에 대한 개별 중량</target>
        </trans-unit>
        <trans-unit id="1fda26bba39629c5adc019bfeebf23b6ea1cae92" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.</source>
          <target state="translated">sample_weight가 전달되고 base_estimator fit 방법이이를 지원하지 않으면 각 샘플에 대한 개별 가중치가 오류를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="fa75f1e4d13933a19ded3aa860129fc7cab3ed7d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample, ignored if None is passed.</source>
          <target state="translated">각 샘플에 대한 개별 가중치. 없음이 전달되면 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="718e842694bb04faf5cc021c83e29c576f98c12d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="translated">각 샘플에 대한 개별 중량. sample_weight가 None이 아니고 solver = 'auto'이면 솔버는 'cholesky'로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="f221e5d04a99f60098722e2605a369c8541e6c9c" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="translated">관성은 정규화 된 측정 항목이 아닙니다. 낮은 값이 더 좋고 0이 최적이라는 것을 알고 있습니다. 그러나 매우 높은 공간에서 유클리드 거리는 팽창하는 경향이 있습니다 (이것은 소위 &quot;차원의 저주&quot;의 예입니다). k- 평균 군집화 전에 &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; 와 같은 차원 축소 알고리즘을 실행하면 이 문제를 완화하고 계산 속도를 높일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e010b0c9058bbaf9975d3f14818f3861029f83a1" translate="yes" xml:space="preserve">
          <source>Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.</source>
          <target state="translated">관성은 클러스터가 볼록하고 등방성이라고 가정합니다. 항상 그런 것은 아닙니다. 길쭉한 클러스터 또는 불규칙한 모양의 매니 폴드에는 제대로 반응하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2d57b1c4d1f958efe3710f23677dd552a8a1384c" translate="yes" xml:space="preserve">
          <source>Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="translated">관성 또는 클러스터 내 제곱 기준 기준은 내부적으로 응집성있는 클러스터의 척도로 인식 될 수 있습니다. 여러 가지 단점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="1b9b7d4cd56309d7954eee8c5d88a8d58559a22d" translate="yes" xml:space="preserve">
          <source>Inference of the model can be time consuming.</source>
          <target state="translated">모델의 추론은 시간이 많이 걸릴 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8f5d2c4b74b9f6de8f1a369f5793fd5ad3db1bd0" translate="yes" xml:space="preserve">
          <source>Influence of outliers on location and covariance estimates</source>
          <target state="translated">위치 및 공분산 추정치에 대한 특이 치의 영향</target>
        </trans-unit>
        <trans-unit id="b8c700f6663aab653644d35fb6aa9a0e53919c01" translate="yes" xml:space="preserve">
          <source>Information on how to contribute. This also contains useful information for advanced users, for example how to build their own estimators.</source>
          <target state="translated">기여 방법에 대한 정보. 여기에는 고급 사용자를위한 유용한 정보 (예 : 자체 견적서를 작성하는 방법)가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="c4fbdb7aab44015fbed39936864f9255a99074c1" translate="yes" xml:space="preserve">
          <source>Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">정보 기준 기반 모델 선택은 매우 빠르지 만, 자유도의 적절한 추정에 의존하고, 큰 샘플 (점근 적 결과)에 대해 도출되며 모델이 올바른 것으로 가정합니다. 즉,이 모델에 의해 데이터가 실제로 생성된다고 가정합니다. 또한 문제가 심하게 조절되면 (샘플보다 더 많은 기능이있는 경우) 중단되는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="168bf67b39abe0e5515698a4ec915c75e07ca524" translate="yes" xml:space="preserve">
          <source>Initial value for the dictionary for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오에 대한 사전의 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="64b2b9f72c239478fc1b9e586ac8147218ca87bd" translate="yes" xml:space="preserve">
          <source>Initial value for the sparse code for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오를위한 스파 스 코드의 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="1a6282c9a9baf1c9231fe6132d9510c0860835e2" translate="yes" xml:space="preserve">
          <source>Initial values for the components for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오를위한 구성 요소의 초기 값.</target>
        </trans-unit>
        <trans-unit id="ff1f0a80e07dd6cd648bd3a5e935a909ec2cb299" translate="yes" xml:space="preserve">
          <source>Initial values for the loadings for warm restart scenarios.</source>
          <target state="translated">웜 재시작 시나리오의로드에 대한 초기 값입니다.</target>
        </trans-unit>
        <trans-unit id="e597ad1e68022a1929058718fd92959e281b3619" translate="yes" xml:space="preserve">
          <source>Initialization of embedding. Possible options are &amp;lsquo;random&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.</source>
          <target state="translated">임베딩 초기화. 가능한 옵션은 'random', 'pca'및 numpy 배열 형태 (n_samples, n_components)입니다. PCA 초기화는 사전 계산 된 거리와 함께 사용할 수 없으며 일반적으로 임의 초기화보다 전체적으로 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="e644154875d3e41452e4b87177ec7a98e4e47540" translate="yes" xml:space="preserve">
          <source>Initialization value for coefficients of logistic regression. Useless for liblinear solver.</source>
          <target state="translated">로지스틱 회귀 계수의 초기화 값입니다. liblinear 솔버에는 쓸모가 없습니다.</target>
        </trans-unit>
        <trans-unit id="ed7011971816dd0f1903ad54a411facedaee4ded" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">스파 스 코드의 초기화 값. &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="5f66672d1a211feccb2663e524389cc7bbdc9544" translate="yes" xml:space="preserve">
          <source>Initialize self. See help(type(self)) for accurate signature.</source>
          <target state="translated">자기를 초기화하십시오. 정확한 서명은 help (type (self))를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="f4694fddfebcd07057574f3bedbe073aecef8cbe" translate="yes" xml:space="preserve">
          <source>Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the &lt;code&gt;score_samples&lt;/code&gt; method, while the threshold can be controlled by the &lt;code&gt;contamination&lt;/code&gt; parameter.</source>
          <target state="translated">특이 치는 1로 표시되고 특이 치는 -1로 표시됩니다. 예측 방법은 추정기에 의해 계산 된 원시 스코어링 함수에 대한 임계 값을 사용합니다. 이 스코어링 기능은 &lt;code&gt;score_samples&lt;/code&gt; 방법을 통해 액세스 할 수 있으며 임계 값은 &lt;code&gt;contamination&lt;/code&gt; 매개 변수 로 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2495be2cc170e277a5d9d5dd99a3f779ff0175c9" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">알고리즘에 의해 유지되는 내부 통계가 충분합니다. 초기화시이를 전달하는 것은 온라인 설정에서 유용하여 진화의 역사를 잃지 않도록합니다. A (n_components, n_components)는 사전 공분산 행렬입니다. B (n_features, n_components)는 데이터 근사 행렬입니다</target>
        </trans-unit>
        <trans-unit id="4cb705151cf2a5dcfea8029a0e74d39c67469fa5" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSC/CSR matrix.</source>
          <target state="translated">CSC / CSR 매트릭스의 열 스케일링</target>
        </trans-unit>
        <trans-unit id="15ad21d3f40808b965488683b0faaff07ad4b5e9" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSR matrix.</source>
          <target state="translated">CSR 매트릭스의 열 스케일링</target>
        </trans-unit>
        <trans-unit id="75290bdbc975498c98c7572ec067e43ddceb0c68" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l1 norm</source>
          <target state="translated">l1 규범을 사용하여 Inplace row 정규화</target>
        </trans-unit>
        <trans-unit id="bc0180825da8415aba8c76f27c29d7e471b70c2a" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l2 norm</source>
          <target state="translated">L2 규범을 사용하여 Inplace Row 정규화</target>
        </trans-unit>
        <trans-unit id="d91ae689358e283ef6d343e24e55244f1fb1cad2" translate="yes" xml:space="preserve">
          <source>Inplace row scaling of a CSR or CSC matrix.</source>
          <target state="translated">CSR 또는 CSC 매트릭스의 적절한 행 스케일링.</target>
        </trans-unit>
        <trans-unit id="16ca749420dd58126c1f3f4ccf95ce2fc49387c9" translate="yes" xml:space="preserve">
          <source>Input array.</source>
          <target state="translated">입력 배열.</target>
        </trans-unit>
        <trans-unit id="f6fcca00499ce6b21e6114d56b450f6d3d43ccb2" translate="yes" xml:space="preserve">
          <source>Input checker utility for building a cross-validator</source>
          <target state="translated">교차 유효성 검사기를 작성하기위한 입력 검사기 유틸리티</target>
        </trans-unit>
        <trans-unit id="3f43a2e4863dbf39da8cfbd5e4e557f3052ec269" translate="yes" xml:space="preserve">
          <source>Input data</source>
          <target state="translated">입력 데이터</target>
        </trans-unit>
        <trans-unit id="41aa04ac5754100b497c803419573444b7e1d42b" translate="yes" xml:space="preserve">
          <source>Input data representation and sparsity</source>
          <target state="translated">입력 데이터 표현 및 희소성</target>
        </trans-unit>
        <trans-unit id="66a8a4e34fe15bd5eafb5d984d77b9c0e3866728" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed.</source>
          <target state="translated">변환 될 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="fbb05f66a147e8520f226c078931507f60d0caac" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed. It cannot be sparse.</source>
          <target state="translated">변환 될 입력 데이터. 드문 드문 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="1e3e0a570b83c9cbe639106afeb9bedd23e3fcfd" translate="yes" xml:space="preserve">
          <source>Input data to be transformed.</source>
          <target state="translated">변환 할 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="071feefe9143dba47a473de169ba49367bfce443" translate="yes" xml:space="preserve">
          <source>Input data to be transformed. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csr_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">변환 할 입력 데이터. 효율성을 극대화 하려면 &lt;code&gt;dtype=np.float32&lt;/code&gt; 를 사용하십시오 . 희소 행렬도 지원됩니다. 스파 스 &lt;code&gt;csr_matrix&lt;/code&gt; 사용 . 최대 효율성을 위해 를 .</target>
        </trans-unit>
        <trans-unit id="688f24dc1e25fac1dc510cf29e6e51a5cdee42ba" translate="yes" xml:space="preserve">
          <source>Input data used to build forests. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">포리스트를 구축하는 데 사용되는 입력 데이터. 사용 &lt;code&gt;dtype=np.float32&lt;/code&gt; 효율성을 극대화 를 .</target>
        </trans-unit>
        <trans-unit id="ece9df27fcdc2d8935842ef4ed6ac1e8d53f8b6c" translate="yes" xml:space="preserve">
          <source>Input data, of which specified subsets are used to fit the transformers.</source>
          <target state="translated">트랜스포머에 맞도록 지정된 서브 세트가 사용되는 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="8c020a67c0398f86d112987222d02c36283701ba" translate="yes" xml:space="preserve">
          <source>Input data, target values.</source>
          <target state="translated">입력 데이터, 목표 값.</target>
        </trans-unit>
        <trans-unit id="0d15fc28721eb2bcf2d53de59215da674d786463" translate="yes" xml:space="preserve">
          <source>Input data, used to fit transformers.</source>
          <target state="translated">변압기에 맞는 입력 데이터.</target>
        </trans-unit>
        <trans-unit id="aee2f5203193bb3069f6e2f7b08e833e91d53841" translate="yes" xml:space="preserve">
          <source>Input data, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">입력 데이터. 여기서 &lt;code&gt;n_samples&lt;/code&gt; 는 샘플 수이고 &lt;code&gt;n_features&lt;/code&gt; 는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="8f9c683e36e31c7c38c7e8d6a2daeccf181d4c94" translate="yes" xml:space="preserve">
          <source>Input data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">입력 데이터. 여기서 n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="260753716624ad2077f13f38ada17a33c0f48cba" translate="yes" xml:space="preserve">
          <source>Input data.</source>
          <target state="translated">입력 데이터.</target>
        </trans-unit>
        <trans-unit id="c414149f534c72aa4d2016c93404604f17aa41fd" translate="yes" xml:space="preserve">
          <source>Input data. Columns are assumed to have unit norm.</source>
          <target state="translated">입력 데이터. 열에는 단위 규범이 있다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="fc921091c020b61d18864b21129c4eb989ef6d69" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;None&lt;/code&gt;, the output will be the pairwise similarities between all samples in &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">입력 데이터. 경우 &lt;code&gt;None&lt;/code&gt; , 출력은 모든 샘플 사이의 페어 유사성 될 것입니다 &lt;code&gt;X&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="afaf08d18a1423dc1a78bf3492e5ce0eb50d8638" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt;, the input should be the dissimilarity matrix.</source>
          <target state="translated">입력 데이터. 경우 &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt; , 입력은 유사성 행렬이어야한다.</target>
        </trans-unit>
        <trans-unit id="c7e2acaca5145e47486c9c2928ab5532aee98fd4" translate="yes" xml:space="preserve">
          <source>Input data. If X is not provided, only the global clustering step is done.</source>
          <target state="translated">입력 데이터. X가 제공되지 않으면 글로벌 클러스터링 단계 만 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="d39fe8b9c923e952612de77f70ad5aff60c34542" translate="yes" xml:space="preserve">
          <source>Input object to check / convert.</source>
          <target state="translated">확인 / 변환 할 입력 객체.</target>
        </trans-unit>
        <trans-unit id="ca7fd27e447d5e335b2e80d1b2bb1406dceac239" translate="yes" xml:space="preserve">
          <source>Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised.</source>
          <target state="translated">확인 / 변환 할 입력 개체입니다. 2 차원 및 제곱이어야합니다. 그렇지 않으면 ValueError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="f24bf584af83fad1f47a5035ff05cdc62fd1daef" translate="yes" xml:space="preserve">
          <source>Input points.</source>
          <target state="translated">입력 포인트.</target>
        </trans-unit>
        <trans-unit id="71b2d21e1f2e71c0dcf88bd09dfe9ef6fd499ea3" translate="yes" xml:space="preserve">
          <source>Input targets</source>
          <target state="translated">입력 목표</target>
        </trans-unit>
        <trans-unit id="b58e4bce1b7ebb41f970a06dbe933522f9ad2c46" translate="yes" xml:space="preserve">
          <source>Input targets multiplied by X: X.T * y</source>
          <target state="translated">입력 대상에 X를 곱한 값 : XT * y</target>
        </trans-unit>
        <trans-unit id="69a0e9f3a009e8ccbba64367545e9fbe88306b19" translate="yes" xml:space="preserve">
          <source>Input targets.</source>
          <target state="translated">입력 대상.</target>
        </trans-unit>
        <trans-unit id="51c9f9fb1fac83429c51404b5e9b2caee57cc2f2" translate="yes" xml:space="preserve">
          <source>Input validation for standard estimators.</source>
          <target state="translated">표준 추정기에 대한 입력 검증.</target>
        </trans-unit>
        <trans-unit id="346da7aa7d7e4eb884706a35a69404b7d3fc9daa" translate="yes" xml:space="preserve">
          <source>Input validation on an array, list, sparse matrix or similar.</source>
          <target state="translated">배열,리스트, 희소 행렬 또는 이와 유사한 것에 대한 입력 검증.</target>
        </trans-unit>
        <trans-unit id="8344beaf285df55c120907e8b74a9a1f87253895" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are 4 independent features uniformly distributed on the intervals:</source>
          <target state="translated">입력 &lt;code&gt;X&lt;/code&gt; 는 간격에 균일하게 분포 된 4 개의 독립적 인 기능입니다.</target>
        </trans-unit>
        <trans-unit id="bcbcb56a88ddeef69ce01bcc9a78a456071e2cf0" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are independent features uniformly distributed on the interval [0, 1]. The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">입력 &lt;code&gt;X&lt;/code&gt; 는 간격 [0, 1]에 균일하게 분포 된 독립적 인 기능입니다. 출력 &lt;code&gt;y&lt;/code&gt; 는 다음 공식에 따라 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">이미 장착 된 LSH 포리스트에 새 데이터를 삽입합니다.</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">이미 장착 된 LSH 포리스트에 새 데이터를 삽입합니다. 비용은 새로운 총 크기에 비례하므로 추가를 일괄 처리해야합니다.</target>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">카디널리티 세트 'n choose k'를 사용하여 계산하는 대신, 여기서 n은 샘플 수이고 k는 서브 샘플 수 (적어도 피처 수)입니다. 'n을 선택한 경우 주어진 최대 크기의 확률 적 하위 모집단 만 고려하십시오. k '는 max_subpopulation보다 큽니다. 작은 문제 크기 이외의 경우이 매개 변수는 n_subsamples가 변경되지 않은 경우 메모리 사용량 및 런타임을 판별합니다.</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">벡터 결과를 제공하는 대신 LARS 솔루션은 매개 변수 벡터의 L1 표준의 각 값에 대한 솔루션을 나타내는 곡선으로 구성됩니다. 계수의 전체 경로는 어레이에 저장 &lt;code&gt;coef_path_&lt;/code&gt; (n_features max_features + 1)의 크기를 갖는다. 첫 번째 열은 항상 0입니다.</target>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">체인의 다양한 구성 요소의 매개 변수를 조정하는 대신 가능한 값의 그리드에서 최상의 매개 변수를 철저히 검색 할 수 있습니다. 우리는 idf의 유무에 관계없이 선형 SVM에 대해 0.01 또는 0.001의 페널티 매개 변수를 사용하여 단어 또는 bigrams에서 모든 분류자를 시도합니다.</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">대신 호출자는 명시 적으로 &lt;code&gt;with_centering=False&lt;/code&gt; 로 설정하고 (이 경우 CSR 매트릭스의 기능에서 분산 스케일링 만 수행됨 &lt;code&gt;X.toarray()&lt;/code&gt; 구체화 된 밀도 배열이 적합 할 것으로 예상되는 경우 X.toarray () 를 호출 해야합니다. 메모리에.</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">대신 호출자는 명시 적으로 &lt;code&gt;with_mean=False&lt;/code&gt; 로 설정하고 (이 경우 CSC 매트릭스의 기능에서 분산 스케일링 만 수행됨 &lt;code&gt;X.toarray()&lt;/code&gt; 구체화 된 밀도 배열이 적합 할 것으로 예상되는 경우 X.toarray () 를 호출 해야합니다. 메모리에.</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">대신, \ (w \)를 통한 분포는 축 평행, 타원형 가우스 분포 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">주어진 &lt;code&gt;encoding&lt;/code&gt; 아닌 문자를 포함하는 분석을 위해 바이트 시퀀스가 ​​제공되는 경우 수행 할 작업에 대한 지침 . 기본적으로 '엄격'이므로 UnicodeDecodeError가 발생합니다. 다른 값은 '무시'및 '바꾸기'입니다.</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">주어진 &lt;code&gt;encoding&lt;/code&gt; 아닌 문자를 포함하는 분석을 위해 바이트 시퀀스가 ​​제공되는 경우 수행 할 작업에 대한 지침 . bytes.decode에 키워드 인수 'errors'로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">레이블의 정수 배열. 제공하지 않으면 레이블은 y_true 및 y_pred에서 유추됩니다.</target>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">의사 결정 기능에 절편 (일명 바이어스)이 추가되었습니다.</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">차단 기간.</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">알고리즘에 의해 유지되는 충분한 내부 통계. 그것들을 유지하는 것은 진화의 역사를 잃어 버리지 않기 위해 온라인 환경에서 유용하지만, 최종 사용자에게는 사용해서는 안됩니다. A (n_components, n_components)는 사전 공분산 행렬입니다. B (n_features, n_components)는 데이터 근사 행렬입니다</target>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">내부적으로 Laplace 근사법은 가우시안이 아닌 가우스 뒤를 근사화하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">내부적으로 목표 &lt;code&gt;y&lt;/code&gt; 는 항상 2 차원 배열로 변환되어 사이 킷 학습 변압기에서 사용됩니다. 예측시 출력은 &lt;code&gt;y&lt;/code&gt; 와 동일한 수의 차원을 갖도록 재구성됩니다 .</target>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">내부적으로는 모든 계산을 처리하기 위해 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 과 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 를 사용합니다. 이 라이브러리는 C와 Cython을 사용하여 랩핑됩니다.</target>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">'Ctrl-C'를 사용하여 다중 프로세스 작업 중단</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">직관적 인 해석 : 불량 V 측정을 사용한 군집화 &lt;strong&gt;는 동종 성과 완전성 측면에서 질적으로 분석되어&lt;/strong&gt; 과제에 의해 발생하는 '종종'실수를 더 잘 느낄 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">직관적으로, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;정밀도&lt;/a&gt; 는 분류기에서 음성 인 샘플을 양성으로 표시하지 않는 능력이며, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;리콜&lt;/a&gt; 은 분류기에서 모든 양성 샘플을 찾는 능력입니다.</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">직관적으로, 히스토그램은 포인트 당 한 블록 씩 블록 스택으로 생각할 수 있습니다. 적절한 그리드 공간에 블록을 쌓아서 히스토그램을 복구합니다. 그러나 블록을 일반 그리드에 쌓는 대신 각 블록을 나타내는 점의 중심에 놓고 각 위치의 총 높이를 합하면 어떻게 될까요? 이 아이디어는 왼쪽 하단 시각화로 이어집니다. 히스토그램만큼 깨끗하지는 않지만 데이터가 블록 위치를 구동한다는 것은 기본 데이터를 훨씬 더 잘 표현한다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">직관적으로, &lt;code&gt;gamma&lt;/code&gt; 매개 변수는 단일 학습 예제의 영향이 도달하는 정도를 정의합니다. 낮은 값은 'far'를 의미하고 높은 값은 'close'를 의미합니다. &lt;code&gt;gamma&lt;/code&gt; 파라미터 지원 벡터로서 모델에 의해 선택된 샘플들의 영향 반경의 역으로 볼 수있다.</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">직관적으로,이 행렬은 의사 특징 (일부 거듭 제곱 된 점)의 행렬로 해석 될 수 있습니다. 행렬은 다항식 커널에 의해 유도 된 행렬과 유사하지만 (이와는 다릅니다).</target>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">정규화 강도의 역수; 양의 부동 소수점이어야합니다. 서포트 벡터 머신과 마찬가지로 값이 작을수록 정규화가 더 강력 해집니다.</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">변형을 거꾸로하십시오.</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">변형을 거꾸로하십시오. 각 피처 그룹에 할당 된 Xred 값으로 nb_features 크기의 벡터를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">역변환 행렬. &lt;code&gt;fit_inverse_transform&lt;/code&gt; 이 True 인 경우에만 사용 가능 합니다.</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">역변환 된 배열.</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">전달 된 추정기의 전달 된 메소드 이름을 호출합니다. method = 'predict_proba'의 경우 열은 정렬 된 순서로 클래스에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">호출 &lt;code&gt;fit&lt;/code&gt; 온 방법을 &lt;code&gt;VotingClassifier&lt;/code&gt; 은 클래스 속성에 저장됩니다 그 원래의 추정량의 클론 맞는 &lt;code&gt;self.estimators_&lt;/code&gt; 을 . 추정기는 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 &lt;code&gt;None&lt;/code&gt; 으로 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">가우스 프로세스 분류 (GPC)를위한 등 확률 라인</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">격리 숲 알고리즘</target>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">포레스트 예제</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">아이소 맵 임베딩</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">R의 동위 원소 최적화 : PAVA (Pool-Adjacent Violators Algorithm) 및 활성 설정 방법 Leeuw, Hornik, Mair Journal of Statistical Software 2009</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">등장 중간 회귀 : 선형 프로그래밍 접근법 Nilotpal Chakravarti 연산 수학 연구 Vol. 14, No. 2 (1989 년 5 월), pp. 303-308</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">등장 성 회귀</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">y의 등장 성.</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">등장 성 회귀 모델.</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">함수가 호출되거나 클래스가 인스턴스화 될 때 경고를 발행하고 docstring에 경고를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">현재 데이터에 적응합니다.</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">평가를 위해 여러 메트릭을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">또한 설정된 카디널리티 공식으로 표현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">&lt;code&gt;kernel&lt;/code&gt; 인수 로 직접 사용할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">또한 부울 랜덤 변수를 고려하는 추정기 (예 : 베이지안 설정에서 Bernoulli 분포를 사용하여 모델링)를위한 전처리 단계로 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">또한 숫자가 아닌 레이블을 해시 가능하고 비교 가능한 한 숫자 레이블로 변환하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">숫자가 아닌 레이블을 (해시 가능하고 비교 가능한 한) 숫자 레이블로 변환하는 데 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">또한 손실 함수에 정규화 항을 추가하여 모델 매개 변수를 축소하여 과적 합을 방지 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">잘린 SVD의 scipy.sparse.linalg ARPACK 구현을 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">매개 변수 &lt;code&gt;(estimator, X, y)&lt;/code&gt; 로 호출 할 수 있습니다 . 여기서 &lt;code&gt;estimator&lt;/code&gt; 는 평가해야하는 모델이고, &lt;code&gt;X&lt;/code&gt; 는 유효성 검증 데이터이며, &lt;code&gt;y&lt;/code&gt; 는 &lt;code&gt;X&lt;/code&gt; (감독 된 경우) 또는 &lt;code&gt;None&lt;/code&gt; (감독되지 않은 경우)의 기본 목표입니다. 케이스).</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 다운로드 /로드 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">항목 당 가중치 차이로 해석 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">k- 평균 (및 미니 배치 k- 평균)은 피처 스케일링에 매우 민감하며이 경우 IDF 가중치는 다음에 의해 제공된 &quot;지상 진실&quot;에 대해 측정 된만큼 클러스터링의 품질을 향상시키는 데 도움이됩니다. 20 개 뉴스 그룹 데이터 세트의 클래스 레이블 지정</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">두 개의 0이 아닌 계수를 가진 &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;OMP (Orthogonal Matching Pursuit)&lt;/a&gt; 의 결과는 하나만 유지할 때보 다 편향이 약간 적습니다 (가장자리가 덜 두드러짐). 또한 Frobenius 규범의 지상 진실과 더 가깝습니다.</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\ (\ nu \)-SVC 공식은 \ (C \)-SVC의 매개 변수화이므로 수학적으로 동등하다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">일 변량 기능 선택에 사용할 수 있습니다 ( &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;사용자 안내서 참조)&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">추정 절차에 정규화 매개 변수를 포함시키는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">학습 속도가 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">클러스터링 최적화의 목표로 사용할 단일 메트릭을 제공하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">[Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; 에서 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 신중하게 구속 될 때 데이터 기반의 부품 기반 표현을 생성하여 해석 가능한 모델을 생성 할 수 있음 이 관찰되었습니다 . 다음 예는 PCA 고유면과 비교하여 Olivetti면 데이터 세트의 이미지에서 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 찾은 16 개의 희소 성분을 표시합니다 .</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">컴퓨터 비전에서 자주 사용되는 지수 카이 제곱 커널과 유사한 속성을 갖지만, 기능 맵의 간단한 Monte Carlo 근사를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">노이즈없는 데이터에 적용되는 ML에서 유용한 것으로 입증되었습니다. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;간단히 말해서 양자 역학을위한 기계 학습을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">랜덤 키친 싱크의 변형을 구현합니다. [1]</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">정규화 기 이전에 L1으로 훈련 된 선형 모델입니다.</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt; 의 대안으로 제공되는 메모리 효율적인 온라인 학습 알고리즘 입니다. 그것은 잎 중심에서 클러스터 중심이 판독되는 트리 데이터 구조를 구성합니다. 이것들은 최종 군집 중심이거나 &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt; 과 같은 다른 군집 알고리즘에 대한 입력으로 제공 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">온라인 학습 방법에서 학습 속도를 제어하는 ​​매개 변수입니다. 점근 적 수렴을 보장하려면 값을 (0.5, 1.0]으로 설정해야합니다. 값이 0.0이고 batch_size가 &lt;code&gt;n_samples&lt;/code&gt; 인 경우 업데이트 방법은 배치 학습과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">95 % 통계 효율을 달성 하려면 매개 변수 &lt;code&gt;epsilon&lt;/code&gt; 을 1.35 로 설정하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">텍스트 처리 커뮤니티에서는 정규화 된 카운트 (일명 용어 빈도) 또는 TF-IDF 값이 지정된 기능이 실제로 약간 더 자주 수행되는 경우에도 이진 기능 값 (아마도 확률 론적 추론을 단순화하기 위해)을 사용하는 것이 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">분산 비율 기준이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">사전 및 / 또는 코드를 데이터에 존재할 수있는 제약 조건과 일치하도록 긍정적으로 제한 할 수도 있습니다. 다음은 서로 다른 양의 구속 조건이 적용된면입니다. 빨간색은 음수 값을, 파란색은 양수 값을, 흰색은 0을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">이웃 지점 사이의 연결을 보여주는 희소 그래프를 효율적으로 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">&lt;code&gt;output_distribution='normal'&lt;/code&gt; 을 설정 하여 &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하여 데이터를 정규 분포에 매핑 할 수도 있습니다 . 홍채 데이터 세트와 함께 앞의 예제를 사용하여 :</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">교차 검증 반복자를 대신 전달하여 다른 교차 검증 전략을 사용할 수도 있습니다. 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">아래 예와 같이 혼합 신호를 분리하는 데 일반적으로 사용됩니다 ( &lt;em&gt;블라인드 소스 분리&lt;/em&gt; 로 알려진 문제 ).</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">순방향 선택만큼 계산이 빠르며 일반 최소 제곱과 동일한 복잡도를 갖습니다.</target>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">F 점수로 변환 된 다음 p- 값으로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">올가미와 같은 다른 추정기에 대한 솔루션을 생성하도록 쉽게 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">분류자가 뉴스 그룹 헤더와 같이 20 개의 뉴스 그룹 데이터에 나타나는 특정 내용에 쉽게 맞출 수 있습니다. 많은 분류 기준은 매우 높은 F 점수를 얻지 만 결과는이 시대에 속하지 않은 다른 문서로는 일반화되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">형상 수가 매우 많은 경우 다른 차원 축소 방법 (예 : 밀도가 높은 데이터의 경우 PCA 또는 희소 데이터의 경우 TruncatedSVD)을 사용하여 차원 수를 적절한 양 (예 : 50)으로 줄이는 것이 좋습니다. 이것은 약간의 노이즈를 억제하고 샘플 사이의 페어 단위 거리 계산 속도를 높입니다. 자세한 내용은 Laurens van der Maaten의 FAQ [2]를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 방법으로 모형을 학습 할 때 레이블이없는 데이터와 함께 레이블이없는 점에 식별자를 지정하는 것이 중요합니다 . 이 구현이 사용하는 식별자는 정수 값 \ (-1 \)입니다.</target>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">샘플 수가 피처 수보다 훨씬 클 경우 수축이 필요하지 않을 것입니다. 이에 대한 직감은 모집단 공분산이 전체 순위 인 경우 표본 수가 증가하면 표본 공분산도 양의 명확한 값이된다는 것입니다. 결과적으로 수축이 필요하지 않으며 방법이 자동으로 수행해야합니다.</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">그것은 홍채에 대한 150 가지 관찰로 이루어지며, 각각 4 가지 특징, 즉 홍채와 꽃잎의 길이와 너비는 &lt;code&gt;iris.DESCR&lt;/code&gt; 에 자세히 설명되어 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">총 수에 비해 적은 수의 피처 만 선택하는 경우 (예 : 피처 수에 비해 샘플 수가 거의없는 경우) LassoCV보다 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">이러한 예측을 평가 지표에 전달하는 것은 적절하지 않습니다. &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt; 를 사용 하여 일반화 오류를 측정 하십시오 .</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">라이브러리에서 Parallel을 호출 할 때 백엔드 이름을 하드 코딩하지 않는 것이 좋습니다. 대신 라이브러리 사용자가 parallel_backend 컨텍스트 관리자를 사용하여 외부에서 백엔드를 변경할 수 있도록 소프트 힌트 (권장) 또는 하드 제한 조건 (필수)을 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">정규화 (벌칙)되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">p &amp;gt;&amp;gt; n 인 문맥에서 수치 적으로 효율적입니다 (즉, 치수의 수가 점의 수보다 훨씬 큰 경우).</target>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">낮은 특이 값과 관련된 성분의 특이 벡터를 제거하여 대부분의 분산을 유지하는 저 차원 공간에 데이터를 투영하는 것이 종종 흥미 롭습니다.</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">최상의 &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 점수를 얻기 위해 하이퍼 파라미터 공간을 검색하는 것이 가능하며 권장됩니다 .</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">이진 화기의 임계 값을 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">평균화 대신 레이블 별 정밀도, 리콜, F1- 점수 및 지원을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; 의사 난수 생성기 를 명시 적으로 시드하여 결과의 ​​재현성을 위해 난수를 제어 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">벡터화 생성자에 콜 러블을 전달하여 동작을 사용자 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;with_mean=False&lt;/code&gt; 또는 &lt;code&gt;with_std=False&lt;/code&gt; 를 &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; 생성자에 전달하여 센터링 또는 스케일링을 비활성화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">다음과 같이 범주 이름을 다시 가져올 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">학습 데이터에서 학습 한 변환의 정확한 특성을 찾기 위해 스케일러 속성을 조사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">로드 할 카테고리 목록을 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt; 함수 에 전달하여 카테고리의 하위 선택 만로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">동일한 실행에서 스파 스 배열과 밀도 배열을 혼합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&quot;해시 트릭&quot;(결합하여 그 한계를 극복 할 수 &lt;a href=&quot;#feature-hashing&quot;&gt;기능 해싱&lt;/a&gt; 에 의해 구현) &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; 의&lt;/a&gt; 클래스와 전처리 텍스트와의 토큰 기능 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Python의 내장 지속성 모델 &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; 을 사용하여 모델을 scikit-learn에 저장할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Python의 내장 지속성 모델, 즉 &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; 을 사용하여 모델을 scikit-learn에 저장할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">매개 변수 &lt;code&gt;categories&lt;/code&gt; 사용하여이를 명시 적으로 지정할 수 있습니다 . 데이터 셋에는 성별, 대륙 4 개, 웹 브라우저 4 개가 있습니다.</target>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">다운 스트림 모델이 피쳐의 선형 독립성을 추가로 가정 할 수 있으므로 피쳐를 독립적으로 중앙에 배치하고 크기를 조정하는 것만으로는 충분하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">특정 트랜스포머의 상태를 다시 사용할 수 있기 때문에 때때로 상태를 저장하는 것이 좋습니다. &lt;code&gt;GridSearchCV&lt;/code&gt; 에서 파이프 라인을 사용하면 이러한 상황이 트리거됩니다. 따라서 인수 &lt;code&gt;memory&lt;/code&gt; 를 사용하여 캐싱을 활성화합니다.</target>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">혼합 모델 학습을위한 가장 빠른 알고리즘입니다</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">큰 값이 클수록 반대입니다. 즉 큰 값은 이너에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">각 트리가 다른 트리와 독립적으로 구축되므로 RandomForests와 ExtraTrees를 여러 코어에 병렬로 장착 할 수 있습니다. AdaBoost의 샘플은 순차적으로 구축되므로 여러 코어를 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">전체 조각 별 선형 솔루션 경로를 생성하는데, 이는 교차 검증 또는 유사한 모델 조정 시도에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">테스트 점수 외에 적합 시간, 점수 시간 (및 선택적으로 훈련 점수 및 적합 추정량)이 포함 된 dict를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">다음과 같은 속성을 가진 사전과 유사한 객체를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">&lt;code&gt;y&lt;/code&gt; 를 참조하여 &lt;code&gt;X&lt;/code&gt; 의 &lt;code&gt;estimator&lt;/code&gt; 예측 품질 을 정량화하는 부동 소수점 숫자를 반환합니다 . 다시 말하지만, 일반적으로 숫자가 높을수록 득점자가 손실을 반환하면 해당 값을 무시해야합니다.</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">Johnson-Lindenstrauss lemma는 데이터 세트의 구조에 대한 가정을하지 않기 때문에 필요한 수의 구성 요소를 매우 보수적으로 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 을 사용 하여 숫자 데이터 세트에서 SVM으로 분류하기위한 RBF 커널의 기능 맵을 근사화하는 방법을 보여줍니다 . 원래 공간에서 선형 SVM을 사용한 결과, 근사 매핑을 사용하고 커널 화 된 SVM을 사용하는 선형 SVM을 비교합니다. 대략적인 매핑을 위해 다양한 양의 Monte Carlo 샘플링 ( &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 의 경우 임의 푸리에 기능을 사용 하는 경우 ) 및 트레이닝 세트의 다른 크기의 하위 집합 ( &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; )에 대한 타이밍 및 정확도 가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">텍스트 문서 모음을 토큰 발생 횟수 (또는 이진 발생 정보)를 보유하는 scipy.sparse 행렬로 변환합니다. norm = 'l1'인 경우 토큰 빈도로 정규화되거나 norm = 'l2'인 경우 유클리드 단위 영역에 투사 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">실수로 모델을 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">Halko 등의 방법으로 Full SVD의 LAPACK 구현 또는 임의 절단 된 SVD를 사용합니다. 입력 데이터의 모양과 추출 할 구성 요소의 수에 따라 2009 년.</target>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;return_X_y&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정하여 거의 모든 함수에서 데이터와 대상 만 포함하는 튜플로 출력을 제한 할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">커널 모양이 결과 분포의 부드러움에 어떤 영향을 미치는지 분명합니다. scikit-learn 커널 밀도 추정기는 다음과 같이 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">수렴 될 때까지 2와 3을 반복하십시오.</target>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">증거를 극대화하기위한 반복 절차</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">듀얼은</target>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;정밀 리콜과 ROC 곡선의 관계&lt;/a&gt; , ICML 2006.</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J. Friedman,&amp;ldquo;다변량 적응 회귀 스플라인&amp;rdquo;, 통계 연보 19 (1), 1-67, 1991 페이지.</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J. Friedman, Greedy Function Approximation : 그라디언트 부스팅 머신, 통계 분석, Vol. 29, No. 5, 2001.</target>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 : 희소 코딩을위한 온라인 사전 학습 ( &lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman, H. Qin 및 R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;무료 오픈 소스 소프트웨어 패키지에서 단어 목록 중지&amp;rdquo;&lt;/a&gt; . 에서 &lt;em&gt;발동. NLP 오픈 소스 소프트웨어 워크숍&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;멀티 클래스 AdaBoost&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">JR 퀸란. C4. 5 : 기계 학습을위한 프로그램. 1993 년 모건 카우프만</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;2 블록 사건에 중점을 둔 부분 최소 제곱 (PLS) 방법에 대한 조사&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">Jaccard 유사성 계수 점수</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">Jacob A. Wegelin. 2 블록 경우에 중점을 둔 부분 최소 제곱 (PLS) 방법에 대한 조사. 기술 보고서 ​​371, 시애틀 워싱턴 대학 통계학과, 2000.</target>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">Jesse Read, Bernhard Pfahringer, Geoff Holmes, 아이브 프랭크,</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,&amp;ldquo;다중 분류 분류를위한 분류기 체인&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">요한나 하딘, 데이비드 엠 로크 강력한 거리 분포. 전산 및 그래픽 통계 저널. 2005 년 12 월 1 일, 14 (4) : 928-946.</target>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">남자. D. Kelleher, Brian Mac Namee, Aoife D' Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;예측 데이터 분석을위한 머신 러닝의 기초 : 알고리즘, 실제 사례 및 사례 연구&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">남자. D. Kelleher, Brian Mac Namee, Aoife D' Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;예측 데이터 분석을위한 기계 학습의 기초 : 알고리즘,&lt;/a&gt; 실제 사례 및 사례 연구 , 2015.</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">멀티 태스킹 올가미를 통한 공동 피처 선택</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">조인트 파라미터 선택</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">기계 학습 연구 저널 15 (10 월) : 3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">1988 년 7 월</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">칠월; 1998</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman, Riccadonna, Furlanello, (2012). 멀티 클래스 예측에서 MCC와 CEN 에러 측정의 비교</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">교육, 사전 처리 (예 : 표준화, 기능 선택 등) 및 유사한 &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;데이터 변환&lt;/a&gt; 에서 보류 된 데이터에 대해 예측 변수를 테스트하는 것이 중요 하듯이 마찬가지로 트레이닝 세트에서 학습하고 예측을 위해 보류 데이터에 적용해야합니다. :</target>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue (a in b)와 같지만 더 좋은 기본 메시지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue (a가 아닌 b)와 비슷하지만 더 좋은 기본 메시지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inen 및 S. &amp;Auml;yr&amp;auml;m&amp;ouml; : &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;강력한 데이터 마이닝을위한 공간 중앙값 계산&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K (X, Y) = &amp;lt;X, Y&amp;gt; / (|| X || * || Y ||)</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">K- 폴드 교차 검증기</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">겹치지 않는 그룹이있는 K- 폴드 반복기 변형.</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K- 평균 군집화</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">사용할 K- 평균 알고리즘. 고전적인 EM 스타일 알고리즘은 &quot;풀&quot;입니다. &amp;ldquo;elkan&amp;rdquo;변형은 삼각형 부등식을 사용하여보다 효율적이지만 현재 희소 데이터는 지원하지 않습니다. &quot;auto&quot;는 밀도가 높은 데이터에 대해서는 &quot;elkan&quot;을 선택하고 희소 데이터에 대해서는 &quot;full&quot;을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">K- 평균은 벡터 양자화에 사용될 수있다. 이것은 훈련 된 &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt; 모델의 변환 방법을 사용하여 달성됩니다 .</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K- 평균 군집 알고리즘.</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K- 평균은 작고 동일한 대각선 공분산 행렬을 사용하는 기대 최대화 알고리즘과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K- 평균은 종종 로이드의 알고리즘으로 지칭된다. 기본적으로 알고리즘에는 세 단계가 있습니다. 첫 번째 단계는 초기 중심을 선택하며 가장 기본적인 방법은 데이터 세트 \ (X \)에서 \ (k \) 샘플을 선택하는 것입니다. 초기화 후 K- 평균은 다른 두 단계 사이에서 반복됩니다. 첫 번째 단계는 각 샘플을 가장 가까운 중심에 할당합니다. 두 번째 단계는 각각의 이전 중심에 할당 된 모든 샘플의 평균값을 취하여 새로운 중심을 생성합니다. 이전 중심과 새로운 중심 사이의 차이가 계산되고 알고리즘은이 값이 임계 값보다 작을 때까지 마지막 두 단계를 반복합니다. 즉, 중심이 크게 움직이지 않을 때까지 반복됩니다.</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K- 평균 양자화</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">빠른 일반화 된 N- 포인트 문제를위한 KDTree</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree (X, leaf_size = 40, metric = 'minkowski', ** kwargs)</target>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">Kappa 점수는 이진 또는 멀티 클래스 문제에 대해 계산할 수 있지만 다중 레이블 문제 (레이블 당 점수를 수동으로 계산하는 경우 제외)에는 적용되지 않으며 두 개 이상의 어노 테이터에는 적용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">3 개의 RGB 채널을 단일 회색 레벨 채널로 평균하는 대신 유지하십시오. 색상이 True 인 경우 데이터의 모양은 color = False 인 모양보다 차원이 하나 더 있습니다.</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">Ken Tang과 Ponnuthurai N. Suganthan 및 Xi Yao 및 A. Kai Qin. 관련 가중치 LDA를 사용한 선형 차원 감소 전기 전자 공학부 남양 기술 대학교 2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">종 분포의 커널 밀도 추정</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">커널 밀도 추정</target>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">커널 PCA</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">커널 PCA는 다음에서 소개되었습니다.</target>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">커널 주성분 분석 (KPCA)</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">커널 대역폭.</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">'rbf', 'poly'및 'sigmoid'에 대한 커널 계수.</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">rbf 커널에 대한 커널 계수.</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">rbf, poly 및 sigmoid 커널에 대한 커널 계수. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">rbf, poly, sigmoid, laplacian 및 chi2 커널에 대한 커널 계수. &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; 에 대해 무시되었습니다 .</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">scikit-learn의 커널 밀도 추정은 &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; 추정기 에서 구현되며, 효율적인 쿼리에 볼 트리 또는 KD 트리를 사용합니다 (자세한 내용은 &lt;a href=&quot;neighbors#neighbors&quot;&gt;가장 가까운 이웃&lt;/a&gt; 참조 ). 위의 예에서는 단순성을 위해 1D 데이터 세트를 사용하지만 실제로 차원의 저주로 인해 높은 차원에서 성능이 저하되지만 커널 밀도 추정은 여러 차원에서 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">로그-마진 가능성이 평가되는 커널 하이퍼 파라미터. None이면 self.kernel_.theta의 사전 계산 된 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">로그-마진 가능성이 평가되는 커널 하이퍼 파라미터. 멀티 클래스 분류의 경우, 세타는 복합 커널 또는 개별 커널의 하이퍼 파라미터 일 수 있습니다. 후자의 경우 모든 개별 커널에 동일한 세타 값이 할당됩니다. None이면 self.kernel_.theta의 사전 계산 된 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">커널 k (X, Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">대략적인 커널 맵. callable은 두 개의 인수와이 객체에 kernel_params로 전달 된 키워드 인수를 허용해야하며 부동 소수점 숫자를 리턴해야합니다.</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">내부적으로 사용되는 커널 매핑. callable은 두 개의 인수와이 객체에 kernel_params로 전달 된 키워드 인수를 허용해야하며 부동 소수점 숫자를 리턴해야합니다. 사전 계산 된 커널 매트릭스를 샘플 대신 추정기 메소드로 전달하려면 &quot;사전 계산&quot;으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">커널 매트릭스.</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">벡터 시스템 지원 또는 커널 PCA와 같은 커널 방법은 커널 힐버트 공간을 재현하는 속성에 의존합니다. 긍정적 인 확실한 커널 함수 \ (k \) (소위 Mercer 커널)의 경우, Hilbert 공간 \ (\ mathcal {H} \)에 대한 \ (\ phi \) 매핑이 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">대체 차원 공간으로 데이터를 투사하는 커널 방법</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">커널 운영자는 하나 또는 두 개의 기본 커널을 가져 와서 새로운 커널로 결합합니다. &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; 커널 개의 커널을 얻어 \ (K1 \)와 \ (K2 \)와 비아 콤바인들을 \ (K_ {합} (X, Y) = (K1) (X, Y) + K2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt; 커널이 커널을한다 \ (K1 \)와 \ (K2 \)와 통해 콤바인을 \ (K_ {제품} (X, Y) = K1 (X, Y) * K2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt; 커널이 하나의 기본 커널 통해 스칼라 파라미터 \ (지수 \) 및 콤바인들을 얻어 \ (K_ {EXP} (X, Y) = K (X, Y) ^ \ 텍스트 {지수} \).</target>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;KRR&lt;/a&gt; (Kernel Ridge Regression) [M2012] 는 &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (선형 최소 제곱과 l2-norm 정규화)을 커널 트릭과 결합합니다. 따라서 각 커널과 데이터에 의해 유도 된 공간에서 선형 함수를 학습합니다. 비선형 커널의 경우 원래 공간의 비선형 함수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">KRR (Kernel ridge Regression)은 릿지 회귀 (리니어 최소 제곱과 l2-norm 정규화)를 커널 트릭과 결합합니다. 따라서 각 커널과 데이터에 의해 유도 된 공간에서 선형 함수를 학습합니다. 비선형 커널의 경우 원래 공간의 비선형 함수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">커널 능선 회귀는 능선 회귀와 커널 트릭을 결합</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">커널 능선 회귀</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">모델에 사용할 커널 : 선형, 다항식, RBF, 시그 모이 드 또는 사전 계산.</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">모델에 사용할 커널 : 선형, 다항식, RBF, 시그 모이 드 또는 사전 계산. 기본적으로 'rbf'.</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">다른 커널 세트로 구성된 커널.</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">핵심. 기본값은&amp;ldquo;선형&amp;rdquo;입니다.</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">커널 (GP와 관련하여 &quot;공분산 함수&quot;라고도 함)은 GP의 앞뒤에있는 모양을 결정하는 GP의 중요한 구성 요소입니다. 이들은 유사한 데이터 포인트가 유사한 목표 값을 가져야한다는 가정과 결합 된 두 데이터 포인트의 &quot;유사성&quot;을 정의하여 학습중인 기능에 대한 가정을 인코딩합니다. 두 가지 범주의 커널을 구별 할 수 있습니다. 고정 커널은 두 데이터 포인트의 거리에만 의존하고 절대 값 \ (k (x_i, x_j) = k (d (x_i, x_j)) \)가 아니라 변환에 따라 변하지 않습니다 고정되지 않은 커널은 데이터 포인트의 특정 값에도 의존합니다. 정지형 커널은 등방성 및 이방성 커널로 세분 될 수 있으며, 등방성 커널도 ​​입력 공간의 회전에 영향을받지 않습니다. 상세 사항은,우리는 4 장을 참조한다&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">커널 &lt;code&gt;b&lt;/code&gt; 는 객체 &lt;code&gt;a&lt;/code&gt; 와 b 가 객체 &lt;code&gt;a&lt;/code&gt; 와 &lt;code&gt;c&lt;/code&gt; 보다&amp;ldquo;더 유사한&amp;rdquo;것으로 간주 되는 경우 유사성의 척도입니다 &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; 예 : s (a, b)&amp;gt; s (a, c)) . 커널은 또한 양의 반 한정이어야합니다.</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">커널은 벡터 \ (\ theta \) 하이퍼 파라미터로 매개 변수화됩니다. 이러한 하이퍼 파라미터는 예를 들어 커널의 길이 또는주기를 제어 할 수 있습니다 (아래 참조). 모든 커널 은 &lt;code&gt;__call__&lt;/code&gt; 메소드 에서 &lt;code&gt;eval_gradient=True&lt;/code&gt; 설정을 통해 \ (\ theta \)와 관련하여 커널의 자동 공분산 분석 그라디언트 계산을 지원합니다 . 이 그라디언트는 로그-마진 우도의 그라디언트를 계산할 때 가우시안 프로세스 (회귀 및 분류기)에 의해 사용되며, 이는 \ (\ theta \)의 값을 결정하는 데 사용됩니다. 그래디언트 상승을 통한 가능성. 각 하이퍼 파라미터에 대해 커널 인스턴스를 만들 때 초기 값과 범위를 지정해야합니다. \ (\ theta \)의 현재 값은 속성을 통해 가져오고 설정할 수 있습니다 &lt;code&gt;theta&lt;/code&gt; 커널 객체의 세타 . 또한 하이퍼 파라미터의 &lt;code&gt;bounds&lt;/code&gt; 는 커널 의 속성 범위 에 의해 액세스 될 수 있습니다 . 두 속성 (세타 및 경계)은 일반적으로 그래디언트 기반 최적화에 더 적합하기 때문에 내부적으로 사용되는 값의 로그 변환 된 값을 반환합니다. 각 hyperparameter의 사양의 인스턴스의 형태로 저장된다 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt; 각 커널이다. 이름이 &quot;x&quot;인 하이퍼 파라미터를 사용하는 커널에는 self.x 및 self.x_bounds 속성이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">Kevin P. Murphy&amp;ldquo;기계 학습 : 확률 론적 관점&amp;rdquo;, MIT Press chapter 14.4.3, pp. 492-493</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">키는 &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; &lt;/a&gt; 로 전달할 수있는 매개 변수 이름입니다 .</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">키워드 인수를 사용하면 이러한 기본값을 특정 데이터 세트에 맞출 수 있습니다 (매개 변수 &lt;code&gt;target_name&lt;/code&gt; , &lt;code&gt;data_name&lt;/code&gt; , &lt;code&gt;transpose_data&lt;/code&gt; 및 아래 예 참조).</target>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">지정된 메트릭 함수에 전달할 키워드 인수</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola 및 Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;대규모 멀티 태스킹 학습을위한 기능 해싱&lt;/a&gt; . Proc. ICML.</target>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma, Diederik 및 Jimmy Ba. &amp;ldquo;아담 : 확률 론적 방법</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger, Y., Basri, R., Chang, JT, &amp;amp; Gerstein, M. (2003). 마이크로 어레이 데이터의 스펙트럼 biclustering : coclustering 유전자 및 조건. 게놈 연구, 13 (4), 703-716.</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">Kluger, Yuval 등 al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;microarray 데이터의 스펙트럼 biclustering : 유전자와 조건을 coclustering&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">샘플 수만 알면 &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt; 은 랜덤 투영에 의해 발생하는 경계 왜곡을 보장하기 위해 랜덤 서브 스페이스의 최소 크기를 보수적으로 추정합니다.</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">코비 크 래머, 요람 싱어. 멀티 클래스 커널 기반 벡터 머신의 알고리즘 구현 기계 학습 연구 2 (2001), 265-292</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">최적화 후 Kullback-Leibler 발산.</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS는 함수의 2 차 부분 미분을 나타내는 헤 시안 행렬에 근사한 솔버입니다. 또한 Hessian 행렬의 역수에 근사하여 매개 변수 업데이트를 수행합니다. 구현은 Scipy 버전의 &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS를 사용&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L. Breiman,&amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, 123-140, 1996 쪽.</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L. Breiman,&amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24 (2), 123-140, 1996.</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L. Breiman.</target>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman, J. Friedman, R. Olshen 및 C. Stone,&amp;ldquo;분류 및 회귀 트리&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman, J. Friedman, R. Olshen 및 C. Stone. 분류 및 회귀 트리. 1984 년 캘리포니아 벨몬트의 워즈워스</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;하위 모델 선택 및 회귀 평가 : X- 랜덤 사례&lt;/a&gt; , 국제 통계 검토 1992;</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman 및 A. Cutler,&amp;ldquo;랜덤 포레스트&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">Robert C. Moore, John DeNero의 멀티 클래스 힌지 손실 모델에 대한 L1 및 L2 정규화.</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">로지스틱 회귀 분석에서의 L1 페널티 및 희소성</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">L1 규범 : \ (R (w) : = \ sum_ {i = 1} ^ {n} | w_i | \), 드문 드문 솔루션으로 이어집니다.</target>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">L2 규범 : \ (R (w) : = \ fra {{1} {2} \ sum_ {i = 1} ^ {n} w_i ^ 2 \),</target>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">L2 페널티 (규정 화 용어) 매개 변수.</target>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR &amp;ndash; 큰 선형 분류를위한 라이브러리</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR : 큰 선형 분류를위한 라이브러리</target>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">LSA는 잠재 시맨틱 인덱싱 (LSI)으로도 알려져 있지만, 정보 검색 목적으로 영구 인덱스에서 사용하는 것을 엄격하게 의미합니다.</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">LSH Forest는 대략적인 방법이므로 인덱싱 된 데이터 집합에서 일부 실제 인접 항목이 결과에서 누락 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">LSH Forest : 지역 민감성 해싱 포레스트 [1]은 바닐라 근사치의 가장 가까운 이웃 검색 방법에 대한 대체 방법입니다. LSH 포리스트 데이터 구조는 정렬 된 배열과 이진 검색 및 32 비트 고정 길이 해시를 사용하여 구현되었습니다. 코사인 거리에 근접한 해시 패밀리로 랜덤 프로젝션이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">인구의 LSTAT % 낮은 상태</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">라벨 전파 분류기</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">라벨 전파 숫자 활성 학습</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">레이블 전파 숫자 : 성능 시연</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">복잡한 구조를 배우는 레이블 전파</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">변환을 통해 각 항목에 할당 된 레이블.</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">긍정적 인 것으로 간주되는 라벨과 부정적인 것으로 간주되는 라벨.</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">LOF 점수 및 오염 매개 변수에 따라 레이블은 내부자에 대해 1이고 특이 치에 대해 -1입니다.</target>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">긍정적 클래스의 레이블. 없음 인 경우 최대 레이블이 양의 클래스로 사용됩니다</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">레이블 전파는 반 감독 그래프 추론 알고리즘의 몇 가지 변형을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">레이블 전파 모델에는 두 가지 기본 제공 커널 메소드가 있습니다. 커널의 선택은 알고리즘의 확장 성과 성능 모두에 영향을줍니다. 다음을 사용할 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">LRAP (Label Ranking Average Precision)는 다음 질문에 대한 답을 표본에 대해 평균화합니다. 각 근거리 레이블에 대해, 상위 레이블의 일부는 실제 레이블입니까? 각 샘플과 관련된 레이블에 더 나은 순위를 부여 할 수있는 경우이 성능 측정 값이 높아집니다. 획득 한 점수는 항상 0보다 엄격하고 최상의 값은 1입니다. 샘플 당 정확히 하나의 관련 레이블이있는 경우 레이블 순위 평균 정밀도는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;평균 역수 순위&lt;/a&gt; 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">LRAP (Label Ranking Average Precision)는 점수가 낮은 실제 레이블과 전체 레이블의 비율을 기준으로 각 샘플에 할당 된 각 기본 진리 레이블에 대한 평균입니다.</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">특이 치 샘플 (주어진 반경에 이웃이없는 샘플)에 대해 제공되는 레이블입니다. None으로 설정하면 특이 치가 감지 될 때 ValueError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">Semi-supervised learning을위한 LabelSpreading 모델</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">모든 클래스 멤버를 동일한 클러스터에 할당하는 레이블이 항상 완전한 것은 아니므로 처벌을받습니다.</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">모든 클래스 멤버를 동일한 클러스터에 할당하는 레이블은 균질하지 않으므로 불이익을받습니다.</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">같은 클래스에서 온 멤버를 가진 순수한 클러스터가있는 레이블은 균질하지만 불필요하게 분할하면 완성도에 해를 끼치며 V 측정에도 불이익을줍니다.</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">라벨이 지정된 데이터.</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">주어진 샘플에 가장 가까운 중심을 찾아서 새로운 샘플에 라벨을 붙입니다.</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">첫 번째 어노 테이터가 지정한 레이블.</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">두 번째 어노 테이터가 지정한 레이블. 카파 통계량은 대칭이므로 &lt;code&gt;y1&lt;/code&gt; 과 &lt;code&gt;y2&lt;/code&gt; 를 바꾸어도 값이 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">서브 클러스터가 전체적으로 클러스터 된 후 서브 클러스터의 중심에 지정된 레이블.</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">각 얼굴 이미지와 관련된 레이블. 이 레이블의 범위는 0-39이며 주제 ID에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">각 얼굴 이미지와 관련된 레이블. 이 레이블의 범위는 0-5748이며 개인 ID에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">각 이미지 쌍과 관련된 레이블. 두 레이블 값은 다른 사람이거나 같은 사람입니다.</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">X 라벨.</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">각 포인트의 라벨</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">각 포인트의 레이블 (comput_labels가 True로 설정된 경우).</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">각 지점의 레이블.</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">그룹 내 순열을 제한하는 레이블, 즉 &lt;code&gt;y&lt;/code&gt; 값은 동일한 그룹 식별자를 가진 샘플간에 순열됩니다. 지정하지 않으면 &lt;code&gt;y&lt;/code&gt; 값이 모든 샘플에서 순열됩니다.</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">큰 &lt;code&gt;n_clusters&lt;/code&gt; 및 &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">큰 &lt;code&gt;n_samples&lt;/code&gt; 및 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">큰 데이터 세트, 이상치 제거, 데이터 축소.</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">큰 특이 치</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Lars는 경로의 각 꼬임에 대해서만 경로 솔루션을 계산합니다. 결과적으로 꼬임이 적을 때 매우 효율적이며, 기능이나 샘플이 거의없는 경우입니다. 또한 메타 매개 변수를 설정하지 않고 전체 경로를 계산할 수 있습니다. 반대로 좌표 하강은 미리 지정된 그리드에서 경로 점을 계산합니다 (여기서는 기본값 사용). 따라서 그리드 점의 수가 경로의 꼬임 수보다 작은 경우 더 효율적입니다. 기능의 수가 실제로 많고 많은 양을 선택하기에 충분한 샘플이있는 경우 이러한 전략은 흥미로울 수 있습니다. 수치 적으로 오차가 큰 변수의 경우 Lars는 더 많은 오차를 누적하는 반면 좌표 하강 알고리즘은 그리드의 경로 만 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">올가미와 탄성 그물</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">희소 신호를위한 올가미 및 탄성 망</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">올가미 및 탄성 그물 (L1 및 L2 벌칙)은 좌표 하강을 사용하여 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">최소 각 회귀로 계산 된 올가미</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">정규화 경로를 따라 반복 피팅이있는 올가미 선형 모델</target>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">모델 선택을 위해 BIC 또는 AIC를 사용하여 Lars에 맞는 올가미 모델</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">최소 각도 회귀에 적합한 올가미 모델</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">최소 각도 회귀에 적합한 올가미 모델</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">올가미 모델 선택 : 교차 검증 / AIC / BIC</target>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">조밀하고 드문 데이터에 올가미</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">LARS를 사용한 올가미 경로</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">좌표 하강을 이용한 올가미 ( &lt;a href=&quot;linear_model#lasso&quot;&gt;올가미&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">Latent Dirichlet Allocation은 텍스트 코포 라와 같은 개별 데이터 집합의 수집을위한 생성 확률 모델입니다. 또한 문서 콜렉션에서 추상 주제를 발견하는 데 사용되는 주제 모델입니다.</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">온라인 변형 베이 즈 알고리즘을 사용한 Latent Dirichlet Allocation</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">데이터의 잠재 표현.</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">나중에 다음을 사용하여 절인 모델 (다른 Python 프로세스에서 가능)을 다시로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">나중에 다음을 사용하여 절인 모델을 다시로드 할 수 있습니다 (다른 Python 프로세스에서 가능).</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">위도 하우스 블록 위도</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; 로&lt;/a&gt; 전달되었습니다 . 이는 트리를 저장하는 데 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 BallTree 또는 KDTree로 전달되었습니다. 이는 트리 저장에 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 BallTree 또는 cKDTree로 전달되었습니다. 이는 트리 저장에 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">데이터 X에 대한 NMF 모델을 학습하고 변환 된 데이터를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">데이터 X에 대한 NMF 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">기능 이름 목록-&amp;gt; 인덱스 맵핑을 배우고 X를 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">기능 이름-&amp;gt; 인덱스 맵핑 목록을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">원시 문서에서 모든 토큰의 어휘 사전을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">열차 데이터의 치수 축소를 배우고 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">X에서 경험적 차이를 배웁니다.</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">Varial Bayes 방법으로 데이터 X에 대한 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">IDF 벡터 (전세계 항 가중치) 알아보기</target>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">사전 계산되지 않은 커널에 대한 역변환을 배우십시오. (즉, 포인트의 사전 이미지를 찾는 법을 배웁니다)</target>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">어휘 사전을 배우고 용어 문서 매트릭스를 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">훈련 세트에서 어휘와 IDF를 배우십시오.</target>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">어휘와 IDF를 배우고 용어 문서 매트릭스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">그래프 구조 학습</target>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">학습과 예측</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">학습 곡선.</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">학습 문제는 몇 가지 범주로 나뉩니다.</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">체중 업데이트에 대한 학습 속도 일정.</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;learning_rate&lt;/code&gt; 은 learning_rate 로 각 분류 자의 기여를 줄입니다 . &lt;code&gt;learning_rate&lt;/code&gt; 와 &lt;code&gt;n_estimators&lt;/code&gt; 사이에는 균형이 있습니다 .</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;learning_rate&lt;/code&gt; 은 learning_rate 로 각 회귀 변수의 기여를 줄입니다 . &lt;code&gt;learning_rate&lt;/code&gt; 와 &lt;code&gt;n_estimators&lt;/code&gt; 사이에는 균형이 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">예측 함수의 매개 변수를 학습하고 동일한 데이터에서 테스트하는 것은 방법 론적 실수입니다. 방금 본 샘플의 레이블을 반복하는 모델은 완벽한 점수를 얻지 만 아직 유용한 것은 예측하지 못합니다. 보이지 않는 데이터. 이 상황을 &lt;strong&gt;과적 합&lt;/strong&gt; 이라고 &lt;strong&gt;합니다&lt;/strong&gt; . 이를 피하기 위해 사용 가능한 데이터의 일부를 &lt;strong&gt;테스트 세트 &lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 로 유지하는 (감독 된) 기계 학습 실험을 수행 할 때 일반적입니다 . 상업적 실험에서도 기계 학습은 일반적으로 실험적으로 시작되기 때문에 &quot;실험&quot;이라는 단어는 학문적 용도만을 나타내는 것이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">연결하지 않고 각 출력을 독립적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">최소 각 회귀 모형</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">최소 각 회귀 모형 (일명 LAR)</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">희소 성분에 대한 데이터의 최소 제곱 투영.</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">최소 절대 편차 ( &lt;code&gt;'lad'&lt;/code&gt; ) : 회귀에 대한 강력한 손실 함수. 초기 모델은 목표 값의 중앙값으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">최소 제곱 ( &lt;code&gt;'ls'&lt;/code&gt; ) : 우수한 계산 특성으로 인해 회귀에 대한 자연스러운 선택입니다. 초기 모델은 목표 값의 평균으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">최소 제곱 : 릿지 회귀.</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">최소 각 회귀 ( &lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;최소 각 회귀&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">최소 각도 회귀 (LARS)는 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 개발 한 고차원 데이터에 대한 회귀 알고리즘입니다. LARS는 순차 단계적 회귀와 유사합니다. 각 단계에서 반응과 가장 관련성이 높은 예측 변수를 찾습니다. 동일한 예측자를 따라 계속 진행하는 대신 동일한 상관 관계를 갖는 여러 예측 변수가있는 경우 예측 변수간에 동일한 방향으로 진행됩니다.</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">하나의 그룹 아웃 교차 유효성 검사기 나가기</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">P 그룹을 교차 유효성 검사기에서 제외</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">P 그룹을 제외하십시오.</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">P 관측 값을 남겨 두십시오.</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">하나의 관찰을 남겨 두십시오.</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">Leave-One-Out 교차 유효성 검사기</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">Leave-P-Out 교차 검증기</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O, Wolf M. Honey, 표본 공분산 행렬을 축소했습니다. 포트폴리오 관리 저널 30 (4), 110-119, 2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolf는 수축률을 계산하는 특수한 형태의 수축 계수로,&amp;ldquo;대형 공분산 행렬에 대한 잘 추정 된 추정기&amp;rdquo;, Ledoit 및 Wolf, 다변량 분석 저널에 설명 된 O. Ledoit 및 M. Wolf의 공식을 사용하여 수축 계수를 계산합니다. , Volume 88, Issue 2, 2004 년 2 월, 365-411 쪽.</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Ledoit-Wolf 및 OAS 추정</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">LedoitWolf Estimator</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">반환 된 커널 k (X, Y)의 왼쪽 인수</target>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">경로의 길이입니다. &lt;code&gt;eps=1e-3&lt;/code&gt; 은 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 의미합니다.</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">경로의 길이입니다. &lt;code&gt;eps=1e-3&lt;/code&gt; 은 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">매개 변수 수에 대한 민감도 감소 :</target>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">K (x, z)를 phi (x) ^ T phi (z)에 의해 정의 된 커널이라고하자. 여기서 phi는 x를 힐버트 공간에 매핑하는 함수입니다. KernelCenterer는 phi (x)를 명시 적으로 계산하지 않고 데이터를 중심에 둡니다 (즉, 평균을 0으로 정규화). sklearn.preprocessing.StandardScaler (with_std = False)를 사용하여 phi (x)를 센터링하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">\ (S \)를 유사도 행렬로하고 \ (X \)를 \ (n \) 입력 점의 좌표로 지정하십시오. 불일치 \ (\ hat {d} _ {ij} \)는 최적의 방법으로 선택한 유사성의 변형입니다. 스트레스라고하는 목표는 \ (sum_ {i &amp;lt;j} d_ {ij} (X)-\ hat {d} _ {ij} (X) \)에 의해 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">노드 \ (m \)의 데이터를 \ (Q \)로 표시하십시오. 피처 \ (j \)와 임계 값 \ (t_m \)으로 구성된 각 후보 분할 \ (\ theta = (j, t_m) \)에 대해 데이터를 \ (Q_ {left} (\ theta) \)로 분할하고 \ (Q_ {right} (\ theta) \) 하위 집합</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">이제 겹쳐진 영역을 평균하여 패치에서 원본 이미지를 재구성 해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">첫 번째로로드 된 파일의 첫 번째 줄을 인쇄 해 봅시다 :</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">샘플 10, 25 및 50에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">샘플 10, 50 및 85에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">샘플 10, 80 및 140에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">&lt;code&gt;KFold&lt;/code&gt; 교차 검증 객체를 찾는 방법을 살펴 보겠습니다 .</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">가장 유익한 기능이 무엇인지 살펴 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">다음과 같은 카운트를 예로 들어 봅시다. 첫 번째 용어는 시간의 100 %이므로 그다지 흥미롭지 않습니다. 다른 두 가지 기능은 시간의 50 % 미만에 불과하므로 문서 내용을 더 잘 나타낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">기본 설정으로 다시 시도하십시오 :</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">이것을 사용하여 텍스트 문서의 최소한의 말뭉치의 단어 발생을 토큰 화하고 계산하십시오.</target>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">많은 scikit-learn 교차 유효성 검사 개체에 대한 교차 유효성 검사 동작을 시각적으로 비교해 보겠습니다. 아래에서는 몇 가지 일반적인 교차 유효성 검사 개체를 반복하여 각각의 동작을 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">자세한 수준.</target>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">3 절 BSD 라이센스에 따라 라이센스가 부여됩니다.</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman, M. (2013). UCI 머신 러닝 리포지토리 [ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]. 캘리포니아 어바인 : 캘리포니아 대학교, 정보 및 컴퓨터 과학부.</target>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">&lt;a href=&quot;tree#tree&quot;&gt;의사 결정 트리&lt;/a&gt; 와 마찬가지로 트리 의 포리스트도 &lt;a href=&quot;tree#tree-multioutput&quot;&gt;다중 출력 문제로&lt;/a&gt; 확장됩니다 (Y가 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 크기의 배열 인 경우 ).</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 과 마찬가지로 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 개별 단계를 대체 할 수 있으며 &lt;code&gt;'drop'&lt;/code&gt; 으로 설정하면 무시됩니다 .</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">MultinomialNB와 마찬가지로이 분류기는 이산 데이터에 적합합니다. 차이점은 MultinomialNB가 발생 횟수와 작동하지만 BernoulliNB는 이진 / 부울 기능을 위해 설계되었다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">fit (X) 다음에 transform (X)이 있지만 메모리에 X를 구체화 할 필요는 없습니다.</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">파이프 라인 및 FeatureUnion에서와 같이 트랜스포머 및 해당 매개 변수를 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 설정 하고 그리드 검색에서 검색 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">파이프 라인과 마찬가지로 피처 유니온에는 구성 요소를 명시 적으로 명명 할 필요가없는 &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt; 이라는 속기 생성자 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">스케일러와 마찬가지로 &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; 는&lt;/a&gt; 모든 기능을 동일하고 알려진 범위 또는 분포에 넣습니다. 그러나 순위 변환을 수행하면 비정상적인 분포가 완화되고 스케일링 방법보다 특이 치의 영향을 덜받습니다. 그러나 피처 내에서 또는 피처간에 거리와 상관 관계가 왜곡됩니다.</target>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">캐시 크기의 바이트 수로 제한합니다.</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">돌아 오는 이웃의 거리를 제한합니다. (기본값은 생성자에 전달 된 값입니다).</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">선형 판별 분석</target>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">선형 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) 및 2 차 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; )은 이름에서 알 수 있듯이 선형 및 2 차 결정 표면이있는 두 개의 클래식 분류기입니다.</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">LDA (Linear Discriminant Analysis)는 &lt;em&gt;클래스 간&lt;/em&gt; 차이 &lt;em&gt;가&lt;/em&gt; 가장 큰 속성을 식별하려고합니다 . 특히 LDA는 PCA와 달리 알려진 클래스 레이블을 사용하는 감독 방법입니다.</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">선형 판별 분석 및 2 차 판별 분석</target>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">L1로 정규화 기 (일명 올가미)로 훈련 된 선형 모델</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">선형 회귀 예제</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">Linear SVC는 기본적으로 확률 적 분류 기가 아니지만이 예제에서 사용 가능한 기본 제공 보정 옵션이 있습니다 ( &lt;code&gt;probability=True&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">선형 SVM</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">선형 합-모든 표본의 합을 보유하는 n 차원 벡터</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">선형 서포트 벡터 분류 ( &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; ) 방송 최대 마진 방법 (비교 니쿨 레스 쿠-Mizil 및 카루 아나 전형적이다 RandomForestClassifier 같은 더욱 S 자형 곡선 &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt; ), 근접 결정 경계한다 어려운 샘플들에있는 초점 ( 지원 벡터).</target>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">선형 지원 벡터 분류.</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">선형 지원 벡터 회귀.</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">공분산 타원체를 사용한 선형 및 2 차 판별 분석</target>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">SGD 교육이 포함 된 선형 분류기 (SVM, 로지스틱 회귀, ao)</target>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">특이 값 중심의 데이터 분해를 사용한 선형 차원 축소 가장 중요한 특이 벡터 만 데이터를 더 낮은 차원 공간에 투영하도록 유지합니다.</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">더 작은 차원 공간으로 데이터를 투영하기 위해 데이터의 특이 값 분해를 사용한 선형 차원 축소.</target>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">선형 커널 ( &lt;code&gt;kernel = 'linear'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">l2 정규화를 갖는 선형 최소 제곱.</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">SGD를 사용하여 정기적 인 경험적 손실을 최소화하여 장착 된 선형 모델</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">많은 회귀 분석기 각각의 개별 효과를 테스트하기위한 선형 모형. 이 기능은 독립형 기능 선택 절차가 아닌 기능 선택 절차에 사용되는 스코어링 기능입니다.</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">선형 모형 : 회귀에서 희소성까지</target>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">선형 모델 : \ (y = X \ beta + \ epsilon \)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">선형 회귀</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">특이 치에 강한 선형 회귀 모델.</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">L1 및 L2 사전을 정규화기로 결합한 선형 회귀</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">선형 능선 회귀</target>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">열차 테스트 입력을 포함하는 목록.</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">(name, class)의 목록. 여기서 &lt;code&gt;name&lt;/code&gt; 은 문자열 인 클래스 이름이고 &lt;code&gt;class&lt;/code&gt; 는 실제 클래스 유형입니다.</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">체인으로 연결된 순서대로 (이름, 변환) 튜플 (구현 적합 / 변형)의 목록이며 마지막 오브젝트는 추정기입니다.</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">데이터의 서브 세트에 적용 할 변환기 오브젝트를 지정하는 (이름, 변환기, 열 (들)) 튜플의 목록.</target>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">&lt;code&gt;n_features&lt;/code&gt; 차원 데이터 포인트 목록 . 각 행은 단일 데이터 포인트에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">y 벡터에 나타날 수있는 모든 클래스 목록.</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 알파가 자동으로 설정 &lt;code&gt;None&lt;/code&gt; 경우</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 알파가 자동으로 설정되지 않은 경우</target>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 제공되지 않으면 자동으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">용어 배열 목록.</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">내장 커널 목록.</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">로지스틱 회귀 모형의 계수 목록입니다. fit_intercept를 True로 설정하면 두 번째 치수는 n_features + 1이되며 마지막 항목은 절편을 나타냅니다. 용 &lt;code&gt;multiclass='multinomial'&lt;/code&gt; , 형상 (n_classes, n_cs, n_features) 또는 (n_classes, n_cs, n_features + 1)이다.</target>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">행렬을 인덱싱 할 레이블 목록입니다. 레이블의 하위 집합을 다시 정렬하거나 선택하는 데 사용할 수 있습니다. 아무 것도 지정하지 않으면 &lt;code&gt;y_true&lt;/code&gt; 또는 &lt;code&gt;y_pred&lt;/code&gt; 에 적어도 한 번 나타나는 항목 이 정렬 된 순서로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">행렬을 인덱싱 할 레이블 목록입니다. 레이블 하위 집합을 선택하는 데 사용할 수 있습니다. 없음 인 경우 &lt;code&gt;y1&lt;/code&gt; 또는 &lt;code&gt;y2&lt;/code&gt; 에 적어도 한 번 나타나는 모든 레이블 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">n_ 특징 차원 데이터 포인트 목록. 각 행은 단일 데이터 포인트에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">n_ 특징 차원 데이터 포인트 목록. 각 행은 단일 쿼리에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">슬라이스 가능성을 보장하는 객체 목록.</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">데이터 X에 첨부 된 샘플 중량 목록</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">샘플 목록.</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">데이터에 적용 할 변환기 객체 목록입니다. 각 튜플의 처음 절반은 변압기의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">정규화 매개 변수의 값 목록 또는 사용해야하는 정규화 매개 변수 수를 지정하는 정수 이 경우 매개 변수는 1e-4와 1e4 사이의 로그 스케일로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">점수를 계산할 가중치 유형 목록입니다. 없음은 가중치가 없음을 의미합니다. &quot;선형&quot;은 선형 가중을 의미하고; &quot;2 차&quot;는 2 차 가중을 의미한다.</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming 및 Zhou, Zhi-Hua. &amp;ldquo;격리 숲&amp;rdquo; 데이터 마이닝, 2008. ICDM'08. 여덟 번째 IEEE 국제 회의.</target>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming 및 Zhou, Zhi-Hua. &amp;ldquo;격리 기반 이상 탐지.&amp;rdquo; 데이터의 지식 발견에 대한 ACM 거래 (TKDD) 6.1 (2012) : 3.</target>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt; 에서 다운로드 한 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">보스턴 주택 가격 데이터 집합을로드하고 반환합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">유방암 위스콘신 데이터 세트를로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">당뇨병 데이터 집합을로드하고 반환합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">숫자 데이터 집합을로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">홍채 데이터 세트를로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">linnerud 데이터 세트를로드하고 반환합니다 (다변량 회귀).</target>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">와인 데이터 셋을로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">SVMlight 형식으로 여러 파일에서 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">svmlight / libsvm 형식의 데이터 세트를 스파 스 CSR 매트릭스에로드</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">이미지 조작을 위해 샘플 이미지를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">카테고리가있는 텍스트 파일을 하위 폴더 이름으로로드하십시오.</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트를로드하고이를 토큰 수 (분류)로 벡터화하십시오.</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">캘리포니아 주택 데이터 세트를로드합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">와일드 한 (LFW) 쌍 데이터 세트 (분류)에 레이블이있는면을로드합니다.</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">Wild (LFW) people 데이터 세트 (분류)에 레이블이있는면을로드합니다.</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">AT &amp;amp; T (분류)에서 Olivetti면 데이터 세트를로드합니다.</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">RCV1 다중 레이블 데이터 세트 (분류)를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">표지 유형 데이터 세트 (분류)를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트 (분류)에서 파일 이름 및 데이터를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">kddcup99 데이터 세트를로드하십시오 (분류).</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">단일 샘플 이미지의 numpy 배열을로드합니다</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">Phillips 등의 종 분포 데이터 세트 로더</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">Phillips 등의 종 분포 데이터 세트 로더 알. (2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">예제 데이터 셋로드</target>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">외부 데이터 세트에서로드</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">20 개 뉴스 그룹 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;china&lt;/code&gt; 과 &lt;code&gt;flower&lt;/code&gt; 모두로드합니다 .</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">국부적으로 선형 임베딩</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">LLE (Locally Linear Embeding)는 로컬 주변 거리를 유지하는 데이터의보다 낮은 차원의 투영을 찾습니다. 최고의 비선형 임베딩을 찾기 위해 전 세계적으로 비교되는 일련의 로컬 주성분 분석으로 생각할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 함수 또는 객체 지향 대응 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 로컬 선형 임베딩을 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">X가 주어진 가우스 혼합의 로그 가능성.</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">p = 0 또는 p = 1에 대해 로그 손실이 정의되지 않으므로 확률은 max (eps, min (1-eps, p))로 잘립니다.</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">로그 손실, 일명 물류 손실 또는 교차 엔트로피 손실.</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">로지스틱 회귀 손실 또는 교차 엔트로피 손실이라고도하는 로그 손실은 확률 추정치에 정의됩니다. 그것은 (다항식) 로지스틱 회귀 및 신경망과 기대 최대화의 일부 변형에서 일반적으로 사용되며, 이산 예측 대신 분류기 의 확률 출력 ( &lt;code&gt;predict_proba&lt;/code&gt; ) 을 평가하는 데 사용될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">확률 추정치의 로그입니다.</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">X에서 각 데이터 포인트의 확률을 기록합니다.</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">각 클래스의 로그 확률 (부드럽게)</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">현재 모델에서 각 샘플의 로그 우도</target>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">접힌 데이터에 대한 로그 아웃 가능성 점수</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">훈련 데이터에 대한 로그의 한계 확률.</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">로그 : 로지스틱 회귀.</target>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">로지스틱 회귀 (일명 logit, MaxEnt) 분류기.</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">로지스틱 회귀 3 클래스 분류기</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">로지스틱 회귀 CV (일명 logit, MaxEnt) 분류기.</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">물류 기능</target>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">원시 픽셀 값에 대한 로지스틱 회귀가 비교를 위해 제공됩니다. 이 예는 BernoulliRBM에서 추출한 기능이 분류 정확도를 향상시키는 데 도움이된다는 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">내장 된 교차 검증을 통한 로지스틱 회귀</target>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">로지스틱 회귀는 이름에도 불구하고 회귀가 아닌 분류에 대한 선형 모형입니다. 로지스틱 회귀는 문헌에서 로짓 회귀, 최대 엔트로피 분류 (MaxEnt) 또는 로그 선형 분류기로도 알려져 있습니다. 이 모델에서 단일 시도의 가능한 결과를 설명하는 확률은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;로지스틱 함수를&lt;/a&gt; 사용하여 모델링 됩니다 .</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegression은 로그 손실을 직접 최적화하므로 잘 보정 된 예측을 반환합니다. 반대로, 다른 방법은 방법마다 다른 바이어스를 사용하여 바이어스 된 확률을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">경도 집 블록 경도</target>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">저수준 방법</target>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">추론에 가장 적합한 가능성 (모델에 대한 훈련 데이터의)에 대한 하한값.</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">EM에 가장 잘 맞는 (모델에 대한 훈련 데이터의) 로그 우도에 대한 하한값.</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;X_train_&lt;/code&gt; 에서 커널의 삼각 삼각형 Cholesky 분해</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M. Bawa, T. Condie 및 P. Ganesan,&amp;ldquo;LSH Forest : 유사 검색을위한 자체 조정 색인&amp;rdquo;, WWW '05 World Wide Web에 관한 제 14 차 국제 회의 절차, 651-660, 2005.</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumont et al., &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;무작위 서브 윈도우 및 다중 출력 무작위 트리가있는 고속 멀티 클래스 이미지 주석&lt;/a&gt; , 2009 국제 컴퓨터 비전 이론 및 응용 프로그램 컨퍼런스</target>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M. Everingham, L. Van Gool, CKI Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt; , IJCV 2010.</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">MAE 출력은 음이 아닌 부동 소수점입니다. 가장 좋은 값은 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV 주택 소유 주택의 평균 가치 $ 1000</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLP는 비선형 모델을 훈련 데이터에 맞출 수 있습니다. &lt;code&gt;clf.coefs_&lt;/code&gt; 는 모델 매개 변수를 구성하는 가중치 행렬을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">MLP는 기능 스케일링에 민감합니다.</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">MLP는 숨겨진 뉴런, 레이어 및 반복 횟수와 같은 여러 하이퍼 파라미터를 조정해야합니다.</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">MLP는 2 개의 배열에 대해 학습한다 : 크기의 배열 X (n_samples, n_features)는 부동 소수점 특징 벡터로 표현 된 학습 샘플을 보유하고; 및 훈련 샘플에 대한 목표 값 (클래스 라벨)을 보유하는 크기 (n_samples)의 배열 y :</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">사용 MLP 열차 &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;확률 그라데이션 하강&lt;/a&gt; , &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;아담&lt;/a&gt; , 또는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS를&lt;/a&gt; . Stochastic Gradient Descent (SGD)는 적응이 필요한 매개 변수와 관련하여 손실 함수의 구배를 사용하여 매개 변수를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">역 전파를 사용하는 MLP 훈련 보다 정확하게는, 어떤 형태의 그라디언트 디센트를 사용하여 학습하고 그라디언트는 역 전파를 사용하여 계산됩니다. 분류를 위해 교차 엔트로피 손실 함수를 최소화하여 샘플 \ (x \) 당 확률 추정값 \ (P (y | x) \) 벡터를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">MLP는 문제 유형에 따라 다른 손실 기능을 사용합니다. 분류에 대한 손실 함수는 교차 엔트로피이며, 이진 경우에는</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">숨겨진 레이어가있는 MLP에는 로컬 최소값이 두 개 이상인 볼록하지 않은 손실 기능이 있습니다. 따라서 다른 임의 가중치 초기화는 다른 유효성 검사 정확도로 이어질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPClassifier는 각 시간 단계에서 모델 파라미터에 대한 손실 함수의 부분 미분 값이 계산되어 파라미터를 업데이트하기 때문에 반복적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPRegressor는 각 시간 단계에서 모델 매개 변수에 대한 손실 함수의 부분 미분 값이 계산되어 매개 변수를 업데이트하므로 반복적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">다항 로지스틱 + L1을 사용한 MNIST 분류</target>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">머신 러닝 알고리즘에는 데이터가 필요합니다. 각 &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; 하위 폴더로 이동 하여 &lt;code&gt;fetch_data.py&lt;/code&gt; 스크립트를 실행하십시오 (먼저 읽은 후).</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">기계 학습은 데이터 세트의 일부 특성을 학습 한 다음 다른 데이터 세트에 대해 해당 특성을 테스트하는 것입니다. 머신 러닝의 일반적인 방법은 데이터 세트를 2 개로 분할하여 알고리즘을 평가하는 것입니다. 우리는 그 세트 중 하나를 &lt;strong&gt;훈련 세트&lt;/strong&gt; 라고 부릅니다 . 우리는 다른 세트를 &lt;strong&gt;테스트 세트&lt;/strong&gt; 라고 부릅니다.이 세트 에서 학습 된 속성을 테스트합니다.</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">기계 학습 : 문제 설정</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">훈련 세트의 Mahalanobis 거리 ( &lt;code&gt;fit&lt;/code&gt; ) 관찰.</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">마할 라 노비스 중심까지 거리</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">입력 데이터를 복사하십시오.</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">2d에서 더 작은 원을 포함하는 큰 원을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">성과 지표 또는 손실 함수로 득점자를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">교차 유효성 검사를 위해 배열을 색인 가능하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">X의 첫 번째 축에 최소 개수의 샘플이 있는지 확인하십시오 (2D 배열의 경우 행).</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">배열이 2D, 정사각형 및 대칭인지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D 어레이에 최소 개수의 기능 (열)이 있는지 확인하십시오. 기본값 1은 빈 데이터 세트를 거부합니다. 이 검사는 X가 효과적으로 2 차원이거나 원래 1D이고 &lt;code&gt;ensure_2d&lt;/code&gt; 가 True 인 경우에만 적용됩니다 . 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D 어레이에 최소 개수의 기능 (열)이 있는지 확인하십시오. 기본값 1은 빈 데이터 세트를 거부합니다. 이 확인은 입력 데이터가 실제로 2 차원이거나 원래 1D이고 &lt;code&gt;ensure_2d&lt;/code&gt; 가 True 인 경우에만 적용됩니다 . 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">배열의 첫 번째 축에 최소 개수의 샘플이 있는지 확인하십시오 (2D 배열의 경우 행). 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">모든 기능에서 동일한 스케일이 사용되는지 확인하십시오. 매니 폴드 학습 방법은 가장 가까운 이웃 검색을 기반으로하기 때문에 알고리즘이 제대로 수행되지 않을 수 있습니다. 이기종 데이터를 확장하는 편리한 방법은 &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">모델을 피팅하기 전에 학습 데이터를 퍼 뮤트 (셔플)하거나 &lt;code&gt;shuffle=True&lt;/code&gt; 를 사용 하여 각 반복 후에 셔플하십시오.</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">두 개의 인터리빙 반원 만들기</target>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">능금산:</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">능금산</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">매니 폴드 학습은 PCA와 같은 선형 프레임 워크를 데이터의 비선형 구조에 민감하게 일반화하려는 시도로 생각할 수 있습니다. 감독되는 변형이 존재하지만 일반적인 매니 폴드 학습 문제는 감독되지 않습니다. 미리 정해진 분류를 사용하지 않고 데이터 자체에서 데이터의 고차원 구조를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">잘린 영역의 매니 폴드 학습 방법</target>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">매니 폴드 학습은 비선형 차원 축소 방법입니다. 이 작업의 알고리즘은 많은 데이터 세트의 차원이 인위적으로 만 높다는 아이디어를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">자필 자릿수에 대한 매니 폴드 학습 : 로컬 선형 임베딩, 아이소 맵&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">많은 응용 프로그램에서 새 관측치가 기존 관측치와 동일한 분포에 속하는지 ( &lt;em&gt;inlier&lt;/em&gt; ) 또는 다른 것으로 간주되어야하는지 ( &lt;em&gt;outlier&lt;/em&gt; ) 결정할 수 있어야합니다 . 종종이 기능은 실제 데이터 세트를 정리하는 데 사용됩니다. 두 가지 중요한 차이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">많은 클러스터, 가능한 연결 제한</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">많은 클러스터, 연결 제약 조건, 비 유클리드 거리</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">많은 클러스터, 고르지 않은 클러스터 크기, 평평하지 않은 기하학</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">많은 데이터 세트에는 텍스트, 부동 및 날짜와 같은 다양한 유형의 기능이 포함되어 있으며 각 유형의 기능에는 별도의 전처리 또는 기능 추출 단계가 필요합니다. scikit-learn 메소드를 적용하기 전에 (예 : &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; 사용) 데이터를 사전 처리하는 것이 가장 쉬운 경우가 많습니다 . scikit-learn으로 데이터를 전달하기 전에 데이터를 처리하는 것은 다음 이유 중 하나로 인해 문제가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt; 와 같은 추가 매개 변수가 필요하기 때문에 많은 메트릭에 &lt;code&gt;scoring&lt;/code&gt; 값 으로 사용할 이름이 제공되지 않습니다 . 이러한 경우 적절한 점수 매기기 개체를 생성해야합니다. 스코어링을 위해 호출 가능한 객체를 생성하는 가장 간단한 방법은 &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; 를 사용하는 것 입니다. 이 기능은 메트릭을 모델 평가에 사용할 수있는 호출 가능으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">많은 통계적 문제는 모집단 공분산 행렬의 추정을 필요로하며, 이는 데이터 세트 산점도 형태의 추정으로 볼 수 있습니다. 대부분의 경우, 이러한 추정은 특성 (크기, 구조, 동질성)이 추정 품질에 큰 영향을 미치는 샘플에서 수행해야합니다. &lt;code&gt;sklearn.covariance&lt;/code&gt; 의 패키지는 정확하게 다양한 설정에서 인구의 공분산 행렬을 추정 할 수있는 도구를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">많은, 더 많은&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">정규 분포에 데이터 매핑</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">전력 변환을 사용하여 데이터를 정규 분포에 매핑합니다.</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; 매개 변수를 사용하여 데이터를 표준 정규 분포에 매핑 합니다.</target>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">i로 인코딩 된 값이 목록에서 i 번째가되도록 각 범주 형 기능 이름을 값 목록에 맵핑합니다.</target>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">변환 된 데이터의 한계 분포. 선택 사항은 'uniform'(기본값) 또는 'normal'입니다.</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">Mark Schmidt, Nicolas Le Roux 및 Francis Bach : &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;확률 평균 그라디언트를 사용하여 유한 합계 최소화&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">X에서 사용할 마스크</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">Matern 커널.</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로 정규화 기 이전에 \ (\ ell_1 \)로 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로, 이것은 정규화 자로 \ (\ ell_1 \) \ (\ ell_2 \)와 \ (\ ell_2 \)를 혼합하여 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로 정규화 기 이전에 혼합 된 \ (\ ell_1 \) \ (\ ell_2 \)로 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">수학적으로이 수축은 경험적 공분산 행렬의 최소값과 최대 값 사이의 비율을 줄이는 것으로 구성됩니다. 주어진 오프셋에 따라 모든 고유 값을 간단히 이동하여 수행 할 수 있습니다. 이는 공분산 행렬의 l2-penalized Maximum Likelihood Estimator를 찾는 것과 같습니다. 실제로 수축은 간단한 볼록 변환으로 요약됩니다. \ (\ Sigma _ {\ rm shrunk} = (1- \ alpha) \ hat {\ Sigma} + \ alpha \ frac {{\ rm Tr} \ hat {\ 시그마}} {p} \ rm ID \).</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">수학적으로, 학습 샘플 \ (X \)에 적용된 잘린 SVD는 낮은 순위 근사화 \ (X \)를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">\ (C_ {i, j} \)가 실제 클래스 \ (i \) 및 예측 된 클래스 \ (j \)의 샘플 수인 행렬 \ (C \). 경우 &lt;code&gt;eps is None&lt;/code&gt; ,이 배열의 DTYPE은 정수가됩니다. 경우 &lt;code&gt;eps&lt;/code&gt; 주어진다는 DTYPE은 플로트 될 것입니다. 수 있을까요 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 경우 &lt;code&gt;sparse=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">점 사이의 유사성의 행렬</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">스케일링 할 매트릭스.</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">분해 할 행렬</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">피처의 분산을 사용하여 정규화 할 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">두 개의 열을 교환 할 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">두 개의 행을 바꾸는 행렬.</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">E- 단계에서 문서 주제 분배를 업데이트하기위한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">ELBO를 최대화하는 것은 \ (q (z, \ theta, \ beta) \)와 실제 후부 \ (p (z, \ theta, \ beta | w, \ alpha, \ eta) \).</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">목표 평균을 뺀 후 로그 한계 우도를 최대화하면 LML이 -83.214 인 다음 커널이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">Johnson-Lindenstrauss lemma에 의해 정의 된 최대 왜곡률. 배열이 제공되면 안전한 배열의 구성 요소를 배열별로 계산합니다.</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">최대 우도 공분산 추정기</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">잔차의 최대 표준입니다. None이 아닌 경우 n_nonzero_coefs를 대체합니다.</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">각 노드에서 최대 CF 서브 클러스터 수 서브 클러스터 수가 branching_factor를 초과하도록 새 샘플이 입력되면 해당 노드는 서브 클러스터가 각각 재분배 된 두 개의 노드로 분할됩니다. 해당 노드의 부모 하위 클러스터가 제거되고 두 개의 새 하위 클러스터가 두 개의 분할 노드의 부모로 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">&lt;code&gt;tol&lt;/code&gt; 개선을 충족시키지 못하는 최대 에포크 수 . solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">최대 반복 횟수</target>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">시간이 초과되기 전의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">맞추는 동안 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">arpack의 최대 반복 횟수입니다. None이면 arpack에서 최적의 값을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 'sparse_cg'및 'lsqr'솔버의 경우 기본값은 scipy.sparse.linalg에 의해 결정됩니다. 'sag'솔버의 경우 기본값은 1000입니다.</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 'sparse_cg'및 'lsqr'솔버의 경우 기본값은 scipy.sparse.linalg에 의해 결정됩니다. 'sag'및 saga 솔버의 경우 기본값은 1000입니다.</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 기본값은 scipy.sparse.linalg에 의해 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">무작위 샘플 선택에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">arpack 솔버의 최대 반복 횟수입니다. eigen_solver == 'dense'인 경우 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">공간 중앙값 계산을위한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">최적화를위한 최대 반복 횟수입니다. 250 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">솔버에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">단일 실행에 대한 SMACOF 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">단일 실행에 대한 k- 평균 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">실행할 k- 평균 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">최적화 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">초기 중지 기준 추론과 독립적으로 중지하기 전에 전체 데이터 세트에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">될 수 반복의 최대 수는 0 라이어 또는에 의해 정의 된 유효하지 않은 데이터 발견으로 인해 생략 &lt;code&gt;is_data_valid&lt;/code&gt; 에 의해 정의되거나 잘못된 모델 &lt;code&gt;is_model_valid&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">scipy.optimize.fmin_l_bfgs_b를 실행해야하는 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 경우 수행 할 최대 반복 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">Lars 알고리즘에서 수행 할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">수행 할 최대 반복 횟수로, 무제한으로 무한대로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">수행 할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">수행 할 최대 반복 횟수입니다. 조기 정지에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">최적화를 중단하기 전에 진행되지 않은 최대 반복 횟수로, 과장된 초기 250 번의 반복 이후에 사용됩니다. 진행률은 50 번 반복 될 때마다 확인되므로이 값은 50의 다음 배수로 반올림됩니다.</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">아직 수렴되지 않은 경우 클러스터링 작업이 종료되기 전 (해당 시드 지점에 대해) 시드 지점 당 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">최대 반복 횟수입니다. 기본값은 300</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">최대 반복 횟수입니다. 기본값은 300입니다.</target>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">최대 반복 횟수입니다. 솔버는 수렴 ( 'tol'으로 결정) 또는이 반복 횟수까지 반복됩니다. 확률 적 솔버 ( 'sgd', 'adam')의 경우, 그래디언트 단계 수가 아니라 에포크 수 (각 데이터 포인트가 몇 번 사용되는지)를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">계산 효율의 Quantile을 추정하는 데 사용되는 최대 샘플 수입니다. 서브 샘플링 절차는 값이 동일한 희소 행렬과 밀도가 높은 행렬에 따라 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">수행 할 최대 반복 횟수이므로 포함 할 최대 기능입니다. &lt;code&gt;n_features&lt;/code&gt; 10 % 이지만 사용 가능한 경우 5 이상입니다.</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; 또는 &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; 인 경로의 노드 수 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; 또는 &lt;code&gt;alpha&lt;/code&gt; 보다 큰 상관 관계가있는 경로의 노드 수 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 또는 &lt;code&gt;n_features&lt;/code&gt; 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">데이터 샘플이 inlier로 분류되는 최대 잔차입니다. 기본적으로 임계 값은 목표 값 &lt;code&gt;y&lt;/code&gt; 의 MAD (중앙 절대 편차)로 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">단일 트레이닝 세트의 최대 크기.</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">샘플에 대한 X의 최대 제곱합. SAG 솔버에서만 사용됩니다. None이면 모든 샘플을 통과하여 계산됩니다. 교차 검증 속도를 높이려면 값을 미리 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">최대 단계 크기 (규정 화). 기본값은 1.0입니다.</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">담즙의 최대 값.</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">오른쪽 경계에 대한 입력 배열 &lt;code&gt;X_&lt;/code&gt; 의 최대 값 .</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">Jaccard 계수를 사용하는 문자열&amp;ldquo;jaccard&amp;rdquo;또는 4 개의 인수를 사용하는 함수일 수 있으며 각 인수는 1d 표시기 벡터입니다 (a_rows, a_columns, b_rows, b_columns).</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">( 'headers', 'footers', 'quotes')의 하위 집합을 포함 할 수 있습니다. 이들 각각은 뉴스 그룹 게시물에서 감지 및 제거되어 분류자가 메타 데이터에 과적 합하지 못하게하는 일종의 텍스트입니다.</target>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">평균 절대 오차 :</target>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">모든 샘플에 대한 평균 실루엣 계수.</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">평균 제곱 오차 :</target>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">평균 절대 오차 회귀 손실</target>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict (X) wrt의 평균 정확도. 와이.</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">best_estimator의 평균 교차 검증 된 점수</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">best_estimator의 교차 검증 된 평균 점수입니다.</target>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">쿼리 포인트 예측 예측 수단</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">쿼리 지점의 예측 분포 평균입니다.</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">훈련 목표의 평균 또는 중앙값 또는 Quantile 또는 사용자가 제공 한 일정한 값.</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">평균 이동 클러스터링은 매끄러운 샘플 밀도에서 &quot;블롭&quot;을 발견하는 것을 목표로합니다. 중심 기반 알고리즘으로, 중심에 대한 후보를 지정된 지역 내 포인트의 평균으로 업데이트하여 작동합니다. 그런 다음 이러한 후보를 후 처리 단계에서 필터링하여 거의 중복되는 부분을 제거하여 최종 중심 세트를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">플랫 커널을 사용한 평균 교대 클러스터링.</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">각 접힘의 검정 세트에 대한 평균 제곱 오차는 l1_ratio 및 alpha에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">평균 제곱 오차 회귀 손실</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">평균 제곱 로그 오류 회귀 손실</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">포인트 집합의 두 군집의 유사성을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">X의 측정 오류</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">y의 측정 오류</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">블록 단위의 MedInc 중간 소득</target>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">절대 오차 회귀 손실의 중앙값</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">중간 &lt;code&gt;n_samples&lt;/code&gt; , 작은 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 회원 은 가중치를 보유합니다 (\ (w \)</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 멤버 는 \ (b \)를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 멤버 는 인터셉트 (일명 오프셋 또는 바이어스)를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">numpy 배열에 대한 젬핑 모드가 작업자에게 전달되었습니다. 자세한 내용은 'max_nbytes'매개 변수 설명서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">큰 샘플 크기를위한 메모리 소비</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">변환 된 대상에서 회귀하는 메타 추정기</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">변압기로 복합 모델을 구축하기위한 메타 추정기</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">중요도 가중치를 기반으로 기능을 선택하기위한 메타 트랜스포머.</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">k- 평균 알고리즘의 초기화 방법; 기본값은 'k- 평균 ++'입니다.</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">초기화 방법, 기본값은 'k- 평균 ++'입니다.</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">초기화 방법은 기본적으로 'k-means ++'입니다.</target>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">방법 이름</target>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">특이 벡터를 biclusters로 정규화하고 변환하는 방법. 'scale', 'bistochastic'또는 'log'중 하나 일 수 있습니다. 저자는 'log'를 사용하는 것이 좋습니다. 그러나 데이터가 드문 경우 로그 정규화가 작동하지 않으므로 기본값이 'bistochastic'입니다. 주의 : &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; 인 경우 데이터가 희박하지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">최단 경로를 찾는 데 사용하는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">변환 된 결과를 인코딩하는 데 사용되는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">절차를 초기화하는 데 사용되는 방법. 기본값 : n_components &amp;lt;n_features 인 경우 'nndsvd', 그렇지 않은 경우 무작위. 유효한 옵션 :</target>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">&lt;code&gt;_component&lt;/code&gt; 를 업데이트하는 데 사용되는 방법 . &lt;code&gt;fit&lt;/code&gt; 방법으로 만 사용하십시오 . 일반적으로 데이터 크기가 크면 온라인 업데이트가 배치 업데이트보다 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">연결을 계산하는 데 사용되는 메트릭입니다. &quot;유클리드&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;맨해튼&quot;, &quot;코사인&quot;또는 &quot;사전 계산 된&quot;일 수 있습니다. 연결이 &quot;병동&quot;이면 &quot;유클리드&quot;만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">다양한 머신 러닝 작업에 사용할 수있는 메트릭은 아래 섹션에 자세히 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">마이클 E. 팁 : &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;스파 스 베이지안 학습 및 관련성 벡터 머신&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">마이클 마샬 (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">미니 배치 K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">미니 배치 스파 스 주요 구성 요소 분석</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">미니 배치 사전 학습</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">미니 배치 스파 스 PCA ( &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt; ) 는 더 빠르지 만 정확성이 &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt; 의 변형입니다 . 주어진 반복 횟수에 대해 기능 집합의 작은 덩어리를 반복하여 속도를 높입니다.</target>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">목적 함수를 최소화합니다 :</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">확률 평균 그라디언트를 사용하여 유한 합계 최소화 &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">최소 공분산 결정기 (MCD) : 공분산의 강력한 추정량.</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">최소 공분산 결정 인자 추정기</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">경로를 따라 최소 상관 관계. 올가미의 정규화 매개 변수 alpha 매개 변수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">충분한 항목이 &lt;code&gt;min_hash_match&lt;/code&gt; 제약 조건을 충족한다고 가정하면 추정 기당 평가 된 최소 후보 수입니다 .</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">원본 데이터에서 무작위로 선택된 최소 샘플 수. &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; 대한 절대 샘플 수로 처리되고 &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; 대한 상대 수 &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; )로 처리 됩니다. 이것은 일반적으로 주어진 &lt;code&gt;base_estimator&lt;/code&gt; 를 추정하는 데 필요한 최소 샘플 수로 선택됩니다 . 기본적으로 &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; 추정기가 가정되고 &lt;code&gt;min_samples&lt;/code&gt; 가 &lt;code&gt;X.shape[1] + 1&lt;/code&gt; 로 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">담즙의 최소값.</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">왼쪽 경계에 대한 입력 배열 &lt;code&gt;X_&lt;/code&gt; 의 최소값 입니다.</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">&lt;code&gt;class_log_prior_&lt;/code&gt; 를 선형 모델로 해석하기 위해 class_log_prior_ 를 미러링 합니다.</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">MultinomialNB를 선형 모델로 해석하기 위해 &lt;code&gt;feature_log_prob_&lt;/code&gt; 를 미러링 합니다.</target>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">누락 된 속성 값 :</target>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt; 은 기본 sklearn.impute.SimpleImputer를 사용하여 평균, 중간 값 또는 가장 빈번한 값으로 대체 할 수 있습니다 . 중앙값은 결과를 지배 할 수있는 높은 크기의 변수가있는 데이터에 대한보다 강력한 추정값입니다 ( '롱테일'이라고도 함).</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">'데이터'에서 결 측값은 NaN으로 표시됩니다. '대상'에서 결 측값은 NaN (숫자 대상) 또는 없음 (범주 대상)으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">Scikit-learn의 모든 bicluster 추정기에 대한 믹스 인 클래스</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 분류자를위한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 클러스터 추정기에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 밀도 추정 기용 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 회귀 추정량에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 변압기에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">모델 복잡성 영향</target>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">모델 선택 인터페이스</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">모델 블렌딩 : 하나의 감독 된 추정기의 예측이 앙상블 방법으로 다른 추정기를 훈련시키는 데 사용될 때.</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">모델 복잡성</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">scikit-learn의 모델 압축은 현재 선형 모델에만 해당됩니다. 이와 관련하여 모델 희소성 (즉, 모델 벡터에서 0이 아닌 좌표의 수)을 제어하려고 함을 의미합니다. 일반적으로 모델 희소성을 희소 입력 데이터 표현과 결합하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">모델 지속성</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">모델 재구성은 사용 가능한 기능 중 일부만 모델에 맞도록 선택하는 것으로 구성됩니다. 다시 말해, 학습 단계에서 모델이 피쳐를 버린 경우 입력에서 피쳐를 제거 할 수 있습니다. 이것은 몇 가지 이점이 있습니다. 먼저 모델 자체의 메모리 (및 시간) 오버 헤드가 줄어 듭니다. 또한 이전 실행에서 유지할 기능을 알고 있으면 파이프 라인에서 명시 적 기능 선택 구성 요소를 버릴 수도 있습니다. 마지막으로, 모델에 의해 폐기 된 기능을 수집 및 구축하지 않음으로써 데이터 액세스 및 기능 추출 계층에서 업스트림 처리 시간 및 I / O 사용을 줄일 수 있습니다. 예를 들어 원시 데이터가 데이터베이스에서 온 경우 쿼리가 더 가벼운 레코드를 반환하도록하여 더 간단하고 빠른 쿼리를 작성하거나 I / O 사용을 줄일 수 있습니다. 현재재구성은 scikit-learn에서 수동으로 수행해야합니다. 드문 드문 입력의 경우 (특히 &lt;code&gt;CSR&lt;/code&gt; 형식)의 경우 일반적으로 관련 기능을 생성하지 않고 열을 비워두면 충분합니다.</target>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; &lt;/a&gt; 와 같은 도구를 사용한 모델 선택 및 평가는 평가 된 평가자에 적용되는 메트릭을 제어 하는 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">다양한 파라미터 설정을 평가하여 모델을 선택하면 레이블이 지정된 데이터를 사용하여 그리드의 파라미터를 &quot;트레이닝&quot;할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">확률 적 PCA 및 요인 분석 (FA)을 사용한 모델 선택</target>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">중첩 된 CV가없는 모델 선택은 동일한 데이터를 사용하여 모델 매개 변수를 조정하고 모델 성능을 평가합니다. 따라서 정보가 모델에 &quot;누설&quot;되어 데이터에 과적 합할 수 있습니다. 이 효과의 크기는 주로 데이터 세트의 크기와 모델의 안정성에 따라 다릅니다. 이러한 문제에 대한 분석은 Cawley 및 Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">모델 선택 : 추정기 및 매개 변수 선택</target>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">모델 검증</target>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">종의 지리적 분포를 모델링하는 것은 보존 생물학에서 중요한 문제입니다. 이 예에서 우리는 과거의 관측치와 14 개의 환경 변수가 주어진 2 개의 남미 포유류의 지리적 분포를 모델링합니다. 우리는 긍정적 인 예만 가지고 있기 때문에 (실패한 관찰은 없다),이 문제를 밀도 추정 문제로 캐스트하고 &lt;code&gt;OneClassSVM&lt;/code&gt; 패키지에서 제공하는 &lt;code&gt;sklearn.svm&lt;/code&gt; 을 모델링 도구로 사용합니다. 데이터 세트는 Phillips et. 알. (2006). 가능한 &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;경우이&lt;/a&gt; 예에서는 베이스 맵 을 사용 하여 남미의 해안선과 국가 경계를 그립니다.</target>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt; 모듈 은 커널 릿지 회귀를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">그라데이션 하강 업데이트를위한 추진력. 0과 1 사이 여야합니다. solver = 'sgd'인 경우에만 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann, F. and Triggs, B. 및 Jurie, F.&amp;ldquo;무작위 클러스터링 포리스트를 사용하는 빠른 차별적 시각적 코드북&amp;rdquo;NIPS 2007</target>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">자세한 내용은 MacKay, David JC의 &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;베이지안 보간&lt;/a&gt; 기사를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt; 설명서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">모델 선택에 사용할 수있는 도구에 대한 자세한 내용은 &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증 : 추정기 성능 평가&lt;/a&gt; 및 추정기 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;의 하이퍼 파라미터 조정&lt;/a&gt; 섹션에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">보다 공식적으로, 샘플 \ (k \)가 샘플 \ (i \)의 모범이되는 책임은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">보다 공식적으로, 코어 샘플을 데이터 세트의 샘플 인 것으로 정의하여 &lt;code&gt;eps&lt;/code&gt; 거리 내에 &lt;code&gt;min_samples&lt;/code&gt; 다른 샘플 이 존재 하도록 하는데 , 코어 샘플의 &lt;em&gt;이웃&lt;/em&gt; 으로 정의됩니다 . 이것은 핵심 샘플이 벡터 공간의 밀집된 영역에 있음을 알려줍니다. 클러스터는 재귀 코어 샘플을 복용 핵심 샘플입니다 이웃 모두를 찾는 모든 찾는하여 구축 할 수 코어 샘플의 집합입니다 &lt;em&gt;그들의&lt;/em&gt; 등 핵심 샘플있는 이웃을합니다. 또한 클러스터에는 비 핵심 샘플 세트가 있습니다.이 샘플은 클러스터의 코어 샘플에 인접하지만 자체 샘플은 아닙니다. 직관적으로 이러한 샘플은 클러스터의 가장자리에 있습니다.&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">보다 일반적으로 분류기의 정확도가 임의에 너무 가까운 경우에는 문제가 발생했음을 의미 할 수 있습니다. 기능이 도움이되지 않거나 하이퍼 파라미터가 올바르게 조정되지 않았으며 분류 기가 클래스 불균형으로 인해 어려움을 겪고 있습니다.</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">자세한 정보는 &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy 설치 페이지&lt;/a&gt; 와 데비안 / 우분투에 대한 단계별 설치 지침이있는 Daniel Nouri 의이 &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;블로그 게시물&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">OpenML의 추가 메타 데이터</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">보다 정확하게는 초기 모델을 설명한 후 목표 대응에 대한 기대; 부분 의존도에는 &lt;code&gt;init&lt;/code&gt; 모델이 포함되지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">더 읽기 쉬운 코드, 특히 인수 목록 구성을 피하기 때문에.</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">보다 구체적으로, 선형 및 2 차 판별 분석의 경우, \ ​​(P (X | y) \)는 밀도를 갖는 다변량 가우스 분포로 모델링됩니다.</target>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">가장 일반적으로 시차는 \ (\ hat {d} _ {ij} = b S_ {ij} \)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">분산의 대부분은 종 유효 곡선의 종 모양 곡선으로 설명 할 수 있습니다. 특이 값 프로파일의 하위 순위 부분은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">대부분의 scikit-learn 모델은 컴파일 된 Cython 확장 또는 최적화 된 컴퓨팅 라이브러리로 구현되므로 일반적으로 매우 빠릅니다. 반면, 많은 실제 응용 프로그램에서 기능 추출 프로세스 (예 : 데이터베이스 행 또는 네트워크 패킷과 같은 원시 데이터를 numpy 배열로 변환)는 전체 예측 시간을 제어합니다. 예를 들어 Reuters 텍스트 분류 작업에서 선택한 모델에 따라 전체 준비 (SGML 파일 읽기 및 구문 분석, 텍스트 토큰 화 및 공통 벡터 공간으로 해싱)는 실제 예측 코드보다 100 ~ 500 배 더 많은 시간이 걸립니다.</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">자연어 처리 (NLP) 및 정보 검색 (IR) 문헌에서 대부분의 LSA 처리는 &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; 모양을 갖도록 행렬의 축을 교환합니다 . 우리는 scikit-learn API와 더 잘 일치하는 다른 방식으로 LSA를 제시하지만 발견 된 특이 값은 동일합니다.</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">다중 대상 분류</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">다중 대상 회귀</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">멀티 클래스 Ada 부스트 결정 트리</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">멀티 클래스 대상.</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">멀티 클래스 대상. 표시기 매트릭스는 다중 레이블 분류를 켭니다.</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">다차원 스케일링</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">다층 퍼셉트론 분류기.</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">다층 퍼셉트론은 기능 스케일링에 민감하므로 데이터를 스케일링하는 것이 좋습니다. 예를 들어 입력 벡터 X의 각 속성을 [0, 1] 또는 [-1, +1]로 스케일링 하거나 평균 0과 분산 1을 갖도록 표준화하십시오 . 테스트 세트에 &lt;em&gt;동일한&lt;/em&gt; 스케일링을 적용해야합니다. 의미있는 결과. &lt;code&gt;StandardScaler&lt;/code&gt; 를 사용하여 표준화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">다층 퍼셉트론 회귀 기.</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">다중 출력 결정 트리 회귀</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">여러 예측 변수에서 예측 된 다중 출력 대상 참고 : 예측 변수마다 별도의 모델이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">다중 출력 대상.</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">다중 출력 대상. 표시기 행렬은 다중 레이블 추정을 켭니다.</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1 / L2 혼합 규범을 정규화기로 학습 한 멀티 태스크 ElasticNet 모델</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 ElasticNet</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 올가미</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 올가미.</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1 / L2 혼합 규범을 정규화기로 훈련 한 다중 작업 올가미 모델</target>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 분류 전략</target>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">멀티 클래스 분류</target>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">멀티 클래스 확률 추정치는 Zadrozny와 Elkan이 권장하는 간단한 정규화에 의한 이진 (1 대 나머지) 추정치에서 파생됩니다.</target>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">멀티 클래스 설정</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">새로운 그룹에 대한 다중 클래스 희소 로지스틱 회귀</target>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">멀티 클래스 스펙트럼 클러스터링, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">멀티 클래스 vs. 멀티 라벨 피팅</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">다차원 스케일링</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">멀티 라벨 분류</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">다중 라벨 순위 측정 항목</target>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">멀티 메트릭 스코어링은 사전 정의 된 스코어 이름의 문자열 목록으로 지정되거나 스코어러 이름을 스코어러 기능 및 / 또는 사전 정의 된 스코어러 이름에 맵핑하는 dict입니다. 자세한 내용은 &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;다중 메트릭 평가 사용&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">다항식 + L1 페널티</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">다항식 + L2 페널티</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">다항 이탈 ( &lt;code&gt;'deviance'&lt;/code&gt; ) : &lt;code&gt;n_classes&lt;/code&gt; 가 서로 배타적 인 클래스를 갖는 다중 클래스 분류를위한 음의 다항 로그 우도 손실 함수 . 확률 추정치를 제공합니다. 초기 모델은 각 클래스의 사전 확률로 제공됩니다. 각 반복에서 &lt;code&gt;n_classes&lt;/code&gt; 회귀 트리를 구성해야하므로 클래스 수가 많은 데이터 세트에 GBRT를 비효율적으로 만들 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">다중 출력 분류 지원은 어떤 분류에 추가 할 수 있습니다 &lt;code&gt;MultiOutputClassifier&lt;/code&gt; . 이 전략은 대상 당 하나의 분류기를 맞추는 것으로 구성됩니다. 이를 통해 여러 대상 변수 분류가 가능합니다. 이 클래스의 목적은 추정값을 확장하여 일련의 응답 (y1, y2, y3)을 예측하기 위해 단일 X 예측 행렬에서 학습 된 일련의 목표 함수 (f1, f2, f3&amp;hellip;, fn)를 추정 할 수 있도록하는 것입니다. &amp;hellip;, yn).</target>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">다중 출력 회귀 지원은 어떤 회귀에 추가 할 수 있습니다 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; . 이 전략은 대상 당 하나의 회귀자를 피팅하는 것으로 구성됩니다. 각 대상은 정확히 하나의 회귀 자로 표시되므로 해당 회귀를 검사하여 대상에 대한 지식을 얻을 수 있습니다. 으로 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 가 대상 당 하나의 회귀를 맞는이 목표 사이의 상관 관계를 이용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 를 사용한 다중 메트릭 평가 (자세한 내용은 &lt;code&gt;scoring&lt;/code&gt; 매개 변수 문서를 참조하십시오)</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">&lt;code&gt;scoring&lt;/code&gt; 매개 변수를 메트릭 스코어러 이름 목록 으로 설정 하거나 스코어러 이름을 스코어러 호출 가능 항목에 맵핑하는 dict를 사용하여 여러 메트릭 매개 변수 검색을 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">변압기 당 기능에 대한 곱하기 가중치. 키는 변압기 이름이며 무게 값입니다.</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">변압기 당 기능에 대한 곱하기 가중치. 변압기의 출력에는 이러한 가중치가 곱해집니다. 키는 변압기 이름이며 무게 값입니다.</target>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">피처에 지정된 값을 곱합니다. 없음 인 경우 [1, 100]에 그려진 임의의 값으로 피처를 스케일링합니다. 이동 후 스케일링이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">멀티 스레드 BLAS 라이브러리는 때때로 Python의 &lt;code&gt;multiprocessing&lt;/code&gt; 모듈 과 충돌 합니다. 예를 들어 &lt;code&gt;GridSearchCV&lt;/code&gt; 및 &lt;code&gt;n_jobs&lt;/code&gt; 인수 를 사용하는 대부분의 다른 추정기 ( &lt;code&gt;SGDClassifier&lt;/code&gt; , &lt;code&gt;SGDRegressor&lt;/code&gt; , &lt;code&gt;Perceptron&lt;/code&gt; , &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; 및 임의 포리스트와 같은 트리 기반 메소드 제외)가 사용합니다. 이것은 OpenMP 지원으로 빌드 된 Apple의 Accelerate 및 OpenBLAS에 해당됩니다.</target>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">partial_fit을 처음 호출 할 때 제공해야하며 후속 호출에서는 생략 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">기본 추정기의 입력 가정을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">상호 정보 (기회에 맞게 조정되지 않음)</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">두 군집 간의 상호 정보.</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">두 랜덤 변수 사이의 상호 정보 (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 은 음이 아닌 값으로, 변수 간의 종속성을 측정합니다. 두 개의 임의 변수가 독립적이고 값이 클수록 종속성이 높은 경우에만 0입니다.</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">두 랜덤 변수 사이의 상호 정보 (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 은 음이 아닌 값으로, 변수 간의 종속성을 측정합니다. 두 개의 임의 변수가 독립적이고 값이 클수록 종속성이 높은 경우에만 0입니다.</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">피처와 대상 간의 상호 정보.</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">연속 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">연속적인 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">개별 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="2555b04ef28112b324874c1cc2f3bf2b5b5c384e" translate="yes" xml:space="preserve">
          <source>Mutual information, a non-negative value</source>
          <target state="translated">음수가 아닌 상호 정보</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="4e1221dedd7ee34eb6931a44dc15d9a84ca69a81" translate="yes" xml:space="preserve">
          <source>N : number of dimensions</source>
          <target state="translated">N : 치수 수</target>
        </trans-unit>
        <trans-unit id="8daf5ce04352d841160e980446540ca50cce58e4" translate="yes" xml:space="preserve">
          <source>N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.</source>
          <target state="translated">구조에 N 그램! 유니 그램 (n = 1)의 간단한 모음을 만드는 대신 연속 단어 쌍의 발생 횟수를 계산하는 bigram (n = 2) 모음을 선호 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="235c4fdc295d941494351e73dad0edc432affd04" translate="yes" xml:space="preserve">
          <source>NFF : number of dims in which both values are False</source>
          <target state="translated">NFF : 두 값이 모두 거짓 인 딤 수</target>
        </trans-unit>
        <trans-unit id="f242d8e1cdbbc8cb628621d8d57f10327047707d" translate="yes" xml:space="preserve">
          <source>NFT : number of dims in which the first value is False, second is True</source>
          <target state="translated">NFT : 첫 번째 값이 False이고 두 번째가 True 인 희미한 수</target>
        </trans-unit>
        <trans-unit id="ced12bb5137dbf26fd788e77cae54623cdb8b2e8" translate="yes" xml:space="preserve">
          <source>NMF is best used with the &lt;code&gt;fit_transform&lt;/code&gt; method, which returns the matrix W. The matrix H is stored into the fitted model in the &lt;code&gt;components_&lt;/code&gt; attribute; the method &lt;code&gt;transform&lt;/code&gt; will decompose a new matrix X_new based on these stored components:</source>
          <target state="translated">NMF는 행렬 W를 리턴 하는 &lt;code&gt;fit_transform&lt;/code&gt; 메소드 와 함께 사용하는 것이 가장 좋습니다 . 행렬 H는 &lt;code&gt;components_&lt;/code&gt; 속성으로 피팅 된 모델에 저장됩니다 . 메소드 &lt;code&gt;transform&lt;/code&gt; 은 저장된 구성 요소를 기반으로 새 행렬 X_new를 분해합니다.</target>
        </trans-unit>
        <trans-unit id="9546ef450bf032f2a099e2b8894066e314108bcc" translate="yes" xml:space="preserve">
          <source>NMI and MI are not adjusted against chance.</source>
          <target state="translated">NMI와 MI는 우연히 조정되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ec8506cc20e415f16975d43b2c6e163b63b7c223" translate="yes" xml:space="preserve">
          <source>NNEQ / (NNEQ + 0.5 * NTT)</source>
          <target state="translated">NNEQ / (NNEQ + 0.5 * NTT)</target>
        </trans-unit>
        <trans-unit id="64142d93685b184d0f4668dd2d38de67d364504a" translate="yes" xml:space="preserve">
          <source>NNEQ / (NTT + NNZ)</source>
          <target state="translated">NNEQ / (NTT + NNZ)</target>
        </trans-unit>
        <trans-unit id="9e2ca45598fef4852f298770d7c7037071a195c1" translate="yes" xml:space="preserve">
          <source>NNEQ / N</source>
          <target state="translated">NNEQ / N</target>
        </trans-unit>
        <trans-unit id="bd22d441438dd8339012b8925c55919834498020" translate="yes" xml:space="preserve">
          <source>NNEQ / NNZ</source>
          <target state="translated">NNEQ / NNZ</target>
        </trans-unit>
        <trans-unit id="a4e22ff89a7f8daef1da10b2c311e81f8eb57054" translate="yes" xml:space="preserve">
          <source>NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT</source>
          <target state="translated">NNEQ : 같지 않은 치수 수, NNEQ = NTF + NFT</target>
        </trans-unit>
        <trans-unit id="80bfd3623c0e507836f83286688a2ee41b18b00e" translate="yes" xml:space="preserve">
          <source>NNZ / N</source>
          <target state="translated">NNZ / ​​N</target>
        </trans-unit>
        <trans-unit id="93209a2edd337e6dc4e7c870a3c72537cea28fdf" translate="yes" xml:space="preserve">
          <source>NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT</source>
          <target state="translated">NNZ : 0이 아닌 차원 수, NNZ = NTF + NFT + NTT</target>
        </trans-unit>
        <trans-unit id="a8ad860c15810cce0e7beac1c91da3ab2cb22c47" translate="yes" xml:space="preserve">
          <source>NOTE</source>
          <target state="translated">NOTE</target>
        </trans-unit>
        <trans-unit id="b81cbdff62e50c72d48e4feea8a9ed88bea18bef" translate="yes" xml:space="preserve">
          <source>NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each.</source>
          <target state="translated">사용자 지정 채점자를 사용할 때 각 채점자는 단일 값을 반환해야합니다. 값 목록 / 배열을 반환하는 메트릭 함수는 각각 하나의 값을 반환하는 여러 스코어러로 래핑 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9764dfb854390dc404102ac64200b55e363e83df" translate="yes" xml:space="preserve">
          <source>NOX nitric oxides concentration (parts per 10 million)</source>
          <target state="translated">NOX 산화 질소 농도 (1,000 만 부)</target>
        </trans-unit>
        <trans-unit id="99542bc2231d38286b9a1dbe4685e8690203b845" translate="yes" xml:space="preserve">
          <source>NTF : number of dims in which the first value is True, second is False</source>
          <target state="translated">NTF : 첫 번째 값이 True이고 두 번째 값이 False 인 딤 수</target>
        </trans-unit>
        <trans-unit id="d7aff2fba38c5d47fc1d509779237efeccf9cd66" translate="yes" xml:space="preserve">
          <source>NTT : number of dims in which both values are True</source>
          <target state="translated">NTT : 두 값이 모두 참인 디딤 수</target>
        </trans-unit>
        <trans-unit id="6e2518fe965a665a40ec6f1bf71cbacd3d7014df" translate="yes" xml:space="preserve">
          <source>NaNs are ignored in the algorithm.</source>
          <target state="translated">알고리즘에서 NaN은 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="7b2cc2bc3bfa4ab2fba6e73cce899558220dd79a" translate="yes" xml:space="preserve">
          <source>NaNs are treated as missing values: disregarded in fit, and maintained in transform.</source>
          <target state="translated">NaN은 결 측값으로 취급됩니다. 적합하지 않고 변환 상태로 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="d13d7452647efb26ab0d2b1a3596526a7f4ca5d6" translate="yes" xml:space="preserve">
          <source>NaNs are treated as missing values: disregarded to compute the statistics, and maintained during the data transformation.</source>
          <target state="translated">NaN은 결 측값으로 처리됩니다. 통계 계산을 무시하고 데이터 변환 중에 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="80d8f13b4e334c4342adf34b360ad118e5e25aa3" translate="yes" xml:space="preserve">
          <source>Naive Bayes classifier for multinomial models</source>
          <target state="translated">다항식 모델을위한 Naive Bayes 분류기</target>
        </trans-unit>
        <trans-unit id="92990e6c1a566f0d055f974e25026ec604b9ccd9" translate="yes" xml:space="preserve">
          <source>Naive Bayes classifier for multivariate Bernoulli models.</source>
          <target state="translated">다변량 베르누이 모델에 대한 나이브 베이 즈 분류기.</target>
        </trans-unit>
        <trans-unit id="c95f9acb4985f23ad6962fd01ae91dec549e7273" translate="yes" xml:space="preserve">
          <source>Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.</source>
          <target state="translated">Naive Bayes 학습자와 분류기는보다 복잡한 방법에 비해 매우 빠릅니다. 클래스 조건부 특징 분포의 분리는 각 분포가 1 차원 분포로 독립적으로 추정 될 수 있음을 의미합니다. 이는 차원의 저주로 인한 문제를 완화하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="bb7ceea48fd3728ed03cf0ba21b4839b323fc974" translate="yes" xml:space="preserve">
          <source>Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes&amp;rsquo; theorem with the &amp;ldquo;naive&amp;rdquo; assumption of conditional independence between every pair of features given the value of the class variable. Bayes&amp;rsquo; theorem states the following relationship, given class variable \(y\) and dependent feature vector \(x_1\) through \(x_n\), :</source>
          <target state="translated">Naive Bayes 방법은 클래스 변수의 값이 주어지면 모든 기능 쌍 사이에 조건부 독립성을 &quot;순진한&quot;가정으로 Bayes 정리를 적용한 것에 기반한 일련의 감독 학습 알고리즘입니다. 베이 즈 정리는 클래스 변수 \ (y \)와 종속 피처 벡터 \ (x_1 \)에서 \ (x_n \)까지 주어진 관계를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f65600325bc091b7b293639582ad70691e2ac960" translate="yes" xml:space="preserve">
          <source>Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, &lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt;&lt;code&gt;MultinomialNB&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.naive_bayes.bernoullinb#sklearn.naive_bayes.BernoulliNB&quot;&gt;&lt;code&gt;BernoulliNB&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;GaussianNB&lt;/code&gt;&lt;/a&gt; expose a &lt;code&gt;partial_fit&lt;/code&gt; method that can be used incrementally as done with other classifiers as demonstrated in &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;. All naive Bayes classifiers support sample weighting.</source>
          <target state="translated">Naive Bayes 모델을 사용하면 전체 교육 세트가 메모리에 맞지 않을 수있는 대규모 분류 문제를 해결할 수 있습니다. 이 경우를 처리하기 위해 &lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt; &lt;code&gt;MultinomialNB&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.naive_bayes.bernoullinb#sklearn.naive_bayes.BernoulliNB&quot;&gt; &lt;code&gt;BernoulliNB&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;GaussianNB&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt; 는 텍스트 문서의 핵심 외 분류&lt;/a&gt; 에서 설명 된 것처럼 다른 분류기와 마찬가지로 점진적으로 사용할 수 있는 &lt;code&gt;partial_fit&lt;/code&gt; 메소드를 제공 합니다 . 모든 순진 Bayes 분류기는 샘플 가중치를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="7f39a3bd9bee33098b86f18fcaf23fc1b1f211a0" translate="yes" xml:space="preserve">
          <source>Name of dataset</source>
          <target state="translated">데이터 세트 이름</target>
        </trans-unit>
        <trans-unit id="325b56c17b8389991fec124de840a19f36bc1993" translate="yes" xml:space="preserve">
          <source>Name of each feature; feature_names[i] holds the name of the feature with index i.</source>
          <target state="translated">각 기능의 이름 feature_names [i]는 색인 i를 가진 기능의 이름을 보유합니다.</target>
        </trans-unit>
        <trans-unit id="1d63bfd9357f039d867c5652a85ab61996c87f94" translate="yes" xml:space="preserve">
          <source>Name of the data set on mldata.org, e.g.: &amp;ldquo;leukemia&amp;rdquo;, &amp;ldquo;Whistler Daily Snowfall&amp;rdquo;, etc. The raw name is automatically converted to a mldata.org URL .</source>
          <target state="translated">mldata.org에 설정된 데이터 이름 (예 : &quot;leukemia&quot;, &quot;Whistler Daily Snowfall&quot;등) 원시 이름은 자동으로 mldata.org URL로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="866f4401ef93cfed3b044fff6753dc1e913659b2" translate="yes" xml:space="preserve">
          <source>Name of the output activation function.</source>
          <target state="translated">출력 활성화 기능의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="5a3a86d298c7e4314e724bb2623d8c6979ee2b6e" translate="yes" xml:space="preserve">
          <source>Name of the parameter that will be varied.</source>
          <target state="translated">변경 될 매개 변수의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="ce3ec81584fa2d87df12f144e2480deecc5a975a" translate="yes" xml:space="preserve">
          <source>Name or index of the column containing the data.</source>
          <target state="translated">데이터가 포함 된 열의 이름 또는 색인입니다.</target>
        </trans-unit>
        <trans-unit id="0ae5e537b1c061ee0b3ccea7c63ace2088ca02dd" translate="yes" xml:space="preserve">
          <source>Name or index of the column containing the target values.</source>
          <target state="translated">대상 값을 포함하는 열의 이름 또는 색인</target>
        </trans-unit>
        <trans-unit id="3170e49e906772d2e2d83c510613a736bad3f541" translate="yes" xml:space="preserve">
          <source>Named features not encountered during fit or fit_transform will be silently ignored.</source>
          <target state="translated">fit 또는 fit_transform 중에 발생하지 않는 명명 된 기능은 자동으로 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="dd3283d9f71127c2e2cb8ea6f07a41ece4a049ce" translate="yes" xml:space="preserve">
          <source>Names of each of the features.</source>
          <target state="translated">각 기능의 이름</target>
        </trans-unit>
        <trans-unit id="99983f06243c41c70b7f7a98a21b26cf2a2ec6a9" translate="yes" xml:space="preserve">
          <source>Names of each of the target classes in ascending numerical order. Only relevant for classification and not supported for multi-output. If &lt;code&gt;True&lt;/code&gt;, shows a symbolic representation of the class name.</source>
          <target state="translated">각 대상 클래스의 이름은 오름차순으로 표시됩니다. 분류에만 관련되며 다중 출력에는 지원되지 않습니다. &lt;code&gt;True&lt;/code&gt; 인 경우 클래스 이름을 상징적으로 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="e9e6ba24a1711383d87f42cc11bb29b29e568b73" translate="yes" xml:space="preserve">
          <source>Names of each target (RCV1 topics), as ordered in dataset.target.</source>
          <target state="translated">dataset.target에서 주문한 각 대상의 이름 (RCV1 주제).</target>
        </trans-unit>
        <trans-unit id="5ee798b80fce1c26ac31847940e2cbb9594ee08b" translate="yes" xml:space="preserve">
          <source>Names of the features produced by transform.</source>
          <target state="translated">변환에 의해 생성 된 기능의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="4aded465f8c4c45d7d7ec8d437901909b15f0b3f" translate="yes" xml:space="preserve">
          <source>Natural handling of data of mixed type (= heterogeneous features)</source>
          <target state="translated">혼합 유형의 데이터 자연 처리 (= 이기종 기능)</target>
        </trans-unit>
        <trans-unit id="0a4d2a1303aed1ff654767155d9269907f0d020c" translate="yes" xml:space="preserve">
          <source>Nearest Centroid Classification</source>
          <target state="translated">가장 가까운 중심 분류</target>
        </trans-unit>
        <trans-unit id="fa1459036257eab60db8e1afe6d9886bbc5e8a42" translate="yes" xml:space="preserve">
          <source>Nearest Neighbors Classification</source>
          <target state="translated">가장 가까운 이웃 분류</target>
        </trans-unit>
        <trans-unit id="c7b70d3a90c9b413590f1c3fdfeae8dc16304398" translate="yes" xml:space="preserve">
          <source>Nearest Neighbors regression</source>
          <target state="translated">가장 가까운 이웃 회귀</target>
        </trans-unit>
        <trans-unit id="cc8575a20e3e28eef4bfc70e48146f9994bd7318" translate="yes" xml:space="preserve">
          <source>Nearest centroid classifier.</source>
          <target state="translated">가장 가까운 중심 분류기.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
