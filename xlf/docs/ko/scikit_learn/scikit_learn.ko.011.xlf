<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">릿지 분류기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">변압기를 X에 끼 웁니다.</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">ARD를 사용하여 회귀 모형의 가중치를 맞추십시오. 회귀 모형의 가중치는 가우스 분포에 있다고 가정합니다. 또한 람다 (가중 분포의 정밀도) 및 알파 (소음 분포의 정밀도)를 모수합니다. 추정은 반복 절차에 의해 수행됩니다 (증거 최대화).</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">데이터에 맞추고 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">X를 확인하여 변압기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">기본 견적에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">FastMCD 알고리즘으로 최소 공분산 결정자를 적합시킵니다.</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">모든 변환을 차례로 맞추고 데이터를 변환 한 다음 변환 된 데이터에 대해 최종 추정기로 fit_transform을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">GraphicalLasso 공분산 모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">GraphicalLasso 모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 Ledoit-Wolf 축소 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 학습 데이터 및 매개 변수에 따라 최대 우도 추정량 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 교육 데이터 및 매개 변수에 따라 Oracle Approximating Shrinkage 공분산 모델에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">추정기에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 수축 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">선택적 매개 변수 fit_params를 사용하여 변환기를 X와 y에 맞추고 변환 된 버전의 X를 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">적합 회귀 기.</target>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">피팅 변압기는 계산 비용이 많이들 수 있습니다. 그것으로 &lt;code&gt;memory&lt;/code&gt; 파라미터 세트, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 호출 한 후 각 변압기를 캐시합니다 &lt;code&gt;fit&lt;/code&gt; . 이 기능은 파라미터와 입력 데이터가 동일한 경우 파이프 라인 내에서 적합한 변압기를 계산하지 않도록하는 데 사용됩니다. 일반적인 예는 변압기를 한 번만 장착하고 각 구성에 재사용 할 수있는 계통 검색의 경우입니다.</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">각 알파에 해당하는 교차 유효성 검사 값을 &lt;code&gt;cv_values_&lt;/code&gt; 특성에 저장해야하는지 여부를 나타내는 플래그입니다 (아래 참조). 이 플래그는 &lt;code&gt;cv=None&lt;/code&gt; 과만 호환됩니다 (즉, Generalized Cross-Validation 사용).</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">일반화 교차 검증을 수행 할 때 사용할 전략을 나타내는 플래그입니다. 옵션은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">밀도 추정에 적합한 평면 형상</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">작업자 프로세스와의 통신을위한 유연한 산 세척 제어.</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">플로 차트</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">작업자 프로세스와 메모리를 공유하기 위해 대형 어레이를 챙겨 내기 위해 풀에서 사용할 폴더입니다. None이면 순서대로 시도합니다.</target>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">랜덤 한 구조 찾기의 알고리즘 4.3 : 근사 행렬 분해 구성을위한 확률 적 알고리즘 Halko, et al., 2009 (arXiv : 909) http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">&quot;one-vs-rest&quot; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 의&lt;/a&gt; 경우 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 속성 은 각각 &lt;code&gt;[n_class, n_features]&lt;/code&gt; 및 &lt;code&gt;[n_class]&lt;/code&gt; 의 모양을 갖습니다 . 계수의 각 행은 &lt;code&gt;n_class&lt;/code&gt; 많은&amp;ldquo;one-vs-rest&amp;rdquo;분류기 중 하나에 해당하며&amp;ldquo;one&amp;rdquo;클래스의 순서로 인터셉트에 대해 유사합니다.</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">추정기 또는 예측이 아닌 &lt;em&gt;샘플&lt;/em&gt; 간의 &quot;쌍별&quot;메트릭에 대해서는 &lt;a href=&quot;metrics#metrics&quot;&gt;쌍별 메트릭, 친화도 및 커널&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">들어 &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; (예 : 랜덤 포레스트, GBT, ExtraTrees 등) 나무의 나무의 수와 깊이는 가장 중요한 역할을한다. 지연 시간과 처리량은 트리 수에 따라 선형으로 확장되어야합니다. 이 경우 &lt;code&gt;n_estimators&lt;/code&gt; 의 n_estimators 매개 변수를 직접 사용 &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">들면 &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt; (예 올가미 ElasticNet, SGDClassifier / 회귀 변수, 릿지 및 RidgeClassifier, PassiveAggressiveClassifier / 회귀 변수, LinearSVC, 로지스틱 회귀 ...)의 예측시에 동일한 (내적)을 적용한 결정 함수이므로 지연 동일해야 .</target>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">용 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; (및 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; )을 NumPy와 배열 복사하여 liblinear 내부 희소 데이터 표현 (배정도 바늘 및 비제로 성분 INT32 인덱스)으로 변환되는 바와 같이 전달 된 입력. 밀도가 높은 numpy C 연속 배정 밀도 배열을 입력으로 복사하지 않고 대규모 선형 분류 기준을 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 려면 SGDClassifier 클래스를 대신 사용하는 것이 좋습니다 . 목적 함수는 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 모델 과 거의 동일하게 구성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">들어 &lt;code&gt;make_classification&lt;/code&gt; , 세 진 두 개의 다중 클래스 분류 데이터 세트는 정보 기능과 클래스 당 클러스터의 다른 번호로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">가우시안 분포 데이터의 경우, 분포 모드까지 관측치 \ (x_i \)의 거리는 다음의 Mahalanobis 거리를 사용하여 계산할 수 있습니다. \ (d _ {(\ mu, \ Sigma)} (x_i) ^ 2 = (x_i- \ mu) '\ Sigma ^ {-1} (x_i-\ mu) \) 여기서 \ (\ mu \) 및 \ (\ Sigma \)는 기본 가우시안 분포의 위치 및 공분산입니다.</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">\ (k \) 클러스터의 경우 Calinski-Harabaz 점수 \ (s \)는 클러스터 간 분산 평균과 클러스터 내 분산의 비율로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">분류 모델의 경우 X의 각 샘플에 대해 예측 된 클래스가 반환됩니다. 회귀 모형의 경우 X를 기반으로 예측 된 값이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">다양한 스케일러, 변압기 및 노멀 라이저를 비교하려면 &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">완전한 확률 모델을 위해서는 잠재 변수 \ (h \)에 대한 사전 분포도 필요합니다. 가장 간단한 가정 (가우시안 분포의 훌륭한 특성에 기초)은 \ (h \ sim \ mathcal {N} (0, \ mathbf {I}) \)입니다. 이것은 \ (x \)의 한계 분포로 가우스를 산출합니다.</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">일정한 학습 속도를 위해 &lt;code&gt;learning_rate='constant'&lt;/code&gt; 를 사용하고 &lt;code&gt;eta0&lt;/code&gt; 을 사용하여 학습 속도를 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">사용 된 알고리즘의 구현 및 세부 사항에 대한 설명은</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">이러한 알고리즘에 대한 논의 및 비교는 &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;매니 폴드 모듈 페이지를 참조하십시오.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">여러 주제에서 생성 된 문서의 경우 모든 주제는 단어 모음을 생성 할 때 동일하게 가중치가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">최고의 biclusters의 경우, 가장 일반적인 문서 범주와 10 개의 가장 중요한 단어가 인쇄됩니다. 최고의 biclusters는 정규화 된 컷에 의해 결정됩니다. 가장 좋은 단어는 bicluster 내부와 외부의 합계를 비교하여 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하여 사용자 지정 기능을 선택 하는 방법을 보여주는 전체 코드 예제는 FunctionTransformer 를 &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;사용하여 열 선택을&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">텍스트 분류 작업에서 코어 외 스케일링의 완전한 예 &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;는 텍스트 문서의 코어 외 분류를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">주어진 &lt;code&gt;n_components&lt;/code&gt; 값에 대해 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 는 종종 Nystroem 보다 정확도가 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 는 계산 비용이 저렴하지만 더 큰 피처 공간을보다 효율적으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">알파를 &lt;a href=&quot;linear_model#lasso&quot;&gt;적절히&lt;/a&gt; 선택 하기 위해 특정 조건을 충족 하는 경우 Lasso 는 관측치가 거의없는 정확한 0이 아닌 변수 세트를 완전히 복구 할 수 있습니다. 특히, 샘플 수는 &quot;충분히 많&quot;거나 L1 모델이 임의로 수행됩니다. 여기서 &quot;충분히 큰&quot;은 0이 아닌 계수의 수, 피처 수의 로그, 노이즈의 양, 0이 아닌 계수의 최소 절대 값 및 설계 행렬 X의 구조. 또한 설계 행렬은 상관되지 않은 특정 특성을 표시해야합니다.</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">N 클래스의 다중 레이블 분류 문제의 경우 N 이진 분류 자에 0과 N-1 사이의 정수가 할당됩니다. 이 정수는 체인의 모델 순서를 정의합니다. 그런 다음 각 분류자는 사용 가능한 교육 데이터와 모델에 더 낮은 번호가 할당 된 클래스의 실제 레이블에 맞습니다.</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">multi_class 문제의 경우 multi_class가 &quot;다항식&quot;으로 설정된 경우 softmax 함수를 사용하여 각 클래스의 예측 확률을 찾습니다. 그렇지 않으면 일대일 접근 방식을 사용하십시오. 즉, 로지스틱 함수를 사용하여 각 클래스가 양수라고 가정 할 때 각 클래스의 확률을 계산하십시오. 모든 클래스에서이 값을 정규화합니다.</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">멀티 클래스 문제의 경우 각 클래스의 하이퍼 파라미터는 모든 폴드와 클래스에서 병렬로 1 대 1을 수행하여 얻은 최고 점수를 사용하여 계산됩니다. 따라서 이것이 진정한 다항식 손실이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">루트로 들어가는 새로운 점의 경우, 가장 가까운 서브 클러스터와 병합되고 선형 합, 제곱합 및 해당 서브 클러스터의 샘플 수가 업데이트됩니다. 리프 노드의 속성이 업데이트 될 때까지 재귀 적으로 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">방법이 구체 데이터 세트에 적용되는 유사한 예 &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;는 잘린 구체의 매니 폴드 학습 방법을&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">방법이 S- 곡선 데이터 세트에 적용되는 유사한 예 &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;는 매니 폴드 학습 방법 비교를&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">적응 적으로 감소하는 학습 속도의 경우 &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; 를 사용하고 &lt;code&gt;eta0&lt;/code&gt; 을 사용하여 시작 학습 속도를 지정하십시오. 정지 기준에 도달하면 학습 속도는 5로 나눠지고 알고리즘은 중단되지 않습니다. 학습 속도가 1e-6 아래로 떨어지면 알고리즘이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">추정기가 효과적이기 위해서는 이웃 지점 사이의 거리가 문제에 따라 \ (d \) 값보다 작아야합니다. 한 차원에서 평균 \ (n \ sim 1 / d \) 포인트가 필요합니다. 위의 \ (k \)-NN 예와 관련하여, 데이터가 0에서 1 사이의 값을 갖는 하나의 특징과 \ (n \) 훈련 관측치에 의해 기술된다면, 새로운 데이터는 더 이상 멀지 않을 것입니다 \ (1 / n \). 따라서 클래스 간 기능 변동의 규모에 비해 \ (1 / n \)이 작 으면 가장 가까운 이웃 결정 규칙이 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">scikit-learn과 함께이 데이터 세트를 사용하는 예는 &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">일반적으로 유니 코드 및 문자 인코딩에 대한 소개는 Joel Spolsky의 &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;절대 모든 소프트웨어 개발자가 유니 코드에 대해 알아야 할 절대적인&lt;/a&gt; 내용을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">1 클래스 모델의 경우 +1 (inlier) 또는 -1 (outlier)이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">1 클래스 모델의 경우 +1 또는 -1이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">scikit-learn에서 사용 가능한 전략에 대한 개요는 &lt;a href=&quot;computing#scaling-strategies&quot;&gt;핵심 학습&lt;/a&gt; 자료 도 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">실제 레이블이 \ (y \ in \ {0,1 \} \)이고 확률 추정이 \ (p = \ operatorname {Pr} (y = 1) \) 인 이진 분류의 경우 샘플 당 로그 손실은 음수입니다. 실제 레이블이 지정된 분류기의 로그 가능성 :</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">이진 분류의 경우 \ (f (x) \)는 로지스틱 함수 \ (g (z) = 1 / (1 + e ^ {-z}) \)를 통과하여 0과 1 사이의 출력 값을 얻습니다. 0.5로 설정된 임계 값은 0.5보다 크거나 같은 출력 샘플을 양수 클래스에 할당하고 나머지는 음수 클래스에 할당합니다.</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">이진 문제의 경우 다음과 같이 진 음성, 오 탐지,가 음성 및 진 양성 카운트를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">&lt;code&gt;loss='deviance'&lt;/code&gt; 인 분류 의 경우 대상 응답은 logit (p)입니다.</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">로지스틱 손실이있는 분류의 경우, 평균 전략을 사용하는 SGD의 또 다른 변형이 SG (Stochastic Average Gradient) 알고리즘으로 제공되며 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 에서 솔버로 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">분류의 경우 링크 함수 이전의 회귀 점수로 생각할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">분류의 경우 &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) 또는 &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II) 와 함께 사용할 수 있습니다 . 회귀의 경우 &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) 또는 &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II) 와 함께 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">분류의 경우, 중요하지 않은 특징 추출 루틴은 새로운 / 보이지 않는 속성에 대처할 수 있지만 증분 학습자 자체는 새로운 / 보이지 않은 대상 클래스에 대처하지 못할 수 있습니다. 이 경우 , &lt;code&gt;classes=&lt;/code&gt; 매개 변수를 사용하여 가능한 모든 클래스를 첫 번째 &lt;code&gt;partial_fit&lt;/code&gt; 호출 로 전달해야합니다 .</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">분류 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;홍채&lt;/a&gt; 작업 에서와 같이 분류의 경우 선형 회귀는 의사 결정 경계에서 멀리 떨어진 데이터에 너무 많은 가중치를 부여하므로 올바른 접근 방식이 아닙니다. 선형 접근법은 S 자형 함수 또는 &lt;strong&gt;로지스틱&lt;/strong&gt; 함수 에 적합 합니다.</target>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">: 분류 &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">비교를 위해, 임의의 코드북 (임의로 픽업 된 컬러)을 사용한 양자화 된 이미지가 또한 도시되어있다.</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">비교를 위해 MiniBatchKMeans를 사용하여 문서도 클러스터됩니다. biclusters에서 파생 된 문서 클러스터는 MiniBatchKMeans에서 찾은 클러스터보다 더 나은 V 측정을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">비교를 위해 &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; 의 출력도 추가합니다 . 충분한 훈련 샘플 (수천)이있는 경우 가우스로 임의의 분포를 강요 할 수 있습니다. 비모수 적 방법이므로 모수 적 방법 (Box-Cox 및 Yeo-Johnson)보다 해석하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">호환성을 위해이 메서드를 사용하는 사용자 코드는 형식 문제를 피하기 위해 &lt;code&gt;np.asarray&lt;/code&gt; 에서 해당 호출을 래핑해야 합니다.</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">위의 &lt;code&gt;C&lt;/code&gt; 와 같은 연속 모수 의 경우 랜덤 분포를 최대한 활용하려면 연속 분포를 지정하는 것이 중요합니다. 이런 식으로 &lt;code&gt;n_iter&lt;/code&gt; 를 늘리면 항상 더 정밀한 검색으로 이어집니다.</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">교차 검증을 위해 LassoCV 클래스에 의해 구현 된 좌표 하강 및 LassoLarsCV 클래스에 의해 구현 된 Lars (최소 각도 회귀)와 같은 올가미 경로를 계산하기 위해 2 가지 알고리즘과 함께 20 배의 알고리즘을 사용합니다. 두 알고리즘 모두 대략 동일한 결과를 제공합니다. 실행 속도와 수치 오류의 원인에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">메시지 문자열에 &quot;% (name) s&quot;가 있으면 사용자 지정 메시지의 경우 견적 자 이름으로 대체됩니다.</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">제공된 커널 함수의 정확한 수학적 공식과 &lt;code&gt;gamma&lt;/code&gt; , &lt;code&gt;coef0&lt;/code&gt; 및 &lt;code&gt;degree&lt;/code&gt; 가 서로 어떻게 영향을 미치는지에 대한 자세한 내용은 설명 설명서에서 해당 섹션을 참조하십시오 : &lt;a href=&quot;../svm#svm-kernels&quot;&gt;커널 함수&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">각 클래스 k에 대해 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 인 형태 [n_features, n_k]의 배열은 가우스 분포의 회전, 즉 주축입니다.</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">각 클래스 k에 대해 모양 [n_k]의 배열. 여기에는 주축을 따라 가우스 분포의 스케일링, 즉 회전 좌표계의 분산이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">각 모델 클래스에 대해 관련 모델 매개 변수를 선택하여 모델 복잡성을 변경하고 계산 성능 (대기 시간)과 예측 전력 (MSE 또는 해밍 손실)에 미치는 영향을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">각 성분 k에 대해 최대 corr (Xk u, Yk v)를 최대화하는 가중치 u, v를 찾고 &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">각 성분 k에 대해 다음을 최적화하는 가중치 u, v를 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">각 성분 k에 대해 다음을 최적화하는 가중치 u, v를 찾으십시오. &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; , &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">X의 각 데이터 포인트 x와 앙상블의 각 트리에 대해 리프 x의 인덱스가 각 추정기에서 끝납니다.</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">X의 각 데이터 포인트 x와 앙상블의 각 트리에 대해 리프 x의 인덱스가 각 추정기에서 끝납니다. 이진 분류의 경우 n_classes는 1입니다.</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">X의 각 데이터 포인트 x와 포리스트의 각 트리에 대해 리프 x의 인덱스가 끝납니다.</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">X의 각 데이터 포인트 x에 대해 리프 x의 인덱스가 끝납니다. 리프는 &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; , 번호 매기기에 공백이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">각 데이터 세트에 대해 샘플의 15 %가 랜덤 균일 노이즈로 생성됩니다. 이 비율은 OneClassSVM의 nu 매개 변수와 다른 이상치 탐지 알고리즘의 오염 매개 변수에 주어진 값입니다. 특이 치 탐지에 사용될 때 새 데이터에 적용 할 예측 방법이 없으므로 LOF (Local Outlier Factor)를 제외하고 특이 치와 특이 치 간의 결정 경계가 검은 색으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">각 문서 &lt;code&gt;#i&lt;/code&gt; 에 대해 각 단어 &lt;code&gt;w&lt;/code&gt; 의 발생 횟수를 세고 &lt;code&gt;X[i, j]&lt;/code&gt; 에 기능 &lt;code&gt;#j&lt;/code&gt; 의 값으로 저장하십시오. 여기서 &lt;code&gt;j&lt;/code&gt; 는 단어 &lt;code&gt;w&lt;/code&gt; 의 색인입니다. 사전에서 .</target>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">각 문서 \ (d \)에 대해 \ (\ theta_d \ sim \ mathrm {Dirichlet} (\ alpha), \ : d = 1 ... D \)를 그립니다.</target>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">각 관측치에 대해 적합 모형에 따라이 값을 내부로 간주해야하는지 (+1 또는 -1)를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">의사 결정 트리는 각 쌍의 홍채 특징에 대해 훈련 샘플에서 추론 된 간단한 임계 값 규칙의 조합으로 이루어진 결정 경계를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">각 샘플에 대해 생성 프로세스는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">각 주제 \ (k \)에 대해 \ (\ beta_k \ sim \ mathrm {Dirichlet} (\ eta), \ : k = 1 ... K \)</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; 의 각 값에 대해 을 플롯합니다.</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 에서 '목표'피처의 각 값에 대해 부분 의존 함수는 '보완'피처의 모든 가능한 값에 대한 트리 예측을 주 변화해야합니다. 의사 결정 트리에서이 기능은 교육 데이터를 참조하지 않고도 효율적으로 평가할 수 있습니다. 각 그리드 포인트에 대해 가중 트리 순회가 수행됩니다. 분할 노드에 '대상'기능이 포함 된 경우 해당 왼쪽 또는 오른쪽 분기가 따르고, 그렇지 않으면 두 분기가 따라 가며 각 분기에는 입력 된 훈련 샘플의 비율이 가중됩니다. 분기. 마지막으로, 부분 의존성은 모든 방문 된 잎의 가중 평균에 의해 주어진다. 트리 앙상블의 경우 각 개별 트리의 결과가 다시 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">문서 \ (d \)의 각 단어 \ (i \)에 대해 :</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">효율성을 위해 한 쌍의 행 벡터 x와 y 사이의 유클리드 거리는 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">여러 메트릭을 평가하려면 (고유 한) 문자열 목록을 제공하거나 이름을 키로, 콜 러블을 값으로 사용하여 받아쓰기를하십시오.</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">예를 들어 &lt;code&gt;average_precision&lt;/code&gt; 또는 roc 곡선 아래 면적은 이산 예측 만 사용하여 계산할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">예를 들어 &lt;code&gt;SVC&lt;/code&gt; 대신 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">예를 들어 &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; 계산 집약적 인 대안 은 &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">예를 들어, &lt;strong&gt;다항식 피처&lt;/strong&gt; 를 구성하여 간단한 선형 회귀를 확장 할 수 있습니다.&lt;strong&gt;&lt;/strong&gt; 계수에서 를 . 표준 선형 회귀 분석의 경우 2 차원 데이터에 대해 다음과 같은 모형이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">예를 들어, 각 점이 단지 하나의 숫자 (8 바이트) 인 경우, 가계도 \ (p \ sim 20 \) 차원에서 유효한 \ (k \)-NN 추정기는 현재 추정 크기보다 더 많은 훈련 데이터를 필요로합니다. 전체 인터넷 (&amp;plusmn; 1000 엑사 바이트 정도)</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">예를 들어, 랜덤 프로젝션에서이 경고는 대상 프로젝션 공간의 차원을 정량화하는 구성 요소의 수가 원래 소스 공간의 차원을 정량화하는 피처의 수보다 높으면 차원을 암시 할 때 발생합니다. 문제의 감소되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">예를 들어 여러 실험의 경우 &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt; 을 사용하여 기반으로 교차 검증을 만들 수 있습니다. 하나를 제외한 모든 실험의 샘플을 사용하여 트레이닝 세트를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">예를 들어, 다항식 Naive Bayes 분류기의 결과를 살펴보면 빠르게 학습하고 적절한 F 점수를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">예를 들어 &lt;code&gt;['words', 'wprds']&lt;/code&gt; 두 문서의 모음을 처리한다고 가정 해 봅시다 . 두 번째 문서에는 단어 'words'의 철자가 틀립니다. 간단한 단어 표현은이 두 가지 기능이 서로 다른 두 가지 매우 다른 문서로 간주합니다. 그러나 문자 2 그램 표현은 8 가지 기능 중 4 가지 기능에서 일치하는 문서를 찾을 수 있으므로 선호하는 분류자가 더 잘 결정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">예를 들어 시퀀스 분류기 (예 : 청커)를 훈련하기 위해 보완 태그로 사용하려는 품사 (PoS) 태그를 추출하는 첫 번째 알고리즘이 있다고 가정합니다. 다음 구절은 '고양이가 매트에 앉았다'는 문장에서 '토'라는 단어 주위에 추출 된 특징의 창일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">예를 들어 다음 스 니펫은 &lt;code&gt;chardet&lt;/code&gt; (scikit-learn과 함께 제공되지 않고 별도로 설치해야 함)을 사용하여 세 개의 텍스트 인코딩을 계산합니다. 그런 다음 텍스트를 벡터화하고 학습 한 어휘를 인쇄합니다. 출력은 여기에 표시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">예를 들어이 경고는 사용자가</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">예를 들어, 마우스 뇌에서 유전자 발현의 데이터 세트를 다운로드하려면 :</target>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">예를 들어, &lt;code&gt;counts&lt;/code&gt; 배열 의 첫 번째 문서에서 첫 번째 항의 tf-idf 를 다음과 같이 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">예를 들어, 부울 기능을 처리 할 때 모든 \ (n \)에 대해 \ (x_i ^ n = x_i \)이므로 쓸모가 없습니다. 그러나 \ (x_i x_j \)는 두 부울의 연결을 나타냅니다. 이런 식으로 선형 분류기로 XOR 문제를 해결할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">예를 들어, 유효성 검증 세트를 사용하는 경우 유효성 검증 세트의 &lt;code&gt;test_fold&lt;/code&gt; 모든 샘플에 대해 test_fold 를 0으로 설정하고 다른 모든 샘플에 대해서는 -1로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">사용 방법에 대한 예는 아래 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">바이어스-분산 분해에 대한 자세한 내용은 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]의&lt;/a&gt; 7.3 단원을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">자세한 내용은&amp;ldquo;t-SNE를 효과적으로 사용하는 방법&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; 은 다양한 매개 변수의 영향에 대한 자세한 설명과 이러한 효과를 탐색하기위한 대화식 그림을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">손으로 쓴 숫자 인식과 같이 흰색 배경에서 픽셀 값을 흑도로 해석 할 수있는 그레이 스케일 이미지 데이터의 경우 Bernoulli Restricted Boltzmann 머신 모델 ( &lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; )은 효과적인 비선형 피쳐 추출을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">공 선형 회귀자가 많은 고차원 데이터 세트의 경우 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 가 가장 선호됩니다. 그러나 &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수 의 관련 값을 더 많이 탐색 할 수 있다는 장점이 있으며, 샘플 수가 기능 수에 비해 매우 적 으면 종종 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 보다 빠릅니다 .</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">NIST 전처리 루틴에 대한 정보는 MD Garris, JL Blue, GT Candela, DL Dimmick, J. Geist, PJ Grother, SA Janet 및 CL Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">예를 들어, 전자 메일과 같은 10,000 개의 짧은 텍스트 문서 모음은 총 100,000 개의 고유 단어 순서의 크기를 가진 어휘를 사용하지만 각 문서는 100에서 1000 개의 고유 단어를 개별적으로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">예를 들어, 학습 벡터 알고리즘의 목적 함수에 사용되는 많은 요소 (예 : Support Vector Machine의 RBF 커널 또는 선형 모델의 L1 및 L2 정규화 기)는 모든 기능이 0을 중심으로하고 동일한 순서로 분산되어 있다고 가정합니다. 특징이 다른 것보다 큰 차수의 변동을 갖는 경우, 목적 함수를 지배하고 추정기가 예상대로 다른 특징에서 올바르게 학습하지 못할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">예를 들어 아래 주어진 테이블</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">예를 들어, 그룹은 샘플을 수집 한 연도 일 수 있으므로 시간 기반 분할에 대한 교차 검증을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">예를 들어, 이너 데이터가 가우스 분포라고 가정하면, 이너 위치와 공분산은 강력한 방식으로 (즉, 이상치의 영향을받지 않음) 추정합니다. 이 추정으로부터 얻은 마할 라 노비스 거리는 외곽의 척도를 도출하는 데 사용됩니다. 이 전략은 아래에 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">예를 들어, 모양 행렬 &lt;code&gt;(10, 10)&lt;/code&gt; 주어지면 3 개의 행과 2 개의 열이있는 하나의 가능한 bicluster는 모양의 행렬 &lt;code&gt;(3, 2)&lt;/code&gt; 유도합니다 .</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">예를 들어, \ (p \) 특이 벡터가 계산 된 경우 \ (q &amp;lt;p \)에서 설명한대로 최적의 \ (q \)를 찾습니다. \ (U \)를 \ (q \) 가장 왼쪽에있는 특이한 단일 벡터 열과 오른쪽에 대해 \ (V \)가있는 행렬로 둡니다. 행을 분할하기 위해 \ (A \)의 행은 \ (q \) 차원 공간으로 투영됩니다 : \ (A * V \). 이 \ (m \ times q \) 행렬의 \ (m \) 행을 샘플로 취급하고 k- 평균을 사용하여 클러스터링하면 행 레이블이 생성됩니다. 마찬가지로 열을 \ (A ^ {\ top} * U \)에 투영하고이 \ (n \ times q \) 행렬을 클러스터하면 열 레이블이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">예를 들어, 얼굴 인식을 위해 64x64 픽셀 회색 레벨 그림으로 작업하는 경우 데이터의 차원은 4096이며 이러한 넓은 데이터에 대해 RBF 지원 벡터 시스템을 학습하는 속도가 느립니다. 또한 우리는 인간 얼굴의 모든 그림이 다소 비슷하게 보이기 때문에 데이터의 고유 차원이 4096보다 훨씬 낮다는 것을 알고 있습니다. 샘플은 훨씬 낮은 치수의 매니 폴드 (예 : 약 200)에 놓여 있습니다. PCA 알고리즘을 사용하면 차원을 줄이고 동시에 설명 된 분산을 대부분 보존하면서 데이터를 선형으로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">예를 들어, 숫자 데이터 세트의 경우, &lt;code&gt;digits.data&lt;/code&gt; 는 숫자 샘플을 분류하는 데 사용할 수있는 기능에 대한 액세스를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">예를 들어, 아래 예에서 의사 결정 트리는 데이터에서 학습하여 if-then-else 결정 규칙 세트로 사인 곡선을 근사화합니다. 나무가 깊을수록 의사 결정 규칙이 더 복잡하고 모델이 더 적합합니다.</target>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">예를 들어, 학습 벡터 알고리즘의 목적 함수에 사용되는 많은 요소 (예 : Support Vector Machines의 RBF 커널 또는 선형 모델의 l1 및 l2 정규화 기)는 모든 기능이 0을 중심으로 동일한 순서로 분산되어 있다고 가정합니다. 특징이 다른 것보다 큰 차수의 변동을 갖는 경우, 목적 함수를 지배하고 추정기가 예상대로 다른 특징에서 올바르게 학습하지 못할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">예를 들어, 다음은 Olivetti 데이터 세트에서 16 개의 샘플 인물 사진 (0.0을 중심으로)입니다. 오른쪽에는 처음 16 개의 특이 벡터가 초상화로 재구성되었습니다. 크기가 \ (n_ {samples} = 400 \)이고 \ (n_ {features} = 64 \ times 64 = 4096 \) 인 데이터 집합의 상위 16 개 특이 벡터 만 필요하므로 계산 시간은 1 초 미만입니다.</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">예를 들어, 샘플에 대해 \ (\ chi ^ 2 \) 테스트를 수행하여 다음과 같은 두 가지 최상의 기능 만 검색 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">예를 들어 :</target>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 멀티 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용되고 그렇지 않으면 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 경우 &lt;code&gt;y&lt;/code&gt; 는 바이너리도 멀티 클래스도 아닌, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; 가&lt;/a&gt; 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 추정기가 분류 자이거나 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스가 아닌 경우 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 분류자가 True이고 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 추정기가 분류 자이고 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">중간 값의 경우 두 번째 그림에서 &lt;code&gt;C&lt;/code&gt; 와 &lt;code&gt;gamma&lt;/code&gt; 의 대각선에서 좋은 모델을 찾을 수 있음을 알 수 있습니다 . 부드러운 점 ( &lt;code&gt;gamma&lt;/code&gt; 값이 낮을수록 )은 각 점을 올바르게 분류하는 중요성 (더 큰 &lt;code&gt;C&lt;/code&gt; 값) 을 증가시킴으로써 더 복잡한 모델을 만들 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 (n_samples_test, n_samples_train)입니다.</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 [n_samples_test, n_samples_train]입니다.</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">대규모 데이터 세트의 경우 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 사용을 고려할 수도 있습니다. '로그'손실과 함께 .</target>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">SVM을 포함한 많은 추정기의 경우, 각 기능에 대해 단위 표준 편차가있는 데이터 세트를 갖는 것이 좋은 예측을 얻는 데 중요합니다.</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">단일 출력 작업의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">자세한 내용은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani (2004)&amp;ldquo;최소 각 회귀 분석&amp;rdquo;, 통계 분석 (토론 포함), 407-499를 참조하십시오. ( &lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">자세한 정보는 &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;계층 적 클러스터링을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">사용법에 대한 자세한 내용은 사용 &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;설명서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">다중 클래스 분류의 경우 &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt; 는 AdaBoost-SAMME 및 AdaBoost-SAMME.R을 구현합니다. &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">멀티 클래스 분류의 경우 n_class 분류기는 일대일 접근 방식으로 훈련됩니다. 구체적으로 이는 Ridge에서 다변량 응답 지원을 활용하여 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">다중 클래스 모델의 경우 &lt;code&gt;label&lt;/code&gt; 인수 를 통해 PDP를 작성해야하는 클래스 레이블을 설정해야합니다 .</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">다중 메트릭 평가의 경우, 모든 스코어러에 대한 점수는 위에 표시된 &lt;code&gt;'_score'&lt;/code&gt; 대신 해당 스코어러 이름 ( &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; )으로 끝나는 키 의 &lt;code&gt;cv_results_&lt;/code&gt; dict에서 사용할 수 있습니다 . ( 'split0_test_precision', 'mean_train_precision'등)</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">다중 메트릭 평가의 경우이 특성은 &lt;code&gt;scoring&lt;/code&gt; 키를 스코어러 호출 가능에 맵핑하는 유효성 검증 된 스코어링 dict를 보유합니다 .</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">다중 메트릭 평가의 경우이 속성은 &lt;code&gt;refit&lt;/code&gt; 가 지정된 경우에만 존재 합니다.</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">다중 메트릭 평가를 들어,이 경우 사용할 수 없습니다 &lt;code&gt;refit&lt;/code&gt; 입니다 &lt;code&gt;False&lt;/code&gt; . 자세한 내용은 &lt;code&gt;refit&lt;/code&gt; 매개 변수를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">다중 메트릭 평가의 경우, &lt;code&gt;refit&lt;/code&gt; 가 지정된 경우에만 나타납니다 .</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">다중 출력 작업의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">다중 출력의 경우 y의 각 열의 가중치가 곱해집니다.</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">&quot;음수 클래스&quot;를 사용하는 멀티 클래스 분류의 경우 일부 레이블을 제외 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">멀티 클래스 문제의 경우 'newton-cg', 'sag', 'saga'및 'lbfgs'만 다항 손실을 처리합니다. 'liblinear'는 1 대 휴식 계획으로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">인스턴스 당 여러 레이블의 경우 &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">다중 메트릭 평가의 경우, 이는 스코어러가 끝에 추정기를 다시 시작하기위한 최상의 매개 변수를 찾는 데 사용됨을 나타내는 문자열이어야합니다.</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">다중 메트릭 평가의 경우, 이는 마지막에 추정기를 다시 시작하기위한 최상의 매개 변수를 찾는 데 사용되는 득점자를 나타내는 문자열이어야합니다.</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">승수 업데이트 ( 'mu') 솔버의 경우, Frobenius 규범 (0.5 * || X-WH || _Fro ^ 2)은 beta_loss 매개 변수를 변경하여 다른 베타-분산 손실로 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">n_components == 'mle'의 경우이 클래스는 &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt; 방법을 사용합니다 . NIPS에서, 598-604 페이지</target>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">스파 &lt;code&gt;coef_&lt;/code&gt; 아닌 모델의 경우, 즉 coef_에 0이 많지 않은 경우 실제로 메모리 사용량 이 &lt;em&gt;증가&lt;/em&gt; 할 수 있으므로이 방법을주의해서 사용하십시오. 일반적으로 &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; 으로 계산할 수있는 0 요소의 수는 50 % 이상이어야 상당한 이점을 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">정규화 된 상호 정보 및 조정 된 상호 정보의 경우, 정규화 값은 일반적으로 각 군집의 엔트로피의 일부 &lt;em&gt;일반화 된&lt;/em&gt; 평균이다. 다양한 일반화 된 수단이 존재하며, 하나를 다른 것보다 선호하기위한 확실한 규칙이 존재하지 않습니다. 결정은 주로 필드별로 이루어집니다. 예를 들어, 커뮤니티 감지에서 산술 평균이 가장 일반적입니다. 각 정규화 방법은 &quot;정 성적으로 유사한 행동&quot;[YAT2016]을 제공합니다. 우리의 구현에서 이것은 &lt;code&gt;average_method&lt;/code&gt; 매개 변수에 의해 제어됩니다 .</target>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">지금은 추정기를 블랙 박스로 간주합니다.</target>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">모수 추정의 사후 분포는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">회귀의 경우 기본 학습 속도 일정은 다음과 같이 주어진 역 스케일링 ( &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; )입니다.</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">제곱 손실과 l2 페널티로 회귀하는 경우 평균 전략을 사용하는 SGD의 또 다른 변형이 SG (Stochastic Average Gradient) 알고리즘으로 제공되며 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 에서 솔버로 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">회귀를 위해 &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt; 는 AdaBoost.R2를 구현합니다 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">회귀를 위해 MLP는 제곱 오류 손실 기능을 사용합니다. 로 쓴</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">회귀 : &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">scikit-learn 버전 0.14.1 이전의 경우, 밀도가 높은 np.matrix 인스턴스를 반환하여 return_as = np.ndarray를 처리했습니다. 앞으로 np.ndarray는 예상대로 np.ndarray를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">간단한 변환을 위해 Transformer 객체 대신 변환 및 역 매핑을 정의하는 함수 쌍을 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">간단하게하기 위해 위의 방정식은 단일 교육 예를 위해 작성되었습니다. 가중치에 대한 구배는 상기의 것에 대응하는 2 개의 용어로 형성된다. 그것들은 일반적으로 각각의 부호 때문에 양의 기울기와 음의 기울기로 알려져 있습니다. 이 구현에서, 기울기는 샘플의 미니 배치에 대해 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">스코어링 매개 변수가 문자열, 호출 가능 또는 없음 인 단일 메트릭 평가의 경우 키는- &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">작은 데이터 세트 (\ (N \) 30 미만)의 경우 \ (\ log (N) \)는 \ (N \)와 비슷하며 무차별 알고리즘은 트리 기반 방식보다 더 효율적일 수 있습니다. &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; 는&lt;/a&gt; 모두 &lt;em&gt;리프 크기&lt;/em&gt; 매개 변수 를 제공 하여이 문제를 해결합니다 . 이는 쿼리가 무차별 대입으로 전환되는 샘플 수를 제어합니다. 이를 통해 두 알고리즘 모두 작은 \ (N \)에 대한 무차별 계산의 효율성에 접근 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">작은 데이터 세트의 경우 'liblinear'가 좋은 선택이지만 'sag'와 'saga'는 큰 데이터의 경우 더 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">일부 응용 분야의 경우, 전통적인 접근 방식에서는 예제, 기능 (또는 둘 다) 및 / 또는 처리 속도가 어려울 수 있습니다. 이 경우 scikit-learn에는 시스템 확장을 고려할 수있는 여러 가지 옵션이 있습니다.</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">일부 애플리케이션의 경우 추정기의 성능 (주로 예측 시간의 대기 시간 및 처리량)이 중요합니다. 교육 처리량을 고려하는 것도 흥미로울 수 있지만 프로덕션 환경 (종종 오프라인에서 발생하는 경우)에서는 덜 중요합니다.</target>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">일부 데이터 집합의 경우 사전 정의 된 데이터 분할을 교육 및 유효성 검사 접음 또는 여러 교차 유효성 검사 접음으로 이미 존재합니다. 사용 &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; 를&lt;/a&gt; 이 하이퍼 파라미터를 검색 할 때 이러한 주름 등을 사용하는 것이 가능하다.</target>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">이미지, 비디오 및 오디오와 같은 기타 데이터의 경우 다음을 참조 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">스파 스 입력의 경우 데이터는 효율적인 Cython 루틴에 공급되기 전에 &lt;strong&gt;압축 스파 스 행 표현&lt;/strong&gt; ( &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 참조 ) 으로 &lt;strong&gt;변환됩니다&lt;/strong&gt; . 불필요한 메모리 복사를 피하려면 업스트림에서 CSR 표현을 선택하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">스파 스 입력의 경우 데이터가 &lt;strong&gt;압축 스파 스 행 표현으로 변환됩니다&lt;/strong&gt; ( &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 참조 ). 불필요한 메모리 복사를 피하려면 업스트림에서 CSR 표현을 선택하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">속도 및 공간 효율성을 위해 &lt;code&gt;scikit-learn&lt;/code&gt; 은 &lt;code&gt;target_names&lt;/code&gt; 목록 의 범주 이름 색인에 해당하는 정수의 배열로 대상 속성을로드 합니다. 각 샘플의 범주 정수 ID는 &lt;code&gt;target&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">속도를 높이기 위해 모든 실제 작업은 copy_predict (libsvm_helper.c) 함수의 C 레벨에서 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">데이터 세트의 명시적인 도메인 별 계층화에 따라 데이터를 분할합니다.</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">데이터 세트의 명시적이고 도메인 별 계층화에 따라 데이터를 분할합니다.</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">svd_solver == 'arpack'에 대해서는 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">svd_solver == 'randomized'의 경우 &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; 또한 &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">'liblinear'의 경우 'sag'및 'lbfgs'솔버는 verbose를 자세한 양수로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">비선형 커널을 사용하는 &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt; 알고리즘 제품군의 경우 지연 시간은 지원 벡터 수에 따라 달라집니다 (더 빠를수록 빠름). 지연 시간과 처리량은 SVC 또는 SVR 모델의 지원 벡터 수에 따라 (무증상) 선형으로 증가해야합니다. 커널은 지원 벡터 당 한 번 입력 벡터의 투영을 계산하는 데 사용되므로 대기 시간에 영향을줍니다. 다음 그래프에서 &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; 의 &lt;code&gt;nu&lt;/code&gt; 매개 변수 는 지원 벡터 수에 영향을주기 위해 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">를 들어 &lt;code&gt;l2&lt;/code&gt; 페널티 경우, 최상의 결과는 케이스에서 온다 &lt;code&gt;C&lt;/code&gt; 가 조절되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">Cs 값의 그리드 (기본적으로 1e-4와 1e4 사이의 로그 스케일에서 10 개의 값으로 설정 됨)의 경우 교차 검증 자 StratifiedKFold에서 최상의 하이퍼 파라미터를 선택하지만 cv 매개 변수를 사용하여 변경할 수 있습니다. newton-cg 및 lbfgs 솔버의 경우 경로를 따라 웜 스타트합니다. 즉, 현재 맞춤의 초기 계수가 이전 맞춤의 수렴 후 얻은 계수 인 것으로 추측하므로 고차원 밀도의 경우 더 빠릅니다. 데이터.</target>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">liblinear 및 lbfgs 솔버의 경우 verbose를 자세한 양수로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">가장 일반적인 사용 사례의 경우 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 사용하여 채점자 개체를 지정할 수 있습니다 . 아래 표는 가능한 모든 값을 보여줍니다. 모든 채점자 개체 &lt;strong&gt;는 반환 값이 높을수록 반환 값이 낮을수록 좋습니다&lt;/strong&gt; . 이와 같은 모델 데이터 사이의 거리를 측정하는 측정 항목 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; 은&lt;/a&gt; 메트릭의 부정 값을 반환 neg_mean_squared_error로서 가능하다.</target>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 라는 이름의 근거 , 즉 선형 분류 자로서 순진 Bayes에 대해서는 J. Rennie et al. (2003), 순진한 Bayes 텍스트 분류기, ICML의 잘못된 가정을 다루기.</target>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">두 데이터 세트 사이에서 가장 가까운 이웃을 찾는 간단한 작업을 위해 &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; 내의 비 감독 알고리즘을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">이 데이터의 경우 &lt;code&gt;'city'&lt;/code&gt; 열을 범주 형 변수로 인코딩하고 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 를 &lt;code&gt;'title'&lt;/code&gt; 열에 적용 할 수 있습니다 . 동일한 열에서 여러 기능 추출 방법을 사용할 수 있으므로 각 변환기에 고유 한 이름 ( &lt;code&gt;'city_category'&lt;/code&gt; 및 &lt;code&gt;'title_bow'&lt;/code&gt; )을 부여 합니다. 기본적으로 나머지 등급 열은 무시됩니다 ( &lt;code&gt;remainder='drop'&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">이 예에서는 각각 103 개의 특징과 14 개의 가능한 레이블이있는 2417 개의 데이터 포인트를 포함 하는 &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;효모&lt;/a&gt; 데이터 세트를 사용합니다 . 각 데이터 포인트에는 하나 이상의 레이블이 있습니다. 기준으로서 우리는 먼저 14 개의 레이블 각각에 대해 로지스틱 회귀 분류기를 훈련시킵니다. 이러한 분류기의 성능을 평가하기 위해 보류 테스트 세트를 예측하고 &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard 유사성 점수를&lt;/a&gt; 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">이 방법에서, M은 밀집 행렬, 희소 행렬 또는 일반 선형 연산자 일 수있다. 경고 : ARPACK은 일부 문제로 인해 불안정 할 수 있습니다. 결과를 확인하기 위해 여러 개의 임의의 씨앗을 사용하는 것이 가장 좋습니다.</target>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">이를 위해 추정기는 '연결성'행렬을 사용하여 어떤 샘플이 연결되는지 제공합니다.</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">이러한 이유로 20 개의 뉴스 그룹 데이터를로드하는 함수는 &lt;strong&gt;remove&lt;/strong&gt; 라는 매개 변수를 제공하여 각 파일에서 어떤 종류의 정보를 제거할지 알려줍니다. &lt;strong&gt;remove&lt;/strong&gt; 는 &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; 하위 집합을 포함하는 튜플이어야하며 헤더, 서명 블록 및 인용 블록을 각각 제거하도록 지시합니다.</target>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">두 군집의 경우 유사성 그래프 에서 &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;정규화 된 절단&lt;/a&gt; 문제 의 볼록한 이완 문제를 해결합니다. 그래프를 2 개로 절단하여 절단면의 무게가 각 군집 내부의 가장자리의 무게에 비해 작습니다. 이 기준은 이미지 작업시 특히 흥미 롭습니다. 그래프 정점은 픽셀이며 유사성 그래프의 가장자리는 이미지의 기울기 함수입니다.</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">여러 가지 이유로, 많은 실제 데이터 세트에는 종종 공백, NaN 또는 기타 자리 표시 자로 인코딩 된 결 측값이 포함됩니다. 그러나 이러한 데이터 세트는 배열의 모든 값이 숫자이고 모두 의미를 가지고 있다고 가정하는 scikit-learn 추정기와 호환되지 않습니다. 불완전한 데이터 세트를 사용하는 기본 전략은 결 측값이 포함 된 전체 행 및 / 또는 열을 삭제하는 것입니다. 그러나 이것은 가치가없는 데이터를 잃어 버릴 수 있습니다 (불완전하지만). 더 나은 전략은 결 측값을 대치하는 것입니다. 즉 데이터의 알려진 부분에서이를 유추하는 것입니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;대치에 대한 공통 용어 및 API 요소 용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">시각화 목적으로 (t-SNE의 주요 사용 사례) Barnes-Hut 방법을 사용하는 것이 좋습니다. 정확한 t-SNE 방법은 더 높은 차원 공간에서 임베딩의 이론적 속성을 확인하는 데 유용하지만 계산 제약으로 인해 작은 데이터 세트로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">시각화를 위해, bicluster가 주어지면, 데이터 행렬의 행과 열은 bicluster를 연속적으로 만들기 위해 재 배열 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">시각화를 위해 2D 캔버스에 다른 기호를 배치해야합니다. 이를 위해 &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold 학습&lt;/a&gt; 기술을 사용 하여 2D 임베딩을 검색합니다.</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;working_memory&lt;/code&gt; 를 줄임으로써 행 단위로 강제 작성 :</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina, M. et al., PARVUS-데이터 탐색, 분류 및 상관 관계를위한 확장 가능한 패키지. 16ga Genoa, Brigata Salerno를 통한 제약 및 식품 분석 및 기술 연구소.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 평균 정밀도는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 범위는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 순위 손실은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">지정된 함수에 의해 주어진 선호도 매트릭스를 형성하고 스펙트럼 분해를 해당 그래프 라플라시안에 적용합니다. 결과 변환은 각 데이터 포인트에 대한 고유 벡터의 값으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">다행히도 &lt;strong&gt;X에서 대부분의 값은 0이 될 것&lt;/strong&gt; 입니다. 주어진 문서에 대해 수천 개 미만의 별개의 단어가 사용되기 때문입니다. 이러한 이유로 우리는 단어 모음이 일반적으로 &lt;strong&gt;고차원 희소 데이터 세트&lt;/strong&gt; 라고 말합니다 . 특징 벡터의 0이 아닌 부분 만 메모리에 저장함으로써 많은 메모리를 절약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">프로젝트와 기여에 대해 자주 묻는 질문.</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman et al., &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&quot;그래픽 올가미를 사용한 희소 역공 분산 추정&quot;&lt;/a&gt; , Biostatistics 9, pp 432, 2008</target>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">Friedman, &quot;스토리지 그라디언트 부스팅&quot;, 1999</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">Friedman, 확률 적 그라디언트 부스팅, 1999</target>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">트레이닝 데이터 &lt;code&gt;X&lt;/code&gt; 와 적합 모델로부터의 재구성 된 데이터 &lt;code&gt;WH&lt;/code&gt; 사이의 매트릭스 차이 또는 베타-분산의 프로 베니 우스 표준 .</target>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">이미지에서</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">발생에서 빈도까지</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">scikit-learn에서 : [ 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']. 이 메트릭은 희소 행렬 입력을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">scipy.spatial.distance에서 : [ 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'minkowski', 'rogerstanimoto ','russellrao ','seuclidean ','sokalmichener ','sokalsneath ','sqeuclidean ','yule '] 이러한 메트릭에 대한 자세한 내용은 scipy.spatial.distance 설명서를 참조하십시오. 이 메트릭은 희소 행렬 입력을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">텍스트에서</target>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">구현 관점에서 이것은 예측 객체로 감싸 진 평범한 최소 제곱 (scipy.linalg.lstsq)입니다.</target>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">프로그래밍 관점에서 볼 때 scikit-learn의 온라인 API를 사용하여 청크별로 매우 큰 데이터 세트를 처리하는 방법을 보여주기 때문에 흥미 롭습니다. 진행 방법은 한 번에 이미지를로드하고이 이미지에서 무작위로 50 개의 패치를 추출하는 것입니다. 10 개의 이미지를 사용 하여이 패치 중 500 개가 누적되면 온라인 KMeans 객체 MiniBatchKMeans 의 &lt;code&gt;partial_fit&lt;/code&gt; 메소드를 실행합니다 .</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값) 또는 점수가있는 단일 배열을 반환하는 함수입니다. 기본값은 f_classif입니다 (아래 참조). 기본 기능은 분류 작업에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값)을 반환하는 함수입니다. 기본값은 f_classif입니다 (아래 참조). 기본 기능은 분류 작업에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값)을 반환하는 함수입니다. '백분위 수'또는 'kbest'모드의 경우 단일 배열 점수를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 전에 &lt;code&gt;y&lt;/code&gt; 에 적용하는 기능 . &lt;code&gt;transformer&lt;/code&gt; 와 동시에 설정할 수 없습니다 . 이 함수는 2 차원 배열을 반환해야합니다. 경우 &lt;code&gt;func&lt;/code&gt; 없습니다 &lt;code&gt;None&lt;/code&gt; , 사용되는 함수는 식별 기능이 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">회귀 예측에 적용하는 기능. &lt;code&gt;transformer&lt;/code&gt; 와 동시에 설정할 수 없습니다 . 이 함수는 2 차원 배열을 반환해야합니다. 역함수는 예측을 원래 훈련 레이블의 동일한 공간으로 되 돌리는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">데이터 센터링 및 스케일링 데이터의 중요성에 대한 자세한 내용은이 FAQ에서 확인할 수 있습니다. 데이터를 &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;표준화 / 표준화 / 조정해야합니까?&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">'whiten = True'를 사용하여 피처에서 선형 상관 관계를 추가로 제거합니다.</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">또한이 모델은 샘플이 둘 이상의 클래스에 속할 수있는 &lt;a href=&quot;multiclass#multiclass&quot;&gt;다중 레이블 분류&lt;/a&gt; 를 지원합니다 . 각 클래스에 대해 원시 출력은 로지스틱 함수를 통과합니다. &lt;code&gt;0.5&lt;/code&gt; 보다 크거나 같은 값 은 &lt;code&gt;1&lt;/code&gt; 로 반올림되고 , 그렇지 않으면 &lt;code&gt;0&lt;/code&gt; 으로 반올림됩니다 . 샘플의 예측 출력에서 ​​값이 &lt;code&gt;1&lt;/code&gt; 인 인덱스 는 해당 샘플의 할당 된 클래스를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">또한 &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt; 는 &lt;strong&gt;대칭입니다&lt;/strong&gt; . 인수를 바꾸어도 점수가 변경되지 않습니다. 따라서 &lt;strong&gt;합의 조치&lt;/strong&gt; 로 사용될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">또한 처리 된 예제 수에 따라 다른 알고리즘의 성능이 발전한 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">또한 기본 매개 변수 &lt;code&gt;smooth_idf=True&lt;/code&gt; 는 컬렉션의 모든 항을 정확히 한 번만 포함하는 추가 문서가있는 것처럼 분자와 분모에&amp;ldquo;1&amp;rdquo;을 추가하여 0 나누기를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">또한 tf 및 idf를 계산하는 데 사용되는 공식은 다음과 같이 IR에 사용 된 SMART 표기법에 해당하는 매개 변수 설정에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;확률로 표현 된 예측 확인&lt;/a&gt; , 월별 날씨 검토 78.1 (1950)</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G. Celeux, M. El Anbari, J.-M. Marin, CP Robert,&amp;ldquo;회귀 규칙 화 : 정보가 부족한 상황에서 베이지안과 잦은 방법 비교&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;통계 학습 소개&lt;/a&gt; , Springer 2013.</target>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. Louppe 및 P. Geurts, &quot;임의 패치에 대한 앙상블&quot;, 데이터베이스의 기계 학습 및 지식 발견, 346-361, 2012.</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G. Louppe,&amp;ldquo;임의의 숲 이해 : 이론에서 실제까지&amp;rdquo;, 박사 학위 논문, Liege of Liege, 2014.</target>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP Box와 DR Cox,&amp;ldquo;변형 분석&amp;rdquo;, Royal Statistical Society B, 26, 211-252 (1964).</target>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB는 단계별 단계 방식으로 덧셈 모델을 만듭니다. 임의의 차별화 가능한 손실 함수를 최적화 할 수 있습니다. 각 단계에서 &lt;code&gt;n_classes_&lt;/code&gt; 회귀 트리는 이항 또는 다항 이탈 손실 함수의 음의 구배에 적합합니다. 이진 분류는 단일 회귀 트리 만 유도되는 특별한 경우입니다.</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GB는 단계별 단계 방식으로 덧셈 모델을 만듭니다. 임의의 차별화 가능한 손실 함수를 최적화 할 수 있습니다. 각 단계에서 회귀 트리는 주어진 손실 함수의 음의 구배에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">GBRT는 다음 형식의 추가 모델을 고려합니다.</target>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">GMM 공분산</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G [i, j]는 그래프를 따라 점 i에서 점 j까지의 최단 거리를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">스핑크스 갤러리에 의해 생성 된 갤러리</target>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">RBF, 라플라시안, 다항식, 지수 chi2 및 시그 모이 드 커널에 대한 감마 매개 변수. 기본값의 해석은 커널에 맡겨져 있습니다. sklearn.metrics.pairwise 설명서를 참조하십시오. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">rbf, poly 및 sigmoid 커널의 감마 매개 변수. 다른 커널에서는 무시됩니다. 기본적으로 0.1입니다.</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">게이츠, GW (1972)&amp;ldquo;가장 가까운 이웃 규칙 감소&amp;rdquo;. 정보 이론에 관한 IEEE 거래, 1972 년 5 월, 431-433.</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">가우스 혼합 모델 타원체</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">가우스 혼합 모델 선택</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">가우스 혼합 모델 사인 곡선</target>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">가우스 혼합물.</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">가우스 나이브 베이 즈 (GaussianNB)</target>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">가우스 프로세스 회귀 : 기본 입문 예</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">가우스 커널 ( &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">가우스 혼합 모델은 다양한 추론에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">군집에 유용한 가우시안 혼합 모델은 혼합 모델 전용 &lt;a href=&quot;mixture#mixture&quot;&gt;설명서의 다른 장에&lt;/a&gt; 설명되어 있습니다. KMeans는 성 분당 공분산이 동일한 가우스 혼합 모델의 특수 사례로 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">가우스 혼합물</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">Laplace 근사를 기반으로 한 가우스 프로세스 분류 (GPC).</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">홍채 데이터 세트의 가우스 프로세스 분류 (GPC)</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">Mauna Loa CO2 데이터에 대한 가우시안 프로세스 회귀 (GPR).</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">잡음 수준 추정 기능이있는 가우시안 프로세스 회귀 (GPR)</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">가우스 프로세스 회귀 (GPR).</target>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayes는 확률을 0 또는 1로 푸시하는 경향이 있습니다 (히스토그램의 카운트 참고). 이는 클래스에 따라 기능이 조건부로 독립적이라는 가정을하기 때문입니다.이 데이터 세트에는 2 개의 중복 기능이 포함되어 있지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">일반 KDD 구조 :</target>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">범용, 균일 한 클러스터 크기, 평평한 형상, 클러스터가 너무 많지 않음</target>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">일반적으로 모델의 복잡성이 증가하면 예측력과 대기 시간이 증가합니다. 예측력을 높이는 것은 일반적으로 흥미롭지 만 많은 응용 프로그램에서 예측 대기 시간을 너무 많이 늘리지 않는 것이 좋습니다. 우리는 이제 여러 가지 감독 모델의이 아이디어를 검토 할 것입니다.</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">선택적 감소로 청크로 거리 매트릭스 청크 생성</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">종 모양의 특이 값으로 대부분 낮은 순위의 행렬 생성</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">지정된 차수 이하의 차수를 가진 모든 다항식 조합으로 구성된 새 피처 행렬을 생성합니다. 예를 들어, 입력 샘플이 2 차원이고 [a, b] 형태 인 경우, 차수 -2 다항식 특징은 [1, a, b, a ^ 2, ab, b ^ 2]입니다.</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">임의의 다중 레이블 분류 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">임의의 n- 클래스 분류 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">드문 상관없는 설계로 랜덤 회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">랜덤 회귀 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">임의의 대칭 양수 한정 행렬을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">사전 요소의 희소 조합으로 신호를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">희소 랜덤 프로젝션 매트릭스 생성</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">희소 대칭 한정 양수 행렬을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">스위스 롤 데이터 세트를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">S 곡선 데이터 셋을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">바이커 스터링을 위해 블록 바둑판 구조로 배열을 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">바이 블러스터 링을 위해 일정한 블록 대각선 구조로 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">각 입력 데이터 포인트에 대해 교차 검증 된 추정값 생성</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">데이터 세트를 생성하십시오. 알고리즘의 확장 성을 볼 수있을만큼 큰 크기를 선택하지만 너무 긴 실행 시간을 피하기에는 너무 크지 않습니다</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">등방성 가우스 및 라벨 샘플을 Quantile로 생성</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">클러스터링을위한 등방성 가우스 블롭을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">X에 대한 결 측값 표시기를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">다항식 및 교호 작용 피처를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">피팅 된 가우스 분포에서 랜덤 표본을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">모델에서 임의의 샘플을 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 1&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 2&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 3&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">Hastie et al.에서 사용 된 이진 분류에 대한 데이터를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">Hastie et al.에서 사용 된 이진 분류에 대한 데이터를 생성합니다. 2009, 실시 예 10.2.</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">사전 정의 된 분할을 기반으로 기차 / 테스트 지수를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">랜덤 순열을 기반으로 기차 / 테스트 지수를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">주어진 분포에서 샘플링 된 모수에 대한 생성기.</target>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">n까지 올라가는 n_packs 슬라이스를 생성하는 생성기</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">형상 (메트릭 사용)</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">조지 W 부시</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">게르하르트 슈뢰더</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">sklearn에서 모든 견적 목록을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">선택한 피처의 마스크 또는 정수 인덱스를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">문자열에서 득점자 확보</target>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">모든 변압기에서 기능 이름을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">이 추정기에 대한 모수를 얻으십시오.</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">이 커널의 매개 변수를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">진단 목적으로 교차 검증의 각 분할에서 예측을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">문자열 식별자에서 주어진 거리 메트릭을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">VotingClassifier의 매개 변수 가져 오기</target>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">정밀 행렬의 게터.</target>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">2 개의 다변량 코 발리 2 차원 데이터 세트, X 및 Y가 주어지면, PLS는 '공분산의 방향', 즉 두 데이터 세트 간의 가장 공유 된 분산을 설명하는 각 데이터 세트의 구성 요소를 추출합니다. 이것은 &lt;strong&gt;산점도 매트릭스&lt;/strong&gt; 디스플레이 에서 명백하다 : 데이터 세트 X 및 데이터 세트 Y의 성분 1은 최대로 상관된다 (점은 첫 번째 대각선 주위에있다). 두 데이터 세트의 구성 요소 2에서도 마찬가지입니다. 그러나 다른 구성 요소에 대한 데이터 세트 간의 상관 관계는 약합니다. 포인트 클라우드는 매우 구형입니다.</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">반복 \ (t \)에 대한 후보 중심 \ (x_i \)이 주어지면 후보는 다음 방정식에 따라 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">두 가지 기능이있는 데이터 세트가 제공되면 인코더가 기능 당 고유 한 값을 찾고 데이터를 이진 one-hot 인코딩으로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">두 개의 특징을 가진 데이터 세트가 주어지면, 우리는 인코더가 특징 당 고유 값을 찾고 데이터를 서수 인코딩으로 변환 할 수있게합니다.</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">\ ((x_1, y_1), (x_2, y_2), \ ldots, (x_n, y_n) \) 일련의 학습 예제가 제공됩니다. 여기서 \ (x_i \ in \ mathbf {R} ^ n \) 및 \ (y_i \ \ {0, 1 \} \)에서 하나의 숨겨진 레이어 하나의 숨겨진 뉴런 MLP는 \ (f (x) = W_2 g (W_1 ^ T x + b_1) + b_2 \) 함수를 학습합니다. 여기서 \ (W_1 \ in \ mathbf {R} ^ m \) 및 \ (W_2, b_1, b_2 \ in \ mathbf {R} \)은 모델 매개 변수입니다. \ (W_1, W_2 \)는 각각 입력 레이어와 숨겨진 레이어의 가중치를 나타냅니다. \ (b_1, b_2 \)는 각각 숨겨진 레이어와 출력 레이어에 추가 된 바이어스를 나타냅니다. \ (g (\ cdot) : R \ rightarrow R \)는 활성화 함수이며 기본적으로 쌍곡선으로 설정됩니다. 다음과 같이 주어진다.</target>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">\ (x_i \ in \ mathbf {R} ^ m \) 및 \ (y_i \ in \ {-1, 1 \} \), 우리의 목표는 모델 매개 변수 \ (w \ in \ mathbf {R} ^ m \)를 가진 선형 채점 함수 \ (f (x) = w ^ T x + b \)를 배우고 가로채는 것입니다. (b \ in \ mathbf {R} \)입니다. 예측을하기 위해 간단히 \ (f (x) \)의 부호를 봅니다. 모델 매개 변수를 찾는 일반적인 선택은 다음과 같이 정규화 된 훈련 오류를 최소화하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">예외, 예외를 발생시킬 수있는 호출 가능 및 메시지 문자열이 제공되면 올바른 예외가 발생하고 메시지가 발생한 오류의 하위 문자열인지 테스트합니다. 예외 중에 발생 된 특정 메시지가 올바른지 테스트하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">특징들 (예를 들어, 선형 모델의 계수들)에 가중치들을 할당하는 외부 추정기가 주어지면, 재귀 특징 제거 ( &lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt; )는 더 작고 더 작은 특징들의 세트를 재귀 적으로 고려함으로써 특징들을 선택하는 것이다. 먼저 추정기는 초기 기능 세트에 대해 &lt;code&gt;coef_&lt;/code&gt; 하고 각 기능의 중요성은 coef_ 속성 또는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 얻습니다 . 그런 다음, 가장 중요한 기능은 현재 기능 세트에서 정리됩니다.이 절차는 선택할 원하는 기능 수가 결국 도달 할 때까지 정리 된 세트에서 반복적으로 반복됩니다.</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">피처에 가중치를 할당하는 외부 추정기 (예 : 선형 모델의 계수)를 고려할 때, 재귀 피처 제거 (RFE)의 목표는 더 작고 더 작은 피처 세트를 재귀 적으로 고려하여 피처를 선택하는 것입니다. 먼저 추정기는 초기 기능 세트에 대해 &lt;code&gt;coef_&lt;/code&gt; 하고 각 기능의 중요성은 coef_ 속성 또는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 얻습니다 . 그런 다음 가장 중요한 기능이 현재 기능 세트에서 제거됩니다. 선택할 수있는 피처 수에 도달 할 때까지 제거 된 세트에서 해당 절차를 반복적으로 반복합니다.</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">충분한 시간이 주어지면 K-means는 항상 수렴되지만 이는 로컬 최소값 일 수 있습니다. 이것은 중심의 초기화에 크게 의존합니다. 결과적으로 계산은 종종 중심의 초기화가 다른 여러 번 수행됩니다. 이 문제를 해결하는 한 가지 방법은 scikit-learn에서 구현 된 k-means ++ 초기화 체계입니다 ( &lt;code&gt;init='k-means++'&lt;/code&gt; 매개 변수 사용). 이것은 중심에서 (일반적으로) 서로 떨어져 있도록 초기화하며, 참조에 표시된 것처럼 무작위 초기화보다 결과가 더 좋습니다.</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">우리는 조리개의 3 종류가 있다고 알고 있지만 레이블을 분류 학자에 액세스하지 않은 경우, 홍채 데이터 집합을 감안할 때 : 우리는 시도 할 수 &lt;strong&gt;클러스터링 작업을&lt;/strong&gt; : 잘 분리 된 그룹이라고에 관찰을 분할 &lt;em&gt;클러스터&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">클래스 지정이 지상 진리의 지식을 감안할 때 &lt;code&gt;labels_true&lt;/code&gt; 과 같은 샘플 우리의 클러스터링 알고리즘 할당 &lt;code&gt;labels_pred&lt;/code&gt; 을 의 &lt;strong&gt;상호 정보&lt;/strong&gt; 함수입니다 측정 &lt;strong&gt;계약&lt;/strong&gt; 순열을 무시하고 두 과제. 이 측정의 두 가지 정규화 된 버전 인 &lt;strong&gt;NMI (Normalized Mutual Information)&lt;/strong&gt; 와 &lt;strong&gt;AMI (Adjusted Mutual Information)가&lt;/strong&gt; 있습니다. NMI는 종종 문헌에 사용되는 반면, AMI는 최근에 제안되었으며 &lt;strong&gt;우연히 정규화되었습니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">클래스 지정이 지상 진리의 지식을 감안할 때 &lt;code&gt;labels_true&lt;/code&gt; 과 같은 샘플 우리의 클러스터링 알고리즘 할당 &lt;code&gt;labels_pred&lt;/code&gt; 의 &lt;strong&gt;조정 랜드 지수&lt;/strong&gt; 를 측정하는 기능입니다 &lt;strong&gt;유사성&lt;/strong&gt; 두 과제, 무시 순열과 &lt;strong&gt;기회의 정상화와 함께&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">샘플의 기본 진리 클래스 할당에 대한 지식이 주어지면 조건부 엔트로피 분석을 사용하여 직관적 인 메트릭을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">이러한 특이 벡터를 고려할 때, 이들은 조각 별 상수 벡터로 가장 잘 추정 할 수있는 순위에 따라 순위가 매겨집니다. 각 벡터에 대한 근사는 1 차원 k- 평균을 사용하여 찾아 유클리드 거리를 사용하여 점수를 매 깁니다. 가장 좋은 왼쪽 및 오른쪽 특이 벡터의 일부가 선택됩니다. 다음으로, 데이터는이 단일 벡터의 가장 좋은 부분 집합에 투영되고 클러스터됩니다.</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">학습 벡터 \ (x_i \ in R ^ n \), i = 1,&amp;hellip;, l 및 레이블 벡터 \ (y \ in R ^ l \)가 주어지면 의사 결정 트리는 공간이 재귀 적으로 분할되어 샘플이 동일한 레이블이 함께 그룹화됩니다.</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">주어진 훈련 벡터 \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n 및 벡터 \ (y \ in \ mathbb {R} ^ n \) \ (\ varepsilon \)- SVR은 다음과 같은 초기 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">주어진 훈련 벡터 \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n, 두 클래스, 벡터 \ (y \ in \ {1, -1 \} ^ n \) SVC는 다음과 같은 초기 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">(복잡한) 샘플링 포인트 수를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">전역 구조는 명시 적으로 보존되지 않습니다. 이 문제는 PCA로 포인트를 초기화하면 ( &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; 사용) 완화됩니다 .</target>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot, Xavier 및 Yoshua Bengio. &amp;ldquo;의 어려움을 이해</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">Golub과 C. Van Loan. 매트릭스 계산, 3 판, 5 장,</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin, (2004). K- 카테고리 상관 계수에 의한 두 K- 카테고리 할당 비교</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">그라디언트 부스팅 대역 외 추정</target>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">그라디언트 부스팅은 가장 가파른 하강을 통해이 최소화 문제를 수치 적으로 해결하려고 시도합니다. 가장 가파른 하강 방향은 현재 모델 \ (F_ {m-1} \)에서 평가 된 손실 함수의 음의 구배입니다.</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">분류를위한 그라디언트 부스팅.</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">회귀에 대한 그라디언트 부스팅.</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">그라디언트 부스팅 회귀</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">그라디언트 부스팅 정규화</target>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">그라디언트 부스팅은 몇 가지 약한 학습자 (회귀 트리)를 결합하여 반복적 인 방식으로 강력한 단일 모델을 생성하는 조립 기술입니다.</target>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">위치 세타에서의 커널 하이퍼 파라미터에 대한 로그-마진 가능성의 구배. eval_gradient가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">그램 직교 매칭 추구 (OMP)</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">입력 데이터의 그램 행렬 : XT * X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">그래프 거리 (예 : 가장 가까운 이웃 그래프)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">픽셀 간 연결 그래프</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">픽셀 간 그라디언트 연결 그래프</target>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">교차 검증에 사용되는 C 그리드.</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">각각에 대해 별개의 수의 값을 갖는 매개 변수 그리드.</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 일부로 분류기의 하이퍼 파라미터와 함께 &lt;code&gt;ColumnTransformer&lt;/code&gt; 객체에 정의 된 다양한 전처리 단계에서 그리드 검색을 수행 할 수도 있습니다 . 우리는 숫자 전처리의 imputer 전략 및 사용하여 로지스틱 회귀의 정규화 매개 변수를 모두 검색합니다 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">그리드 검색 및 교차 검증 추정기</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCV는 &quot;적합&quot;및 &quot;점수&quot;방법을 구현합니다. 또한 &quot;추정&quot;, &quot;predict_proba&quot;, &quot;decision_function&quot;, &quot;transform&quot;및 &quot;inverse_transform&quot;이 사용 된 추정기에서 구현 된 경우 구현합니다.</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas, 이오 아니스 카타 키스. 다중 레이블 분류 : 개요. 국제 데이터웨어 하우징 및 저널, 3 (3), 1-13, 2007 년 7 월 -9 월.</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">n_samples 샘플에 대한 정확한 사실 (올바른) 레이블.</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">진실 (정확한) 라벨.</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">진실 (올바른) 목표 값.</target>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">참조로 사용할지면 진실 클래스 레이블</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">데이터 세트를 기차 / 테스트 세트로 분할하는 동안 사용 된 샘플의 레이블을 그룹화합니다.</target>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">데이터 세트를 기차 / 테스트 세트로 분할하는 동안 사용 된 샘플의 레이블을 그룹화합니다. 다른 '파라미터'는 생략 할 수 있지만이 '그룹'파라미터는 항상 스플릿 수를 계산하기 위해 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; 가 있는 트리를 가장 먼저 성장 시킵니다 . 최상의 노드는 불순물의 상대적 감소로 정의됩니다. None 인 경우 무제한의 리프 노드.</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; 로 나무를 가장 먼저 성장 시키십시오 . 최상의 노드는 불순물의 상대적 감소로 정의됩니다. None 인 경우 무제한의 리프 노드.</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">Guide</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">Guyon, I., Weston, J., Barnhill, S. 및 Vapnik, V.,&amp;ldquo;서포트 벡터 머신을 사용한 암 분류를위한 유전자 선택&amp;rdquo;, Mach. 배우십시오., 46 (1-3), 389&amp;ndash;422, 2002.</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;나이브 베이 즈의 최적 성. &lt;/a&gt;Proc. 플레어.</target>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">하가이 아티 아스. (2000). &amp;ldquo;그래픽 모델을위한 변형 베이지안 프레임 워크&amp;rdquo;. 신경 정보 처리 시스템의 발전 12.</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">할 키디, 마리아; 바티스타 키스, 야 니스; Vazirgiannis, Michalis (2001). &amp;ldquo;클러스터링 유효성 검사 기술&amp;rdquo;Journal of Intelligent Information Systems, 17 (2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi : 10.1023 / A : 1012801612483&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">출력 파일의 핸들 또는 이름 경우 &lt;code&gt;None&lt;/code&gt; , 결과는 문자열로 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">백엔드를 선택하기 어려운 제약 조건 'sharedmem'으로 설정하면 사용자가 parallel_backend를 사용하여 스레드가 아닌 기반 백엔드를 요청한 경우에도 선택한 백엔드는 단일 호스트 및 스레드 기반이됩니다.</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">솔버 내 반복에 대한 하드 제한 또는 제한이없는 경우 -1</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">해리슨, D.와 루빈 펠트, DL</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">트리의 해시 함수 g (p, x)는 데이터 세트와 동일한 차원을 갖는 임의로 생성 된 32 개의 부동 소수점 배열로 구성된 배열입니다. 이 배열은 GaussianRandomProjectionHash 객체에 저장되며 &lt;code&gt;components_&lt;/code&gt; 속성 에서 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">완전 랜덤 트리를 사용한 해싱 기능 변환</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizer는 상태 비 저장 모델이므로 IDF 가중치를 제공하지 않습니다 (맞춤 방법은 아무 것도 수행하지 않음). IDF 가중치가 필요한 경우 출력을 TfidfTransformer 인스턴스에 파이프 라이닝하여 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizer는 충돌 가능성이있는 고정 된 차원 공간으로 단어 어커런스를 해시합니다. 단어 카운트 벡터는 각각 1- 규모 (유클리드 단위-볼에 투영 됨)와 동일한 l2-norm을 갖도록 정규화되는데, 이는 k- 평균이 고차원 공간에서 작동하는 데 중요한 것으로 보인다.</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie, R. Tibshirani 및 J. Friedman,&amp;ldquo;통계 학습의 요소 Ed. 2&amp;rdquo;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">상기 봐 가지고 &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;해싱 벡터화&lt;/a&gt; 에 메모리 효율적인 대안으로 &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">사용하여 한 번 봐 가지고 &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;밖으로의 핵심 분류&lt;/a&gt; 컴퓨터 메인 메모리에 맞지 않을 데이터로부터 배울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">그는 Kaiming 등 &amp;ldquo;정류기 심층 분석 : 인간 수준을 능가</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">읽을 수있는 병렬 매핑을위한 도우미 클래스</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">예외에서 발생한 메시지를 테스트하는 도우미 기능</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">따라서 입력 공간에 64 개의 피처 만있는 숫자 데이터 세트에 임의의 투영을 사용하는 것은 의미가 없습니다.이 경우 치수 축소를 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">따라서 훈련 코퍼스에서 볼 수 없었던 단어는 나중에 transform 메소드 호출에서 완전히 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">따라서 없음의 경우 결과는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">여기서 &lt;code&gt;func&lt;/code&gt; 은 두 개의 1 차원 numpy 배열을 가져 와서 거리를 반환하는 함수입니다. BallTree 내에서 사용하려면 거리가 실제 측정 항목이어야합니다. 즉, 다음 속성을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">여기서 사인 함수는 0에 가까운 값에 대해 차수 3의 다항식에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">다음은 이진 클래스 문제에서 svm 분류기와 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수를 사용하는 방법을 보여주는 작은 예제입니다 .</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">다음은 가우스 랜덤 프로젝션 변압기를 사용하는 방법을 보여주는 작은 발췌문입니다.</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">다음은 희소 랜덤 프로젝션 변압기를 사용하는 방법을 보여주는 작은 발췌문입니다.</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">이 자습서를 마치면 scikit-learn 직관을 향상시키는 데 도움이되는 몇 가지 제안이 있습니다.</target>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">표준 컬럼 데이터를 scikit-learn이 사용할 수있는 형식으로로드하는 몇 가지 권장 방법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">이진 분류의 작은 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">다음은 다양한 작업을위한 증분 추정기 목록입니다.</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">다음은 쿼드 코어 머신에서 실행 한 샘플 출력입니다.</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">이 목표를 달성하기 위해 설계된 시스템의 스케치는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt; 함수 의 사용법을 보여주는 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예는 다음과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">이 함수의 사용법에 대한 작은 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">이 함수를 사용하는 간단한 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">사용 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">다음은 이러한 혼동 행렬을 시각적으로 나타낸 것입니다 (이 그림은 &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;혼동 행렬&lt;/a&gt; 예 에서 나옴 ).</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다. 참고 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; 이&lt;/a&gt; 클래스 또는 그룹의 영향을받지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다. 참고 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 이&lt;/a&gt; 클래스 또는 그룹의 영향을받지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">다음은 멀티 클래스 문제에서 svm 분류기와 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수를 사용하는 방법을 보여주는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">단일 메트릭을 사용하는 &lt;code&gt;cross_validate&lt;/code&gt; 의 예는 다음과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">다음은 다양한 각도의 다항식 특징을 사용하여이 아이디어를 1 차원 데이터에 적용하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">다음은 사용자 지정 채점자를 작성하고 &lt;code&gt;greater_is_better&lt;/code&gt; 매개 변수 를 사용하는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">다음은 장난감 데이터 매트릭스를 &lt;code&gt;[0, 1]&lt;/code&gt; 범위 로 스케일링하는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">다음은 &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; 를 &lt;code&gt;elasticnet&lt;/code&gt; 패널티 와 함께 사용하는 예 입니다. 정규화 강도는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수에 의해 전체적으로 제어됩니다 . 충분히 높은 &lt;code&gt;alpha&lt;/code&gt; , 모델 계수에서 다양한 레벨의 희소성을 강제하기 위해 &lt;code&gt;elasticnet&lt;/code&gt; 의 &lt;code&gt;l1_ratio&lt;/code&gt; 파라미터 를 증가시킬 수 있습니다 . 여기서 희소성이 클수록 모델을 완전히 설명하기 위해 더 적은 계수가 필요하므로 모델 복잡성이 줄어 듭니다. 물론 희소 도트 곱은 0이 아닌 계수의 수에 대략 비례하는 시간이 걸리기 때문에 희소성이 차례로 예측 시간에 영향을 미칩니다.</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">이 스케일러에서 이전 예제의 장난감 데이터를 사용하는 방법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">다음은 &lt;code&gt;sparsify()&lt;/code&gt; 메서드 사용을 보여주는 샘플 코드입니다 .</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">다음은 입력 희소성을 테스트하는 샘플 코드입니다.</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">다음은 자동 모델 선택을위한 AIC (Akaike Information Criterion) 또는 BIC (Bayesian Information Criterion)의 이점이있는 모델 목록입니다.</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">이러한 모델 목록은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">여기에서 scikit-learn으로 구현 된 Martinsson의 Randomized SVD 알고리즘 덕분에 계산이 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">여기서 정규화 매개 변수 C에 대한 우리의 선택이 최선이 아니기 때문에 결과가 좋지 않을 수 있습니다. 실제 애플리케이션에서이 매개 변수는 일반적으로 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;추정기의 하이퍼 파라미터 튜닝을&lt;/a&gt; 사용하여 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">여기서는 SA1 솔버를 선택하는데, 이는 매끄럽고 드문 드문 한 l1 페널티로 로지스틱 회귀 손실을 효율적으로 최적화 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">여기서 우리는 3 가지 접근법을 비교합니다 :</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">여기 우리는 Dirichlet 프로세스 혼합물에 변형 추론 알고리즘을 설명합니다. Dirichlet 프로세스는 &lt;em&gt;무한한 무제한의 파티션&lt;/em&gt; 으로 &lt;em&gt;클러스터링&lt;/em&gt; 에 대한 사전 확률 분포입니다 . 변형 기술을 사용하여 유한 가우시안 혼합 모델과 비교하여 추론 시간에 거의 페널티없이 가우시안 혼합 모델에이 사전 구조를 통합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">여기서 우리는 MNIST 숫자 분류 작업의 부분 집합에 L1 페널티를 갖는 다항 로지스틱 회귀 분석에 적합합니다. 우리는이를 위해 SAGA 알고리즘을 사용합니다. 이것은 샘플 수가 피처 수보다 훨씬 많을 때 빠르며 l1 페널티의 경우와 같이 부드럽 지 않은 목적 함수를 미세하게 최적화 할 수있는 솔버입니다. 테스트 정확도는&amp;gt; 0.8에 도달하는 반면 가중치 벡터는 &lt;em&gt;희박&lt;/em&gt; 하게 유지 되므로 더 쉽게 &lt;em&gt;해석&lt;/em&gt; 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">여기 에서 위에서 본 것처럼 &lt;code&gt;kernel='gaussian'&lt;/code&gt; 을 사용했습니다. 수학적으로 커널은 긍정적 인 함수 \ (K (x; h) \)이며 대역폭 매개 변수 \ (h \)에 의해 제어됩니다. 이 커널 형식이 주어지면 점 그룹 \ (x_i; i = 1 \ cdots N \) 내의 점 \ (y \)에서의 밀도 추정값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">여기서는 자유도가 낮은 2 명의 학생 T 인 비 가우시안 프로세스 (왼쪽 상단 그림)를 사용하여 독립적 인 소스를 시뮬레이션합니다. 우리는 그것들을 혼합하여 관찰을 만듭니다 (오른쪽 상단 그림). 이 원시 관측 공간에서 PCA로 식별되는 방향은 주황색 벡터로 표시됩니다. PCA 벡터 (왼쪽 아래)에 해당하는 분산으로 미백 한 후 PCA 공간의 신호를 나타냅니다. ICA를 실행하면이 공간에서 가장 비 가우시안 방향 (오른쪽 아래)을 식별하기 위해 회전을 찾는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">여기서는 정보가없는 기능의 가중치를 0으로 자르는 l1 희소성을 사용합니다. 목표가 각 클래스의 강력한 차별적 어휘를 추출하는 것이라면 좋습니다. 최고의 예측 정확도를 얻는 것이 목표라면, 희소성을 유발하지 않는 l2 페널티를 대신 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">여기서 \ (\ alpha \ geq 0 \)은 수축량을 제어하는 ​​복잡성 매개 변수입니다. \ (\ alpha \)의 값이 클수록 수축량이 커지므로 계수가 공선성에 대해보다 강력 해집니다.</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">여기서 분류자는 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; 를&lt;/a&gt; 사용하여 &lt;code&gt;y&lt;/code&gt; 의 2 차원 이진 레이블 표현에서 &lt;code&gt;fit()&lt;/code&gt; 입니다 . 이 경우 &lt;code&gt;predict()&lt;/code&gt; 는 해당 다중 레이블 예측을 나타내는 2d 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">여기서 기본 커널 &lt;code&gt;rbf&lt;/code&gt; 는 추정기가 구성된 후 &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt; 를 통해 먼저 &lt;code&gt;linear&lt;/code&gt; 변경되고 추정기 를 다시 맞추고 두 번째 예측을하기 위해 &lt;code&gt;rbf&lt;/code&gt; 로 다시 변경됩니다 .</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">여기에서 &lt;code&gt;iris.target&lt;/code&gt; (정수 배열)이 &lt;code&gt;fit&lt;/code&gt; 에서 사용 되었으므로 첫 번째 &lt;code&gt;predict()&lt;/code&gt; 는 정수 배열을 반환합니다 . &lt;code&gt;iris.target_names&lt;/code&gt; 가 적합하기 때문에 두 번째 &lt;code&gt;predict()&lt;/code&gt; 는 문자열 배열을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">여기에서 표본의 수는 차원의 수보다 약간 크므로 경험적 공분산은 여전히 ​​되돌릴 수 없습니다. 그러나 관측치가 서로 밀접하게 연관되어 있기 때문에 경험적 공분산 행렬은 조건이 좋지 않으며 결과적으로 그 역수 &amp;ndash; 경험적 정밀 행렬 &amp;ndash;은 실제와는 거리가 멀다.</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">여기에서 예측 된 클래스 레이블은 평균 확률이 가장 높으므로 2입니다.</target>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">Hessian Eigenmapping (Hessian 기반 LLE : HLLE이라고도 함)은 LLE의 정규화 문제를 해결하는 또 다른 방법입니다. 그것은 각 지역에서 헤 시안 기반의 이차 형태를 중심으로 회전하며, 이는 국소 적으로 선형 인 구조를 회복하는데 사용됩니다. 다른 구현에서는 데이터 크기에 &lt;code&gt;sklearn&lt;/code&gt; 확장 성이 좋지 않지만 sklearn 은 약간의 알고리즘 개선을 구현하여 비용이 작은 출력 차원의 다른 LLE 변형과 비교할 수 있습니다. HLLE는 키워드 &lt;code&gt;method = 'hessian'&lt;/code&gt; &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 함수 또는 객체 지향 대응 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 함수로 수행 할 수 있습니다 . 그것은 필요 &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">계층 적 응집 클러스터링 : 워드</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">계층 적 클러스터링은 중첩 클러스터를 병합하거나 분할하여 중첩 클러스터를 구축하는 일반적인 클러스터링 알고리즘 제품군입니다. 이 클러스터 계층은 트리 (또는 덴드로 그램)로 표시됩니다. 트리의 루트는 모든 샘플을 수집하는 고유 한 클러스터이며, 나뭇잎은 하나의 샘플 만있는 클러스터입니다. 자세한 내용은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia 페이지&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">계층 적 클러스터링 : 구조적 및 비 구조적 와드</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">고차원 데이터 세트는 시각화하기가 매우 어려울 수 있습니다. 2 차원 또는 3 차원의 데이터는 데이터의 고유 한 구조를 보여주기 위해 플로팅 될 수 있지만 동등한 고차원 플롯은 훨씬 덜 직관적입니다. 데이터 세트의 구조를 시각화하려면 차원을 어느 정도 줄여야합니다.</target>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">경첩 : (소프트 마진) 지원 벡터 머신.</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">힌튼, 제프리 E.</target>
        </trans-unit>
        <trans-unit id="b8cd925555526484cde7ba65174f600e33250f6b" translate="yes" xml:space="preserve">
          <source>Hochreiter, Bodenhofer, et. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: factor analysis for bicluster acquisition&lt;/a&gt;.</source>
          <target state="translated">Hochreiter, Bodenhofer 등 . 등, 2010 년 &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;의 Fabia : bicluster 획득을위한 요인 분석&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7a5c3ac7ac58dcde82acc59c23c0dce0d31966d1" translate="yes" xml:space="preserve">
          <source>Holds the label for each class.</source>
          <target state="translated">각 클래스의 레이블을 보유합니다.</target>
        </trans-unit>
        <trans-unit id="4b7dceb5fe5f7e92199815a2b66fef8fdf05dc27" translate="yes" xml:space="preserve">
          <source>Homogeneity and completeness scores are formally given by:</source>
          <target state="translated">균질성과 완전성 점수는 공식적으로 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="7c13e77bc830b382c041eec0e3fcc1723f375e05" translate="yes" xml:space="preserve">
          <source>Homogeneity metric of a cluster labeling given a ground truth.</source>
          <target state="translated">근거가 주어진 클러스터 라벨링의 동질성 메트릭.</target>
        </trans-unit>
        <trans-unit id="3afc9b67230d985e8782525c7b58b615e013e8f9" translate="yes" xml:space="preserve">
          <source>Homogeneity, completeness and V-measure can be computed at once using &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt;&lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt;&lt;/a&gt; as follows:</source>
          <target state="translated">균질성, 완전성 및 V 측정은 다음과 같이 &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt; &lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt; &lt;/a&gt; _ 완전성 _v_measure를 사용하여 한 번에 계산할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0878824f511837fc1a1c8d27240af19053ebdbd4" translate="yes" xml:space="preserve">
          <source>HouseAge median house age in block</source>
          <target state="translated">블록에서 HouseAge 중간 주택 연령</target>
        </trans-unit>
        <trans-unit id="be45c283b4c54643c38f84bc65a4bfc525d6d30a" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="translated">난이도를 평가하는 빈도. &lt;code&gt;fit&lt;/code&gt; 방법으로 만 사용하십시오 . 훈련의 난이도를 전혀 평가하지 않으려면 0 또는 음수로 설정하십시오. 난이도를 평가하면 교육 과정에서 수렴을 확인하는 데 도움이되지만 총 교육 시간도 늘어납니다. 반복 할 때마다 난이도를 평가하면 훈련 시간이 최대 2 배 증가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="060faa287065b4ad6ba6c00f598635b42adc21de" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">분모에서 노멀 라이저를 계산하는 방법. 가능한 옵션은 'min', 'geometric', 'arithmetic'및 'max'입니다. '경고'인 경우 '형상'이 사용됩니다. 버전 0.22에서는 기본값이 '산술'로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="b8efa217d0db9ce56ac60653645beebe151304f9" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;max&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">분모에서 노멀 라이저를 계산하는 방법. 가능한 옵션은 'min', 'geometric', 'arithmetic'및 'max'입니다. '경고'이면 'max'가 사용됩니다. 버전 0.22에서는 기본값이 '산술'로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="cf894bb3f8fceedab10cdd5da9a5edd37e00865d" translate="yes" xml:space="preserve">
          <source>How to construct the affinity matrix.</source>
          <target state="translated">선호도 매트릭스 구성 방법</target>
        </trans-unit>
        <trans-unit id="d34268ba2716d71aaaeee2c35e527ca55a46dd2f" translate="yes" xml:space="preserve">
          <source>However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).</source>
          <target state="translated">그러나 ARI는 TODO (클러스터링 모델 선택)에 사용할 수있는 컨센서스 인덱스의 빌딩 블록으로 순수하게 감독되지 않은 설정에서 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0121c2b22395d1a9db3fd77f24d0a9f0e41170e3" translate="yes" xml:space="preserve">
          <source>However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.</source>
          <target state="translated">그러나 MI 기반 측정은 순전히 감독되지 않은 설정에서 클러스터링 모델 선택에 사용할 수있는 합의 색인의 빌딩 블록으로 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="117b6230e0e8ab3fcdc1b277507becef8a05f759" translate="yes" xml:space="preserve">
          <source>However care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.</source>
          <target state="translated">그러나 고유 벡터 분해가 예상대로 작동하도록 항상 선호도 행렬을 대칭으로 만들도록주의해야합니다.</target>
        </trans-unit>
        <trans-unit id="925f5b77eb2888a89c04118c35bff0f0ace7255e" translate="yes" xml:space="preserve">
          <source>However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).</source>
          <target state="translated">그러나 RI 점수는 임의의 레이블 할당이 0에 가까워지는 값을 보장하지 않습니다 (클러스터 수가 샘플 수와 동일한 크기 인 경우).</target>
        </trans-unit>
        <trans-unit id="11d179b15971b8eaae6270be9fce57c0bd0d2416" translate="yes" xml:space="preserve">
          <source>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</source>
          <target state="translated">그러나 사용 가능한 데이터를 세 세트로 분할함으로써 모델 학습에 사용할 수있는 샘플 수를 대폭 줄이며 결과는 (트레인, 유효성 검사) 세트의 특정 무작위 선택에 따라 달라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="56c680421b5fb07e56baa9a65f13a80fce385b54" translate="yes" xml:space="preserve">
          <source>However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">그러나 정규 최소 제곱의 계수 추정치는 모형 항의 독립성에 의존합니다. 항이 서로 연관되어 있고 설계 행렬 \ (X \)의 열이 대략 선형 의존성을 가질 때, 설계 행렬은 특이에 가까워지고 결과적으로 최소 제곱 추정값은 관측 된 응답의 랜덤 오차에 매우 민감 해집니다. 큰 차이를 만들어냅니다. 이러한 &lt;em&gt;다중 공선 성&lt;/em&gt; 상황은 예를 들어 실험 설계없이 데이터를 수집 할 때 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bf734282463bfc3a9cb343729f546342ec401691" translate="yes" xml:space="preserve">
          <source>However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.</source>
          <target state="translated">그러나 학습 곡선이 문제의 훈련 규모에 비해 가파르면 5 배 또는 10 배 교차 검증이 일반화 오류를 과대 평가할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fc162d85afa0b20b4064f40b16eb0e55ca89c629" translate="yes" xml:space="preserve">
          <source>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</source>
          <target state="translated">그러나 추정기가 일부 하이퍼 파라미터 값에 대해 과적 합하는지 또는 과적 합하는지 여부를 확인하기 위해 단일 하이퍼 파라미터가 훈련 스코어 및 유효성 검증 스코어에 미치는 영향을 플롯하는 것이 때때로 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="5c8b24673bb3f660f66f90035a856044be6d6f9e" translate="yes" xml:space="preserve">
          <source>However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; to complete binary one-hot encoding.</source>
          <target state="translated">그러나이 변환기는 기능 값이 문자열 유형 인 경우 2 진 1- 핫 인코딩 만 수행합니다. 범주 형 기능이 int와 같은 숫자 값으로 표시되는 경우 DictVectorizer 뒤에 &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt; 를 사용하여 이진 one-hot 인코딩을 완료 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b25d9ad009118aef0894664f601ac10786f8b49" translate="yes" xml:space="preserve">
          <source>However, this is not the most precise way of doing this computation, and the distance matrix returned by this function may not be exactly symmetric as required by, e.g., &lt;code&gt;scipy.spatial.distance&lt;/code&gt; functions.</source>
          <target state="translated">그러나 이것이이 계산을 수행하는 가장 정확한 방법은 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 함수가 반환하는 거리 행렬이 예를 들어 scipy.spatial.distance 함수에서 요구하는대로 정확하게 대칭이 아닐 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="379cfb166aa26713fe1131478e9b37a4224780ad" translate="yes" xml:space="preserve">
          <source>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</source>
          <target state="translated">치앙 린, 샹란 황, 샹 푸유 (2011). 이중 좌표 하강</target>
        </trans-unit>
        <trans-unit id="15d1d4b26d2ba06629e368bf6c8a860d8762ef89" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="translated">Huber ( &lt;code&gt;'huber'&lt;/code&gt; ) : 최소 제곱과 최소 절대 편차를 결합한 또 다른 강력한 손실 함수. 이상치에 대한 민감도를 제어 하려면 &lt;code&gt;alpha&lt;/code&gt; 를 사용 하십시오 (자세한 내용은 &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; 참조).</target>
        </trans-unit>
        <trans-unit id="3d12d436101cbc3212af50bf81000f6d78d4cf01" translate="yes" xml:space="preserve">
          <source>HuberRegressor vs Ridge on dataset with strong outliers</source>
          <target state="translated">특이 치가 강한 데이터 세트의 HuberRegressor vs Ridge</target>
        </trans-unit>
        <trans-unit id="7e58a6e8d89e8504ad31e135de9b485ad40f05f6" translate="yes" xml:space="preserve">
          <source>Hue</source>
          <target state="translated">Hue</target>
        </trans-unit>
        <trans-unit id="c7a8b2b20a9c45f674f17cd8ef7ece305e1c36eb" translate="yes" xml:space="preserve">
          <source>Hue:</source>
          <target state="translated">Hue:</target>
        </trans-unit>
        <trans-unit id="4e99bcdee413a9c98d317e0e8e4199a2bf582f90" translate="yes" xml:space="preserve">
          <source>Hugo Chavez</source>
          <target state="translated">휴고 차베스</target>
        </trans-unit>
        <trans-unit id="135e7e12c7ab5ea7496649e72ee134478ecf558e" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="761054fe5bafcad49a16f057764235860452b8da" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1입니다 .e-6</target>
        </trans-unit>
        <trans-unit id="6001ea6392d3003569381e7107254e88f75fd600" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 역 척도 매개 변수 (속도 매개 변수). 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="81e171654bf22a490946ec147c219e96694497ff" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수보다 감마 분포에 대한 모양 매개 변수입니다. 기본값은 1입니다 .e-6</target>
        </trans-unit>
        <trans-unit id="b07af48fd68aeaacb4df041ef30bae006150c237" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 알파 매개 변수보다 감마 분포에 대한 모양 매개 변수입니다. 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="1398aea0b1e181e76b6d9d73db4040ccf06ee2f7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter : 람다 매개 변수 이전의 감마 분포에 대한 모양 매개 변수. 기본값은 1.e-6입니다.</target>
        </trans-unit>
        <trans-unit id="7a5b8a439bb2492412d2944256add4dcdf337928" translate="yes" xml:space="preserve">
          <source>Hyper-parameter optimizers</source>
          <target state="translated">하이퍼 파라미터 옵티 마이저</target>
        </trans-unit>
        <trans-unit id="223bf115da53d3d9cdf837b624135b565596fd92" translate="yes" xml:space="preserve">
          <source>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; for Support Vector Classifier, &lt;code&gt;alpha&lt;/code&gt; for Lasso, etc.</source>
          <target state="translated">하이퍼 파라미터는 추정기 내에서 직접 학습되지 않는 파라미터입니다. scikit-learn에서 이들은 추정자 클래스의 생성자에 인수로 전달됩니다. 일반적인 예로는 &lt;code&gt;C&lt;/code&gt; , &lt;code&gt;kernel&lt;/code&gt; 및 지원 벡터 분류기의 &lt;code&gt;gamma&lt;/code&gt; , 올가미의 &lt;code&gt;alpha&lt;/code&gt; 등이 있습니다.</target>
        </trans-unit>
        <trans-unit id="568b05951392672a52de0358537dd29fcafbe544" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="translated">추정기의 하이퍼 파라미터는 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params ()&lt;/a&gt; 메소드 를 통해 생성 된 후에 업데이트 될 수 있습니다 . &lt;code&gt;fit()&lt;/code&gt; 두 번 이상 호출 하면 이전 &lt;code&gt;fit()&lt;/code&gt; 에서 학습 한 내용을 덮어 씁니다 .</target>
        </trans-unit>
        <trans-unit id="1db8c072507305b4aa23189287be39423349b8f4" translate="yes" xml:space="preserve">
          <source>Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).</source>
          <target state="translated">역변환을 학습하는 능형 회귀의 하이퍼 파라미터 (fit_inverse_transform = True 일 때).</target>
        </trans-unit>
        <trans-unit id="181eca8daf7aaeed93f61701c7eddb643dc6b36a" translate="yes" xml:space="preserve">
          <source>Hyperparameters:</source>
          <target state="translated">Hyperparameters:</target>
        </trans-unit>
        <trans-unit id="8bb86931be2a9d0449c3eec151da751cb88591f1" translate="yes" xml:space="preserve">
          <source>I. Guyon, &amp;ldquo;Design of experiments for the NIPS 2003 variable selection benchmark&amp;rdquo;, 2003.</source>
          <target state="translated">I. Guyon,&amp;ldquo;NIPS 2003 변수 선택 벤치 마크 실험 설계&amp;rdquo;, 2003.</target>
        </trans-unit>
        <trans-unit id="a238a89365b9d0ce7f5fb26e189eb03cdc08fbe5" translate="yes" xml:space="preserve">
          <source>ICA can also be used as yet another non linear decomposition that finds components with some sparsity:</source>
          <target state="translated">ICA는 또한 희소성이있는 구성 요소를 찾는 또 다른 비선형 분해로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fcc34dd193c826ae2f0c8b804c532252b4a25480" translate="yes" xml:space="preserve">
          <source>INDUS proportion of non-retail business acres per town</source>
          <target state="translated">도시 당 비 소매 비즈니스 에이커의 INDUS 비율</target>
        </trans-unit>
        <trans-unit id="44a4d7b7db7815be999da6a406f4dadd2c4327c5" translate="yes" xml:space="preserve">
          <source>Identification number of each sample, as ordered in dataset.data.</source>
          <target state="translated">dataset.data에서 주문 된 각 샘플의 식별 번호입니다.</target>
        </trans-unit>
        <trans-unit id="02d51b4f13558cbcfc807b53522b1ffb156ad7e7" translate="yes" xml:space="preserve">
          <source>Identity: d(x, y) = 0 if and only if x == y</source>
          <target state="translated">동일성 : x == y 인 경우에만 d (x, y) = 0</target>
        </trans-unit>
        <trans-unit id="35bd2069c37f2c6a308bc5401948b247d5bcfc02" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;all&amp;rdquo;, the imputer mask will represent all features.</source>
          <target state="translated">&quot;all&quot;인 경우 imputer 마스크는 모든 기능을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="84934b5d658c0a370c458ee55fa3255bb65884a6" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo; (default), the imputer mask will be of same type as input.</source>
          <target state="translated">&quot;auto&quot;(기본값) 인 경우 imputer 마스크는 입력과 동일한 유형입니다.</target>
        </trans-unit>
        <trans-unit id="7cdc1bc49e801caf1ff212e1000d88bb13d7e93e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">&amp;ldquo;auto&amp;rdquo;인 경우 &lt;code&gt;max_features=n_features&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="164a2722286c1b34bc2df80a90c75397afce3e6b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;auto&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="911a50d98b398312fa01572b5d7b864da542117b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt;.</source>
          <target state="translated">&quot;auto&quot;이면 &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="dca169b413a6ec050ae0928eb38f43b00b9c08e0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;constant&amp;rdquo;, then replace missing values with fill_value. Can be used with strings or numeric data.</source>
          <target state="translated">&quot;일정한&quot;경우 누락 된 값을 fill_value로 바꾸십시오. 문자열 또는 숫자 데이터와 함께 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fed653e1ff76c14b62a8cb9c0f4474c620b2641e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;log2&amp;rdquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;log2&quot;이면 &lt;code&gt;max_features=log2(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="e799052bdd1932be1b28378fc91f87421f6d1065" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along each column. Can only be used with numeric data.</source>
          <target state="translated">&quot;평균&quot;이면 각 열의 평균을 사용하여 결 측값을 바꾸십시오. 숫자 데이터에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c28cbae695709f5ae6daaba6d2035fa26d4e040" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along the axis.</source>
          <target state="translated">&quot;평균&quot;인 경우 축을 따라 평균을 사용하여 결 측값을 바꾸십시오.</target>
        </trans-unit>
        <trans-unit id="d5353b7f39f25231d62cbbc36fcd604e05d2faa0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along each column. Can only be used with numeric data.</source>
          <target state="translated">&quot;중앙값&quot;인 경우 각 열의 중앙값을 사용하여 누락 된 값을 바꾸십시오. 숫자 데이터에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2c7bf0a70af62c9d1ff80c38810d3732da415b46" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along the axis.</source>
          <target state="translated">&quot;중간 값&quot;인 경우 축을 따라 중간 값을 사용하여 누락 된 값을 바꾸십시오.</target>
        </trans-unit>
        <trans-unit id="b9dfb246debbef95e2bc6e78da2b0aca54b8e768" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;missing-only&amp;rdquo; (default), the imputer mask will only represent features containing missing values during fit time.</source>
          <target state="translated">&quot;missing-only&quot;(기본값) 인 경우, imputer 마스크는 맞춤 시간 동안 결 측값이 포함 된 피처 만 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="fb9cd590a090a11e857ebbc7c5d49f19787d4a57" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.</source>
          <target state="translated">&quot;most_frequent&quot;인 경우 각 열에서 가장 빈번한 값을 사용하여 누락 된 것을 교체하십시오. 문자열 또는 숫자 데이터와 함께 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a90ab2bff0e7c4a2db0c7d70bcb17fa44e1b8cb3" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along the axis.</source>
          <target state="translated">&quot;most_frequent&quot;인 경우 축을 따라 가장 빈번한 값을 사용하여 누락 된 부분을 교체하십시오.</target>
        </trans-unit>
        <trans-unit id="d25972b438acba3aa495003bff5b880c3dc78f95" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration.</source>
          <target state="translated">&quot;prefit&quot;이 통과되면 base_estimator가 이미 장착 된 것으로 가정하고 모든 데이터를 교정에 사용합니다.</target>
        </trans-unit>
        <trans-unit id="ddc01f2adfc0b5da49bb0bea104c04055f37082b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; (same as &amp;ldquo;auto&amp;rdquo;).</source>
          <target state="translated">&quot;sqrt&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; ( &quot;auto&quot;와 동일)</target>
        </trans-unit>
        <trans-unit id="050de520f25e08567f8dcb7bcdaa6887bd8ca53c" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">&quot;sqrt&quot;이면 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="274dd12ab1d70a7a9d00df8bbe2aa7f35f2ca3c4" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;SAMME.R&amp;rsquo; then use the SAMME.R real boosting algorithm. &lt;code&gt;base_estimator&lt;/code&gt; must support calculation of class probabilities. If &amp;lsquo;SAMME&amp;rsquo; then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</source>
          <target state="translated">'SAMME.R'인 경우 SAMME.R 실제 부스팅 알고리즘을 사용하십시오. &lt;code&gt;base_estimator&lt;/code&gt; 는 클래스 확률 계산을 지원해야합니다. 'SAMME'인 경우 SAMME 이산 부스팅 알고리즘을 사용하십시오. SAMME.R 알고리즘은 일반적으로 SAMME보다 빠르게 수렴되므로 부스팅 반복 횟수가 줄어 테스트 오류가 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="7bac8214432dad215f7331c0af16dfd3868b9e19" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;balanced&amp;rsquo;, class weights will be given by &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.</source>
          <target state="translated">'balanced'인 경우 클래스 가중치는 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 됩니다. 사전이 제공되면 키는 클래스이고 값은 해당 클래스 가중치입니다. 없음을 지정하면 클래스 가중치가 균일합니다.</target>
        </trans-unit>
        <trans-unit id="456401c87cfc2a15cdd408f2dd8be315dc3e833e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;english&amp;rsquo;, a built-in stop word list for English is used. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">'english'이면 영어 용 내장 중지 단어 목록이 사용됩니다. 'english'에는 몇 가지 알려진 문제가 있으며 대안을 고려해야합니다 ( &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;중지 단어 사용&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="f2019bdbdfa8956b7b54e8a5677954f4996f6374" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;file&amp;rsquo;, the sequence items must have a &amp;lsquo;read&amp;rsquo; method (file-like object) that is called to fetch the bytes in memory.</source>
          <target state="translated">'file'인 경우 시퀀스 항목에는 메모리에서 바이트를 페치하기 위해 호출되는 'read'메소드 (파일과 유사한 오브젝트)가 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="3dd1f3ca74314afe1ddf15184f6ab04977d1a20b" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;filename&amp;rsquo;, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</source>
          <target state="translated">'filename'인 경우, 맞는 인수로 전달 된 시퀀스는 분석 할 원시 컨텐츠를 페치하기 위해 읽어야하는 파일 이름의 목록 일 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="6381402e5f026c95872c614d3f284a846d432d3e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;hard&amp;rsquo;, uses predicted class labels for majority rule voting. Else if &amp;lsquo;soft&amp;rsquo;, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.</source>
          <target state="translated">'하드'인 경우 다수의 규칙 투표에 예측 된 클래스 레이블을 사용합니다. 그렇지 않으면 'soft'인 경우 예측 된 확률의 합의 argmax를 기반으로 클래스 레이블을 예측하며, 이는 잘 보정 된 분류기의 앙상블에 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="034a602ff18129b75a77961464f62c916fb178cb" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;precomputed&amp;rsquo;, the training input X is expected to be a distance matrix.</source>
          <target state="translated">'사전 계산 된'경우 훈련 입력 X는 거리 매트릭스 일 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="efe2693abb0ad6fb0bb32fc7410403c6f8a6131c" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">'warm_start'가 True 인 경우 마지막 피팅 솔루션은 다음 fit () 호출에 대한 초기화로 사용됩니다. 이렇게하면 비슷한 문제에서 적합이 여러 번 호출 될 때 수렴 속도가 빨라질 수 있습니다. 이 경우 'n_init'는 무시되고 첫 번째 호출시 단일 초기화 만 발생합니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="245f671779646599b33075b7dcf37bf735b34555" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">'warm_start'가 True 인 경우 마지막 피팅 솔루션은 다음 fit () 호출에 대한 초기화로 사용됩니다. 이렇게하면 비슷한 문제에서 적합이 여러 번 호출 될 때 수렴 속도가 빨라질 수 있습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e311856e2dbc16a358c30263eb21c62e3f976c09" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; is given an explicit &lt;code&gt;feature_range=(min, max)&lt;/code&gt; the full formula is:</source>
          <target state="translated">경우 &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; 가&lt;/a&gt; 명시 주어진다 &lt;code&gt;feature_range=(min, max)&lt;/code&gt; 전체 공식은 :</target>
        </trans-unit>
        <trans-unit id="6f352ac5787c6e0994736f1793f840aefef2eb74" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.</source>
          <target state="translated">만일 &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; 및 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; , 요구가 설명 될 것을 분산의 양 n_components 의해 특정 비율 이상이되도록 구성 요소의 수를 선택한다.</target>
        </trans-unit>
        <trans-unit id="6154e481a1bfebf053da4021c41ed6b15075ac75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;Gram&lt;/code&gt; is overwritten.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 &lt;code&gt;Gram&lt;/code&gt; 을 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="c8ea59a59509714d84c6c3be2a8959e87ca2c339" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; is overwritten.</source>
          <target state="translated">경우 &lt;code&gt;False&lt;/code&gt; , &lt;code&gt;X&lt;/code&gt; 는 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="ecd953eee019b7cf39fa95c1745e9486ce5fb903" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 올바르게 분류 된 샘플 수를 반환하십시오. 그렇지 않으면 올바르게 분류 된 샘플의 일부를 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="8dd52da2b8b6d856acbbc6b969b86fd0a4246941" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of misclassifications. Otherwise, return the fraction of misclassifications.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 분류 오류 수를 반환하십시오. 그렇지 않으면 오 분류 부분을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="85fcfc531652a8814592a07e791b2030fbc9598e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 인 경우 샘플 세트에 대한 Jaccard 유사성 계수의 합을 반환하십시오. 그렇지 않으면 Jaccard 유사성 계수의 평균을 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="c21045a17e85201e2f77134fc96d9edd698a8ef9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores.</source>
          <target state="translated">경우 &lt;code&gt;False&lt;/code&gt; 의 &lt;code&gt;cv_results_&lt;/code&gt; 의 속성은 교육 점수를하지 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="4f7a2b9af6d7b5ad533a302e51ca948f564886c6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="translated">경우 &lt;code&gt;None&lt;/code&gt; 추의 기본 득점 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0206caad9c301767e28c1eb37b72a3a7f7598e94" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">경우 &lt;code&gt;None&lt;/code&gt; , 각 클래스에 대한 점수가 반환됩니다. 그렇지 않으면 데이터에서 수행되는 평균화 유형을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="f1170dbf5a5618e80add067200212cb5804a58cd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">경우 &lt;code&gt;True&lt;/code&gt; 전체 경로에 저장됩니다 &lt;code&gt;coef_path_&lt;/code&gt; 의 속성. 큰 문제 나 많은 대상에 대한 솔루션을 계산하는 경우 &lt;code&gt;fit_path&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정 하면 특히 작은 알파에서 속도가 빨라 집니다 .</target>
        </trans-unit>
        <trans-unit id="e703de20e5680ee264e2b1b950a8b1ca587cd24f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="translated">경우 &lt;code&gt;True&lt;/code&gt; , X가 복사됩니다; 그렇지 않으면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e26b0bb501133486ba99496e311f44a49ce472b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, perform metric MDS; otherwise, perform nonmetric MDS.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 메트릭 MDS를 수행하십시오. 그렇지 않으면 비 메트릭 MDS를 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="a2b129bca8e38a348fd53d1896c7796acded2f57" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return a sparse feature matrix</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 희소 피쳐 행렬을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="887d26ef7077238a636d223d3985225a211c8d82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 데이터가 추출 된 클래스에 대해 지정된 클래스의 확률 및 조건부 확률을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0b7bef40bad08d9d2c4e67da6c9b56ea79751005" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, some instances might not belong to any class.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 인 경우 일부 인스턴스는 어떤 클래스에도 속하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e223ba26a8622e808f0df02e86007844177282d6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; or &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; 또는 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 패널티가 L1 놈인가된다. 경우 &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 계수가 제로로 압축되는보다 임계 값의 절대 값이다. 경우 &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt; , &lt;code&gt;alpha&lt;/code&gt; 허용 매개 변수 : 재건 오류의 값이 대상. 이 경우 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 를 대체 합니다 .</target>
        </trans-unit>
        <trans-unit id="a88caf8f6b6232395c9c1524315c6ed672bcf763" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt; and X is encoded as a CSR matrix;</source>
          <target state="translated">&lt;code&gt;axis=0&lt;/code&gt; 이고 X가 CSR 매트릭스로 인코딩 된 경우 ;</target>
        </trans-unit>
        <trans-unit id="b74f02ef0c3e3aceaf2040e39764c8bf5d153fee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, then impute along columns.</source>
          <target state="translated">&lt;code&gt;axis=0&lt;/code&gt; 인 경우 열을 따라 대치합니다.</target>
        </trans-unit>
        <trans-unit id="231cba4e2ed9eee1fca5c3a6b02c0c6f66c47550" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt; and X is encoded as a CSC matrix.</source>
          <target state="translated">&lt;code&gt;axis=1&lt;/code&gt; 이고 X가 CSC 행렬로 인코딩 된 경우 .</target>
        </trans-unit>
        <trans-unit id="4405a4c1e894889993d89bb6694cef1a6a8f7db3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;, then impute along rows.</source>
          <target state="translated">&lt;code&gt;axis=1&lt;/code&gt; 인 경우 행을 따라 대치합니다.</target>
        </trans-unit>
        <trans-unit id="bf71a4a70c1ff1b9076f02c43d89e78c4b0ffc27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;backend&lt;/code&gt; is a string it must match a previously registered implementation using the &lt;code&gt;register_parallel_backend&lt;/code&gt; function.</source>
          <target state="translated">경우 &lt;code&gt;backend&lt;/code&gt; 문자열이 그것을 사용하여 이전에 등록 된 구현과 일치해야 &lt;code&gt;register_parallel_backend&lt;/code&gt; 의 기능을.</target>
        </trans-unit>
        <trans-unit id="6456a4494f2ba1f052aff4cf6d35ef66e787bc14" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;base_estimator&lt;/code&gt; is None, then &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; is used for target values of dtype float.</source>
          <target state="translated">경우 &lt;code&gt;base_estimator&lt;/code&gt; 은 다음 없음, 없다 &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; DTYPE 플로트의 목표 값에 사용된다.</target>
        </trans-unit>
        <trans-unit id="898731158b64382ef9aad2307b39d22b5cd2a315" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dense&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the dense binary indicator format. If &lt;code&gt;'sparse'&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the sparse binary indicator format. &lt;code&gt;False&lt;/code&gt; returns a list of lists of labels.</source>
          <target state="translated">&lt;code&gt;dense&lt;/code&gt; 경우 밀도 가 높은 이진 표시기 형식으로 &lt;code&gt;Y&lt;/code&gt; 를 반환 합니다. 만약 &lt;code&gt;'sparse'&lt;/code&gt; 복귀 &lt;code&gt;Y&lt;/code&gt; 스파 스 이진 표시 형식입니다. &lt;code&gt;False&lt;/code&gt; 는 레이블 목록의 목록을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b1213caecc623f2a5139e82ba5db339edb5f6265" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="translated">경우 &lt;code&gt;fit_intercept&lt;/code&gt; 이 False로 설정되어, 절편은 0으로 설정됩니다. 주어진 문제가 이진 인 경우 &lt;code&gt;intercept_&lt;/code&gt; 의 모양은 (1,)입니다. 특히 &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt; 경우 &lt;code&gt;intercept_&lt;/code&gt; 는 결과 1 (True)에 해당 하고 &lt;code&gt;-intercept_&lt;/code&gt; 는 결과 0 (False)에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="4504cae87026fef1f6989cfa20e2e5bc171d37e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape(1,) when the problem is binary.</source>
          <target state="translated">경우 &lt;code&gt;fit_intercept&lt;/code&gt; 이 False로 설정되어, 절편은 0으로 설정됩니다. &lt;code&gt;intercept_&lt;/code&gt; 는 문제가 이진 인 경우 shape (1,)입니다.</target>
        </trans-unit>
        <trans-unit id="646836188c841e4fea39e4e4200d1f27e6191986" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;loss&lt;/code&gt; is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on &lt;code&gt;X[i]&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;loss&lt;/code&gt; 호출 가능한 인, 그것은 입력, 실제 및 예측 된 값과 두 배열 소요의 손실에 대응하는 어레이의 i 번째 값이 1-D 어레이를 리턴하는 함수이어야 &lt;code&gt;X[i]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c42d62680a26d66fc0f439c634f24ee01c670c77" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;memory&lt;/code&gt; is not joblib.Memory-like.</source>
          <target state="translated">&lt;code&gt;memory&lt;/code&gt; 가 작동하지 않는 경우 메모리 와 유사합니다.</target>
        </trans-unit>
        <trans-unit id="e14dd7c153267d74f6b2214ce66bcf041482d792" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_bins&lt;/code&gt; is an array, and there is an ignored feature at index &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;n_bins[i]&lt;/code&gt; will be ignored.</source>
          <target state="translated">경우 &lt;code&gt;n_bins&lt;/code&gt; 가 배열이며, 인덱스에서 무시 기능가 &lt;code&gt;i&lt;/code&gt; , &lt;code&gt;n_bins[i]&lt;/code&gt; 무시된다.</target>
        </trans-unit>
        <trans-unit id="20ab457ec31da79f104a0f7a337ba6bfe94b5438" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.</source>
          <target state="translated">경우 &lt;code&gt;n_clusters&lt;/code&gt; 가 없음으로 설정하고, 데이터는 클러스터 (158)의 세트에 10 개 샘플에서 감소된다. 이는 최종 (전역) 클러스터링 단계 이전의 전처리 단계로 볼 수 있으며이 158 개의 클러스터를 100 개의 클러스터로 추가로 줄입니다.</target>
        </trans-unit>
        <trans-unit id="1769c2fe615105013ff722090827f85ac960dff9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components == 'mle'&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, Minka&amp;rsquo;s MLE is used to guess the dimension. Use of &lt;code&gt;n_components == 'mle'&lt;/code&gt; will interpret &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; as &lt;code&gt;svd_solver == 'full'&lt;/code&gt;.</source>
          <target state="translated">만약 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 와 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; , Minka의 MLE는 차원을 추측하는 데 사용됩니다. 의 사용 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 해석합니다 &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; 같은 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="959758e689ea656dd0e72e3bda18b0a45bbef2e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is not set then all components are stored and the sum of the ratios is equal to 1.0.</source>
          <target state="translated">경우 &lt;code&gt;n_components&lt;/code&gt; 가 설정되지 않는 모든 성분은 저장 비율의 합은 1.0와 동일한다.</target>
        </trans-unit>
        <trans-unit id="4aaa2df3fa37c88b24d06638ef7188d7e8ebe112" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each parameter setting(and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;n_jobs&lt;/code&gt; 가 1보다 높은 값으로 설정된 경우 데이터는 각 매개 변수 설정 ( &lt;code&gt;n_jobs&lt;/code&gt; 시간이 아님) 에 대해 복사됩니다 . 이는 개별 작업에 시간이 거의 걸리지 않는 효율성을 위해 수행되지만 데이터 세트가 크고 사용 가능한 메모리가 충분하지 않으면 오류가 발생할 수 있습니다. 이 경우의 해결 방법은 &lt;code&gt;pre_dispatch&lt;/code&gt; 를 설정하는 것 입니다. 그런 다음 메모리는 &lt;code&gt;pre_dispatch&lt;/code&gt; 만 여러 번 복사됩니다 . &lt;code&gt;pre_dispatch&lt;/code&gt; 의 적절한 값 은 &lt;code&gt;2 * n_jobs&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="53682a81a25d0884d79ca09b064b0fc6e7cabd67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each point in the grid (and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;n_jobs&lt;/code&gt; 가 하나보다 높은 값으로 설정하고, 데이터는 각 격자 점에 대한 복사 (되지 않고 &lt;code&gt;n_jobs&lt;/code&gt; 회). 이는 개별 작업에 시간이 거의 걸리지 않는 효율성을 위해 수행되지만 데이터 세트가 크고 사용 가능한 메모리가 충분하지 않으면 오류가 발생할 수 있습니다. 이 경우의 해결 방법은 &lt;code&gt;pre_dispatch&lt;/code&gt; 를 설정하는 것 입니다. 그런 다음 메모리는 &lt;code&gt;pre_dispatch&lt;/code&gt; 만 여러 번 복사됩니다 . &lt;code&gt;pre_dispatch&lt;/code&gt; 의 적절한 값 은 &lt;code&gt;2 * n_jobs&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="c1cc035dd2ff12188f95601d6fe5c4679ad6bb78" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_samples == 10000&lt;/code&gt;, storing &lt;code&gt;X&lt;/code&gt; as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = &lt;strong&gt;4GB in RAM&lt;/strong&gt; which is barely manageable on today&amp;rsquo;s computers.</source>
          <target state="translated">경우 &lt;code&gt;n_samples == 10000&lt;/code&gt; , 저장 &lt;code&gt;X&lt;/code&gt; 형 float32의 NumPy와 배열 등은 10000 X 100000 &amp;times; 4 바이트 = 필요 &lt;strong&gt;RAM 4GB를&lt;/strong&gt; 오늘날의 컴퓨터에서 거의 관리입니다.</target>
        </trans-unit>
        <trans-unit id="8e65ba558868cb56b0c8a76632ea67901b254afe" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.</source>
          <target state="translated">경우 &lt;code&gt;normalize == True&lt;/code&gt; , 그렇지 않으면 샘플 세트를 통해 인 Jaccard 유사성 계수의 합계를 반환 평균 인 Jaccard 유사성 계수를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c606413521700e073d3e669024415faa2300a113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 인 경우 올바르게 분류 된 샘플의 소수 (float)를 반환하고, 그렇지 않으면 올바르게 분류 된 샘플의 수 (int)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="cd9dc36a4d7167817c2823908b4ce633913b9765" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; 인 경우 오 분류의 소수 (float)를 반환하고 그렇지 않으면 오 분류 수 (int)를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d0b860961bc5b4a4c45189c0fd4de5c2d61f1ab4" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;out=None&lt;/code&gt;, returns a new array containing the mean values, otherwise a reference to the output array is returned.</source>
          <target state="translated">&lt;code&gt;out=None&lt;/code&gt; 인 경우 평균값이 포함 된 새 배열을 반환하고, 그렇지 않으면 출력 배열에 대한 참조가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="a1e72f87e91fc5bb6f8882ece59a46bf9ee089e2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;pos_label is None&lt;/code&gt; and in binary classification, this function returns the average precision, recall and F-measure if &lt;code&gt;average&lt;/code&gt; is one of &lt;code&gt;'micro'&lt;/code&gt;, &lt;code&gt;'macro'&lt;/code&gt;, &lt;code&gt;'weighted'&lt;/code&gt; or &lt;code&gt;'samples'&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;pos_label is None&lt;/code&gt; 이진 분류하는 경우,이 함수는 평균 정밀도, 소환 및 F 측정 값을 반환 &lt;code&gt;average&lt;/code&gt; 의 하나 인 &lt;code&gt;'micro'&lt;/code&gt; , &lt;code&gt;'macro'&lt;/code&gt; , &lt;code&gt;'weighted'&lt;/code&gt; 또는 &lt;code&gt;'samples'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7bb4c6eca31ede3ca3e8fe5a9b41ecd9a55b266b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_path==True&lt;/code&gt; returns the entire path, else returns only the last point of the path.</source>
          <target state="translated">만약 &lt;code&gt;return_path==True&lt;/code&gt; 경로의 반환 전체 경로, 다른 반환 마지막 지점.</target>
        </trans-unit>
        <trans-unit id="9e477bb8072307702a812f869b8166fe31bf9ca0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</source>
          <target state="translated">경우 &lt;code&gt;smooth_idf=True&lt;/code&gt; (d, t IDF (디폴트), 상수 &quot;1&quot;분자와 방위군의 분모에 추가되는 여분의 문서는 제로 분열을 방지하기 회만 컬렉션에있는 모든 단어를 포함하는 본 것처럼 ) = 로그 [(1 + n) / (1 + df (d, t))] + 1.</target>
        </trans-unit>
        <trans-unit id="3f772487671a5560a2a2bb3464b54cf0bad5b348" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt;, the number of components must be strictly less than the minimum of n_features and n_samples.</source>
          <target state="translated">경우 &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt; , 구성 요소의 수는 n_features와 N_SAMPLES의 최소보다 엄격 이하 여야합니다.</target>
        </trans-unit>
        <trans-unit id="abdb8ed2b871d03510343cf9ee77736674d298ab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;validate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; will be checked.</source>
          <target state="translated">경우 &lt;code&gt;validate&lt;/code&gt; 있다 &lt;code&gt;True&lt;/code&gt; , &lt;code&gt;X&lt;/code&gt; 가 확인됩니다.</target>
        </trans-unit>
        <trans-unit id="5cfb594032bd50fcef2738a25df520e95867f411" translate="yes" xml:space="preserve">
          <source>If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:</source>
          <target state="translated">C가 기본 진리 클래스 지정이고 K가 클러스터링이면 \ (a \) 및 \ (b \)를 다음과 같이 정의하십시오.</target>
        </trans-unit>
        <trans-unit id="40e72ab25b1921db07187a1c526cc9080a10eaea" translate="yes" xml:space="preserve">
          <source>If False, X will be overwritten. &lt;code&gt;copy=False&lt;/code&gt; can be used to save memory but is unsafe for general use.</source>
          <target state="translated">False이면 X를 덮어 씁니다. &lt;code&gt;copy=False&lt;/code&gt; 를 사용하여 메모리를 절약 할 수 있지만 일반적인 용도로는 안전하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b5379fd8e8700833a560e6ab84ea58c40e10b6a8" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</source>
          <target state="translated">False, fit으로 전달 된 데이터를 겹쳐 쓰고 fit (X) .transform (X)을 실행하면 예상 결과가 나오지 않고 대신 fit_transform (X)를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="3d545281a4ef31d03ffb244079b728a3c5cc8b18" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten. Defaults to True.</source>
          <target state="translated">False이면 적합하게 전달 된 데이터를 덮어 씁니다. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="4bf616e8d9d604d2525c59d889782525e410270c" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned</source>
          <target state="translated">False이면 거리가 반환되지 않습니다</target>
        </trans-unit>
        <trans-unit id="d8cdf8e9cb326e6212f67c80aca3a4f04326fc4c" translate="yes" xml:space="preserve">
          <source>If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False 인 경우 소스 사이트에서 데이터를 다운로드하지 않고 로컬로 데이터를 사용할 수없는 경우 IOError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="707f36c34b2f81eacbaf143a8e62cb9371b1332e" translate="yes" xml:space="preserve">
          <source>If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">False 인 경우 소스 사이트에서 데이터를 다운로드하지 않고 로컬로 데이터를 사용할 수없는 경우 IOError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="d32001af2bcb0806daa431ab8cf432f700c0bb79" translate="yes" xml:space="preserve">
          <source>If False, the imputer mask will be a numpy array.</source>
          <target state="translated">False이면 imputer 마스크는 numpy 배열입니다.</target>
        </trans-unit>
        <trans-unit id="2823ebb07c9bdb5cae0bfca227f5db48d585ba5a" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and dictionary will not be checked.</source>
          <target state="translated">False이면 입력 배열 X와 사전이 확인되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bd8e933f9aa74b9b27da566d4ffb96f4e62218cf" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and y will not be checked.</source>
          <target state="translated">False이면 입력 배열 X와 y는 확인되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="85aa52dd8c7d5d29b6bebfd616a7ca3fe91cde14" translate="yes" xml:space="preserve">
          <source>If False, the projected data uses a sparse representation if the input is sparse.</source>
          <target state="translated">False이면 입력이 희소 인 경우 투영 된 데이터가 희소 표현을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="7bfec8f3204bdf713e2d3557ec53ea6f3960ad24" translate="yes" xml:space="preserve">
          <source>If False, there is no input validation.</source>
          <target state="translated">False이면 입력 유효성 검사가 없습니다.</target>
        </trans-unit>
        <trans-unit id="a67320198a0b746d35fcc941198f1221ee73c87b" translate="yes" xml:space="preserve">
          <source>If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</source>
          <target state="translated">False이면 복사를 피하고 대신 스케일을 조정하십시오. 항상 제대로 작동한다고 보장되는 것은 아닙니다. 예를 들어 데이터가 NumPy 배열 또는 scipy.sparse CSR 행렬이 아닌 경우에도 사본이 반환 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fcd08eda0bca1685e28de89ae046095006b92653" translate="yes" xml:space="preserve">
          <source>If None (default), load all the categories. If not None, list of category names to load (other categories ignored).</source>
          <target state="translated">None (기본값)이면 모든 범주를로드하십시오. None이 아닌 경우,로드 할 카테고리 이름 목록 (다른 카테고리는 무시 됨).</target>
        </trans-unit>
        <trans-unit id="a7cbd99fe721a3cd173045eddaf688b83e7620c1" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s default scorer, if available, is used.</source>
          <target state="translated">None 인 경우, 추정기의 기본 채점자가 사용 가능한 경우 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="96ad58db5ee1b903109c173fcab72b0955bb0408" translate="yes" xml:space="preserve">
          <source>If None, defaults to 1.0 / n_features</source>
          <target state="translated">None 인 경우 기본값은 1.0 / n_features입니다.</target>
        </trans-unit>
        <trans-unit id="e5ce9a9046a52014390758ba790166ae01779c1f" translate="yes" xml:space="preserve">
          <source>If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.</source>
          <target state="translated">없음 인 경우 파일의 내용을 해독하지 마십시오 (예 : 이미지 또는 텍스트가 아닌 다른 내용의 경우). None이 아니면 load_content가 True 인 경우 텍스트 파일을 유니 코드로 디코딩하는 데 사용할 인코딩입니다.</target>
        </trans-unit>
        <trans-unit id="3e5e8d666168a7a15a80edb16364256ae0a379e4" translate="yes" xml:space="preserve">
          <source>If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</source>
          <target state="translated">None이면 중지 단어가 사용되지 않습니다. max_df는 [0.7, 1.0) 범위의 값으로 설정하여 코퍼스 내 문서 용어 빈도를 기반으로 정지 단어를 자동으로 감지하고 필터링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02542a43a2f09f5328657402d69f49ce442cb6c2" translate="yes" xml:space="preserve">
          <source>If None, pairwise_distances_chunked returns a generator of vertical chunks of the distance matrix.</source>
          <target state="translated">None이면 pairwise_distances_chunked는 거리 행렬의 수직 청크 생성기를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="eb5d73cb83520641b0e2c815c109159135d569cc" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s default scorer (if available) is used.</source>
          <target state="translated">None 인 경우 추정기의 기본 채점자 (사용 가능한 경우)가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="651c3653c15c90a6722a00465ba57c19c30adb9b" translate="yes" xml:space="preserve">
          <source>If None, the threshold is assumed to be half way between neg_label and pos_label.</source>
          <target state="translated">없음 인 경우 임계 값은 neg_label과 pos_label 사이의 절반으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="ac652d29bc285e4e46f5aaf2fe5415c63aee1f09" translate="yes" xml:space="preserve">
          <source>If None, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">None이면 &lt;code&gt;max_features=n_features&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="2f9e5ee96434f57529c2481b71d631d9dd0cb5e7" translate="yes" xml:space="preserve">
          <source>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</source>
          <target state="translated">True (기본값) 인 경우 제곱 오류 규범은 n_features로 나뉩니다. False이면 제곱 오류 규범의 크기가 조정되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="da82574bb396bf8045c493d20398be74e4e9ef51" translate="yes" xml:space="preserve">
          <source>If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).</source>
          <target state="translated">True (기본값) 인 경우 모든 다항식 거듭 제곱이 0 인 특징 (즉, 1의 열-선형 모델에서 절편 항의 역할을 함) 인 바이어스 열을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="d150b2a4c21e929dfd726f6463d03ca9f005e91a" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="translated">True (기본값) 인 경우 누락 된 값이없는 변환에 누락 된 값이있는 피처가있는 경우 변환에서 오류가 발생 &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt; 경우에만 적용 가능합니다 .</target>
        </trans-unit>
        <trans-unit id="d91ad850130f94be79fb668041bf3eecd01a29b0" translate="yes" xml:space="preserve">
          <source>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to &amp;lsquo;sag&amp;rsquo;. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</source>
          <target state="translated">True이고 X가 희소 인 경우이 방법은 절편도 반환하며 솔버는 자동으로 'sag'로 변경됩니다. 이것은 희소 데이터로 절편을 맞추기위한 임시 수정일뿐입니다. 밀도가 높은 데이터의 경우 회귀 전에 sklearn.linear_model._preprocess_data를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="a68108e0ea5bf983075f127e077ee80517d6dfdd" translate="yes" xml:space="preserve">
          <source>If True the covariance matrices are computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="translated">True이면 공분산 행렬이 계산되어 &lt;code&gt;self.covariance_&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="5c5d5facc126a265032a533a4664c8926339ade0" translate="yes" xml:space="preserve">
          <source>If True the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">True이면 전체 경로가 &lt;code&gt;coef_path_&lt;/code&gt; 속성에 저장됩니다 . 큰 문제 나 많은 대상에 대한 솔루션을 계산하는 경우 &lt;code&gt;fit_path&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정 하면 특히 작은 알파에서 속도가 빨라 집니다 .</target>
        </trans-unit>
        <trans-unit id="53e960778922c6ba257a9c66f18d0e260655f043" translate="yes" xml:space="preserve">
          <source>If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs.</source>
          <target state="translated">True 인 경우 함수는 pairwise distance matrix를 반환하고 그렇지 않으면 componentwise L1 pairwise-distances를 반환합니다. 희소 행렬 입력에는 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2546c89362b151bbba35dab463b810d1f7c0a359" translate="yes" xml:space="preserve">
          <source>If True the order of the dataset is shuffled to avoid having images of the same person grouped.</source>
          <target state="translated">True 인 경우 동일한 사람의 이미지가 그룹화되지 않도록 데이터 세트의 순서가 섞입니다.</target>
        </trans-unit>
        <trans-unit id="618a67ac95fc4ccc3385ae319143bf344e1ffb63" translate="yes" xml:space="preserve">
          <source>If True then raise a warning if conversion is required.</source>
          <target state="translated">True이면 변환이 필요한 경우 경고를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="19362eed638b2dc6d204e12092075aedd87e6e93" translate="yes" xml:space="preserve">
          <source>If True then raise an exception if array is not symmetric.</source>
          <target state="translated">True이면 배열이 대칭이 아닌 경우 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="baf82faf959595f525e5d5a94c6b8526ad942779" translate="yes" xml:space="preserve">
          <source>If True, X will be copied; else, it may be overwritten.</source>
          <target state="translated">True이면 X가 복사됩니다. 그렇지 않으면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa91f074d713faca021e35ddd5331805b9848f9e" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, a copy may still be returned if X&amp;rsquo;s dtype is not a floating point type.</source>
          <target state="translated">True이면 X의 사본이 생성됩니다. False 인 경우 X의 dtype이 부동 소수점 유형이 아닌 경우에도 사본이 리턴 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="054b374d036034be5d7258a6a975038eb8fec935" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if &lt;code&gt;copy=False&lt;/code&gt;:</source>
          <target state="translated">True이면 X의 사본이 생성됩니다. False이면 대치가 가능할 때마다 제자리에서 수행됩니다. 다음과 같은 경우 &lt;code&gt;copy=False&lt;/code&gt; 인 경우에도 항상 새 사본이 작성됩니다 .</target>
        </trans-unit>
        <trans-unit id="f237ade75e04520befb53fc36267d58be41dc5ae" translate="yes" xml:space="preserve">
          <source>If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.</source>
          <target state="translated">True 인 경우 교육 데이터의 영구 사본이 객체에 저장됩니다. 그렇지 않으면 훈련 데이터에 대한 참조 만 저장되므로 데이터를 외부에서 수정하면 예측이 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5b4ca3bdeb594ab34289df8cfc7fa70c9919ad95" translate="yes" xml:space="preserve">
          <source>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</source>
          <target state="translated">True 인 경우 0이 아닌 모든 카운트가 1로 설정됩니다. 이는 정수 카운트가 아닌 이진 이벤트를 모델링하는 이산 확률 모델에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="0beec2b2d6b910456e155063f83ec1e59c6c0df1" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)</source>
          <target state="translated">True 인 경우 0이 아닌 모든 항의 개수는 1로 설정됩니다. 이는 출력에 0/1 값만있는 것은 아니며 tf-idf의 tf 항은 이진수라는 의미입니다. 0/1 출력을 얻으려면 idf 및 normalization을 False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="b69242de00e60526a79082b37846a2a724d7b3dd" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling.</source>
          <target state="translated">True이면 스케일링하기 전에 데이터를 중앙에 배치하십시오.</target>
        </trans-unit>
        <trans-unit id="6c9a4d2da449884c97015be12ce056a53dc04757" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">True이면 스케일링하기 전에 데이터를 중앙에 배치하십시오. 희소 행렬을 시도 할 때 작동하지 않으며 예외가 발생합니다. 중복 행렬을 중앙에 배치하면 일반적인 사용 사례에서 메모리에 비해 너무 큰 밀도가 높은 행렬을 작성해야하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="9d7135e468914be214009091b1fc11f6721afd0a" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This will cause &lt;code&gt;transform&lt;/code&gt; to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">True 인 경우 스케일링하기 전에 데이터를 중앙에 배치하십시오. 희소 행렬을 시도 할 때 &lt;code&gt;transform&lt;/code&gt; 으로 인해 예외가 발생합니다. 중심 행렬은 일반적인 사용 사례에서 너무 커서 메모리에 맞지 않는 밀도가 높은 행렬을 작성하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="20839df17b3bca6b55533337f17b7c17f0881d1a" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False</source>
          <target state="translated">True 인 경우 모델의 각 단계에서 목적 함수를 계산하십시오. 기본값은 거짓입니다</target>
        </trans-unit>
        <trans-unit id="afb80999f635ad0619301e31bb4cd6b353188af0" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False.</source>
          <target state="translated">True 인 경우 모델의 각 단계에서 목적 함수를 계산하십시오. 기본값은 거짓입니다.</target>
        </trans-unit>
        <trans-unit id="9df36ef1b24eb49a93a9d932c395b3324554689c" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False이면 데이터가 계산 전에 중심에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="cdd266c276f30f1ed8fec5e418bed1790d829f9f" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 거의 같지만 정확히 0이 아닌 데이터로 작업 할 때 유용합니다. False (기본값)이면 데이터가 계산 전에 중앙에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="b4e1adfc1374b7a62eee31a2ac4be08eea958149" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">True 인 경우 데이터가 계산 전에 중심에 있지 않습니다. 평균이 거의 같지만 정확히 0이 아닌 데이터로 작업 할 때 유용합니다. False이면 데이터가 계산 전에 중심에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="a2902b07926590508dee697d1e97c541f35cd531" translate="yes" xml:space="preserve">
          <source>If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.</source>
          <target state="translated">True 인 경우 입력 및 랜덤 프로젝션 매트릭스가 모두 희소 한 경우에도 랜덤 프로젝션의 출력이 밀도가 높은 numpy 배열인지 확인하십시오. 실제로, 구성 요소의 수가 적 으면 프로젝션 된 데이터의 제로 구성 요소의 수가 매우 적고 밀도가 높은 표현을 사용하는 것이 CPU 및 메모리 효율성이 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="a3946c2202800dbed0f15da39a81b563f2054e41" translate="yes" xml:space="preserve">
          <source>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">True 인 경우 개별 트리는 대체로 샘플링 된 훈련 데이터의 임의의 하위 집합에 적합합니다. False이면 교체하지 않은 샘플링이 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="47a8f3ebe1bf3e97122b0404e375e9c09f1cef3f" translate="yes" xml:space="preserve">
          <source>If True, input X is copied and stored by the model in the &lt;code&gt;X_fit_&lt;/code&gt; attribute. If no further changes will be done to X, setting &lt;code&gt;copy_X=False&lt;/code&gt; saves memory by storing a reference.</source>
          <target state="translated">True 인 경우 입력 X는 모델에 의해 &lt;code&gt;X_fit_&lt;/code&gt; 속성으로 복사 및 저장됩니다 . X를 더 이상 변경하지 않으면 &lt;code&gt;copy_X=False&lt;/code&gt; 로 설정 하면 참조를 저장하여 메모리가 절약됩니다.</target>
        </trans-unit>
        <trans-unit id="10ab7d9c4ef9751f0861c3cfbcd363da08ee1196" translate="yes" xml:space="preserve">
          <source>If True, return a sparse CSR continency matrix. If &lt;code&gt;eps is not None&lt;/code&gt;, and &lt;code&gt;sparse is True&lt;/code&gt;, will throw ValueError.</source>
          <target state="translated">True이면 희소 CSR 연속성 행렬을 반환합니다. 경우 &lt;code&gt;eps is not None&lt;/code&gt; , 및 &lt;code&gt;sparse is True&lt;/code&gt; , ValueError를 발생합니다.</target>
        </trans-unit>
        <trans-unit id="bbadcb21277fb2fd7500e5e018984adc0515f847" translate="yes" xml:space="preserve">
          <source>If True, return output as dict</source>
          <target state="translated">True이면 출력을 dict로 반환</target>
        </trans-unit>
        <trans-unit id="4c0f661b8fac7f22363a5a1e55327c6967bb56d8" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.21, to correspond to the standard definition of cross-validation.</source>
          <target state="translated">True 인 경우 각 테스트 세트의 샘플 수에 따라 가중치를 부여한 접기 수에 대한 평균 점수를 반환합니다. 이 경우 데이터는 폴드에 걸쳐 동일하게 분포 된 것으로 가정되며 최소화 된 손실은 폴드 전체의 평균 손실이 아니라 샘플 당 총 손실입니다. False이면 폴드에 대한 평균 점수를 반환합니다. 기본값은 True이지만 버전 0.21에서 표준 교차 정의의 정의에 따라 False로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="cd7031ac688b02e25258c5831c7d3ea164486db6" translate="yes" xml:space="preserve">
          <source>If True, return the distance between the clusters.</source>
          <target state="translated">참이면 군집 사이의 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="48eeb388f7c432fd1b00c136703cc691939b77aa" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data, target)&lt;/code&gt; 반환 합니다. &lt;code&gt;data&lt;/code&gt; 및 &lt;code&gt;target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b8ef976b3bf0a8c5fe0d328bf69ca77595314915" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; objects.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data, target)&lt;/code&gt; 반환 합니다. &lt;code&gt;data&lt;/code&gt; 및 &lt;code&gt;target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d467c22a2bd71ab649cbad26364f06a31ce58ac1" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data.data, data.target)&lt;/code&gt; instead of a Bunch object.</source>
          <target state="translated">True 인 경우 Bunch 객체 대신 &lt;code&gt;(data.data, data.target)&lt;/code&gt; 반환 합니다.</target>
        </trans-unit>
        <trans-unit id="248b6d0d481e47f352d5f3203242e6800072c658" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;dataset.data&lt;/code&gt; and &lt;code&gt;dataset.target&lt;/code&gt; object.</source>
          <target state="translated">True이면 Bunch 객체 대신 &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; 반환 합니다. &lt;code&gt;dataset.data&lt;/code&gt; 및 &lt;code&gt;dataset.target&lt;/code&gt; 객체 에 대한 자세한 내용은 아래를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="c220af25fde2fda1e9985d78b63166a331905d63" translate="yes" xml:space="preserve">
          <source>If True, scale the data to interquartile range.</source>
          <target state="translated">True 인 경우 데이터를 사 분위수 범위로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="0f395f8257fe0bdfb8a823c88f3be9843583f8a6" translate="yes" xml:space="preserve">
          <source>If True, scale the data to unit variance (or equivalently, unit standard deviation).</source>
          <target state="translated">True 인 경우 데이터를 단위 분산 (또는 동등하게 단위 표준 편차)으로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="a491ca8d8797fa01293c195b8787d725c30e073a" translate="yes" xml:space="preserve">
          <source>If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.</source>
          <target state="translated">True이면 클러스터가 하이퍼 큐브의 정점에 배치됩니다. False이면 클러스터는 임의 폴리 토프의 정점에 배치됩니다.</target>
        </trans-unit>
        <trans-unit id="dc426ca785aa82bc3726faf833e38673875ff1ab" translate="yes" xml:space="preserve">
          <source>If True, the coefficients of the underlying linear model are returned.</source>
          <target state="translated">True이면 기본 선형 모델의 계수가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="5724be8fac97575b0a1ba6783afc1c2fbe0fcac8" translate="yes" xml:space="preserve">
          <source>If True, the covariance of the joint predictive distribution at the query points is returned along with the mean</source>
          <target state="translated">True 인 경우 쿼리 지점에서 공동 예측 분포의 공분산이 평균과 함께 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="d28765388f526f443e8440713d9f120592d23759" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.</source>
          <target state="translated">True 인 경우, 위치 theta의 커널 하이퍼 파라미터에 대한 로그 한계 가능성의 기울기가 추가로 반환됩니다. True이면 theta는 None이 아니어야합니다.</target>
        </trans-unit>
        <trans-unit id="2ecaf6f4ca3e741011335b25b38b91313c972ea3" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.</source>
          <target state="translated">True 인 경우, 위치 theta의 커널 하이퍼 파라미터에 대한 로그 한계 가능성의 기울기가 추가로 반환됩니다. 비 이진 분류에는 그래디언트 계산이 지원되지 않습니다. True이면 theta는 None이 아니어야합니다.</target>
        </trans-unit>
        <trans-unit id="65ddb79c8d6a76beebeeb976d10455d7eb1fb46d" translate="yes" xml:space="preserve">
          <source>If True, the imputer mask will be a sparse matrix.</source>
          <target state="translated">True 인 경우 imputer 마스크는 희소 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="a134a28236267fd097c12ab6f4f7b85d957aa9ca" translate="yes" xml:space="preserve">
          <source>If True, the method also returns &lt;code&gt;n_iter&lt;/code&gt;, the actual number of iteration performed by the solver.</source>
          <target state="translated">True 인 경우, 메소드는 솔버가 수행 한 실제 반복 횟수 인 &lt;code&gt;n_iter&lt;/code&gt; 도 리턴합니다 .</target>
        </trans-unit>
        <trans-unit id="af50e40087f45610f2fbcc323b88ccd93dcf9324" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">True이면 회귀 분석 X가 회귀 분석 전에 정규화됩니다. &lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . 회귀 분석기가 정규화되면 하이퍼 파라미터가 학습 된 수와 샘플 수에 대해 거의 독립적임을 알게됩니다. 표준화 된 데이터에는 동일한 속성이 유효하지 않습니다. 그러나 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 견적 도구에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="016e8fefc2c3108a2b7a4351de86dada7ecbd68e" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">True이면 회귀 분석 X가 회귀 분석 전에 정규화됩니다. &lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . 회귀 분석기가 정규화되면 하이퍼 파라미터가 학습 된 수와 샘플 수에 대해 거의 독립적임을 알게됩니다. 표준화 된 데이터에는 동일한 속성이 유효하지 않습니다. 그러나 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 견적 도구에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="c3fdb7e36f654834d666d698b8cb7219451a4481" translate="yes" xml:space="preserve">
          <source>If True, the return value will be an array of integers, rather than a boolean mask.</source>
          <target state="translated">True 인 경우 반환 값은 부울 마스크가 아닌 정수 배열입니다.</target>
        </trans-unit>
        <trans-unit id="d17d4bbcdf2df4ef33c01a7114516c2e4e90d7d8" translate="yes" xml:space="preserve">
          <source>If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.</source>
          <target state="translated">True 인 경우 쿼리 지점에서 예측 분포의 표준 편차가 평균과 함께 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="3f294130716f356d91654999eee9757bd2ba6c1c" translate="yes" xml:space="preserve">
          <source>If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">True 인 경우 강력한 위치 및 공분산 추정값에 대한 지원이 계산되고 데이터를 중심으로하지 않고 공분산 추정값이 재 계산됩니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False 인 경우 추가 처리없이 FastMCD 알고리즘을 사용하여 강력한 위치 및 공분산을 직접 계산합니다.</target>
        </trans-unit>
        <trans-unit id="667a36b1a1b94aa0d760aa610f277fcd1918ae0a" translate="yes" xml:space="preserve">
          <source>If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">True 인 경우 강력한 위치 및 공분산 추정값에 대한 지원이 계산되고 데이터를 중심으로하지 않고 공분산 추정값이 재 계산됩니다. 평균이 0과 같지만 정확히 0이 아닌 데이터를 처리하는 데 유용합니다. False이면 추가 처리없이 FastMCD 알고리즘을 사용하여 강력한 위치 및 공분산을 직접 계산합니다.</target>
        </trans-unit>
        <trans-unit id="28346b69fb6f1a64d688b2ef42aabb524b6563d8" translate="yes" xml:space="preserve">
          <source>If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised.</source>
          <target state="translated">True 인 경우 X는 2 차원 NumPy 배열 또는 희소 행렬로 변환됩니다. 변환이 불가능하면 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="5c0fef9c1e6748fc4d3cb84e62459147a039a4fd" translate="yes" xml:space="preserve">
          <source>If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be &amp;lt; n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.</source>
          <target state="translated">True 인 경우 고유 값이 0 인 모든 구성 요소가 제거되므로 출력의 구성 요소 수가 &amp;lt;n_components (숫자 불안정성으로 인해 때로는 0)가 될 수 있습니다. n_components가 None이면이 매개 변수는 무시되고 고유 값이 0 인 구성 요소는 상관없이 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="42874c4e7adb064c98a0fb44835161462f985220" translate="yes" xml:space="preserve">
          <source>If True, then compute normalized Laplacian.</source>
          <target state="translated">True이면 정규화 된 라플라시안을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="7818bd11ce52f2496690f2a19701e1fdc3bace9d" translate="yes" xml:space="preserve">
          <source>If True, transpose the downloaded data array.</source>
          <target state="translated">True이면 다운로드 한 데이터 배열을 바꿉니다.</target>
        </trans-unit>
        <trans-unit id="09f8cdcb36703e0413a45867eb062d48bc42e4a4" translate="yes" xml:space="preserve">
          <source>If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error. Global default: False.</source>
          <target state="translated">True이면 유한성에 대한 유효성 검사를 건너 뛰어 시간을 절약 할 수 있지만 충돌이 발생할 수 있습니다. False이면 유한성을 검증하여 오류를 방지합니다. 전역 기본값 : False</target>
        </trans-unit>
        <trans-unit id="f770d204acb3934762188e63b6bd0977cfe619aa" translate="yes" xml:space="preserve">
          <source>If True, will return the parameters for this estimator and contained subobjects that are estimators.</source>
          <target state="translated">True 인 경우이 추정기 및 추정 기인 하위 오브젝트에 대한 매개 변수를 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="edc518974aa9f8ea115d0f36469bc2b1e2cd15a2" translate="yes" xml:space="preserve">
          <source>If True, will return the query_id array for each file.</source>
          <target state="translated">True이면 각 파일에 대해 query_id 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="80fdba1026cc980884a84dfa72ad1747c034afc6" translate="yes" xml:space="preserve">
          <source>If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.</source>
          <target state="translated">X와 y가 C 순서가 아니며 np.float64의 연속 배열이고 X가 scipy.sparse.csr_matrix가 아닌 경우 X 및 / 또는 y가 복사 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a601440183ce5a872479c357e441563196aab652" translate="yes" xml:space="preserve">
          <source>If X is a dense array, then the other methods will not support sparse matrices as input.</source>
          <target state="translated">X가 고밀도 배열 인 경우 다른 방법은 희소 행렬을 입력으로 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="efab9063b47a2afb85758f067069188b21b34f3e" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix.</source>
          <target state="translated">X가 CSR 매트릭스로 인코딩 된 경우</target>
        </trans-unit>
        <trans-unit id="da96e9fabf18fc39905756390120e4a7a1e09f97" translate="yes" xml:space="preserve">
          <source>If X is not a C-ordered contiguous array it is copied.</source>
          <target state="translated">X가 C 순서의 연속 배열이 아닌 경우 복사됩니다.</target>
        </trans-unit>
        <trans-unit id="9cc8f34afbd30e04cc65d91f1b233abc1c382996" translate="yes" xml:space="preserve">
          <source>If X is not an array of floating values;</source>
          <target state="translated">X가 부동 값의 배열이 아닌 경우</target>
        </trans-unit>
        <trans-unit id="e7ca7ce1d419c3d60265041304210343f2e8b91d" translate="yes" xml:space="preserve">
          <source>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that &lt;em&gt;X = L C&lt;/em&gt;. Different criteria exist to choose the components</source>
          <target state="translated">X가 다변량 데이터 인 경우 우리가 해결하려고하는 문제는 다른 관측 기준으로 다시 작성하는 것입니다. 하중 L과 &lt;em&gt;X = LC&lt;/em&gt; 와 같은 성분 C 세트를 배우고 싶습니다 . 구성 요소를 선택하기위한 다른 기준</target>
        </trans-unit>
        <trans-unit id="4691b6eeb6f44a63af5f24ac30dc066ab023aa61" translate="yes" xml:space="preserve">
          <source>If X is sparse and &lt;code&gt;missing_values=0&lt;/code&gt;;</source>
          <target state="translated">X가 희소이고 &lt;code&gt;missing_values=0&lt;/code&gt; ;</target>
        </trans-unit>
        <trans-unit id="5bf131136f9283115170d3f032466f07678834a5" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.</source>
          <target state="translated">Y가 제공되면 (기본값은 None) 반환 된 행렬은 X와 Y의 배열 사이의 쌍별 거리입니다.</target>
        </trans-unit>
        <trans-unit id="a6e7fe3e345be28d5e67984fa49ad01aeaa444dd" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.</source>
          <target state="translated">Y가 주어지면 (기본값은 None), 반환 된 행렬은 X와 Y의 배열 사이의 쌍 커널입니다.</target>
        </trans-unit>
        <trans-unit id="15e0fd61e3b85d8ebad2fc2135f6d5822723ce41" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;, the square of the standard deviation, then the explained variance is estimated as follow:</source>
          <target state="translated">\ (\ hat {y} \)가 추정 대상 출력이고 \ (y \) 해당 (올바른) 대상 출력이고 \ (Var \)가 표준 편차의 제곱 인 &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt; 인 경우 설명 된 분산은 다음과 같습니다. 다음과 같이 추정됩니다 :</target>
        </trans-unit>
        <trans-unit id="c8fb389b8a60a2b57fc22c9161412c484a73dd18" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측값이고 \ (y_i \)가 해당하는 참값이면 0-1 손실 \ (L_ {0-1 } \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="01fda7f04ce93ad5d6b5843c80c53ee91e04866d" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값인 경우 \ (n_ \ text { 샘플} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="af46aeec0a0c654990b43c84ec26ca8a3817bbd3" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값 인 경우 \ (n_ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="27ab05c62dcfc0b1ca98228a2c106c2bd25d72f6" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the score R&amp;sup2; estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값이면 \ (n _ {\ text { 샘플}} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="e9be139031431f33624d9549cf24272bbec27cad" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값이면 \ (평균 절대 오차 (MAE)는 \ ( n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b7836e2114e345225a74c22cbe0d3b5d52c8f253" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 표본의 예측 된 값이고 \ (y_i \)가 해당하는 실제 값이면 \ (n)에 대해 추정 된 평균 제곱 오차 (MSE) n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="1cf47fc0a0aaaffd1c0a818b8704218b8ae722c1" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">\ (\ hat {y} _i \)가 \ (i \) 번째 샘플의 예측 된 값이고 \ (y_i \)가 해당하는 참값 인 경우, 평균 제곱 로그 오류 (MSLE)는 \ (n _ {\ text {samples}} \)는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="f295fa8851d145ac326bc63b92809e2fbf3d1f72" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_j\) is the predicted value for the \(j\)-th label of a given sample, \(y_j\) is the corresponding true value, and \(n_\text{labels}\) is the number of classes or labels, then the Hamming loss \(L_{Hamming}\) between two samples is defined as:</source>
          <target state="translated">\ (\ hat {y} _j \)가 주어진 샘플의 \ (j \) 번째 레이블에 대해 예측 된 값인 경우, \ ​​(y_j \)는 해당하는 참값이고 \ (n_ \ text {labels} \)는 클래스 또는 레이블의 수이며, 두 샘플 간의 해밍 손실 \ (L_ {Hamming} \)은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="7fa4bf510f83c73a55e8dec038abaacf960351ca" translate="yes" xml:space="preserve">
          <source>If \(c_0 = 0\) the kernel is said to be homogeneous.</source>
          <target state="translated">\ (c_0 = 0 \)이면 커널은 동종이라고합니다.</target>
        </trans-unit>
        <trans-unit id="c7d07701826b4f4c8efa455b46a49990993930f2" translate="yes" xml:space="preserve">
          <source>If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:</source>
          <target state="translated">\ (h_i \)가 주어지면 위의 방정식은 다음과 같은 확률 론적 해석을 자동으로 암시합니다.</target>
        </trans-unit>
        <trans-unit id="9d9af8bc9b90a6fbf0d539b126efbd694bdaddf6" translate="yes" xml:space="preserve">
          <source>If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:</source>
          <target state="translated">\ (y_i \)가 \ (i \) 번째 샘플의 실제 값이고 \ (w_i \)가 해당하는 샘플 무게이면 샘플 무게를 다음과 같이 조정합니다.</target>
        </trans-unit>
        <trans-unit id="4c76ddad0ef7a743f202685a0861880cbc2062e2" translate="yes" xml:space="preserve">
          <source>If \(y_w\) is the predicted decision for true label and \(y_t\) is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</source>
          <target state="translated">\ (y_w \)가 실제 레이블에 대한 예측 결정이고 \ (y_t \)가 다른 모든 레이블에 대한 예측 결정의 최대 값 인 경우 (예측 결정이 결정 기능에 의해 출력되는 경우) 다중 클래스 힌지 손실은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="b8371056060d53aef38082b274a12c6e92dd981b" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">accept_sparse가 CSR, CSC, COO 또는 BSR 스파 스 행렬을 제공하고 승인하는 경우 accept_large_sparse는 해당 인덱스가 32 비트 dtype으로 저장된 경우에만이를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="ca2f554a4272574081b19f205bd8db66223aa9f8" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse=False will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">accept_sparse가 CSR, CSC, COO 또는 BSR 스파 스 행렬을 제공하고 승인하는 경우 accept_large_sparse = False는 해당 인덱스가 32 비트 dtype으로 저장된 경우에만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="b0090a224443cfea422c2167734e98ec705e71a5" translate="yes" xml:space="preserve">
          <source>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</source>
          <target state="translated">호출 가능 항목이 전달되면 처리되지 않은 원시 입력에서 기능 시퀀스를 추출하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="dcd8eacb988e0fa27afa1a7692943530b07747f6" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, k and and a random state and return an initialization.</source>
          <target state="translated">호출 가능 항목이 전달되면 인수 X, k 및 임의 상태를 취하여 초기화를 리턴해야합니다.</target>
        </trans-unit>
        <trans-unit id="15b5ddf5d35e7b6d7436896e31247316b726ba30" translate="yes" xml:space="preserve">
          <source>If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If &lt;code&gt;None&lt;/code&gt;, nothing is adjusted.</source>
          <target state="translated">float이면 해당 값이 우연성 행렬의 모든 값에 추가됩니다. 이것은 NaN 전파를 막는 데 도움이됩니다. 경우 &lt;code&gt;None&lt;/code&gt; , 아무것도 조정하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="aec58020de2b924f9656034ee47c95a7ace302db" translate="yes" xml:space="preserve">
          <source>If a list is passed it&amp;rsquo;s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">목록이 전달되면 n_targets 같은 배열 중 하나 일 것으로 예상됩니다. 경로를 따라 다양한 계수 값. &lt;code&gt;fit_path&lt;/code&gt; 매개 변수가 &lt;code&gt;False&lt;/code&gt; 인 경우 존재하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="8e3b6cd9422926a607fefd39c3e9bd3020c06d14" translate="yes" xml:space="preserve">
          <source>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if &lt;code&gt;analyzer == 'word'&lt;/code&gt;.</source>
          <target state="translated">목록 인 경우 해당 목록에는 중지 단어가 포함 된 것으로 간주되며 모든 결과 토큰에서 제거됩니다. &lt;code&gt;analyzer == 'word'&lt;/code&gt; 경우에만 적용됩니다 .</target>
        </trans-unit>
        <trans-unit id="58d1b02436a7aa9160e580f582400827e1ad046d" translate="yes" xml:space="preserve">
          <source>If a string, it is passed to _check_stop_list and the appropriate stop list is returned. &amp;lsquo;english&amp;rsquo; is currently the only supported string value. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">문자열 인 경우 _check_stop_list로 전달되고 적절한 중지 목록이 리턴됩니다. 'english'는 현재 유일하게 지원되는 문자열 값입니다. 'english'에는 몇 가지 알려진 문제가 있으며 대안을 고려해야합니다 ( &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;중지 단어 사용&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="564b43bc82acf22a1de3b85bb28ac591ff97b4bf" translate="yes" xml:space="preserve">
          <source>If a string, this may be one of &amp;lsquo;nearest_neighbors&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo; or one of the kernels supported by &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt;.</source>
          <target state="translated">문자열 인 경우 'nearest_neighbors', 'precomputed', 'rbf'또는 &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt; 가 지원하는 커널 중 하나 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="675cbfc234c52633edd36cba3388cd72c1b8e2d3" translate="yes" xml:space="preserve">
          <source>If a target is a classification outcome taking on values 0,1,&amp;hellip;,K-1, for node \(m\), representing a region \(R_m\) with \(N_m\) observations, let</source>
          <target state="translated">대상이 \ (N_m \) 관측 값이있는 영역 \ (R_m \)을 나타내는 노드 \ (m \)에 대해 0,1,&amp;hellip;, K-1 값을 사용하는 분류 결과 인 경우</target>
        </trans-unit>
        <trans-unit id="373500a68bbbd934744d157d24ba37240f790a20" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="translated">선호도가 &quot;사전 계산 된&quot;X : 배열과 같은 모양 (n_samples, n_samples) 인 경우 X를 사전 계산 된 인접 그래프로 해석하여 샘플에서 계산합니다.</target>
        </trans-unit>
        <trans-unit id="c306493486d7abaed871db69a1b0f3a0d3e230f4" translate="yes" xml:space="preserve">
          <source>If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.</source>
          <target state="translated">선호도가 그래프의 인접 행렬 인 경우이 방법을 사용하여 정규화 된 그래프 컷을 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fef3ba1186af33eef8e244a6a4bb530d43ffdf84" translate="yes" xml:space="preserve">
          <source>If all examples are from the same class, it uses a one-class SVM.</source>
          <target state="translated">모든 예제가 동일한 클래스에서 온 경우 단일 클래스 SVM을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="b53805960d76925767243aca9c19243fc1b08d06" translate="yes" xml:space="preserve">
          <source>If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.</source>
          <target state="translated">모든 매개 변수가 목록으로 표시되면 교체하지 않은 샘플링이 수행됩니다. 분포로 적어도 하나의 매개 변수가 제공되면 대체 샘플링이 사용됩니다. 연속 모수에 연속 분포를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="4f219d1953670922fbbfe88159427df7222c9397" translate="yes" xml:space="preserve">
          <source>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</source>
          <target state="translated">선형 서포트 벡터 머신 또는 PCA와 같은 알고리즘이 데이터 포인트 \ (x_i \)의 스칼라 곱에만 의존하는 경우 적용에 해당하는 \ (k (x_i, x_j) \) 값을 사용할 수 있습니다. 매핑 된 데이터 포인트에 대한 알고리즘 \ (\ phi (x_i) \). \ (k \)를 사용하면 이점은 명시 적으로 계산할 필요가 없으며 임의의 큰 기능 (무한까지도)을 허용한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="9fea95f95d0577b3d2b8dde1c99a4f5f11240b1d" translate="yes" xml:space="preserve">
          <source>If an exception is triggered, use &lt;code&gt;%debug&lt;/code&gt; to fire-up a post mortem ipdb session.</source>
          <target state="translated">예외가 발생하면 &lt;code&gt;%debug&lt;/code&gt; 를 사용 하여 사후 iptem 세션을 시작하십시오.</target>
        </trans-unit>
        <trans-unit id="5c4a826b768bf0ea44b0de0cf9a279c59410da1d" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.</source>
          <target state="translated">정수가 제공되면 사용되는 알파 격자의 점 수를 수정합니다. 목록이 제공되면 사용할 그리드를 제공합니다. 자세한 내용은 docstring 클래스의 메모를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3f54c86c80b29ba93fdb4405121f2a742f42412e" translate="yes" xml:space="preserve">
          <source>If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</source>
          <target state="translated">ndarray가 전달되면 모양 (n_clusters, n_features)이어야하고 초기 중심을 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="7bf20b6ab9e24e0313be1f29e5bd87350c62fc72" translate="yes" xml:space="preserve">
          <source>If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently.</source>
          <target state="translated">대역폭이 제공되지 않으면 모든 쌍별 거리의 중앙값을 기반으로 휴리스틱을 사용하여 결정됩니다. 샘플 수에 이차 시간이 걸립니다. sklearn.cluster.estimate_bandwidth 함수를 사용하여보다 효율적으로 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="307d133eac653119e68c3f800803dcc4776573b9" translate="yes" xml:space="preserve">
          <source>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If &amp;lsquo;auto&amp;rsquo;, it is assigned to False for dense &lt;code&gt;X&lt;/code&gt; and to True for sparse &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">부울 인 경우 모든 기능을 이산 또는 연속으로 고려할지 여부를 결정합니다. 배열 인 경우 모양이있는 부울 마스크 (n_features)이거나 개별 기능의 인덱스가있는 배열이어야합니다. 'auto'인 경우 밀도가 높은 &lt;code&gt;X&lt;/code&gt; 의 경우 False , 희소 한 &lt;code&gt;X&lt;/code&gt; 의 경우 True로 지정됩니다 .</target>
        </trans-unit>
        <trans-unit id="14d26c2cb6ccf4f84a440b5eee94360749d30490" translate="yes" xml:space="preserve">
          <source>If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.</source>
          <target state="translated">부울 인 경우, 증가 또는 감소와 함께 등장 성 회귀에 적합할지 여부.</target>
        </trans-unit>
        <trans-unit id="c7380002883f78b7443a4cf144369e2cdf9c7dd5" translate="yes" xml:space="preserve">
          <source>If bytes or files are given to analyze, this encoding is used to decode.</source>
          <target state="translated">바이트 또는 파일을 분석하기 위해 제공 한 경우이 인코딩을 사용하여 디코딩합니다.</target>
        </trans-unit>
        <trans-unit id="b1cd46fc8b5b18d3d8ec258c92a5832fe49ecb69" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 AMI가 null입니다.</target>
        </trans-unit>
        <trans-unit id="e6f2dbc2c288fdff952bc5d6d0612fad044f50c2" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 NMI가 널입니다.</target>
        </trans-unit>
        <trans-unit id="60b93fba5d2befe30dad173ef2989a32caf2707d" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 불완전하므로 ARI가 매우 낮습니다.</target>
        </trans-unit>
        <trans-unit id="e02bb35b2a969fdf2ff25b865a0b3ba938fb92a9" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당이 완전히 완료되지 않으므로 V-Measure가 null입니다.</target>
        </trans-unit>
        <trans-unit id="d0974a75f074fb11fd0a08f495fdb803227dd0c6" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 완전히 분할 된 경우 할당은 완전히 임의이므로 FMI는 null입니다.</target>
        </trans-unit>
        <trans-unit id="3a4c44f6cadbe5141305e79bc3f8068062652c4d" translate="yes" xml:space="preserve">
          <source>If classes members are split across different clusters, the assignment cannot be complete:</source>
          <target state="translated">클래스 멤버가 다른 클러스터로 분할 된 경우 할당을 완료 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="baf96abe9df5cd386826eafcd47454c9dcc36819" translate="yes" xml:space="preserve">
          <source>If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency</source>
          <target state="translated">copy가 False 인 경우 메모리 효율성을 위해 선호도 매트릭스가 알고리즘에 의해 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="671e4e16873255449d2ba54f06975c272f73c34d" translate="yes" xml:space="preserve">
          <source>If density = &amp;lsquo;auto&amp;rsquo;, the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).</source>
          <target state="translated">밀도 = '자동'인 경우 Ping Li 등의 권장에 따라 값이 최소 밀도로 설정됩니다. 1 / sqrt (n_features).</target>
        </trans-unit>
        <trans-unit id="391517cb3cfce3ac9c6c32b7ceac049807282afc" translate="yes" xml:space="preserve">
          <source>If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass &lt;code&gt;analyzer=str.split&lt;/code&gt;</source>
          <target state="translated">문서가 외부 패키지에 의해 사전 토큰 화 된 경우 토큰을 공백으로 구분하여 파일 (또는 문자열)에 저장하고 &lt;code&gt;analyzer=str.split&lt;/code&gt; 을 전달 하십시오.</target>
        </trans-unit>
        <trans-unit id="80416b24b5f24b22b80d50d90aa942e77a2dadfc" translate="yes" xml:space="preserve">
          <source>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</source>
          <target state="translated">각 행과 각 열이 정확히 하나의 bicluster에 속하는 경우 데이터 행렬의 행과 열을 다시 정렬하면 대각선에 biclusters가 나타납니다. 여기에 구조화 기호가 다른 행과 열보다 높은 평균값을 갖는이 구조의 예가 있습니다.</target>
        </trans-unit>
        <trans-unit id="1db9c10662b6d49b6b84ca8b7b7ce2a9580afa10" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the parameter is unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로 (매개 변수는 지정되지 않음) 값은 0.1로 설정됩니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.1로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="73ce93bb920d84bef49715afdf02f771938f8206" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.1로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.1로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="aa74ab3ce21513c4e7c83e9b91dad63582c835b8" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.2로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.2로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="dfe0bc4ba0825c6b9e54b3177711e714d6e28a2b" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 테스트 스플릿에 포함 할 데이터 집합의 비율을 나타냅니다. int이면 테스트 샘플의 절대 수를 나타냅니다. None 인 경우, 값은 열차 크기의 보수로 설정됩니다. 기본적으로이 값은 0.25로 설정되어 있습니다. 기본값은 버전 0.21에서 변경됩니다. &lt;code&gt;train_size&lt;/code&gt; 가 지정되지 않은 경우에만 0.25로 유지되며 , 그렇지 않으면 지정된 &lt;code&gt;train_size&lt;/code&gt; 를 보완합니다 .</target>
        </trans-unit>
        <trans-unit id="a3b7da8f21403a8e0fce55a369b8d82b4da5bfd1" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">float 인 경우 0.0과 1.0 사이 여야하며 열차 분할에 포함 할 데이터 세트의 비율을 나타냅니다. int이면 열차 샘플의 절대 수를 나타냅니다. None이면 값이 테스트 크기의 보수로 자동 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="62b47f7a89d7c976d2813299381cdc4f4f5b3cad" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">플로트 인 경우 0.0과 1.0 사이 여야하며 열차 분할에 포함 할 그룹의 비율을 나타냅니다. int이면 열차 그룹의 절대 수를 나타냅니다. None이면 값이 테스트 크기의 보수로 자동 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="b0ffbda1c59db43809cf9245f33efa45a65a5c05" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;max_features&lt;/code&gt; is a fraction and &lt;code&gt;int(max_features * n_features)&lt;/code&gt; features are considered at each split.</source>
          <target state="translated">float이면 &lt;code&gt;max_features&lt;/code&gt; 는 분수이며 &lt;code&gt;int(max_features * n_features)&lt;/code&gt; 기능은 각 분할에서 고려됩니다.</target>
        </trans-unit>
        <trans-unit id="47d0c1ebc9dc44a7018a0ce88d0d45201068f385" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; are the minimum number of samples for each node.</source>
          <target state="translated">float 인 경우 &lt;code&gt;min_samples_leaf&lt;/code&gt; 는 분수이고 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 은 각 노드의 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="c217707834c84f95b745c6fd735e46ef1d5cb29d" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; is the minimum number of samples for each node.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_leaf&lt;/code&gt; 는 분수이고 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 은 각 노드의 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="f5813e9c1656f619ed6234eb86815e1c76ec9071" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; are the minimum number of samples for each split.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_split&lt;/code&gt; 은 분수이고 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 은 각 분할에 대한 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="c32957ea85344bbb6b41aa874b4a5e7763ff6b76" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; is the minimum number of samples for each split.</source>
          <target state="translated">float이면 &lt;code&gt;min_samples_split&lt;/code&gt; 은 분수이고 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 은 각 분할에 대한 최소 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="81fe24c94c96599e85080c0cc195542bdb1ce722" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; features.</source>
          <target state="translated">float이면 &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; 기능을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="a2b1676fcae8577852e20614ce418906e2f79102" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples.</source>
          <target state="translated">float이면 &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; 샘플을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="f234188a9694365b66e83538b40de1fa4059a074" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration.</source>
          <target state="translated">1보다 크거나 같은 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 (정수) 기능 수에 해당합니다. (0.0, 1.0) 이내 인 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 기능의 백분율 (반올림)에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="d3aeb6a39c457b69bdde12a943780461d08c388d" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than &lt;code&gt;step&lt;/code&gt; features in order to reach &lt;code&gt;min_features_to_select&lt;/code&gt;.</source>
          <target state="translated">1보다 크거나 같은 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 (정수) 기능 수에 해당합니다. (0.0, 1.0) 이내 인 경우 &lt;code&gt;step&lt;/code&gt; 는 각 반복에서 제거 할 기능의 백분율 (반올림)에 해당합니다. &lt;code&gt;min_features_to_select&lt;/code&gt; 에 도달하기 위해 마지막 반복 에서 &lt;code&gt;step&lt;/code&gt; 기능 보다 적은 수가 제거 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1351e34960b08f78869e356e9d9129a529a17168" translate="yes" xml:space="preserve">
          <source>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">QDA 모델에서 공분산 행렬이 대각선이라고 가정하면 입력이 각 클래스에서 조건부로 독립적이라고 가정하고 결과 분류기는 가우스 Naive Bayes 분류기 &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt; 와 같습니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="47884bf7f490577d7025ceb970813cb997acfc82" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution.</source>
          <target state="translated">init = 'custom'이면 솔루션의 초기 추측으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="5b9c788e52ff7adb618d2b8cf3dd396e088800c8" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points equally divided among clusters. If array-like, each element of the sequence indicates the number of samples per cluster.</source>
          <target state="translated">int이면 클러스터간에 똑같이 나누어 진 총 포인트 수입니다. 배열과 같은 경우 시퀀스의 각 요소는 클러스터 당 샘플 수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="2403d0bd3d2ac8c8a9756c4cde9cd9202cbe9926" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="676c2454734bf9216c5797d643595f0b850b4e7a" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Note that different initializations might result in different local minima of the cost function.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . 초기화가 다를 경우 비용 함수의 로컬 최소값이 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a1933900181bd8e24f0237ebecc7a06b2ef8b486" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Only used when &lt;code&gt;svd_method&lt;/code&gt; equals &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;svd_method&lt;/code&gt; 가 'randomized'와 같은 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="4914440c8828885c0b1efea7679daefd5f1e4eaf" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;eigen_solver&lt;/code&gt; == 'arpack'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="90657492e3c46d11fb3a1c799a4569e4854211ed" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; == True.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;shuffle&lt;/code&gt; == True 일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="7e39c8ed74a38762200c178da772671eaa6b3f52" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; is True.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;shuffle&lt;/code&gt; 이 참일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="636d95a8f3edcd08bb7a122de05f8944c1a330ee" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'arpack'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="d8bb54edf7a318cb4f3734b3f7721b6c840db8b1" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;svd_solver&lt;/code&gt; == 'arpack'또는 'randomized'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="49fc99a0f8ec79ab5cf6633c8b91ad397447b0ae" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.</source>
          <target state="translated">int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. None이면 난수 생성기는 np.random에서 사용하는 RandomState 인스턴스입니다. 이는 서브 샘플링 및 평활 노이즈에서 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c8faba5e55a8f0e899120109354364cac8c2354b" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;max_features&lt;/code&gt; features at each split.</source>
          <target state="translated">int이면 각 분할에서 &lt;code&gt;max_features&lt;/code&gt; 기능 을 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="79438cfe8b8de1684467307814da6af61cdfe6ba" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_leaf&lt;/code&gt; as the minimum number.</source>
          <target state="translated">int이면 &lt;code&gt;min_samples_leaf&lt;/code&gt; 를 최소 숫자로 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="69e04ca78560d3ef445be4d724f5c0cc8198187a" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_split&lt;/code&gt; as the minimum number.</source>
          <target state="translated">int이면 &lt;code&gt;min_samples_split&lt;/code&gt; 을 최소값으로 고려 하십시오 .</target>
        </trans-unit>
        <trans-unit id="a8d276c242fbe315ce14903af35e7ebf9a0c3619" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_features&lt;/code&gt; features.</source>
          <target state="translated">int이면 &lt;code&gt;max_features&lt;/code&gt; 기능을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="0771ca4ef29dd427aac0ffda56943aa541e3af54" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_samples&lt;/code&gt; samples.</source>
          <target state="translated">int이면 &lt;code&gt;max_samples&lt;/code&gt; 샘플을 그립니다 .</target>
        </trans-unit>
        <trans-unit id="4430c154e22158c0d6435f75a2d640312ee73ab2" translate="yes" xml:space="preserve">
          <source>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the &amp;ldquo;first&amp;rdquo; singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.</source>
          <target state="translated">로그 정규화가 사용 된 경우 모든 특이 벡터가 의미가 있습니다. 그러나 독립적 인 정규화 또는 비 스트로크 화가 사용 된 경우 첫 번째 특이 벡터 \ (u_1 \) 및 \ (v_1 \). 폐기됩니다. 이제부터 &quot;첫 번째&quot;특이 벡터는 로그 정규화를 제외하고 \ (u_2 \ dots u_ {p + 1} \) 및 \ (v_2 \ dots v_ {p + 1} \)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f38434d38fce86523bd80aac7625c65019a5f868" translate="yes" xml:space="preserve">
          <source>If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).</source>
          <target state="translated">max_samples가 제공된 샘플 수보다 크면 모든 샘플이 모든 트리에 사용됩니다 (샘플링 없음).</target>
        </trans-unit>
        <trans-unit id="0512919782ceb898f5e82137605d37f1918f7fe8" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;auto&amp;rdquo;, the ratio of n_samples / n_population is used to determine which algorithm to use: If ratio is between 0 and 0.01, tracking selection is used. If ratio is between 0.01 and 0.99, numpy.random.permutation is used. If ratio is greater than 0.99, reservoir sampling is used. The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">method == &quot;auto&quot;인 경우 n_samples / n_population의 비율을 사용하여 사용할 알고리즘을 결정합니다. 비율이 0과 0.01 사이 인 경우 추적 선택이 사용됩니다. 비율이 0.01과 0.99 사이이면 numpy.random.permutation이 사용됩니다. 비율이 0.99보다 큰 경우 저수지 샘플링이 사용됩니다. 선택한 정수의 순서는 정의되어 있지 않습니다. 무작위 순서가 필요한 경우 선택한 하위 집합을 섞어 야합니다.</target>
        </trans-unit>
        <trans-unit id="aa3ae5990e2e00fef99c00cc07049cdbc9d8e931" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;pool&amp;rdquo;, a pool based algorithm is particularly fast, even faster than the tracking selection method. Hovewer, a vector containing the entire population has to be initialized. If n_samples ~ n_population, the reservoir sampling method is faster.</source>
          <target state="translated">method == &quot;pool&quot;인 경우 풀 기반 알고리즘이 추적 선택 방법보다 훨씬 빠릅니다. Hovewer, 전체 모집단을 포함하는 벡터를 초기화해야합니다. n_samples ~ n_population이면 저수지 샘플링 방법이 더 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="6916dcd6d6c8f00e1ac0ef865ab4c75ff38ebedf" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;reservoir_sampling&amp;rdquo;, a reservoir sampling algorithm is used which is suitable for high memory constraint or when O(&lt;code&gt;n_samples&lt;/code&gt;) ~ O(&lt;code&gt;n_population&lt;/code&gt;). The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">method ==&amp;ldquo;reservoir_sampling&amp;rdquo;인 경우 높은 메모리 제약 조건이나 O ( &lt;code&gt;n_samples&lt;/code&gt; ) ~ O ( &lt;code&gt;n_population&lt;/code&gt; )에 적합한 저장소 샘플링 알고리즘이 사용됩니다 . 선택한 정수의 순서는 정의되어 있지 않습니다. 무작위 순서가 필요한 경우 선택한 하위 집합을 섞어 야합니다.</target>
        </trans-unit>
        <trans-unit id="0dea8a6c91cef90e0430014d895bb3954c8fb19c" translate="yes" xml:space="preserve">
          <source>If method ==&amp;rdquo;tracking_selection&amp;rdquo;, a set based implementation is used which is suitable for &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt;.</source>
          <target state="translated">method ==&amp;rdquo;tracking_selection&amp;rdquo;이면 &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt; 에 적합한 세트 기반 구현이 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="79ae0bba548597b9902c41c70fbd6bf9602e8534" translate="yes" xml:space="preserve">
          <source>If metric is &amp;lsquo;precomputed&amp;rsquo;, Y is ignored and X is returned.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 Y는 무시되고 X가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="ca8cb47e72e166fd730a116349e050254e6876d5" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string.</source>
          <target state="translated">메트릭이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 두 개의 배열을 입력으로 받아서 이들 사이의 거리를 나타내는 하나의 값을 반환해야합니다. 이는 Scipy의 메트릭에 적용되지만 메트릭 이름을 문자열로 전달하는 것보다 덜 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="27305db22802f1bb3d3e2d60db076f6f0d275369" translate="yes" xml:space="preserve">
          <source>If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.</source>
          <target state="translated">미니 배치 k- 평균을 사용하는 경우 최상의 초기화가 선택되고 알고리즘이 한 번 실행됩니다. 그렇지 않으면 각 초기화 및 선택한 최상의 솔루션에 대해 알고리즘이 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="16e6684b95c26e373af21b6c0d2ea50b705c0505" translate="yes" xml:space="preserve">
          <source>If multioutput is &amp;lsquo;raw_values&amp;rsquo;, then mean absolute error is returned for each output separately. If multioutput is &amp;lsquo;uniform_average&amp;rsquo; or an ndarray of weights, then the weighted average of all output errors is returned.</source>
          <target state="translated">다중 출력이 'raw_values'인 경우 각 출력에 대해 개별적으로 절대 오류가 반환됩니다. 다중 출력이 'uniform_average'또는 가중치의 ndarray이면 모든 출력 오류의 가중 평균이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="caf3d42023b133b9efdfbb493fc66df503092e71" translate="yes" xml:space="preserve">
          <source>If no scoring is specified and the estimator has no score function, we can either return None or raise an exception.</source>
          <target state="translated">스코어링이 지정되지 않고 추정기에 스코어 함수가없는 경우 None을 리턴하거나 예외를 발생시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5519c1c6825bf59bfd06c08f68bb64b64ee1a0ae" translate="yes" xml:space="preserve">
          <source>If no valid consensus set could be found. This occurs if &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; return False for all &lt;code&gt;max_trials&lt;/code&gt; randomly chosen sub-samples.</source>
          <target state="translated">유효한 합의 세트를 찾을 수없는 경우 이 경우 발생 &lt;code&gt;is_data_valid&lt;/code&gt; 및 &lt;code&gt;is_model_valid&lt;/code&gt; 모든 반환 거짓 &lt;code&gt;max_trials&lt;/code&gt; 무작위로 하위 샘플을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="e48a960f323664c14eed43108cfafa1159796090" translate="yes" xml:space="preserve">
          <source>If normalize is &lt;code&gt;True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.</source>
          <target state="translated">normalize가 &lt;code&gt;True&lt;/code&gt; 인 경우 오 분류의 소수 (float)를 반환하고 그렇지 않으면 오 분류 수 (int)를 반환합니다. 최상의 성능은 0입니다.</target>
        </trans-unit>
        <trans-unit id="d93355ca0c397a92c0eb63483bbae0b531d00cf1" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; over the range [0, max_fpr] is returned.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; 이 아닌 경우 [0, max_fpr] 범위 에서 표준화 된 부분 AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3&lt;/a&gt; ]이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="954d968337062d6fae676f5915fb0dc48db9ccef" translate="yes" xml:space="preserve">
          <source>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</source>
          <target state="translated">None이 아니라면, 말뭉치의 용어 빈도로 정렬 된 최고 max_features 만 고려하는 어휘를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="6b6dff5f6d294c2bdbfe5ee6b0ee56319193880c" translate="yes" xml:space="preserve">
          <source>If not None, data is split in a stratified fashion, using this as the class labels.</source>
          <target state="translated">None이 아닌 경우 데이터를 클래스 레이블로 사용하여 계층화 된 방식으로 데이터가 분할됩니다.</target>
        </trans-unit>
        <trans-unit id="d9fe4271c08ca870db7143f08e0938aa49f2d1d0" translate="yes" xml:space="preserve">
          <source>If not None, set the highest value of the fit to y_max.</source>
          <target state="translated">None이 아니라면 가장 큰 적합치를 y_max로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="3c138b5d1ed12eddb3226ed7535814059b7a615c" translate="yes" xml:space="preserve">
          <source>If not None, set the lowest value of the fit to y_min.</source>
          <target state="translated">None이 아니라면 가장 작은 적합치를 y_min으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="ebcf44116da09ed76a723aed5cadbe6d4ed2530d" translate="yes" xml:space="preserve">
          <source>If not None, this argument is passed as &lt;code&gt;sample_weight&lt;/code&gt; keyword argument to the &lt;code&gt;score&lt;/code&gt; method of the final estimator.</source>
          <target state="translated">None이 아닌 경우,이 인수는 &lt;code&gt;sample_weight&lt;/code&gt; 키워드 인수로 최종 추정기 의 &lt;code&gt;score&lt;/code&gt; 메소드에 전달됩니다 .</target>
        </trans-unit>
        <trans-unit id="f0f7d0b7263b16cf926e8314af8096b9ae6c9066" translate="yes" xml:space="preserve">
          <source>If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).</source>
          <target state="translated">지정하지 않으면 대역폭은 sklearn.cluster.estimate_bandwidth; 확장성에 대한 힌트는 해당 기능의 설명서를 참조하십시오 (아래 참고 사항 참조).</target>
        </trans-unit>
        <trans-unit id="3fc57ade66d3b29b2e5dacfcee394aac7f4ec951" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;. .. versionadded:: 0.18</source>
          <target state="translated">제공하지 않으면 레이블은 y_true에서 유추됩니다. 경우 &lt;code&gt;labels&lt;/code&gt; 없는 &lt;code&gt;None&lt;/code&gt; 과 &lt;code&gt;y_pred&lt;/code&gt; 형상 (N_SAMPLES은)을 가지는 라벨 진 것으로 가정으로부터 추론 &lt;code&gt;y_true&lt;/code&gt; . .. 버전 추가 :: 0.18</target>
        </trans-unit>
        <trans-unit id="d3a1f4e96f04c6f8dfd4835d50de53587903b2e7" translate="yes" xml:space="preserve">
          <source>If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones.</source>
          <target state="translated">K-one 코딩이 범주 형 피처에 적용되는 경우 생성 된 피처 이름은 포함되지만 원래 피처 이름은 포함되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c3b8faf61102e14148418b48bf3dbb3389d54ef3" translate="yes" xml:space="preserve">
          <source>If only the diagonal of the auto-covariance is being used, the method &lt;code&gt;diag()&lt;/code&gt; of a kernel can be called, which is more computationally efficient than the equivalent call to &lt;code&gt;__call__&lt;/code&gt;: &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</source>
          <target state="translated">자동 공분산의 대각선 만 사용 하는 경우 커널의 &lt;code&gt;diag()&lt;/code&gt; 메서드를 호출 할 수 있으며 &lt;code&gt;__call__&lt;/code&gt; 에 대한 동등한 호출보다 계산 효율이 높습니다. . &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="db7f35e5fc1dd73c10c86bdccb4a2449d5a89ec7" translate="yes" xml:space="preserve">
          <source>If order is &amp;lsquo;random&amp;rsquo; a random ordering will be used.</source>
          <target state="translated">주문이 '무작위'인 경우 무작위 순서가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f42275492b00fc14b5861ea85e0f4944992d0324" translate="yes" xml:space="preserve">
          <source>If passed, include the name of the estimator in warning messages.</source>
          <target state="translated">전달 된 경우 경고 메시지에 추정기 이름을 포함 시키십시오.</target>
        </trans-unit>
        <trans-unit id="908cd551a5eab6201799122746b2ad3d99f4a3d2" translate="yes" xml:space="preserve">
          <source>If positive, restrict regression coefficients to be positive</source>
          <target state="translated">양수이면 회귀 계수를 양수로 제한하십시오.</target>
        </trans-unit>
        <trans-unit id="c5484f943f94af044829e2453a87ea1beff675d6" translate="yes" xml:space="preserve">
          <source>If return_costs is True, the objective function and dual gap at each iteration are returned.</source>
          <target state="translated">return_costs가 True이면 각 반복에서 목적 함수와 이중 간격이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="3a07f641c209d2556442bcd652915ffb4ab857db" translate="yes" xml:space="preserve">
          <source>If safe is false, clone will fall back to a deep copy on objects that are not estimators.</source>
          <target state="translated">safe가 false이면 clone은 추정값이 아닌 객체의 깊은 복사본으로 폴백됩니다.</target>
        </trans-unit>
        <trans-unit id="179d83839b7c246b21dd4fad6260ec3c338cc783" translate="yes" xml:space="preserve">
          <source>If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError.</source>
          <target state="translated">seed가 None이면 np.random에서 사용하는 RandomState 싱글 톤을 반환합니다. seed가 int 인 경우 seed로 seed 된 새 RandomState 인스턴스를 리턴하십시오. seed가 이미 RandomState 인스턴스 인 경우이를 반환하십시오. 그렇지 않으면 ValueError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="7108bbb3c9ecad70c2ad038e49ece7ce906a1c8f" translate="yes" xml:space="preserve">
          <source>If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be &amp;lt; len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.</source>
          <target state="translated">seq [i]가 int이거나 하나의 int 값을 가진 튜플 인 경우 단방향 PDP가 작성됩니다. seq [i]가 두 정수의 튜플이면 양방향 PDP가 생성됩니다. feature_names가 지정되고 seq [i]가 int 인 경우 seq [i]는 &amp;lt;len (feature_names)이어야합니다. seq [i]가 문자열이면 feature_names를 지정하고 seq [i]는 feature_names에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="b01ea458e6ed79ec076f21dd79564eecc3f7a882" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4</source>
          <target state="translated">'random'으로 설정하면 랜덤 계수가 기본적으로 기능을 순차적으로 반복하지 않고 모든 반복마다 업데이트됩니다. 이것은 ( '무작위'로 설정) 특히 tol이 1e-4보다 높을 때 수렴 속도가 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="7a4374896942a67a58d05d59133607fc7483d7a2" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4.</source>
          <target state="translated">'random'으로 설정하면 랜덤 계수가 기본적으로 기능을 순차적으로 반복하지 않고 모든 반복마다 업데이트됩니다. 이것은 ( '무작위'로 설정) 특히 tol이 1e-4보다 높을 때 수렴 속도가 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="d15f7a009cd3b6e81a757534913f0edc7a2b7947" translate="yes" xml:space="preserve">
          <source>If set to True, forces coefficients to be positive. (Only allowed when &lt;code&gt;y.ndim == 1&lt;/code&gt;).</source>
          <target state="translated">True로 설정하면 계수가 양수가됩니다. (만 &lt;code&gt;y.ndim == 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6bd31584b0a279bb6a357ab195ba9534cb5cad4e" translate="yes" xml:space="preserve">
          <source>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</source>
          <target state="translated">True로 설정하면 점수는 모든 접기에서 평균화되며 최고 점수에 해당하는 계수와 C가 취해지고이 매개 변수를 사용하여 최종 개조가 수행됩니다. 그렇지 않으면 폴드에서 최고 점수에 해당하는 계수, 절편 및 C가 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="f3d43f9f7c9e3af1ca6eddc0268b8ee91bb07ee3" translate="yes" xml:space="preserve">
          <source>If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.</source>
          <target state="translated">설정되면 scikit-learn은 임시 배열의 크기를이 MiB 수 (병렬화 작업 당)로 제한하려고 시도하여 종종 청크 단위로 수행 할 수있는 값 비싼 작업에서 계산 시간과 메모리를 절약합니다. 전역 기본값 : 1024</target>
        </trans-unit>
        <trans-unit id="cd9d66e1ab8be1fe689482ddb0cbca43b44a3950" translate="yes" xml:space="preserve">
          <source>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</source>
          <target state="translated">엄격하게 양수이면 파일의 위치가 (오프셋 + 길이) 바이트 임계 값에 도달하면 새로운 데이터 행을 읽지 마십시오.</target>
        </trans-unit>
        <trans-unit id="af99c20b0f1015ebcecc8bfb6a50ca848ab08d15" translate="yes" xml:space="preserve">
          <source>If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.</source>
          <target state="translated">문자열 인 경우 데이터를 포함 할 경로를 지정합니다. 파일과 같은 경우 데이터가 f에 기록됩니다. f는 이진 모드로 열어야합니다.</target>
        </trans-unit>
        <trans-unit id="d25cbbfb18995acebbdc8e788e3994e16b21b8ae" translate="yes" xml:space="preserve">
          <source>If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.</source>
          <target state="translated">sum_over_features가 False 인 모양이 (n_samples_X * n_samples_Y, n_features)이고 D가 성분 별 L1 쌍별 거리 (즉, 절대 차이)를 포함하는 경우 모양은 (n_samples_X, n_samples_Y)이고 D는 쌍별 L1 거리를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="1979731cc29c616c5ac5593ab888192599b2d46b" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;loss&lt;/code&gt; does not support probabilities.</source>
          <target state="translated">&lt;code&gt;loss&lt;/code&gt; 이 확률을 지원하지 않는 경우 .</target>
        </trans-unit>
        <trans-unit id="41f96f9118cd39448d94e68aca8aa6d327b368ef" translate="yes" xml:space="preserve">
          <source>If the algorithm is &amp;ldquo;deflation&amp;rdquo;, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.</source>
          <target state="translated">알고리즘이 &quot;deflation&quot;인 경우 n_iter는 모든 구성 요소에서 실행되는 최대 반복 횟수입니다. 그렇지 않으면 그것들은 수렴하기 위해 반복되는 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="b72ab6a8a780a6da86f798b7381c54dc2236b80c" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; of &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;means_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;means_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="translated">알고리즘은 이전에 완전히 (때문에의 수렴을 중지하는 경우 &lt;code&gt;tol&lt;/code&gt; 의 &lt;code&gt;max_iter&lt;/code&gt; ), &lt;code&gt;labels_&lt;/code&gt; 및 &lt;code&gt;means_&lt;/code&gt; 가 일치하지 않을 것, 즉 &lt;code&gt;means_&lt;/code&gt; 는 각 클러스터에있는 점의 수단이 될 수 없습니다. 또한 추정기는 마지막 반복 이후에 &lt;code&gt;labels_&lt;/code&gt; 을 재 할당 하여 &lt;code&gt;labels_&lt;/code&gt; 훈련 세트에 대한 &lt;code&gt;predict&lt;/code&gt; 과 일관성있게 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="4043c787702cc081c41f25b031d69ea98cf35c42" translate="yes" xml:space="preserve">
          <source>If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.</source>
          <target state="translated">배열이 대칭이 아닌 경우 대칭 버전이 반환됩니다. 선택적으로, 행렬이 대칭이 아닌 경우 경고 또는 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d4238935d45ce51eea0be6c47146a609e05c26ed" translate="yes" xml:space="preserve">
          <source>If the attributes are not found.</source>
          <target state="translated">속성을 찾을 수없는 경우</target>
        </trans-unit>
        <trans-unit id="10f86bc0a8ef8d94dd88200305e21d6ac290743f" translate="yes" xml:space="preserve">
          <source>If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).</source>
          <target state="translated">분류 기가 두 클래스에서 모두 동일하게 잘 수행되면이 용어는 일반적인 정확도 (즉, 정확한 예측 수를 총 예측 수로 나눈 값)로 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="9d0651dbf433477af9dfe8c9482b03c0b28a7aea" translate="yes" xml:space="preserve">
          <source>If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.</source>
          <target state="translated">데이터 순서가 임의적이지 않은 경우 (예 : 동일한 클래스 레이블을 가진 샘플이 연속적 임) 의미있는 교차 검증 결과를 얻으려면 먼저 순서를 섞어 야합니다. 그러나 샘플이 독립적으로 동일하게 분포되어 있지 않은 경우에는 그 반대 일 수 있습니다. 예를 들어, 샘플이 뉴스 기사에 해당하고 출판 시점에 따라 주문 된 경우 데이터를 섞으면 과적 합 모델과 검증 점수가 비정상적으로 증가 할 수 있습니다. 인공적으로 유사한 샘플에서 테스트됩니다 (닫기 훈련 시간).</target>
        </trans-unit>
        <trans-unit id="4fdc5debb409dcec7a673c288152f0ef6e4738ef" translate="yes" xml:space="preserve">
          <source>If the default value is passed, then &lt;code&gt;keepdims&lt;/code&gt; will not be passed through to the &lt;code&gt;mean&lt;/code&gt; method of sub-classes of &lt;code&gt;ndarray&lt;/code&gt;, however any non-default value will be. If the sub-class&amp;rsquo; method does not implement &lt;code&gt;keepdims&lt;/code&gt; any exceptions will be raised.</source>
          <target state="translated">기본 값이 전달되면, &lt;code&gt;keepdims&lt;/code&gt; 는 받는 사람을 통해 전달되지 않습니다 &lt;code&gt;mean&lt;/code&gt; 의 하위 클래스의 방법 &lt;code&gt;ndarray&lt;/code&gt; 그러나이 아닌 디폴트 값은 될 것이다. 하위 클래스의 메소드가 &lt;code&gt;keepdims&lt;/code&gt; 를 구현하지 않는 경우 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="ca5777d1057fb92ff835301c12f93dc71bd51069" translate="yes" xml:space="preserve">
          <source>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</source>
          <target state="translated">현재 예측과 올바른 레이블의 차이가이 임계 값보다 낮 으면 모델이 업데이트되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="54f187a0c12dbeb2b22455f8308653334a568512" translate="yes" xml:space="preserve">
          <source>If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.</source>
          <target state="translated">추정기가 증분 학습을 지원하는 경우 다른 학습 세트 크기에 맞게 속도를 높이는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="28446974a089033b0f005a14dd2e5e7cde0d019d" translate="yes" xml:space="preserve">
          <source>If the file does not exist yet, it is downloaded from mldata.org .</source>
          <target state="translated">파일이 아직 없으면 mldata.org에서 다운로드됩니다.</target>
        </trans-unit>
        <trans-unit id="3377386ec971b5f97505ad0b0efacd641307a6b2" translate="yes" xml:space="preserve">
          <source>If the folder does not already exist, it is automatically created.</source>
          <target state="translated">폴더가 없으면 자동으로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="bcf86cd76452a354a39a384d6cc008f0521fadc0" translate="yes" xml:space="preserve">
          <source>If the gradient norm is below this threshold, the optimization will be stopped.</source>
          <target state="translated">그래디언트 표준이이 임계 값보다 낮 으면 최적화가 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="aaea0ac91de1101ebb5583d72a39edadc546a9ed" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt;&lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt;&lt;/a&gt;) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:</source>
          <target state="translated">기본 진리 레이블을 모르는 경우 모델 자체를 사용하여 평가를 수행해야합니다. Silhouette Coefficient ( &lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt; &lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt; &lt;/a&gt; )는 이러한 평가의 예이며, Silhouette Coefficient 점수가 높을수록 클러스터가 더 잘 정의 된 모델과 관련됩니다. Silhouette Coefficient는 각 샘플에 대해 정의되며 두 개의 점수로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="bb3f5944370bdcf362ff4bffb46e8bf1ded41ef1" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabaz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.</source>
          <target state="translated">기본 진리 레이블을 알 수없는 경우 분산 비율 기준이라고도 하는 Calinski-Harabaz 지수 ( &lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt; &lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt; &lt;/a&gt; )를 사용하여 모델을 평가할 수 있습니다. 여기서 Calinski-Harabaz 점수가 높은 모델은 더 잘 정의 된 클러스터.</target>
        </trans-unit>
        <trans-unit id="a4a519d35f95c18e319df7e8878b98f013f9bd44" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Davies-Bouldin index (&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt;&lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt;&lt;/a&gt;) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.</source>
          <target state="translated">기본 진리 레이블을 알 수없는 경우 Davies-Bouldin 지수 ( &lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt; &lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt; &lt;/a&gt; )를 사용하여 모형을 평가할 수 있습니다. 여기서 Davies-Bouldin 지수가 낮을수록 군집간에 더 잘 분리 된 모형과 관련됩니다.</target>
        </trans-unit>
        <trans-unit id="7ce6861d9ec6948a6bc8aef858e97abae7ed0654" translate="yes" xml:space="preserve">
          <source>If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.</source>
          <target state="translated">입력이 희소 행렬 인 경우 Binarizer 클래스는 0이 아닌 값만 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="c10a8d9d8c40f9f50dafe36b727f5b47e7075f19" translate="yes" xml:space="preserve">
          <source>If the input matrix X is very sparse, it is recommended to convert to sparse &lt;code&gt;csc_matrix&lt;/code&gt; before calling fit and sparse &lt;code&gt;csr_matrix&lt;/code&gt; before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.</source>
          <target state="translated">입력 행렬 X가 매우 드문 경우 fit 및 sparse &lt;code&gt;csr_matrix&lt;/code&gt; 를 호출하기 전에 스파 스 &lt;code&gt;csc_matrix&lt;/code&gt; 로 변환하는 것이 좋습니다. 예측 호출하기 전에. 피처가 대부분의 샘플에서 값이 0 인 경우 밀도가 높은 매트릭스에 비해 희소 매트릭스 입력에 대해 훈련 시간이 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="661cb29a3f5fe68fda8ea5f8c53273efb7753ce1" translate="yes" xml:space="preserve">
          <source>If the labels are encoded with +1 and -1, \(y\): is the true value, and \(w\) is the predicted decisions as output by &lt;code&gt;decision_function&lt;/code&gt;, then the hinge loss is defined as:</source>
          <target state="translated">레이블이 +1 및 -1로 인코딩 된 경우 \ (y \) :는 true 값이고 \ (w \)는 decision_function에 의해 출력 된 예측 &lt;code&gt;decision_function&lt;/code&gt; 후 힌지 손실은 다음과 같이 정의된다 :</target>
        </trans-unit>
        <trans-unit id="30e7605353fedb22ff0c25c7418abbab28137e70" translate="yes" xml:space="preserve">
          <source>If the loss on a sample is greater than the &lt;code&gt;residual_threshold&lt;/code&gt;, then this sample is classified as an outlier.</source>
          <target state="translated">샘플의 손실이보다 큰 경우 &lt;code&gt;residual_threshold&lt;/code&gt; ,이 샘플은 이상치 (outlier)로 분류된다.</target>
        </trans-unit>
        <trans-unit id="fd47c1f065810b1b45f0d8d994aa0a4ff29505d4" translate="yes" xml:space="preserve">
          <source>If the metric constructor parameter is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be the distance matrix between the data to be predicted and &lt;code&gt;self.centroids_&lt;/code&gt;.</source>
          <target state="translated">메트릭 생성자 매개 변수가 &quot;사전 계산 된&quot;경우 X는 예측할 데이터와 &lt;code&gt;self.centroids_&lt;/code&gt; 사이의 거리 행렬로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="c8476d320358236ab03a9b2e5775d6207658d4f7" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 X는 제곱 거리 행렬이어야합니다. 그렇지 않으면 행당 샘플이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="ed2846275337b6c05f70ce353bc177c3fd2fc1b0" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;.</source>
          <target state="translated">메트릭이 '사전 계산 된'경우 X는 제곱 거리 행렬이어야합니다. 그렇지 않으면 행당 샘플이 포함됩니다. 방법이 '정확한'이면 X는 'csr', 'csc'또는 'coo'유형의 희소 행렬 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="be2b4ccc2ee21bcc622b72ad8a09e29905f86d22" translate="yes" xml:space="preserve">
          <source>If the number of features is \(p\), you now require \(n \sim 1/d^p\) points. Let&amp;rsquo;s say that we require 10 points in one dimension: now \(10^p\) points are required in \(p\) dimensions to pave the \([0, 1]\) space. As \(p\) becomes large, the number of training points required for a good estimator grows exponentially.</source>
          <target state="translated">기능의 수가 \ (p \)이면 이제 \ (n \ sim 1 / d ^ p \) 포인트가 필요합니다. 한 차원에 10 개의 점이 필요하다고 가정 해 봅시다. 이제 \ ([0, 1] \) 공간을 넓히려면 \ (p \) 차원에 \ (10 ​​^ p \) 점이 필요합니다. \ (p \)가 커짐에 따라, 좋은 견적에 필요한 훈련 포인트의 수가 기하 급수적으로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="31f6fadae8fdb5e25318685f0dd36c90a3234b90" translate="yes" xml:space="preserve">
          <source>If the number of features is much greater than the number of samples, avoid over-fitting in choosing &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; and regularization term is crucial.</source>
          <target state="translated">피처 수가 샘플 수보다 훨씬 많은 경우 &lt;a href=&quot;#svm-kernels&quot;&gt;커널 기능&lt;/a&gt; 을 선택할 때 과적 합을 피하고 정규화 용어가 중요합니다.</target>
        </trans-unit>
        <trans-unit id="421f2017551080d266c05ad587f0ba1ecff6bff8" translate="yes" xml:space="preserve">
          <source>If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.</source>
          <target state="translated">데이터 인스턴스 수를 줄이거 나 전처리 단계 또는 기타 방법으로 많은 수의 하위 클러스터를 원할 경우 Birch가 MiniBatchKMeans보다 유용합니다.</target>
        </trans-unit>
        <trans-unit id="0169ea68b458a3b315ac7acca32e943f8bdb1bed" translate="yes" xml:space="preserve">
          <source>If the option chosen is &amp;lsquo;ovr&amp;rsquo;, then a binary problem is fit for each label. For &amp;lsquo;multinomial&amp;rsquo; the loss minimised is the multinomial loss fit across the entire probability distribution, &lt;em&gt;even when the data is binary&lt;/em&gt;. &amp;lsquo;multinomial&amp;rsquo; is unavailable when solver=&amp;rsquo;liblinear&amp;rsquo;. &amp;lsquo;auto&amp;rsquo; selects &amp;lsquo;ovr&amp;rsquo; if the data is binary, or if solver=&amp;rsquo;liblinear&amp;rsquo;, and otherwise selects &amp;lsquo;multinomial&amp;rsquo;.</source>
          <target state="translated">선택한 옵션이 'ovr'이면 이진 문제가 각 레이블에 적합합니다. '다항식'의 경우 최소화 된 손실 &lt;em&gt;은 데이터가 이진 인 경우에도&lt;/em&gt; 전체 확률 분포에 맞는 다항식 손실 입니다. solver = 'liblinear'인 경우 '다항식'을 사용할 수 없습니다. 'auto'는 데이터가 이진이거나 'solver ='liblinear '인 경우'ovr '을 선택하고 그렇지 않으면'다항식 '을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="fb7bde2a134f67333f646adb3cb422fdb3b0a619" translate="yes" xml:space="preserve">
          <source>If the prediction task is to classify the observations in a set of finite labels, in other words to &amp;ldquo;name&amp;rdquo; the objects observed, the task is said to be a &lt;strong&gt;classification&lt;/strong&gt; task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a &lt;strong&gt;regression&lt;/strong&gt; task.</source>
          <target state="translated">예측 작업이 관측치를 유한 레이블 집합으로 분류하는 것, 즉 관찰 된 대상을 &quot;이름 지정&quot;하는 경우 작업은 &lt;strong&gt;분류&lt;/strong&gt; 작업이라고합니다. 반면에 목표가 연속적인 목표 변수를 예측하는 것이라면 &lt;strong&gt;회귀&lt;/strong&gt; 작업이라고합니다.</target>
        </trans-unit>
        <trans-unit id="c305135e1b17987e86652a7910deafacf0f5dc9d" translate="yes" xml:space="preserve">
          <source>If the pyamg package is installed, it is used: this greatly speeds up computation.</source>
          <target state="translated">pyamg 패키지가 설치되면 사용됩니다 : 계산 속도가 크게 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="04f07265ff7d7b70145be5aec52784e1b24d6f09" translate="yes" xml:space="preserve">
          <source>If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.</source>
          <target state="translated">새 샘플과 가장 가까운 서브 클러스터를 병합하여 얻은 서브 클러스터의 반지름이 임계 값의 제곱보다 크고 서브 클러스터 수가 분기 계수보다 크면이 새 샘플에 공간이 임시로 할당됩니다. 가장 먼 2 개의 서브 클러스터가 취해지고 서브 클러스터는 이들 서브 클러스터 사이의 거리에 기초하여 2 개의 그룹으로 분할된다.</target>
        </trans-unit>
        <trans-unit id="41eef1b8a501219131b2619b097a82294e78c28a" translate="yes" xml:space="preserve">
          <source>If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.</source>
          <target state="translated">샘플에 가중치가 부여되면 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; 와 같은 가중치 기반 사전 정리 기준을 사용하여 트리 구조를 최적화하기가 쉬워 리프 노드에 전체 샘플 가중치 합계의 적어도 일부가 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="0c1baaebbfab363e25ace0c6b6ee91539fab116b" translate="yes" xml:space="preserve">
          <source>If the selected solver is &amp;lsquo;L-BFGS&amp;rsquo;, training does not support online nor mini-batch learning.</source>
          <target state="translated">선택한 솔버가 'L-BFGS'인 경우 교육은 온라인 또는 미니 배치 학습을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6b79b273c5ccf0e85d810ff5a4ad56e9102a13be" translate="yes" xml:space="preserve">
          <source>If the target is a continuous value, then for node \(m\), representing a region \(R_m\) with \(N_m\) observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.</source>
          <target state="translated">대상이 연속 값인 경우 \ (N_m \) 관측 값으로 영역 \ (R_m \)을 나타내는 노드 \ (m \)의 경우 향후 분할 위치를 결정할 때 최소화하는 일반적인 기준은 평균 제곱 오차입니다. 터미널 노드에서 평균값을 사용하여 L2 오류를 최소화하고 터미널 절대 값에서 중간 값을 사용하여 L1 오류를 최소화하는 평균 절대 오류.</target>
        </trans-unit>
        <trans-unit id="bd360b0d17d9758c91116a373249c96c3c493365" translate="yes" xml:space="preserve">
          <source>If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as &lt;code&gt;latin-1&lt;/code&gt;. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.</source>
          <target state="translated">텍스트가 정렬하기 너무 어려운 인코딩의 혼란에 빠지면 (20 뉴스 그룹 데이터 세트의 경우) &lt;code&gt;latin-1&lt;/code&gt; 과 같은 간단한 단일 바이트 인코딩으로 대체 할 수 있습니다 . 일부 텍스트는 잘못 표시 될 수 있지만 최소한 동일한 바이트 시퀀스는 항상 동일한 기능을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="21286b430a50cd4c4f50df67c996d4b5b475416f" translate="yes" xml:space="preserve">
          <source>If the text you are loading is not actually encoded with UTF-8, however, you will get a &lt;code&gt;UnicodeDecodeError&lt;/code&gt;. The vectorizers can be told to be silent about decoding errors by setting the &lt;code&gt;decode_error&lt;/code&gt; parameter to either &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; or &lt;code&gt;&quot;replace&quot;&lt;/code&gt;. See the documentation for the Python function &lt;code&gt;bytes.decode&lt;/code&gt; for more details (type &lt;code&gt;help(bytes.decode)&lt;/code&gt; at the Python prompt).</source>
          <target state="translated">그러나로드하는 텍스트가 실제로 UTF-8로 인코딩되지 않은 경우 &lt;code&gt;UnicodeDecodeError&lt;/code&gt; 가 발생 합니다. 벡터 &lt;code&gt;decode_error&lt;/code&gt; 는 decode_error 매개 변수를 &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; 또는 &lt;code&gt;&quot;replace&quot;&lt;/code&gt; 로 설정하여 디코딩 오류에 대해 침묵하도록 지시 할 수 있습니다 . 자세한 내용은 Python 함수 &lt;code&gt;bytes.decode&lt;/code&gt; 설명서 를 참조하십시오 ( Python 프롬프트에서 &lt;code&gt;help(bytes.decode)&lt;/code&gt; 를 입력하십시오 ).</target>
        </trans-unit>
        <trans-unit id="13a9f813159d9e6258a164870cb1dc6a301dddd5" translate="yes" xml:space="preserve">
          <source>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter \(\gamma\) of an SVM on the digits dataset.</source>
          <target state="translated">훈련 점수와 유효성 검사 점수가 모두 낮 으면 추정기가 적합하지 않습니다. 훈련 점수가 높고 유효성 검사 점수가 낮 으면 견적자가 과적 합하고 그렇지 않으면 매우 잘 작동합니다. 낮은 교육 점수와 높은 검증 점수는 일반적으로 불가능합니다. 세 가지 경우 모두 아래 그림에서 찾을 수 있습니다. 여기서 숫자 데이터 세트에서 SVM의 매개 변수 \ (\ gamma \)를 변경합니다.</target>
        </trans-unit>
        <trans-unit id="80e789647361ff21671194a00300fe312dc530d2" translate="yes" xml:space="preserve">
          <source>If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored.</source>
          <target state="translated">변환 된 출력이 희소 데이터와 밀도가 높은 데이터로 구성된 경우 밀도가이 값보다 낮 으면 희소 행렬로 누적됩니다. 항상 밀도를 리턴 하려면 &lt;code&gt;sparse_threshold=0&lt;/code&gt; 을 사용하십시오 . 변환 된 출력이 모든 희소 데이터 또는 모든 조밀 한 데이터로 구성된 경우 누적 결과는 각각 희소 또는 조밀하며이 키워드는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="efca83041c066057e65d83989c4190b74b69dba6" translate="yes" xml:space="preserve">
          <source>If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.</source>
          <target state="translated">기본 그래프에 평균 노드보다 훨씬 많은 연결이있는 노드가있는 경우 알고리즘에서 이러한 연결 중 일부가 누락됩니다.</target>
        </trans-unit>
        <trans-unit id="27f470c8c1d74aa01f92e63860f4d2b9149dd0bb" translate="yes" xml:space="preserve">
          <source>If there are few data points per dimension, noise in the observations induces high variance:</source>
          <target state="translated">차원 당 데이터 포인트가 거의없는 경우 관측치의 노이즈가 높은 분산을 유발합니다.</target>
        </trans-unit>
        <trans-unit id="fbaec65eed1139944f6f906201326eaef7bc4d08" translate="yes" xml:space="preserve">
          <source>If there are more than two classes, \(f(x)\) itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,</source>
          <target state="translated">클래스가 두 개 이상인 경우 \ (f (x) \) 자체는 크기 (n_classes)의 벡터가됩니다. 로지스틱 함수를 통과하는 대신 softmax 함수를 통과합니다.</target>
        </trans-unit>
        <trans-unit id="e2c9b002eac60ce02e4a3cafb47196266855c43e" translate="yes" xml:space="preserve">
          <source>If there are more than two labels, &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; uses a multiclass variant due to Crammer &amp;amp; Singer. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;Here&lt;/a&gt; is the paper describing it.</source>
          <target state="translated">레이블이 두 개 이상인 경우 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 은 Crammer &amp;amp; Singer로 인해 멀티 클래스 변형을 사용합니다. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;여기&lt;/a&gt; 에 그것을 설명하는 논문이 있습니다.</target>
        </trans-unit>
        <trans-unit id="362b6e0ad023f937918b9ce5c294c8243f2c8ea1" translate="yes" xml:space="preserve">
          <source>If there is a possibility that the training data might have missing categorical features, it can often be better to specify &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; instead of setting the &lt;code&gt;categories&lt;/code&gt; manually as above. When &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (&lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is only supported for one-hot encoding):</source>
          <target state="translated">훈련 데이터에 범주 형 기능이 없을 수있는 경우 , 위와 같이 수동으로 &lt;code&gt;categories&lt;/code&gt; 를 설정하는 대신 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 를 지정하는 것이 좋습니다 . 때 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 지정되고 알 수없는 범주를 변환하는 동안 발생하는, 오류가 발생되지 않습니다하지만이 기능의 결과로 하나의 뜨거운 인코딩 된 열 (모두 제로가 될 것입니다 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 하나의 뜨거운 인코딩 지원 ) :</target>
        </trans-unit>
        <trans-unit id="76c3aee0f8dcd7757eeddd2a91b271bc2108fb17" translate="yes" xml:space="preserve">
          <source>If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.</source>
          <target state="translated">그러한 값이 두 개 이상인 경우 첫 번째 값만 반환됩니다. 모달 빈의 빈 개수도 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e4599c6e53b844db2376ed9e56ed7b49e63c6ec3" translate="yes" xml:space="preserve">
          <source>If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.</source>
          <target state="translated">이것이 int의 튜플 인 경우 이전과 같이 단일 축 또는 모든 축 대신 여러 축에 대해 평균이 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="015e500928e7c3f86f2c9b5121c746246fcd7a9f" translate="yes" xml:space="preserve">
          <source>If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</source>
          <target state="translated">이것이 True로 설정되면 축소 된 축은 결과적으로 크기가 1 인 치수로 남습니다. 이 옵션을 사용하면 결과가 입력 배열에 대해 올바르게 브로드 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="1d47783a4de427039b4db643f305b3dd195e7ba2" translate="yes" xml:space="preserve">
          <source>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.</source>
          <target state="translated">이 분할 노드에 상위 하위 클러스터가 있고 새 하위 클러스터를위한 공간이 있으면 상위가 두 개로 분할됩니다. 공간이 없으면이 노드는 다시 두 개로 분할되고 프로세스는 루트에 도달 할 때까지 재귀 적으로 계속됩니다.</target>
        </trans-unit>
        <trans-unit id="012f5a7e85e6e4a424dae20b5f0c103c4fa516ee" translate="yes" xml:space="preserve">
          <source>If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.</source>
          <target state="translated">true 인 경우 (기본값) 문제에 폭 우선 접근 방식을 사용하십시오. 그렇지 않으면 깊이 우선 접근 방식을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="8002efc50268110232cc0ffbdad97c50440caadd" translate="yes" xml:space="preserve">
          <source>If true, X and y will be centered.</source>
          <target state="translated">참이면 X와 y가 중앙에 위치합니다.</target>
        </trans-unit>
        <trans-unit id="de73b79cb6d725781d21d3fce59be150e3f40a4c" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None.</source>
          <target state="translated">true 인 경우 초기 커널 위치는 모든 포인트의 위치가 아니라 이산 된 포인트 버전의 위치입니다. 이 옵션을 True로 설정하면 더 적은 시드가 초기화되므로 알고리즘 속도가 빨라집니다. 씨앗 인수가 없음이 아닌 경우 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="382a329c7d03e3dc0838d8f846116bb309725e41" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None.</source>
          <target state="translated">true 인 경우 초기 커널 위치는 모든 포인트의 위치가 아니라 이산 된 포인트 버전의 위치입니다. 이 옵션을 True로 설정하면 더 적은 시드가 초기화되므로 알고리즘 속도가 빨라집니다. 기본값 : False seed 인수가 None이 아닌 경우 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="5f8f1503f4ad4447aabcfddb9ab2a5dcd7bfde79" translate="yes" xml:space="preserve">
          <source>If true, only interaction features are produced: features that are products of at most &lt;code&gt;degree&lt;/code&gt;&lt;em&gt;distinct&lt;/em&gt; input features (so not &lt;code&gt;x[1] ** 2&lt;/code&gt;, &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt;, etc.).</source>
          <target state="translated">참이면 만의 상호 작용 기능을 생산하는 대부분의 제품에있는 기능 &lt;code&gt;degree&lt;/code&gt; &lt;em&gt;구별&lt;/em&gt; 입력 기능 (그렇게하지 &lt;code&gt;x[1] ** 2&lt;/code&gt; , &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt; 등).</target>
        </trans-unit>
        <trans-unit id="2d6ce18ffe19be253728c95b7f8882b85215b418" translate="yes" xml:space="preserve">
          <source>If true, randomize the order of coordinates in the CD solver.</source>
          <target state="translated">참이면 CD 솔버에서 좌표 순서를 랜덤 화하십시오.</target>
        </trans-unit>
        <trans-unit id="c9d7c7ecbdd08be425848806cc6f9d68c29d7323" translate="yes" xml:space="preserve">
          <source>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</source>
          <target state="translated">참이면 샘플 당 평균 손실을 반환합니다. 그렇지 않으면 샘플 당 손실의 합을 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="6d32e9cabd3e10e0279ad9dd2b7c71514057bf52" translate="yes" xml:space="preserve">
          <source>If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.</source>
          <target state="translated">참이면, 커널 내에없는 고아까지도 모든 지점이 클러스터됩니다. 고아는 가장 가까운 커널에 할당됩니다. false 인 경우 고아에게 클러스터 레이블 -1이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="87535a59e28d16a49b32c83a6882f2aefd67503d" translate="yes" xml:space="preserve">
          <source>If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="translated">참이면 듀얼 트리 알고리즘을 사용하십시오. 그렇지 않으면 단일 트리 알고리즘을 사용하십시오. 이중 트리 알고리즘은 큰 N에 대해 더 나은 확장 성을 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a62f22814f97612f53d5c62d8ea50a484acea3fc" translate="yes" xml:space="preserve">
          <source>If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="translated">두 변수가 반응과 거의 동일하게 상관되는 경우 계수는 거의 같은 속도로 증가해야합니다. 따라서 알고리즘은 직감이 예상하는대로 작동하며 더욱 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="c6d813716240a58cb1e341ee096ed78ff4d17824" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are plotted at each iteration.</source>
          <target state="translated">verbose가 True이면 목적 함수와 이중 간격이 각 반복에 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="c845cf733c967b921d034159fbd6e6d551e8f5a1" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are printed at each iteration.</source>
          <target state="translated">verbose가 True이면 각 반복마다 목적 함수와 이중 간격이 인쇄됩니다.</target>
        </trans-unit>
        <trans-unit id="d04bb65f95446e17ac813ad52d517cc52bee8bef" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and duality gap are printed at each iteration.</source>
          <target state="translated">verbose가 True이면 각 반복마다 목적 함수와 이중성 간격이 인쇄됩니다.</target>
        </trans-unit>
        <trans-unit id="c297e2c2dea63aea70529d05594e08d33ba95930" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">웜 스타트가 활성화 된 경우, 이후 모드의 Laplace 근사에서 마지막 Newton 반복 솔루션은 다음 _posterior_mode () 호출에 대한 초기화로 사용됩니다. 하이퍼 파라미터 최적화와 비슷한 문제에서 _posterior_mode를 여러 번 호출하면 수렴 속도가 빨라질 수 있습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0beda1307d8fba8bf455ad043421d0e1b5743334" translate="yes" xml:space="preserve">
          <source>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</source>
          <target state="translated">손실 함수를 표본 당 개별 오차로 간주하면 표본을 더 추가할수록 데이터 적합 항 또는 각 표본에 대한 오차의 합이 증가합니다. 그러나 벌칙은 증가하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="232b0b83965c86185ba5cb4047fca7ca1eb2e5e8" translate="yes" xml:space="preserve">
          <source>If we define &lt;code&gt;s = 1 / density&lt;/code&gt;, the elements of the random matrix are drawn from</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; 정의 하면 랜덤 행렬의 요소는</target>
        </trans-unit>
        <trans-unit id="1427e0ae27c6ada10257117ddf3e602836e812e6" translate="yes" xml:space="preserve">
          <source>If we note &lt;code&gt;s = 1 / density&lt;/code&gt; the components of the random matrix are drawn from:</source>
          <target state="translated">&lt;code&gt;s = 1 / density&lt;/code&gt; 주목 하면 랜덤 매트릭스의 구성 요소는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f9cbc4d2964857d1865d4f91b696f12d3cce3cff" translate="yes" xml:space="preserve">
          <source>If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">\ (n _ {\ max} = \ max (n _ {\ mathrm {samples}}, n _ {\ mathrm {features}}) \) 및 \ (n _ {\ min} = \ min (n _ {\ mathrm {samples}}, n _ {\ mathrm {features}}) \), 무작위 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 의 시간 복잡도 는 \ (O (n _ {\ max} ^ 2 \ cdot n _ {\ mathrm {components}}) \)입니다. 구현 정확한 방법 \ (O (N _ {\ 최대} ^ 2 \ cdot N _ {\ 분}) \)의 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="483854d9b52bcc6a7814b6c99e197398a7767c0e" translate="yes" xml:space="preserve">
          <source>If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot. As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal, but the off-diagonal structure is lost.</source>
          <target state="translated">Ledoit-Wolf Estimator와 마찬가지로 l2 수축을 사용하는 경우 샘플 수가 적으므로 많이 축소해야합니다. 결과적으로, Ledoit-Wolf 정밀도는지면 진실 정밀도에 상당히 가깝습니다. 즉, 대각선이 아닌 멀지 않은 대각선 구조입니다.</target>
        </trans-unit>
        <trans-unit id="c048d2d806d4fc664d0d49e2240da1f9038d7165" translate="yes" xml:space="preserve">
          <source>If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:</source>
          <target state="translated">평면 대신 데이터에 포물면을 맞추려면 모형을 다음과 같이 2 차 다항식으로 결합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9cd2bdd13889db844fd297c5019d226d5f2214ec" translate="yes" xml:space="preserve">
          <source>If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain &lt;code&gt;PPCA&lt;/code&gt;.</source>
          <target state="translated">가우스 잡음이 등방성 (모든 대각선 항목이 동일 함)이라고 가정하여 모델을 더 제한하면 &lt;code&gt;PPCA&lt;/code&gt; 를 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="722d8ecf13f293f3aeb1f206f30063d8afe3b1aa" translate="yes" xml:space="preserve">
          <source>If whiten is false, the data is already considered to be whitened, and no whitening is performed.</source>
          <target state="translated">미백이 거짓 인 경우, 데이터는 이미 미백 된 것으로 간주되며 미백이 수행되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7d7146f3cf5f6ee3a12daad9561636b3070c19dc" translate="yes" xml:space="preserve">
          <source>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</source>
          <target state="translated">미백이 활성화 된 경우 inverse_transform은 반전 미백을 포함한 정확한 역 연산을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d6701cdf426d6a22381b3dd206332eab0defeb4b" translate="yes" xml:space="preserve">
          <source>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant &lt;code&gt;c&lt;/code&gt; such that the average L2 norm of the training data equals one.</source>
          <target state="translated">PCA를 사용하여 추출한 피처에 SGD를 적용하는 경우 훈련 데이터의 평균 L2 규범이 1이되도록 일정한 &lt;code&gt;c&lt;/code&gt; 로 피처 값을 스케일링하는 것이 현명하다는 것을 알았습니다 .</target>
        </trans-unit>
        <trans-unit id="88e7b5f8ea1b769958767cd4c4986cb0d84b69d4" translate="yes" xml:space="preserve">
          <source>If you are having trouble decoding text, here are some things to try:</source>
          <target state="translated">텍스트를 디코딩하는 데 문제가있는 경우 다음을 시도해보십시오.</target>
        </trans-unit>
        <trans-unit id="79c8dbf5a9dd8e0bcb21dbc0b3d21d4a002af1f5" translate="yes" xml:space="preserve">
          <source>If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:</source>
          <target state="translated">L1 및 L2 페널티를 개별적으로 제어하려는 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fda5569b1e927ca58d1243554ef8331577e43162" translate="yes" xml:space="preserve">
          <source>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.</source>
          <target state="translated">사전 사전을 제공하지 않고 특정 유형의 기능 선택을 수행하는 분석기를 사용하지 않으면 기능 수는 데이터를 분석하여 찾은 어휘 크기와 같습니다.</target>
        </trans-unit>
        <trans-unit id="44906a85511569286aafb87826155b3c2905ddf9" translate="yes" xml:space="preserve">
          <source>If you don&amp;rsquo;t have labels, try using &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;Clustering&lt;/a&gt; on your problem.</source>
          <target state="translated">레이블이 없으면 문제에 대해 &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;클러스터링&lt;/a&gt; 을 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="1e3c9a76f4703e92f09f761bb01632ba0994c7fc" translate="yes" xml:space="preserve">
          <source>If you experience hanging subprocesses with &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; or &lt;code&gt;n_jobs=-1&lt;/code&gt;, make sure you have a single-threaded BLAS library, or set &lt;code&gt;n_jobs=1&lt;/code&gt;, or upgrade to Python 3.4 which has a new version of &lt;code&gt;multiprocessing&lt;/code&gt; that should be immune to this problem.</source>
          <target state="translated">&lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 또는 &lt;code&gt;n_jobs=-1&lt;/code&gt; 로 하위 프로세스가 중단 되는 경우 단일 스레드 BLAS 라이브러리가 있는지 확인하거나 &lt;code&gt;n_jobs=1&lt;/code&gt; 을 설정 하거나 새로운 버전의 &lt;code&gt;multiprocessing&lt;/code&gt; 이있는 Python 3.4로 업그레이드 하십시오. 문제.</target>
        </trans-unit>
        <trans-unit id="b4d3f940390acd5b7c567c108aba27222ddceb7d" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(phi\) followed by removal of the mean in that space.</source>
          <target state="translated">함수 \ (phi \)에 의해 정의 된 기능 공간에서 내적을 계산하는 커널 \ (K \)의 커널 행렬이있는 경우 &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt; &lt;code&gt;KernelCenterer&lt;/code&gt; &lt;/a&gt; 는 정의 된 기능 공간에 내부 제품이 포함되도록 커널 행렬을 변환 할 수 있습니다. \ (phi \) 다음에 그 공간의 평균을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="2615ef2dcc8005e7f8f1fe1a8b864f8c5c8b40ba" translate="yes" xml:space="preserve">
          <source>If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel:</source>
          <target state="translated">거리 행렬과 같이 친 화성 행렬 (예 : 0이 동일한 요소를 의미하고 높은 값이 매우 다른 요소를 의미 함)을 갖는 경우 가우시안 (RBF)을 적용하여 알고리즘에 적합한 유사성 행렬로 변환 할 수 있습니다. 열) 커널 :</target>
        </trans-unit>
        <trans-unit id="fe35ae96a69c356c4c790a684b706493b567f769" translate="yes" xml:space="preserve">
          <source>If you have multiple labels per document, e.g categories, have a look at the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel section&lt;/a&gt;.</source>
          <target state="translated">문서마다 여러 레이블 (예 : 카테고리)이있는 경우 &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;다중 클래스 및 다중 레이블 섹션을&lt;/a&gt; 살펴보십시오 .</target>
        </trans-unit>
        <trans-unit id="4c5f38a361bcbafd12cc29508483a444dfcf9728" translate="yes" xml:space="preserve">
          <source>If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.</source>
          <target state="translated">예측할 클래스가 여러 개인 경우, 일반적으로 사용되는 옵션은 모든 분류기에 적합하고 최종 결정에 투표 휴리스틱을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f97615967054f5695c197161c38be5160cb380e9" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots you can use the &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt;&lt;code&gt;partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">플롯이 아닌 부분 의존 함수의 원시 값이 필요한 경우 &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt; &lt;code&gt;partial_dependence&lt;/code&gt; &lt;/a&gt; 함수를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="57ce2ca8cd47ab25ffa05da2880829913ab0ea8d" translate="yes" xml:space="preserve">
          <source>If you really want to use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator. In this case, &lt;code&gt;fit_predict&lt;/code&gt; is not available.</source>
          <target state="translated">참신 탐지 (예 : 레이블 예측 또는 보이지 않는 새로운 데이터의 비정상 점수 계산)에 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 을 실제로 사용 하려면 추정기를 피팅하기 전에 &lt;code&gt;novelty&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정 하여 추정기를 인스턴스화 할 수 있습니다 . 이 경우 &lt;code&gt;fit_predict&lt;/code&gt; 를 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="9f2c392b35ce2be9956cf3b6740e21a9cb0e7eb8" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;.</source>
          <target state="translated">load_content = True를 설정하면 'encoding'매개 변수를 사용하여 텍스트의 인코딩도 지정해야합니다. 많은 최신 텍스트 파일의 경우 'utf-8'이 올바른 인코딩입니다. 인코딩을 없음으로두면 내용이 유니 코드 대신 바이트로 만들어지며 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 에서 대부분의 기능을 사용할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="bc2de8e7176de3a7ec5f2ba66eebe7612bd92fb1" translate="yes" xml:space="preserve">
          <source>If you specify &lt;code&gt;max_depth=h&lt;/code&gt; then complete binary trees of depth &lt;code&gt;h&lt;/code&gt; will be grown. Such trees will have (at most) &lt;code&gt;2**h&lt;/code&gt; leaf nodes and &lt;code&gt;2**h - 1&lt;/code&gt; split nodes.</source>
          <target state="translated">&lt;code&gt;max_depth=h&lt;/code&gt; 를 지정하면 깊이 &lt;code&gt;h&lt;/code&gt; 의 완전한 이진 트리 가 커집니다. 이러한 나무는 (최대) &lt;code&gt;2**h&lt;/code&gt; 리프 노드와 &lt;code&gt;2**h - 1&lt;/code&gt; 분할 노드 를 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="b5e591b33a0bd24a7e9f3db1117e493d465fb600" translate="yes" xml:space="preserve">
          <source>If you use sparse data (i.e. data represented as sparse matrices), &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt; will deal with the data without making it dense.</source>
          <target state="translated">당신이 스파 스 데이터 (희소 행렬로 표현 즉, 데이터)를 사용하는 경우 &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; 을&lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; 는&lt;/a&gt; 밀도를하지 않고 데이터를 처리합니다.</target>
        </trans-unit>
        <trans-unit id="a1ac2d367786359c2f555cec450b280865094e6b" translate="yes" xml:space="preserve">
          <source>If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using &lt;code&gt;warm_start=True&lt;/code&gt; and &lt;code&gt;max_iter=1&lt;/code&gt; and iterating yourself can be helpful:</source>
          <target state="translated">SGD에서 중지 기준 또는 학습 속도에 대한 제어를 강화하거나 &lt;code&gt;warm_start=True&lt;/code&gt; 및 &lt;code&gt;max_iter=1&lt;/code&gt; 을 사용하여 추가 모니터링을 수행 하고 자신을 반복하는 것이 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5572f4380789e92c52811040d71f90082bdb6315" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="translated">이러한 문제에 대해 더 알고 다른 가능한 직렬화 방법을 탐색하려면 &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;Alex Gaynor&lt;/a&gt; 의이 대화 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="c9d28c176517636a408517ab464ebf5a8ed78a10" translate="yes" xml:space="preserve">
          <source>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</source>
          <target state="translated">속성에 고유 한 스케일 (예 : 단어 빈도 또는 표시기 기능)이있는 경우 스케일링이 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1c54fdc8274e03b04e776296dd28d5ff9fd81085" translate="yes" xml:space="preserve">
          <source>If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt;&lt;code&gt;RobustScaler&lt;/code&gt;&lt;/a&gt; as drop-in replacements instead. They use more robust estimates for the center and range of your data.</source>
          <target state="translated">데이터에 특이 치가 많이 포함 된 경우 데이터의 평균 및 분산을 사용하여 조정하면 제대로 작동하지 않을 수 있습니다. 이러한 경우, 대신 &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt; &lt;code&gt;RobustScaler&lt;/code&gt; &lt;/a&gt; 교체로 &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt; 및 RobustScaler 를 사용할 수 있습니다 . 데이터의 중심과 범위에 대해보다 강력한 추정치를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="a10bfba29a759d89e81956a541262290109cf294" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="translated">기능 수가 많으면 감독 단계 전에 감독되지 않은 단계로 기능을 줄이는 것이 유용 할 수 있습니다. 많은 &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;비지도 학습&lt;/a&gt; 방법 은 차원을 줄이는 데 사용할 수 있는 &lt;code&gt;transform&lt;/code&gt; 방법을 구현 합니다. 아래에서는 많이 사용되는이 패턴의 두 가지 특정 예에 대해 설명합니다.</target>
        </trans-unit>
        <trans-unit id="933c257247173f2464c3cf8d4de4af2d678e5a7c" translate="yes" xml:space="preserve">
          <source>If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.</source>
          <target state="translated">관측치 수가 기본 그래프의 모서리 수와 비교하여 크지 않으면이를 복구하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4f34c772816074a373c0d5e918e2e9ac136448b7" translate="yes" xml:space="preserve">
          <source>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</source>
          <target state="translated">앞으로 탐색하여 오프셋 첫 바이트를 무시하고 다음 줄 바꾸기 문자까지 다음 바이트를 버립니다.</target>
        </trans-unit>
        <trans-unit id="78fee1435d74666b84850cd5e82c18229351da5d" translate="yes" xml:space="preserve">
          <source>Ignored</source>
          <target state="translated">Ignored</target>
        </trans-unit>
        <trans-unit id="1e65bb4eca2d3c71529c96890a4b735eb7dafeac" translate="yes" xml:space="preserve">
          <source>Ignored.</source>
          <target state="translated">Ignored.</target>
        </trans-unit>
        <trans-unit id="1e417badfc4d52f79664b451110854e41b4a0daf" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.</source>
          <target state="translated">무시되었습니다. 이 매개 변수는 sklearn.pipeline.Pipeline과의 호환성을 위해서만 존재합니다.</target>
        </trans-unit>
        <trans-unit id="2d34b7c897f7b41a0f0625575a2c9cc21b1078a7" translate="yes" xml:space="preserve">
          <source>Illustration of &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 및 &lt;code&gt;GridSearchCV&lt;/code&gt; 의 그림</target>
        </trans-unit>
        <trans-unit id="643998f34944846c305de7d49de1c3e80f814d2d" translate="yes" xml:space="preserve">
          <source>Illustration of Gaussian process classification (GPC) on the XOR dataset</source>
          <target state="translated">XOR 데이터 세트의 가우스 프로세스 분류 (GPC) 그림</target>
        </trans-unit>
        <trans-unit id="c2cd661f8089fd4df71dfb566ea137083aa22024" translate="yes" xml:space="preserve">
          <source>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.</source>
          <target state="translated">보이지 않는 데이터 (테스트 데이터)에 대한 추정기의 성능이 훈련 데이터의 성능과 어떻게 다른지 설명합니다. 정규화가 증가함에 따라 열차의 성능이 저하되는 반면, 테스트의 성능은 정규화 매개 변수의 값 범위 내에서 최적입니다. Elastic-Net 회귀 모형과 성능의 예는 설명 된 분산 (R ^ 2)을 사용하여 측정됩니다.</target>
        </trans-unit>
        <trans-unit id="5790a5aaa3a6c4543a820b9b12ce6d261eeb0581" translate="yes" xml:space="preserve">
          <source>Illustration of prior and posterior Gaussian process for different kernels</source>
          <target state="translated">다른 커널에 대한 이전 및 이후 가우스 프로세스의 그림</target>
        </trans-unit>
        <trans-unit id="27c062ea4e410688effe23ac51313a8ab9a70f1c" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">그라디언트 부스팅에 대한 다양한 정규화 전략의 효과를 보여줍니다. 이 예는 Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 에서 발췌 한 것이다 .</target>
        </trans-unit>
        <trans-unit id="5beb1d257bbb65ddb7ec568445a1c3acdcb42d37" translate="yes" xml:space="preserve">
          <source>Image denoising using dictionary learning</source>
          <target state="translated">사전 학습을 사용한 이미지 노이즈 제거</target>
        </trans-unit>
        <trans-unit id="5ab7decf36c80b04aff06a11c0e8ef068c85a1b9" translate="yes" xml:space="preserve">
          <source>Image histogram</source>
          <target state="translated">이미지 히스토그램</target>
        </trans-unit>
        <trans-unit id="5c328038b14054033bab1147ef5d1ad234b3373d" translate="yes" xml:space="preserve">
          <source>Imagine you have three subjects, each with an associated number from 1 to 3:</source>
          <target state="translated">각각 1에서 3까지의 관련 번호를 가진 세 가지 주제가 있다고 가정하십시오.</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="8d522809f4125f5930c1f4f77ec91f8735a003d8" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</source>
          <target state="translated">를 기반으로 구현 &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b6ac8df85fe47d2d00b4a78e1facdef4fbcae73b" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</source>
          <target state="translated">libsvm을 사용한 Support Vector Machine 분류기의 구현 : 커널은 비선형 일 수 있지만 LinearSVC처럼 SMO 알고리즘은 많은 수의 샘플로 확장되지 않습니다. 또한 SVC 멀티 클래스 모드는 일대일 방식을 사용하여 구현되는 반면 LinearSVC는 일대일 방식을 사용합니다. &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 랩퍼 를 사용하여 SVC를 사용하여 나머지 하나를 구현할 수 있습니다 . 마지막으로 입력이 C 연속 인 경우 SVC는 메모리 복사없이 밀도가 높은 데이터를 맞출 수 있습니다. 스파 스 데이터는 여전히 메모리 복사를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="78b58091d5da65fb36aa747d9975841d97302dec" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</source>
          <target state="translated">이 클래스와 동일한 라이브러리를 사용하여 Support Vector Machine 분류기 구현 (liblinear).</target>
        </trans-unit>
        <trans-unit id="0faf8832b17d93a1b230f7a6ca4feb36d15cc4ac" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</source>
          <target state="translated">libsvm을 사용한 Support Vector Machine 회귀 구현 : 커널은 비선형 일 수 있지만 LinearSVC처럼 SMO 알고리즘은 많은 수의 샘플로 확장되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="adae10003f16f5885f71700e866f2cc76e2c6af9" translate="yes" xml:space="preserve">
          <source>Implements feature hashing, aka the hashing trick.</source>
          <target state="translated">해싱 기능 (일명 해시 트릭)을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="d98e09b894119d4a2d55bf3e2f04052ec103359a" translate="yes" xml:space="preserve">
          <source>Implements resampling with replacement. If False, this will implement (sliced) random permutations.</source>
          <target state="translated">교체로 리샘플링을 구현합니다. False이면 무작위 순열을 구현합니다 (슬라이스).</target>
        </trans-unit>
        <trans-unit id="9f9d0b6a3b9dbc770ff8e17c3a6979d6ebb5425d" translate="yes" xml:space="preserve">
          <source>Implements the Birch clustering algorithm.</source>
          <target state="translated">버치 클러스터링 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="82028db75262dc1a82a0dc4cf2e6f254032ff9f7" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; 에서 증분 PCA 모델을 구현합니다 . 2008 년 5 월 참조 &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="06be3bf25c44efb35fdd12e8816c051b94a6e5d6" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: &lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt;Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt; Tipping, ME 및 Bishop, CM (1999) 에서 확률 적 PCA 모델을 구현합니다 . &amp;ldquo;확률 적 주성분 분석&amp;rdquo;. 왕립 통계 학회지 : 시리즈 B (통계 방법론), 61 (3), 611-622. score 및 score_samples 메소드를 통해 참조 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf를&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="496d8573358dc0bbf8fad0d466b95c37d153e5fd" translate="yes" xml:space="preserve">
          <source>Importance of Feature Scaling</source>
          <target state="translated">기능 스케일링의 중요성</target>
        </trans-unit>
        <trans-unit id="dee0fbd7a096536203f3e083c7a95f20ef772057" translate="yes" xml:space="preserve">
          <source>Important members are fit, predict.</source>
          <target state="translated">중요한 멤버는 적합합니다.</target>
        </trans-unit>
        <trans-unit id="0004bf233145469d6159f141af0ae0b05f3c5e9a" translate="yes" xml:space="preserve">
          <source>Imputation transformer for completing missing values.</source>
          <target state="translated">결 측값을 완료하기위한 대치 변압기.</target>
        </trans-unit>
        <trans-unit id="8154b566118976ff2097cfffb2c92470797b0a69" translate="yes" xml:space="preserve">
          <source>Impute all missing values in X.</source>
          <target state="translated">X에서 모든 결 측값을 대치합니다.</target>
        </trans-unit>
        <trans-unit id="510c592fb9a4fd828788fc0bdd902c165ca78889" translate="yes" xml:space="preserve">
          <source>Imputing missing values before building an estimator</source>
          <target state="translated">추정기를 작성하기 전에 결 측값 대치</target>
        </trans-unit>
        <trans-unit id="e5d148df74ab3f703a9d283fda0c99f4936ff674" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt;, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in &lt;code&gt;ElasticNet&lt;/code&gt;, we control the combination of L1 and L2 with the &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) parameter, and the intensity of the regularization with the &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) parameter. Then the priors terms are:</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; , L1 및 L2는 전과 모델 규칙 화하기 위해 손실 함수에 첨가 될 수있다. L2 이전은 Frobenius 표준을 사용하고 L1 이전은 요소 별 L1 표준을 사용합니다. 마찬가지로 &lt;code&gt;ElasticNet&lt;/code&gt; , 우리는와 L1과 L2의 조합 제어 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ RHO \)) 파라미터와 함께 정규화 강도 &lt;code&gt;alpha&lt;/code&gt; (\ (\ 알파 \)) 파라미터. 그런 다음 이전 조건은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="eee03375c59654f18a55a7961e4f26a36fbc2cee" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt;&lt;code&gt;OutputCodeClassifier&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;code_size&lt;/code&gt; attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt; &lt;code&gt;OutputCodeClassifier&lt;/code&gt; &lt;/a&gt; 상기 &lt;code&gt;code_size&lt;/code&gt; 의 특성은 사용되는 분류의 수를 제어하도록 사용자에게 허용한다. 총 클래스 수의 백분율입니다.</target>
        </trans-unit>
        <trans-unit id="92869ea1268ab85728d32cc145b2fc2a3cf98201" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if data for classification are unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">에서는 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 분류 데이터 (예를 들어, 많은 양의 거의 제외) 세트 불평형 경우 &lt;code&gt;class_weight='balanced'&lt;/code&gt; 및 / 또는 다른 페널티 파라미터를 시도 &lt;code&gt;C&lt;/code&gt; 를 .</target>
        </trans-unit>
        <trans-unit id="d4a2dd93e9c8bc18123ea577d336109c88e8c2c3" translate="yes" xml:space="preserve">
          <source>In &lt;strong&gt;averaging methods&lt;/strong&gt;, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</source>
          <target state="translated">에서 &lt;strong&gt;방법을 평균&lt;/strong&gt; , 구동 원리는 평균 독립적으로 그들의 예측을 여러 추정량을 구축하는 것입니다. 평균적으로, 결합 추정량은 분산이 줄어들 기 때문에 일반적으로 단일 기본 추정량보다 낫습니다.</target>
        </trans-unit>
        <trans-unit id="baeac0931c0e4b4385579000935f2bb52ceb9f07" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;lsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="translated">이진 분류 작업에서``긍정적 ''및``부정적 ''이라는 용어는 분류 자의 예측을 나타내며``true ''및``거짓 ''은 해당 예측이 외부 판단에 해당하는지 여부를 나타냅니다 ( 때때로``관찰 ''이라고도 함). 이러한 정의가 주어지면 다음 표를 공식화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="995a1ae8b5be72e5d8cbdf391052b665e77e9964" translate="yes" xml:space="preserve">
          <source>In a first step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it&amp;rsquo;s a hierarchical clustering with structure prior.</source>
          <target state="translated">첫 번째 단계에서 계층 적 군집화는 구조에 대한 연결 제약 조건없이 수행되며 거리 만 기준으로하는 반면 두 번째 단계에서는 클러스터링이 k- 최근 접 이웃 그래프로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="0336cb4d8e7c9adb72abdea9417802db49cccd1f" translate="yes" xml:space="preserve">
          <source>In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</source>
          <target state="translated">큰 텍스트 모음에서 일부 단어가 매우 많이 존재하므로 (예 : 영어로 &quot;the&quot;, &quot;a&quot;, &quot;is&quot;) 문서의 실제 내용에 대한 의미있는 정보가 거의 없습니다. 직접 카운트 데이터를 분류 자에게 직접 공급한다면 매우 빈번한 용어는 더 희귀하지만 더 흥미로운 용어의 빈도를 가리게됩니다.</target>
        </trans-unit>
        <trans-unit id="cd0ed349168abd35a1fce87e6ae9e8ccc5ff58f4" translate="yes" xml:space="preserve">
          <source>In a nutshell, the following table summarizes the solvers characteristics:</source>
          <target state="translated">간단히 말해서, 다음 표는 솔버 특성을 요약 한 것입니다.</target>
        </trans-unit>
        <trans-unit id="b4ec4bcaff3e86d4d99c938f5623ab4b737e65c7" translate="yes" xml:space="preserve">
          <source>In a real world setting, the &lt;code&gt;n_features&lt;/code&gt; parameter can be left to its default value of &lt;code&gt;2 ** 20&lt;/code&gt; (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as &lt;code&gt;2 **
18&lt;/code&gt; might help without introducing too many additional collisions on typical text classification tasks.</source>
          <target state="translated">실제 환경에서 &lt;code&gt;n_features&lt;/code&gt; 매개 변수는 기본값 &lt;code&gt;2 ** 20&lt;/code&gt; (약 100 만 개의 가능한 기능)으로 유지 될 수 있습니다. 메모리 또는 다운 스트림 모델 크기가 &lt;code&gt;2 ** 18&lt;/code&gt; 과 같은 낮은 값을 선택하는 데 문제가있는 경우 일반적인 텍스트 분류 작업에 너무 많은 추가 충돌이 발생하지 않으면 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c75e295f24e05d06cacc1a2f9bb570a61bdd802e" translate="yes" xml:space="preserve">
          <source>In a similar manner, the boston housing data set is used to show the impact of transforming the targets before learning a model. In this example, the targets to be predicted corresponds to the weighted distances to the five Boston employment centers.</source>
          <target state="translated">비슷한 방식으로, 보스턴 하우징 데이터 세트는 모델을 학습하기 전에 대상을 변환하는 데 따른 영향을 나타내는 데 사용됩니다. 이 예에서, 예측 될 목표는 5 개의 보스턴 고용 센터까지의 가중 거리에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="c210295417ef2f29cc593be46bbc5efed5892a5f" translate="yes" xml:space="preserve">
          <source>In addition of using an imputing method, we can also keep an indication of the missing information using &lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt;&lt;/a&gt; which might carry some information.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt; &lt;/a&gt; 방법을 사용 하는 것 외에도 일부 정보를 전달할 수있는 sklearn.impute.MissingIndicator 를 사용하여 누락 된 정보를 표시 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="45e3263783ee36dc90c5821ae7ffc8d210750151" translate="yes" xml:space="preserve">
          <source>In addition to its current contents, this module will eventually be home to refurbished versions of Pipeline and FeatureUnion.</source>
          <target state="translated">이 모듈은 현재 내용 외에도 파이프 라인 및 FeatureUnion의 리퍼브 버전이 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="3dda0db479e61a3dc2539c918fe85f072a3cc4a4" translate="yes" xml:space="preserve">
          <source>In addition to standard scikit-learn estimator API, GaussianProcessRegressor:</source>
          <target state="translated">표준 scikit-learn 추정기 API 외에도 GaussianProcessRegressor는 다음을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="6c259b1081efe473add72a6c01ee26dbc96ee486" translate="yes" xml:space="preserve">
          <source>In addition to the mean of the predictive distribution, also its standard deviation can be returned.</source>
          <target state="translated">예측 분포의 평균 외에도 표준 편차가 반환 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f5f01da0b407208bd57121f71ed7408cbbce3fe6" translate="yes" xml:space="preserve">
          <source>In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi partition of the graph.</source>
          <target state="translated">또한 이미지의 강도 나 그레디언트에 유용한 정보가 없기 때문에 그래디언트에 약한 정보 만있는 그래프에서 스펙트럼 클러스터링을 수행하도록 선택합니다. 이것은 그래프의 보로 노이 분할 수행에 가깝습니다.</target>
        </trans-unit>
        <trans-unit id="8be2789c3e2f5a1d410c34b62e20c28c8adc9fef" translate="yes" xml:space="preserve">
          <source>In addition, if the &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;distributed&lt;/code&gt; Python packages are installed, it is possible to use the &amp;lsquo;dask&amp;rsquo; backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</source>
          <target state="translated">또한 &lt;code&gt;dask&lt;/code&gt; 및 &lt;code&gt;distributed&lt;/code&gt; Python 패키지가 설치되어 있으면 초과 가입없이 중첩 된 병렬 호출을 더 잘 예약하고 '호스트'백엔드를 사용하여 여러 호스트의 네트워크 클러스터를 통해 병렬 호출을 분산시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1068dbee0dd3c16e2fc93e44b0ce455f5b052f8b" translate="yes" xml:space="preserve">
          <source>In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.</source>
          <target state="translated">또한, scikit-learn에는 제어 된 크기와 복잡성의 인공 데이터 세트를 구축하는 데 사용할 수있는 다양한 랜덤 샘플 생성기가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="67ed28f1d0cd9fef0560dd0bbfe2b568680ea5a6" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellanous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="translated">또한 &lt;a href=&quot;#loading-other-datasets&quot;&gt;다른 데이터 세트로드&lt;/a&gt; 섹션에 설명 된 다른 형식 또는 다른 위치의 데이터 세트를로드하는 기타 도구도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="846c6c3d11b49bd5243a8b72c066c7aae06dcfe3" translate="yes" xml:space="preserve">
          <source>In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are interested in separating the objects one from the other, and not from the background.</source>
          <target state="translated">또한 객체의 마스크를 사용하여 그래프를 객체의 윤곽으로 제한합니다. 이 예에서는 배경이 아닌 객체를 서로 분리하는 데 관심이 있습니다.</target>
        </trans-unit>
        <trans-unit id="b490744f01019d4237b3a8568465e031a5ae6e1f" translate="yes" xml:space="preserve">
          <source>In all these strategies, the &lt;code&gt;predict&lt;/code&gt; method completely ignores the input data.</source>
          <target state="translated">이러한 모든 전략에서 &lt;code&gt;predict&lt;/code&gt; 방법은 입력 데이터를 완전히 무시합니다.</target>
        </trans-unit>
        <trans-unit id="450c8a41f7c5da3bb2b7da9a15306b194b36c681" translate="yes" xml:space="preserve">
          <source>In an &lt;strong&gt;unsupervised setting&lt;/strong&gt; it can be used to group similar documents together by applying clustering algorithms such as &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt;:</source>
          <target state="translated">&lt;strong&gt;감독되지 않은 설정&lt;/strong&gt; 에서는 &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt; 와 같은 클러스터링 알고리즘을 적용하여 유사한 문서를 그룹화하는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e5af8b183011d6bf7238d58f71b94384a5a96f84" translate="yes" xml:space="preserve">
          <source>In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.</source>
          <target state="translated">어쨌든 모델 복잡성을 줄이면 위에서 언급 한대로 정확도가 떨어질 수 있다는 경고가 표시됩니다. 예를 들어 비선형 적으로 분리 가능한 문제는 빠른 선형 모델로 처리 할 수 ​​있지만 그 과정에서 예측력이 저하 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c17ce8abfedb506f7ed46ebde27121f581c8bb7" translate="yes" xml:space="preserve">
          <source>In applications where a high false positive rate is not tolerable the parameter &lt;code&gt;max_fpr&lt;/code&gt; of &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; can be used to summarize the ROC curve up to the given limit.</source>
          <target state="translated">높은 오 탐지율이 허용되지 않는 응용 분야에서는 &lt;code&gt;max_fpr&lt;/code&gt; 의 &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; 매개 변수를 사용하여 ROC 곡선을 주어진 한도까지 요약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bbcc07c440f8193b3e7ecf64eaaca0e386adcbad" translate="yes" xml:space="preserve">
          <source>In bin edges for feature &lt;code&gt;i&lt;/code&gt;, the first and last values are used only for &lt;code&gt;inverse_transform&lt;/code&gt;. During transform, bin edges are extended to:</source>
          <target state="translated">기능 &lt;code&gt;i&lt;/code&gt; 의 &lt;code&gt;inverse_transform&lt;/code&gt; 가장자리 에서 첫 번째 값과 마지막 값은 inverse_transform 에만 사용됩니다 . 변환하는 동안 빈 가장자리는 다음과 같이 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="9d083c0b55e1a00133d4b27dd85f5ea26aca3922" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서 Jaccard 유사성 계수 점수는 분류 정확도와 같습니다.</target>
        </trans-unit>
        <trans-unit id="b0b30958f6975dadd3d7eaf24f5447254d56c629" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_similarity_score&lt;/code&gt; function.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서이 함수는 &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 함수와 같습니다.</target>
        </trans-unit>
        <trans-unit id="e731c9654f26a8d7255165d2c0d5f78e47c3bd9a" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equivalent to the &lt;code&gt;accuracy_score&lt;/code&gt;. It differs in the multilabel classification problem.</source>
          <target state="translated">이진 및 멀티 클래스 분류에서이 함수는 &lt;code&gt;accuracy_score&lt;/code&gt; _ 점수와 같습니다 . 다중 레이블 분류 문제가 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c4cb57bb3e2c0bb1485829473f6ca6362cee5b90" translate="yes" xml:space="preserve">
          <source>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; is always negative (since the signs disagree), implying &lt;code&gt;1 - margin&lt;/code&gt; is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">이진 클래스의 경우, y_true의 레이블이 +1 및 -1로 인코딩되었다고 가정하면 예측 실수가 발생하면 &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; 은 항상 음수이며 (부호가 일치하지 않기 때문에) &lt;code&gt;1 - margin&lt;/code&gt; 은 항상 1보다 큽니다. 따라서 누적 힌지 손실은 분류기에 의해 발생한 실수의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="f2c8a5d61695d64c32adbdec8051b4ecc7f404cb" translate="yes" xml:space="preserve">
          <source>In binary classification settings</source>
          <target state="translated">이진 분류 설정에서</target>
        </trans-unit>
        <trans-unit id="0e8524872beef3003e748e1d9b4f90c7ce280313" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with a tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="translated">두 경우 모두, 기준은 에포크 (epoch)에 의해 한 번 평가되며 기준이 행에서 &lt;code&gt;n_iter_no_change&lt;/code&gt; 시간을 개선하지 않으면 알고리즘이 중지됩니다 . 개선은 공차 &lt;code&gt;tol&lt;/code&gt; 을 사용하여 평가되며 , 최대 반복 횟수 &lt;code&gt;max_iter&lt;/code&gt; 후에 알고리즘이 중지됩니다 .</target>
        </trans-unit>
        <trans-unit id="f03bffae8069d1108a85252dc34a4485434865b8" translate="yes" xml:space="preserve">
          <source>In both cases, the kernel&amp;rsquo;s parameters are estimated using the maximum likelihood principle.</source>
          <target state="translated">두 경우 모두 커널의 매개 변수는 최대 우도 원칙을 사용하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="dc8004c8d5b437fc6c479e2e5882eb635fa97592" translate="yes" xml:space="preserve">
          <source>In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.</source>
          <target state="translated">아래의 두 가지 예에서, 주된 결과는 비 강건한 것으로 경험적 공분산 추정치가 이질적인 관측 구조에 의해 크게 영향을 받는다는 것입니다. 강력한 공분산 추정은 데이터 분포의 주 모드에 초점을 맞출 수 있지만, 데이터가 가우스 분포되어야한다는 가정을 고수하여 데이터 구조에 대한 편향된 추정치를 산출하지만 어느 정도는 정확합니다. One-Class SVM은 파라 메트릭 형태의 데이터 분포를 가정하지 않으므로 복잡한 데이터 형태를 훨씬 더 잘 모델링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7f2b051010be4e679b8556fa182a1724fa1e49b9" translate="yes" xml:space="preserve">
          <source>In case the file contains a pairwise preference constraint (known as &amp;ldquo;qid&amp;rdquo; in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</source>
          <target state="translated">파일에 쌍별 환경 설정 제한 조건 (svmlight 형식에서 &quot;qid&quot;라고 함)이 포함 된 경우 query_id 매개 변수가 True로 설정되어 있지 않으면 무시됩니다. 이러한 pairwise preference constraints는 pairwise loss 함수를 사용할 때 (일부 학습에서 문제 순위를 매기는 경우와 같이) 동일한 query_id 값을 가진 쌍만 고려되도록 샘플 조합을 제한하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b2d3bbc7ab1d1edcd850a10f6fb01a251c14a28b" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zero&amp;rsquo;s in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="translated">알 수없는 범주가 발견되면 (One-hot 인코딩에서 모두 0) 이 범주를 나타내는 데 &lt;code&gt;None&lt;/code&gt; 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ca499264726caa99e06bab43fc3d4644ea84df01" translate="yes" xml:space="preserve">
          <source>In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in &lt;code&gt;working_memory&lt;/code&gt;-sized chunks. If &lt;code&gt;reduce_func&lt;/code&gt; is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.</source>
          <target state="translated">페어 단위 거리 행렬을 모두 한 번에 저장할 필요가없는 경우 &lt;code&gt;working_memory&lt;/code&gt; 크기의 청크 에서 페어 단위 거리를 계산하는 데 사용됩니다 . 경우 &lt;code&gt;reduce_func&lt;/code&gt; 이 주어집니다, 각 덩어리에서 실행하고 반환 값은 목록, 배열 또는 희소 행렬로 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="261de18f8066fcaced5cb3f145cb26c170301e09" translate="yes" xml:space="preserve">
          <source>In cases where the data is not uniformly sampled, radius-based neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt;&lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt;&lt;/a&gt; can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;.</source>
          <target state="translated">데이터가 균일하게 샘플링되지 않은 경우 &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt; &lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt; 의&lt;/a&gt; 반경 기반 이웃 분류 가 더 나은 선택이 될 수 있습니다. 사용자는 고정 반경 \ (r \)을 지정하여 스파 서 이웃의 포인트가 분류에 가장 가까운 이웃을 사용하도록합니다. 고차원 매개 변수 공간의 경우이 방법은 소위 &quot;차원의 저주&quot;로 인해 덜 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="46149a533d1136e96a72fc2595f06ccb02814862" translate="yes" xml:space="preserve">
          <source>In certain cases Theil-Sen performs better than &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the &lt;code&gt;residual_threshold&lt;/code&gt; parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the &lt;code&gt;max_subpopulation&lt;/code&gt; parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset.</source>
          <target state="translated">어떤 경우에는 Theil-Sen이 &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; 보다 성능이 좋으며 이는 강력한 방법이기도합니다. 이것은 x 축 섭동 RANSAC에 대한 특이 치가있는 아래 두 번째 예에 설명되어 있습니다. 튜닝 &lt;code&gt;residual_threshold&lt;/code&gt; 의 RANSAC 요법이의하지만 데이터 및 필요한 이상치의 성격에 대한 일반적인 사전 지식이 매개 변수를. Theil-Sen의 계산상의 복잡성으로 인해 샘플 및 기능 수의 측면에서 작은 문제에 대해서만 사용하는 것이 좋습니다. 더 큰 문제의 경우 &lt;code&gt;max_subpopulation&lt;/code&gt; 파라미터는 p 개의 서브 샘플 포인트의 모든 가능한 조합의 크기를 무작위로 선택된 서브 세트로 제한하므로 런타임도 제한합니다. 따라서 Theil-Sen은 임의의 하위 집합에서 작동하기 때문에 일부 수학적 속성을 잃는 단점이있는 더 큰 문제에 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08403787ed9849b402f6d04f68a0bae46063dfaf" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="translated">&lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt; 과 달리 \ (w_ {i} \)의 각 좌표에는 자체 표준 편차 \ (\ lambda_i \)가 있습니다. 이전의 모든 \ (\ lambda_i \)는 하이퍼 파라미터 \ (\ lambda_1 \) 및 \ (\ lambda_2 \)에 의해 제공된 동일한 감마 분포로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="741dc2ca1b0b96b753a4293cdc66da483cb961b9" translate="yes" xml:space="preserve">
          <source>In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</source>
          <target state="translated">GridSearchCV와 달리 모든 매개 변수 값이 시도되는 것이 아니라 지정된 분포에서 고정 된 수의 매개 변수 설정이 샘플링됩니다. 시도 된 매개 변수 설정의 수는 n_iter에 의해 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="f7007cbebb915951a7329a621ec59e7bfd3c1528" translate="yes" xml:space="preserve">
          <source>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</source>
          <target state="translated">다수 투표 (하드 투표)와 달리 소프트 투표는 클래스 레이블을 예측 된 확률의 합의 argmax로 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a5222e41535c7e60d0bed8020d5a39a4cdb9c58d" translate="yes" xml:space="preserve">
          <source>In contrast to the original publication &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</source>
          <target state="translated">최초의 출판 &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt; 과는 달리, scikit-learn 구현은 각 분류자가 단일 클래스에 투표하는 대신 확률 론적 예측을 평균화하여 분류기를 결합합니다.</target>
        </trans-unit>
        <trans-unit id="092465bd0b61837459fb29bf14c2dda6ed20e949" translate="yes" xml:space="preserve">
          <source>In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">회귀 설정과 달리 가우시안 가능성은 이산 클래스 레이블에 부적합하기 때문에 GP의 경우에도 잠재 함수 \ (f \)의 후부가 가우시안이 아닙니다. 오히려, 로지스틱 링크 함수 (logit)에 대응하는 비 가우시안 가능성이 사용된다. GaussianProcessClassifier는 Laplace 근사법에 따라 가우시안이 아닌 후부를 근사합니다. 자세한 내용은 &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt; 3 장에서 확인할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2d7f12a42ea8277b24625f1aff8d53cb363a14e0" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{\text{n\_classes}}\).</source>
          <target state="translated">반대로, 분류자가 불균형 테스트 세트를 이용하기 때문에 기존 정확도가 우연 일 경우에만 균형 정확도가 \ (\ frac {1} {\ text {n \ _classes}} \로 떨어집니다. ).</target>
        </trans-unit>
        <trans-unit id="89611c1358b346353d5469c5670ea64fb02fcdd7" translate="yes" xml:space="preserve">
          <source>In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored using 10 fold cross validation, we see:</source>
          <target state="translated">내림차순으로, 30 개의 추정자를 사용하여 4 가지 기능 모두에 대해 훈련하고 (이 예제 외부) 10 배 교차 검증을 사용하여 점수를 매길 때,</target>
        </trans-unit>
        <trans-unit id="0732cca6c2251b860da4c331fa5748d479b14945" translate="yes" xml:space="preserve">
          <source>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</source>
          <target state="translated">앙상블 알고리즘에서 배깅 방법은 일련의 알고리즘 클래스를 구성하여 원래 트레이닝 세트의 임의의 서브 세트에 블랙 박스 추정기의 여러 인스턴스를 구축 한 다음 개별 예측을 집계하여 최종 예측을 형성합니다. 이 방법들은 임의의 구성 절차에 랜덤 화를 도입 한 후 그로부터 앙상블을 만들어서베이스 추정기 (예를 들어, 결정 트리)의 분산을 줄이는 방법으로 사용됩니다. 많은 경우 배깅 방법은 기본 알고리즘을 적용 할 필요없이 단일 모델과 관련하여 개선 할 수있는 매우 간단한 방법입니다. 과적 합을 줄이는 방법을 제공하므로 배깅 방법은 일반적으로 약한 모델 (예 :얕은 의사 결정 나무).</target>
        </trans-unit>
        <trans-unit id="5305d1e9b70806a8391e61e804a0df6abd8f6cc5" translate="yes" xml:space="preserve">
          <source>In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">이진 메트릭을 멀티 클래스 또는 멀티 라벨 문제로 확장 할 때 데이터는 각 클래스마다 하나씩 이진 문제 모음으로 처리됩니다. 클래스 집합에 걸쳐 이진 메트릭 계산을 평균화하는 여러 가지 방법이 있으며, 각 방법은 일부 시나리오에서 유용 할 수 있습니다. 가능한 경우 &lt;code&gt;average&lt;/code&gt; 매개 변수를 사용하여 이들 중에서 선택해야합니다 .</target>
        </trans-unit>
        <trans-unit id="e87cfc9dff0fe670bd40ebf7e26edaa15ca842ad" translate="yes" xml:space="preserve">
          <source>In extremely randomized trees (see &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</source>
          <target state="translated">매우 무작위 화 된 트리 ( &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt; 클래스 참조 )에서 분할은 계산 방식에서 한 단계 더 나아갑니다. 임의 포리스트에서와 같이 후보 기능의 임의의 하위 집합이 사용되지만 가장 차별화 된 임계 값을 찾는 대신 각 후보 기능에 대해 임계 값이 무작위로 생성되며 이러한 무작위로 생성 된 임계 값 중 최고가 분할 규칙으로 선택됩니다. 이것은 일반적으로 바이어스의 약간 더 큰 증가를 희생시키면서 모델의 분산을 조금 더 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5c1305e3ce4cbb99adc8d313e42a43efab81ea5c" translate="yes" xml:space="preserve">
          <source>In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:</source>
          <target state="translated">실제로이 데이터 세트에는 하나의 버전 만 있습니다. 반면 홍채 데이터 세트에는 여러 버전이 있습니다.</target>
        </trans-unit>
        <trans-unit id="63493dde535d33b43819cf48666bb2a9620c2476" translate="yes" xml:space="preserve">
          <source>In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">프랑스어로되어 있지만 참조 : Tenenhaus, M. (1998). La regression PLS : 이론과 실용. 파리 : 에디션 테크닉.</target>
        </trans-unit>
        <trans-unit id="6e95c3ada3b2525ed5f608da19594b4a42ad3dc4" translate="yes" xml:space="preserve">
          <source>In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:</source>
          <target state="translated">일반적으로 대량 예측 (많은 인스턴스가 동시에)을 수행하는 것은 여러 가지 이유 (분기 예측 성, CPU 캐시, 선형 대수 라이브러리 최적화 등)로 인해 더 효율적입니다. 여기서는 추정기 선택과 무관하게 벌크 모드가 항상 더 빠르며 일부 기능의 경우 1-2 배 정도 큰 기능이 거의없는 설정에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d5f14cdf8cb9c0df1b6ffce69bd866cdeffd9355" translate="yes" xml:space="preserve">
          <source>In general, a learning problem considers a set of n &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;samples&lt;/a&gt; of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;multivariate&lt;/a&gt; data), it is said to have several attributes or &lt;strong&gt;features&lt;/strong&gt;.</source>
          <target state="translated">일반적으로 학습 문제는 n 개의 데이터 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;샘플&lt;/a&gt; 집합을 고려한 다음 알 수없는 데이터의 속성을 예측하려고합니다. 각 표본이 단일 숫자 이상이고 다차원 항목 (일명 &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;다변량&lt;/a&gt; 데이터) 인 경우 여러 특성 또는 &lt;strong&gt;특징&lt;/strong&gt; 이 있다고 &lt;strong&gt;합니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="9cf7334c38597a2189c7af702ab9abdbe9f10093" translate="yes" xml:space="preserve">
          <source>In general, is a technique used for analyzing similarity or dissimilarity data. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="translated">일반적으로 유사성 또는 비 유사성 데이터를 분석하는 데 사용되는 기술입니다. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt; 는 형상 공간에서 거리로 유사성 또는 비 유사성 데이터를 모델링하려고 시도합니다. 데이터는 객체 간의 유사성, 분자의 상호 작용 빈도 또는 국가 간 거래 지수 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="71aab6786f00490669e72ac36911ce2d2486dab4" translate="yes" xml:space="preserve">
          <source>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</source>
          <target state="translated">일반적으로, 초기 관측 분포의 윤곽을 한정하는 거칠고 가까운 프론티어를 배우려고합니다. 그런 다음 추가 관측치가 경계로 구분 된 하위 공간 내에 있으면 초기 관측치와 동일한 모집단에서 온 것으로 간주됩니다. 그렇지 않은 경우, 그들이 국경을 벗어난 경우, 우리는 평가에 대한 확신을 가지고 비정상적이라고 말할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c9bca25ec918e4e036ec8a37ec502896ec56d542" translate="yes" xml:space="preserve">
          <source>In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;Compare the effect of different scalers on data with outliers&lt;/a&gt;.</source>
          <target state="translated">일반적으로 학습 알고리즘은 데이터 세트의 표준화를 통해 이점을 얻습니다. 세트에 일부 이상 치가있는 경우 강력한 스케일러 또는 변압기가 더 적합합니다. 한계 이상 값을 포함하는 데이터 세트에서 다양한 스케일러, 변환기 및 노멀 라이저의 동작은 데이터에 대한 &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;다른 스케일러의 효과와 특이 값 비교에서&lt;/a&gt; 강조 표시됩니다 .</target>
        </trans-unit>
        <trans-unit id="baeb2b7a43c2bc0dd04675c021d6ed663a58bf2d" translate="yes" xml:space="preserve">
          <source>In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in entropy. This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).</source>
          <target state="translated">일반적으로 균형 이진 트리를 구성하는 런타임 비용은 \ (O (n_ {samples} n_ {features} \ log (n_ {samples})) \)이고 쿼리 시간은 \ (O (\ log (n_ {samples) })) \). 트리 구성 알고리즘은 균형 트리를 생성하려고 시도하지만 항상 균형을 유지하는 것은 아닙니다. 하위 트리가 대략 균형을 유지한다고 가정하면 각 노드의 비용은 \ (O (n_ {features}) \)를 통해 검색하여 엔트로피를 가장 많이 줄인 기능을 찾습니다. 각 노드에서 \ (O (n_ {features} n_ {samples} \ log (n_ {samples})) \)의 비용이 발생하므로 전체 트리에 대한 총 비용이 발생합니다 (각 노드의 비용을 합산). \ (O (n_ {기능} n_ {samples} ^ {2} \ log (n_ {samples})) \)</target>
        </trans-unit>
        <trans-unit id="635895acc09f2d99381585bc2d144c9a66a85f3a" translate="yes" xml:space="preserve">
          <source>In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,</source>
          <target state="translated">경사 하강에서는 가중치에 대한 손실의 경사 \ (\ nabla Loss_ {W} \)가 계산되어 \ (W \)에서 차감됩니다. 보다 공식적으로 이것은 다음과 같이 표현됩니다.</target>
        </trans-unit>
        <trans-unit id="2c51a2af5a19ac0ce7e4fb04fd6d887c03b6fecb" translate="yes" xml:space="preserve">
          <source>In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data.</source>
          <target state="translated">고차원 공간에서 선형 분류기는 종종 우수한 정확도를 달성합니다. 희소 이진 데이터의 경우 BernoulliNB가 특히 적합합니다. 맨 아래 행은 변환 된 공간에서 BernoulliNB가 획득 한 의사 결정 경계를 원래 데이터에서 학습 한 ExtraTreesClassifier 포리스트와 비교합니다.</target>
        </trans-unit>
        <trans-unit id="7b577c96674cf299faa19ce0d11e2224d3c2c813" translate="yes" xml:space="preserve">
          <source>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</source>
          <target state="translated">다수결에서 특정 샘플에 대한 예측 등급 레이블은 각 개별 분류 기준에 의해 예측 된 등급 레이블의 다수 (모드)를 나타내는 등급 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="589394183aec0e7af2afe4b456559f6baedc9992" translate="yes" xml:space="preserve">
          <source>In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.</source>
          <target state="translated">따라서 대부분의 경우 기능 추출 코드를 신중하게 시간 지정하고 프로파일 링하는 것이 좋습니다. 전체 지연 시간이 애플리케이션에 비해 너무 느린 경우 최적화를 시작하기에 좋은 장소 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aeae04273a5ed1fc88f796de718e3c2190c04f0d" translate="yes" xml:space="preserve">
          <source>In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.</source>
          <target state="translated">많은 모델링 시나리오에서 데이터 세트의 기능의 정규성이 바람직합니다. 전력 변환은 분산을 안정화하고 왜도를 최소화하기 위해 모든 분포의 데이터를 가우시안 분포에 가깝게 매핑하는 것을 목표로하는 파라 메트릭 모노 토닉 변환 패밀리입니다.</target>
        </trans-unit>
        <trans-unit id="c82f65d47c3f4e11ad468a4165bdc787c51720a5" translate="yes" xml:space="preserve">
          <source>In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use &lt;code&gt;FeatureUnion&lt;/code&gt; to combine features obtained by PCA and univariate selection.</source>
          <target state="translated">많은 실제 사례에서 데이터 세트에서 기능을 추출하는 방법에는 여러 가지가 있습니다. 좋은 성능을 얻기 위해 여러 가지 방법을 결합하는 것이 종종 유리합니다. 이 예는 &lt;code&gt;FeatureUnion&lt;/code&gt; 을 사용 하여 PCA와 일 변량 선택에서 얻은 기능을 결합 하는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="9c0b7f3861d3fe001968b978c49f3447d1233fa3" translate="yes" xml:space="preserve">
          <source>In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</source>
          <target state="translated">수학에서 Johnson-Lindenstrauss의 정리는 고차원에서 저 차원 유클리드 공간으로 점의 왜곡이 적은 임베딩에 관한 결과입니다. 정리는 고차원 공간의 작은 점 집합이 점들 사이의 거리가 거의 보존되는 방식으로 훨씬 낮은 차원의 공간에 포함될 수 있다고 말합니다. 임베딩에 사용 된 맵은 적어도 Lipschitz이며 직교 투영으로도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="35a3805825da50966c5f8cb649b1d2ea852b8f59" translate="yes" xml:space="preserve">
          <source>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</source>
          <target state="translated">로그 우도를 최대화 할 때 양의 기울기는 모형이 관측 된 훈련 데이터와 호환되는 숨겨진 상태를 선호하게합니다. RBM의 이분 구조로 인해 효율적으로 계산할 수 있습니다. 그러나 음의 구배는 다루기 어렵습니다. 이 모델의 목표는 모델이 선호하는 조인트 상태의 에너지를 낮추어 데이터에 충실하게 유지하는 것입니다. 체인이 혼합 될 때까지 다른 주어진 \ (v \) 및 \ (h \) 각각을 반복적으로 샘플링하여 블록 Gibbs 샘플링을 사용하여 Markov 체인 Monte Carlo에 의해 근사화 될 수 있습니다. 이러한 방식으로 생성 된 샘플을 판타지 입자라고도합니다. 이는 비효율적이며 Markov 체인의 혼합 여부를 결정하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="54db7da5f1b2e2f16e8f4dc3a375dac661b78213" translate="yes" xml:space="preserve">
          <source>In multi-label classification, the &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function is extended by averaging over the labels as &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;.</source>
          <target state="translated">다중 레이블 분류에서 &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; 함수는 &lt;a href=&quot;#average&quot;&gt;위와 같이&lt;/a&gt; 레이블을 평균화하여 확장됩니다 .</target>
        </trans-unit>
        <trans-unit id="d9be5dcb267dcb84c278d12d7b1a881ada760886" translate="yes" xml:space="preserve">
          <source>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</source>
          <target state="translated">다중 레이블 분류에서 이는 각 레이블 집합을 올바르게 예측해야하는 각 샘플에 대해 필요하기 때문에 가혹한 메트릭 인 하위 집합 정확도입니다.</target>
        </trans-unit>
        <trans-unit id="9ff5420b9cd3095ee44bf9941c38c72dce6d517a" translate="yes" xml:space="preserve">
          <source>In multi-label settings</source>
          <target state="translated">다중 라벨 설정에서</target>
        </trans-unit>
        <trans-unit id="cf7a69d811fd496380ea6a3966d13bf17ca83f43" translate="yes" xml:space="preserve">
          <source>In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the &lt;code&gt;average&lt;/code&gt; argument to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; (multilabel only), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; functions, as described &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;. Note that if all labels are included, &amp;ldquo;micro&amp;rdquo;-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy. Also note that &amp;ldquo;weighted&amp;rdquo; averaging may produce an F-score that is not between precision and recall.</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 분류 작업에서 정밀도, 리콜 및 F 측정의 개념을 각 라벨에 독립적으로 적용 할 수 있습니다. 설명 된 것처럼 &lt;code&gt;average&lt;/code&gt; 인수에 의해 &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; (멀티 라벨 에만 해당), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt; 함수 에 대한 평균 인수로 지정된 레이블간에 결과를 결합하는 몇 가지 방법이 있습니다.&lt;a href=&quot;#average&quot;&gt; 위에서 것처럼&lt;/a&gt;. 모든 레이블이 포함 된 경우 멀티 클래스 설정에서 &quot;마이크로&quot;평균은 정확도와 동일한 정밀도, 리콜 및 \ (F \)를 생성합니다. 또한 &quot;가중&quot;평균화는 정밀도와 리콜 사이에 있지 않은 F- 점수를 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="afc91520f5287da47360dcd6fd00b4fb446bcf96" translate="yes" xml:space="preserve">
          <source>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer&amp;rsquo;s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">멀티 클래스의 경우, 함수는 모든 레이블이 y_true에 포함되거나 모든 레이블을 포함하는 선택적 레이블 인수가 제공 될 것으로 예상합니다. 다중 레이블 마진은 Crammer-Singer의 방법에 따라 계산됩니다. 이진 경우와 마찬가지로 누적 힌지 손실은 분류자가 실수 수의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="a7ec36140af641cfb5e4e5e11dec536798cfb2f8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss correspond to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function.</source>
          <target state="translated">멀티 클래스 분류에서 해밍 손실은 &lt;code&gt;y_true&lt;/code&gt; 와 &lt;code&gt;y_pred&lt;/code&gt; 사이의 해밍 거리에 해당하며 이는 하위 집합 &lt;code&gt;zero_one_loss&lt;/code&gt; 함수 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="ff1916ae5265c4d87d1472e5cc3e0c2594a22de8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is similar to the &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.</source>
          <target state="translated">멀티 클래스 분류에서 해밍 손실은 &lt;code&gt;y_true&lt;/code&gt; 와 &lt;code&gt;y_pred&lt;/code&gt; 사이의 해밍 거리에 해당 하며 &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; 함수 와 유사 합니다. 그러나 0 대 1 손실은 실제 세트와 정확히 일치하지 않는 예측 세트에 불이익을 주지만 해밍 손실은 개별 레이블에 불이익을줍니다. 따라서 0-1 손실로 상한 인 해밍 손실은 항상 0과 1 사이입니다. 진정한 라벨의 적절한 서브셋 또는 수퍼 셋을 예측하면 0에서 1 사이의 해밍 손실이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="cf7ce831a18d046dad4e38dc2cae92648b792778" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">다중 레이블 분류에서 &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt; 는 레이블이 예측과 정확히 일치하는 경우 하위 집합을 1로, 오류가있는 경우 0으로 점수를 매 깁니다 . 기본적으로이 함수는 불완전하게 예측 된 부분 집합의 백분율을 반환합니다. 대신 이러한 하위 집합의 수를 얻으려면 &lt;code&gt;normalize&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="2cdc777c3fd9aacea19e984339f1423c55608098" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.</source>
          <target state="translated">다중 레이블 분류에서 해밍 손실은 부분 집합 0 대 손실과 다릅니다. 일대일 손실은 주어진 샘플에 대한 전체 레이블 세트가 실제 레이블 세트와 완전히 일치하는 경우 올바르지 않은 것으로 간주합니다. 해밍 손실은 개별 레이블에 불이익을 준다는 점에서 더 관대합니다.</target>
        </trans-unit>
        <trans-unit id="00e9bece59054d08c4ac787e06eeb4fc8070bdab" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.</source>
          <target state="translated">다중 레이블 분류에서 함수는 부분 집합 정확도를 반환합니다. 표본에 대한 전체 예측 레이블 세트가 실제 레이블 세트와 완전히 일치하면 부분 집합 정확도는 1.0입니다. 그렇지 않으면 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="7cd1b88a6c55666089bdc7543f7e259d70d5898d" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.</source>
          <target state="translated">다중 레이블 분류에서 zero_one_loss 함수는 하위 집합의 0 대 1 손실에 해당합니다. 각 샘플에 대해 전체 레이블 세트를 정확하게 예측해야합니다. 그렇지 않으면 해당 샘플의 손실이 1과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c56a96e702a01557c0cb1c7c6c5d254cdaebcc8b" translate="yes" xml:space="preserve">
          <source>In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must &lt;em&gt;exactly&lt;/em&gt; match the corresponding set of labels in y_true.</source>
          <target state="translated">다중 레이블 분류에서이 함수는 부분 집합 정확도를 계산합니다. 샘플에 대해 예측 된 레이블 세트는 y_true의 해당 레이블 세트와 &lt;em&gt;정확히&lt;/em&gt; 일치 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="3fad4287dcc0210ad8169708b233947ca706f077" translate="yes" xml:space="preserve">
          <source>In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.</source>
          <target state="translated">멀티 라벨 학습에서, 각 샘플은 그와 관련된 다수의 그라운드 진실 라벨을 가질 수 있습니다. 목표는 사실 점수에 높은 점수와 더 나은 순위를 부여하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="9d6449537c42279d12e406059563c338784d06f3" translate="yes" xml:space="preserve">
          <source>In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.</source>
          <target state="translated">다중 레이블 학습에서 이진 분류 작업의 공동 세트는 레이블 이진 표시기 배열로 표현됩니다. 각 샘플은 이진 값을 갖는 2 차원 형상 배열 (n_samples, n_classes)의 한 행입니다. 즉, 0이 아닌 요소는 라벨의 부분 집합 &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; 와 같은 배열 은 첫 번째 샘플의 레이블 0, 두 번째 샘플의 레이블 1 및 2를 나타냅니다. , 세 번째 샘플에는 라벨이 없습니다.</target>
        </trans-unit>
        <trans-unit id="6c2c0f769c8a98dc6df3f2e7afe566ac80c0f339" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.</source>
          <target state="translated">일반적으로 Calinski-Harabaz 지수는 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="5f0c7d20ec265094d1673fd625fd38165b384452" translate="yes" xml:space="preserve">
          <source>In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:</source>
          <target state="translated">일반적인 사용법에서 Davies-Bouldin 지수는 다음과 같이 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="0488e7351783ef8ef785f4bdea49af8c75724adf" translate="yes" xml:space="preserve">
          <source>In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.</source>
          <target state="translated">일반적으로 Silhouette Coefficient는 군집 분석 결과에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="af7916eabb756a4304309b1e18ceea097a7a5071" translate="yes" xml:space="preserve">
          <source>In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as &amp;ldquo;Structured output&amp;rdquo; problems which are currently outside of the scope of scikit-learn.</source>
          <target state="translated">자연어 이해의 광범위한 과제를 해결하기 위해 문장과 단락의 현지 구조를 고려해야합니다. 따라서 이러한 많은 모델은 현재 scikit-learn의 범위를 벗어난 &quot;구조적 출력&quot;문제로 분류됩니다.</target>
        </trans-unit>
        <trans-unit id="819693d214fc959100941f9c2bf3cb570fc069ec" translate="yes" xml:space="preserve">
          <source>In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:</source>
          <target state="translated">이 문제를 해결하기 위해 scikit-learn은 다음과 같이 텍스트 내용에서 숫자 기능을 추출하는 가장 일반적인 방법에 대한 유틸리티를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="5bdd52099ccc039c40b609f18b326c63aea62fae" translate="yes" xml:space="preserve">
          <source>In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the &lt;code&gt;scipy.sparse&lt;/code&gt; package.</source>
          <target state="translated">이러한 행렬을 메모리에 저장하고 대수 연산 행렬 / 벡터의 속도를 높이기 위해 구현에서는 일반적으로 &lt;code&gt;scipy.sparse&lt;/code&gt; 패키지 에서 사용 가능한 구현과 같은 희소 표현을 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="b0bf98f40bc311f4824763dea8c552bc0812d861" translate="yes" xml:space="preserve">
          <source>In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; as demonstrated in the following example that extract &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; vectors of unigram tokens from a subset of 20news:</source>
          <target state="translated">텍스트 데이터로 예측 또는 군집 모델을 제공하려면 먼저 텍스트를 통계 분석에 적합한 숫자 값의 벡터로 변환해야합니다. 이는 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 의 서브 세트에서 유니 그램 토큰의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; 벡터 를 추출하는 다음 예에서 설명 된 sklearn.feature_extraction.text 유틸리티를 사용하여 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="a439a73e36b65ee0a94b3f1d9d89e3ac154697cf" translate="yes" xml:space="preserve">
          <source>In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:</source>
          <target state="translated">이 첫 번째 예제의 실행 시간을 단축하기 위해 데이터 세트에서 사용 가능한 20 개 중 4 개 범주 만 사용하는 부분 데이터 세트에 대해 작업합니다.</target>
        </trans-unit>
        <trans-unit id="da7edac191ef2f2a6bab6d167570c5dc3d626b83" translate="yes" xml:space="preserve">
          <source>In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.</source>
          <target state="translated">작은 데이터 세트에서 좋은 잠재 표현을 배우기 위해 각 방향으로 1 픽셀의 선형 이동으로 학습 데이터를 교란시켜 더 많은 레이블이 지정된 데이터를 인위적으로 생성합니다.</target>
        </trans-unit>
        <trans-unit id="6983d2c6ff1cbf277ea5d9522b128070bfd0a615" translate="yes" xml:space="preserve">
          <source>In order to make the vectorizer =&amp;gt; transformer =&amp;gt; classifier easier to work with, &lt;code&gt;scikit-learn&lt;/code&gt; provides a &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; class that behaves like a compound classifier:</source>
          <target state="translated">벡터 &lt;code&gt;scikit-learn&lt;/code&gt; =&amp;gt; 변환기 =&amp;gt; 분류기를 사용하기 쉽게 만들기 위해 scikit-learn 은 복합 분류기처럼 동작 하는 &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 클래스를 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="5257e11193f291f6f81d5d2347e3cbb71ec9f310" translate="yes" xml:space="preserve">
          <source>In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.</source>
          <target state="translated">텍스트 문서에 대한 기계 학습을 수행하려면 먼저 텍스트 내용을 숫자 피처 벡터로 바꿔야합니다.</target>
        </trans-unit>
        <trans-unit id="7b973d24b18f4331d1cc68b945953f9c40c766fe" translate="yes" xml:space="preserve">
          <source>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support &lt;code&gt;predict_proba&lt;/code&gt; method):</source>
          <target state="translated">예측 된 클래스 확률을 기반으로 클래스 레이블을 예측하려면 (VotingClassifier의 스키 킷 학습 추정기가 &lt;code&gt;predict_proba&lt;/code&gt; 메소드 를 지원해야합니다 .)</target>
        </trans-unit>
        <trans-unit id="a7ffbb7849ad7a74935991324e062c6b6722378d" translate="yes" xml:space="preserve">
          <source>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf&amp;ndash;idf transform.</source>
          <target state="translated">카운트 기능을 분류자가 사용하기에 적합한 부동 소수점 값으로 다시 가중하기 위해 tf-idf 변환을 사용하는 것이 매우 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="4707665df8a323c1a68b209bc6166b3798e4ea75" translate="yes" xml:space="preserve">
          <source>In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:</source>
          <target state="translated">이후 버전의 scikit-learn을 사용하여 유사한 모델을 재 구축하려면 추가 된 메타 데이터를 절인 모델과 함께 저장해야합니다.</target>
        </trans-unit>
        <trans-unit id="168239ecf279021917cbfef805f1d7d711ae1c44" translate="yes" xml:space="preserve">
          <source>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.</source>
          <target state="translated">분류 점수가 유의한지 테스트하기 위해, 라벨을 무작위 화, 순열 한 후 분류 절차를 반복하는 기술. 그런 다음 p- 값은 획득 한 점수가 처음에 얻은 분류 점수보다 큰 런의 백분율로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="fdc8e1656ba1332f0933f9f656403151b15252d2" translate="yes" xml:space="preserve">
          <source>In other words, return an input X_original whose transform would be X.</source>
          <target state="translated">즉, 변환이 X 인 입력 X_original을 리턴하십시오.</target>
        </trans-unit>
        <trans-unit id="f84fbaf022a2c87e2f72b92c7b8059751d7f8963" translate="yes" xml:space="preserve">
          <source>In other words, we &lt;em&gt;decomposed&lt;/em&gt; matrix \(\mathbf{X}\).</source>
          <target state="translated">즉, 행렬 \ (\ mathbf {X} \)를 &lt;em&gt;분해했습니다&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="573ad5780d66d8749d635925a4f90732aa002652" translate="yes" xml:space="preserve">
          <source>In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:</source>
          <target state="translated">특히 Rosenberg와 Hirschberg (2007)는 모든 클러스터 할당에 대해 다음 두 가지 바람직한 목표를 정의합니다.</target>
        </trans-unit>
        <trans-unit id="dafd8fff090495231531a6dce6a0d9bf23cd3c87" translate="yes" xml:space="preserve">
          <source>In particular in a &lt;strong&gt;supervised setting&lt;/strong&gt; it can be successfully combined with fast and scalable linear models to train &lt;strong&gt;document classifiers&lt;/strong&gt;, for instance:</source>
          <target state="translated">특히 &lt;strong&gt;감독 설정&lt;/strong&gt; 에서 &lt;strong&gt;문서 분류기&lt;/strong&gt; 를 훈련시키기 위해 빠르고 확장 가능한 선형 모델과 성공적으로 결합 될 수 있습니다 .</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
