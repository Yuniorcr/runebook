<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="d4e46a5f1bb78af76b3523ee33b19eb9c786ce08" translate="yes" xml:space="preserve">
          <source>A common practice for evaluating the results of image denoising is by looking at the difference between the reconstruction and the original image. If the reconstruction is perfect this will look like Gaussian noise.</source>
          <target state="translated">이미지 노이즈 제거 결과를 평가하는 일반적인 방법은 재구성과 원본 이미지의 차이를 살펴 보는 것입니다. 재구성이 완벽하다면 이것은 가우시안 잡음처럼 보일 것입니다.</target>
        </trans-unit>
        <trans-unit id="4ed0a0436668c33a0351f627732f0587ac7983ed" translate="yes" xml:space="preserve">
          <source>A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.</source>
          <target state="translated">합성 데이터 세트에 대한 scikit-learn의 여러 분류기 비교. 이 예의 요점은 다른 분류기의 결정 경계 특성을 설명하는 것입니다. 이러한 예에 의해 전달 된 직관이 반드시 실제 데이터 세트로 전달되는 것은 아니기 때문에 소금 한 알을 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="08896462e97fc3765d56051f9cf494df2d13585e" translate="yes" xml:space="preserve">
          <source>A comparison of different values for regularization parameter &amp;lsquo;alpha&amp;rsquo; on synthetic datasets. The plot shows that different alphas yield different decision functions.</source>
          <target state="translated">합성 데이터 세트에서 정규화 매개 변수 'alpha'에 대한 서로 다른 값의 비교. 플롯은 다른 알파가 다른 결정 함수를 생성 함을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="5ffdb4122629b0408fc648de5eb2e8856b001ee3" translate="yes" xml:space="preserve">
          <source>A comparison of the clustering algorithms in scikit-learn</source>
          <target state="translated">scikit-learn의 클러스터링 알고리즘 비교</target>
        </trans-unit>
        <trans-unit id="3aba9c608b9764b22ebdea01be3ad9e7396b0707" translate="yes" xml:space="preserve">
          <source>A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">scikit-learn의 이상치 탐지 알고리즘 비교. LOF (Local Outlier Factor)는 이상치 탐지에 사용될 때 새 데이터에 적용 할 예측 방법이 없으므로 결정 경계를 검은 색으로 표시하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d90a43f9bc5a8cfdf7c44591b758b4103cd4b78a" translate="yes" xml:space="preserve">
          <source>A complete example of this classification problem is available as an example that you can run and study: &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;Recognizing hand-written digits&lt;/a&gt;.</source>
          <target state="translated">이 분류 문제의 완전한 예는 다음을 실행하고 연구 할 수있는 예입니다 . &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;손으로 쓴 숫자 인식&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5fa8afb91b45355653b6f013837f985ec55a809c" translate="yes" xml:space="preserve">
          <source>A constant prediction baseline</source>
          <target state="translated">일정한 예측 기준</target>
        </trans-unit>
        <trans-unit id="4711e9024cec2c4a1383670cb0218b599982c096" translate="yes" xml:space="preserve">
          <source>A context object for caching a function&amp;rsquo;s return value each time it is called with the same input arguments.</source>
          <target state="translated">함수가 동일한 입력 인수로 호출 될 때마다 함수의 리턴 값을 캐시하기위한 컨텍스트 오브젝트입니다.</target>
        </trans-unit>
        <trans-unit id="198c09de283f0c6001f8ae5be18eb83356d81a3a" translate="yes" xml:space="preserve">
          <source>A contiguous slice of distance matrix, optionally processed by &lt;code&gt;reduce_func&lt;/code&gt;.</source>
          <target state="translated">선택적으로 &lt;code&gt;reduce_func&lt;/code&gt; 에 의해 처리되는 연속 거리 매트릭스 조각 .</target>
        </trans-unit>
        <trans-unit id="2ac4a68a26c3c2eae7c82c7be7376e0b1206d26c" translate="yes" xml:space="preserve">
          <source>A contingency matrix given by the &lt;code&gt;contingency_matrix&lt;/code&gt; function. If value is &lt;code&gt;None&lt;/code&gt;, it will be computed, otherwise the given value is used, with &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt; ignored.</source>
          <target state="translated">&lt;code&gt;contingency_matrix&lt;/code&gt; 함수가 제공하는 분할 행렬 . value가 &lt;code&gt;None&lt;/code&gt; 이면 계산되고, 그렇지 않으면 &lt;code&gt;labels_true&lt;/code&gt; 및 &lt;code&gt;labels_pred&lt;/code&gt; 가 무시 되고 지정된 값이 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="e9c011a87c5032bcc5be043ba28885f86dd1572a" translate="yes" xml:space="preserve">
          <source>A continuous log-uniform random variable is available through &lt;code&gt;loguniform&lt;/code&gt;. This is a continuous version of log-spaced parameters. For example to specify &lt;code&gt;C&lt;/code&gt; above, &lt;code&gt;loguniform(1,
100)&lt;/code&gt; can be used instead of &lt;code&gt;[1, 10, 100]&lt;/code&gt; or &lt;code&gt;np.logspace(0, 2,
num=1000)&lt;/code&gt;. This is an alias to SciPy&amp;rsquo;s &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html&quot;&gt;stats.reciprocal&lt;/a&gt;.</source>
          <target state="translated">연속 로그 균일 랜덤 변수는 &lt;code&gt;loguniform&lt;/code&gt; 을 통해 사용할 수 있습니다 . 이것은 로그 공간 매개 변수의 연속 버전입니다. 예를 들어 위의 &lt;code&gt;C&lt;/code&gt; 를 지정하려면 &lt;code&gt;[1, 10, 100]&lt;/code&gt; 또는 &lt;code&gt;np.logspace(0, 2, num=1000)&lt;/code&gt; 대신 &lt;code&gt;loguniform(1, 100)&lt;/code&gt; 사용할 수 있습니다 . 이것은 SciPy의 &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html&quot;&gt;stats.reciprocal&lt;/a&gt; 별칭 입니다.</target>
        </trans-unit>
        <trans-unit id="c54ec3e229cd991b7b39939aa120ad2133da3723" translate="yes" xml:space="preserve">
          <source>A copy of the &lt;code&gt;classes&lt;/code&gt; parameter where provided, or otherwise, the sorted set of classes found when fitting.</source>
          <target state="translated">제공된 경우 &lt;code&gt;classes&lt;/code&gt; 매개 변수 의 사본 또는 그렇지 않으면 정렬 된 클래스 세트.</target>
        </trans-unit>
        <trans-unit id="0460778fea705652baf3b5d397ac145e2c8d559f" translate="yes" xml:space="preserve">
          <source>A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.</source>
          <target state="translated">따라서, 문서 코퍼스는 문서 당 하나의 행과 코퍼스에서 발생하는 토큰 (예 : 단어) 당 하나의 열을 갖는 행렬로 표현 될 수있다.</target>
        </trans-unit>
        <trans-unit id="b1d91a72335bd05329f277d8169a200f3a7e7461" translate="yes" xml:space="preserve">
          <source>A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each training subset size and the test set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.</source>
          <target state="translated">교차 유효성 검사 생성기는 전체 데이터 집합을 교육 및 테스트 데이터에서 k 배로 나눕니다. 다양한 크기의 트레이닝 세트의 서브 세트가 추정기를 트레이닝하는 데 사용되며 각 트레이닝 서브 세트 크기에 대한 점수와 테스트 세트가 계산됩니다. 그 후, 점수는 각 훈련 서브 세트 크기에 대한 모든 k 런에 대해 평균화 될 것이다.</target>
        </trans-unit>
        <trans-unit id="2e60520122aededd78427759b8d5e2ce36a7e309" translate="yes" xml:space="preserve">
          <source>A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the &lt;code&gt;.data&lt;/code&gt; member, which is a &lt;code&gt;n_samples, n_features&lt;/code&gt; array. In the case of supervised problem, one or more response variables are stored in the &lt;code&gt;.target&lt;/code&gt; member. More details on the different datasets can be found in the &lt;a href=&quot;../../datasets/index#datasets&quot;&gt;dedicated section&lt;/a&gt;.</source>
          <target state="translated">데이터 세트는 데이터에 대한 모든 데이터와 일부 메타 데이터를 보유하는 사전과 유사한 객체입니다. 이 데이터는 &lt;code&gt;n_samples, n_features&lt;/code&gt; 배열 인 &lt;code&gt;.data&lt;/code&gt; 멤버에 저장됩니다 . 감독되는 문제의 경우 하나 이상의 응답 변수가 &lt;code&gt;.target&lt;/code&gt; 멤버에 저장됩니다 . 다른 데이터 세트에 대한 자세한 내용은 &lt;a href=&quot;../../datasets/index#datasets&quot;&gt;전용 섹션을 참조하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c689c278ed2c402419ef82b79e626634acb9d7dd" translate="yes" xml:space="preserve">
          <source>A dataset is uniquely specified by its &lt;code&gt;data_id&lt;/code&gt;, but not necessarily by its name. Several different &amp;ldquo;versions&amp;rdquo; of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; can yield different results at different times if earlier versions become inactive. You can see that the dataset with &lt;code&gt;data_id&lt;/code&gt; 40966 that we fetched above is the version 1 of the &amp;ldquo;miceprotein&amp;rdquo; dataset:</source>
          <target state="translated">데이터 세트는 &lt;code&gt;data_id&lt;/code&gt; 에 의해 고유하게 지정 되지만 반드시 이름에 의해서는 아닙니다. 완전히 다른 데이터 세트를 포함 할 수있는 동일한 이름을 가진 데이터 세트의 여러 버전이 존재할 수 있습니다. 특정 버전의 데이터 세트에 중대한 문제가있는 것으로 밝혀지면 비활성화 될 수 있습니다. 이름을 사용하여 데이터 세트를 지정하면 여전히 활성화 된 데이터 세트의 초기 버전이 생성됩니다. 즉, &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; 은 이전 버전이 비활성화되면 다른 시간에 다른 결과를 산출 할 수 있음을 의미합니다 . 위에서 가져온 &lt;code&gt;data_id&lt;/code&gt; 40966 이있는 데이터 세트가 &quot;미생물 단백질&quot;데이터 세트의 버전 1임을 알 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="015b04be42703b2ddb8ba533bada1a317c896a28" translate="yes" xml:space="preserve">
          <source>A decision tree classifier.</source>
          <target state="translated">의사 결정 트리 분류기.</target>
        </trans-unit>
        <trans-unit id="2630b9dcfdbd7604846f7a233491e190f45a9740" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="translated">의사 결정 트리는 가우시안 노이즈가 적은 1D 정현파 데이터 세트 에서 AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; 알고리즘을 사용하여 향상됩니다 . 299 개의 부스트 (300 개의 의사 결정 트리)가 단일 의사 결정 트리 회귀 자와 비교됩니다. 부스트의 수가 증가함에 따라 회귀자는 더 세부 사항에 맞출 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c1457482037e3da549c4d599a20c71477d87290a" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="translated">소량의 가우시안 잡음을 갖는 1D 정현파 데이터 세트 에서 AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 알고리즘을 사용하여 의사 결정 트리가 강화됩니다 . 299 개의 부스트 (300 개의 의사 결정 트리)가 단일 의사 결정 트리 회귀 자와 비교됩니다. 부스트 횟수가 증가함에 따라 회귀자가보다 상세하게 맞출 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="98423fef8caf500459be69ac8cff91bec1b557c8" translate="yes" xml:space="preserve">
          <source>A decision tree regressor.</source>
          <target state="translated">의사 결정 트리 회귀 기.</target>
        </trans-unit>
        <trans-unit id="fac54dbce7b4a9529cb6b07cf132d857e0fb7a2e" translate="yes" xml:space="preserve">
          <source>A demo of K-Means clustering on the handwritten digits data</source>
          <target state="translated">손으로 쓴 숫자 데이터에 대한 K- 평균 군집 데모</target>
        </trans-unit>
        <trans-unit id="985d8495d6de7661e6b4b5a0919641f87e6bc3f6" translate="yes" xml:space="preserve">
          <source>A demo of structured Ward hierarchical clustering on an image of coins</source>
          <target state="translated">동전 이미지에 구조화 된 Ward 계층 적 군집 데모</target>
        </trans-unit>
        <trans-unit id="728da4fce5aa5c78dc45e16c348781662170d8aa" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Biclustering algorithm</source>
          <target state="translated">Spectral Biclustering 알고리즘의 데모</target>
        </trans-unit>
        <trans-unit id="9435014b37ab8720ce8e8dead6831ddaa2609878" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Co-Clustering algorithm</source>
          <target state="translated">Spectral Co-Clustering 알고리즘 데모</target>
        </trans-unit>
        <trans-unit id="35f2c0aaf840683641795d09aa0a1e201f37f9de" translate="yes" xml:space="preserve">
          <source>A demo of the mean-shift clustering algorithm</source>
          <target state="translated">평균 이동 클러스터링 알고리즘 데모</target>
        </trans-unit>
        <trans-unit id="d3781231daa22a497eef6f674b9f3cd7b216bf2a" translate="yes" xml:space="preserve">
          <source>A demonstration of feature discretization on synthetic classification datasets. Feature discretization decomposes each feature into a set of bins, here equally distributed in width. The discrete values are then one-hot encoded, and given to a linear classifier. This preprocessing enables a non-linear behavior even though the classifier is linear.</source>
          <target state="translated">합성 분류 데이터 세트에 대한 특징 이산화 데모. 피처 이산화는 각 피처를 하나의 빈 세트로 분해합니다. 여기에서 너비는 동일하게 분산됩니다. 이산 값은 원-핫 인코딩되어 선형 분류기에 제공됩니다. 이 사전 처리는 분류 기가 선형 인 경우에도 비선형 동작을 가능하게합니다.</target>
        </trans-unit>
        <trans-unit id="95eced0791e7dfa2447624723a21d4f3bf963a74" translate="yes" xml:space="preserve">
          <source>A detailed description of the algorithm can be found in the documentation of the &lt;code&gt;linear_model&lt;/code&gt; sub-package.</source>
          <target state="translated">알고리즘에 대한 자세한 설명은 &lt;code&gt;linear_model&lt;/code&gt; 서브 패키지 의 문서에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85ef0d1a4425c2a7d76f392327f60a1b7513d0f1" translate="yes" xml:space="preserve">
          <source>A dict of arrays containing the score/time arrays for each scorer is returned. The possible keys for this &lt;code&gt;dict&lt;/code&gt; are:</source>
          <target state="translated">각 득점자에 대한 점수 / 시간 배열을 포함하는 배열의 dict가 반환됩니다. 이 &lt;code&gt;dict&lt;/code&gt; 의 가능한 키는 다음 과 같습니다.</target>
        </trans-unit>
        <trans-unit id="111521e1b62dc92a5d55cc421f659d37509b3d83" translate="yes" xml:space="preserve">
          <source>A dict with keys as column headers and values as columns, that can be imported into a pandas &lt;code&gt;DataFrame&lt;/code&gt;.</source>
          <target state="translated">팬더 &lt;code&gt;DataFrame&lt;/code&gt; 으로 가져올 수있는 키를 열 헤더로 사용하고 값을 열로 사용하는 dict .</target>
        </trans-unit>
        <trans-unit id="c8159c56c705a647167c0db6b7eec6aec52babc5" translate="yes" xml:space="preserve">
          <source>A dictionary mapping feature names to feature indices.</source>
          <target state="translated">피처 이름을 피처 인덱스에 매핑하는 사전입니다.</target>
        </trans-unit>
        <trans-unit id="d69a0d863960291435690723c91aee6afd547cfb" translate="yes" xml:space="preserve">
          <source>A dictionary of {dataset_name: data_dict}, or {dataset_name: (data_dict, ordering). &lt;code&gt;data_dict&lt;/code&gt; itself is a dictionary of {column_name: data_array}, and &lt;code&gt;ordering&lt;/code&gt; is a list of column_names to determine the ordering in the data set (see &lt;code&gt;fake_mldata&lt;/code&gt; for details).</source>
          <target state="translated">{dataset_name : data_dict} 또는 {dataset_name : (data_dict, 순서)의 사전입니다. &lt;code&gt;data_dict&lt;/code&gt; 자체는 {column_name : data_array}의 사전이며 &lt;code&gt;ordering&lt;/code&gt; 은 데이터 세트의 순서를 결정하기위한 column_name 목록입니다 ( &lt;code&gt;fake_mldata&lt;/code&gt; 참조) . 참조).</target>
        </trans-unit>
        <trans-unit id="f0df6656a799b1b490376f17c246c83878449378" translate="yes" xml:space="preserve">
          <source>A different approach for approximating an additive variant of the chi squared kernel.</source>
          <target state="translated">카이 제곱 커널의 부가 변형을 근사화하기위한 다른 접근법.</target>
        </trans-unit>
        <trans-unit id="183fe0e3f615bfbf70ad6cec7cbf647c2be26b19" translate="yes" xml:space="preserve">
          <source>A discrepancy between the number of terms reported for DictVectorizer and for FeatureHasher is to be expected due to hash collisions.</source>
          <target state="translated">해시 충돌로 인해 DictVectorizer와 FeatureHasher에 대해보고 된 용어 수의 불일치가 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="5359a8a92b18a35ca1595de7bb20b2a0c7e2882d" translate="yes" xml:space="preserve">
          <source>A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.</source>
          <target state="translated">Y가 없음 인 경우, D_ {i, j}가 주어진 행렬 X의 i 번째와 j 번째 벡터 사이의 거리 인 거리 행렬 D. Y가 없음이 아닌 경우 D_ {i, j}는 X에서 i 번째 배열과 Y에서 j 번째 배열 사이의 거리입니다.</target>
        </trans-unit>
        <trans-unit id="91104d203f36a74818ec42e8962e98d2f258acd0" translate="yes" xml:space="preserve">
          <source>A document is a sequence of \(N\) words.</source>
          <target state="translated">문서는 \ (N \) 단어의 시퀀스입니다.</target>
        </trans-unit>
        <trans-unit id="ce7ab60e9677f7708dd8a607942d2c0cccce5364" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if &lt;code&gt;metric='precomputed'&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;metric='precomputed'&lt;/code&gt; 인 경우 피처 배열 또는 샘플 간 거리 배열입니다 .</target>
        </trans-unit>
        <trans-unit id="b2db61a5e11f04d01b2535f105cf88de339d5444" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;</source>
          <target state="translated">특성 배열 또는 metric = 'precomputed'인 경우 샘플 간 거리 배열</target>
        </trans-unit>
        <trans-unit id="eb559e312d64d98c335a9c4be5e943c082d9b127" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">특성 배열 또는 metric = 'precomputed'인 경우 샘플 간의 거리 배열입니다.</target>
        </trans-unit>
        <trans-unit id="798664f5f45f763ed33c1b77b763944574aa4f4f" translate="yes" xml:space="preserve">
          <source>A few definitions:</source>
          <target state="translated">몇 가지 정의 :</target>
        </trans-unit>
        <trans-unit id="86908273dab820320f981714d4826279093534cf" translate="yes" xml:space="preserve">
          <source>A few definitions: a &lt;em&gt;claim&lt;/em&gt; is the request made by a policyholder to the insurer to compensate for a loss covered by the insurance. The &lt;em&gt;claim amount&lt;/em&gt; is the amount of money that the insurer must pay. The &lt;em&gt;exposure&lt;/em&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="translated">몇 가지 정의 : &lt;em&gt;청구&lt;/em&gt; 는 보험 계약자가 보험에 포함 된 손실을 보상하기 위해 보험사에 요청 &lt;em&gt;하는 것&lt;/em&gt; 입니다. &lt;em&gt;청구 금액은&lt;/em&gt; 보험 회사가 지불해야하는 돈의 양입니다. &lt;em&gt;노출은&lt;/em&gt; 몇 년 동안, 주어진 정책의 보험 기간입니다.</target>
        </trans-unit>
        <trans-unit id="24d02e76c875ea70e2d1746a2ae2b7543f98dedf" translate="yes" xml:space="preserve">
          <source>A few features available in this model:</source>
          <target state="translated">이 모델에서 사용 가능한 몇 가지 기능 :</target>
        </trans-unit>
        <trans-unit id="a596b46f1e97e6f01add8ad1641f2aa816638c4f" translate="yes" xml:space="preserve">
          <source>A figure object onto which the plots will be drawn, after the figure has been cleared. By default, a new one is created.</source>
          <target state="translated">그림이 지워진 후 플롯이 그려 질 그림 개체입니다. 기본적으로 새 항목이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="24026427c908afb044a16ec627f1f7bf5b040d61" translate="yes" xml:space="preserve">
          <source>A fitted estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;, or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;. Multioutput-multiclass classifiers are not supported.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt; , &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; 또는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function을&lt;/a&gt; 구현하는 피팅 된 추정기 객체 입니다. 다중 출력 다중 클래스 분류기는 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="06304e4581de12feb43d312c564ae9a70106b9ba" translate="yes" xml:space="preserve">
          <source>A fitted gradient boosting model.</source>
          <target state="translated">장착 된 그라디언트 부스팅 모델.</target>
        </trans-unit>
        <trans-unit id="ad78d890244343cb481ec0d46fb95ae01e19e178" translate="yes" xml:space="preserve">
          <source>A function to handle preprocessing, tokenization and n-grams generation.</source>
          <target state="translated">전처리, 토큰 화 및 n-gram 생성을 처리하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="5c6e87ebdac3e8a70f2ac508aaddfaa8df544da7" translate="yes" xml:space="preserve">
          <source>A function to preprocess the text before tokenization.</source>
          <target state="translated">토큰 화 전에 텍스트를 전처리하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="004f6b369bbbaaeac6e581b99b63af10868df162" translate="yes" xml:space="preserve">
          <source>A function to split a string into a sequence of tokens.</source>
          <target state="translated">문자열을 일련의 토큰으로 분할하는 함수입니다.</target>
        </trans-unit>
        <trans-unit id="70c3f4d0379ae967616e2ce28baef3bbea4be9cf" translate="yes" xml:space="preserve">
          <source>A generator over parameter settings, constructed from param_distributions.</source>
          <target state="translated">param_distributions로 구성된 매개 변수 설정에 대한 생성기입니다.</target>
        </trans-unit>
        <trans-unit id="ab5af31ec50522e8f84b5fd320b5185d1e649aae" translate="yes" xml:space="preserve">
          <source>A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning</source>
          <target state="translated">베이지안 방법에 대한 좋은 소개는 C. Bishop : Pattern Recognition and Machine learning에서 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="2abfd97b78d2efa9a468ce9e53e4e40c6a740a0c" translate="yes" xml:space="preserve">
          <source>A good value reported by this method does not imply the best information retrieval.</source>
          <target state="translated">이 방법으로보고 된 좋은 가치는 최상의 정보 검색을 의미하지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="ee0139a4c757902f8a15e776dbb86ec75b8b58ee" translate="yes" xml:space="preserve">
          <source>A graphical overview of basic areas of machine learning, and guidance which kind of algorithms to use in a given situation.</source>
          <target state="translated">기계 학습의 기본 영역에 대한 그래픽 개요 및 주어진 상황에서 사용할 알고리즘 종류에 대한 지침</target>
        </trans-unit>
        <trans-unit id="09adb6eadfd2176464d83487c19995d3ec4f7162" translate="yes" xml:space="preserve">
          <source>A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure:</source>
          <target state="translated">히스토그램은 구간이 정의되고 각 구간 내의 데이터 포인트 수가 계산 된 간단한 데이터 시각화입니다. 히스토그램의 예는 다음 그림의 왼쪽 상단 패널에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7bef6839991533f8c855219658630fb1d97c3e13" translate="yes" xml:space="preserve">
          <source>A kernel between the gene sequences is defined using R-convolution &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; by integrating a binary letter-wise kernel over all pairs of letters among a pair of strings.</source>
          <target state="translated">유전자 서열 사이의 커널은 한 쌍의 문자열 중 모든 문자 쌍에 대해 이진 문자 별 커널을 통합 하여 R-convolution &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; 을 사용하여 정의 됩니다.</target>
        </trans-unit>
        <trans-unit id="9d7d25ec6ddbab84b2f2e99ec27b1c5e75159286" translate="yes" xml:space="preserve">
          <source>A kernel hyperparameter&amp;rsquo;s specification in form of a namedtuple.</source>
          <target state="translated">명명 된 튜플 형태의 커널 하이퍼 파라미터 사양.</target>
        </trans-unit>
        <trans-unit id="9dd2e21d515f945fa766366930e3f2f884774833" translate="yes" xml:space="preserve">
          <source>A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y.</source>
          <target state="translated">Y가 없음 인 경우 K_ {i, j}가 주어진 행렬 X의 i 번째 벡터와 j 번째 벡터 사이의 커널이되도록하는 커널 행렬 K Y가 None이 아니면 K_ {i, j}는 X의 i 번째 배열과 Y의 j 번째 배열 사이의 커널입니다.</target>
        </trans-unit>
        <trans-unit id="1c832d0ad2e131abf1539cfeafad1048954835e4" translate="yes" xml:space="preserve">
          <source>A larger &lt;code&gt;leaf_size&lt;/code&gt; leads to a faster tree construction time, because fewer nodes need to be created</source>
          <target state="translated">&lt;code&gt;leaf_size&lt;/code&gt; 가 클수록 더 적은 수의 노드를 작성해야하므로 트리 구성 시간이 단축됩니다.</target>
        </trans-unit>
        <trans-unit id="5116c45ec5d86e0d701daa6fab158df33c292bf3" translate="yes" xml:space="preserve">
          <source>A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. &lt;code&gt;cv&lt;/code&gt; is not used for model evaluation but for prediction.</source>
          <target state="translated">훈련 샘플 수가 충분히 많으면 분할 수가 많을수록 이점이 없습니다. 실제로 훈련 시간이 늘어날 것입니다. &lt;code&gt;cv&lt;/code&gt; 는 모델 평가가 아니라 예측에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9f498b24974153e962b445930a2302cf78d89cfc" translate="yes" xml:space="preserve">
          <source>A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.</source>
          <target state="translated">마지막 주요 매개 변수는 대량 또는 한 번에 하나씩 모드에서 예측을 수행 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="b50d9ba875cfa3e1bbfa88757252d5cc5b663f5e" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM.</source>
          <target state="translated">학습 곡선은 다양한 수의 훈련 샘플에 대한 추정기의 검증 및 훈련 점수를 보여줍니다. 더 많은 훈련 데이터를 추가하여 얼마나 많은 이점을 얻고 추정자가 분산 오류 ​​또는 편향 오류로 인해 더 많은 고통을 받는지 알아 보는 도구입니다. 나이브 베이 즈 분류기와 SVM의 학습 곡선을 그리는 다음 예제를 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="7ca8b138a43ce55d825b3e612cada7898d228b4c" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.</source>
          <target state="translated">학습 곡선은 다양한 수의 훈련 샘플에 대한 추정기의 검증 및 훈련 점수를 보여줍니다. 더 많은 훈련 데이터를 추가하여 얻을 수있는 혜택의 양과 추정기가 분산 오차 또는 편향 오차로 인해 더 많은 고통을 받는지 알아내는 도구입니다. 검증 점수와 훈련 점수가 모두 훈련 세트의 크기가 커짐에 따라 너무 낮은 값으로 수렴하면 더 많은 훈련 데이터의 이점을 얻지 못할 것입니다. 다음 그림에서 예를 볼 수 있습니다. 순진 Bayes는 대략 낮은 점수로 수렴합니다.</target>
        </trans-unit>
        <trans-unit id="2cc6d3a8d1e5bdfb3693da69e5fd147a58a0255e" translate="yes" xml:space="preserve">
          <source>A list of arguments name to ignore in the hashing</source>
          <target state="translated">해싱에서 무시할 인수 이름 목록</target>
        </trans-unit>
        <trans-unit id="977999f316fb9d6c637d2852021b1d8363940c27" translate="yes" xml:space="preserve">
          <source>A list of arrays of length &lt;code&gt;len(estimators_)&lt;/code&gt; containing the class labels for each estimator in the chain.</source>
          <target state="translated">체인의 각 추정기에 대한 클래스 레이블을 포함하는 길이 &lt;code&gt;len(estimators_)&lt;/code&gt; 의 배열 목록입니다 .</target>
        </trans-unit>
        <trans-unit id="8b21b4fe65d64da1a4d1d84446f550bb55759446" translate="yes" xml:space="preserve">
          <source>A list of class labels known to the classifier.</source>
          <target state="translated">분류 자에게 알려진 클래스 레이블 목록입니다.</target>
        </trans-unit>
        <trans-unit id="c5eaed1c27ab8dfc53ee38efebd6c102a71098c5" translate="yes" xml:space="preserve">
          <source>A list of classes or column indices to select some (or to force inclusion of classes absent from the data)</source>
          <target state="translated">일부를 선택할 수있는 클래스 또는 열 인덱스 목록 (또는 데이터에없는 클래스를 강제로 포함)</target>
        </trans-unit>
        <trans-unit id="a8fb99b6c7b15c5d0f9079fcee9eb1e89cb6ab1c" translate="yes" xml:space="preserve">
          <source>A list of clones of base_estimator.</source>
          <target state="translated">base_estimator의 클론 목록입니다.</target>
        </trans-unit>
        <trans-unit id="53426e5188a28c01da334e8bfdbe7c9957c50862" translate="yes" xml:space="preserve">
          <source>A list of feature names.</source>
          <target state="translated">기능 이름 목록입니다.</target>
        </trans-unit>
        <trans-unit id="371683d834fcd55b1b47a55ccdff2980b0102eb8" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names (e.g., &amp;ldquo;f=ham&amp;rdquo; and &amp;ldquo;f=spam&amp;rdquo;).</source>
          <target state="translated">기능 이름이 포함 된 길이 n_features 목록 (예 : &quot;f = ham&quot;및 &quot;f = spam&quot;)</target>
        </trans-unit>
        <trans-unit id="3e9dc6d758a5816faa5f49abd255ba444796f1a7" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names. If None generic names will be used (&amp;ldquo;feature_0&amp;rdquo;, &amp;ldquo;feature_1&amp;rdquo;, &amp;hellip;).</source>
          <target state="translated">기능 이름을 포함하는 길이 n_features의 목록입니다. None 일반 이름이 사용되는 경우 ( &quot;feature_0&quot;, &quot;feature_1&quot;,&amp;hellip;).</target>
        </trans-unit>
        <trans-unit id="7951c01a96e3650048b499324860f7763c837156" translate="yes" xml:space="preserve">
          <source>A list of stop words.</source>
          <target state="translated">불용어 목록.</target>
        </trans-unit>
        <trans-unit id="c84a50cbe409f7bcbe9aef60e47724b3d0374c13" translate="yes" xml:space="preserve">
          <source>A logistic regression with L1 penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">L1 페널티가있는 로지스틱 회귀는 희소 모델을 생성하므로 &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1 기반 기능 선택에&lt;/a&gt; 자세히 설명 된대로 기능 선택을 수행하는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2e8f6767450aa8f965fa6ba15414ee47e53d061d" translate="yes" xml:space="preserve">
          <source>A logistic regression with \(\ell_1\) penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">\ (\ ell_1 \) 패널티가있는 로지스틱 회귀는 희소 모델을 생성하므로 &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1 기반 기능 선택에&lt;/a&gt; 자세히 설명 된대로 기능 선택을 수행하는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ed6a763f542c3b72714c079a2432308cb03a943b" translate="yes" xml:space="preserve">
          <source>A major difference is that GPR can choose the kernel&amp;rsquo;s hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions.</source>
          <target state="translated">가장 큰 차이점은 GPR이 한계 우도 함수에 대한 기울기 상승에 따라 커널의 하이퍼 파라미터를 선택할 수 있고 KRR은 교차 검증 된 손실 함수 (평균 제곱 오차 손실)에 대한 그리드 검색을 수행해야한다는 것입니다. 또 다른 차이점은 GPR이 목표 함수의 생성, 확률 론적 모델을 배우므로 예측과 함께 의미있는 신뢰 구간과 사후 샘플을 제공 할 수 있고 KRR은 예측 만 제공한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="413fd0e2f5ff27577a8397f2320f65ec5d748528" translate="yes" xml:space="preserve">
          <source>A major motivation of this method is F1-scoring, when the positive class is in the minority.</source>
          <target state="translated">이 방법의 주요 동기는 긍정적 클래스가 소수에있을 때 F1 스코어링입니다.</target>
        </trans-unit>
        <trans-unit id="ef86aa8565e00b57d2c91be8664b6a11d798a158" translate="yes" xml:space="preserve">
          <source>A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data.</source>
          <target state="translated">그러나 히스토그램의 주요 문제점은 비닝 선택이 결과 시각화에 불균형 한 영향을 줄 수 있다는 것입니다. 위 그림의 오른쪽 위 패널을 고려하십시오. 빈이 오른쪽으로 이동 된 상태에서 동일한 데이터에 대한 히스토그램을 보여줍니다. 두 시각화의 결과는 완전히 다르게 보이고 데이터에 대한 다른 해석으로 이어질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b4945c81d17c82b3985167f2fe21eac206551aeb" translate="yes" xml:space="preserve">
          <source>A mapping of terms to feature indices.</source>
          <target state="translated">용어를 피처 인덱스에 매핑</target>
        </trans-unit>
        <trans-unit id="d80b1965676bd061b195356f9e0d7d581d27bcda" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.</source>
          <target state="translated">보정 및 재가 중 전에 위치 및 모양에 대한 원시의 강력한 추정치를 계산하는 데 사용 된 관측치의 마스크</target>
        </trans-unit>
        <trans-unit id="0ec3f28dc02dc03ba22d583507907375d9c62b5f" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.</source>
          <target state="translated">다시 가중 된 견고한 위치 및 공분산 추정값을 계산하는 데 사용 된 관측치의 마스크입니다.</target>
        </trans-unit>
        <trans-unit id="adea06a15a23aac7c9d9327f52e8025add2d08fa" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the robust estimates of location and shape.</source>
          <target state="translated">위치 및 모양의 강력한 추정치를 계산하는 데 사용 된 관측치의 마스크입니다.</target>
        </trans-unit>
        <trans-unit id="8be3be97461376c787b398256771e5f8ab1ed15d" translate="yes" xml:space="preserve">
          <source>A matrix containing only 1s ands 0s.</source>
          <target state="translated">1과 0 만 포함하는 행렬.</target>
        </trans-unit>
        <trans-unit id="957a75d1eeb1c72e470b64334da958786ce3e979" translate="yes" xml:space="preserve">
          <source>A matrix of shape (n_samples, n_samples) will be created from this.</source>
          <target state="translated">이로부터 형태의 행렬 (n_samples, n_samples)이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="259daba30c783e777c062485d929bc6df3c76ec7" translate="yes" xml:space="preserve">
          <source>A matrix of term/token counts.</source>
          <target state="translated">용어 / 토큰 수의 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="151a7f165ba08edb6c7ba1bd07841827b86e653b" translate="yes" xml:space="preserve">
          <source>A matrix such that &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; iff &lt;code&gt;classes_[j]&lt;/code&gt; is in &lt;code&gt;y[i]&lt;/code&gt;, and 0 otherwise.</source>
          <target state="translated">매트릭스되도록 &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; IFF &lt;code&gt;classes_[j]&lt;/code&gt; 가 되어 &lt;code&gt;y[i]&lt;/code&gt; , 그렇지 않으면 0이.</target>
        </trans-unit>
        <trans-unit id="6ac1281f3fac07a890a7987a1940a19d553f0540" translate="yes" xml:space="preserve">
          <source>A model is trained using \(k-1\) of the folds as training data;</source>
          <target state="translated">폴드의 \ (k-1 \)를 훈련 데이터로 사용하여 모델을 훈련합니다.</target>
        </trans-unit>
        <trans-unit id="ca1ea4d381438e8c7ee31dbdc82c2c3f3b443e43" translate="yes" xml:space="preserve">
          <source>A more detailed summary of the search is available at &lt;code&gt;gs_clf.cv_results_&lt;/code&gt;.</source>
          <target state="translated">검색에 대한 자세한 요약은 &lt;code&gt;gs_clf.cv_results_&lt;/code&gt; 에서 볼 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0abf01b95edac52233877bfdb4963759d68aad5b" translate="yes" xml:space="preserve">
          <source>A more sophisticated approach is to use the &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output &lt;code&gt;y&lt;/code&gt; and the other feature columns are treated as inputs &lt;code&gt;X&lt;/code&gt;. A regressor is fit on &lt;code&gt;(X,
y)&lt;/code&gt; for known &lt;code&gt;y&lt;/code&gt;. Then, the regressor is used to predict the missing values of &lt;code&gt;y&lt;/code&gt;. This is done for each feature in an iterative fashion, and then is repeated for &lt;code&gt;max_iter&lt;/code&gt; imputation rounds. The results of the final imputation round are returned.</source>
          <target state="translated">보다 정교한 접근 방식은 &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;IterativeImputer&lt;/code&gt; &lt;/a&gt; 클래스 를 사용하는 것입니다.이 클래스는 누락 된 값이있는 각 기능을 다른 기능의 함수로 모델링하고 해당 추정치를 대치에 사용합니다. 반복 된 라운드 로빈 방식으로 수행합니다. 각 단계에서 특성 열은 출력 &lt;code&gt;y&lt;/code&gt; 로 지정되고 다른 특성 열은 입력 &lt;code&gt;X&lt;/code&gt; 로 처리됩니다 . 회귀 변수는 알려진 &lt;code&gt;y&lt;/code&gt; 에 대해 &lt;code&gt;(X, y)&lt;/code&gt; 에 적합합니다 . 그런 다음 회귀 변수를 사용하여 &lt;code&gt;y&lt;/code&gt; 의 결 측값을 예측합니다 . 이는 각 기능에 대해 반복적 인 방식으로 수행 된 다음 &lt;code&gt;max_iter&lt;/code&gt; 대치 라운드에 대해 반복됩니다 . 최종 대치 라운드의 결과가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e1a254f5635c860c720fd64108216f6d8c4b1064" translate="yes" xml:space="preserve">
          <source>A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional (l2-penalised) logistic regression model.</source>
          <target state="translated">입력 피처의 희소 한 서브 세트를 예측하는 더 전통적인 (그리고 아마도 더 나은) 방법은 일 변량 피처 선택에 이어 전통적인 (l2 처벌) 로지스틱 회귀 모델을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="fc3b3b1809b0fddd0d084276f73f16c23ecf93ac" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges binary classifiers into a chain.</source>
          <target state="translated">이진 분류기를 체인으로 배열하는 다중 레이블 모델입니다.</target>
        </trans-unit>
        <trans-unit id="07157eeb8d3c41fd6ae5f1ce4e96d98c52fa44ce" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges regressions into a chain.</source>
          <target state="translated">회귀를 체인으로 배열하는 다중 레이블 모델입니다.</target>
        </trans-unit>
        <trans-unit id="9e05b7bdec595d3e973dea3540db250a47de0ecd" translate="yes" xml:space="preserve">
          <source>A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;.</source>
          <target state="translated">다중 출력 문제는 예측할 여러 출력이있는 감독 학습 문제입니다. 즉 Y가 크기 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 의 2 차원 배열 인 경우입니다 .</target>
        </trans-unit>
        <trans-unit id="0d0f038bfe45a99c12fb61542c6de1b37df0ed3c" translate="yes" xml:space="preserve">
          <source>A new plotting API is available for creating visualizations. This new API allows for quickly adjusting the visuals of a plot without involving any recomputation. It is also possible to add different plots to the same figure. The following example illustrates &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt;, but other plots utilities are supported like &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt;. Read more about this new API in the &lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">시각화를 만드는 데 새로운 플로팅 API를 사용할 수 있습니다. 이 새로운 API를 사용하면 재 계산없이 플롯의 비주얼을 빠르게 조정할 수 있습니다. 동일한 그림에 다른 플롯을 추가 할 수도 있습니다. 다음 예제는 &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt; &lt;code&gt;plot_roc_curve&lt;/code&gt; &lt;/a&gt; 를 보여 주지만 &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt; &lt;code&gt;plot_precision_recall_curve&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt; &lt;code&gt;plot_confusion_matrix&lt;/code&gt; &lt;/a&gt; 와 같은 다른 플롯 유틸리티가 지원됩니다 . &lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;사용자 가이드&lt;/a&gt; 에서이 새로운 API에 대해 자세히 알아보세요 .</target>
        </trans-unit>
        <trans-unit id="df611f66ef7be3083c893378a9603e78fdf46201" translate="yes" xml:space="preserve">
          <source>A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated.</source>
          <target state="translated">CF 노드 인 CF 트리의 루트에 새로운 샘플이 삽입됩니다. 그런 다음 병합 후 반지름이 가장 작은 루트의 하위 클러스터와 병합되어 임계 값 및 분기 계수 조건에 의해 제한됩니다. 하위 클러스터에 하위 노드가있는 경우 리프에 도달 할 때까지이 작업이 반복됩니다. 리프에서 가장 가까운 서브 클러스터를 찾은 후이 서브 클러스터 및 부모 서브 클러스터의 속성이 재귀 적으로 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="06b4df58df49d31652c1e508653814db2c7c4b45" translate="yes" xml:space="preserve">
          <source>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</source>
          <target state="translated">이 분할로 불순물이이 값 이상으로 감소하면 노드가 분할됩니다.</target>
        </trans-unit>
        <trans-unit id="f2f0f4cbbbaee87b8b7904e37994705d9bd8774b" translate="yes" xml:space="preserve">
          <source>A noise-free case</source>
          <target state="translated">무소음 케이스</target>
        </trans-unit>
        <trans-unit id="a9448dea0ed4384de356358aa919a44c7d032339" translate="yes" xml:space="preserve">
          <source>A noisy case with known noise-level per datapoint</source>
          <target state="translated">데이터 포인트 당 알려진 노이즈 레벨의 노이즈 사례</target>
        </trans-unit>
        <trans-unit id="b96ec23e53414db9b11937cf799cd6c1f6432a32" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.</source>
          <target state="translated">음수가 아닌 부동 소수점 값 (최상의 값은 0.0) 또는 각 개별 대상에 대해 하나씩 부동 소수점 값의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="2ced62a814e481bd22a6aa7667bcac7ac64675cc" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0).</source>
          <target state="translated">음수가 아닌 부동 소수점 값 (최상의 값은 0.0)입니다.</target>
        </trans-unit>
        <trans-unit id="5ef179d616825b29b180ecdb28f9b1e0bfb64faf" translate="yes" xml:space="preserve">
          <source>A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</source>
          <target state="translated">분류에 사용되는 비모수 적지도 학습 방법입니다. 데이터 특성에서 추론 된 간단한 결정 규칙을 학습하여 대상 변수의 값을 예측하는 모델을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="e9c1cbeaf3f9c4d5469e9c590dbc955ea1a377cb" translate="yes" xml:space="preserve">
          <source>A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since &lt;code&gt;log2(n_classes)&lt;/code&gt; is much smaller than n_classes.</source>
          <target state="translated">0과 1 사이의 숫자는 나머지 하나보다 분류 기가 더 적습니다. 이론적으로 &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; 는 각 클래스를 명확하게 나타내 기에 충분합니다. 그러나 실제로 &lt;code&gt;log2(n_classes)&lt;/code&gt; 가 n_classes보다 훨씬 작기 때문에 정확도가 좋지 않을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="75b2970d65b634b9e917fd730d5f1ac86be5fb84" translate="yes" xml:space="preserve">
          <source>A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name &amp;ldquo;error-correcting&amp;rdquo;. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.</source>
          <target state="translated">1보다 큰 숫자는 나머지 하나보다 많은 분류 기가 필요합니다. 이 경우 일부 분류기는 이론상 다른 분류기의 실수를 수정하므로 이름은 &quot;오류 수정&quot;입니다. 그러나 분류기 실수가 일반적으로 상관되므로 실제로는 이런 일이 발생하지 않을 수 있습니다. 오류 수정 출력 코드는 가방과 유사한 효과를 갖습니다.</target>
        </trans-unit>
        <trans-unit id="303cdccb0a0b9ef8ee02f659c78579cbfb972892" translate="yes" xml:space="preserve">
          <source>A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">해당 유형의 객체는 각 그리드 포인트에 대해 인스턴스화됩니다. 이것은 scikit-learn 추정기 인터페이스를 구현하는 것으로 가정합니다. 평가자가 &lt;code&gt;score&lt;/code&gt; 함수 를 제공 하거나 &lt;code&gt;scoring&lt;/code&gt; 를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="56be8330a991d6aa887f39d43a91aa1ac6760e20" translate="yes" xml:space="preserve">
          <source>A one-dimensional array of distances</source>
          <target state="translated">거리의 1 차원 배열</target>
        </trans-unit>
        <trans-unit id="77da2f75f17e6ea0f8d3b4beab5a8c65d7801934" translate="yes" xml:space="preserve">
          <source>A paragraph describing the characteristic of the dataset: its source, reference, etc.</source>
          <target state="translated">데이터 세트의 특성을 설명하는 단락 : 소스, 참조 등</target>
        </trans-unit>
        <trans-unit id="8831c41b4fb10cdce1530c46ecbd5c89f650c66c" translate="yes" xml:space="preserve">
          <source>A parameter can be given to allow K-means to be run in parallel, called &lt;code&gt;n_jobs&lt;/code&gt;. Giving this parameter a positive value uses that many processors (default: 1). A value of -1 uses all available processors, with -2 using one less, and so on. Parallelization generally speeds up computation at the cost of memory (in this case, multiple copies of centroids need to be stored, one for each job).</source>
          <target state="translated">K- 평균을 &lt;code&gt;n_jobs&lt;/code&gt; 라고하는 병렬로 실행할 수 있도록 매개 변수를 제공 할 수 있습니다 . 이 매개 변수에 양수를 지정하면 많은 프로세서가 사용됩니다 (기본값 : 1). -1의 값은 사용 가능한 모든 프로세서를 사용하고 -2는 1을 덜 사용합니다. 병렬화는 일반적으로 메모리 비용으로 계산 속도를 높입니다 (이 경우 각 작업마다 하나씩 여러 개의 중심 사본을 저장해야 함).</target>
        </trans-unit>
        <trans-unit id="c76c7128eb10e519c4cde4416a2b5ebd1e864530" translate="yes" xml:space="preserve">
          <source>A plot that compares the various Beta-divergence loss functions supported by the Multiplicative-Update (&amp;lsquo;mu&amp;rsquo;) solver in &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; &lt;/a&gt; 의 Multiplicative-Update ( 'mu') 솔버가 지원하는 다양한 베타-분산 손실 함수를 비교하는 플롯입니다 .</target>
        </trans-unit>
        <trans-unit id="c428b2107a14d30575de38f7fc951c176630f469" translate="yes" xml:space="preserve">
          <source>A plot that compares the various convex loss functions supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; 가&lt;/a&gt; 지원하는 다양한 볼록 손실 함수를 비교하는 플롯입니다 .</target>
        </trans-unit>
        <trans-unit id="9c371e4431820ea4253b6da8f883638e2996e440" translate="yes" xml:space="preserve">
          <source>A plot will appear showing the top 5 most uncertain digits for each iteration of training. These may or may not contain mistakes, but we will train the next model with their true labels.</source>
          <target state="translated">각 훈련 반복마다 가장 불확실한 상위 5 자리를 보여주는 그림이 나타납니다. 여기에는 실수가 포함되거나 포함되지 않을 수 있지만 다음 레이블은 실제 모델로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="d0a927be776ddcc1a3b0a115e5ff9bf99815db03" translate="yes" xml:space="preserve">
          <source>A positive floating point value (the best value is 0.0).</source>
          <target state="translated">양의 부동 소수점 값 (최상의 값은 0.0)입니다.</target>
        </trans-unit>
        <trans-unit id="2e47d08f91be7ad2360165e81b098e1d32db277c" translate="yes" xml:space="preserve">
          <source>A positive monotonic constraint is a constraint of the form:</source>
          <target state="translated">양의 단조 제약은 다음 형식의 제약입니다.</target>
        </trans-unit>
        <trans-unit id="9fa1e2038dd5a4a82d42f5d5bcbe0df63da34a6c" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="translated">Lasso와 Ridge 간의 실질적인 이점은 Elastic-Net이 회전시 Ridge의 안정성 중 일부를 상속 할 수 있다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="fd73c2da7375a102550eb9b6f798b301111f7f0e" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="translated">Lasso와 Ridge 간의 트레이딩 오프의 실질적인 이점은 Elastic-Net이 회전시 Ridge의 안정성 중 일부를 상속 할 수 있다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="0644879ffb5a73e0cac4122d0a74647c32667b9b" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator object or a seed for it if int. If &lt;code&gt;init='random'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is used to initialize the random transformation. If &lt;code&gt;init='pca'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is passed as an argument to PCA when initializing the transformation. Pass an int for reproducible results across multiple function calls. See :term: &lt;code&gt;Glossary &amp;lt;random_state&amp;gt;&lt;/code&gt;.</source>
          <target state="translated">의사 난수 생성기 객체 또는 int 인 경우에 대한 시드입니다. 경우 &lt;code&gt;init='random'&lt;/code&gt; , &lt;code&gt;random_state&lt;/code&gt; 는 랜덤 변환을 초기화하는 데 사용된다. 경우 &lt;code&gt;init='pca'&lt;/code&gt; , &lt;code&gt;random_state&lt;/code&gt; 는 PCA의 인수로서 전달되는 변환을 초기화 할 때. 여러 함수 호출에서 재현 가능한 결과를 얻으려면 int를 전달하십시오. : term : &lt;code&gt;Glossary &amp;lt;random_state&amp;gt;&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="4370bf36aea658b96c83523c6e4601f6e89a2b33" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when &lt;code&gt;eigen_solver='amg'&lt;/code&gt; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;eigen_solver='amg'&lt;/code&gt; 일 때 lobpcg 고유 벡터 분해의 초기화 및 K- 평균 초기화에 사용되는 의사 난수 생성기 입니다. 임의성을 결정적으로 만들려면 int를 사용하십시오. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5ce137982e9454ec15b3a9266e29af4a861559f3" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">eigen_solver == 'amg'이고 K-Means 초기화에 의해 lobpcg 고유 벡터 분해의 초기화에 사용되는 의사 난수 생성기입니다. int를 사용하여 임의성을 결정 론적으로 만드십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="4da70ee0892b2024f8c0c33c157ae6f4ed05fab9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">eigen_solver == 'amg'일 때 lobpcg 고유 벡터 분해 초기화 및 K- 평균 초기화에 사용되는 의사 난수 생성기입니다. 임의성을 결정적으로 만들려면 int를 사용하십시오. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0c41ce7b7d007fb54c5cb75f35d5c711d3e908a9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">lobpcg 고유 벡터 분해의 초기화에 사용되는 의사 난수 생성기. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'amg'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="8d91d68f1ffad3cd242144da0cec28854cf478b1" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">lobpcg 고유 벡터의 초기화에 사용되는 의사 난수 생성기입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;solver&lt;/code&gt; == 'amg'일 때 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="ee2bf152e538d224d6fde70d9b0dd7a1cdbb5755" translate="yes" xml:space="preserve">
          <source>A random forest classifier.</source>
          <target state="translated">임의의 포리스트 분류기.</target>
        </trans-unit>
        <trans-unit id="e60aacdcc2887bf4d3963c64796dccc1910a3c9c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">랜덤 포레스트는 데이터 세트의 다양한 하위 샘플에 대한 여러 분류 의사 결정 트리에 맞는 메타 추정기이며 평균을 사용하여 예측 정확도를 개선하고 과적 합을 제어합니다. 하위 샘플 크기는 항상 원래 입력 샘플 크기와 동일하지만 &lt;code&gt;bootstrap=True&lt;/code&gt; (기본값) 인 경우 샘플이 대체로 그려집니다 .</target>
        </trans-unit>
        <trans-unit id="6d7fe13f3eb79eb708ba336fe6e8fed09167f0c0" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="translated">랜덤 포레스트는 데이터 세트의 다양한 하위 샘플에 대한 여러 분류 의사 결정 트리를 맞추고 평균을 사용하여 예측 정확도를 높이고 과적 합을 제어하는 ​​메타 추정기입니다. 하위 샘플 크기는 &lt;code&gt;bootstrap=True&lt;/code&gt; (기본값) 인 경우 &lt;code&gt;max_samples&lt;/code&gt; 매개 변수로 제어되며 , 그렇지 않으면 전체 데이터 세트가 각 트리를 빌드하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6afddbc046b0606e01c6a10f7c579e615a468bc2" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">랜덤 포레스트는 여러 의사 결정 트리 분류기에 데이터 집합의 다양한 하위 샘플에 적합하고 평균을 사용하여 예측 정확도를 개선하고 과적 합을 제어하는 ​​메타 추정기입니다. 하위 샘플 크기는 항상 원래 입력 샘플 크기와 동일하지만 &lt;code&gt;bootstrap=True&lt;/code&gt; (기본값) 인 경우 샘플이 대체로 그려집니다 .</target>
        </trans-unit>
        <trans-unit id="8049315f314116153326eb819ff277cc1b244e8c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="translated">랜덤 포레스트는 데이터 세트의 다양한 하위 샘플에 대한 여러 의사 결정 트리 분류기를 맞추고 평균을 사용하여 예측 정확도를 높이고 과적 합을 제어하는 ​​메타 추정기입니다. 하위 샘플 크기는 &lt;code&gt;bootstrap=True&lt;/code&gt; (기본값) 인 경우 &lt;code&gt;max_samples&lt;/code&gt; 매개 변수로 제어되며 , 그렇지 않으면 전체 데이터 세트가 각 트리를 빌드하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4c30f02c56b168ad378f278a145ee7ca666d2b4b" translate="yes" xml:space="preserve">
          <source>A random forest regressor.</source>
          <target state="translated">임의의 포리스트 회귀 기</target>
        </trans-unit>
        <trans-unit id="517674d6fed45c030e35daafc36d33d7e1cb17fe" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.</source>
          <target state="translated">난수 생성기의 상태를 정의하는 난수 생성기 인스턴스입니다. 정수가 제공되면 시드를 수정합니다. 전역 numpy 난수 생성기로 기본 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="ad42d75eb3232815220e50c42e13b1c5a583a979" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">난수 생성기의 상태를 정의하는 난수 생성기 인스턴스입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="282455615e66e12f3be6646a3e4853abf573194f" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;</source>
          <target state="translated">임의 순열 생성기의 상태를 정의하는 난수 생성기 인스턴스입니다. 여러 함수 호출에서 재현 가능한 출력을 위해 int를 전달합니다. &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;용어집&lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="3de0a58d62c3c2bcc234bd4e43f83d387ef4775e" translate="yes" xml:space="preserve">
          <source>A random order for each round.</source>
          <target state="translated">각 라운드에 대한 무작위 순서.</target>
        </trans-unit>
        <trans-unit id="4166975533e11bd5bc32126880dea04c49362bfe" translate="yes" xml:space="preserve">
          <source>A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</source>
          <target state="translated">Gunnar Martinsson 행렬, Vladimir Rokhlin 및 Mark Tygert 행렬 분해를위한 무작위 알고리즘</target>
        </trans-unit>
        <trans-unit id="1bef857d80e5f022acdf94565f064371e78877b1" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example showing the relevance of pixels in a digit classification task.</source>
          <target state="translated">숫자 분류 작업에서 픽셀의 관련성을 보여주는 재귀 기능 제거 예제입니다.</target>
        </trans-unit>
        <trans-unit id="59abf2d80f7d73a7cc0db592c8bcede424657b87" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.</source>
          <target state="translated">교차 유효성 검사로 선택한 기능 수를 자동으로 조정하는 재귀 기능 제거 예제입니다.</target>
        </trans-unit>
        <trans-unit id="dcc2f34db796875ac21f4e9f9639246afd97a032" translate="yes" xml:space="preserve">
          <source>A reference (and not a copy) of the first argument in the &lt;code&gt;fit()&lt;/code&gt; method is stored for future reference. If that array changes between the use of &lt;code&gt;fit()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; you will have unexpected results.</source>
          <target state="translated">&lt;code&gt;fit()&lt;/code&gt; 메소드 의 첫 번째 인수에 대한 참조 (복사본 아님)는 나중에 참조 할 수 있도록 저장됩니다. &lt;code&gt;fit()&lt;/code&gt; 과 &lt;code&gt;predict()&lt;/code&gt; 사용 사이에서 해당 배열이 변경되면 예기치 않은 결과가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d5fde252cc168f1a7ffd9faadab134fe0513cbda" translate="yes" xml:space="preserve">
          <source>A regressor which will be used to combine the base estimators. The default regressor is a &lt;code&gt;RidgeCV&lt;/code&gt;.</source>
          <target state="translated">기본 추정량을 결합하는 데 사용되는 회귀 변수입니다. 기본 회귀 변수는 &lt;code&gt;RidgeCV&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="b815536c2d161fd6acf2bcd856fde57736561c74" translate="yes" xml:space="preserve">
          <source>A representation of the full diabetes dataset would involve 11 dimensions (10 feature dimensions and one of the target variable). It is hard to develop an intuition on such representation, but it may be useful to keep in mind that it would be a fairly &lt;em&gt;empty&lt;/em&gt; space.</source>
          <target state="translated">전체 당뇨병 데이터 세트의 표현은 11 개의 차원 (10 개의 특징 차원 및 목표 변수 중 하나)을 포함 할 것이다. 그러한 표현에 대한 직관을 개발하는 것은 어렵지만, 그것이 &lt;em&gt;빈&lt;/em&gt; 공간 이라는 것을 명심하는 것이 유용 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="fb7d808e9a808e07d251ec0ab6593856a70a9b8b" translate="yes" xml:space="preserve">
          <source>A scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">서명 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 있는 스코어러 호출 가능 객체 / 함수 .</target>
        </trans-unit>
        <trans-unit id="cd458b3d90a800480ecc75cb9b07374a934f767b" translate="yes" xml:space="preserve">
          <source>A search consists of:</source>
          <target state="translated">검색은 다음으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="4faa007b7ff82792cbf9f6b91017a8816af3cc88" translate="yes" xml:space="preserve">
          <source>A second feature array only if X has shape [n_samples_a, n_features].</source>
          <target state="translated">X의 모양이 [n_samples_a, n_features] 인 경우에만 두 번째 형상 배열입니다.</target>
        </trans-unit>
        <trans-unit id="b3743bb79cbeb6292d7788299c27fa0302da1677" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to exclude. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">제외 할 dtype 선택. 자세한 내용은 &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt; &lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="9ed7392ea2bcc5742fc5a21d5c3ccda544426c7d" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to include. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">포함 할 dtype의 선택. 자세한 내용은 &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt; &lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="ee9834606bdd77c4a06472befa3674177dd91a7e" translate="yes" xml:space="preserve">
          <source>A seq of Axis objects, one for each subplot.</source>
          <target state="translated">서브 플롯마다 하나씩 Axis 객체의 시퀀스.</target>
        </trans-unit>
        <trans-unit id="09a68a5ad2629329af5cff530eb7ba2ddf7ad320" translate="yes" xml:space="preserve">
          <source>A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below.</source>
          <target state="translated">일련의 dicts는 검색 할 일련의 그리드를 나타내며, 의미가 없거나 효과가없는 매개 변수 조합 탐색을 피하는 데 유용합니다. 아래 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5ad6cb6da544247e185919510fbbf9bbac2dfc37" translate="yes" xml:space="preserve">
          <source>A set of labels (any orderable and hashable object) for each sample. If the &lt;code&gt;classes&lt;/code&gt; parameter is set, &lt;code&gt;y&lt;/code&gt; will not be iterated.</source>
          <target state="translated">각 샘플에 대한 레이블 세트 (주문 가능 및 해시 가능 객체). 경우] &lt;code&gt;classes&lt;/code&gt; 매개 변수를 설정하고, &lt;code&gt;y&lt;/code&gt; 반복되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7b19a5a4863d81ac862d0e9be41e4683721f5dd5" translate="yes" xml:space="preserve">
          <source>A similar clustering at multiple values of eps. Our implementation is optimized for memory usage.</source>
          <target state="translated">여러 eps 값에서 유사한 클러스터링. 우리의 구현은 메모리 사용에 최적화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="eb593a41404fe5b14fb797c942b194b27f9badb7" translate="yes" xml:space="preserve">
          <source>A similar clustering for a specified neighborhood radius (eps). Our implementation is optimized for runtime.</source>
          <target state="translated">지정된 인접 반경 (eps)에 대한 유사한 클러스터링입니다. 우리의 구현은 런타임에 최적화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="134a4703b97e34542f6c5ec20a8b6025ee87dd8f" translate="yes" xml:space="preserve">
          <source>A simple choice to construct \(R_ij\) so that it is nonnegative and symmetric is:</source>
          <target state="translated">음이 아니고 대칭이되도록 \ (R_ij \)를 구성하는 간단한 선택은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="11b768780ec203929a5802877f9f67a2310663fd" translate="yes" xml:space="preserve">
          <source>A simple example shipped with scikit-learn: iris dataset</source>
          <target state="translated">scikit-learn과 함께 제공되는 간단한 예 : iris dataset</target>
        </trans-unit>
        <trans-unit id="8e839688e01ba01e9dfe145feb09335f1aa963d0" translate="yes" xml:space="preserve">
          <source>A simple example:</source>
          <target state="translated">간단한 예 :</target>
        </trans-unit>
        <trans-unit id="2a7be91a45219aa7eb5e6e5584cdc7ab00269326" translate="yes" xml:space="preserve">
          <source>A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point and click and visualize the decision region induced by different kernels and parameter settings.</source>
          <target state="translated">Libsvm의 간단한 그래픽 프론트 엔드는 주로 교훈적인 목적으로 사용되었습니다. 포인트별로 데이터 포인트를 생성하고 클릭하여 다른 커널 및 매개 변수 설정으로 인한 의사 결정 영역을 시각화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08a70c5b3e130b5b3a3556a3a9d846d499b809af" translate="yes" xml:space="preserve">
          <source>A simple linear generative model with Gaussian latent variables.</source>
          <target state="translated">가우스 잠재 변수가있는 간단한 선형 생성 모델.</target>
        </trans-unit>
        <trans-unit id="7ed72a91acd5901b2715fbe7249ca1b4b882cab1" translate="yes" xml:space="preserve">
          <source>A simple one-dimensional regression example computed in two different ways:</source>
          <target state="translated">간단한 1 차원 회귀 예제는 두 가지 방식으로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="d0a2166f9acbabe38964ccee31d9e7127ebb3bd0" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms.</source>
          <target state="translated">군집 및 분류 알고리즘을 시각화하는 간단한 장난감 데이터 집합입니다.</target>
        </trans-unit>
        <trans-unit id="47127c0ac17be6b42bd52820b6c53e975ec33072" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms. Read more in the &lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">군집 및 분류 알고리즘을 시각화하는 간단한 장난감 데이터 집합입니다. &lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;사용자 안내서&lt;/a&gt; 에서 자세한 내용을 읽으십시오 .</target>
        </trans-unit>
        <trans-unit id="0963c917bb15b907590115ef7d8ee882d54970b8" translate="yes" xml:space="preserve">
          <source>A single str (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="translated">테스트 세트에 대한 예측을 평가하기 위한 단일 str ( &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;점수 매개 변수 : 모델 평가 규칙 &lt;/a&gt;&lt;a href=&quot;../model_evaluation#scoring&quot;&gt;정의&lt;/a&gt; 참조) 또는 콜 러블 ( 메트릭 함수에서 스코어링 전략 정의 참조 ).</target>
        </trans-unit>
        <trans-unit id="6cc426ce246dcb8426ecbbb6a60a4ff7a07e84ce" translate="yes" xml:space="preserve">
          <source>A single string (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="translated">테스트 집합에 대한 예측을 평가하기 위한 단일 문자열 ( &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;점수 매개 변수 : 모델 평가 규칙 &lt;/a&gt;&lt;a href=&quot;../model_evaluation#scoring&quot;&gt;정의&lt;/a&gt; 참조) 또는 호출 가능 ( 메트릭 함수에서 점수 전략 정의 참조 )</target>
        </trans-unit>
        <trans-unit id="2e53707c3f177aad6668f2e995ecf10221437949" translate="yes" xml:space="preserve">
          <source>A small value of &lt;code&gt;C&lt;/code&gt; includes more/all the observations, allowing the margins to be calculated using all the data in the area.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 의 작은 값 에는 더 많은 / 모든 관측치가 포함되므로 해당 영역의 모든 데이터를 사용하여 여백을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e33601e97cbb479e0128e302bee052e1e2030a89" translate="yes" xml:space="preserve">
          <source>A solution in high-dimensional statistical learning is to &lt;em&gt;shrink&lt;/em&gt; the regression coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; regression:</source>
          <target state="translated">고차원 통계 학습의 해결책 은 회귀 계수를 0 으로 &lt;em&gt;축소&lt;/em&gt; 하는 것입니다. 무작위로 선택된 두 관측 값 세트는 서로 관련이 없을 수 있습니다. 이것을 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 라고합니다 회귀 .</target>
        </trans-unit>
        <trans-unit id="dcd1f54984c6dffbbcff2667b694f4185f6ec040" translate="yes" xml:space="preserve">
          <source>A solution to this problem is a procedure called &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;cross-validation&lt;/a&gt; (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called &lt;em&gt;k&lt;/em&gt;-fold CV, the training set is split into &lt;em&gt;k&lt;/em&gt; smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the &lt;em&gt;k&lt;/em&gt; &amp;ldquo;folds&amp;rdquo;:</source>
          <target state="translated">이 문제에 대한 해결책은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;교차 검증&lt;/a&gt; (CV) 이라고하는 절차 입니다. 최종 평가를 위해 테스트 세트를 계속 유지해야하지만 CV를 수행 할 때 더 이상 유효성 검증 세트가 필요하지 않습니다. &lt;em&gt;k-&lt;/em&gt; 폴드 CV 라고하는 기본 접근법 에서 훈련 세트는 &lt;em&gt;k 개의&lt;/em&gt; 작은 세트 로 나뉩니다 (다른 접근법은 아래에 설명되어 있지만 일반적으로 동일한 원칙을 따릅니다). &lt;em&gt;k 개의&lt;/em&gt; 각 겹 에 대해 다음 절차를 따릅니다 .</target>
        </trans-unit>
        <trans-unit id="538e1506057d7a3220a186af08ab5489676c59df" translate="yes" xml:space="preserve">
          <source>A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with &lt;code&gt;metric='precomputed'&lt;/code&gt;. See &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">희소 반경 근방 그래프 (누락 된 항목이 eps를 벗어난 것으로 추정되는 경우)는 메모리 효율적인 방식으로 사전 계산 될 수 있으며 &lt;code&gt;metric='precomputed'&lt;/code&gt; 를 사용 하여 dbscan을 실행할 수 있습니다 . &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5434739c8a85282e43e6d41eec6550c57895597f" translate="yes" xml:space="preserve">
          <source>A star marks the expected sample for each class; its size reflects the probability of selecting that class label.</source>
          <target state="translated">별표는 각 클래스의 예상 샘플을 표시합니다. 크기는 해당 클래스 레이블을 선택할 가능성을 반영합니다.</target>
        </trans-unit>
        <trans-unit id="a226eb090c390674deacec5886c6aefd3ca76b60" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; which should return only a single value.</source>
          <target state="translated">str (모델 평가 문서 참조) 또는 단일 값만 반환해야하는 서명 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 가 있는 득점자 호출 가능 객체 / 함수 .</target>
        </trans-unit>
        <trans-unit id="7313aa5f13cd15f69aa6d50563adc57324fb0eb6" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">str (모델 평가 문서 참조) 또는 서명 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 하는 득점자 호출 가능 객체 / 함수 .</target>
        </trans-unit>
        <trans-unit id="d85d389496ba61c19ce84f015d0fceafa4c84844" translate="yes" xml:space="preserve">
          <source>A str, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="translated">'2 * n_jobs'에서와 같이 n_jobs의 함수로 표현식을 제공하는 str</target>
        </trans-unit>
        <trans-unit id="31ecf6c64ee79cd0b8c2879dbc516ad7450905ee" translate="yes" xml:space="preserve">
          <source>A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.</source>
          <target state="translated">라운드 로빈 방식으로 다른 기능의 기능으로 결 측값이있는 각 기능을 모델링하여 결 측값을 대치하는 전략입니다.</target>
        </trans-unit>
        <trans-unit id="9cc68f40152c74e41efd28d7122ce96e71e57739" translate="yes" xml:space="preserve">
          <source>A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task.</source>
          <target state="translated">코어 외부 스케일링을 구현하는 전략은 미니 배치로 데이터를 추정기로 스트리밍하는 것입니다. 각 미니 배치는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 를&lt;/a&gt; 사용하여 벡터화 되어 추정기의 입력 공간이 항상 동일한 차원 을 갖도록 합니다. 따라서 언제든지 사용되는 메모리 양은 미니 배치 크기에 의해 제한됩니다. 이러한 접근 방식을 사용하여 수집 할 수있는 데이터의 양에는 제한이 없지만 실제 관점에서 학습 시간은 종종 작업에 소비하려는 CPU 시간에 의해 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="7c0e35a146ea0efee84f42e6ee454306c5da4827" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">문자열 (모델 평가 설명서 참조) 또는 시그니처와 기록원 호출 객체 / 함수 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fad6fe3853f8abecb73cc5e995f9131ee653eef0" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. For a list of scoring functions that can be used, look at &lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt;. The default scoring option used is &amp;lsquo;accuracy&amp;rsquo;.</source>
          <target state="translated">문자열 (모델 평가 설명서 참조) 또는 시그니처와 기록원 호출 객체 / 함수 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; . 사용할 수있는 스코어링 기능 목록은 &lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 를 참조하십시오.&lt;/a&gt; . 기본 점수 매기기 옵션은 '정확도'입니다.</target>
        </trans-unit>
        <trans-unit id="1963bcb955646804f10ec7ad2d71cbad666ea477" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. If None, the negative mean squared error if cv is &amp;lsquo;auto&amp;rsquo; or None (i.e. when using generalized cross-validation), and r2 score otherwise.</source>
          <target state="translated">문자열 (모델 평가 문서 참조) 또는 서명 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 하는 Scorer 호출 가능 객체 / 함수 . None이면 cv가 'auto'또는 None (예 : 일반화 된 교차 검증을 사용할 때)이면 음의 제곱 오차를 의미하고, 그렇지 않으면 r2 점수입니다.</target>
        </trans-unit>
        <trans-unit id="e94d51ba3f9b9473065d98a0a11ca48ef9ac3a04" translate="yes" xml:space="preserve">
          <source>A string of unicode symbols.</source>
          <target state="translated">유니 코드 기호의 문자열입니다.</target>
        </trans-unit>
        <trans-unit id="93b243cc3849b7302c5dad12c7987dfe501650ad" translate="yes" xml:space="preserve">
          <source>A string, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="translated">'2 * n_jobs'에서와 같이 n_jobs의 함수로 표현식을 제공하는 문자열</target>
        </trans-unit>
        <trans-unit id="82f0cd7ca226a86963abc9437ca213dfd9c32ccc" translate="yes" xml:space="preserve">
          <source>A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):</source>
          <target state="translated">하위 파이프 라인은 목록이나 문자열과 같은 Python 시퀀스에 일반적으로 사용되는 슬라이싱 표기법을 사용하여 추출 할 수도 있습니다 (단 1 단계 만 허용됨). 이는 일부 변환 (또는 그 역) 만 수행하는 데 편리합니다.</target>
        </trans-unit>
        <trans-unit id="84dd7b04896fc19b0373bd22f46ebced4bcbae51" translate="yes" xml:space="preserve">
          <source>A supervised learning estimator with a &lt;code&gt;fit&lt;/code&gt; method that provides information about feature importance either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute.</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 속성 또는 &lt;code&gt;feature_importances_&lt;/code&gt; 를 통해 기능 중요도에 대한 정보를 제공 하는 &lt;code&gt;fit&lt;/code&gt; 방법이 있는지도 학습 학습 추정기 속성 .</target>
        </trans-unit>
        <trans-unit id="be778589a42569621d412c8d143e2c4ecab0280f" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.</source>
          <target state="translated">서포트 벡터 머신은 분류, 회귀 또는 기타 작업에 사용될 수있는 높거나 무한한 차원 공간에서 하이퍼 평면 또는 하이퍼 평면 세트를 구성합니다. 직관적으로, 모든 클래스의 가장 가까운 훈련 데이터 포인트 (소위 기능적 마진)까지의 거리가 가장 큰 하이퍼 플레인에 의해 우수한 분리가 달성됩니다. 일반적으로 마진이 클수록 분류기의 일반화 오류가 낮아지기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="b0f8a884660f9ed7117b59341660ac6dff079372" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called &amp;ldquo;support vectors&amp;rdquo;:</source>
          <target state="translated">서포트 벡터 머신은 분류, 회귀 또는 기타 작업에 사용할 수있는 고차원 또는 무한 차원 공간에서 초평면 또는 초평면 집합을 구성합니다. 일반적으로 마진이 클수록 분류기의 일반화 오류가 낮아 지므로 모든 클래스의 가장 가까운 훈련 데이터 포인트 (소위 기능 마진)까지 가장 큰 거리를 갖는 하이퍼 플레인이 직관적으로 좋은 분리를 달성합니다. 아래 그림은 &quot;지지 벡터&quot;라고하는 여백 경계에 세 개의 샘플이있는 선형으로 분리 가능한 문제에 대한 결정 함수를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="3a7e8b3dcde624d3d8f97ed6de9c3edf28cc3212" translate="yes" xml:space="preserve">
          <source>A synthetic random regression problem is generated. The targets &lt;code&gt;y&lt;/code&gt; are modified by: (i) translating all targets such that all entries are non-negative and (ii) applying an exponential function to obtain non-linear targets which cannot be fitted using a simple linear model.</source>
          <target state="translated">합성 랜덤 회귀 문제가 발생합니다. 목표 ( &lt;code&gt;y&lt;/code&gt; ) 는 (i) 모든 엔트리가 음이 아니도록 모든 목표를 번역하고 (ii) 간단한 선형 모델로는 적합 할 수없는 비선형 목표를 얻기 위해 지수 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="0903abbf1faeca209d35b8701b7569a1792a87c6" translate="yes" xml:space="preserve">
          <source>A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.</source>
          <target state="translated">회수율은 높지만 정밀도가 낮은 시스템은 많은 결과를 반환하지만 대부분의 예측 된 레이블은 교육 레이블과 비교할 때 정확하지 않습니다. 정밀도는 높지만 회수율이 낮은 시스템은 그 반대의 결과를 거의 나타내지 않지만, 대부분의 예측 된 레이블은 훈련 레이블과 비교할 때 정확합니다. 높은 정밀도와 높은 회수율을 가진 이상적인 시스템은 모든 결과에 올바르게 레이블이 지정된 많은 결과를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="173f587454933c3828450ea8ca8b53629bab1434" translate="yes" xml:space="preserve">
          <source>A thin wrapper around the functionality of the kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwise로 커널 기능을 둘러싼 얇은 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="66676f8254f2529112cd062a4d4d85367f781237" translate="yes" xml:space="preserve">
          <source>A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities \(\hat{d}_{ij}\) are normalized.</source>
          <target state="translated">이 문제에 대한 사소한 해결책은 원점의 모든 점을 설정하는 것입니다. 이를 방지하기 위해 시차 \ (\ hat {d} _ {ij} \)가 정규화됩니다.</target>
        </trans-unit>
        <trans-unit id="92110b9d440ed0f7743804ea18583b68d039b315" translate="yes" xml:space="preserve">
          <source>A tutorial exercise for using different SVM kernels.</source>
          <target state="translated">다른 SVM 커널 사용을위한 자습서 연습</target>
        </trans-unit>
        <trans-unit id="b027711ce35320d3d1f8472a3216da00739d6438" translate="yes" xml:space="preserve">
          <source>A tutorial exercise regarding the use of classification techniques on the Digits dataset.</source>
          <target state="translated">Digits 데이터 세트에서 분류 기술 사용에 관한 자습서 연습입니다.</target>
        </trans-unit>
        <trans-unit id="f2776a357a1de5e5a81f9ce6aa4ab14f87148c15" translate="yes" xml:space="preserve">
          <source>A tutorial exercise using Cross-validation with an SVM on the Digits dataset.</source>
          <target state="translated">Digits 데이터 세트에서 SVM과 교차 검증을 사용하는 튜토리얼 연습.</target>
        </trans-unit>
        <trans-unit id="b2628903b486483a05c00dab06ec0310f6a300cc" translate="yes" xml:space="preserve">
          <source>A tutorial exercise which uses cross-validation with linear models.</source>
          <target state="translated">선형 모델과 교차 검증을 사용하는 튜토리얼 연습.</target>
        </trans-unit>
        <trans-unit id="cd8b8f382c4d69a8c061676d971b4b4296bfd8b7" translate="yes" xml:space="preserve">
          <source>A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">과학적 데이터 처리를위한 통계 학습에 대한 자습서</target>
        </trans-unit>
        <trans-unit id="59f7f31eba85f0c9b2945699e7dc03221dcc0732" translate="yes" xml:space="preserve">
          <source>A two-dimensional classification example showing iso-probability lines for the predicted probabilities.</source>
          <target state="translated">예측 확률에 대한 등 확률 선을 보여주는 2 차원 분류 예제입니다.</target>
        </trans-unit>
        <trans-unit id="027a04b42d0e6eeb4156be0ce184aaf749b2c919" translate="yes" xml:space="preserve">
          <source>A typical &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;benchmark&lt;/a&gt; on synthetic data yields a &amp;gt;30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.</source>
          <target state="translated">전형적인 &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;벤치 마크&lt;/a&gt;합성 데이터에 는 모델과 입력이 모두 드문 경우 (각각 0이 아닌 계수 비율이 0.000024 및 0.027400 인 경우) 대기 시간이 30 % 이상 감소합니다. 마일리지는 데이터 및 모델의 희소성 및 크기에 따라 달라질 수 있습니다. 또한 프로덕션 서버에 배포 된 예측 모델의 메모리 사용을 줄이는 데 스파 스 화가 매우 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec7bb89c7f50ac1476a296b31de1d976e3adfdc7" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel&quot;&gt;multilabel&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is an either dense or sparse &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-binary&quot;&gt;binary&lt;/a&gt; matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt;. Each column represents a class. The &lt;code&gt;1&lt;/code&gt;&amp;rsquo;s in each row denote the positive classes a sample has been labelled with. An example of a dense matrix &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel&quot;&gt;다중 레이블 &lt;/a&gt; &lt;code&gt;y&lt;/code&gt; 의 유효한 표현은 형태의 조밀하거나 희소 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-binary&quot;&gt;이진&lt;/a&gt; 행렬 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; 입니다. 각 열은 클래스를 나타냅니다. 각 행 의 &lt;code&gt;1&lt;/code&gt; 은 샘플에 레이블이 지정된 포지티브 클래스를 나타냅니다. 3 개 샘플에 대한 조밀 행렬 &lt;code&gt;y&lt;/code&gt; 의 예 :</target>
        </trans-unit>
        <trans-unit id="cd911da25d74dfd6322d9ef6e738de82cbe5f6c0" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of class labels. A column wise concatenation of 1d &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;다중 출력 &lt;/a&gt; &lt;code&gt;y&lt;/code&gt; 의 유효한 표현 은 클래스 레이블 모양 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; 의 조밀 행렬입니다 . 1d &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;다중 클래스&lt;/a&gt; 변수 의 열 방식 연결 . 3 개 샘플에 대한 &lt;code&gt;y&lt;/code&gt; 의 예 :</target>
        </trans-unit>
        <trans-unit id="9b5488b6a26a1d957fe479981670f261e01fd257" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of floats. A column wise concatenation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-continuous&quot;&gt;continuous&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;다중 출력 &lt;/a&gt; &lt;code&gt;y&lt;/code&gt; 의 유효한 표현 은 부동 소수점 모양 &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; 의 조밀 한 행렬입니다 . &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-continuous&quot;&gt;연속&lt;/a&gt; 변수 의 열 방식 연결입니다 . 3 개 샘플에 대한 &lt;code&gt;y&lt;/code&gt; 의 예 :</target>
        </trans-unit>
        <trans-unit id="f7b09774b632b4a2428bc3dfc2607f68bc8654b3" translate="yes" xml:space="preserve">
          <source>A value ranges from 0 to 1. Radius neighbors will be searched until the ratio between total neighbors within the radius and the total candidates becomes less than this value unless it is terminated by hash length reaching &lt;code&gt;min_hash_match&lt;/code&gt;.</source>
          <target state="translated">값의 범위는 &lt;code&gt;min_hash_match&lt;/code&gt; . 반경 내 총 이웃과 총 후보 간의 비율이 min_hash_match 에 도달하는 해시 길이로 종료되지 않는 한이 값보다 작아 질 때까지 반경 이웃을 검색합니다. 입니다. .</target>
        </trans-unit>
        <trans-unit id="2582a27b00b61e2b9522f341f9a346d9f6709d01" translate="yes" xml:space="preserve">
          <source>A vector of size n_samples with the values of Xred assigned to each of the cluster of samples.</source>
          <target state="translated">크기가 n_samples 인 벡터는 Xred 값이 각 샘플 군집에 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="d6682290692f64a366a7947dddf0230094bb4014" translate="yes" xml:space="preserve">
          <source>A very short introduction into machine learning problems and how to solve them using scikit-learn. Introduced basic concepts and conventions.</source>
          <target state="translated">머신 러닝 문제와 scikit-learn을 사용하여 문제를 해결하는 방법에 대한 매우 짧은 소개. 기본 개념과 규칙을 도입했습니다.</target>
        </trans-unit>
        <trans-unit id="c7690c5e442eb70039d5b5e2448652422a789b71" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.</source>
          <target state="translated">투표 회귀 분석기는 전체 데이터 세트에서 각각 여러 기본 회귀 분석기에 맞는 앙상블 메타 추정기입니다. 그런 다음 개별 예측을 평균하여 최종 예측을 형성합니다.</target>
        </trans-unit>
        <trans-unit id="f36937969f4f8fe7fe83a3d1b4c3ed127cd62323" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. We will use three different regressors to predict the data: &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;). Then the above 3 regressors will be used for the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">투표 회귀 분석기는 전체 데이터 세트에서 각각 여러 기본 회귀 분석기에 맞는 앙상블 메타 추정기입니다. 그런 다음 개별 예측을 평균하여 최종 예측을 형성합니다. 데이터를 예측하기 위해 세 가지 다른 회귀 변수를 사용합니다 : &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt; &lt;code&gt;LinearRegression&lt;/code&gt; &lt;/a&gt; ). 그런 다음 위의 3 개의 회귀 변수가 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; 에&lt;/a&gt; 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="9e5f7b45681bf1702c76ccf69a63504aee5a5896" translate="yes" xml:space="preserve">
          <source>A {n_samples by n_samples} size matrix will be created from this</source>
          <target state="translated">이것으로부터 {n_samples by n_samples} 크기 행렬이 생성됩니다</target>
        </trans-unit>
        <trans-unit id="fcc549a5a2b9c6ffeadeffd9e820de04a6f70e50" translate="yes" xml:space="preserve">
          <source>A. Kraskov, H. Stogbauer and P. Grassberger, &amp;ldquo;Estimating mutual information&amp;rdquo;. Phys. Rev. E 69, 2004.</source>
          <target state="translated">A. Kraskov, H. Stogbauer 및 P. Grassberger, &quot;상호 정보 추정&quot;. Phys. 개정 E 69, 2004.</target>
        </trans-unit>
        <trans-unit id="2f18974816a09bbd22c4a7050c6bc6030b7596e3" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;A comparison of event models for Naive Bayes text classification.&lt;/a&gt; Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A. McCallum and K. Nigam (1998). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;Naive Bayes 텍스트 분류에 대한 이벤트 모델 비교 &lt;/a&gt;Proc. AAAI / ICML-98 텍스트 분류 학습 워크샵, pp. 41-48.</target>
        </trans-unit>
        <trans-unit id="09ff19e0224a8e064521a1f035498a8575f6b1a4" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A. McCallum and K. Nigam (1998). 순진한 Bayes 텍스트 분류를위한 이벤트 모델 비교. Proc. AAAI / ICML-98 텍스트 분류 학습 워크샵, pp. 41-48.</target>
        </trans-unit>
        <trans-unit id="664157aad39c267bf285cc58317f2c271aa8f944" translate="yes" xml:space="preserve">
          <source>A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor Third-Party Liability Claims (November 8, 2018). &lt;a href=&quot;http://dx.doi.org/10.2139/ssrn.3164764&quot;&gt;doi:10.2139/ssrn.3164764&lt;/a&gt;</source>
          <target state="translated">A. Noll, R. Salzmann 및 MV Wuthrich, 사례 연구 : 프랑스 자동차 제 3 자 책임 청구 (2018 년 11 월 8 일). &lt;a href=&quot;http://dx.doi.org/10.2139/ssrn.3164764&quot;&gt;도이 : 10.2139 / ssrn.3164764&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4aaae5b2c1983dccf44b8ac2ef0c2ae6d4b3ffdb" translate="yes" xml:space="preserve">
          <source>AGE</source>
          <target state="translated">AGE</target>
        </trans-unit>
        <trans-unit id="4aecbad800270fae47218d640b45ace7ade395c4" translate="yes" xml:space="preserve">
          <source>AGE proportion of owner-occupied units built prior to 1940</source>
          <target state="translated">1940 년 이전에 건축 된 소유자 점유 장치의 연령 비율</target>
        </trans-unit>
        <trans-unit id="80cd3c2daea500fa1de49e5116e93d233f023eef" translate="yes" xml:space="preserve">
          <source>AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.</source>
          <target state="translated">AIC는 Akaike 정보 기준이고 BIC는 Bayes Information 기준입니다. 이러한 기준은 적합도와 모델의 복잡성간에 균형을 유지하여 정규화 매개 변수의 값을 선택하는 데 유용합니다. 좋은 모델은 단순하면서도 데이터를 잘 설명해야합니다.</target>
        </trans-unit>
        <trans-unit id="fedf36066038cc2bbe4c1f8d6f37bca1d4a271f3" translate="yes" xml:space="preserve">
          <source>AMI</source>
          <target state="translated">AMI</target>
        </trans-unit>
        <trans-unit id="977d221308080b656b11030badda34761fa1379d" translate="yes" xml:space="preserve">
          <source>ANOVA F-value between label/feature for classification tasks.</source>
          <target state="translated">분류 작업을위한 레이블 / 기능 간의 분산 F- 값.</target>
        </trans-unit>
        <trans-unit id="500622cc8c5af556f9bf30d45e5a5dc4d38771d9" translate="yes" xml:space="preserve">
          <source>AP and the trapezoidal area under the operating points (&lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt;&lt;code&gt;sklearn.metrics.auc&lt;/code&gt;&lt;/a&gt;) are common ways to summarize a precision-recall curve that lead to different results. Read more in the &lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">작동 점 아래의 AP와 사다리꼴 영역 ( &lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt; &lt;code&gt;sklearn.metrics.auc&lt;/code&gt; &lt;/a&gt; )은 다른 결과를 초래하는 정밀 리콜 곡선을 요약하는 일반적인 방법입니다. &lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;사용자 안내서&lt;/a&gt; 에서 자세한 내용을 읽으십시오 .</target>
        </trans-unit>
        <trans-unit id="7ce6548ae70272727979635a979ae4f9ea8a3a93" translate="yes" xml:space="preserve">
          <source>AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:</source>
          <target state="translated">AP는 정밀도로 리콜 곡선을 각 임계 값에서 달성 된 가중 정밀도의 평균으로 요약하며, 이전 임계 값에서 리콜이 증가하면 가중치로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d93d10ff0fbef1b4aa0ddc24e10e907746d3c85a" translate="yes" xml:space="preserve">
          <source>API</source>
          <target state="translated">API</target>
        </trans-unit>
        <trans-unit id="b276f94cd8d0e74a21de6e5939b8c10ca9a975d6" translate="yes" xml:space="preserve">
          <source>API Reference</source>
          <target state="translated">API 참조</target>
        </trans-unit>
        <trans-unit id="044c42df47a473e04593a603631e399678960367" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id16&quot; id=&quot;id12&quot;&gt;3&lt;/a&gt;&lt;a href=&quot;#id18&quot; id=&quot;id13&quot;&gt;4&lt;/a&gt;.</source>
          <target state="translated">ARD는 문헌에서 &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; 및 &lt;em&gt;Relevance Vector Machine &lt;/em&gt;&lt;a href=&quot;#id16&quot; id=&quot;id12&quot;&gt;3 &lt;/a&gt;&lt;a href=&quot;#id18&quot; id=&quot;id13&quot;&gt;4&lt;/a&gt; 로도 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="fc664a8aed1b7702083559e78f4a1b25e63a7739" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3]&lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">ARD는 또한 문헌에서 &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine &lt;/em&gt;&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3] &lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt; 로 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="a1df128dfacd3f460cbb61bb4087bb92287d3fcb" translate="yes" xml:space="preserve">
          <source>ARI</source>
          <target state="translated">ARI</target>
        </trans-unit>
        <trans-unit id="8c442cbb4124477c731be288798913c884c906af" translate="yes" xml:space="preserve">
          <source>ARI is a symmetric measure:</source>
          <target state="translated">ARI는 대칭 측정입니다.</target>
        </trans-unit>
        <trans-unit id="b6cdde34aa4cbbe354d4a8fe12fbb26711ad6710" translate="yes" xml:space="preserve">
          <source>ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized:</source>
          <target state="translated">ARI는 대칭이므로 동일한 클래스에서 멤버가 있지만 불필요한 스플릿이있는 순수한 클러스터가있는 레이블링은 불이익을받습니다.</target>
        </trans-unit>
        <trans-unit id="5045d88e766edf44d9736d9751b0aa0b647864ac" translate="yes" xml:space="preserve">
          <source>A[i, j] is assigned the weight of edge that connects i to j.</source>
          <target state="translated">A [i, j]에는 i를 j에 연결하는 모서리의 가중치가 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="9f4a63ae0bf1594deea5ea12c509b27f409ce3db" translate="yes" xml:space="preserve">
          <source>Aaron Defazio, Francis Bach, Simon Lacoste-Julien: &lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.&lt;/a&gt;</source>
          <target state="translated">Aaron Defazio, Francis Bach, Simon Lacoste-Julien : &lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA : 볼록하지 않은 볼록한 복합 목표를 지원하는 빠른 증분 그라데이션 방법.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e82c46cc189803ea687c96da2038982e9dcb6a4b" translate="yes" xml:space="preserve">
          <source>Ability to use shared memory efficiently with worker processes for large numpy-based datastructures.</source>
          <target state="translated">대규모 기반 데이터 구조에 작업자 프로세스와 함께 공유 메모리를 효율적으로 사용하는 기능.</target>
        </trans-unit>
        <trans-unit id="54d1141ddccb7851744d76084cc8611f93e2cfc3" translate="yes" xml:space="preserve">
          <source>Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See &lt;a href=&quot;#tree-algorithms&quot;&gt;algorithms&lt;/a&gt; for more information.</source>
          <target state="translated">수치 데이터와 범주 데이터를 모두 처리 할 수 ​​있습니다. 다른 기술은 일반적으로 한 가지 유형의 변수 만있는 데이터 세트를 분석하는 데 특화되어 있습니다. 자세한 내용은 &lt;a href=&quot;#tree-algorithms&quot;&gt;알고리즘&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="2367c2ca7022225c20205571d9433b5b6b84cd68" translate="yes" xml:space="preserve">
          <source>Able to handle multi-output problems.</source>
          <target state="translated">다중 출력 문제를 처리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="da22d4ef976c68e8fef4ef4e1a2681784cdddf3a" translate="yes" xml:space="preserve">
          <source>Above, we limited this regularization to a very little amount. Regularization improves the conditioning of the problem and reduces the variance of the estimates. RidgeCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for prediction.</source>
          <target state="translated">위에서 우리는이 정규화를 아주 적은 양으로 제한했습니다. 정규화는 문제의 조건을 개선하고 추정값의 분산을 줄입니다. RidgeCV는 정규화 매개 변수 ( &lt;code&gt;alpha&lt;/code&gt; ) 의 어떤 값이 예측에 가장 적합한 지 결정하기 위해 교차 검증을 적용 합니다.</target>
        </trans-unit>
        <trans-unit id="1c09e3c38ba7261734dc4ab72dcf1ef3efd8a152" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular values are non-significant are discarded. Only used if solver is &amp;lsquo;svd&amp;rsquo;.</source>
          <target state="translated">X의 특이 값이 중요하다고 간주되는 절대 임계 값은 X의 순위를 추정하는 데 사용됩니다. 특이 값이 중요하지 않은 차원은 삭제됩니다. 솔버가 'svd'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="2aa749baee7b6d0eb7f8d7395643fd7e673c649b" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value to be considered significant, used to estimate the rank of &lt;code&gt;Xk&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples in class k. This parameter does not affect the predictions. It only controls a warning that is raised when features are considered to be colinear.</source>
          <target state="translated">중요한 것으로 간주되는 특이 값에 대한 절대 임계 값으로, &lt;code&gt;Xk&lt;/code&gt; 의 순위를 추정하는 데 사용됩니다. 여기서 &lt;code&gt;Xk&lt;/code&gt; 는 클래스 k에있는 샘플의 중심 행렬입니다. 이 매개 변수는 예측에 영향을주지 않습니다. 피쳐가 동일 선상에 있다고 간주 될 때 발생하는 경고 만 제어합니다.</target>
        </trans-unit>
        <trans-unit id="f587c3583da9a191a72d5ff01fbfedde272f2898" translate="yes" xml:space="preserve">
          <source>Absolute tolerance for equivalence of arrays. Default = 1E-10.</source>
          <target state="translated">배열의 동등성에 대한 절대 허용 오차. 기본값은 1E-10입니다.</target>
        </trans-unit>
        <trans-unit id="d79faf207bac3f682107ababce293b4eceadf34c" translate="yes" xml:space="preserve">
          <source>Acceptable data types for the parameter.</source>
          <target state="translated">매개 변수에 허용되는 데이터 유형입니다.</target>
        </trans-unit>
        <trans-unit id="a62a0f9649f4eb6fd92f87e85b475e87182d6e46" translate="yes" xml:space="preserve">
          <source>Access the fitted transformer by name.</source>
          <target state="translated">이름으로 장착 된 변압기에 액세스하십시오.</target>
        </trans-unit>
        <trans-unit id="9e0e7377700ae6d6f91649710d89b9dd54aaa033" translate="yes" xml:space="preserve">
          <source>According to the JL lemma, projecting 500 samples without too much distortion will require at least several thousands dimensions, irrespective of the number of features of the original dataset.</source>
          <target state="translated">JL 정리에 따르면, 너무 많은 왜곡없이 500 개의 샘플을 투사하려면 원본 데이터 세트의 기능 수에 관계없이 최소 수천 개의 치수가 필요합니다.</target>
        </trans-unit>
        <trans-unit id="caff4ed631e2a64d90ad9e75e695e4cb077ff929" translate="yes" xml:space="preserve">
          <source>According to the model above, the log of the posterior is:</source>
          <target state="translated">위의 모델에 따르면 사후 로그는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1e10c7e434a3850cc1ce980a85ce2c40965ebbd1" translate="yes" xml:space="preserve">
          <source>According to the observed data, the frequency of accidents is higher for drivers younger than 30 years old, and is positively correlated with the &lt;code&gt;BonusMalus&lt;/code&gt; variable. Our model is able to mostly correctly model this behaviour.</source>
          <target state="translated">관찰 된 데이터에 따르면 사고 빈도는 30 세 미만 운전자의 경우 더 높으며 &lt;code&gt;BonusMalus&lt;/code&gt; 변수 와 양의 상관 관계가 있습니다. 우리 모델은이 동작을 대부분 올바르게 모델링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7b168ce1b26b0ed19f0857c9a9e0275bc2a86de2" translate="yes" xml:space="preserve">
          <source>Accuracy classification score.</source>
          <target state="translated">정확도 분류 점수.</target>
        </trans-unit>
        <trans-unit id="ed1b27307b9c829fdd35ed1ccf294d2e82a6dfcb" translate="yes" xml:space="preserve">
          <source>Accuracy of the Model</source>
          <target state="translated">모델의 정확성</target>
        </trans-unit>
        <trans-unit id="b171ff92fbbc89c347ecf11b61ad701495ae9495" translate="yes" xml:space="preserve">
          <source>Accuracy vs alpha for training and testing sets</source>
          <target state="translated">훈련 및 테스트 세트에 대한 정확도 대 알파</target>
        </trans-unit>
        <trans-unit id="3159fc0c287c6fa355557d0a32e1e4fbdd60d32b" translate="yes" xml:space="preserve">
          <source>Across the module, we designate the vector \(w = (w_1, ..., w_p)\) as &lt;code&gt;coef_&lt;/code&gt; and \(w_0\) as &lt;code&gt;intercept_&lt;/code&gt;.</source>
          <target state="translated">모듈에서 우리는 벡터 \ (w = (w_1, ..., w_p) \)를 &lt;code&gt;coef_&lt;/code&gt; 로 지정 하고 \ (w_0 \)를 &lt;code&gt;intercept_&lt;/code&gt; 로 지정 합니다.</target>
        </trans-unit>
        <trans-unit id="1e74068a27362b129f4808ec6412c747359a90d1" translate="yes" xml:space="preserve">
          <source>Activation function for the hidden layer.</source>
          <target state="translated">숨겨진 레이어의 활성화 기능.</target>
        </trans-unit>
        <trans-unit id="13466b0d67f826e470f7f662a0fa9b98771bd57d" translate="yes" xml:space="preserve">
          <source>Actual class (observation)</source>
          <target state="translated">실제 수업 (관찰)</target>
        </trans-unit>
        <trans-unit id="4bda103291a01841e5d33c04e072de1679753a3f" translate="yes" xml:space="preserve">
          <source>Actual number of iteration for each Cs.</source>
          <target state="translated">각 C의 실제 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="b9e108d70e5d6e1ab362876cb4abfe8b3ea0d61c" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1.</source>
          <target state="translated">모든 클래스, 폴드 및 C에 대한 실제 반복 횟수입니다. 이항 또는 다항식의 경우 첫 번째 차원은 1과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e15b0921ef0e026565cb96038a661f88e504a7d1" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If &lt;code&gt;penalty='elasticnet'&lt;/code&gt;, the shape is &lt;code&gt;(n_classes, n_folds,
n_cs, n_l1_ratios)&lt;/code&gt; or &lt;code&gt;(1, n_folds, n_cs, n_l1_ratios)&lt;/code&gt;.</source>
          <target state="translated">모든 클래스, 접기 및 C에 대한 실제 반복 횟수입니다. 바이너리 또는 다항식의 경우, 상기 제 1 치수는 동일하면 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; , 형상이다 &lt;code&gt;(n_classes, n_folds, n_cs, n_l1_ratios)&lt;/code&gt; 또는 &lt;code&gt;(1, n_folds, n_cs, n_l1_ratios)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e7c00438aea9f14670d9e5ce3ad69eadb9e56fa7" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.</source>
          <target state="translated">모든 클래스에 대한 실제 반복 횟수 이항 또는 다항이면 1 개의 요소 만 반환합니다. liblinear 솔버의 경우 모든 클래스에서 최대 반복 횟수 만 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="41eb0ffb0b86a2b7fa1c30caad1eab7979f038bb" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.</source>
          <target state="translated">각 대상에 대한 실제 반복 횟수입니다. sag 및 lsqr 솔버에만 사용 가능합니다. 다른 솔버는 None을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5f183a64b01f4bbfef50fc734f02625115726f0b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations used in the solver.</source>
          <target state="translated">솔버에 사용 된 실제 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="a5f57bad41ea8a1cc3f7ad65dabd66fae549152b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations.</source>
          <target state="translated">실제 반복 횟수</target>
        </trans-unit>
        <trans-unit id="e88dbeae46a975e549a3e1b429ac335f1717a327" translate="yes" xml:space="preserve">
          <source>AdaBoost can be used both for classification and regression problems:</source>
          <target state="translated">AdaBoost는 분류 및 회귀 문제 모두에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1c3d2a8d1e7ee52688369f7b268da1f090f613a4" translate="yes" xml:space="preserve">
          <source>Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.</source>
          <target state="translated">Adam은 확률 적 옵티 마이저라는 점에서 SGD와 유사하지만 하위 모멘트의 적응 추정값을 기반으로 매개 변수를 업데이트하도록 양을 자동으로 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="37193f0def1de066166921b64d73e81a4207486f" translate="yes" xml:space="preserve">
          <source>Add plots</source>
          <target state="translated">플롯 추가</target>
        </trans-unit>
        <trans-unit id="71042ebf81b88cec72926e5e2da3d76737a8fc09" translate="yes" xml:space="preserve">
          <source>Adding a constant kernel is equivalent to adding a constant:</source>
          <target state="translated">상수 커널을 추가하는 것은 상수를 추가하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="57edf9b3cf9cc602f38d55f2f3497cc8e4458efe" translate="yes" xml:space="preserve">
          <source>Adding parameters that do not influence the performance does not decrease efficiency.</source>
          <target state="translated">성능에 영향을 미치지 않는 매개 변수를 추가해도 효율성이 저하되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d8b82f8ac1fd93a8c5863d25cc6558cb656b7f40" translate="yes" xml:space="preserve">
          <source>Additional Resources</source>
          <target state="translated">추가 자료</target>
        </trans-unit>
        <trans-unit id="28c6073bb46543d76ed56428ba51e1f98f703736" translate="yes" xml:space="preserve">
          <source>Additional fit parameters.</source>
          <target state="translated">추가 맞춤 매개 변수.</target>
        </trans-unit>
        <trans-unit id="3bc09d37c7749b14dd88f92786b449d978558dbf" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function.</source>
          <target state="translated">메트릭 함수에 대한 추가 키워드 인수</target>
        </trans-unit>
        <trans-unit id="448e0ea40ef1f0816d1ba3c00c7fd9818ee1579c" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function. For most metrics will be same with &lt;code&gt;metric_params&lt;/code&gt; parameter, but may also contain the &lt;code&gt;p&lt;/code&gt; parameter value if the &lt;code&gt;effective_metric_&lt;/code&gt; attribute is set to &amp;lsquo;minkowski&amp;rsquo;.</source>
          <target state="translated">메트릭 함수에 대한 추가 키워드 인수입니다. 대부분의 메트릭은 &lt;code&gt;metric_params&lt;/code&gt; 매개 변수 와 동일 하지만 , &lt;code&gt;effective_metric_&lt;/code&gt; 속성이 'minkowski'로 설정된 경우 &lt;code&gt;p&lt;/code&gt; 매개 변수 값 도 포함 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9a84da277628ee33f1ee92b82eb0c5a6af6d0524" translate="yes" xml:space="preserve">
          <source>Additional number of random vectors to sample the range of M so as to ensure proper conditioning. The total number of random vectors used to find the range of M is n_components + n_oversamples. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values.</source>
          <target state="translated">적절한 컨디셔닝을 보장하기 위해 M 범위를 샘플링하는 추가 수의 랜덤 벡터. M의 범위를 찾는 데 사용되는 랜덤 벡터의 총 개수는 n_components + n_oversamples입니다. 숫자가 작을수록 속도는 향상되지만 특이 벡터 및 특이 값의 근사 품질에 부정적인 영향을 줄 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7cc2d5826a730e9f6abc3aeeee28797fe00b8570" translate="yes" xml:space="preserve">
          <source>Additional parameter passed to the fit function of the estimator.</source>
          <target state="translated">추정기의 적합 함수에 전달 된 추가 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="df9b29356cfac6bb58ec5dca9394f2d9dfc4703b" translate="yes" xml:space="preserve">
          <source>Additional parameters (keyword arguments) for kernel function passed as callable object.</source>
          <target state="translated">커널 함수에 대한 추가 매개 변수 (키워드 인수)는 호출 가능한 객체로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="39a26d6ec93b4bd557ff0d3e3333c06661f787d5" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to score_func.</source>
          <target state="translated">score_func에 전달할 추가 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="71247cd4c08f9e15e8cf580ef4b1b215b5be4541" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to the tree for use with the metric. For more information, see the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">메트릭과 함께 사용하기 위해 트리에 전달할 추가 매개 변수입니다. 자세한 내용은 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 설명서를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="36da43716a16aade125d9759679637e39761ccc9" translate="yes" xml:space="preserve">
          <source>Additionally compute class covariance matrix (default False), used only in &amp;lsquo;svd&amp;rsquo; solver.</source>
          <target state="translated">또한 'svd'솔버에서만 사용되는 클래스 공분산 행렬 (기본 False)을 계산하십시오.</target>
        </trans-unit>
        <trans-unit id="cb731300b5471fde45bf8790ae9ce49720693175" translate="yes" xml:space="preserve">
          <source>Additionally, &lt;code&gt;Pipeline&lt;/code&gt; can be instantiated with the &lt;code&gt;memory&lt;/code&gt; argument to memoize the transformers within the pipeline, avoiding to fit again the same transformers over and over.</source>
          <target state="translated">또한 &lt;code&gt;Pipeline&lt;/code&gt; 을 &lt;code&gt;memory&lt;/code&gt; 로 인스턴스화 할 수 있습니다. 인수로 하여 파이프 라인 내에서 변압기를 하여 동일한 변압기를 반복해서 다시 장착하지 않아도됩니다.</target>
        </trans-unit>
        <trans-unit id="4fcf36ef35c92f01311334b57dea32bb115ca779" translate="yes" xml:space="preserve">
          <source>Additionally, latent semantic analysis can also be used to reduce dimensionality and discover latent patterns in the data.</source>
          <target state="translated">또한 잠재 의미론 분석을 사용하여 차원을 줄이고 데이터에서 잠재 패턴을 발견 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="c13231af636098d832d9fddca4838809fd525e6e" translate="yes" xml:space="preserve">
          <source>Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</source>
          <target state="translated">첨가제 (라플라스 / 리드 스톤) 스무딩 매개 변수 (스무딩이없는 경우 0).</target>
        </trans-unit>
        <trans-unit id="b8c1aaec1a2a62d157d744d0faee837709b2772c" translate="yes" xml:space="preserve">
          <source>Adjacency matrix of the graph</source>
          <target state="translated">그래프의 인접 행렬</target>
        </trans-unit>
        <trans-unit id="250995e59d0465859871db4d7d37821f2e9e268c" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information</source>
          <target state="translated">조정 된 상호 정보</target>
        </trans-unit>
        <trans-unit id="11e5941af4a261b664e9f0491f0ecd18e8412b46" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared. For two clusterings \(U\) and \(V\), the AMI is given as:</source>
          <target state="translated">AMI (Adjusted Mutual Information)는 MI (Mutual Information) 점수를 조정하여 우연을 나타냅니다. 실제로 더 많은 정보가 공유되는지 여부에 관계없이 MI가 일반적으로 더 많은 수의 클러스터를 갖는 두 개의 클러스터링에 대해 더 높다는 사실을 설명합니다. 두 개의 클러스터링 \ (U \) 및 \ (V \)의 경우 AMI는 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="4a1b6ee1509942e0e22b1880790459ca848319f4" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (adjusted against chance)</source>
          <target state="translated">상호 정보 조정 (기회에 맞게 조정)</target>
        </trans-unit>
        <trans-unit id="4630f03a5a119c76517981dad27161c68958ec63" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information between two clusterings.</source>
          <target state="translated">두 군집 간의 조정 된 상호 정보.</target>
        </trans-unit>
        <trans-unit id="1802b8bdeb7acaeda57735feed31e0ee765d7139" translate="yes" xml:space="preserve">
          <source>Adjusted Rand Index</source>
          <target state="translated">조정 랜드 지수</target>
        </trans-unit>
        <trans-unit id="31e214397fb70765d3e9e011e4c9138d3d446804" translate="yes" xml:space="preserve">
          <source>Adjusted against chance Mutual Information</source>
          <target state="translated">우연에 대한 조정 상호 정보</target>
        </trans-unit>
        <trans-unit id="a058b0be3c60201e595de35ba7261933ab6a3173" translate="yes" xml:space="preserve">
          <source>Adjusted for chance measure such as ARI display some random variations centered around a mean score of 0.0 for any number of samples and clusters.</source>
          <target state="translated">ARI와 같은 확률 측정에 맞게 조정되어 임의의 수의 샘플 및 군집에 대해 0.0의 평균 점수를 중심으로하는 임의의 변형이 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="b80cf8c9df3b30a05f03bffeb2976e09c960b38a" translate="yes" xml:space="preserve">
          <source>Adjustment for chance in clustering performance evaluation</source>
          <target state="translated">군집 성능 평가 기회 조정</target>
        </trans-unit>
        <trans-unit id="7a26e45b68e65e5d705e3f16c6dd31629c5f071a" translate="yes" xml:space="preserve">
          <source>Advanced Plotting With Partial Dependence</source>
          <target state="translated">부분 의존성이있는 고급 플로팅</target>
        </trans-unit>
        <trans-unit id="c22815e21882a0aa7a6a9b8a93ee5e8e17ca7614" translate="yes" xml:space="preserve">
          <source>Affects shape of transform output only when voting=&amp;rsquo;soft&amp;rsquo; If voting=&amp;rsquo;soft&amp;rsquo; and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).</source>
          <target state="translated">voting = 'soft'인 경우에만 변환 출력의 모양에 영향을줍니다. voting = 'soft'및 flatten_transform = True 인 경우 변환 방법은 모양이있는 행렬을 반환합니다 (n_samples, n_classifiers * n_classes). flatten_transform = False이면 (n_classifiers, n_samples, n_classes)를 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="6be196ba0996dde7d9ed4c8d27213a288c55819b" translate="yes" xml:space="preserve">
          <source>Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the &lt;em&gt;preference&lt;/em&gt;, which controls how many exemplars are used, and the &lt;em&gt;damping factor&lt;/em&gt; which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.</source>
          <target state="translated">선호도 전파는 제공된 데이터를 기반으로 클러스터 수를 선택하므로 흥미로울 수 있습니다. 이를 위해 두 가지 중요한 매개 변수는 &lt;em&gt;preference&lt;/em&gt; 이며 사용되는 예제 수와 &lt;em&gt;댐핑 팩터&lt;/em&gt; 를 제어합니다.&lt;em&gt;&lt;/em&gt; 와 이러한 메시지를 업데이트 할 때 수치 변동을 피하기 위해 책임 및 가용성 메시지를 감쇠시키는 입니다.</target>
        </trans-unit>
        <trans-unit id="f0a35e8a6a6b08571acff68f4d3ca53b52b812c7" translate="yes" xml:space="preserve">
          <source>Affinity matrix used for clustering. Available only if after calling &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">클러스터링에 사용되는 선호도 매트릭스 &lt;code&gt;fit&lt;/code&gt; 를 호출 한 후에 만 ​​사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="63246ae129dc66d8916faa22ac2026722fe870df" translate="yes" xml:space="preserve">
          <source>Affinity propagation</source>
          <target state="translated">선호도 전파</target>
        </trans-unit>
        <trans-unit id="50cd6f7778a2976318486216f7be1102ea3c2dbd" translate="yes" xml:space="preserve">
          <source>Affinity_matrix constructed from samples or precomputed.</source>
          <target state="translated">샘플로 구성되거나 사전 계산 된 Affinity_matrix</target>
        </trans-unit>
        <trans-unit id="9d6ec54ac3f9f07d1c79bc7828709b2a1ebe0b76" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict new values:</source>
          <target state="translated">그런 다음 모델을 사용하여 새 값을 예측할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="56033184bfd4d9eb6d5cedda898ff603a56457b5" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict the class of samples:</source>
          <target state="translated">피팅 된 후 모델을 사용하여 샘플 클래스를 예측할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d1a9437feb3595025967367029f3b25aca08ed48" translate="yes" xml:space="preserve">
          <source>After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.</source>
          <target state="translated">이 메소드를 호출 한 후 densify를 호출 할 때까지 partial_fit 메소드 (있는 경우)를 더 적합하게 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="1ac4b13a36333967e31b510f39d76728b1ade858" translate="yes" xml:space="preserve">
          <source>After discretization, linear regression and decision tree make exactly the same prediction. As features are constant within each bin, any model must predict the same value for all points within a bin. Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible. Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.</source>
          <target state="translated">이산화 후 선형 회귀 및 의사 결정 트리는 정확히 동일한 예측을합니다. 각 구간 내에서 형상이 일정하므로 모든 모델은 구간 내 모든 점에 대해 동일한 값을 예측해야합니다. 이산화 이전의 결과와 비교할 때, 의사 결정 트리의 유연성은 떨어지지 만 선형 모델은 훨씬 유연 해집니다. 비닝 기능은 일반적으로 트리 기반 모델에 유리한 효과가 없습니다. 이러한 모델은 데이터를 어디서나 분할하는 방법을 배울 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="eef177bc3be0e883eb6e86188495b80899ebaefb" translate="yes" xml:space="preserve">
          <source>After fitting (training), the model can predict labels for new samples:</source>
          <target state="translated">피팅 (트레이닝) 후 모델은 새 샘플의 레이블을 예측할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5e6425357ee2c6e34c7ccf4e621fb9b2e0d28394" translate="yes" xml:space="preserve">
          <source>After fitting a model, row and column cluster membership can be found in the &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes. &lt;code&gt;rows_[i]&lt;/code&gt; is a binary vector with nonzero entries corresponding to rows that belong to bicluster &lt;code&gt;i&lt;/code&gt;. Similarly, &lt;code&gt;columns_[i]&lt;/code&gt; indicates which columns belong to bicluster &lt;code&gt;i&lt;/code&gt;.</source>
          <target state="translated">모델을 피팅 한 후 &lt;code&gt;rows_&lt;/code&gt; 및 &lt;code&gt;columns_&lt;/code&gt; 속성 에서 행 및 열 클러스터 멤버쉽을 찾을 수 있습니다 . &lt;code&gt;rows_[i]&lt;/code&gt; 는 bicluster &lt;code&gt;i&lt;/code&gt; 에 속하는 행에 해당하는 0이 아닌 항목을 가진 이진 벡터입니다 . 마찬가지로 &lt;code&gt;columns_[i]&lt;/code&gt; 는 bicluster &lt;code&gt;i&lt;/code&gt; 에 속하는 열을 나타냅니다 .</target>
        </trans-unit>
        <trans-unit id="71ae50af31deda9eff4041050d01116db2fe0b76" translate="yes" xml:space="preserve">
          <source>After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.</source>
          <target state="translated">정규화 후, 스펙트럼 공동 클러스터링 알고리즘에서와 같이 처음 몇 개의 특이 벡터가 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="c326685e6de3fbd40d347b380da45cf33d03051c" translate="yes" xml:space="preserve">
          <source>After this operation, \(U_k \Sigma_k^\top\) is the transformed training set with \(k\) features (called &lt;code&gt;n_components&lt;/code&gt; in the API).</source>
          <target state="translated">이 작업 후 \ (U_k \ Sigma_k ^ \ top \)는 \ (k \) 기능 ( API에서 &lt;code&gt;n_components&lt;/code&gt; 라고 함)으로 변환 된 트레이닝 세트입니다 .</target>
        </trans-unit>
        <trans-unit id="d10a093c92eea73e29249b26fe409d51c485f495" translate="yes" xml:space="preserve">
          <source>After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. We&amp;rsquo;ll also review a few security and maintainability issues when working with pickle serialization.</source>
          <target state="translated">scikit-learn 모델을 학습 한 후에는 재교육없이 나중에 사용할 수 있도록 모델을 유지할 수있는 방법이 필요합니다. 다음 섹션에서는 피클을 사용하여 모델을 유지하는 방법의 예를 제공합니다. 또한 피클 직렬화 작업을 할 때 몇 가지 보안 및 유지 관리 문제를 검토합니다.</target>
        </trans-unit>
        <trans-unit id="24449d2cd36cc7fedaa85e80b22b4a39b7f8e2cc" translate="yes" xml:space="preserve">
          <source>After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see &lt;a href=&quot;#sparsecoder&quot;&gt;Sparse coding with a precomputed dictionary&lt;/a&gt;).</source>
          <target state="translated">그러한 절차를 사전에 맞추기 위해 사용한 후, 변환은 모든 사전 학습 객체와 동일한 구현을 공유하는 스파 스 코딩 단계 일뿐입니다 ( &lt;a href=&quot;#sparsecoder&quot;&gt;사전 계산 된 사전을 사용한 스파 스 코딩&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="2b35bdc816ab119e837a0f526e39f92fef72f3ba" translate="yes" xml:space="preserve">
          <source>Again please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details on all the parameters.</source>
          <target state="translated">모든 매개 변수에 대한 자세한 내용은 &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;참조 설명서&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="99e1009ea0e8d7b9e4e16a4193c8696d68c4efff" translate="yes" xml:space="preserve">
          <source>Again, we check the performance of the computed model using, for example, the median absolute error of the model and the R squared coefficient.</source>
          <target state="translated">다시 말하지만, 모델의 절대 오차 중앙값과 R 제곱 계수를 사용하여 계산 된 모델의 성능을 확인합니다.</target>
        </trans-unit>
        <trans-unit id="ff9f1ff32120d8b893c1ded522d49590353b29a6" translate="yes" xml:space="preserve">
          <source>Age</source>
          <target state="translated">Age</target>
        </trans-unit>
        <trans-unit id="e869aecf975cf3b336fce1699b0d4503e42564de" translate="yes" xml:space="preserve">
          <source>Agglomerate features.</source>
          <target state="translated">덩어리 특징.</target>
        </trans-unit>
        <trans-unit id="dbb914491e4e2790ae24eaabcabbe0b9aa470c77" translate="yes" xml:space="preserve">
          <source>Agglomerative Clustering</source>
          <target state="translated">응집 클러스터링</target>
        </trans-unit>
        <trans-unit id="35bb423e043758761d2f3e248257c1ca705a5ac8" translate="yes" xml:space="preserve">
          <source>Agglomerative cluster has a &amp;ldquo;rich get richer&amp;rdquo; behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data.</source>
          <target state="translated">집성 클러스터는 &quot;풍부 해지면서 더 풍부한&quot;동작을 가지므로 클러스터 크기가 고르지 않습니다. 이와 관련하여 단일 연계는 최악의 전략이며, Ward는 가장 규칙적인 크기를 제공합니다. 그러나 선호도 (또는 클러스터링에 사용 된 거리)는 Ward에 따라 달라질 수 없으므로 유클리드가 아닌 메트릭의 경우 평균 연결이 좋은 대안입니다. 노이즈가 많은 데이터에는 강력하지 않지만 단일 연결은 매우 효율적으로 계산 될 수 있으므로 더 큰 데이터 세트의 계층 적 클러스터링을 제공하는 데 유용 할 수 있습니다. 단일 링키지는 구형이 아닌 데이터에서도 잘 수행 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="890d73cb8666b8b11b8fc128eb14fe1823b0fbff" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering</source>
          <target state="translated">응집 클러스터링</target>
        </trans-unit>
        <trans-unit id="21c09abace100a2c8351e8288af87cf02023a3a7" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with and without structure</source>
          <target state="translated">구조가 있거나없는 응집 클러스터링</target>
        </trans-unit>
        <trans-unit id="06e589f9bfed18f64327ae7d57413054b3c6f8de" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with different metrics</source>
          <target state="translated">다른 메트릭스를 사용하는 집계 클러스터링</target>
        </trans-unit>
        <trans-unit id="4d6b62f2cefed7f7111116c687eea9e3676aa5b0" translate="yes" xml:space="preserve">
          <source>Agnostic</source>
          <target state="translated">Agnostic</target>
        </trans-unit>
        <trans-unit id="ac9f2566d02b5e4600cd56af348af55d70c023a7" translate="yes" xml:space="preserve">
          <source>Agnostic:</source>
          <target state="translated">Agnostic:</target>
        </trans-unit>
        <trans-unit id="a679479691140b63a2247391cf7941ca13e56589" translate="yes" xml:space="preserve">
          <source>Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma).</source>
          <target state="translated">농업 / 기상 모델링 : 연간 강우 이벤트 수 (Poisson), 이벤트 당 강우량 (Gamma), 연간 총 강우량 (Tweedie / Compound Poisson Gamma).</target>
        </trans-unit>
        <trans-unit id="a88029fe8eca09e8e53e31e892c06950617c42cc" translate="yes" xml:space="preserve">
          <source>Akaike information criterion for the current model on the input X.</source>
          <target state="translated">입력 X에서 현재 모델에 대한 Akaike 정보 기준.</target>
        </trans-unit>
        <trans-unit id="471a24dc120a414840a5a390be002dd1fb20dce7" translate="yes" xml:space="preserve">
          <source>Alcalinity of Ash:</source>
          <target state="translated">재의 Alcalinity :</target>
        </trans-unit>
        <trans-unit id="1c996292fc0f34e9abb2ed1bce253f665b2889f0" translate="yes" xml:space="preserve">
          <source>Alcalinity of ash</source>
          <target state="translated">재의 Alcalinity</target>
        </trans-unit>
        <trans-unit id="35fa6a7b518a822ef300d3ac95936a0d4551ed5f" translate="yes" xml:space="preserve">
          <source>Alcohol</source>
          <target state="translated">Alcohol</target>
        </trans-unit>
        <trans-unit id="4537230d988c507a37af5da4b6f068cb25fae3fd" translate="yes" xml:space="preserve">
          <source>Alcohol:</source>
          <target state="translated">Alcohol:</target>
        </trans-unit>
        <trans-unit id="a755c4c6a00484a10fdd39c78f5316741e33717a" translate="yes" xml:space="preserve">
          <source>Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning, in Proceedings of the 22nd International Conference on Machine Learning (ICML). See section 4 (Qualitative Analysis of Predictions).</source>
          <target state="translated">Alexandru Niculescu-Mizil와 Rich Caruana (2005)는 제 22 차 국제 머신 러닝 국제 회의 (ICML)에서 진행되는 감독 학습을 통해 좋은 확률을 예측합니다. 섹션 4 (예측의 질적 분석)을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="e00df3c495c9c818a9560b719b9887130853cc0a" translate="yes" xml:space="preserve">
          <source>Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance.</source>
          <target state="translated">가장 가까운 이웃 검색에 사용할 알고리즘으로, neighbors.NearestNeighbors 인스턴스로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="c6ad136737477e6b0db26cf9286713a801e6cd45" translate="yes" xml:space="preserve">
          <source>Algorithm to use in the optimization problem.</source>
          <target state="translated">최적화 문제에 사용할 알고리즘.</target>
        </trans-unit>
        <trans-unit id="c739143784c9537893f5a8383165eeec97c8efbf" translate="yes" xml:space="preserve">
          <source>Algorithm used to compute the nearest neighbors:</source>
          <target state="translated">가장 가까운 이웃을 계산하는 데 사용되는 알고리즘 :</target>
        </trans-unit>
        <trans-unit id="cb151b633578d2167df8c911cd7019b2a2913090" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">데이터 lars를 변환하는 데 사용되는 알고리즘 : 최소 각도 회귀 방법 사용 (linear_model.lars_path) lasso_lars : Lars를 사용하여 Lasso 솔루션 계산 lasso_cd : 좌표 하강 방법을 사용하여 Lasso 솔루션 (linear_model.Lasso)을 계산합니다. 추정 된 구성 요소가 드문 경우 lasso_lars가 더 빠릅니다. omp : 희소 솔루션 임계 값을 추정하기 위해 직교 매칭 추구를 사용합니다. 투영 &lt;code&gt;dictionary * X'&lt;/code&gt; 에서 알파보다 작은 모든 계수를 0으로 스쿼시합니다 * X '</target>
        </trans-unit>
        <trans-unit id="59c5f2e6c851221e36e62f9314e77aa5b776cd59" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X&amp;rsquo;</source>
          <target state="translated">데이터를 변환하는 데 사용되는 알고리즘. lars : 최소 각도 회귀 방법 (linear_model.lars_path)을 사용합니다. lasso_lars : Lars를 사용하여 올가미 솔루션 계산 lasso_cd : 좌표 하강 방법을 사용하여 올가미 솔루션 (linear_model.Lasso)을 계산합니다. 추정 된 구성 요소가 드문 경우 lasso_lars가 더 빠릅니다. omp : 희소 솔루션 임계 값을 추정하기 위해 직교 매칭 추구를 사용합니다. 투영 사전에서 알파보다 작은 모든 계수를 0으로 스쿼시합니다 * X '</target>
        </trans-unit>
        <trans-unit id="d57ea62b7e17ad495e3491babf97d100e18a1702" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">데이터를 변환하는 데 사용되는 알고리즘 : lars : 최소 각도 회귀 방법 사용 (linear_model.lars_path) lasso_lars : Lars를 사용하여 Lasso 솔루션 계산 lasso_cd : 좌표 하강 방법을 사용하여 Lasso 솔루션 (linear_model.Lasso)을 계산합니다. 추정 된 구성 요소가 드문 경우 lasso_lars가 더 빠릅니다. omp : 희소 솔루션 임계 값을 추정하기 위해 직교 매칭 추구를 사용합니다. 투영 &lt;code&gt;dictionary * X'&lt;/code&gt; 에서 알파보다 작은 모든 계수를 0으로 스쿼시합니다 * X '</target>
        </trans-unit>
        <trans-unit id="6dd589ff391e3fe9203a6e5ffaf1bc46cb8b74f7" translate="yes" xml:space="preserve">
          <source>Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.</source>
          <target state="translated">알고리즘은 또한 행과 열이 바이 클러스터에 할당 될 수있는 방식이 다르며, 이로 인해 상이한 바이 클러스터 구조가 발생합니다. 블록 대각선 또는 바둑판 구조는 행과 열이 파티션으로 분할 될 때 발생합니다.</target>
        </trans-unit>
        <trans-unit id="06560884189da8b18b9514c9f15fcee660a6560e" translate="yes" xml:space="preserve">
          <source>Algorithms differ in how they define biclusters. Some of the common types include:</source>
          <target state="translated">알고리즘은 방광을 정의하는 방법이 다릅니다. 일반적인 유형 중 일부는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">필드 번호 0의 ​​별명</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">필드 번호 1의 별명</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">필드 번호 2의 별명</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">필드 번호 3의 별명</target>
        </trans-unit>
        <trans-unit id="8ba0e524b527e040b1c69a39c8c3727540cd8735" translate="yes" xml:space="preserve">
          <source>Alias for field number 4</source>
          <target state="translated">필드 번호 4의 별명</target>
        </trans-unit>
        <trans-unit id="e972f77e5bbab5ce66decc0654515ea9c4cce33b" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to &lt;code&gt;pairwise_kernels&lt;/code&gt; from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="translated">모든 가우시안 프로세스 커널과 상호 운용 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; 반대 및 부사장 :의 서브 클래스의 인스턴스 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 로 전달 될 수있다 &lt;code&gt;metric&lt;/code&gt; 에 &lt;code&gt;pairwise_kernels&lt;/code&gt; 에서 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; . 또한 래퍼 클래스 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt; &lt;code&gt;PairwiseKernel&lt;/code&gt; &lt;/a&gt; 을 사용하여 pairwise의 커널 함수를 GP 커널로 사용할 수 있습니다 . 유일한주의 사항은 하이퍼 파라미터의 기울기가 분석적이지 않고 수치 적이며 모든 커널이 등방성 거리 만 지원한다는 것입니다. 매개 변수 &lt;code&gt;gamma&lt;/code&gt; 는 초 매개 변수 로 간주되며 최적화 될 수 있습니다. 다른 커널 매개 변수는 초기화시 직접 설정되며 고정 된 상태로 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="c99cb316a9285e1cd15f4f7521535d0751148726" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to pairwise_kernels`` from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="translated">모든 Gaussian 프로세스 커널은 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; 와 상호 운용 가능 하며 그 반대도 가능합니다. &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 의 서브 클래스 인스턴스는 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; 에서 pairwise_kernels에 &lt;code&gt;metric&lt;/code&gt; 으로 전달 될 수 있습니다 . 또한 pairwise의 커널 함수는 랩퍼 클래스 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt; &lt;code&gt;PairwiseKernel&lt;/code&gt; &lt;/a&gt; 을 사용하여 GP 커널로 사용할 수 있습니다 . 유일한주의 사항은 하이퍼 파라미터의 기울기가 분석적이지 않지만 숫자이며 모든 커널이 등방성 거리 만 지원한다는 것입니다. 매개 변수 &lt;code&gt;gamma&lt;/code&gt; 는 하이퍼 파라미터 로 간주되며 최적화 될 수 있습니다. 다른 커널 매개 변수는 초기화시 직접 설정되며 고정되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="facb53030a78059078bd7255fa4340b830781f6e" translate="yes" xml:space="preserve">
          <source>All above functions (i.e. &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt;) accept 1D array which can be useful in some specific case.</source>
          <target state="translated">위의 모든 함수 (예 : &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt; )는 특정 경우에 유용 할 수있는 1D 배열을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="e88245fc777e7533587c2f8e718d50cd7116a3f1" translate="yes" xml:space="preserve">
          <source>All available versions</source>
          <target state="translated">사용 가능한 모든 버전</target>
        </trans-unit>
        <trans-unit id="03af622cdb4aac726985802a2b9d3765c7a41749" translate="yes" xml:space="preserve">
          <source>All bins in each feature have identical widths.</source>
          <target state="translated">각 피처의 모든 빈은 너비가 동일합니다.</target>
        </trans-unit>
        <trans-unit id="59c634de3245b210109c9f97187eafdfb201b388" translate="yes" xml:space="preserve">
          <source>All bins in each feature have the same number of points.</source>
          <target state="translated">각 기능의 모든 구간에는 동일한 개수의 포인트가 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e7d5dca10b34ad961d839250513b9c8a0893f0e" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn do multiclass classification out-of-the-box. You don&amp;rsquo;t need to use the &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module unless you want to experiment with different multiclass strategies.</source>
          <target state="translated">scikit-learn의 모든 분류기는 기본적으로 멀티 클래스 분류를 수행합니다. 다른 멀티 클래스 전략을 실험하지 않는 한 &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt; 모듈 을 사용할 필요가 없습니다 .</target>
        </trans-unit>
        <trans-unit id="026ccd1f86687bfe3e5f08bd5747feaae826b1dc" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn implement multiclass classification; you only need to use this module if you want to experiment with custom multiclass strategies.</source>
          <target state="translated">scikit-learn의 모든 분류기는 다중 클래스 분류를 구현합니다. 사용자 정의 멀티 클래스 전략을 실험하려는 경우에만이 모듈을 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="c78fdb80201033cef297ce6fb5232c2383bd7521" translate="yes" xml:space="preserve">
          <source>All decision trees use &lt;code&gt;np.float32&lt;/code&gt; arrays internally. If training data is not in this format, a copy of the dataset will be made.</source>
          <target state="translated">모든 의사 결정 트리는 내부적으로 &lt;code&gt;np.float32&lt;/code&gt; 배열을 사용 합니다. 교육 데이터가이 형식이 아닌 경우 데이터 세트의 사본이 작성됩니다.</target>
        </trans-unit>
        <trans-unit id="41b549b6eacc2f621d683cbb4d8d7de1b7ed3ca8" translate="yes" xml:space="preserve">
          <source>All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel function.</source>
          <target state="translated">이 dict (있는 경우)의 모든 항목은 pairwise kernel 함수에 키워드 인수로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="ff969c4b66fd28ef01340faf70c8e905bbf72131" translate="yes" xml:space="preserve">
          <source>All estimator objects expose a &lt;code&gt;fit&lt;/code&gt; method that takes a dataset (usually a 2-d array):</source>
          <target state="translated">모든 추정기 객체 는 데이터 세트 (보통 2 차원 배열)를 취하는 &lt;code&gt;fit&lt;/code&gt; 메소드를 노출합니다 .</target>
        </trans-unit>
        <trans-unit id="8781771bbb6ededabb6ec83fb1b33a34b860476e" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="translated">마지막 하나를 제외하고 파이프 라인의 모든 추정기는 변환기 여야합니다 (즉, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;변환&lt;/a&gt; 방법 이 있어야 함 ). 마지막 추정기는 모든 유형 (변환기, 분류기 등)이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e9616c63898ea085d32d1415c7461f768e275753" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;code&gt;transform&lt;/code&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="translated">마지막 파이프 라인을 제외한 파이프 라인의 모든 추정기는 변환기 여야합니다 (즉, &lt;code&gt;transform&lt;/code&gt; 방법 이 있어야 함 ). 마지막 추정기는 모든 유형 (변압기, 분류기 등) 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7c3fa432791090758c929f832210b21538e1fa2f" translate="yes" xml:space="preserve">
          <source>All estimators in the pipeline must support &lt;code&gt;inverse_transform&lt;/code&gt;.</source>
          <target state="translated">파이프 라인의 모든 추정기는 &lt;code&gt;inverse_transform&lt;/code&gt; 을 지원해야합니다 .</target>
        </trans-unit>
        <trans-unit id="b2ef621d2a7c2b25a3be25bb6ebef0201032958b" translate="yes" xml:space="preserve">
          <source>All estimators should specify all the parameters that can be set at the class level in their &lt;code&gt;__init__&lt;/code&gt; as explicit keyword arguments (no &lt;code&gt;*args&lt;/code&gt; or &lt;code&gt;**kwargs&lt;/code&gt;).</source>
          <target state="translated">모든 추정자는 &lt;code&gt;__init__&lt;/code&gt; 의 클래스 레벨에서 명시 적 키워드 인수 ( &lt;code&gt;*args&lt;/code&gt; 또는 &lt;code&gt;**kwargs&lt;/code&gt; 없음 ) 로 설정할 수있는 모든 매개 변수를 지정해야합니다 .</target>
        </trans-unit>
        <trans-unit id="24e3691f195d78233bed4d7732d29903c7a0bfe6" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="translated">마지막 5 개의 솔버 모두 고밀도 및 희소 데이터를 모두 지원합니다. 그러나 &lt;code&gt;fit_intercept&lt;/code&gt; 가 True 인 경우 'sag'및 'saga'만 스파 스 입력을 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="61f9c2a4b604c0e088d2123f4269900a4c32a338" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when`fit_intercept` is True.</source>
          <target state="translated">마지막 5 개의 솔버 모두 고밀도 및 희소 데이터를 모두 지원합니다. 그러나 'fit_intercept'가 True 인 경우 'sag'및 'saga'만 스파 스 입력을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="f728f573fb469cf76a885b25e7cd8d2d6661b65d" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;sparse_cg&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="translated">마지막 5 개의 솔버는 모두 고밀도 데이터와 희소 데이터를 모두 지원합니다. 그러나 &lt;code&gt;fit_intercept&lt;/code&gt; 가 True 인 경우 'sag'및 'sparse_cg'만 희소 입력을 지원합니다 .</target>
        </trans-unit>
        <trans-unit id="9b477d9d300bf316e6d73adbb21d261c724739c5" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MaxAbsScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">모든 X는 단일 배치로 처리됩니다. 이는 매우 많은 수의 &lt;code&gt;n_samples&lt;/code&gt; 로 인해 &lt;a href=&quot;#sklearn.preprocessing.MaxAbsScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 하지 않거나 X가 연속 스트림에서 읽혀지기 때문에 적합 하지 않은 경우를위한 것입니다 .</target>
        </trans-unit>
        <trans-unit id="b3fc8e75fa02a8bfaeaedb8d116922428a0fe800" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MinMaxScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">모든 X는 단일 배치로 처리됩니다. 이는 매우 많은 수의 &lt;code&gt;n_samples&lt;/code&gt; 로 인해 &lt;a href=&quot;#sklearn.preprocessing.MinMaxScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 하지 않거나 X가 연속 스트림에서 읽혀지기 때문에 적합 하지 않은 경우를위한 것입니다 .</target>
        </trans-unit>
        <trans-unit id="0771ca4e648606c13a5db59d2963430f3dd6f313" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.StandardScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="translated">모든 X는 단일 배치로 처리됩니다. 이는 매우 많은 수의 &lt;code&gt;n_samples&lt;/code&gt; 로 인해 &lt;a href=&quot;#sklearn.preprocessing.StandardScaler.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 하지 않거나 X가 연속 스트림에서 읽혀지기 때문에 적합 하지 않은 경우를위한 것입니다 .</target>
        </trans-unit>
        <trans-unit id="8e879aa8d08a0dbe80a902ae3611f1bc99c99eb6" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">위의 모든 항목은 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; 에서&lt;/a&gt; 지원됩니다 .</target>
        </trans-unit>
        <trans-unit id="fe669fffadfc5710970ab03a9fde631f4966ca91" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt;.</source>
          <target state="translated">위의 모든 사항은 &lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt; 에서 지원됩니다 .</target>
        </trans-unit>
        <trans-unit id="b5ed2e5e9ebf405638935350a19afaab363efb49" translate="yes" xml:space="preserve">
          <source>All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.</source>
          <target state="translated">위의 손실 함수는 모두 아래 그림과 같이 오 분류 오차 (제로원 손실)의 상한으로 간주 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0050a0a20cce6934eea13e5fb5f70c03d8a3e6cd" translate="yes" xml:space="preserve">
          <source>All penalization parameters explored.</source>
          <target state="translated">모든 벌칙 매개 변수를 조사했습니다.</target>
        </trans-unit>
        <trans-unit id="955d9d437b529cc06e590b033bee6c6f48038cb1" translate="yes" xml:space="preserve">
          <source>All scikit-learn classifiers are capable of multiclass classification, but the meta-estimators offered by &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).</source>
          <target state="translated">모든 scikit-learn 분류기는 멀티 클래스 분류가 가능하지만 &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; 에서&lt;/a&gt; 제공하는 메타 추정기는 두 개 이상의 클래스를 처리하는 방식을 변경할 수 있습니다. 이는 일반화 오류 또는 필요한 계산 측면에서 분류기 성능에 영향을 줄 수 있기 때문입니다. 자원).</target>
        </trans-unit>
        <trans-unit id="196b45b70e9cf2661d4f75a2029c5a0c8898e090" translate="yes" xml:space="preserve">
          <source>All settings, not just those presently modified, will be returned to their previous values when the context manager is exited. This is not thread-safe.</source>
          <target state="translated">컨텍스트 관리자가 종료되면 현재 수정 된 설정뿐만 아니라 모든 설정이 이전 값으로 돌아갑니다. 스레드 안전하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="aa66fd6d2b23c29862eb90bc96dba9c2ff577f39" translate="yes" xml:space="preserve">
          <source>All supervised &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimators&lt;/a&gt; in scikit-learn implement a &lt;code&gt;fit(X, y)&lt;/code&gt; method to fit the model and a &lt;code&gt;predict(X)&lt;/code&gt; method that, given unlabeled observations &lt;code&gt;X&lt;/code&gt;, returns the predicted labels &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">scikit-learn의 모든 감독 &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;추정기&lt;/a&gt; 는 모형에 &lt;code&gt;fit(X, y)&lt;/code&gt; 하기 위해 fit (X, y) 방법과 레이블이없는 관측치 &lt;code&gt;X&lt;/code&gt; 가 지정된 경우 예측 된 레이블 &lt;code&gt;y&lt;/code&gt; 를 반환 하는 &lt;code&gt;predict(X)&lt;/code&gt; 방법을 구현 합니다 .</target>
        </trans-unit>
        <trans-unit id="5fd3433668568da7d84a79ee4e0518a15dd72a27" translate="yes" xml:space="preserve">
          <source>All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a dedicated marker value for unlabeled samples.</source>
          <target state="translated">모든 입력 데이터는 비 표지 샘플에 대한 전용 마커 값을 갖는 매트릭스 X (표지 및 비 표지) 및 상응하는 표지 매트릭스 y를 제공한다.</target>
        </trans-unit>
        <trans-unit id="799926dcab2e28e1fbc27d0dc0ec06e58ac91465" translate="yes" xml:space="preserve">
          <source>All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt;, as given by &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;. With mode &lt;code&gt;mode='connectivity'&lt;/code&gt;, these functions return a binary adjacency sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;. Whereas with &lt;code&gt;mode='distance'&lt;/code&gt;, they return a distance sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt;. To include these functions in a scikit-learn pipeline, one can also use the corresponding classes &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The benefits of this sparse graph API are multiple.</source>
          <target state="translated">이러한 모든 추정기는 내부적으로 가장 가까운 이웃을 계산할 수 있지만, 대부분은 &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;kneighbors_graph&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;radius_neighbors_graph&lt;/code&gt; 에서&lt;/a&gt; 제공하는 것처럼 사전 계산 된 가장 가까운 이웃 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;희소 그래프를 &lt;/a&gt;허용 합니다. 모드로 &lt;code&gt;mode='connectivity'&lt;/code&gt; , 이러한 기능으로, 예를 들어, 필요에 따라 이진 스파 스 인접 그래프를 반환 &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;SpectralClustering&lt;/code&gt; &lt;/a&gt; . 반면와 &lt;code&gt;mode='distance'&lt;/code&gt; , 그들에, 예를 들면, 필요에 따라 부족한 거리 그래프를 반환 &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt; . scikit-learn 파이프 라인에 이러한 함수를 포함하려면 해당 클래스 &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;KNeighborsTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt; &lt;code&gt;RadiusNeighborsTransformer&lt;/code&gt; 를&lt;/a&gt; 사용할 수도 있습니다.. 이 희소 그래프 API의 이점은 여러 가지입니다.</target>
        </trans-unit>
        <trans-unit id="f3103275737fc127cec8d65804a5477574232497" translate="yes" xml:space="preserve">
          <source>All three models are significantly better than chance but also very far from making perfect predictions.</source>
          <target state="translated">세 가지 모델 모두 우연보다 훨씬 낫지 만 완벽한 예측과는 거리가 멉니 다.</target>
        </trans-unit>
        <trans-unit id="68201d9ddadd1b10424b1028599271aea0a81e03" translate="yes" xml:space="preserve">
          <source>All values are cached on the filesystem, in a deep directory structure.</source>
          <target state="translated">모든 값은 파일 시스템에서 깊은 디렉토리 구조로 캐시됩니다.</target>
        </trans-unit>
        <trans-unit id="ad92361405543cadf3390872502b0368070801b1" translate="yes" xml:space="preserve">
          <source>All, &lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt;&lt;code&gt;mutual_info_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; are symmetric: swapping the argument does not change the score. Thus they can be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">모든, &lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt; &lt;code&gt;mutual_info_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; 는&lt;/a&gt; 대칭 : 인수를 교환하면 점수를 변경하지 않습니다. 따라서 그것들은 &lt;strong&gt;합의 척도&lt;/strong&gt; 로서 사용될 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="9cc3cbd54e82f76756e09725006efed713dfb14b" translate="yes" xml:space="preserve">
          <source>Allow to bypass several input checking. Don&amp;rsquo;t use this parameter unless you know what you do.</source>
          <target state="translated">여러 입력 검사를 무시할 수 있습니다. 자신이하는 일을 모른다면이 매개 변수를 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="b2988d20e10f74030eeb38e913a183de75189dbb" translate="yes" xml:space="preserve">
          <source>Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.</source>
          <target state="translated">허용되는 입력은 목록, numpy 배열, scipy-sparse 행렬 또는 pandas 데이터 프레임입니다.</target>
        </trans-unit>
        <trans-unit id="246073e83f7bacaf2b27a7cb44f4ce78f91064b5" translate="yes" xml:space="preserve">
          <source>Allows NaN in the input.</source>
          <target state="translated">입력에 NaN을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="7d1bd9e91706d2b4396d936c8102ad0f0e779fcd" translate="yes" xml:space="preserve">
          <source>Allows NaN/Inf in the input if the underlying estimator does as well.</source>
          <target state="translated">기본 추정자가 수행하는 경우 입력에 NaN / Inf를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="b369263cc488ea5fca6946112c8afe7517b680b5" translate="yes" xml:space="preserve">
          <source>Allows simple indexing of lists or arrays.</source>
          <target state="translated">목록이나 배열을 간단하게 색인 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bd0b505b007b0a8de2703b572d4c9c27c39f3415" translate="yes" xml:space="preserve">
          <source>Allows to examine the spread of each true cluster across predicted clusters and vice versa.</source>
          <target state="translated">예측 된 클러스터간에 각 실제 클러스터의 확산을 검사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="036fd67d51f9b182736132cac7fb44d0406f98ef" translate="yes" xml:space="preserve">
          <source>Almost every group is distinguished by whether headers such as &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; and &lt;code&gt;Distribution:&lt;/code&gt; appear more or less often.</source>
          <target state="translated">거의 모든 그룹은 &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; 및 &lt;code&gt;Distribution:&lt;/code&gt; 과 같은 헤더 가 더 자주 나타나는지 여부에 따라 구별됩니다 .</target>
        </trans-unit>
        <trans-unit id="54076ee073b95c8e2227a7529bbd3c040278da4f" translate="yes" xml:space="preserve">
          <source>Alpaydin (alpaydin &amp;lsquo;@&amp;rsquo; boun.edu.tr)</source>
          <target state="translated">Alpaydin (alpaydin '@'boun.edu.tr)</target>
        </trans-unit>
        <trans-unit id="78d537ced7a0f9450c0b7f5b446829da6458dfc9" translate="yes" xml:space="preserve">
          <source>Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.</source>
          <target state="translated">Alpaydin, C. Kaynak (1998) 캐스 케이 딩 분류기, Kybernetika.</target>
        </trans-unit>
        <trans-unit id="bc0f2eb3e0375e2eb0ff510eb47611a2b3ce02af" translate="yes" xml:space="preserve">
          <source>Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.</source>
          <target state="translated">알파는 가중치의 크기를 제한하여 과적 합을 방지하는 정규화 항 (일명 페널티 항)의 매개 변수입니다. 알파를 늘리면 더 작은 가중치를 장려함으로써 높은 분산 (과적 합의 징후)을 고칠 수 있고, 더 적은 곡률로 나타나는 결정 경계 플롯이 생성 될 수 있습니다. 유사하게, 알파를 줄이면 더 큰 가중치를 장려함으로써 높은 편향 (부적합의 징후)을 고칠 수 있으며, 이로 인해 더 복잡한 결정 경계가 생길 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a01b7375f883c68c86f4064f9c436e0a18c14d5d" translate="yes" xml:space="preserve">
          <source>Alpha is again treated as a random variable that is to be estimated from the data.</source>
          <target state="translated">알파는 다시 데이터에서 추정 할 임의의 변수로 취급됩니다.</target>
        </trans-unit>
        <trans-unit id="9bc4ba22610a89d08abe53892ec649b1e8a46ce9" translate="yes" xml:space="preserve">
          <source>Also for multiple metric evaluation, the attributes &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; will only be available if &lt;code&gt;refit&lt;/code&gt; is set and all of them will be determined w.r.t this specific scorer.</source>
          <target state="translated">또한 여러 메트릭 평가를 위해, 속성은 &lt;code&gt;best_index_&lt;/code&gt; , &lt;code&gt;best_score_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 는 경우에만 사용할 수 있습니다 &lt;code&gt;refit&lt;/code&gt; 설정되고 모두가이 특정한 득점 WRT 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="7c913c796a2efc191b66ad3c6a4c672e085ca88e" translate="yes" xml:space="preserve">
          <source>Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when &lt;code&gt;n_clusters&lt;/code&gt; is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the &lt;code&gt;n_clusters&lt;/code&gt; is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labelled scatter plot on the right.</source>
          <target state="translated">또한 실루엣 플롯의 두께에서 클러스터 크기를 시각화 할 수 있습니다. &lt;code&gt;n_clusters&lt;/code&gt; 가 2 인 경우 클러스터 0의 실루엣 플롯 은 3 개의 하위 클러스터를 하나의 큰 클러스터로 그룹화하기 때문에 크기가 더 큽니다. 그러나 &lt;code&gt;n_clusters&lt;/code&gt; 가 4 인 경우 모든 플롯의 두께는 다소 비슷하므로 오른쪽의 레이블 된 산점도에서 확인할 수있는 것과 비슷한 크기입니다.</target>
        </trans-unit>
        <trans-unit id="11074bab0d4cf60477f10f484031e0877ac22e6b" translate="yes" xml:space="preserve">
          <source>Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.</source>
          <target state="translated">일대일이라고도하는이 전략은 클래스 당 하나의 분류 기준을 맞추는 것으로 구성됩니다. 각 분류 자에 대해 클래스는 다른 모든 클래스에 적합합니다. 계산 효율성 ( &lt;code&gt;n_classes&lt;/code&gt; 분류 자만 필요) 외에도이 접근 방식의 한 가지 장점은 해석 가능성입니다. 각 클래스는 하나의 분류 자로 만 표시되므로 해당 분류자를 검사하여 클래스에 대한 지식을 얻을 수 있습니다. 이것이 멀티 클래스 분류에 가장 일반적으로 사용되는 전략이며 공정한 기본 선택입니다.</target>
        </trans-unit>
        <trans-unit id="cc34e41c8fa23138f4c5476496b3bc4a71c58fb4" translate="yes" xml:space="preserve">
          <source>Also note that both random features have very low importances (close to 0) as expected.</source>
          <target state="translated">또한 두 임의 기능 모두 예상대로 중요도가 매우 낮습니다 (0에 가까움).</target>
        </trans-unit>
        <trans-unit id="6f4d07ff4d50db829057964fee42d298814f3483" translate="yes" xml:space="preserve">
          <source>Also note that even though Box-Cox seems to perform better than Yeo-Johnson for lognormal and chi-squared distributions, keep in mind that Box-Cox does not support inputs with negative values.</source>
          <target state="translated">또한 Box-Cox가 로그 정규 분포 및 카이 제곱 분포에서 Yeo-Johnson보다 성능이 우수한 것 같지만 Box-Cox는 음수 입력을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0af9b79bdfeacc61c23bfadd6f95c644ff726db7" translate="yes" xml:space="preserve">
          <source>Also note that for the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="translated">또한 선형 사례의 경우 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 구현에 의해 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 에서 사용되는 알고리즘 은 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 기반 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 대응 알고리즘 보다 훨씬 효율적이며 수백만 개의 샘플 및 / 또는 기능에 거의 선형으로 확장 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5de10864fc42a72508361320516be4ba6cf282ee" translate="yes" xml:space="preserve">
          <source>Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data.</source>
          <target state="translated">또한 숫자 레이블은 t-SNE에 의해 발견 된 자연 그룹과 대략 일치하지만 PCA 모델의 선형 2D 투영은 레이블 영역이 크게 겹치는 표현을 생성합니다. 이것은이 데이터가 로컬 구조에 중점을 둔 비선형 방법 (예 : 가우시안 RBF 커널이있는 SVM)으로 잘 분리 될 수 있다는 강력한 단서입니다. 그러나, 2D에서 t-SNE를 갖는 잘 분리 된 균질하게 표지 된 그룹을 시각화하지 못한다고해서 반드시 감독 된 모델에 의해 데이터가 올바르게 분류 될 수 없다는 것을 의미하지는 않습니다. 2 차원이 데이터의 내부 구조를 정확하게 나타낼만큼 충분히 낮지 않은 경우 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ab26a7a4f86bc166b4ac9642686ddf7141b2b207" translate="yes" xml:space="preserve">
          <source>Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefficients.</source>
          <target state="translated">또한 계수를 수집하기 전에 모형이 수렴되었는지 확인하기 위해 공차에 낮은 값을 설정했습니다.</target>
        </trans-unit>
        <trans-unit id="7d37adf37d9aef6924cdccc898bbb4ca10bb0758" translate="yes" xml:space="preserve">
          <source>Also useful for lower-level tasks is the function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; that computes the coefficients along the full path of possible values.</source>
          <target state="translated">하위 수준 작업에도 유용합니다. &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt; 함수 는 가능한 값의 전체 경로를 따라 계수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="2587ad1f75524017fb1ecdf6376d89726a0f6825" translate="yes" xml:space="preserve">
          <source>Also, by evaluating log marginal likelihood (L) of these models, we can determine which one is better. It can be concluded that the model with larger L is more likely.</source>
          <target state="translated">또한 이러한 모델의 로그 한계 우도 (L)를 평가하여 어느 것이 더 나은지 결정할 수 있습니다. L이 더 큰 모형이 더 가능성이 높다는 결론을 내릴 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0b5de16e36a8364053840b53f5f6fe23ce35a1bc" translate="yes" xml:space="preserve">
          <source>Also, the EXPERIENCE and AGE are strongly linearly correlated.</source>
          <target state="translated">또한 EXPERIENCE와 AGE는 강한 선형 상관 관계가 있습니다.</target>
        </trans-unit>
        <trans-unit id="4a2b5f07f08a470a2b2e7ef1ae5a0e4e195af3e0" translate="yes" xml:space="preserve">
          <source>Also, these routines have not been tested for graphs with negative distances. Negative distances can lead to infinite cycles that must be handled by specialized algorithms.</source>
          <target state="translated">또한 이러한 루틴은 음의 거리를 가진 그래프에 대해서는 테스트되지 않았습니다. 음의 거리는 특수 알고리즘으로 처리해야하는 무한 사이클을 유발할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9ba49a974ea195b74ff8d485a45574a6c2262ea1" translate="yes" xml:space="preserve">
          <source>Also, this estimator is different from the R implementation of Robust Regression (&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt;) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.</source>
          <target state="translated">또한 R 추정은 가중치가 주어진 최소 가중치 제곱 구현을 수행하기 때문에이 추정기는 R 강력한 구현의 R 구현 ( &lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt; ) 과 다릅니다. 잔차가 특정 임계 값보다 얼마나 큰지를 기준으로 각 샘플.</target>
        </trans-unit>
        <trans-unit id="8c3843c6d1de204ea6ff77660712728c7734da35" translate="yes" xml:space="preserve">
          <source>Alternate label propagation strategy more robust to noise</source>
          <target state="translated">소음에보다 강력한 대체 라벨 전파 전략</target>
        </trans-unit>
        <trans-unit id="a1df7e48a8694bc511c33f4fdb83c6299c349bb3" translate="yes" xml:space="preserve">
          <source>Alternate output array in which to place the result. The default is &lt;code&gt;None&lt;/code&gt;; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See &lt;code&gt;doc.ufuncs&lt;/code&gt; for details.</source>
          <target state="translated">결과를 배치 할 대체 출력 배열입니다. 디폴트는 &lt;code&gt;None&lt;/code&gt; 입니다 . 제공되는 경우 예상 출력과 모양이 같아야하지만 필요한 경우 유형이 캐스트됩니다. 자세한 내용은 &lt;code&gt;doc.ufuncs&lt;/code&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="1444a36563243cd516125e47e643f6c86af2b2d3" translate="yes" xml:space="preserve">
          <source>Alternating Least Squares (Fast HALS).</source>
          <target state="translated">교대 최소 제곱 (Fast HALS).</target>
        </trans-unit>
        <trans-unit id="16e8b22da1997a9e7a46f4b2f3065cb731917af6" translate="yes" xml:space="preserve">
          <source>Alternative implementation that does incremental updates of the centers&amp;rsquo; positions using mini-batches.</source>
          <target state="translated">미니 배치를 사용하여 센터 위치를 점진적으로 업데이트하는 대체 구현.</target>
        </trans-unit>
        <trans-unit id="9319ffecfc2401bba3b77152ec4f1cf3bfc1dbce" translate="yes" xml:space="preserve">
          <source>Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &amp;gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation.</source>
          <target state="translated">미니 배치를 사용하여 센터 위치의 증분 업데이트를 수행하는 대체 온라인 구현. 대규모 학습 (예 : n_samples&amp;gt; 10k)의 경우 MiniBatchKMeans는 기본 배치 구현보다 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="203de1a7976ebc427fc76b372cb16da457115e31" translate="yes" xml:space="preserve">
          <source>Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with &lt;code&gt;pip install graphviz&lt;/code&gt;.</source>
          <target state="translated">또는 graphviz의 바이너리는 graphviz 프로젝트 홈페이지에서 다운로드 할 수 있으며 Python 래퍼는 &lt;code&gt;pip install graphviz&lt;/code&gt; 와 함께 pypi에서 설치할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a86f6ee74cd1a30708099bd1d22a36b2f08e9a0f" translate="yes" xml:space="preserve">
          <source>Alternatively the backend can be passed directly as an instance.</source>
          <target state="translated">또는 백엔드를 인스턴스로 직접 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="76c8a1ac67e02525fd024c6509339aad00aeced5" translate="yes" xml:space="preserve">
          <source>Alternatively, it can be set by the &amp;lsquo;SCIKIT_LEARN_DATA&amp;rsquo; environment variable or programmatically by giving an explicit folder path. The &amp;lsquo;~&amp;rsquo; symbol is expanded to the user home folder.</source>
          <target state="translated">또는 'SCIKIT_LEARN_DATA'환경 변수로 설정하거나 명시적인 폴더 경로를 제공하여 프로그래밍 방식으로 설정할 수 있습니다. '~'기호가 사용자 홈 폴더로 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="adf895b415f65b1116cbe9c2579f72b5380f5476" translate="yes" xml:space="preserve">
          <source>Alternatively, one can directly model the total loss with a unique Compound Poisson Gamma generalized linear model (with a log link function). This model is a special case of the Tweedie GLM with a &amp;ldquo;power&amp;rdquo; parameter \(p \in (1, 2)\). Here, we fix apriori the &lt;code&gt;power&lt;/code&gt; parameter of the Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one would select this value via grid-search by minimizing the negative log-likelihood of the Tweedie model, but unfortunately the current implementation does not allow for this (yet).</source>
          <target state="translated">또는 고유 한 Compound Poisson Gamma 일반화 선형 모델 (로그 링크 함수 포함)을 사용하여 총 손실을 직접 모델링 할 수 있습니다. 이 모델은 &quot;power&quot;매개 변수 \ (p \ in (1, 2) \)가있는 Tweedie GLM의 특별한 경우입니다. 여기서는 Tweedie 모델 의 &lt;code&gt;power&lt;/code&gt; 매개 변수를 유효한 범위 내에서 임의의 값 (1.9)으로 사전에 수정 합니다. ㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ 이상적으로는 Tweedie 모델의 음의 로그 가능성을 최소화하여 그리드 검색을 통해이 값을 선택하는 것이 좋지만 안타깝게도 현재 구현에서는이를 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="5dd953719fa9152bd6124ae49c16c66b8c780a34" translate="yes" xml:space="preserve">
          <source>Alternatively, one can use the &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; classes directly to find nearest neighbors. This is the functionality wrapped by the &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt;&lt;code&gt;NearestNeighbors&lt;/code&gt;&lt;/a&gt; class used above. The Ball Tree and KD Tree have the same interface; we&amp;rsquo;ll show an example of using the KD Tree here:</source>
          <target state="translated">또는 &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 클래스를 직접 사용하여 가장 가까운 이웃을 찾을 수 있습니다. 이것은 위에서 사용 된 &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt; &lt;code&gt;NearestNeighbors&lt;/code&gt; &lt;/a&gt; 클래스에 의해 감싸 진 기능 입니다. Ball Tree와 KD Tree는 동일한 인터페이스를 가지고 있습니다. KD 트리를 사용하는 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a2b3bf63690d9fdf9c988aa4c9e184e0fbef9897" translate="yes" xml:space="preserve">
          <source>Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:</source>
          <target state="translated">대안 적으로, 직교 매칭 추구는 특정 개수의 0이 아닌 계수 대신 특정 에러를 목표로 할 수있다. 이것은 다음과 같이 표현 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0f7ec2ff37de8d3d768baf118074df7d6149a11f" translate="yes" xml:space="preserve">
          <source>Alternatively, the &lt;code&gt;scoring&lt;/code&gt; argument can be provided to specify an alternative scoring method.</source>
          <target state="translated">대안 적으로, 대안적인 스코어링 방법을 지정하기 위해 &lt;code&gt;scoring&lt;/code&gt; 인수가 제공 될 수있다.</target>
        </trans-unit>
        <trans-unit id="ea61f1eddfe969d509258a2bd8bcd0672a722400" translate="yes" xml:space="preserve">
          <source>Alternatively, the estimator &lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt;&lt;code&gt;LassoLarsIC&lt;/code&gt;&lt;/a&gt; proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">대안으로, 추정기 &lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt; &lt;code&gt;LassoLarsIC&lt;/code&gt; &lt;/a&gt; 는 Akaike 정보 기준 (AIC) 및 Bayes 정보 기준 (BIC)의 사용을 제안합니다. k-fold cross-validation을 사용할 때 정규화 경로가 k + 1 번 대신 한 번만 계산되므로 최적의 알파 값을 찾는 것이 계산 상 저렴한 대안입니다. 그러나, 이러한 기준은 솔루션의 자유도에 대한 적절한 추정이 필요하고, 큰 샘플 (점근 적 결과)에 대해 도출되며 모델이 올바르다 고 가정합니다. 즉,이 모델에 의해 데이터가 실제로 생성된다고 가정합니다. 또한 문제가 심하게 조절되면 (샘플보다 더 많은 기능이있는 경우) 중단되는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="f454663104fe93624afb99e1fe4ea6ef9bcd01cf" translate="yes" xml:space="preserve">
          <source>Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:</source>
          <target state="translated">대안 적으로, 각 클래스의 확률은 예측 될 수 있는데, 이는 리프에서 동일한 클래스의 트레이닝 샘플의 비율이다 :</target>
        </trans-unit>
        <trans-unit id="5c74d35687e3587d17adc94ee7b35af54787e26a" translate="yes" xml:space="preserve">
          <source>Alternatively, the tree can also be exported in textual format with the function &lt;a href=&quot;generated/sklearn.tree.export_text#sklearn.tree.export_text&quot;&gt;&lt;code&gt;export_text&lt;/code&gt;&lt;/a&gt;. This method doesn&amp;rsquo;t require the installation of external libraries and is more compact:</source>
          <target state="translated">또는 &lt;a href=&quot;generated/sklearn.tree.export_text#sklearn.tree.export_text&quot;&gt; &lt;code&gt;export_text&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 트리를 텍스트 형식으로 내보낼 수도 있습니다 . 이 방법은 외부 라이브러리를 설치할 필요가 없으며 더 간단합니다.</target>
        </trans-unit>
        <trans-unit id="3d7768459c8a27625c6b1edd47cdb7cdd42a8dbd" translate="yes" xml:space="preserve">
          <source>Alternatively, using &lt;code&gt;precomputed&lt;/code&gt;, a user-provided affinity matrix can be used.</source>
          <target state="translated">대안 적으로, &lt;code&gt;precomputed&lt;/code&gt; 사용하여 , 사용자 제공 선호도 매트릭스가 사용될 수있다.</target>
        </trans-unit>
        <trans-unit id="408564450de584fdcd4c6ffe287273935ea61be3" translate="yes" xml:space="preserve">
          <source>Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt;. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; has &lt;code&gt;k - 1&lt;/code&gt; split nodes and thus can model interactions of up to order &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; .</source>
          <target state="translated">또는 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 매개 변수를 통해 리프 노드 수를 지정하여 트리 크기를 제어 할 수 있습니다 . 이 경우 불순물이 가장 많이 개선 된 노드가 먼저 확장되는 최상의 우선 검색을 사용하여 트리가 성장합니다. &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 인 트리 에는 &lt;code&gt;k - 1&lt;/code&gt; 분할 노드가 있으므로 최대 &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; 상호 작용을 모델링 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9d8e5a012d3af6b5916c5e0e392c2cf43112fb2f" translate="yes" xml:space="preserve">
          <source>Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.</source>
          <target state="translated">GMM은 종종 클러스터링에 사용되지만, 얻은 클러스터를 데이터 세트의 실제 클래스와 비교할 수 있습니다. 우리는이 비교를 유효하게 만들기 위해 훈련 세트의 클래스 수단으로 가우시안의 수단을 초기화합니다.</target>
        </trans-unit>
        <trans-unit id="9e3f75157b99771eea902cdfb247d8150fcbbb65" translate="yes" xml:space="preserve">
          <source>Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.</source>
          <target state="translated">세트 또는 튜플 목록은 다중 레이블 데이터를위한 매우 직관적 인 형식이지만 처리하기가 어렵습니다. 이 변환기는이 직관적 형식과 지원되는 다중 레이블 형식 사이를 변환합니다. 클래스 레이블이 있음을 나타내는 (샘플 x 클래스) 이진 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="6b97831f2bf852269cd56a6950af72a787322f90" translate="yes" xml:space="preserve">
          <source>Although online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="translated">온라인 방법이 로컬 최적 지점으로 수렴되는 것이 보장되지만 최적 지점의 품질과 수렴 속도는 미니 배치 크기 및 학습 속도 설정과 관련된 속성에 따라 달라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="be1a3f200b8c31300cb53ca0638c27940f9fd773" translate="yes" xml:space="preserve">
          <source>Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="translated">온라인 방식은 로컬 최적 점으로 수렴하는 것이 보장되지만, 최적 점의 품질과 수렴 속도는 학습률 설정과 관련된 미니 배치 크기 및 속성에 따라 달라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b824f979746fff21942ed5a8526c30831b67aa67" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility.</source>
          <target state="translated">항상 무시되며 호환성을 위해 존재합니다.</target>
        </trans-unit>
        <trans-unit id="f671e33354d7ef61d0b19d9b0bec50200bf9529c" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility. &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; may be used as a placeholder.</source>
          <target state="translated">항상 무시되며 호환성을 위해 존재합니다. &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; 는 자리 표시 자로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="86f5a3adc76607b6ec083bd7a2c05aeeca017cf3" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.</source>
          <target state="translated">transform 메소드를 호출 할 때 컨디셔닝을 개선하기 위해 적용 할 리지 수축량.</target>
        </trans-unit>
        <trans-unit id="a7500e59c81a09edaea684ef869962960b125989" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning.</source>
          <target state="translated">컨디셔닝을 개선하기 위해 적용 할 리지 수축량.</target>
        </trans-unit>
        <trans-unit id="a93d54da303ddad51202c963e4858505c997f8d0" translate="yes" xml:space="preserve">
          <source>Amount of verbosity.</source>
          <target state="translated">자세한 양.</target>
        </trans-unit>
        <trans-unit id="40cf2c52a2e789ddb84ca29dcc3ddee6f4122f49" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</source>
          <target state="translated">AdaBoost [1] 분류기는 원본 데이터 세트에 분류자를 맞추는 것으로 시작한 다음 동일한 데이터 세트에 분류기의 추가 사본을 맞추지 만 잘못 분류 된 인스턴스의 가중치가 조정되어 후속 분류 기가 더 집중하도록 메타 추정기입니다. 어려운 경우.</target>
        </trans-unit>
        <trans-unit id="043d85ce90b3af29bb38c14a359edb987b27e86d" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.</source>
          <target state="translated">AdaBoost [1] 회귀 분석기는 원래 데이터 세트에 회귀 분석자를 시작하여 시작한 다음 동일한 데이터 세트에 회귀 분석기의 추가 사본을 맞추지 만 현재 가중치의 오류에 따라 인스턴스 가중치가 조정되는 메타 추정기입니다. 따라서 후속 회귀 분석기는 어려운 경우에 더 집중합니다.</target>
        </trans-unit>
        <trans-unit id="a0726965169fa1a924e6755570036e923d55d00c" translate="yes" xml:space="preserve">
          <source>An AdaBoost classifier.</source>
          <target state="translated">AdaBoost 분류기.</target>
        </trans-unit>
        <trans-unit id="3388a3be3246d31996f2f786da9e288b6011901a" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.</source>
          <target state="translated">원래 데이터 세트에 회귀자를 맞추는 것으로 시작한 AdaBoost 회귀 분석기는 동일한 데이터 세트에 회귀 자의 추가 복사본을 맞추지 만 현재 예측의 오류에 따라 인스턴스의 가중치가 조정되는 경우입니다.</target>
        </trans-unit>
        <trans-unit id="0f161ccb66d6dfc0409a9a54a8f62d08c472ddf6" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor.</source>
          <target state="translated">AdaBoost 회귀 기.</target>
        </trans-unit>
        <trans-unit id="6356fae027d92d258a26309d7a3f256df2c9cc55" translate="yes" xml:space="preserve">
          <source>An Exception object.</source>
          <target state="translated">예외 객체.</target>
        </trans-unit>
        <trans-unit id="91b401262ec54e41ccc28fc97e3dca77764edee1" translate="yes" xml:space="preserve">
          <source>An already fitted classifier can be calibrated by setting &lt;code&gt;cv=&quot;prefit&quot;&lt;/code&gt;. In this case, the data is only used to fit the regressor. It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor.</source>
          <target state="translated">이미 장착 된 분류기는 &lt;code&gt;cv=&quot;prefit&quot;&lt;/code&gt; 을 설정하여 보정 할 수 있습니다 . 이 경우 데이터는 회귀 변수를 맞추는 데만 사용됩니다. 분류자를 피팅하는 데 사용되는 데이터가 회귀자를 피팅하는 데 사용되는 데이터와 분리되어 있는지 확인하는 것은 사용자에게 달려 있습니다.</target>
        </trans-unit>
        <trans-unit id="96b2243b1ac08495762e9c93f82c1e26829c8a97" translate="yes" xml:space="preserve">
          <source>An alternative and recommended approach is to use &lt;code&gt;StandardScaler&lt;/code&gt; in a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">대안 및 권장 방법은 &lt;code&gt;Pipeline&lt;/code&gt; 에서 &lt;code&gt;StandardScaler&lt;/code&gt; 를 사용 하는 것입니다</target>
        </trans-unit>
        <trans-unit id="6bcd8e2a5d780cc52692b09904c485c359805e94" translate="yes" xml:space="preserve">
          <source>An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt;&lt;code&gt;MaxAbsScaler&lt;/code&gt;&lt;/a&gt;, respectively.</source>
          <target state="translated">대안적인 표준화는 주어진 최소값과 최대 값 사이, 종종 0과 1 사이, 또는 각 기능의 최대 절대 값이 단위 크기로 조정되도록 기능을 스케일링하는 것입니다. 이것은 각각 &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt; &lt;code&gt;MaxAbsScaler&lt;/code&gt; 를&lt;/a&gt; 사용하여 달성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="dc1db7ec650725e3eb3f215f1e02c7a5ab5d599a" translate="yes" xml:space="preserve">
          <source>An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.</source>
          <target state="translated">다른 작업 인 얼굴 인식 또는 얼굴 식별은 다음과 같습니다. 알 수없는 사람의 얼굴 사진이 제공된 경우 이전에 확인 된 식별 된 사람들의 사진 갤러리를 참조하여 사람의 이름을 식별합니다.</target>
        </trans-unit>
        <trans-unit id="9f1396eaada67d4771525d15c3b9982306c357b5" translate="yes" xml:space="preserve">
          <source>An alternative to pickling is to export the model to another format using one of the model export tools listed under &lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;. Unlike pickling, once exported you cannot recover the full Scikit-learn estimator object, but you can deploy the model for prediction, usually by using tools supporting open model interchange formats such as &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; or &lt;a href=&quot;http://dmg.org/pmml/v4-4/GeneralStructure.html&quot;&gt;PMML&lt;/a&gt;.</source>
          <target state="translated">산세의 대안은 &lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;관련 프로젝트에&lt;/a&gt; 나열된 모델 내보내기 도구 중 하나를 사용하여 모델을 다른 형식으로 내보내는 것 입니다. 피클 링과 달리 일단 내 보낸 후에는 전체 Scikit-learn 추정기 객체를 복구 할 수 없지만 일반적으로 &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; 또는 &lt;a href=&quot;http://dmg.org/pmml/v4-4/GeneralStructure.html&quot;&gt;PMML&lt;/a&gt; 과 같은 개방형 모델 교환 형식을 지원하는 도구를 사용하여 예측을 위해 모델을 배포 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="63983f26490d9d0dd330cb65d368efc57d509991" translate="yes" xml:space="preserve">
          <source>An application of the different &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to &amp;lsquo;spread it open&amp;rsquo; whilst projecting it onto two dimensions.</source>
          <target state="translated">구형 데이터 세트에 대한 다양한 &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;매니 폴드 학습&lt;/a&gt; 기법 의 적용 . 여기에서 매니 폴드 학습 방법에 대한 직관을 얻기 위해 차원 축소의 사용을 볼 수 있습니다. 데이터 세트와 관련하여 기둥은 측면에서 얇게자를뿐만 아니라 구에서 절단됩니다. 이를 통해 매니 폴드 학습 기술은이를 2 차원으로 투사하면서 '확산'할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7255375783faf14b00594786271748afe79b54ea" translate="yes" xml:space="preserve">
          <source>An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix \(A\) has shape \(m \times n\), the Laplacian matrix for the corresponding bipartite graph has shape \((m + n) \times (m + n)\). However, in this case it is possible to work directly with \(A\), which is smaller and more efficient.</source>
          <target state="translated">최적의 정규화 된 절단에 대한 대략적인 솔루션은 그래프의 라플라시안의 일반화 된 고유 값 분해를 통해 찾을 수 있습니다. 일반적으로 이것은 라플라시안 행렬과 직접 작업하는 것을 의미합니다. 원래 데이터 행렬 \ (A \)의 모양이 \ (m \ times n \)이면 해당 이분 그래프의 라플라시안 행렬의 모양은 \ ((m + n) \ times (m + n) \)입니다. 그러나이 경우보다 작고 효율적인 \ (A \)를 사용하여 직접 작업 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="abda993d7d851644a2cce6e5b3201fc8e649cfdf" translate="yes" xml:space="preserve">
          <source>An approximation to the RBF kernel using random Fourier features.</source>
          <target state="translated">임의 푸리에 기능을 사용한 RBF 커널에 대한 근사치.</target>
        </trans-unit>
        <trans-unit id="56451d3e9b79dc1326101c93a26155d78a7c4638" translate="yes" xml:space="preserve">
          <source>An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size &lt;code&gt;radius&lt;/code&gt; around the query points.</source>
          <target state="translated">쿼리 지점 주위의 &lt;code&gt;radius&lt;/code&gt; 내에있는 모집단 행렬에서 가장 가까운 가장 가까운 점의 인덱스 배열로 구성된 배열입니다 .</target>
        </trans-unit>
        <trans-unit id="69a7cca6f3965cdbc1f2a0f6316fc86cbe697d1b" translate="yes" xml:space="preserve">
          <source>An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm &amp;lsquo;l1&amp;rsquo; or &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">X에 대해 주어진 축을 따라 규범의 배열입니다. X가 드문 경우 규범 'l1'또는 'l2'에 대해 NotImplementedError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="9eb7e49edfb49a7576a2bc3ab22563bf222603f7" translate="yes" xml:space="preserve">
          <source>An array of points to query</source>
          <target state="translated">쿼리 할 포인트의 배열</target>
        </trans-unit>
        <trans-unit id="deb131dede53f65a17272e64b8336596d48d67e7" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data (n_features).</source>
          <target state="translated">쿼리 할 포인트의 배열입니다. 마지막 차원은 훈련 데이터의 차원 (n_features)과 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="ce1fbc6a1ce43e2f3a3c1ddf094fbdf1907a173f" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data.</source>
          <target state="translated">쿼리 할 포인트의 배열입니다. 마지막 차원은 훈련 데이터의 차원과 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="f7729a5fd61fb2317d29018126beb8ea2ea03054" translate="yes" xml:space="preserve">
          <source>An array of type np.float</source>
          <target state="translated">np.float 유형의 배열</target>
        </trans-unit>
        <trans-unit id="9febb5eef4fe704c0e2f76fc17196ae84e5bb310" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_features).</source>
          <target state="translated">모양이 배열 (n_samples_X, n_features)입니다.</target>
        </trans-unit>
        <trans-unit id="43ecc2546079acafbd51894522d368c0004da034" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_samples_Y).</source>
          <target state="translated">모양이 배열 (n_samples_X, n_samples_Y)입니다.</target>
        </trans-unit>
        <trans-unit id="4e6f0a1fd0fa00a29e1c148218c8f262ec65e6a9" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_Y, n_features).</source>
          <target state="translated">모양이 배열 (n_samples_Y, n_features)입니다.</target>
        </trans-unit>
        <trans-unit id="55ae6635fc7f12f40e0f75b8c113526f67ceb18f" translate="yes" xml:space="preserve">
          <source>An axis object onto which the plots will be drawn.</source>
          <target state="translated">플롯이 그려지는 축 객체입니다.</target>
        </trans-unit>
        <trans-unit id="25ca61d5a0ba319e8f5dd67899dd1b75491f14b8" translate="yes" xml:space="preserve">
          <source>An early approach to taking advantage of this aggregate information was the &lt;em&gt;KD tree&lt;/em&gt; data structure (short for &lt;em&gt;K-dimensional tree&lt;/em&gt;), which generalizes two-dimensional &lt;em&gt;Quad-trees&lt;/em&gt; and 3-dimensional &lt;em&gt;Oct-trees&lt;/em&gt; to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \(D\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \(O[\log(N)]\) distance computations. Though the KD tree approach is very fast for low-dimensional (\(D &amp;lt; 20\)) neighbors searches, it becomes inefficient as \(D\) grows very large: this is one manifestation of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;. In scikit-learn, KD tree neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 집계 정보를 활용하기위한 초기 접근 방식은 &lt;em&gt;KD 트리&lt;/em&gt; 데이터 구조 ( &lt;em&gt;K 차원 트리의&lt;/em&gt; 약자 )로, 2 차원 &lt;em&gt;쿼드 트리&lt;/em&gt; 및 3 차원 &lt;em&gt;Oct-tree&lt;/em&gt; 를 일반화했습니다.&lt;em&gt;&lt;/em&gt;임의의 수의 차원으로. KD 트리는 데이터 축을 따라 매개 변수 공간을 재귀 적으로 분할하여 데이터 포인트가 저장되는 중첩 된 직교 이방성 영역으로 나누는 이진 트리 구조입니다. KD 트리의 구성은 매우 빠릅니다. 파티셔닝은 데이터 축을 따라서 만 수행되므로 \ (D \) 차원 거리를 계산할 필요가 없습니다. 일단 구축되면, 쿼리 포인트의 가장 가까운 이웃은 \ (O [\ log (N)] \) 거리 계산만으로 결정될 수 있습니다. KD 트리 접근 방식은 저 차원 (\ (D &amp;lt;20 \)) 이웃 검색에 대해 매우 빠르지 만 \ (D \)가 매우 커짐에 따라 비효율적입니다. &amp;rdquo;. scikit-learn에서 KD 트리 이웃 검색은 키워드 &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt; 사용하여 지정됩니다.&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 클래스를 사용하여 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="099567709025ab0aa48e57d57280b0443b9a24ed" translate="yes" xml:space="preserve">
          <source>An empty dict signifies default parameters.</source>
          <target state="translated">빈 dict는 기본 매개 변수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="ffa5ed2f9779625aa20adf682c3351ed0a53fb7f" translate="yes" xml:space="preserve">
          <source>An encoding can also be called a &amp;lsquo;character set&amp;rsquo;, but this term is less accurate: several encodings can exist for a single character set.</source>
          <target state="translated">인코딩을 '문자 세트'라고도 할 수 있지만이 용어의 정확성은 떨어집니다. 단일 문자 세트에 여러 인코딩이 존재할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="15c84c56e7d6dd7f29f03b3419a89a1fa5861864" translate="yes" xml:space="preserve">
          <source>An ensemble of totally random trees.</source>
          <target state="translated">완전히 임의의 나무의 앙상블.</target>
        </trans-unit>
        <trans-unit id="372a9ef151ce0e02d102211d2e14fc577127d782" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; 및 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict를&lt;/a&gt; 구현하는 추정기 개체 입니다.</target>
        </trans-unit>
        <trans-unit id="b0193485700595d6695b40a0a211b9e29e959231" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and one of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합을&lt;/a&gt; 구현하는 추정기 객체 와 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; 또는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; 중 하나입니다 .</target>
        </trans-unit>
        <trans-unit id="215970b6b504c6bfb744566538a8accc41cd92e5" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; , &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt; 및 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba를&lt;/a&gt; 구현하는 추정기 개체 입니다.</target>
        </trans-unit>
        <trans-unit id="ed51316796470f5285f1ea5227efd05cf75b44c0" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 과 &lt;code&gt;predict&lt;/code&gt; 를 구현하는 추정기 객체 .</target>
        </trans-unit>
        <trans-unit id="2c736b1e847c7ee4aecf59b3bb14cb64e436dee2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and one of &lt;code&gt;decision_function&lt;/code&gt; or &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 및 &lt;code&gt;decision_function&lt;/code&gt; 또는 &lt;code&gt;predict_proba&lt;/code&gt; 중 하나를 구현하는 추정기 오브젝트 입니다.</target>
        </trans-unit>
        <trans-unit id="016774ede32e1f200ea316bb37e829ebc5d620e2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;score&lt;/code&gt; and &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; , &lt;code&gt;score&lt;/code&gt; 및 &lt;code&gt;predict_proba&lt;/code&gt; 를 구현하는 추정기 객체 입니다.</target>
        </trans-unit>
        <trans-unit id="bc6d2e5beaf0995cbcda2b429dbc0e874dd47ce4" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.predict_proba&quot;&gt;&lt;code&gt;predict_proba&lt;/code&gt;&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default, a &lt;code&gt;DummyEstimator&lt;/code&gt; predicting the classes priors is used.</source>
          <target state="translated">초기 예측을 계산하는 데 사용되는 추정기 객체입니다. &lt;code&gt;init&lt;/code&gt; 는 &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.predict_proba&quot;&gt; &lt;code&gt;predict_proba&lt;/code&gt; &lt;/a&gt; 를 제공해야합니다 . '0'이면 초기 원시 예측이 0으로 설정됩니다. 기본적으로 클래스 사전을 예측 하는 &lt;code&gt;DummyEstimator&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="cd1e7319f04194cc6e7a0ff87c048ca39034bdd7" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default a &lt;code&gt;DummyEstimator&lt;/code&gt; is used, predicting either the average target value (for loss=&amp;rsquo;ls&amp;rsquo;), or a quantile for the other losses.</source>
          <target state="translated">초기 예측을 계산하는 데 사용되는 추정기 객체입니다. &lt;code&gt;init&lt;/code&gt; 는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합&lt;/a&gt; 하고 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;예측해야&lt;/a&gt; 합니다. '0'이면 초기 원시 예측이 0으로 설정됩니다. 기본적으로 &lt;code&gt;DummyEstimator&lt;/code&gt; 가 사용되어 평균 목표 값 (loss = 'ls'의 경우) 또는 다른 손실의 분위수를 예측합니다.</target>
        </trans-unit>
        <trans-unit id="3e3cbd1a80e427709b1e224e290f485edaa6e701" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. If None it uses &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">초기 예측을 계산하는 데 사용되는 추정기 개체입니다. &lt;code&gt;init&lt;/code&gt; 은 &lt;code&gt;fit&lt;/code&gt; 하고 &lt;code&gt;predict&lt;/code&gt; 합니다. None이면 &lt;code&gt;loss.init_estimator&lt;/code&gt; 를 사용 합니다 .</target>
        </trans-unit>
        <trans-unit id="cf6861479f811af12e4fa2074ef55363e1ab1784" translate="yes" xml:space="preserve">
          <source>An estimator that has already been &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fitted&quot;&gt;fitted&lt;/a&gt; and is compatible with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scorer&quot;&gt;scorer&lt;/a&gt;.</source>
          <target state="translated">이미 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fitted&quot;&gt;장착되어&lt;/a&gt; 있고 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scorer&quot;&gt;득점자&lt;/a&gt; 와 호환 되는 추정기 .</target>
        </trans-unit>
        <trans-unit id="8add3305e3de95c19db62ec2f9ba60a9992f6c8c" translate="yes" xml:space="preserve">
          <source>An estimator to inspect.</source>
          <target state="translated">검사 할 추정기.</target>
        </trans-unit>
        <trans-unit id="0c14b451345a9bebab4624cdfc1eb448ab003b65" translate="yes" xml:space="preserve">
          <source>An example comparing nearest neighbors classification with and without Neighborhood Components Analysis.</source>
          <target state="translated">Neighborhood Components Analysis를 사용하거나 사용하지 않고 최근 접 이웃 분류를 비교하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="0583622ca0751c993f08e9d98041286805dcca3a" translate="yes" xml:space="preserve">
          <source>An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online &lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;Dictionary Learning&lt;/a&gt; and various transform methods.</source>
          <target state="translated">온라인 &lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;사전 학습&lt;/a&gt; 과 다양한 변형 방법을 사용하여 너구리 얼굴 이미지의 노이즈 조각을 재구성하는 효과를 비교하는 예제 입니다.</target>
        </trans-unit>
        <trans-unit id="d3ed7be079cbbb4b66d35ed08e7318047e508129" translate="yes" xml:space="preserve">
          <source>An example illustrating the approximation of the feature map of an RBF kernel.</source>
          <target state="translated">RBF 커널의 기능 맵의 근사값을 보여주는 예제입니다.</target>
        </trans-unit>
        <trans-unit id="5f84ab4970b6751b484f351a6364e917ee9e2e2e" translate="yes" xml:space="preserve">
          <source>An example of a chunked operation adhering to this setting is &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt;, which facilitates computing row-wise reductions of a pairwise distance matrix.</source>
          <target state="translated">이 설정을 준수하는 청크 연산의 예는 &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt; 이며, 이는 페어 단위 거리 행렬의 행 단위 감소 계산을 용이하게합니다.</target>
        </trans-unit>
        <trans-unit id="d30da83c344e58d7fad635964b30a7c96a578d3f" translate="yes" xml:space="preserve">
          <source>An example of an estimator is the class &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;, which implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;support vector classification&lt;/a&gt;. The estimator&amp;rsquo;s constructor takes as arguments the model&amp;rsquo;s parameters.</source>
          <target state="translated">추정기의 예는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;지원 벡터 분류&lt;/a&gt; 를 구현 하는 &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; 클래스 입니다. 추정기의 생성자는 모델의 매개 변수를 인수로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2a28c2a5262858339108957d662b2d9ad76c60fe" translate="yes" xml:space="preserve">
          <source>An example of biclusters formed by partitioning rows and columns.</source>
          <target state="translated">행과 열을 분할하여 형성되는 biclusters의 예입니다.</target>
        </trans-unit>
        <trans-unit id="4ec3a210a521488a01f06a7254c4cbcafb1acb47" translate="yes" xml:space="preserve">
          <source>An example of checkerboard biclusters.</source>
          <target state="translated">바둑판 biclusters의 예입니다.</target>
        </trans-unit>
        <trans-unit id="263d7481d0f6606e447078322b3482f64ed58b85" translate="yes" xml:space="preserve">
          <source>An example of estimating sources from noisy data.</source>
          <target state="translated">시끄러운 데이터에서 소스를 추정하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="ba8a60e5ae0a34a0a9e8c9683b9f2c1dae098cc6" translate="yes" xml:space="preserve">
          <source>An example of reshaping data would be the digits dataset</source>
          <target state="translated">데이터 재구성의 예는 숫자 데이터 세트입니다.</target>
        </trans-unit>
        <trans-unit id="fbe3a4b0f4df558e025b4b6c278ce0b8f2e24f89" translate="yes" xml:space="preserve">
          <source>An example of the HTML output can be seen in the &lt;strong&gt;HTML representation of Pipeline&lt;/strong&gt; section of &lt;a href=&quot;../auto_examples/compose/plot_column_transformer_mixed_types#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py&quot;&gt;Column Transformer with Mixed Types&lt;/a&gt;. As an alternative, the HTML can be written to a file using &lt;a href=&quot;generated/sklearn.utils.estimator_html_repr#sklearn.utils.estimator_html_repr&quot;&gt;&lt;code&gt;estimator_html_repr&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">HTML 출력의 예는 &lt;a href=&quot;../auto_examples/compose/plot_column_transformer_mixed_types#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py&quot;&gt;혼합 유형&lt;/a&gt; 이있는 열 변환기 &lt;strong&gt;의 파이프 라인&lt;/strong&gt; 섹션의 &lt;strong&gt;HTML 표현&lt;/strong&gt; 에서 볼 수 있습니다 . 대안으로 &lt;a href=&quot;generated/sklearn.utils.estimator_html_repr#sklearn.utils.estimator_html_repr&quot;&gt; &lt;code&gt;estimator_html_repr&lt;/code&gt; 을&lt;/a&gt; 사용하여 HTML을 파일에 쓸 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1fbe1528edfe5c2ef6e2542ba5d3a7a740cb4806" translate="yes" xml:space="preserve">
          <source>An example of the same &lt;code&gt;y&lt;/code&gt; in sparse matrix form:</source>
          <target state="translated">희소 행렬 형식 의 동일한 &lt;code&gt;y&lt;/code&gt; 예 :</target>
        </trans-unit>
        <trans-unit id="d2d2bef04bd500898cd35385a3db44c487620221" translate="yes" xml:space="preserve">
          <source>An example showing how different online solvers perform on the hand-written digits dataset.</source>
          <target state="translated">손으로 쓴 숫자 데이터 집합에서 다양한 온라인 솔버가 수행되는 방식을 보여주는 예입니다.</target>
        </trans-unit>
        <trans-unit id="2aae52ed09d5204233cf6faccc3b62129219ab9a" translate="yes" xml:space="preserve">
          <source>An example showing how the scikit-learn can be used to recognize images of hand-written digits.</source>
          <target state="translated">scikit-learn을 사용하여 손으로 쓴 숫자의 이미지를 인식하는 방법을 보여주는 예.</target>
        </trans-unit>
        <trans-unit id="dc6fbf3a5e9982b266901310af78a9acc8d8b83c" translate="yes" xml:space="preserve">
          <source>An example showing univariate feature selection.</source>
          <target state="translated">일 변량 피처 선택을 보여주는 예입니다.</target>
        </trans-unit>
        <trans-unit id="a63d45f44a041fe05e157072e07ec905aa6168f0" translate="yes" xml:space="preserve">
          <source>An example to compare multi-output regression with random forest and the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator.</source>
          <target state="translated">다중 출력 회귀 분석과 임의 포리스트 및 &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; 메타 추정기 를 비교하는 예제 입니다.</target>
        </trans-unit>
        <trans-unit id="d69827da4fe888278b671529418570646583bc79" translate="yes" xml:space="preserve">
          <source>An example to illustrate multi-output regression with decision tree.</source>
          <target state="translated">의사 결정 트리를 사용하여 다중 출력 회귀를 설명하는 예제입니다.</target>
        </trans-unit>
        <trans-unit id="278a8759739f9d084f6d58de5359105935a98d5a" translate="yes" xml:space="preserve">
          <source>An example to show covariance estimation with the Mahalanobis distances on Gaussian distributed data.</source>
          <target state="translated">가우시안 분포 데이터에서 Mahalanobis 거리와 공분산 추정을 보여주는 예제입니다.</target>
        </trans-unit>
        <trans-unit id="79be1ff44d921e5597feeeddf48473c82478df46" translate="yes" xml:space="preserve">
          <source>An example using &lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; for anomaly detection.</source>
          <target state="translated">이상 감지에 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt; 를 사용하는 예 입니다.</target>
        </trans-unit>
        <trans-unit id="c015cad72773b21af2994319c7e4935c0925533a" translate="yes" xml:space="preserve">
          <source>An example using a one-class SVM for novelty detection.</source>
          <target state="translated">참신 탐지를 위해 1 클래스 SVM을 사용하는 예.</target>
        </trans-unit>
        <trans-unit id="e3e6ce179701e2cc84a27da4032032e9c54e1a80" translate="yes" xml:space="preserve">
          <source>An extra-trees classifier.</source>
          <target state="translated">여분의 나무 분류기.</target>
        </trans-unit>
        <trans-unit id="402fd1d845155a85adc98aea2772c11c414eb821" translate="yes" xml:space="preserve">
          <source>An extra-trees regressor.</source>
          <target state="translated">여분의 나무 회귀 기.</target>
        </trans-unit>
        <trans-unit id="36e1ed7c225c31bf0ba3eb4dff97aa286624cf9d" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree classifier.</source>
          <target state="translated">매우 무작위 화 된 트리 분류기.</target>
        </trans-unit>
        <trans-unit id="2963ff7051490c77b458af39546e6bfce2134347" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree regressor.</source>
          <target state="translated">매우 무작위 화 된 트리 회귀 기.</target>
        </trans-unit>
        <trans-unit id="2e305a3161f1f3a1a75895f3bf89737b2fa1110e" translate="yes" xml:space="preserve">
          <source>An illustration of Swiss Roll reduction with locally linear embedding</source>
          <target state="translated">로컬 선형 임베딩을 사용한 스위스 롤 축소 그림</target>
        </trans-unit>
        <trans-unit id="ff57c22861af0b46c7c064d86f712f1102614de2" translate="yes" xml:space="preserve">
          <source>An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.</source>
          <target state="translated">다양한 매니 폴드 학습 방법을 사용하여 S- 곡선 데이터 세트의 차원 축소 그림.</target>
        </trans-unit>
        <trans-unit id="461c40d4fafb15fd1dbf5dddb7defd80f66220bc" translate="yes" xml:space="preserve">
          <source>An illustration of t-SNE on the two concentric circles and the S-curve datasets for different perplexity values.</source>
          <target state="translated">두 개의 동심원에서의 t-SNE와 다른 난수 값에 대한 S- 곡선 데이터 세트의 그림.</target>
        </trans-unit>
        <trans-unit id="4217a4a05133f009315b7adf6df3894014f6eea5" translate="yes" xml:space="preserve">
          <source>An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the mean squared error on the training data. The benefit of such a model is that it does not assume any form for the target function such as linearity. For comparison a linear regression is also presented.</source>
          <target state="translated">생성 된 데이터의 등장 성 회귀 그림. 등장 성 회귀는 훈련 데이터에 대한 평균 제곱 오차를 최소화하면서 함수의 감소하지 않는 근사값을 찾습니다. 이러한 모델의 이점은 선형성과 같은 대상 함수에 대해 어떤 형태도 가정하지 않는다는 것입니다. 비교를 위해 선형 회귀도 제시됩니다.</target>
        </trans-unit>
        <trans-unit id="95f191130b61aa88bf4d9f0c56ed2e78e6bd9498" translate="yes" xml:space="preserve">
          <source>An illustration of the metric and non-metric MDS on generated noisy data.</source>
          <target state="translated">생성 된 노이즈 데이터에 대한 메트릭 및 비 메트릭 MDS를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="4ed4596d6f2bc0049548fe72a3bd1528d6ff1a95" translate="yes" xml:space="preserve">
          <source>An illustration of various embeddings on the digits dataset.</source>
          <target state="translated">숫자 데이터 세트에 다양한 임베딩을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="5349a7af33dbb0da98921a0576af67237bdedc22" translate="yes" xml:space="preserve">
          <source>An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset.</source>
          <target state="translated">숫자 데이터 세트의 2D 임베딩에 대한 집성 클러스터링을위한 다양한 연계 옵션을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="9431c782d6d1945d9109d26f9bdc51c8ca5b4cc9" translate="yes" xml:space="preserve">
          <source>An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014</source>
          <target state="translated">주성분 분석을위한 무작위 알고리즘의 구현 A. Szlam et al. 2014 년</target>
        </trans-unit>
        <trans-unit id="97eec2318d7e6e3549c069ff7f108d7733cec0af" translate="yes" xml:space="preserve">
          <source>An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.</source>
          <target state="translated">성능 최적화의 중요한 측면은 예측 정확도를 떨어 뜨릴 수 있다는 것입니다. 실제로 더 단순한 모델 (예 : 비선형 대신 선형 또는 더 적은 수의 매개 변수)은 더 빠르게 실행되지만 더 복잡한 모델과 동일한 데이터의 정확한 속성을 항상 고려할 수는 없습니다.</target>
        </trans-unit>
        <trans-unit id="44f31261fda2a48216b1472f9c206633a0f349f1" translate="yes" xml:space="preserve">
          <source>An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data.</source>
          <target state="translated">견고한 피팅의 중요한 개념은 고 장점이라는 것입니다. 적합치에 포함 된 데이터가 누락되기 시작하기에 적합하지 않은 데이터의 비율입니다.</target>
        </trans-unit>
        <trans-unit id="98153081fcfad9474299ea23738f0ec83b423b23" translate="yes" xml:space="preserve">
          <source>An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn&amp;rsquo;t fit this manual, one can think of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;stick breaking process&lt;/a&gt; analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don&amp;rsquo;t fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller value of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights).</source>
          <target state="translated">중요한 질문은 Dirichlet 프로세스가 무한한 무제한의 클러스터를 사용하고 일관성을 유지하는 방법입니다. 전체 설명이이 매뉴얼에 맞지 않지만 &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;스틱 브레이킹 프로세스를&lt;/a&gt; 생각할 수 있습니다.그것을 이해하는 데 도움이되는 비유. 스틱 브레이킹 프로세스는 Dirichlet 프로세스의 생성 스토리입니다. 우리는 단위 길이의 스틱으로 시작하고 각 단계에서 나머지 스틱의 일부를 끊습니다. 매번 스틱 조각의 길이를 혼합물 그룹에 속하는 점의 비율에 연결합니다. 결국, 무한한 혼합물을 나타 내기 위해 스틱의 마지막 남은 부분을 다른 모든 그룹에 속하지 않는 점의 비율과 연결합니다. 각 조각의 길이는 농도 매개 변수에 비례하는 확률을 갖는 임의 변수입니다. 더 작은 농도 값은 단위 길이를 더 큰 스틱 조각으로 나눕니다 (더 집중된 분포를 정의).더 큰 농도 값은 더 작은 스틱 조각을 생성합니다 (무게가 0 인 구성 요소의 수를 증가시킵니다).</target>
        </trans-unit>
        <trans-unit id="627c954c961a3acd43edae0e16b57eae5630ce43" translate="yes" xml:space="preserve">
          <source>An improvement of the Ledoit-Wolf shrinkage, the &lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;sklearn.covariance.OAS&lt;/code&gt;&lt;/a&gt;, proposed by Chen et al. Its convergence is significantly better under the assumption that the data are Gaussian, in particular for small samples.</source>
          <target state="translated">Chen 등이 제안한 &lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;sklearn.covariance.OAS&lt;/code&gt; &lt;/a&gt; 인 Ledoit-Wolf 수축의 개선 . 수렴은 특히 작은 샘플의 경우 데이터가 가우시안이라는 가정하에 훨씬 더 좋습니다.</target>
        </trans-unit>
        <trans-unit id="370f3995d8ef065bc41012596e70d603a42a06c3" translate="yes" xml:space="preserve">
          <source>An index that selects the retained features from a feature vector. If &lt;code&gt;indices&lt;/code&gt; is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If &lt;code&gt;indices&lt;/code&gt; is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.</source>
          <target state="translated">피처 벡터에서 유지 된 피처를 선택하는 인덱스입니다. 경우 &lt;code&gt;indices&lt;/code&gt; 거짓이고,이 그의 대응하는 기능이 유지된다 IFF에 선택된 요소가 트루 인 형상의 부울 배열 [# 입력 특징]이다. &lt;code&gt;indices&lt;/code&gt; 가 True 인 경우 값이 입력 피처 벡터에 대한 인덱스 인 모양 [# 출력 피처]의 정수 배열입니다.</target>
        </trans-unit>
        <trans-unit id="537bad4eab470a047cae67c34b767bf9f3d01c01" translate="yes" xml:space="preserve">
          <source>An instance of the estimator.</source>
          <target state="translated">추정기의 인스턴스입니다.</target>
        </trans-unit>
        <trans-unit id="d0f36ad8d88245eefc6653aba92b1989e3a7de69" translate="yes" xml:space="preserve">
          <source>An int, giving the exact number of total jobs that are spawned</source>
          <target state="translated">생성 된 정확한 총 작업 수를 제공하는 int</target>
        </trans-unit>
        <trans-unit id="efb853d408e1363bd14470415d5367521c99f9fa" translate="yes" xml:space="preserve">
          <source>An interesting aspect of &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt; 의 흥미로운 측면은 각 데이터에 대해 주어진 데이터 구조에 따라 인접 샘플을 정의하는 연결 매트릭스를 통해 연결 제약 조건을이 알고리즘에 추가 할 수 있다는 것입니다 (인접한 클러스터 만 병합 할 수 있음). 예를 들어, 아래 스위스 롤 예에서 연결 제한은 스위스 롤에 인접하지 않은 포인트의 병합을 금지하므로 롤의 겹치는 부분에 걸쳐 확장되는 클러스터를 형성하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="edca22917cc04c2cee4dca2af159124718d78594" translate="yes" xml:space="preserve">
          <source>An interesting development of using a &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is the ability to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;out-of-core&lt;/a&gt; scaling. This means that we can learn from data that does not fit into the computer&amp;rsquo;s main memory.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt; 를 사용하는 흥미로운 개발은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;코어 외부&lt;/a&gt; 스케일링 을 수행 하는 기능입니다 . 이것은 컴퓨터의 메인 메모리에 맞지 않는 데이터로부터 배울 수 있다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="e061f170f65c98ddc125ba623de0d484bced6052" translate="yes" xml:space="preserve">
          <source>An introduction to machine learning with scikit-learn</source>
          <target state="translated">scikit-learn을 통한 머신 러닝 소개</target>
        </trans-unit>
        <trans-unit id="f8000912a05784b4ed09f166d2281bb382154409" translate="yes" xml:space="preserve">
          <source>An iterable which yields either str, unicode or file objects.</source>
          <target state="translated">str, unicode 또는 file 객체를 생성하는 iterable.</target>
        </trans-unit>
        <trans-unit id="ed21dc2777fffee9a18d3be232cdf0f7a675b104" translate="yes" xml:space="preserve">
          <source>An iterable yielding (train, test) splits as arrays of indices.</source>
          <target state="translated">반복 가능한 양보 (학습, 테스트)는 인덱스 배열로 분할됩니다.</target>
        </trans-unit>
        <trans-unit id="26021975f66731cfc83771c3f9a3b8617a224127" translate="yes" xml:space="preserve">
          <source>An iterable yielding train, test splits.</source>
          <target state="translated">반복 가능한 항복 열차, 테스트 분할.</target>
        </trans-unit>
        <trans-unit id="793ecf7b6fedca1d245cd9de3dfad1789ad435c2" translate="yes" xml:space="preserve">
          <source>An iterable yielding train/test splits.</source>
          <target state="translated">반복 가능한 항복 열차 / 테스트 분할.</target>
        </trans-unit>
        <trans-unit id="2c74b450a38327bc579731e9a6b693f5a72972a9" translate="yes" xml:space="preserve">
          <source>An object for detecting outliers in a Gaussian distributed dataset.</source>
          <target state="translated">가우스 분산 데이터 세트에서 특이 치를 탐지하기위한 객체입니다.</target>
        </trans-unit>
        <trans-unit id="bb3e30a61bd34f61e007c1401c3e81f0d43c8edb" translate="yes" xml:space="preserve">
          <source>An object of that type which is cloned for each validation.</source>
          <target state="translated">각 유효성 검사에 대해 복제되는 해당 유형의 개체입니다.</target>
        </trans-unit>
        <trans-unit id="9720a88e8aa2bbe2107788283b8978f9a7cb4b22" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator,</source>
          <target state="translated">교차 검증 생성기로 사용할 객체,</target>
        </trans-unit>
        <trans-unit id="bbffbe09f32930b2800698ca47bd093017747bdf" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator.</source>
          <target state="translated">교차 유효성 검사 생성기로 사용되는 개체입니다.</target>
        </trans-unit>
        <trans-unit id="a7002171a59c54b21633748422d618355d53d5f0" translate="yes" xml:space="preserve">
          <source>An optional mask of the image, to consider only part of the pixels.</source>
          <target state="translated">픽셀의 일부만 고려하는 이미지의 선택적 마스크입니다.</target>
        </trans-unit>
        <trans-unit id="fe8123eda49913b2b31a4b7b815dc029043ed233" translate="yes" xml:space="preserve">
          <source>An optional progress meter.</source>
          <target state="translated">선택적 진행 측정기.</target>
        </trans-unit>
        <trans-unit id="229c2b7073ef4d939c3f4e9dc0933cc0c25d3c6e" translate="yes" xml:space="preserve">
          <source>An optional second feature array. Only allowed if metric != &amp;ldquo;precomputed&amp;rdquo;.</source>
          <target state="translated">선택적 두 번째 기능 배열입니다. 메트릭! = &quot;사전 계산 된&quot;경우에만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="0072ba612e2ac9888a715c6d07a5f35671c0b0f4" translate="yes" xml:space="preserve">
          <source>An ordered array of unique labels.</source>
          <target state="translated">순서가 지정된 고유 레이블 배열.</target>
        </trans-unit>
        <trans-unit id="a760a22c84feac040244658e5cd6e733d7fd7a9c" translate="yes" xml:space="preserve">
          <source>An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.</source>
          <target state="translated">감독되지 않은 데이터 집합을 고차원 희소 표현으로 변환합니다. 데이터 포인트는 정렬되는 각 트리의 리프에 따라 코딩됩니다. 나뭇잎의 원 핫 인코딩을 사용하면 포리스트에 트리가있는 수만큼의 이진 코딩이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="8bba08ddfbe8b42a7ddb2ba7f5d85088746ededd" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of margin errors (see &lt;a href=&quot;../svm#nu-svc&quot;&gt;User Guide&lt;/a&gt;) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="translated">여백 오류 비율의 상한 (사용 &lt;a href=&quot;../svm#nu-svc&quot;&gt;설명서 참조&lt;/a&gt; ) 및지지 벡터 비율의 하한. 간격 (0, 1]에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="0d82cfe79ef09c873c85764ea87217dd841df582" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="translated">훈련 오류의 비율에 대한 상한 및 지원 벡터의 비율에 대한 하한 간격 (0, 1)에 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="5f9f028f3aaaed07cbca0fd41b78759c2fe2428d" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.</source>
          <target state="translated">훈련 오류의 비율에 대한 상한 및 지원 벡터의 비율에 대한 하한 간격 (0, 1)에 있어야하며 기본적으로 0.5가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c638b2709b286483a90c4415602f89072f7cd76d" translate="yes" xml:space="preserve">
          <source>Analysis of the plots</source>
          <target state="translated">플롯 분석</target>
        </trans-unit>
        <trans-unit id="130d90555949c1ba7863adb5fc8aa4d5f4dd611e" translate="yes" xml:space="preserve">
          <source>Analyzing a portion of the ROC curve. McClish, 1989</source>
          <target state="translated">ROC 곡선의 일부 분석. 맥 클리시, 1989</target>
        </trans-unit>
        <trans-unit id="b98195adea1583e9b237e74652523f846b2b1879" translate="yes" xml:space="preserve">
          <source>And for multiple metric evaluation, the return value is a dict with the following keys - &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">그리고 다중 메트릭 평가의 경우 반환 값은 &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c48696369bc4cd123e13a474e64394fa9533d40a" translate="yes" xml:space="preserve">
          <source>And some work with binary and multilabel (but not multiclass) problems:</source>
          <target state="translated">그리고 일부는 바이너리 및 멀티 라벨 (멀티 클래스는 아님) 문제로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="551fc600bc5b2c147bfd7d608b1227c6f0c95818" translate="yes" xml:space="preserve">
          <source>And the L2-normalized tf-idf changes to</source>
          <target state="translated">L2 정규화 된 tf-idf는</target>
        </trans-unit>
        <trans-unit id="fd06a66dae1b69b6eb93cccfb52c33888c5a371c" translate="yes" xml:space="preserve">
          <source>And the classifier &amp;ldquo;predictions&amp;rdquo; are perfect:</source>
          <target state="translated">분류기&amp;ldquo;예측&amp;rdquo;은 완벽합니다.</target>
        </trans-unit>
        <trans-unit id="b646eac54bde60aa4d7c89014b7fd028412efa2f" translate="yes" xml:space="preserve">
          <source>Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure</source>
          <target state="translated">Andrew Rosenberg와 Julia Hirschberg, 2007. V-Measure : 조건부 엔트로피 기반 외부 클러스터 평가 측정</target>
        </trans-unit>
        <trans-unit id="5ca338dedf0d8331238e60ad1d0242b94a749529" translate="yes" xml:space="preserve">
          <source>Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J&amp;ouml;rg Sander. &amp;ldquo;OPTICS: ordering points to identify the clustering structure.&amp;rdquo; ACM SIGMOD Record 28, no. 2 (1999): 49-60.</source>
          <target state="translated">Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel 및 J&amp;ouml;rg Sander. &quot;OPTICS : 클러스터링 구조를 식별하기위한 순서 지정 지점.&quot; ACM SIGMOD 레코드 28, no. 2 (1999) : 49-60.</target>
        </trans-unit>
        <trans-unit id="6e2b1b23fa02a3f4b2a576f98b5ef19fd6944167" translate="yes" xml:space="preserve">
          <source>Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points.</source>
          <target state="translated">다른 대안은 포인트의 k 개의 가장 가까운 이웃 연결 매트릭스의 대칭 버전을 취하는 것이다.</target>
        </trans-unit>
        <trans-unit id="c8a5ccc165016bcc8540311d11c1eec6480387f0" translate="yes" xml:space="preserve">
          <source>Another approach is to monitor convergence on a validation score. In this case, the input data is split into a training set and a validation set. The model is then fitted on the training set and the stopping criterion is based on the prediction score computed on the validation set. This enables us to find the least number of iterations which is sufficient to build a model that generalizes well to unseen data and reduces the chance of over-fitting the training data.</source>
          <target state="translated">다른 접근 방식은 유효성 검사 점수에 대한 수렴을 모니터링하는 것입니다. 이 경우 입력 데이터는 학습 세트와 유효성 검사 세트로 분할됩니다. 그런 다음 모델은 학습 세트에 맞춰지고 중지 기준은 유효성 검사 세트에서 계산 된 예측 점수를 기반으로합니다. 이를 통해 우리는 보이지 않는 데이터를 잘 일반화하고 훈련 데이터에 과적 합할 가능성을 줄이는 모델을 구축하기에 충분한 최소 반복 횟수를 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="be7184b0e2bd98f85cd552cd8ec12d77ca9e3964" translate="yes" xml:space="preserve">
          <source>Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the &lt;code&gt;Perceptron&lt;/code&gt; is still sensitive to badly labeled examples even after many examples whereas the &lt;code&gt;SGD*&lt;/code&gt; and &lt;code&gt;PassiveAggressive*&lt;/code&gt; families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.</source>
          <target state="translated">적절한 알고리즘을 선택할 때 고려해야 할 또 다른 측면은 시간이 지남에 따라 모든 예제에서 동일한 중요성을 부여하지는 않는다는 것입니다. 즉, &lt;code&gt;Perceptron&lt;/code&gt; 은 많은 예제 후에도 레이블이 잘못 지정된 예제에 여전히 민감하지만 &lt;code&gt;SGD*&lt;/code&gt; 및 &lt;code&gt;PassiveAggressive*&lt;/code&gt; 제품군은 이러한 종류의 아티팩트에 더 강력합니다. 반대로, 후자는 학습 속도가 시간이 지남에 따라 감소함에 따라 스트림에서 늦게 올 때 놀랍도록 다르지만 적절하게 레이블이 지정된 예제에 대해서는 덜 중요합니다.</target>
        </trans-unit>
        <trans-unit id="32a1cd2cb87665a11effa4dc4df8f03517ac0638" translate="yes" xml:space="preserve">
          <source>Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">또 다른 일반적인 응용은 시간 정보를 사용하는 것입니다. 예를 들어, 그룹은 샘플을 수집 한 연도 일 수 있으므로 시간 기반 분할에 대한 교차 검증을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="8230731c6a46697977b2ab40afba474c6e0f5aba" translate="yes" xml:space="preserve">
          <source>Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm.</source>
          <target state="translated">중간 수준의 높은 차원 데이터 집합에서 특이 값 탐지를 수행하는 또 다른 효율적인 방법은 LOF (Local Outlier Factor) 알고리즘을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="421e4566a6c577d45c51d939beed6fdb20866913" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="translated">멀티 클래스 분류에 대한 또 다른 평가 방법은 매크로 평균화로 각 레이블의 분류에 동일한 가중치를 부여합니다.</target>
        </trans-unit>
        <trans-unit id="9aeb8b5cf9580937985ce741399f76caf7f788f7" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-label classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="translated">다중 레이블 분류에 대한 또 다른 평가 척도는 각 레이블의 분류에 동일한 가중치를 부여하는 매크로 평균입니다.</target>
        </trans-unit>
        <trans-unit id="103287c3ab74c62412ea0fba62e90af2b9fbac2a" translate="yes" xml:space="preserve">
          <source>Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt; example that measures this quantity for a number of estimators on synthetic data:</source>
          <target state="translated">생산 시스템의 크기를 조정할 때주의해야 할 또 다른 중요한 지표는 처리량, 즉 주어진 시간에 예측할 수있는 횟수입니다. 다음은 &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;예측 지연 시간&lt;/a&gt; 예제 의 벤치 마크로 합성 데이터에 대한 여러 추정기의이 수량을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="57f765049776b7061ec3ebaa517aed91ea1819db" translate="yes" xml:space="preserve">
          <source>Another option is the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt;. This uses round-robin linear regression, modeling each feature with missing values as a function of other features, in turn. The version implemented assumes Gaussian (output) variables. If your features are obviously non-normal, consider transforming them to look more normal to potentially improve performance.</source>
          <target state="translated">또 다른 옵션은 &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt; &lt;/a&gt; 입니다. 이것은 라운드 로빈 선형 회귀를 사용하여 결 측값이있는 각 기능을 다른 기능의 함수로 차례로 모델링합니다. 구현 된 버전은 가우스 (출력) 변수를 가정합니다. 기능이 분명히 비정상적인 경우 성능을 잠재적으로 향상시키기 위해 더 정상적으로 보이도록 변환하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="c8eb27e015d30244a13ff2ea0a8b9e91973d57ab" translate="yes" xml:space="preserve">
          <source>Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:</source>
          <target state="translated">또 다른 옵션은 반복 가능한 양보 (학습, 테스트) 분할을 인덱스 배열로 사용하는 것입니다. 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ebe975675e662edf348908a46736c6c5923bfe28" translate="yes" xml:space="preserve">
          <source>Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt;, which transforms each categorical feature with &lt;code&gt;n_categories&lt;/code&gt; possible values into &lt;code&gt;n_categories&lt;/code&gt; binary features, with one of them 1, and all others 0.</source>
          <target state="translated">범주 형 기능을 scikit-learn 추정기와 함께 사용할 수있는 기능으로 변환 할 수있는 또 다른 가능성은 one-hot 또는 dummy 인코딩으로 알려진 K를 사용하는 것입니다. 이 유형의 인코딩은 &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;OneHotEncoder&lt;/code&gt; &lt;/a&gt; 를 사용하여 얻을 수 있습니다. OneHotEncoder 는 &lt;code&gt;n_categories&lt;/code&gt; 가능한 값을 가진 각 범주 별 기능 을 &lt;code&gt;n_categories&lt;/code&gt; 이진 기능으로 변환하고 그 중 하나는 1이고 다른 모든 기능은 0입니다.</target>
        </trans-unit>
        <trans-unit id="be92131a59ab295e72fcba022db0017e8b66e901" translate="yes" xml:space="preserve">
          <source>Another possibility to take into account correlated variables in the dataset, is to estimate sparse coefficients. In some way we already did it manually when we dropped the AGE column in a previous Ridge estimation.</source>
          <target state="translated">데이터 세트의 상관 변수를 고려할 수있는 또 다른 가능성은 희소 계수를 추정하는 것입니다. 어떤 식 으로든 이전 Ridge 추정에서 AGE 열을 삭제했을 때 이미 수동으로 수행했습니다.</target>
        </trans-unit>
        <trans-unit id="bd4ca3c02f5c305e57327d7ebc584dba1a900469" translate="yes" xml:space="preserve">
          <source>Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.</source>
          <target state="translated">tf의 또 다른 개선점은 말뭉치의 많은 문서에서 발생하는 단어의 가중치를 낮추어 말뭉치의 작은 부분에서만 발생하는 단어보다 덜 유익합니다.</target>
        </trans-unit>
        <trans-unit id="b069cfdd4eba1168499f693bf3d06a42576bfe6a" translate="yes" xml:space="preserve">
          <source>Another set of biclusters like &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">biclusters의 또 다른 세트는 좋아 &lt;code&gt;a&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="dbed96b9cee766d1a5bef03a5ad136b7f76de1ff" translate="yes" xml:space="preserve">
          <source>Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature.</source>
          <target state="translated">또 다른 중요한 특징은 발신자가 헤더 또는 서명으로 표시된 것처럼 대학과 제휴하는지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="943b31c92155c4448885494636d7164b8dc15821" translate="yes" xml:space="preserve">
          <source>Another strategy to reduce the variance is by subsampling the features analogous to the random splits in &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; . The number of subsampled features can be controlled via the &lt;code&gt;max_features&lt;/code&gt; parameter.</source>
          <target state="translated">분산을 줄이는 또 다른 전략은 &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 의 랜덤 분할과 유사한 기능을 서브 샘플링하는 것 입니다. 서브 샘플링 된 기능의 수는 &lt;code&gt;max_features&lt;/code&gt; 매개 변수 를 통해 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9bb442d3ca7e5d02384e286ef663eab8b5ac3081" translate="yes" xml:space="preserve">
          <source>Another way to compare the curves is to plot them on top of each other. Here, we create a figure with one row and two columns. The axes are passed into the &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay.plot&quot;&gt;&lt;code&gt;plot&lt;/code&gt;&lt;/a&gt; function as a list, which will plot the partial dependence curves of each model on the same axes. The length of the axes list must be equal to the number of plots drawn.</source>
          <target state="translated">곡선을 비교하는 또 다른 방법은 곡선을 서로 위에 플로팅하는 것입니다. 여기에서는 행 1 개와 열 2 개가있는 그림을 만듭니다. 축은 동일한 축에 각 모델의 부분 의존성 곡선을 그리는 목록으로 &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay.plot&quot;&gt; &lt;code&gt;plot&lt;/code&gt; &lt;/a&gt; 함수에 전달됩니다 . 좌표축 목록의 길이는 그려진 플롯 수와 같아야합니다.</target>
        </trans-unit>
        <trans-unit id="f7f320421764ca2c9e221a3ad89c0802b9dda00c" translate="yes" xml:space="preserve">
          <source>Another way to reduce memory and computation time is to remove (near-)duplicate points and use &lt;code&gt;sample_weight&lt;/code&gt; instead.</source>
          <target state="translated">메모리와 계산 시간을 줄이는 또 다른 방법은 중복 지점을 제거하고 대신 &lt;code&gt;sample_weight&lt;/code&gt; 를 사용 하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c89aea8f1a22bc16a7ca2249f6b1aa681a716bbf" translate="yes" xml:space="preserve">
          <source>Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least &lt;code&gt;eps&lt;/code&gt; in distance from any core sample, is considered an outlier by the algorithm.</source>
          <target state="translated">모든 핵심 샘플은 정의에 따라 클러스터의 일부입니다. 코어 샘플이 아니고 코어 샘플과 거리가 적어도 &lt;code&gt;eps&lt;/code&gt; 인 샘플은 알고리즘에 의해 이상치로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="3a31e829c68185fcae1e17d8f929a1d6c89d47e3" translate="yes" xml:space="preserve">
          <source>Any estimator using the Huber loss would also be robust to outliers, e.g. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;loss='huber'&lt;/code&gt;.</source>
          <target state="translated">후버 손실을 사용하는 모든 추정기는 또한 이상치 예에 강력한 것 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;loss='huber'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="68c9d8de07d4bc7df0a34b129a047212dbcfefbc" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a &lt;code&gt;scipy.spatial.distance&lt;/code&gt; metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">추가 매개 변수는 거리 함수로 직접 전달됩니다. &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 메트릭을 사용하는 경우 매개 변수는 여전히 메트릭에 따라 다릅니다. 사용법 예제는 scipy 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="38f47dd09163cb40bcb092eb016347a6afcf892a" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">추가 매개 변수는 거리 함수로 직접 전달됩니다. scipy.spatial.distance 메트릭을 사용하는 경우 매개 변수는 여전히 메트릭에 따라 다릅니다. 사용법 예제는 scipy 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3ac4cf049edcb6bb9798310f2f664afd413547f9" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the kernel function.</source>
          <target state="translated">추가 매개 변수는 커널 함수로 직접 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="02eaff0fa77b6c45a1e6d5d1d919c0643a037a82" translate="yes" xml:space="preserve">
          <source>Any pairwise distance</source>
          <target state="translated">페어 와이즈 거리</target>
        </trans-unit>
        <trans-unit id="b551d3f373e53d945ef845b2e543fa8abdb837a0" translate="yes" xml:space="preserve">
          <source>Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:</source>
          <target state="translated">추정기를 구성 할 때 제공되는 임의의 파라미터는 이러한 방식으로 최적화 될 수있다. 특히, 주어진 추정량에 대한 모든 매개 변수의 이름과 현재 값을 찾으려면 다음을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="767d84a7d28245adee41a36e340241d2beefe72a" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt; is used to select columns based on data type or column name:</source>
          <target state="translated">스칼라 또는 단일 항목 목록과 별도로 열 선택은 여러 항목의 목록, 정수 배열, 슬라이스, 부울 마스크 또는 &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt; &lt;code&gt;make_column_selector&lt;/code&gt; 를&lt;/a&gt; 사용하여 지정할 수 있습니다. &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt; &lt;code&gt;make_column_selector&lt;/code&gt; 는&lt;/a&gt; 데이터 유형 또는 열 이름을 기준으로 열을 선택하는 데 사용됩니다 :</target>
        </trans-unit>
        <trans-unit id="7c69ec44124fc60e723479a565a556b4cda43066" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.</source>
          <target state="translated">스칼라 또는 단일 항목 목록 외에도 열 선택은 여러 항목, 정수 배열, 슬라이스 또는 부울 마스크의 목록으로 지정할 수 있습니다. 입력이 DataFrame 인 경우 문자열은 열을 참조 할 수 있으며 정수는 항상 위치 열로 해석됩니다.</target>
        </trans-unit>
        <trans-unit id="4989a2f6df8d0581f9c1d9a7bb746db90f17c597" translate="yes" xml:space="preserve">
          <source>Apple Accelerate and vecLib frameworks (OSX only)</source>
          <target state="translated">Apple Accelerate 및 vecLib 프레임 워크 (OSX 만 해당)</target>
        </trans-unit>
        <trans-unit id="a10c2e4fa481f8ae4b6f36b04c8c93e1d4615114" translate="yes" xml:space="preserve">
          <source>Applications to real world problems with some medium sized datasets or interactive user interface.</source>
          <target state="translated">일부 중간 크기의 데이터 세트 또는 대화 형 사용자 인터페이스를 사용하여 실제 문제에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="781951ec1e6932c1ea70532655c1b03656c5caff" translate="yes" xml:space="preserve">
          <source>Applies fit_predict of last step in pipeline after transforms.</source>
          <target state="translated">변환 후 파이프 라인에서 마지막 단계의 fit_predict를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b55937b5acc0d0ace1d8fc39be387e51d59bfd33" translate="yes" xml:space="preserve">
          <source>Applies fit_transforms of a pipeline to the data, followed by the fit_predict method of the final estimator in the pipeline. Valid only if the final estimator implements fit_predict.</source>
          <target state="translated">파이프 라인의 fit_transforms를 데이터에 적용한 다음 파이프 라인의 최종 추정기의 fit_predict 메소드를 적용합니다. 최종 추정기가 fit_predict를 구현 한 경우에만 유효합니다.</target>
        </trans-unit>
        <trans-unit id="e8dcdf95b2627510067599d6b4df4640274e7f9c" translate="yes" xml:space="preserve">
          <source>Applies the learned transformation to the given data.</source>
          <target state="translated">학습 된 변환을 주어진 데이터에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="e21a5ab651c86df31d86409fda574ccc4b1b0811" translate="yes" xml:space="preserve">
          <source>Applies transformers to columns of an array or pandas DataFrame.</source>
          <target state="translated">변환기를 배열 또는 팬더 DataFrame의 열에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="bc1fc2d494d882727c783481b882146100e6cdea" translate="yes" xml:space="preserve">
          <source>Apply Term Frequency Inverse Document Frequency normalization to a sparse matrix of occurrence counts.</source>
          <target state="translated">용어 빈도 역 문서 빈도 정규화를 발생 빈도의 희소 행렬에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="48ea2e033ed029c8a6ff54b6ec3dbb5f6c117a68" translate="yes" xml:space="preserve">
          <source>Apply a correction to raw Minimum Covariance Determinant estimates.</source>
          <target state="translated">원시 최소 공분산 결정 추정값에 수정을 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="3438e96e607439758fb53dafc23a0586474578cd" translate="yes" xml:space="preserve">
          <source>Apply a power transform featurewise to make data more Gaussian-like.</source>
          <target state="translated">데이터를 더 가우시안처럼 만들려면 전력 변환 기능을 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="2abb9abf1584e2411d2ab4707e053695cc0afad4" translate="yes" xml:space="preserve">
          <source>Apply approximate feature map to X.</source>
          <target state="translated">대략적인 기능 맵을 X에 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="72616a249f3c4471d707b9e1210b174fad823e8d" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection of the normalized Laplacian.</source>
          <target state="translated">정규화 된 라플라시안의 투영에 클러스터링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b5fc36d5b38e5cc48bf65799eb3ba1f5f5dd4c40" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection to the normalized laplacian.</source>
          <target state="translated">정규화 된 라플라시안의 투영에 클러스터링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8426b1a96ac4159519d28aa77b01e6e7ff1b8a94" translate="yes" xml:space="preserve">
          <source>Apply decision function to an array of samples.</source>
          <target state="translated">샘플 배열에 결정 기능을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ae4427937ffbc660a10ac85e3e1e095b79e74a1a" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X using the model.</source>
          <target state="translated">모형을 사용하여 차원 축소를 X에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="94efd724519284be8c4f7f9c1263433e19686d0b" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X.</source>
          <target state="translated">X에 차원 축소를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="067a30cda0bacc8769ba06b4343f4bb4f1512cf6" translate="yes" xml:space="preserve">
          <source>Apply feature map to X.</source>
          <target state="translated">X에 기능 맵을 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="019b3cb4563fab68f9b7e6cb2810fdfe13086cd9" translate="yes" xml:space="preserve">
          <source>Apply inverse transformations in reverse order</source>
          <target state="translated">역변환을 역순으로 적용</target>
        </trans-unit>
        <trans-unit id="8ba6ffbae8fd06f4432eaf3f6080e23e84a08a61" translate="yes" xml:space="preserve">
          <source>Apply parallel or deflational algorithm for FastICA.</source>
          <target state="translated">FastICA에 병렬 또는 변형 알고리즘을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="052f087320408b50bb181caf401c6c09884401a8" translate="yes" xml:space="preserve">
          <source>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</source>
          <target state="translated">하위 선형 tf 스케일링을 적용합니다. 즉, tf를 1 + log (tf)로 바꿉니다.</target>
        </trans-unit>
        <trans-unit id="e79510482ec62ac16315b7e449f2ba4c8500bc2e" translate="yes" xml:space="preserve">
          <source>Apply the approximate feature map to X.</source>
          <target state="translated">대략적인 기능 맵을 X에 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="125fbeef808b6029be65b8119e0b6dcb36461fee" translate="yes" xml:space="preserve">
          <source>Apply the dimension reduction learned on the train data.</source>
          <target state="translated">열차 데이터에서 학습 한 치수 축소를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ccff7a762ba43bf734237ef3a3f86ce4e5f76a2a" translate="yes" xml:space="preserve">
          <source>Apply the inverse power transformation using the fitted lambdas.</source>
          <target state="translated">장착 된 람다를 사용하여 역 전력 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="0977868498dbf8902c5244b7ef40fc62ab65dab8" translate="yes" xml:space="preserve">
          <source>Apply the power transform to each feature using the fitted lambdas.</source>
          <target state="translated">장착 된 람다를 사용하여 각 기능에 전력 변환을 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="82b34785a3e300efc01281606b13c116a0d93f34" translate="yes" xml:space="preserve">
          <source>Apply transforms to the data, and predict with the final estimator</source>
          <target state="translated">데이터에 변환을 적용하고 최종 추정기로 예측</target>
        </trans-unit>
        <trans-unit id="ca1ed4363821df5fce03d98ccc01802e4bf47ad5" translate="yes" xml:space="preserve">
          <source>Apply transforms, and decision_function of the final estimator</source>
          <target state="translated">최종 추정기의 변환 및 의사 결정 기능 적용</target>
        </trans-unit>
        <trans-unit id="2f4f9e2645ee8d058965cbdacfaf2a7eb9788d03" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_log_proba of the final estimator</source>
          <target state="translated">최종 추정기의 변환 및 predict_log_proba 적용</target>
        </trans-unit>
        <trans-unit id="476ce75d7df4464fc2850d9c9869c6985e6ab19f" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_proba of the final estimator</source>
          <target state="translated">최종 추정기의 변환 및 predict_proba 적용</target>
        </trans-unit>
        <trans-unit id="4521c8878b7f577e2b39e24b7f82d01d4c53bdb4" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score with the final estimator</source>
          <target state="translated">변환을 적용하고 최종 추정기로 점수를 매 깁니다.</target>
        </trans-unit>
        <trans-unit id="d02b9d52a2cce495b36832a6628a7547a7253b34" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score_samples of the final estimator.</source>
          <target state="translated">최종 추정기의 변환 및 score_samples를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ef451a675f7839fb36e9b71a5c79c50af524605d" translate="yes" xml:space="preserve">
          <source>Apply transforms, and transform with the final estimator</source>
          <target state="translated">변환을 적용하고 최종 추정기로 변환</target>
        </trans-unit>
        <trans-unit id="de81ef7ceb9f90b36294e9caf98f17d7f5fb716c" translate="yes" xml:space="preserve">
          <source>Apply trees in the ensemble to X, return leaf indices.</source>
          <target state="translated">앙상블의 나무를 X에 적용하고 잎 인덱스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="46e9e1565b766b82cfa48c8669ac20be4ae1efca" translate="yes" xml:space="preserve">
          <source>Apply trees in the forest to X, return leaf indices.</source>
          <target state="translated">X에 숲의 나무를 적용하고 잎 인덱스를 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="c1f7ec5473437cc6f706dcde780cf75354c5c9db" translate="yes" xml:space="preserve">
          <source>Approximate a kernel map using a subset of the training data.</source>
          <target state="translated">훈련 데이터의 하위 집합을 사용하여 대략적인 커널 맵.</target>
        </trans-unit>
        <trans-unit id="99c9230e6f99f216d46b329fe823b6165b309c69" translate="yes" xml:space="preserve">
          <source>Approximate feature map for additive chi2 kernel.</source>
          <target state="translated">부가적인 chi2 커널에 대한 대략적인 기능 맵.</target>
        </trans-unit>
        <trans-unit id="08c300a2e35faf622b6d14b8c7a89fd4c1146cc4" translate="yes" xml:space="preserve">
          <source>Approximate nearest neighbors in TSNE</source>
          <target state="translated">TSNE의 대략적인 최근 접 이웃</target>
        </trans-unit>
        <trans-unit id="79a233ba78739efc19c54579d317a0c2897cb08a" translate="yes" xml:space="preserve">
          <source>Approximated breakdown point.</source>
          <target state="translated">대략적인 고 장점.</target>
        </trans-unit>
        <trans-unit id="80c9ab0b0026778753b68f0b79aed7c300d9ec18" translate="yes" xml:space="preserve">
          <source>Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">푸리에 변환의 몬테카를로 근사에 의한 RBF 커널의 대략적인 기능 맵.</target>
        </trans-unit>
        <trans-unit id="c5cd123a52fffa6abe1c128079596542a14c93e1" translate="yes" xml:space="preserve">
          <source>Approximates feature map of the &amp;ldquo;skewed chi-squared&amp;rdquo; kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">푸리에 변환의 몬테카를로 근사에 의한 &quot;비뚤어진 카이 제곱&quot;커널의 대략적인 기능 맵.</target>
        </trans-unit>
        <trans-unit id="0747da454303400f203ff73991292f2e6b1d8099" translate="yes" xml:space="preserve">
          <source>Approximations to the Likelihood Gradient. International Conference on Machine Learning (ICML) 2008</source>
          <target state="translated">가능성 그라디언트에 대한 근사치. 국제 머신 러닝 컨퍼런스 (ICML) 2008</target>
        </trans-unit>
        <trans-unit id="3121863aa17e74b4053a8b1b5b5e368203cc9eae" translate="yes" xml:space="preserve">
          <source>Are computed such that:</source>
          <target state="translated">다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="2745debaa64a20eedb49d9f14a0b807c87aa2d2a" translate="yes" xml:space="preserve">
          <source>Area</source>
          <target state="translated">Area</target>
        </trans-unit>
        <trans-unit id="40f1ed31aa37ba1df6f1e12267ca0106d885adfa" translate="yes" xml:space="preserve">
          <source>Area under ROC curve. If None, the roc_auc score is not shown.</source>
          <target state="translated">ROC 곡선 아래 영역. None이면 roc_auc 점수가 표시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="ef8721938ea54cef3b6761a4de1893a7d8b789aa" translate="yes" xml:space="preserve">
          <source>Area under ROC for the multiclass problem</source>
          <target state="translated">다중 클래스 문제에 대한 ROC 아래 영역</target>
        </trans-unit>
        <trans-unit id="9981f730845b36824e5458276d9c866bb62f0b81" translate="yes" xml:space="preserve">
          <source>Area under the precision-recall curve</source>
          <target state="translated">정밀 회수 곡선 아래 면적</target>
        </trans-unit>
        <trans-unit id="0d216eb7e93b45a2be7855da027249526cd9509f" translate="yes" xml:space="preserve">
          <source>Argument to the kernel.</source>
          <target state="translated">커널에 대한 인수입니다.</target>
        </trans-unit>
        <trans-unit id="5e08f171a6f1a9519e5708f61bf31cc48538834b" translate="yes" xml:space="preserve">
          <source>Arguments to send to the functional form. If empty and if fun=&amp;rsquo;logcosh&amp;rsquo;, fun_args will take value {&amp;lsquo;alpha&amp;rsquo; : 1.0}.</source>
          <target state="translated">기능적 형태로 보낼 인수. 비어 있고 fun = 'logcosh'인 경우 fun_args는 { 'alpha': 1.0} 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="c45228c98edaaa951ea722f923e4cffb69731e7a" translate="yes" xml:space="preserve">
          <source>Ariel Sharon</source>
          <target state="translated">아리엘 샤론</target>
        </trans-unit>
        <trans-unit id="11f09cc06ef4f6b2c8293917d6e2b9c293adc607" translate="yes" xml:space="preserve">
          <source>Array 1 for distance computation.</source>
          <target state="translated">거리 계산을위한 배열 1</target>
        </trans-unit>
        <trans-unit id="2b2aa765e7ec61bcc3223e5c31148344ebd52c37" translate="yes" xml:space="preserve">
          <source>Array 2 for distance computation.</source>
          <target state="translated">거리 계산을위한 배열 2</target>
        </trans-unit>
        <trans-unit id="8a65824231356d07c95aeae6f87001027ea0bef6" translate="yes" xml:space="preserve">
          <source>Array containing labels.</source>
          <target state="translated">레이블을 포함하는 배열입니다.</target>
        </trans-unit>
        <trans-unit id="e6449394cdaf60dea5ac71fdd09a0ed9e9fcf7ee" translate="yes" xml:space="preserve">
          <source>Array containing numbers whose mean is desired. If &lt;code&gt;a&lt;/code&gt; is not an array, a conversion is attempted.</source>
          <target state="translated">평균을 원하는 숫자가 포함 된 배열입니다. &lt;code&gt;a&lt;/code&gt; 가 배열이 아닌 경우 변환이 시도됩니다.</target>
        </trans-unit>
        <trans-unit id="c018e704aa26bba356828629d984b5289beaf058" translate="yes" xml:space="preserve">
          <source>Array containing pairwise preference constraints (qid in svmlight format).</source>
          <target state="translated">쌍별 기본 설정 제약 조건을 포함하는 배열 (svmlight 형식의 qid).</target>
        </trans-unit>
        <trans-unit id="91335d05435a126405d2a8a45dca985aad48295a" translate="yes" xml:space="preserve">
          <source>Array containing points.</source>
          <target state="translated">점을 포함하는 배열입니다.</target>
        </trans-unit>
        <trans-unit id="6f0a003e95ad4e5813fec46743d334c40fd183c4" translate="yes" xml:space="preserve">
          <source>Array dimensions of training vector &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">훈련 벡터 &lt;code&gt;X&lt;/code&gt; 의 배열 차원 .</target>
        </trans-unit>
        <trans-unit id="e25bba3f8d7bf2293b0ad16bc13aa9e306ca9914" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name</source>
          <target state="translated">피처 정수 인덱스에서 피처 이름으로의 배열 매핑</target>
        </trans-unit>
        <trans-unit id="ea79422025037006dbb9fa56f6952fdafa7b797f" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name.</source>
          <target state="translated">기능 정수 인덱스에서 기능 이름으로의 배열 매핑.</target>
        </trans-unit>
        <trans-unit id="70e68dbf5038082528f259bb78c1b52974cdd1b7" translate="yes" xml:space="preserve">
          <source>Array of C i.e. inverse of regularization parameter values used for cross-validation.</source>
          <target state="translated">교차 검증에 사용되는 정규화 매개 변수 값의 역인 C의 배열.</target>
        </trans-unit>
        <trans-unit id="f23db680f590249d8c9bf389de4a77cfbca412ab" translate="yes" xml:space="preserve">
          <source>Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;C_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="translated">모든 클래스에서 최고 점수에 매핑되는 C의 배열입니다. 개조가 False로 설정된 경우 각 클래스에 대해 최고 C는 각 폴드에 대한 최고 점수에 해당하는 C의 평균입니다. 문제가 이진 인 경우 &lt;code&gt;C_&lt;/code&gt; 는 형태 (n_classes)입니다.</target>
        </trans-unit>
        <trans-unit id="a55c3729a06f5322394066f1c0c8cd702750ec36" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">시도 할 알파 값의 배열입니다. 정규화 강도; 양수 부동이어야합니다. 정규화는 문제의 조건을 개선하고 추정값의 분산을 줄입니다. 값이 클수록 더 강력한 정규화를 지정합니다. Alpha 는 &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt; 와 같은 다른 선형 모델에서 &lt;code&gt;1 / (2C)&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="ee7705f66f6136357dc8015abf939ac9ba5f0a41" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;. If using generalized cross-validation, alphas must be positive.</source>
          <target state="translated">시도 할 알파 값의 배열입니다. 정규화 강도; 양수 부동이어야합니다. 정규화는 문제의 조건을 개선하고 추정값의 분산을 줄입니다. 값이 클수록 더 강력한 정규화를 지정합니다. Alpha 는 &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt; 와 같은 다른 선형 모델에서 &lt;code&gt;1 / (2C)&lt;/code&gt; 에 해당합니다 . 일반화 된 교차 검증을 사용하는 경우 알파는 양수 여야합니다.</target>
        </trans-unit>
        <trans-unit id="1bcf7c58e5cd52bc0017cb778f5c70ae9fb7f4c3" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;C^-1&lt;/code&gt; in other linear models such as LogisticRegression or LinearSVC.</source>
          <target state="translated">시도 할 알파 값의 배열입니다. 정규화 강도; 양수 부동이어야합니다. 정규화는 문제의 컨디셔닝을 개선하고 추정값의 분산을 줄입니다. 값이 클수록 정규화가 더 강력 해집니다. 알파 는 LogisticRegression 또는 LinearSVC와 같은 다른 선형 모델에서 &lt;code&gt;C^-1&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="56fbe430ec1266429078a0ab631106976ff42e91" translate="yes" xml:space="preserve">
          <source>Array of feature names.</source>
          <target state="translated">기능 이름의 배열.</target>
        </trans-unit>
        <trans-unit id="f66e9a562c0b4e0d27e475880657f80a6deb9514" translate="yes" xml:space="preserve">
          <source>Array of feature-wise means to update with the new data X.</source>
          <target state="translated">기능별 배열은 새로운 데이터 X로 업데이트하는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="f6c98b4462671d9e778e0d714eb0f164a1614fcd" translate="yes" xml:space="preserve">
          <source>Array of feature-wise var to update with the new data X.</source>
          <target state="translated">새로운 데이터 X로 업데이트 할 기능별 var 배열입니다.</target>
        </trans-unit>
        <trans-unit id="5fcd28abe38a5e4606c96fde73b0772080b7a651" translate="yes" xml:space="preserve">
          <source>Array of images from which to extract patches. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">패치를 추출 할 이미지 배열입니다. 컬러 이미지의 경우 마지막 차원은 채널을 지정합니다. RGB 이미지는 &lt;code&gt;n_channels=3&lt;/code&gt; 을 갖 습니다 .</target>
        </trans-unit>
        <trans-unit id="6f81a8f931ec6cc551fbd84a217b7d3ae7ae6320" translate="yes" xml:space="preserve">
          <source>Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only &amp;ldquo;balanced&amp;rdquo; is supported for class_weight if this is provided.</source>
          <target state="translated">서브 샘플에 사용될 인덱스 배열. 서브 샘플의 경우 길이가 n_sample보다 작거나 인덱스가 반복되는 부트 스트랩 서브 샘플의 경우 n_samples와 같을 수 있습니다. None 인 경우, 샘플 무게는 전체 샘플에 대해 계산됩니다. class_weight가 제공되면 &quot;balanced&quot;만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="68493ef46dcb2bca2edbdc205fc9d64a4faedf4e" translate="yes" xml:space="preserve">
          <source>Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;l1_ratio_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="translated">모든 클래스에서 최고 점수에 매핑되는 l1_ratio의 배열입니다. refit이 False로 설정된 경우 각 클래스에 대해 최고 l1_ratio는 각 접기에 대한 최고 점수에 해당하는 l1_ratio의 평균입니다. &lt;code&gt;l1_ratio_&lt;/code&gt; 는 문제가 이진일 때 형태 (n_classes,)입니다.</target>
        </trans-unit>
        <trans-unit id="fac2540408435b364a56995dfc908b1ce3b3f1b5" translate="yes" xml:space="preserve">
          <source>Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not &amp;lsquo;elasticnet&amp;rsquo;), this is set to &lt;code&gt;[None]&lt;/code&gt;</source>
          <target state="translated">교차 검증에 사용되는 l1_ratios의 배열입니다. l1_ratio가 사용되지 않으면 (즉, 패널티가 'elasticnet'이 아님) &lt;code&gt;[None]&lt;/code&gt; 으로 설정됩니다 .</target>
        </trans-unit>
        <trans-unit id="eeab30110685755d1d6fcfd1bdec963e11fd2b40" translate="yes" xml:space="preserve">
          <source>Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data.</source>
          <target state="translated">입력 데이터에 지정된 레이블의 배열입니다. 적합하지 않고 partial_fit을 사용하면 마지막 데이터 배치에 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="7713344316e6072a30843b91363d87cccda00c35" translate="yes" xml:space="preserve">
          <source>Array of matplotlib axes. &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;include_values&lt;/code&gt; is false.</source>
          <target state="translated">matplotlib 축의 배열. &lt;code&gt;include_values&lt;/code&gt; 가 거짓 이면 &lt;code&gt;None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d4370a66f6bded85adaf7548951f4179c0716e58" translate="yes" xml:space="preserve">
          <source>Array of modal values.</source>
          <target state="translated">모달 값의 배열.</target>
        </trans-unit>
        <trans-unit id="1fd74761ce4c1dfdbf88f10700afe7876dc7373c" translate="yes" xml:space="preserve">
          <source>Array of ordered feature names used in the dataset.</source>
          <target state="translated">데이터 세트에 사용 된 정렬 된 피쳐 이름의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="0f885b177d721fef81c6f0296028811159f9ce61" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample.</source>
          <target state="translated">샘플 당 원래 클래스 레이블의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="7800a77bf571d12be284509e1c1715411139d56e" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample;</source>
          <target state="translated">샘플 당 원본 클래스 레이블의 배열.</target>
        </trans-unit>
        <trans-unit id="96778e504b9d48e677b9fd575a4cafc7576e9a4e" translate="yes" xml:space="preserve">
          <source>Array of pairwise distances between samples, or a feature array.</source>
          <target state="translated">샘플 또는 피쳐 배열 간의 쌍별 거리 배열입니다.</target>
        </trans-unit>
        <trans-unit id="e091a26de25f0600efda87ce52273e245aa786c4" translate="yes" xml:space="preserve">
          <source>Array of pairwise kernels between samples, or a feature array.</source>
          <target state="translated">샘플 사이의 페어 단위 커널 배열 또는 기능 배열.</target>
        </trans-unit>
        <trans-unit id="7ad46daa8cb15b6370dc0bc3d5a37d6d0d152fc8" translate="yes" xml:space="preserve">
          <source>Array of positive distances. If vertex i is connected to vertex j, then dist_matrix[i,j] gives the distance between the vertices. If vertex i is not connected to vertex j, then dist_matrix[i,j] = 0</source>
          <target state="translated">양의 거리 배열. 정점 i가 정점 j에 연결된 경우 dist_matrix [i, j]는 정점 사이의 거리를 제공합니다. 정점 i가 정점 j에 연결되어 있지 않으면 dist_matrix [i, j] = 0</target>
        </trans-unit>
        <trans-unit id="4e5f1b979067f81092a1e7cd9ffe302fc627e739" translate="yes" xml:space="preserve">
          <source>Array of precomputed feature-wise values to use for scaling.</source>
          <target state="translated">스케일링에 사용할 사전 계산 된 기능별 값의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="6812a29eee4afd325221734d77b9a4dae2cd6936" translate="yes" xml:space="preserve">
          <source>Array of precomputed sample-wise values to use for scaling.</source>
          <target state="translated">스케일링에 사용할 사전 계산 된 샘플 단위 값의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="fdeed78489293ac715c2ab2eb4b0404b2b3869aa" translate="yes" xml:space="preserve">
          <source>Array of samples (test vectors).</source>
          <target state="translated">샘플 배열 (테스트 벡터).</target>
        </trans-unit>
        <trans-unit id="24f920c86b265a90a5e0d4a36297a04667e3e9df" translate="yes" xml:space="preserve">
          <source>Array of samples/test vectors.</source>
          <target state="translated">샘플 / 테스트 벡터의 배열.</target>
        </trans-unit>
        <trans-unit id="3fbc39a14b9b4da5186fad432e1588423be425d4" translate="yes" xml:space="preserve">
          <source>Array of scores of the estimator for each run of the cross validation.</source>
          <target state="translated">교차 검증의 각 실행에 대한 추정기의 점수 배열.</target>
        </trans-unit>
        <trans-unit id="45ad4c3a294e4989b4c11ac5e758294b0026c988" translate="yes" xml:space="preserve">
          <source>Array of shape (Nx, D), representing Nx points in D dimensions.</source>
          <target state="translated">D 차원의 Nx 점을 나타내는 모양의 배열 (Nx, D)입니다.</target>
        </trans-unit>
        <trans-unit id="fc8b99cd3247136d0c5bc8738933867c6b7310fe" translate="yes" xml:space="preserve">
          <source>Array of shape (Ny, D), representing Ny points in D dimensions. If not specified, then Y=X.</source>
          <target state="translated">D 차원의 Ny 점을 나타내는 모양의 배열 (Ny, D)입니다. 지정하지 않으면 Y = X입니다.</target>
        </trans-unit>
        <trans-unit id="250d74905474bfb993e7135f2cc0cab46c1b40f6" translate="yes" xml:space="preserve">
          <source>Array of the classes occurring in the data, as given by &lt;code&gt;np.unique(y_org)&lt;/code&gt; with &lt;code&gt;y_org&lt;/code&gt; the original class labels.</source>
          <target state="translated">원래 클래스 레이블 이 &lt;code&gt;y_org&lt;/code&gt; 인 &lt;code&gt;np.unique(y_org)&lt;/code&gt; 에서 제공 한 데이터에서 발생하는 클래스의 배열입니다 .</target>
        </trans-unit>
        <trans-unit id="b78e425c6e49838f94af51fc89b925b7dde837d3" translate="yes" xml:space="preserve">
          <source>Array of weighted counts for each mode.</source>
          <target state="translated">각 모드에 대한 가중치 카운트의 배열.</target>
        </trans-unit>
        <trans-unit id="94dd9c0571a23f9d9a0e09a5e6b6a6f23711b3ea" translate="yes" xml:space="preserve">
          <source>Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</source>
          <target state="translated">개별 샘플에 할당 된 가중치의 배열. 제공하지 않으면 각 샘플에 단위 중량이 부여됩니다.</target>
        </trans-unit>
        <trans-unit id="2d23bb743e4774802fe069cba46f38744d42d230" translate="yes" xml:space="preserve">
          <source>Array representing the cosine distances to each point, only present if return_distance=True.</source>
          <target state="translated">각 포인트까지의 코사인 거리를 나타내는 배열로, return_distance = True 인 경우에만 존재합니다.</target>
        </trans-unit>
        <trans-unit id="a5f32f9aefd1234d15dbae57ece52e2602c42baf" translate="yes" xml:space="preserve">
          <source>Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the &lt;code&gt;metric&lt;/code&gt; constructor parameter.</source>
          <target state="translated">각 지점까지의 거리를 나타내는 배열로, return_distance = True 인 경우에만 존재합니다. 거리 값은 &lt;code&gt;metric&lt;/code&gt; 생성자 매개 변수 에 따라 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="dadd9836b98c2a8bfba00660688e29f8956840ae" translate="yes" xml:space="preserve">
          <source>Array representing the lengths to points, only present if return_distance=True</source>
          <target state="translated">길이를 포인트로 나타내는 배열. return_distance = True 인 경우에만 존재</target>
        </trans-unit>
        <trans-unit id="128b0965caa89fb141c0f27357f3b344c8ae14e0" translate="yes" xml:space="preserve">
          <source>Array with class_weight_vect[i] the weight for i-th class</source>
          <target state="translated">i 번째 클래스의 가중치를 class_weight_vect [i] 인 배열</target>
        </trans-unit>
        <trans-unit id="8071ea6e98367794e2540e75731701074b3502c7" translate="yes" xml:space="preserve">
          <source>Array with sample weights as applied to the original y</source>
          <target state="translated">원래 y에 적용된 샘플 가중치가있는 배열</target>
        </trans-unit>
        <trans-unit id="454c01b5a8f0e04bcddbd63e1a11312ba378a0a5" translate="yes" xml:space="preserve">
          <source>Arrays containing points.</source>
          <target state="translated">점을 포함하는 배열.</target>
        </trans-unit>
        <trans-unit id="55e1c09be8a72c22c42432b49cce52b8593ad9f3" translate="yes" xml:space="preserve">
          <source>Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features)</source>
          <target state="translated">점을 포함하는 배열. 각 모양 (n_samples1, n_features) 및 (n_samples2, n_features)</target>
        </trans-unit>
        <trans-unit id="617153f408d9e36a02ce1e1b9ef6e01cb4b4ace4" translate="yes" xml:space="preserve">
          <source>Arrays for storing tree data, index, node data and node bounds.</source>
          <target state="translated">트리 데이터, 인덱스, 노드 데이터 및 노드 경계를 저장하기위한 배열.</target>
        </trans-unit>
        <trans-unit id="3c2af9f8f5cb14e6db46f51362b9496e9213d67c" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="translated">올가미와 능선 회귀의 강력한 하이브리드 인 Art B. Owen (2006). &lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c8d50e29fabbeb579c5217b8bfc0480f3d25842" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;https://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;https://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="translated">Art B. Owen (2006), 올가미와 능선 회귀의 강력한 하이브리드. &lt;a href=&quot;https://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;https://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="280c547a1f162abeda960694ff094a09bc3e0f0d" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1).</source>
          <target state="translated">&lt;code&gt;RobustScaler&lt;/code&gt; 와 마찬가지로 &lt;code&gt;QuantileTransformer&lt;/code&gt; 는 학습 세트에 특이 치를 추가하거나 제거하면 보류 된 데이터에 대해 거의 동일한 변환을 생성한다는 점에서 특이 치에 강력합니다. 그러나 &lt;code&gt;RobustScaler&lt;/code&gt; 와 달리 &lt;code&gt;QuantileTransformer&lt;/code&gt; 는 우선 순위를 사전 정의 된 범위 경계 (0 및 1)로 설정하여 이상 값을 자동으로 축소합니다.</target>
        </trans-unit>
        <trans-unit id="4f9342a94a131883760ca784477b6aa2e1e17246" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;StandardScaler&lt;/code&gt;, &lt;code&gt;MinMaxScaler&lt;/code&gt; is very sensitive to the presence of outliers.</source>
          <target state="translated">마찬가지로 &lt;code&gt;StandardScaler&lt;/code&gt; , &lt;code&gt;MinMaxScaler&lt;/code&gt; 은 특이점의 존재에 매우 민감하다.</target>
        </trans-unit>
        <trans-unit id="0473784855cee607ac3c3e8942e446aa86c45018" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;leaf_size&lt;/code&gt; increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a \(D\)-dimensional centroid for each node. The required storage space for &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; is approximately &lt;code&gt;1 / leaf_size&lt;/code&gt; times the size of the training set.</source>
          <target state="translated">마찬가지로 &lt;code&gt;leaf_size&lt;/code&gt; 증가, 메모리는 트리 구조가 감소 저장하는데 필요한. 이것은 각 노드에 대해 \ (D \) 차원 중심을 저장하는 볼 트리의 경우에 특히 중요합니다. &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; 에&lt;/a&gt; 필요한 저장 공간 은 훈련 세트 크기의 약 &lt;code&gt;1 / leaf_size&lt;/code&gt; 배입니다.</target>
        </trans-unit>
        <trans-unit id="5ba99528590cd9030f3ddfaa7772e7d00f041704" translate="yes" xml:space="preserve">
          <source>As F-test captures only linear dependency, it rates x_1 as the most discriminative feature. On the other hand, mutual information can capture any kind of dependency between variables and it rates x_2 as the most discriminative feature, which probably agrees better with our intuitive perception for this example. Both methods correctly marks x_3 as irrelevant.</source>
          <target state="translated">F- 검정은 선형 의존성 만 캡처하므로 x_1을 가장 차별화 된 기능으로 평가합니다. 한편, 상호 정보는 변수 간의 모든 종류의 종속성을 캡처 할 수 있으며 x_2를 가장 차별적 인 기능으로 평가하므로이 예제에 대한 직관적 인 인식에 더 잘 맞을 것입니다. 두 방법 모두 x_3을 관련없는 것으로 올바르게 표시합니다.</target>
        </trans-unit>
        <trans-unit id="d26c8ba867b91d66c4620ebdebaef0da4c712fd2" translate="yes" xml:space="preserve">
          <source>As \(\nu\rightarrow\infty\), the Mat&amp;eacute;rn kernel converges to the RBF kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel, i.e.,</source>
          <target state="translated">\ (\ nu \ rightarrow \ infty \)와 같이 Mat&amp;eacute;rn 커널은 RBF 커널로 수렴됩니다. \ (\ nu = 1/2 \) 일 때 Mat&amp;eacute;rn 커널은 절대 지수 커널과 동일하게됩니다.</target>
        </trans-unit>
        <trans-unit id="fd1774dd2550566b72e9349a744134b65f1d5b37" translate="yes" xml:space="preserve">
          <source>As \(k\) becomes large compared to \(N\), the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.</source>
          <target state="translated">\ (N \)에 비해 \ (k \)가 커지면 트리 기반 쿼리에서 분기를 제거하는 기능이 줄어 듭니다. 이 상황에서 Brute force 쿼리가 더 효율적일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="faf1e694d41950e191ce698208593222026d0616" translate="yes" xml:space="preserve">
          <source>As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.</source>
          <target state="translated">일반적으로 대부분의 저자와 경험적 증거는 LOO보다 5 배 또는 10 배의 교차 검증이 선호되어야한다고 제안합니다.</target>
        </trans-unit>
        <trans-unit id="7511794679052faea86272ecd7141a948a5ee019" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="translated">일반적으로 희소 비율이 90 %보다 크면 희소 형식의 이점을 얻을 수 있다고 생각할 수 있습니다. 희소 행렬 형식을 &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;작성&lt;/a&gt; (또는 데이터로 변환)하는 방법에 대한 자세한 내용은 Scipy의 희소 행렬 형식 설명서 를 참조하십시오. 대부분 &lt;code&gt;CSR&lt;/code&gt; 및 &lt;code&gt;CSC&lt;/code&gt; 형식이 가장 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="e6d544b03b9c4badced861e6939e6f09869b8c86" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="translated">경험상 희소성 비율이 90 %보다 크면 희소 형식의 이점을 얻을 수 있다는 것을 고려할 수 있습니다. 희소 행렬 형식을 &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;작성&lt;/a&gt; (또는 데이터를 변환)하는 방법에 대한 자세한 내용은 Scipy의 희소 행렬 형식 문서 를 확인하세요 . 대부분의 경우 &lt;code&gt;CSR&lt;/code&gt; 및 &lt;code&gt;CSC&lt;/code&gt; 형식이 가장 잘 작동합니다.</target>
        </trans-unit>
        <trans-unit id="353bdb2ad3c8962f2cd7a3693251833639b2a66a" translate="yes" xml:space="preserve">
          <source>As a stochastic method, the loss function is not necessarily decreasing at each iteration, and convergence is only guaranteed in expectation. For this reason, monitoring the convergence on the loss function can be difficult.</source>
          <target state="translated">확률 론적 방법으로 손실 함수가 반복 될 때마다 반드시 감소하는 것은 아니며 수렴은 예상에서만 보장됩니다. 이러한 이유로 손실 기능에 대한 수렴을 모니터링하는 것은 어려울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="42def8933b75f8dc81489a17ca6bf2a4d3b1aab6" translate="yes" xml:space="preserve">
          <source>As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager:</source>
          <target state="translated">사용자는 컨텍스트 관리자를 사용하여 joblib가 사용할 백엔드를 제어 할 수 있습니다 (scikit-learn 권장 사항에 관계없이).</target>
        </trans-unit>
        <trans-unit id="c5c1051c685f7a17c464794391d76e887c975059" translate="yes" xml:space="preserve">
          <source>As alpha tends toward zero the coefficients found by Ridge regression stabilize towards the randomly sampled vector w. For big alpha (strong regularisation) the coefficients are smaller (eventually converging at 0) leading to a simpler and biased solution. These dependencies can be observed on the left plot.</source>
          <target state="translated">알파가 0을 향하는 경향이 있으므로 릿지 회귀에 의해 발견 된 계수는 무작위로 샘플링 된 벡터 w를 향해 안정화됩니다. 큰 알파 (강한 정규화)의 경우 계수가 더 작아서 (결과적으로 0으로 수렴) 더 단순하고 편향된 솔루션으로 이어집니다. 이러한 의존성은 왼쪽 그림에서 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="24f37f11c718f612ad3749aae9a0a774bc0cfd6c" translate="yes" xml:space="preserve">
          <source>As an alternative, the permutation importances of &lt;code&gt;rf&lt;/code&gt; are computed on a held out test set. This shows that the low cardinality categorical feature, &lt;code&gt;sex&lt;/code&gt; is the most important feature.</source>
          <target state="translated">대안으로, &lt;code&gt;rf&lt;/code&gt; 의 순열 중요도 는 홀드 아웃 테스트 세트에서 계산됩니다. 이것은 낮은 카디널리티 범주 기능인 &lt;code&gt;sex&lt;/code&gt; 가 가장 중요한 기능 임을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="3c04824c2f2c674faf4d7d29dd0f1d9a774a897e" translate="yes" xml:space="preserve">
          <source>As an example, consider a word-level natural language processing task that needs features extracted from &lt;code&gt;(token, part_of_speech)&lt;/code&gt; pairs. One could use a Python generator function to extract features:</source>
          <target state="translated">예를 들어, &lt;code&gt;(token, part_of_speech)&lt;/code&gt; 쌍 에서 추출 된 기능이 필요한 단어 레벨 자연 언어 처리 태스크를 고려하십시오 . 파이썬 생성기 함수를 사용하여 기능을 추출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="440e47d9bdf54f0d76b208827e3f8a7246ce9187" translate="yes" xml:space="preserve">
          <source>As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by</source>
          <target state="translated">예를 들어, 부울 피처가있는 데이터 세트가 있고 샘플의 80 % 이상에서 1 또는 0 (켜기 또는 끄기) 인 모든 피처를 제거하려고한다고 가정합니다. 부울 피처는 Bernoulli 랜덤 변수이며 이러한 변수의 분산은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="0fc331775bcbccb582b3c6648f3387e8f638bc0a" translate="yes" xml:space="preserve">
          <source>As an iterable of string metrics::</source>
          <target state="translated">반복 가능한 문자열 메트릭으로 ::</target>
        </trans-unit>
        <trans-unit id="670a68b39de7d2dc4b153165f015b7c7235ba10c" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:</source>
          <target state="translated">최적화 문제로서 이진 클래스 L2에 로지스틱 회귀가 적용되면 다음과 같은 비용 함수가 최소화됩니다.</target>
        </trans-unit>
        <trans-unit id="f278c77eb641c2fca3187e677f3037bfd6ce972e" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class \(\ell_2\) penalized logistic regression minimizes the following cost function:</source>
          <target state="translated">최적화 문제로 이진 클래스 \ (\ ell_2 \) 페널티 로지스틱 회귀는 다음 비용 함수를 최소화합니다.</target>
        </trans-unit>
        <trans-unit id="a0923a21facc4895041c36b66fbffdd3ccd57707" translate="yes" xml:space="preserve">
          <source>As currently implemented, Dijkstra&amp;rsquo;s algorithm does not work for graphs with direction-dependent distances when directed == False. i.e., if dist_matrix[i,j] and dist_matrix[j,i] are not equal and both are nonzero, method=&amp;rsquo;D&amp;rsquo; will not necessarily yield the correct result.</source>
          <target state="translated">현재 구현 된대로 Dijkstra의 알고리즘은 방향 == 거짓 인 경우 방향에 따른 거리가있는 그래프에서 작동하지 않습니다. 즉, dist_matrix [i, j]와 dist_matrix [j, i]가 같지 않고 둘 다 0이 아닌 경우 method = 'D'는 반드시 정확한 결과를 산출하지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="18ae6c61fa89653926b85f1ce590dca96bbcbbc7" translate="yes" xml:space="preserve">
          <source>As described on the original website:</source>
          <target state="translated">원래 웹 사이트에 설명 된대로 :</target>
        </trans-unit>
        <trans-unit id="5aa4aad7dfb43ef2769419ae7f460971c9284dbe" translate="yes" xml:space="preserve">
          <source>As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:</source>
          <target state="translated">앞에서 설명했듯이 가장 널리 사용되는 거리 함수는 Frobenius 제곱입니다. 이는 유클리드 표준을 행렬로 확장 한 것입니다.</target>
        </trans-unit>
        <trans-unit id="e81ebe44fbcb1c67bb2bf8d6d2417ec1c47e735b" translate="yes" xml:space="preserve">
          <source>As expected the confusion matrix shows that posts from the newsgroups on atheism and Christianity are more often confused for one another than with computer graphics.</source>
          <target state="translated">예상대로 혼란 매트릭스는 무신론과 기독교에 관한 뉴스 그룹의 게시물이 컴퓨터 그래픽보다 서로 더 자주 혼동된다는 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a21f305b83f6ecc940d012c4c2d2b2809b12f48f" translate="yes" xml:space="preserve">
          <source>As expected, &lt;code&gt;VarianceThreshold&lt;/code&gt; has removed the first column, which has a probability \(p = 5/6 &amp;gt; .8\) of containing a zero.</source>
          <target state="translated">예상대로 &lt;code&gt;VarianceThreshold&lt;/code&gt; 는 첫 번째 열을 제거했습니다. 첫 번째 열은 0을 포함 할 확률 \ (p = 5/6&amp;gt; .8 \)입니다.</target>
        </trans-unit>
        <trans-unit id="0a862c171ad9c6c90137a8961c4a49a8109f123e" translate="yes" xml:space="preserve">
          <source>As expected, the dummy regressor is unable to correctly rank the samples and therefore performs the worst on this plot.</source>
          <target state="translated">예상대로 더미 회귀 분석기는 표본의 순위를 올바르게 지정할 수 없으므로이 그림에서 최악의 결과를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="1461782e678132cdda4c11cb8a9f0f36fdc4dc12" translate="yes" xml:space="preserve">
          <source>As expected, the plot suggests that 3 features are informative, while the remaining are not.</source>
          <target state="translated">예상 한 바와 같이, 플롯은 3 가지 기능이 유익하지만 나머지 기능은 그렇지 않다는 것을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="21dea3d956b97f59f6554850f2e02a18d3e3f037" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt;, the utility class &lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt;&lt;code&gt;Binarizer&lt;/code&gt;&lt;/a&gt; is meant to be used in the early stages of &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;fit&lt;/code&gt; method does nothing as each sample is treated independently of others:</source>
          <target state="translated">에 관해서는 &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt; , 유틸리티 클래스 &lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt; &lt;code&gt;Binarizer&lt;/code&gt; &lt;/a&gt; 의미는의 초기 단계에 사용되는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; . &lt;code&gt;fit&lt;/code&gt; 각각의 샘플이 다른 독립적으로 처리로 메서드는 아무 작업도 수행하지 않습니다 :</target>
        </trans-unit>
        <trans-unit id="448fad632016e4e86e34b67a2dd205dc93981193" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; classes, the preprocessing module provides a companion function &lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt;&lt;code&gt;binarize&lt;/code&gt;&lt;/a&gt; to be used when the transformer API is not necessary.</source>
          <target state="translated">관해서 &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt; 클래스, 상기 전처리 모듈은 동반자 기능 제공 &lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt; &lt;code&gt;binarize&lt;/code&gt; &lt;/a&gt; 트랜스 API가 필요하지 않을 때 사용된다.</target>
        </trans-unit>
        <trans-unit id="7e4621ab54b3c13fd3d03c5dfae345e68a0fd95a" translate="yes" xml:space="preserve">
          <source>As in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; centers but does not scale the input data for each feature before applying the SVD.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 에서&lt;/a&gt; 와 같이 &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; &lt;/a&gt; 는 SVD를 적용하기 전에 각 기능에 대한 입력 데이터를 조정하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="79a9833b8f532d00a6b35f7523ca2a620de3d115" translate="yes" xml:space="preserve">
          <source>As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">분류 설정에서와 같이 fit 메소드는 인수 배열 X 및 y로 사용되며이 경우 y에만 정수 값 대신 부동 소수점 값이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="357549c786d11a83e9ad1e4e4484c10d048f813a" translate="yes" xml:space="preserve">
          <source>As is shown in the result before discretization, linear model is fast to build and relatively straightforward to interpret, but can only model linear relationships, while decision tree can build a much more complex model of the data. One way to make linear model more powerful on continuous data is to use discretization (also known as binning). In the example, we discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide, there would appear to be a substantially increased risk of overfitting, so the discretizer parameters should usually be tuned under cross validation.</source>
          <target state="translated">이산화 이전의 결과에서 볼 수 있듯이 선형 모델은 해석이 쉽고 빠르지 만 선형 관계 만 모델링 할 수 있지만 의사 결정 트리는 훨씬 더 복잡한 데이터 모델을 작성할 수 있습니다. 연속 데이터에서 선형 모델을 더 강력하게 만드는 한 가지 방법은 이산화 (비닝이라고도 함)를 사용하는 것입니다. 이 예에서는 피처를 이산화시키고 변환 된 데이터를 1- 핫 인코딩합니다. 빈이 합리적으로 넓지 않으면 과적 합의 위험이 실질적으로 증가한 것으로 보이므로, 이산 기 파라미터는 일반적으로 교차 검증하에 조정되어야합니다.</target>
        </trans-unit>
        <trans-unit id="67535506d22839df286d2c74a169c907b47bc78e" translate="yes" xml:space="preserve">
          <source>As mentioned above, we can interpret LDA as assigning \(x\) to the class whose mean \(\mu_k\) is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first &lt;em&gt;sphering&lt;/em&gt; the data so that the covariance matrix is the identity, and then assigning \(x\) to the closest mean in terms of Euclidean distance (still accounting for the class priors).</source>
          <target state="translated">위에서 언급했듯이 LDA는 평균 \ (\ mu_k \)가 Mahalanobis 거리 측면에서 가장 가까운 클래스에 \ (x \)를 할당하는 동시에 클래스 사전 확률도 고려하는 것으로 해석 할 수 있습니다. 또는 LDA는 먼저 공분산 행렬이 ID가되도록 데이터를 &lt;em&gt;구형 화&lt;/em&gt; 한 다음 \ (x \)를 유클리드 거리 측면에서 가장 가까운 평균에 할당하는 것과 동일합니다 (여전히 클래스 사전을 고려함).</target>
        </trans-unit>
        <trans-unit id="4d80681701a6bd5948db2415d41ef49851684994" translate="yes" xml:space="preserve">
          <source>As mentioned in the introduction, the total claim amount per unit of exposure can be modeled as the product of the prediction of the frequency model by the prediction of the severity model.</source>
          <target state="translated">서론에서 언급했듯이 노출 단위당 총 청구 금액은 심각도 모델의 예측에 의한 빈도 모델 예측의 곱으로 모델링 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9026eb9b1171dae37e620c4611084cbf5214d244" translate="yes" xml:space="preserve">
          <source>As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).</source>
          <target state="translated">대부분의 문서는 일반적으로 말뭉치에 사용되는 단어의 아주 작은 부분 집합을 사용하므로 결과 행렬에는 0 (일반적으로 99 % 이상)의 많은 특성 값이 있습니다.</target>
        </trans-unit>
        <trans-unit id="80903649b427f38994df0cc2bd896c257816e4cc" translate="yes" xml:space="preserve">
          <source>As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.</source>
          <target state="translated">인접한 데이터 포인트가 트리의 동일한 리프 내에있을 가능성이 높으므로 변환은 암시적인 비모수 적 밀도 추정을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="604835d70709f3a91a0d5148ef02789e712875c2" translate="yes" xml:space="preserve">
          <source>As neither of these datasets have missing values, we will remove some values to create new versions with artificially missing data. The performance of &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; on the full original dataset is then compared the performance on the altered datasets with the artificially missing values imputed using different techniques.</source>
          <target state="translated">이러한 데이터 세트에는 누락 된 값이 없으므로 일부 값을 제거하여 인위적으로 누락 된 데이터가있는 새 버전을 만듭니다. 성능 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt; 전체 원래 데이터 세트는 인공적 측값 변경된 데이터 세트의 성능은 다른 기술을 이용하여 전가 비교된다.</target>
        </trans-unit>
        <trans-unit id="4b3b8dbd06a75124d0b25055322ec22921d45503" translate="yes" xml:space="preserve">
          <source>As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter &lt;code&gt;leaf_size&lt;/code&gt;. This parameter choice has many effects:</source>
          <target state="translated">위에서 언급했듯이 작은 표본 크기의 경우 무차별 검색은 트리 기반 쿼리보다 효율적일 수 있습니다. 이 사실은 내부적으로 리프 노드 내에서 무차별 대입 검색으로 전환하여 볼 트리와 KD 트리에서 설명됩니다. 이 스위치의 레벨은 &lt;code&gt;leaf_size&lt;/code&gt; 매개 변수로 지정할 수 있습니다 . 이 매개 변수 선택은 많은 영향을 미칩니다.</target>
        </trans-unit>
        <trans-unit id="02cd1e4aacf73905da270451ea593d54a1eead69" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; holding the training samples, and an array &lt;code&gt;y&lt;/code&gt; of class labels (strings or integers), of shape &lt;code&gt;(n_samples)&lt;/code&gt;:</source>
          <target state="translated">다른 분류 자처럼 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 는 두 개의 배열을 입력으로 취합니다. 하나 는 훈련 샘플을 보유하는 형태 의 배열 &lt;code&gt;X&lt;/code&gt; &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 와 클래스 레이블 (문자열 또는 정수), 형태 &lt;code&gt;(n_samples)&lt;/code&gt; 의 배열 &lt;code&gt;y&lt;/code&gt; 입니다 .</target>
        </trans-unit>
        <trans-unit id="480621b1ef230a82c36b7031ee3539b5a66ef9c0" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array X of size &lt;code&gt;[n_samples,
n_features]&lt;/code&gt; holding the training samples, and an array y of class labels (strings or integers), size &lt;code&gt;[n_samples]&lt;/code&gt;:</source>
          <target state="translated">다른 분류기로서 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 는 입력으로 두 개의 배열 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 훈련 샘플을 보유하는 크기 [n_samples, n_features] 의 배열 X 와 클래스 레이블 (문자열 또는 정수)의 배열 y, 크기 &lt;code&gt;[n_samples]&lt;/code&gt; 의 두 배열을 입력으로 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="7f73865c72a9e8a8b8d3c39d4c4f128f641ec5e7" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples:</source>
          <target state="translated">다른 분류기와 마찬가지로 SGD에는 두 개의 배열이 장착되어야합니다. 하나 는 훈련 샘플을 보유하는 형태 의 배열 &lt;code&gt;X&lt;/code&gt; (n_samples, n_features)와 훈련 샘플의 목표 값 (클래스 레이블)을 보유하는 형태의 y 배열 (n_samples)입니다. :</target>
        </trans-unit>
        <trans-unit id="86b96557b2fb9eb69b558f788f743c67cbdf526d" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:</source>
          <target state="translated">다른 분류기로서 SGD에는 두 개의 배열, 즉 훈련 샘플을 보유하는 [n_samples, n_features] 크기의 배열 X와 훈련 샘플의 목표 값 (클래스 레이블)을 보유하는 [n_samples] 크기의 배열 Y가 있습니다.</target>
        </trans-unit>
        <trans-unit id="ba8715d103d1eabb437e2b429e94c0804a44a2eb" translate="yes" xml:space="preserve">
          <source>As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of size &lt;code&gt;[n_samples]&lt;/code&gt; holding the target values (class labels) for the training samples:</source>
          <target state="translated">다른 분류기로서, 포리스트 분류기에는 훈련 샘플을 보유한 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 크기의 희소 또는 고밀도 배열 X 와 대상 값 (클래스 레이블)을 보유하는 &lt;code&gt;[n_samples]&lt;/code&gt; 크기의 배열 Y의 두 배열이 장착되어야합니다. 훈련 샘플 :</target>
        </trans-unit>
        <trans-unit id="9d7e042981f1b86e720810a559a0b5f85e715465" translate="yes" xml:space="preserve">
          <source>As said above (see &amp;ldquo;&lt;a href=&quot;#the-pipeline&quot;&gt;The machine-learning pipeline&lt;/a&gt;&amp;rdquo;), we could also choose to scale numerical values before training the model. This can be useful to apply a similar amount regularization to all of them in the Ridge. The preprocessor is redefined in order to subtract the mean and scale variables to unit variance.</source>
          <target state="translated">위에서 언급했듯이 (&amp;ldquo; &lt;a href=&quot;#the-pipeline&quot;&gt;기계 학습 파이프 라인&lt;/a&gt; &amp;rdquo;참조) 모델을 학습하기 전에 숫자 값을 조정하도록 선택할 수도 있습니다. 이는 Ridge의 모든 항목에 유사한 양의 정규화를 적용하는 데 유용 할 수 있습니다. 평균 및 척도 변수를 단위 분산으로 빼기 위해 전처리 기가 재정의됩니다.</target>
        </trans-unit>
        <trans-unit id="d9c002345c84e7b31630185ef2325d2598748527" translate="yes" xml:space="preserve">
          <source>As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized &lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt; library.</source>
          <target state="translated">scikit-learn은 일반적으로 Numpy / Scipy 및 선형 대수에 크게 의존하기 때문에 이러한 라이브러리의 버전을 명시 적으로 관리하는 것이 좋습니다. 기본적으로 Numpy가 최적화 된 &lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt; 라이브러리를 사용하여 빌드되었는지 확인해야합니다 .</target>
        </trans-unit>
        <trans-unit id="52ae38c76153b785a0e48dca405bc79c3254bacc" translate="yes" xml:space="preserve">
          <source>As seen previously, the dataset contains columns with different data types and we need to apply a specific preprocessing for each data types. In particular categorical variables cannot be included in linear model if not coded as integers first. In addition, to avoid categorical features to be treated as ordered values, we need to one-hot-encode them. Our pre-processor will</source>
          <target state="translated">앞서 살펴본 것처럼 데이터 세트에는 데이터 유형이 서로 다른 열이 포함되어 있으며 각 데이터 유형에 대해 특정 전처리를 적용해야합니다. ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ 특히 범주 형 변수는 먼저 정수로 코딩되지 않으면 선형 모델에 포함될 수 없습니다. 또한 범주 형 기능이 정렬 된 값으로 처리되지 않도록하려면이를 원-핫 인코딩해야합니다. 우리의 전처리 기는</target>
        </trans-unit>
        <trans-unit id="de312b28ec8e48929f9dedcb0b6804aa5a471fe2" translate="yes" xml:space="preserve">
          <source>As shown below, t-SNE for higher perplexities finds meaningful topology of two concentric circles, however the size and the distance of the circles varies slightly from the original. Contrary to the two circles dataset, the shapes visually diverge from S-curve topology on the S-curve dataset even for larger perplexity values.</source>
          <target state="translated">아래에서 볼 수 있듯이, 난이도가 높은 t-SNE는 두 동심원의 의미있는 토폴로지를 찾지 만 원의 크기와 거리는 원본과 약간 다릅니다. 두 개의 원 데이터 셋과 달리 셰이프는 S- 커브 데이터 셋에서 S- 커브 토폴로지와 시각적으로 차이가 더 큰 경우에도 크게 달라집니다.</target>
        </trans-unit>
        <trans-unit id="d368728d274b369ac777e6745357cba862711ccd" translate="yes" xml:space="preserve">
          <source>As such variance is dataset dependent, R&amp;sup2; may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R&amp;sup2; score of 0.0.</source>
          <target state="translated">이러한 분산은 데이터 세트에 따라 다르므로 R&amp;sup2;는 다른 데이터 세트에서 의미있게 비교할 수 없습니다. 가능한 최고 점수는 1.0이며 음수 일 수 있습니다 (모델이 임의로 나빠질 수 있기 때문). 입력 특성을 무시하고 y의 예상 값을 항상 예측하는 상수 모델은 0.0의 R&amp;sup2; 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="8731c43edea0fd3a0536e8161228e2825929062b" translate="yes" xml:space="preserve">
          <source>As tf&amp;ndash;idf is very often used for text features, there is also another class called &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; that combines all the options of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; in a single model:</source>
          <target state="translated">tf&amp;ndash;idf는 텍스트 기능에 매우 자주 사용 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 의 모든 옵션을 단일 모델로 결합하는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; 라는 또 다른 클래스도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9ec60e63b98738f893cbc703b7a17df4dbce6a3b" translate="yes" xml:space="preserve">
          <source>As the Earth is nearly spherical, the haversine formula provides a good approximation of the distance between two points of the Earth surface, with a less than 1% error on average.</source>
          <target state="translated">지구가 거의 구형이기 때문에, haversine 공식은 평균 1 % 미만의 오차로 지구 표면의 두 지점 사이의 거리에 대한 좋은 근사치를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ff6adce24795bb660ac39f8e682709377eb07fe5" translate="yes" xml:space="preserve">
          <source>As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">올가미 회귀 분석은 희소 모델을 생성하므로 &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1 기반 기능 선택에&lt;/a&gt; 자세히 설명 된대로 기능 선택을 수행하는 데 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="dc66d7007c269247f1b0a3bd5e17ce48735d32b4" translate="yes" xml:space="preserve">
          <source>As the algorithm tries to balance the volume (ie balance the region sizes), if we take circles with different sizes, the segmentation fails.</source>
          <target state="translated">알고리즘이 볼륨의 균형 (예 : 영역 크기의 균형)을 시도 할 때 크기가 다른 원을 사용하면 분할이 실패합니다.</target>
        </trans-unit>
        <trans-unit id="ca54f3bcc5c655eebc87fa446a643204f651e5fe" translate="yes" xml:space="preserve">
          <source>As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.</source>
          <target state="translated">여기에서 그라운드 진실이 알려져 있으므로, 클러스터 레이블의 적합성이지면 진실에 맞는지 판단하기 위해 다른 클러스터 품질 메트릭을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="26f19c73cbdda7d655c1c419bc1b28d6aee89730" translate="yes" xml:space="preserve">
          <source>As the negative of a distance, this kernel is only conditionally positive definite.</source>
          <target state="translated">거리의 음수 인 것처럼,이 커널은 조건부 양수입니다.</target>
        </trans-unit>
        <trans-unit id="74315005f7ce6ac1c1dafe71dd19e66293fa501a" translate="yes" xml:space="preserve">
          <source>As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian.</source>
          <target state="translated">가중치에 대한 선행은 가우시안 이전이므로 추정 된 가중치의 히스토그램은 가우스입니다.</target>
        </trans-unit>
        <trans-unit id="f78b7e4ef3de6f9908e43081399b599b6d846fba" translate="yes" xml:space="preserve">
          <source>As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.</source>
          <target state="translated">이 알고리즘은 가능성 만 최대화하므로 평균을 0으로 편향 시키거나 클러스터 크기를 편향하여 적용하거나 적용하지 않을 특정 구조를 갖지 않습니다.</target>
        </trans-unit>
        <trans-unit id="492cebfc31b9e0ae2de0bb7669534987ce58057c" translate="yes" xml:space="preserve">
          <source>As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:</source>
          <target state="translated">일반적으로 지형지 물 추출 매개 변수를 조정하는 가장 좋은 방법은 예를 들어 분류기를 사용하여 지형지 물 추출기를 파이프 라인하여 교차 검증 된 그리드 검색을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="ea78a4c7988b659783feac98fcd695de89f8d61f" translate="yes" xml:space="preserve">
          <source>As we have seen, every estimator exposes a &lt;code&gt;score&lt;/code&gt; method that can judge the quality of the fit (or the prediction) on new data. &lt;strong&gt;Bigger is better&lt;/strong&gt;.</source>
          <target state="translated">우리가 본 바와 같이, 모든 추정기는 새로운 데이터에 대한 적합 (또는 예측)의 품질을 판단 할 수 있는 &lt;code&gt;score&lt;/code&gt; 방법을 노출합니다 . &lt;strong&gt;클수록 좋습니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="fff4f5f3ab6438d863abba19bbb7c007a060f360" translate="yes" xml:space="preserve">
          <source>As we&amp;rsquo;ll see, some cross-validation objects do specific things with labeled data, others behave differently with grouped data, and others do not use this information.</source>
          <target state="translated">보시다시피 일부 교차 유효성 검사 개체는 레이블이 지정된 데이터로 특정 작업을 수행하고 다른 개체는 그룹화 된 데이터와 다르게 동작하며 다른 사용자는이 정보를 사용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c2a5e94d01ebd06105b2b62c25b4b493ea944024" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, the module further provides convenience functions &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt; if you don&amp;rsquo;t want to create an object.</source>
          <target state="translated">와 마찬가지로 &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; , 모듈은 더 편리한 기능을 제공합니다 &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt; 객체를 생성하지 않으려면.</target>
        </trans-unit>
        <trans-unit id="c97bbdc8d450f400aad05e1727632d201bf17f90" translate="yes" xml:space="preserve">
          <source>As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">분류 클래스와 마찬가지로 fit 메소드는 인수 벡터 X, y를 취하며이 경우 y는 정수 값 대신 부동 소수점 값을 갖도록 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="320d01bbc29ff09708f415fa50fd9da6e6972d95" translate="yes" xml:space="preserve">
          <source>As with other classifiers, &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; takes as input two arrays: an array X, sparse or dense, of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of integer values, size &lt;code&gt;[n_samples]&lt;/code&gt;, holding the class labels for the training samples:</source>
          <target state="translated">다른 분류 자와 마찬가지로 &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; 는 두 개의 배열을 입력합니다. 훈련 샘플을 보유하는 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 크기의 배열 X, 희소 또는 밀도 배열 , 클래스 레이블을 보유하는 정수 값, 크기 &lt;code&gt;[n_samples]&lt;/code&gt; 의 배열 Y 훈련 샘플 :</target>
        </trans-unit>
        <trans-unit id="211544b1f9de9dd9d971b4d12a5c9dd1ee84dd6a" translate="yes" xml:space="preserve">
          <source>As with other linear models, &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; will take in its &lt;code&gt;fit&lt;/code&gt; method arrays X, y and will store the coefficients \(w\) of the linear model in its &lt;code&gt;coef_&lt;/code&gt; member:</source>
          <target state="translated">다른 선형 모델과 마찬가지로 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;fit&lt;/code&gt; 방법 배열 X, y 를 취하여 선형 모델 의 계수 \ (w \)를 &lt;code&gt;coef_&lt;/code&gt; 멤버에 저장합니다.</target>
        </trans-unit>
        <trans-unit id="0550c97cec757ba988ca448a8879bb1fa8d6048a" translate="yes" xml:space="preserve">
          <source>As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the &lt;code&gt;DictVectorizer&lt;/code&gt; class uses a &lt;code&gt;scipy.sparse&lt;/code&gt; matrix by default instead of a &lt;code&gt;numpy.ndarray&lt;/code&gt;.</source>
          <target state="translated">아시다시피, 문서 모음의 각 개별 단어 주위에서 이러한 컨텍스트를 추출하면 결과 행렬은 매우 넓고 (대부분의 인기 기능) 대부분의 시간은 거의 0으로 평가됩니다. 결과 데이터 구조를 메모리에 맞추기 위해 &lt;code&gt;DictVectorizer&lt;/code&gt; 클래스는 기본적으로 &lt;code&gt;numpy.ndarray&lt;/code&gt; 대신 &lt;code&gt;scipy.sparse&lt;/code&gt; 행렬을 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="bf1b1999c59fa9b3c0bf1aeb80e6b35e3cde33f4" translate="yes" xml:space="preserve">
          <source>As you can see, by default the KFold cross-validation iterator does not take either datapoint class or group into consideration. We can change this by using the &lt;code&gt;StratifiedKFold&lt;/code&gt; like so.</source>
          <target state="translated">보시다시피 기본적으로 KFold 교차 유효성 검사 반복기는 데이터 포인트 클래스 또는 그룹을 고려하지 않습니다. &lt;code&gt;StratifiedKFold&lt;/code&gt; 를 사용하여이를 변경할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4243f9f7d96f83f5f8fe40416e149534c5c3a7c1" translate="yes" xml:space="preserve">
          <source>As you can see, it is a challenging task: after all, the images are of poor resolution. Do you agree with the classifier?</source>
          <target state="translated">보시다시피 어려운 과제입니다. 결국 이미지의 해상도가 떨어집니다. 분류기에 동의하십니까?</target>
        </trans-unit>
        <trans-unit id="91d172a6c45b01ca0f06c000f8b26183873978d6" translate="yes" xml:space="preserve">
          <source>As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:</source>
          <target state="translated">보다시피, [[0.5]]와 [[2]]를 반환합니다. 이는 요소가 0.5 거리에 있고 샘플의 세 번째 요소임을 의미합니다 (인덱스는 0에서 시작). 여러 포인트를 쿼리 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="285f08a9d27e1e1ebbb2de47dbd27bd9e8a8b2c7" translate="yes" xml:space="preserve">
          <source>As you can see, the &lt;code&gt;[1, 0]&lt;/code&gt; is comfortably classified as &lt;code&gt;1&lt;/code&gt; since the first two samples are ignored due to their sample weights.</source>
          <target state="translated">보시다시피 &lt;code&gt;[1, 0]&lt;/code&gt; 은 샘플 가중치로 인해 처음 두 샘플이 무시 되므로 &lt;code&gt;1&lt;/code&gt; 로 편안하게 분류됩니다 .</target>
        </trans-unit>
        <trans-unit id="1273827574a8e654ba4e8fa3b455e8b899058288" translate="yes" xml:space="preserve">
          <source>Ash</source>
          <target state="translated">Ash</target>
        </trans-unit>
        <trans-unit id="7e00bdd748db07e532eed9a3a30221202b903645" translate="yes" xml:space="preserve">
          <source>Ash:</source>
          <target state="translated">Ash:</target>
        </trans-unit>
        <trans-unit id="fa6467e4d61dfb46a998615f44d406b657f3e65c" translate="yes" xml:space="preserve">
          <source>Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).</source>
          <target state="translated">트레이닝 세트의 모든 문서에서 발생하는 각 단어에 고정 정수 ID를 할당하십시오 (예 : 단어를 정수 인덱스로 사전을 작성하여).</target>
        </trans-unit>
        <trans-unit id="07fb5452be9437f82662fee13a0f5824d7cc38d4" translate="yes" xml:space="preserve">
          <source>Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm.</source>
          <target state="translated">일대일 방식으로 한 세트에서 다른 세트로 biclusters를 지정하여 유사성의 합계를 최대화하십시오. 이 단계는 헝가리어 알고리즘을 사용하여 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="341c86e4b7fe6ff7ce7735c5402c926cbe3cd37f" translate="yes" xml:space="preserve">
          <source>Assume that there are no ties in y_score (which is likely to be the case if y_score is continuous) for efficiency gains.</source>
          <target state="translated">효율성 향상을 위해 y_score (y_score가 연속적인 경우 일 가능성이 높음)에 동점이 없다고 가정합니다.</target>
        </trans-unit>
        <trans-unit id="eeab86fb8cce55c225e83b0d2dc18f408531cb78" translate="yes" xml:space="preserve">
          <source>Assume two label assignments (of the same N objects), \(U\) and \(V\). Their entropy is the amount of uncertainty for a partition set, defined by:</source>
          <target state="translated">\ (U \) 및 \ (V \)라는 두 개의 레이블 할당 (동일한 N 개체)을 가정합니다. 그들의 엔트로피는 파티션 세트에 대한 불확실성의 정도입니다.</target>
        </trans-unit>
        <trans-unit id="0e5d6eebf8d779ccbfdee109f1d6bfd1baee97c2" translate="yes" xml:space="preserve">
          <source>Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.</source>
          <target state="translated">일부 데이터가 독립적이고 동일하게 분포되어 있다고 가정하면 (iid) 모든 샘플이 동일한 생성 프로세스에서 발생하고 생성 프로세스에 과거 생성 된 샘플의 메모리가없는 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="fc888006cf2fe36cad38d794da87b3aa0beb9fc6" translate="yes" xml:space="preserve">
          <source>At each stage the decision tree \(h_m(x)\) is chosen to minimize the loss function \(L\) given the current model \(F_{m-1}\) and its fit \(F_{m-1}(x_i)\)</source>
          <target state="translated">각 단계에서 결정 트리 \ (h_m (x) \)는 현재 모델 \ (F_ {m-1} \)과 적합도 \ (F_ {m-1)를 고려하여 손실 함수 \ (L \)를 최소화하기 위해 선택됩니다. } (x_i) \)</target>
        </trans-unit>
        <trans-unit id="bfe84c1effa1f2d8d0a150671e591a10f6f5331c" translate="yes" xml:space="preserve">
          <source>At first, a linear model will be applied on the original targets. Due to the non-linearity, the model trained will not be precise during the prediction. Subsequently, a logarithmic function is used to linearize the targets, allowing better prediction even with a similar linear model as reported by the median absolute error (MAE).</source>
          <target state="translated">처음에는 선형 모델이 원래 대상에 적용됩니다. 비선형 성으로 인해 훈련 ​​된 모델은 예측 중에 정확하지 않습니다. 결과적으로, 로그 함수는 목표를 선형화하는 데 사용되어 MAE (median absolute error)에 의해보고 된 것과 유사한 선형 모델에서도 더 나은 예측이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="3103892102cc58ce3e902fe26301aa542c051586" translate="yes" xml:space="preserve">
          <source>At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.</source>
          <target state="translated">피팅 타임에, 코드북에서 비트 당 하나의 이진 분류 기가 장착된다. 예측시, 분류기는 클래스 공간에서 새로운 포인트를 투영하는 데 사용되며 포인트에 가장 가까운 클래스가 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="a0707d969d20d6e3136e641e1fe81f003f9dea63" translate="yes" xml:space="preserve">
          <source>At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.</source>
          <target state="translated">학습 시간은 단순히 수업 당 하나의 회귀 또는 이진 분류기를 학습하는 것으로 구성됩니다. 그렇게 할 때 다중 클래스 레이블을 이진 레이블로 변환해야합니다 (클래스에 속하거나 속하지 않음). LabelBinarizer를 사용하면 변환 방법으로이 프로세스를 쉽게 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="934e61570171dfc9a4fde1dc5a30d65977fe4f97" translate="yes" xml:space="preserve">
          <source>At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.</source>
          <target state="translated">예측시, 해당 모델이 가장 큰 신뢰를 얻은 클래스를 할당합니다. LabelBinarizer를 사용하면 inverse_transform 메소드를 사용하여이를 쉽게 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="01685dbfb47dea411fa9fd7d65acd3b07b6bad19" translate="yes" xml:space="preserve">
          <source>At present, no metric in &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; supports the multioutput-multiclass classification task.</source>
          <target state="translated">현재 &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; 에는 다중 출력 멀티 클래스 분류 작업을 지원하는 메트릭이 없습니다 .</target>
        </trans-unit>
        <trans-unit id="e6b9e03e5fa2d5e24dd0fa35a80f06e2085ef3ae" translate="yes" xml:space="preserve">
          <source>At the end, the top 10 most uncertain predictions will be shown.</source>
          <target state="translated">마지막으로 가장 불확실한 상위 10 개의 예측이 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="7dcf986df80f51359fc05a76ec4e2e0108cdc275" translate="yes" xml:space="preserve">
          <source>At the moment, we also don&amp;rsquo;t allow &amp;ldquo;multiclass-multioutput&amp;rdquo; input type.</source>
          <target state="translated">현재는&amp;ldquo;멀티 클래스 멀티 출력&amp;rdquo;입력 유형도 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c524e1372c32ce7785a6874af380c32795f14a92" translate="yes" xml:space="preserve">
          <source>At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by &lt;code&gt;pip&lt;/code&gt;) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the &amp;ldquo;defaults&amp;rdquo; channel from anaconda.org are linked by default with MKL.</source>
          <target state="translated">작성 당시 (2019), pypi.org ( &lt;code&gt;pip&lt;/code&gt; 에서 사용 ) 및 conda-forge 채널에 배포 된 NumPy 및 SciPy 패키지 는 OpenBLAS와 연결되어있는 반면 anaconda.org의 &quot;defaults&quot;채널에서 배송 된 conda 패키지는 다음과 같습니다. 기본적으로 MKL과 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="9df8fa75ddc8543ff453e545d20ad90388cfe52d" translate="yes" xml:space="preserve">
          <source>Atlas (need hardware specific tuning by rebuilding on the target machine)</source>
          <target state="translated">Atlas (대상 시스템을 재구성하여 하드웨어 특정 조정이 필요함)</target>
        </trans-unit>
        <trans-unit id="ea953d1f635660a779a7b571a3caa0a7e2c19942" translate="yes" xml:space="preserve">
          <source>Attribute Information</source>
          <target state="translated">속성 정보</target>
        </trans-unit>
        <trans-unit id="9f84e7881c1e8ab9dc1a7da3d9d41fc35c6c92bb" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order)</source>
          <target state="translated">속성 정보 (순서대로)</target>
        </trans-unit>
        <trans-unit id="023272de4f331b86e2df4744d3b1e32a0350f127" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order):</source>
          <target state="translated">속성 정보 (순서) :</target>
        </trans-unit>
        <trans-unit id="f66b29f74a8ebab57ad67bac7a0ee9e151e247d3" translate="yes" xml:space="preserve">
          <source>Attribute Information:</source>
          <target state="translated">속성 정보 :</target>
        </trans-unit>
        <trans-unit id="08359a131c436e0cfbcef05e6e7b301827c6d117" translate="yes" xml:space="preserve">
          <source>Attribute name(s) given as string or a list/tuple of strings Eg.: &lt;code&gt;[&quot;coef_&quot;, &quot;estimator_&quot;, ...], &quot;coef_&quot;&lt;/code&gt;</source>
          <target state="translated">문자열 또는 문자열 목록 / 튜플로 제공된 속성 이름 예 : &lt;code&gt;[&quot;coef_&quot;, &quot;estimator_&quot;, ...], &quot;coef_&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d3d9c6c9c4fb8ae7805f1c9bc9a51e56b622aec7" translate="yes" xml:space="preserve">
          <source>Attribute to access any fitted sub-estimators by name.</source>
          <target state="translated">이름별로 적합한 하위 추정자에 액세스하는 속성입니다.</target>
        </trans-unit>
        <trans-unit id="e30390c6b25519953f15954ce4132cba67fdd587" translate="yes" xml:space="preserve">
          <source>AttributeError</source>
          <target state="translated">AttributeError</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="12b65a8e129440be6a666c5f76241c3731cf3a1b" translate="yes" xml:space="preserve">
          <source>Attributes of named_steps map to keys, enabling tab completion in interactive environments:</source>
          <target state="translated">named_steps의 속성이 키에 맵핑되어 대화식 환경에서 탭 완성이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="b8087185e5ee37cef4c337de5697d35d75d909fd" translate="yes" xml:space="preserve">
          <source>Attributes:</source>
          <target state="translated">Attributes:</target>
        </trans-unit>
        <trans-unit id="662dd9f8d8390601f16824595a7c397648198b36" translate="yes" xml:space="preserve">
          <source>Augment dataset with an additional dummy feature.</source>
          <target state="translated">추가 더미 기능으로 데이터 세트를 보강하십시오.</target>
        </trans-unit>
        <trans-unit id="6060232846b2935a9975d9bd33986976db84e5d5" translate="yes" xml:space="preserve">
          <source>Authors : Kemal Eren License: BSD 3 clause</source>
          <target state="translated">저자 : Kemal Eren 라이센스 : BSD 3 조항</target>
        </trans-unit>
        <trans-unit id="c8696543eb048b80bb9394a738477bc586b5e9f7" translate="yes" xml:space="preserve">
          <source>Authors: &lt;a href=&quot;mailto:mks542%40nyu.edu&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;https://github.com/maikia&quot;&gt;Maria Telenczuk&lt;/a&gt;</source>
          <target state="translated">저자 : &lt;a href=&quot;mailto:mks542%40nyu.edu&quot;&gt;Manoj Kumar&lt;/a&gt; , &lt;a href=&quot;https://github.com/maikia&quot;&gt;Maria Telenczuk&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b98ce7a4dae4be4f8e868a70ee741cf11d8125c8" translate="yes" xml:space="preserve">
          <source>Automatic Relevance Determination Regression (ARD)</source>
          <target state="translated">자동 관련성 결정 회귀 (ARD)</target>
        </trans-unit>
        <trans-unit id="79d2e9f45916ec9baf098fb8e9c666bf3b326d9b" translate="yes" xml:space="preserve">
          <source>Automatic grouping of similar objects into sets.</source>
          <target state="translated">유사한 개체를 집합으로 자동 그룹화합니다.</target>
        </trans-unit>
        <trans-unit id="ef442c781e8f4741229b1581adda336c7e2079fb" translate="yes" xml:space="preserve">
          <source>Automatic selection</source>
          <target state="translated">자동 선택</target>
        </trans-unit>
        <trans-unit id="e1af4aed4c1976557d8776224b00250d32e50c1e" translate="yes" xml:space="preserve">
          <source>Automatic selection:</source>
          <target state="translated">자동 선택 :</target>
        </trans-unit>
        <trans-unit id="023744d610124c3af17954739cd092606ebe3ff4" translate="yes" xml:space="preserve">
          <source>Automatically extract clusters according to the Xi-steep method.</source>
          <target state="translated">Xi-steep 방법에 따라 클러스터를 자동으로 추출합니다.</target>
        </trans-unit>
        <trans-unit id="6cfaabc56d1e51ca7fb8958edc60fddd81d43492" translate="yes" xml:space="preserve">
          <source>Available Metrics</source>
          <target state="translated">사용 가능한 측정 항목</target>
        </trans-unit>
        <trans-unit id="66f9440a5b62bebabad643e5080d2b83ccab9f91" translate="yes" xml:space="preserve">
          <source>Available Metrics The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">사용 가능한 메트릭 다음은 문자열 메트릭 식별자 및 관련 거리 메트릭 클래스 목록입니다.</target>
        </trans-unit>
        <trans-unit id="7d386374f19b16d2a68aae1af1097f01c0217752" translate="yes" xml:space="preserve">
          <source>Available losses for regression are &amp;lsquo;least_squares&amp;rsquo;, &amp;lsquo;least_absolute_deviation&amp;rsquo;, which is less sensitive to outliers, and &amp;lsquo;poisson&amp;rsquo;, which is well suited to model counts and frequencies. For classification, &amp;lsquo;binary_crossentropy&amp;rsquo; is used for binary classification and &amp;lsquo;categorical_crossentropy&amp;rsquo; is used for multiclass classification. By default the loss is &amp;lsquo;auto&amp;rsquo; and will select the appropriate loss depending on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;y&lt;/a&gt; passed to &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;.</source>
          <target state="translated">회귀에 사용할 수있는 손실은 이상 값에 덜 민감한 'least_squares', 'least_absolute_deviation', 모델 수 및 빈도에 적합한 'poisson'입니다. 분류의 경우 이진 분류에는 'binary_crossentropy'가 사용되고 다중 클래스 분류에는 'categorical_crossentropy'가 사용됩니다. 기본적으로 손실은 '자동'이며 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit에&lt;/a&gt; 전달 된 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;y&lt;/a&gt; 에 따라 적절한 손실을 선택합니다 .</target>
        </trans-unit>
        <trans-unit id="eca931fa62c0bd7287f5c68d38715d784e965be8" translate="yes" xml:space="preserve">
          <source>AveBedrms average number of bedrooms</source>
          <target state="translated">AveBedrms 평균 침실 수</target>
        </trans-unit>
        <trans-unit id="a283071b7681c2d8946f1976d0c70df9b6239788" translate="yes" xml:space="preserve">
          <source>AveOccup average house occupancy</source>
          <target state="translated">AveOccup 평균 주택 점유율</target>
        </trans-unit>
        <trans-unit id="17df5670db0dd155c5ad44fbbe5780684721e711" translate="yes" xml:space="preserve">
          <source>AveRooms average number of rooms</source>
          <target state="translated">AveRooms 평균 객실 수</target>
        </trans-unit>
        <trans-unit id="09b414b4c6acadd2002166fddf247cff6dfe513e" translate="yes" xml:space="preserve">
          <source>Average anomaly score of X of the base classifiers.</source>
          <target state="translated">기본 분류 기준 X의 평균 이상 점수입니다.</target>
        </trans-unit>
        <trans-unit id="aaed8a8037e41bff9561818c652346d8c7001f53" translate="yes" xml:space="preserve">
          <source>Average blood pressure</source>
          <target state="translated">평균 혈압</target>
        </trans-unit>
        <trans-unit id="6a54e71704b0c1b179b46e11f33d130b77d3a0d5" translate="yes" xml:space="preserve">
          <source>Average hinge loss (non-regularized)</source>
          <target state="translated">평균 힌지 손실 (비정규)</target>
        </trans-unit>
        <trans-unit id="6b044db8b528a2f509603d248114c200773b5ebd" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model</source>
          <target state="translated">현재 모델에서 샘플의 평균 로그 우도</target>
        </trans-unit>
        <trans-unit id="eb68b4c700400b34d7c3583315c40c631209f07e" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model.</source>
          <target state="translated">현재 모델에서 샘플의 평균 로그 가능도입니다.</target>
        </trans-unit>
        <trans-unit id="c993da9cb729cacf8ff6fdcfed939fa26cd8fbe9" translate="yes" xml:space="preserve">
          <source>Average of each column of kernel matrix</source>
          <target state="translated">커널 행렬의 각 열 평균</target>
        </trans-unit>
        <trans-unit id="85f369d4432a9c6380c3e6b6c74d7a111da4715c" translate="yes" xml:space="preserve">
          <source>Average of kernel matrix</source>
          <target state="translated">커널 행렬의 평균</target>
        </trans-unit>
        <trans-unit id="d0fd4fa14e78cac718fcf8ea4ba2f14bf0250758" translate="yes" xml:space="preserve">
          <source>Average of the decision functions of the base classifiers.</source>
          <target state="translated">기본 분류기의 결정 기능의 평균입니다.</target>
        </trans-unit>
        <trans-unit id="6599ad46541efb6c59215eff61a003e0c6f84646" translate="yes" xml:space="preserve">
          <source>Average precision. If None, the average precision is not shown.</source>
          <target state="translated">평균 정밀도. None이면 평균 정밀도가 표시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="af6f79d617c2284de9fdf1ff445ccb0bef8560e1" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features.</source>
          <target state="translated">기능에 할당 된 평균 가중치.</target>
        </trans-unit>
        <trans-unit id="f2d4013e90ff32393ce2936382f74ff4d17a82cb" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="translated">기능에 할당 된 평균 가중치입니다. &lt;code&gt;average=True&lt;/code&gt; 인 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="964142eff5ddb8396f41dd41083f429f4f575768" translate="yes" xml:space="preserve">
          <source>Avoid computation of the row norms of X.</source>
          <target state="translated">X의 행 규범을 계산하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="9941876a6243b1a2b044ada9bc274f0eac824c27" translate="yes" xml:space="preserve">
          <source>Axes object to plot on. If &lt;code&gt;None&lt;/code&gt;, a new figure and axes is created.</source>
          <target state="translated">플로팅 할 축 객체입니다. 경우 &lt;code&gt;None&lt;/code&gt; , 새로운 모습과 축이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="cdbc35f14da17d95fedf51be8b6d0e5ffbe47113" translate="yes" xml:space="preserve">
          <source>Axes to plot to. If None, use current axis. Any previous content is cleared.</source>
          <target state="translated">플로팅 할 축입니다. None이면 현재 축을 사용합니다. 이전 콘텐츠는 모두 지워집니다.</target>
        </trans-unit>
        <trans-unit id="4301ccb7939c0c6e359abdb2441c99fb2531f767" translate="yes" xml:space="preserve">
          <source>Axes with ROC Curve.</source>
          <target state="translated">ROC 곡선이있는 축.</target>
        </trans-unit>
        <trans-unit id="ff3b86b5f5ed2e8ea1a56f1b370527db0b52d66c" translate="yes" xml:space="preserve">
          <source>Axes with confusion matrix.</source>
          <target state="translated">혼동 행렬이있는 축.</target>
        </trans-unit>
        <trans-unit id="d788fc3169dcbb97f9fc05c8f3617be236c43224" translate="yes" xml:space="preserve">
          <source>Axes with precision recall curve.</source>
          <target state="translated">정밀 리콜 곡선이있는 축.</target>
        </trans-unit>
        <trans-unit id="94256a7c3ba8c276f8434ee8dd2981a7e1ebede5" translate="yes" xml:space="preserve">
          <source>Axis along which the argmin and distances are to be computed.</source>
          <target state="translated">argmin과 거리가 계산되는 축.</target>
        </trans-unit>
        <trans-unit id="9912ed29fe6a32ef154522ba9276f54ba95862ff" translate="yes" xml:space="preserve">
          <source>Axis along which the axis should be computed.</source>
          <target state="translated">축을 계산해야하는 축입니다.</target>
        </trans-unit>
        <trans-unit id="e908ee13d1946f0bd5dd05b5fc4432878bcf585b" translate="yes" xml:space="preserve">
          <source>Axis along which to operate. Default is 0, i.e. the first axis.</source>
          <target state="translated">작동 할 축. 기본값은 0, 즉 첫 번째 축입니다.</target>
        </trans-unit>
        <trans-unit id="5bcbcdbcc90358e775edd4243e64cbf53abf5bcb" translate="yes" xml:space="preserve">
          <source>Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.</source>
          <target state="translated">평균이 계산되는 축입니다. 기본값은 평평한 배열의 평균을 계산하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="ebbac4e2a88e22f8f6d03a59528261348ca05f77" translate="yes" xml:space="preserve">
          <source>Axis used to compute the means and standard deviations along. If 0, transform each feature, otherwise (if 1) transform each sample.</source>
          <target state="translated">평균과 표준 편차를 계산하는 데 사용되는 축입니다. 0이면 각 피처를 변환하고, 그렇지 않으면 (1 인 경우) 각 샘플을 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="1f1b4d8a7c2bf89e7c753b7dde1a7de7538ad410" translate="yes" xml:space="preserve">
          <source>Axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample.</source>
          <target state="translated">크기를 조정하는 데 사용되는 축입니다. 0이면 각 특성을 독립적으로 확장하고, 그렇지 않으면 (1이면) 각 샘플의 배율을 조정합니다.</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="f9392f6fda3d4e0cffc8528669a8abc510a0238a" translate="yes" xml:space="preserve">
          <source>B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</source>
          <target state="translated">B 1000 (Bk-0.63) ^ 2 여기서 Bk는 도시 별 검정 비율입니다.</target>
        </trans-unit>
        <trans-unit id="0286abf8af6c149d27c85dea93b91d7cc057559a" translate="yes" xml:space="preserve">
          <source>B. C. Ross &amp;ldquo;Mutual Information between Discrete and Continuous Data Sets&amp;rdquo;. PLoS ONE 9(2), 2014.</source>
          <target state="translated">BC Ross&amp;ldquo;이산 및 연속 데이터 세트 간의 상호 정보&amp;rdquo;. PLoS ONE 9 (2), 2014.</target>
        </trans-unit>
        <trans-unit id="e776d2e85d14b59e337010c96ae3a1db78bda84a" translate="yes" xml:space="preserve">
          <source>B12</source>
          <target state="translated">B12</target>
        </trans-unit>
        <trans-unit id="598b91099876ac145b645491cf669799147b8703" translate="yes" xml:space="preserve">
          <source>Back-projection to the original space.</source>
          <target state="translated">원래 공간으로 다시 투영합니다.</target>
        </trans-unit>
        <trans-unit id="e935196220898869d9edfac83ba997f81f0fb5f8" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have negative or close to 0.0 scores:</source>
          <target state="translated">불량 (예 : 독립적 인 라벨링)은 음수 또는 0.0 점에 가깝습니다.</target>
        </trans-unit>
        <trans-unit id="739e4c11d3b87131b9d408fbaefcc186a18d8f06" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have non-positive scores:</source>
          <target state="translated">불량 (예 : 독립적 인 라벨링)에는 양수가 아닌 점수가 있습니다.</target>
        </trans-unit>
        <trans-unit id="3594d99d3d09849089991e6e49125443f8f3dc1b" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have zero scores:</source>
          <target state="translated">불량 (예 : 독립 라벨링)은 점수가 0입니다.</target>
        </trans-unit>
        <trans-unit id="ab5267c44135f5b9260b00b1b8283ecbffae624a" translate="yes" xml:space="preserve">
          <source>Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:</source>
          <target state="translated">자루에 넣는 방법은 여러 가지 맛이 있지만 훈련 세트의 무작위 하위 집합을 그리는 방식에 따라 서로 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c18dc564424c60fd05a12c3420895fdf2cfdfe52" translate="yes" xml:space="preserve">
          <source>Bags of words</source>
          <target state="translated">단어의 가방</target>
        </trans-unit>
        <trans-unit id="c108b519256622b208a24e6481a9d957224afa52" translate="yes" xml:space="preserve">
          <source>Balance model complexity and cross-validated score</source>
          <target state="translated">모델 복잡성과 교차 검증 된 점수의 균형</target>
        </trans-unit>
        <trans-unit id="95bff14a4bbf238a2164b4e0cae458c0fe2685ef" translate="yes" xml:space="preserve">
          <source>Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (&lt;code&gt;sample_weight&lt;/code&gt;) for each class to the same value. Also note that weight-based pre-pruning criteria, such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">나무가 지배적 인 클래스에 편향되는 것을 방지하기 위해 훈련 전에 데이터 세트의 균형을 유지하십시오. 클래스 밸런싱은 각 클래스에서 동일한 수의 샘플을 샘플링하거나 바람직하게는 각 클래스에 대한 샘플 가중치의 합 ( &lt;code&gt;sample_weight&lt;/code&gt; )을 동일한 값 으로 정규화하여 수행 할 수 있습니다 . 또한 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; 와 같은 가중치 기반 사전 정리 기준은 min_samples_leaf 와 같은 샘플 가중치를 인식하지 않는 기준보다 우세한 클래스에 대해 편향이 &lt;code&gt;min_samples_leaf&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="05f675285d8ed3983e70827edce654b1139be242" translate="yes" xml:space="preserve">
          <source>Balanced Accuracy as described in &lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]&lt;/a&gt;: the average of sensitivity and specificity is computed for each class and then averaged over total number of classes.</source>
          <target state="translated">&lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]에&lt;/a&gt; 설명 된 바와 같이 균형 잡힌 정확도 : 감도 및 특이성의 평균은 각 클래스에 대해 계산 된 다음 총 클래스 수에 대해 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="784fa76a8b1c1ec60f28239d94fc65e8d7be9167" translate="yes" xml:space="preserve">
          <source>Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the accuracy of prediction algorithms for classification: an overview</source>
          <target state="translated">Baldi, Brunak, Chauvin, Andersen 및 Nielsen (2000). 분류를위한 예측 알고리즘의 정확성 평가 : 개요</target>
        </trans-unit>
        <trans-unit id="06f9184e762e9e45e71254eb60f8399d4c411472" translate="yes" xml:space="preserve">
          <source>Ball tree for fast generalized N-point problems.</source>
          <target state="translated">빠른 일반화 된 N- 포인트 문제를위한 볼 트리.</target>
        </trans-unit>
        <trans-unit id="7faaf3d73987f1f3ca80fb71528f4fcc07c23b8e" translate="yes" xml:space="preserve">
          <source>BallTree for fast generalized N-point problems</source>
          <target state="translated">빠른 일반화 된 N- 포인트 문제를위한 BallTree</target>
        </trans-unit>
        <trans-unit id="fef5acd09f6d09b9952e2688ee6340fe9a396e5b" translate="yes" xml:space="preserve">
          <source>BallTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">볼 트리 (X, leaf_size = 40, metric = 'minkowski', ** kwargs)</target>
        </trans-unit>
        <trans-unit id="f72ce994093395e37676061f1dd51a4ea80c1082" translate="yes" xml:space="preserve">
          <source>Bandwidth used in the RBF kernel.</source>
          <target state="translated">RBF 커널에서 사용되는 대역폭.</target>
        </trans-unit>
        <trans-unit id="0c486c3167e8e5060c79997e836642fbe914971f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method=&amp;rdquo;exact&amp;rdquo;</source>
          <target state="translated">반즈-헛은 정확한 방법의 근사치입니다. 근사값은 각도 매개 변수로 매개 변수화되므로 method =&amp;rdquo;exact&amp;rdquo;인 경우 각도 매개 변수는 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="3b0ab9922dcf0fcbb725191c9b8bc0a7c9ecc19c" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable</source>
          <target state="translated">Barnes-Hut은 확장 성이 훨씬 뛰어납니다. Barnes-Hut을 사용하면 수십만 개의 데이터 포인트를 임베드 할 수 있으며 정확한 방법으로 계산하기 어려운 수천 개의 샘플을 처리 할 수 ​​있습니다</target>
        </trans-unit>
        <trans-unit id="b10a9a2aa738bc222bc8e12bf3c9da484bcf459f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">반즈-헛은 밀집된 입력 데이터에서만 작동합니다. 희소 데이터 행렬은 정확한 방법으로 만 임베드되거나 &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt; &lt;/a&gt; 를 사용하여 밀도가 낮은 저급 투영으로 근사화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7063b67add41dd6938d0fe03decc31a786e18ce9" translate="yes" xml:space="preserve">
          <source>Base class for all estimators in scikit-learn</source>
          <target state="translated">scikit-learn의 모든 견적 자에 대한 기본 클래스</target>
        </trans-unit>
        <trans-unit id="b3ed1a2c57def49034ad9434aa34e34b64c59294" translate="yes" xml:space="preserve">
          <source>Base class for all kernels.</source>
          <target state="translated">모든 커널의 기본 클래스.</target>
        </trans-unit>
        <trans-unit id="597d1d5f179914ea7470a3760bd8ee3a2400c1bc" translate="yes" xml:space="preserve">
          <source>Base classes</source>
          <target state="translated">기본 수업</target>
        </trans-unit>
        <trans-unit id="425994da22262a2dfe067d38bec718a9f15fcaea" translate="yes" xml:space="preserve">
          <source>Base classes for all estimators.</source>
          <target state="translated">모든 견적 자에 대한 기본 클래스.</target>
        </trans-unit>
        <trans-unit id="b0cdf2e205135846db5a02006385fb2f510f153e" translate="yes" xml:space="preserve">
          <source>Base classifier for this ensemble.</source>
          <target state="translated">이 앙상블의 기본 분류기.</target>
        </trans-unit>
        <trans-unit id="61ae0e2f9a4051180ae0aa7361de2f44f1e6a032" translate="yes" xml:space="preserve">
          <source>Base estimator for this ensemble.</source>
          <target state="translated">이 앙상블의 기본 추정기.</target>
        </trans-unit>
        <trans-unit id="96ce9dc70ae7445a77ac8ebb388087887574787e" translate="yes" xml:space="preserve">
          <source>Base estimator object which implements the following methods:</source>
          <target state="translated">다음 방법을 구현하는 기본 추정기 개체 :</target>
        </trans-unit>
        <trans-unit id="a4f9906b1c0bdc268249c9569eb1485c9b45d816" translate="yes" xml:space="preserve">
          <source>Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to &amp;lsquo;drop&amp;rsquo; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">함께 쌓일 기본 견적. 목록의 각 요소는 문자열 (예 : 이름)의 튜플과 추정기 인스턴스로 정의됩니다. 추정기는 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 'drop'으로 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d3f0bf51e2f9cb27fcde269872aa74bb4b0677d2" translate="yes" xml:space="preserve">
          <source>Base of the logarithm used for the discount. A low value means a sharper discount (top results are more important).</source>
          <target state="translated">할인에 사용되는 로그의 밑입니다. 낮은 값은 더 큰 할인을 의미합니다 (상위 결과가 더 중요 함).</target>
        </trans-unit>
        <trans-unit id="f6856d61f849440214e260ddb8260a2004382c80" translate="yes" xml:space="preserve">
          <source>Based on these bin intervals, &lt;code&gt;X&lt;/code&gt; is transformed as follows:</source>
          <target state="translated">이러한 구간 간격을 기준으로 &lt;code&gt;X&lt;/code&gt; 는 다음과 같이 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="623eb094427cb27d5c4cd4b74097a0449f38ab15" translate="yes" xml:space="preserve">
          <source>Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.</source>
          <target state="translated">기본적으로 1. 하드 드라이브의 파일, 데이터베이스, 네트워크 스트림 등에서 인스턴스를 생성하는 리더 일 수 있습니다. 그러나이를 달성하는 방법에 대한 자세한 내용은이 문서의 범위를 벗어납니다.</target>
        </trans-unit>
        <trans-unit id="93b2071c2229aa4389154bc62f3ee3617cb13ae0" translate="yes" xml:space="preserve">
          <source>Bayesian ARD regression.</source>
          <target state="translated">베이지안 ARD 회귀</target>
        </trans-unit>
        <trans-unit id="53c3e11f0d41baa7b5753d8723cd4012a9d964c3" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression</source>
          <target state="translated">베이지안 릿지 회귀</target>
        </trans-unit>
        <trans-unit id="5f8e5b5e24015d9adc82586dba204556446370ab" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression is used for regression:</source>
          <target state="translated">베이지안 릿지 회귀는 회귀에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c13c97854320151b1c6271dc7d1a9fd73998cc11" translate="yes" xml:space="preserve">
          <source>Bayesian information criterion for the current model on the input X.</source>
          <target state="translated">입력 X의 현재 모델에 대한 베이지안 정보 기준.</target>
        </trans-unit>
        <trans-unit id="9caaf8acefcf32287fe9e5bedbc0bc32de3a2bfd" translate="yes" xml:space="preserve">
          <source>Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.</source>
          <target state="translated">베이지안 회귀 기술은 추정 절차에서 정규화 파라미터를 포함하기 위해 사용될 수있다 : 정규화 파라미터는 어려운 의미로 설정되지 않고 현재 데이터에 맞게 조정된다.</target>
        </trans-unit>
        <trans-unit id="f02284fac00f823c24638d260dbf19c33e10b7f5" translate="yes" xml:space="preserve">
          <source>Bayesian regressors</source>
          <target state="translated">베이지안 회귀 변수</target>
        </trans-unit>
        <trans-unit id="2b720ec1817dce1bf9d7dcbcab6e206b85efdaa6" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression</source>
          <target state="translated">베이지안 능선 회귀</target>
        </trans-unit>
        <trans-unit id="66a81420e9a002d3d7820b1130f20af6976fb2b7" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression.</source>
          <target state="translated">베이지안 능선 회귀.</target>
        </trans-unit>
        <trans-unit id="13244381ac86550f492dc53b5005b711b59fb204" translate="yes" xml:space="preserve">
          <source>Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting.</source>
          <target state="translated">출력 배열의 피처 수는 입력 배열의 피처 수에서 기하 급수적으로, 지수 적으로도 비례한다는 점에 유의하십시오. 고도가 높으면 과적 합이 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3faa94ddadd318647d89d2a02f2d45606c753b0d" translate="yes" xml:space="preserve">
          <source>Be invariant to class label: relabelling &lt;code&gt;y = [&quot;Happy&quot;, &quot;Sad&quot;]&lt;/code&gt; to &lt;code&gt;y = [1, 0]&lt;/code&gt; should not change the indices generated.</source>
          <target state="translated">클래스 레이블에 변함이 없습니다. &lt;code&gt;y = [&quot;Happy&quot;, &quot;Sad&quot;]&lt;/code&gt; 를 &lt;code&gt;y = [1, 0]&lt;/code&gt; 레이블링 하면 생성 된 인덱스가 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="3a63e14a349843f9c954f28468802eec39d0181f" translate="yes" xml:space="preserve">
          <source>Be mindful that this function is an order of magnitude slower than other metrics, such as the Adjusted Rand Index.</source>
          <target state="translated">이 기능은 조정 랜드 인덱스와 같은 다른 메트릭보다 훨씬 느리다는 점에 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="83921ebb17d38b8a4b1a1e3f6653eb2aa6327ec9" translate="yes" xml:space="preserve">
          <source>Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.</source>
          <target state="translated">LARS는 잔차에 대한 반복적 인 반복을 기반으로하기 때문에 노이즈의 영향에 특히 민감한 것으로 보입니다. 이 문제는 Efron et al.의 토론 섹션에서 Weisberg에 의해 자세히 논의됩니다. (2004) Annals of Statistics 기사.</target>
        </trans-unit>
        <trans-unit id="5dbb3cd2ef2ab4d2456b8c42be279603ef8045ba" translate="yes" xml:space="preserve">
          <source>Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs)</source>
          <target state="translated">이 방법으로 수행되는 스케일링으로 인해 스케일이 변하지 않는 메소드 (SVM과 같은)와 함께 사용하지 않는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="63be2fc59867472cdd54d70a5100dc5be4d8ddfa" translate="yes" xml:space="preserve">
          <source>Because of the Python object overhead involved in calling the python function, this will be fairly slow, but it will have the same scaling as other distances.</source>
          <target state="translated">python 함수 호출과 관련된 Python 객체 오버 헤드로 인해 속도가 느려지지만 다른 거리와 동일한 배율을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="8e7a8c8acef8e25d8225bca274332d6c56e271be" translate="yes" xml:space="preserve">
          <source>Because the models in each chain are arranged randomly there is significant variation in performance among the chains. Presumably there is an optimal ordering of the classes in a chain that will yield the best performance. However we do not know that ordering a priori. Instead we can construct an voting ensemble of classifier chains by averaging the binary predictions of the chains and apply a threshold of 0.5. The Jaccard similarity score of the ensemble is greater than that of the independent models and tends to exceed the score of each chain in the ensemble (although this is not guaranteed with randomly ordered chains).</source>
          <target state="translated">각 체인의 모델이 무작위로 배열되기 때문에 체인간에 성능에 큰 차이가 있습니다. 아마도 체인에서 클래스가 최적으로 정렬되어 최상의 성능을 얻을 수 있습니다. 그러나 우리는 선험적으로 주문하는 것을 모른다. 대신 체인의 이진 예측을 평균화하여 분류기 체인의 투표 앙상블을 구성하고 임계 값을 0.5로 적용 할 수 있습니다. 앙상블의 Jaccard 유사성 점수는 독립 모델보다 높으며 앙상블의 각 체인의 점수를 초과하는 경향이 있습니다 (임의 순서로 체인이 보장되는 것은 아님).</target>
        </trans-unit>
        <trans-unit id="7223d10fe616d4b0ef82e34ecf5c077bba0a2d4c" translate="yes" xml:space="preserve">
          <source>Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, &lt;code&gt;radius_neighbors&lt;/code&gt; returns arrays of objects, where each object is a 1D array of indices or distances.</source>
          <target state="translated">각 포인트의 이웃 수가 반드시 같을 필요는 없으므로 여러 쿼리 포인트의 결과를 표준 데이터 배열에 맞출 수 없습니다. 효율성을 위해 &lt;code&gt;radius_neighbors&lt;/code&gt; 는 객체의 배열을 반환합니다. 여기서 각 객체는 1D 인덱스 또는 거리의 배열입니다.</target>
        </trans-unit>
        <trans-unit id="5b27b93fd142831db995faecc3827454df4cf37d" translate="yes" xml:space="preserve">
          <source>Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.</source>
          <target state="translated">쿼리 세트가 훈련 세트와 일치하기 때문에 각 포인트의 가장 가까운 이웃은 포인트 자체이며 거리는 0입니다.</target>
        </trans-unit>
        <trans-unit id="40a1ac9f9c5627c60c3da1e0a0050f7b91b8daf1" translate="yes" xml:space="preserve">
          <source>Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2).</source>
          <target state="translated">이 구현에서는 플랫 커널과 볼 트리를 사용하여 각 커널의 멤버를 조회하므로 복잡성이 낮은 차원에서 O (T * n * log (n)), n은 샘플 수, T는 포인트들. 더 높은 차원에서 복잡성은 O (T * n ^ 2)로 향하게됩니다.</target>
        </trans-unit>
        <trans-unit id="5430204387d0c34e554350a9ea0af84d5f7d324c" translate="yes" xml:space="preserve">
          <source>Before we can use Ames dataset we still need to do some preprocessing. First, the dataset has many missing values. To impute them, we will exchange categorical missing values with the new category &amp;lsquo;missing&amp;rsquo; while the numerical missing values with the &amp;lsquo;mean&amp;rsquo; of the column. We will also encode the categories with either &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;&lt;/a&gt; depending for which type of model we will use them (linear or non-linear model). To falicitate this preprocessing we will make two pipelines. You can skip this section if your data is ready to use and does not need preprocessing</source>
          <target state="translated">Ames 데이터 셋을 사용하기 전에 몇 가지 전처리를해야합니다. 첫째, 데이터 세트에 많은 결 측값이 있습니다. 이를 대치하기 위해 범주 형 결 측값을 새 범주 '결 측값'으로 교환하고 숫자 결 측값을 열의 '평균값'으로 교환합니다. 또한 사용할 모델 유형 (선형 또는 비선형 모델)에 따라 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt; 로&lt;/a&gt; 범주를 인코딩합니다 . 이 전처리를 용이하게하기 위해 우리는 두 개의 파이프 라인을 만들 것입니다. 데이터를 사용할 준비가되어 있고 전처리가 필요하지 않은 경우이 섹션을 건너 뛸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f3c17036de4b5ca609e4df67622292c35365a396" translate="yes" xml:space="preserve">
          <source>Behaviour of the &lt;code&gt;decision_function&lt;/code&gt; which can be either &amp;lsquo;old&amp;rsquo; or &amp;lsquo;new&amp;rsquo;. Passing &lt;code&gt;behaviour='new'&lt;/code&gt; makes the &lt;code&gt;decision_function&lt;/code&gt; change to match other anomaly detection algorithm API which will be the default behaviour in the future. As explained in details in the &lt;code&gt;offset_&lt;/code&gt; attribute documentation, the &lt;code&gt;decision_function&lt;/code&gt; becomes dependent on the contamination parameter, in such a way that 0 becomes its natural threshold to detect outliers.</source>
          <target state="translated">'old'또는 'new'일 수있는 &lt;code&gt;decision_function&lt;/code&gt; 의 동작입니다 . &lt;code&gt;behaviour='new'&lt;/code&gt; 를 전달 하면 앞으로 기본 동작이 될 다른 이상 감지 알고리즘 API와 일치 하도록 &lt;code&gt;decision_function&lt;/code&gt; 이 변경됩니다. &lt;code&gt;offset_&lt;/code&gt; 속성 문서 에 자세히 설명 된 것처럼 , &lt;code&gt;decision_function&lt;/code&gt; 은 0이 이상 값을 감지하기위한 자연 임계 값이되는 방식으로 오염 매개 변수에 따라 달라집니다.</target>
        </trans-unit>
        <trans-unit id="20bc631a66b3cc1b1a3392b3fbbb196ee59f89ba" translate="yes" xml:space="preserve">
          <source>Being a forward feature selection method like &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:</source>
          <target state="translated">&lt;a href=&quot;#least-angle-regression&quot;&gt;최소 각도 회귀&lt;/a&gt; 와 같은 정방향 피쳐 선택 방법이기 때문에 직교 정합 추구는 고정 된 개수의 0이 아닌 요소로 최적 솔루션 벡터를 근사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="305fd1e902f73a5b60bc86a4bdd6b4a6ea0e533f" translate="yes" xml:space="preserve">
          <source>Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.</source>
          <target state="translated">다음은 다양한 확률 분포에 적용된 Box-Cox 및 Yeo-Johnson의 예입니다. 특정 분포에 적용될 때, 전력 변환은 매우 가우시안과 유사한 결과를 얻지 만 다른 것에서는 효과가 없습니다. 이는 변환 전후 데이터 시각화의 중요성을 강조합니다.</target>
        </trans-unit>
        <trans-unit id="15f2be220fb72bee910f5e62c59989aad06195d5" translate="yes" xml:space="preserve">
          <source>Below is a summary of the classifiers supported by scikit-learn grouped by strategy; you don&amp;rsquo;t need the meta-estimators in this class if you&amp;rsquo;re using one of these, unless you want custom multiclass behavior:</source>
          <target state="translated">다음은 전략별로 그룹화 된 scikit-learn이 지원하는 분류기의 요약입니다. 사용자 정의 멀티 클래스 동작을 원하지 않는 한이 클래스 중 하나를 사용하는 경우이 클래스에 메타 추정기가 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="108ff87be25a1c1dd2316c9f1fb3ba28787755c8" translate="yes" xml:space="preserve">
          <source>Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file &lt;code&gt;iris.pdf&lt;/code&gt;:</source>
          <target state="translated">아래는 전체 홍채 데이터 세트에 대해 훈련 된 위의 트리의 graphviz 내보내기 예제입니다. 결과는 출력 파일 &lt;code&gt;iris.pdf&lt;/code&gt; 에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="f4264cf8db047b13d2c0262177f6f646451f4fbb" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using Output-Codes:</source>
          <target state="translated">다음은 출력 코드를 사용한 멀티 클래스 학습의 예입니다.</target>
        </trans-unit>
        <trans-unit id="b49bf188ea86adad6efb34f880d43936d0fc0ac2" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvO:</source>
          <target state="translated">다음은 OvO를 사용한 멀티 클래스 학습의 예입니다.</target>
        </trans-unit>
        <trans-unit id="687c4cfff2712863f68152405d4b2638d130c286" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvR:</source>
          <target state="translated">다음은 OvR을 사용한 멀티 클래스 학습의 예입니다.</target>
        </trans-unit>
        <trans-unit id="c80585dafc7b9164da5261a6f83e5756b02054c8" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput classification:</source>
          <target state="translated">다음은 다중 출력 분류의 예입니다.</target>
        </trans-unit>
        <trans-unit id="5323f7dfcb00efe99d53b8ad7c93f56a3771bb8d" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput regression:</source>
          <target state="translated">다음은 다중 출력 회귀의 예입니다.</target>
        </trans-unit>
        <trans-unit id="c1920b9ad7a1724faf2c50a13254e745783c4989" translate="yes" xml:space="preserve">
          <source>Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:</source>
          <target state="translated">아래는 4 가지 기능으로 구성되어 있으며 가장 많은 차이를 설명하는 2 차원으로 투영 된 홍채 데이터 세트의 예입니다.</target>
        </trans-unit>
        <trans-unit id="fe179d558d43e31394bc90d2c2bd912f3d7ad712" translate="yes" xml:space="preserve">
          <source>Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&amp;rsquo;, Wiley, 1980. 244-261.</source>
          <target state="translated">Belsley, Kuh &amp;amp; Welsch, '회귀 진단 : 영향력있는 데이터 및 공선 성의 원천 식별', Wiley, 1980. 244-261.</target>
        </trans-unit>
        <trans-unit id="9f292c5936105126748d157311146c4c77a1cc65" translate="yes" xml:space="preserve">
          <source>Benchmark classifiers</source>
          <target state="translated">벤치 마크 분류기</target>
        </trans-unit>
        <trans-unit id="fe07a59cb91f8e159239a44b010528ab1c647c88" translate="yes" xml:space="preserve">
          <source>Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)</source>
          <target state="translated">Bergstra, J. and Bengio, Y., 하이퍼 파라미터 최적화를위한 랜덤 검색, 저널 오브 머신 러닝 리서치 (2012)</target>
        </trans-unit>
        <trans-unit id="acb0e0e454302529b80959f07b2776ace520ba67" translate="yes" xml:space="preserve">
          <source>Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller. 1999. Kernel principal component analysis. In Advances in kernel methods, MIT Press, Cambridge, MA, USA 327-352.</source>
          <target state="translated">Bernhard Schoelkopf, Alexander J. Smola 및 Klaus-Robert Mueller. 커널 주성분 분석. 커널 방법의 발전에서, MIT Press, 케임브리지, MA, 미국 327-352.</target>
        </trans-unit>
        <trans-unit id="d8b44488165a02f44f4a67e6ba1ce39e5b9c9bb9" translate="yes" xml:space="preserve">
          <source>Bernoulli Restricted Boltzmann Machine (RBM).</source>
          <target state="translated">Bernoulli 제한 볼츠만 기계 (RBM).</target>
        </trans-unit>
        <trans-unit id="5923a4b6b18db19ea579977a7a07a6737359d838" translate="yes" xml:space="preserve">
          <source>Besides scikit-learn, NumPy and SciPy also use BLAS internally, as explained earlier.</source>
          <target state="translated">Scikit-learn 외에도 NumPy 및 SciPy는 앞에서 설명한대로 BLAS를 내부적으로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="7d9a50881bd5d34faa0a8f3ad342e9a7c84f5b08" translate="yes" xml:space="preserve">
          <source>Best fitted model (copy of the &lt;code&gt;base_estimator&lt;/code&gt; object).</source>
          <target state="translated">가장 적합한 모델 ( &lt;code&gt;base_estimator&lt;/code&gt; 객체의 사본 ).</target>
        </trans-unit>
        <trans-unit id="12a49239d2ca4095c10a87ae8e694332610beadd" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="792cb529b422a1c11c9b2f8bb241ed31b32984d9" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0, lower values are worse.</source>
          <target state="translated">최고 점수는 1.0이며 값이 낮을수록 나쁩니다.</target>
        </trans-unit>
        <trans-unit id="28c6b37b189c99aed8957aff1f021f2f9f59464b" translate="yes" xml:space="preserve">
          <source>Beta-divergence loss functions</source>
          <target state="translated">베타 발산 손실 기능</target>
        </trans-unit>
        <trans-unit id="d2c95a0bd9e48b10416c4f3dc812e08319356fa2" translate="yes" xml:space="preserve">
          <source>Beware not to use a regression scoring function with a classification problem, you will get useless results.</source>
          <target state="translated">분류 문제와 함께 회귀 스코어링 기능을 사용하지 않도록주의하면 쓸모없는 결과를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a9f3a13f44f7a1a81c6757a16dde70a0eb1d4b70" translate="yes" xml:space="preserve">
          <source>Bias</source>
          <target state="translated">Bias</target>
        </trans-unit>
        <trans-unit id="ba237b5bed1b1a1e028b039eda8969e7ed30ca0a" translate="yes" xml:space="preserve">
          <source>Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;Bias-variance dilemma&lt;/a&gt;). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.</source>
          <target state="translated">바이어스와 분산은 추정기의 고유 속성이며, 일반적으로 바이어스와 분산이 최대한 낮아 지도록 학습 알고리즘과 하이퍼 파라미터를 선택해야합니다 ( &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;바이어스 분산 딜레마&lt;/a&gt; 참조 ). 모델의 분산을 줄이는 또 다른 방법은 더 많은 훈련 데이터를 사용하는 것입니다. 그러나 실제 함수가 너무 복잡하여 분산이 낮은 추정량에 의해 추정하기에는 더 많은 훈련 데이터 만 수집해야합니다.</target>
        </trans-unit>
        <trans-unit id="06f19409a98653c678fe6ff3c120895b283ca5ff" translate="yes" xml:space="preserve">
          <source>Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators</source>
          <target state="translated">수축률 설정시 편차 편차-Ledoit-Wolf 및 OAS 추정기의 선택 비교</target>
        </trans-unit>
        <trans-unit id="7c5f1fb15e060b340fb270b747d162cde9961929" translate="yes" xml:space="preserve">
          <source>Bias.</source>
          <target state="translated">Bias.</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="6b339e821e48cfc38068b641fe05c82a58e3e342" translate="yes" xml:space="preserve">
          <source>Biases of the hidden units.</source>
          <target state="translated">숨겨진 유닛의 Biases.</target>
        </trans-unit>
        <trans-unit id="e50dfa9d24499c67dd80dc7ae0f62d209963644d" translate="yes" xml:space="preserve">
          <source>Biases of the visible units.</source>
          <target state="translated">보이는 단위의 Biases.</target>
        </trans-unit>
        <trans-unit id="d4404195d10795d2fac0ea59f4bd8efbaf381e98" translate="yes" xml:space="preserve">
          <source>Biclustering</source>
          <target state="translated">Biclustering</target>
        </trans-unit>
        <trans-unit id="a252b47dd9e6fba33ce507e3a8e6ec1bb1e9c7d0" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt;&lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;&lt;/a&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt; &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt; &lt;/a&gt; 모듈로 바이러스 터링을 수행 할 수 있습니다 . Biclustering 알고리즘은 데이터 행렬의 행과 열을 동시에 클러스터링합니다. 이 행과 열의 클러스터를 바이 클러스터라고합니다. 각각 원하는 속성을 가진 원본 데이터 매트릭스의 하위 행렬을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="edf2774b82282aef622b14eeaa9c74d15c4f211e" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="translated">&lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt; 모듈을 사용하여 이중 클러스터링을 수행 할 수 있습니다 . 이중 클러스터링 알고리즘은 데이터 행렬의 행과 열을 동시에 클러스터링합니다. 이러한 행과 열의 군집을 이중 군집이라고합니다. 각각은 원하는 속성을 가진 원본 데이터 행렬의 부분 행렬을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="ec46a574c9badafd039dd72d03169fe1273b5603" translate="yes" xml:space="preserve">
          <source>Biclustering documents with the Spectral Co-clustering algorithm</source>
          <target state="translated">스펙트럼 공동 클러스터링 알고리즘을 사용한 문서 클러스터링</target>
        </trans-unit>
        <trans-unit id="ee86c7bb4d236ce105b92fe63ada05dd4ec5870a" translate="yes" xml:space="preserve">
          <source>Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral Co-Clustering algorithm, reflect these alternate names.</source>
          <target state="translated">Biclustering은 공동 클러스터링, 2- 모드 클러스터링, 양방향 클러스터링, 블록 클러스터링, 결합 된 양방향 클러스터링 등을 포함하여 다른 분야에서 다른 많은 이름을 가지고 있습니다. Spectral Co-Clustering 알고리즘과 같은 일부 알고리즘의 이름은 다음을 반영합니다. 이 대체 이름.</target>
        </trans-unit>
        <trans-unit id="9dbdcbb1f58605adc858ff7ea9bd66e335bd0acc" translate="yes" xml:space="preserve">
          <source>Biclustering metrics</source>
          <target state="translated">바이러스 터링 지표</target>
        </trans-unit>
        <trans-unit id="d9cf1c1fb4521641a79265f68d061558d2a8f977" translate="yes" xml:space="preserve">
          <source>Bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">클수록 클수록 좋습니다. 즉 큰 값은 이너에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="8a391f29f990251d22520f34bdb4ac600bbfd771" translate="yes" xml:space="preserve">
          <source>Bin continuous data into intervals.</source>
          <target state="translated">연속 데이터를 간격으로 묶습니다.</target>
        </trans-unit>
        <trans-unit id="271e5dda8b524cc22b71bebf5401052359debca7" translate="yes" xml:space="preserve">
          <source>Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.</source>
          <target state="translated">이진화는 텍스트 개수 데이터에 대한 일반적인 작업으로, 분석가가 예를 들어 수량화 된 발생 횟수보다는 기능의 존재 여부 만 고려하기로 결정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="98a952cd8923e662b6dc4cb343f8e07b8c748673" translate="yes" xml:space="preserve">
          <source>Binarize data (set feature values to 0 or 1) according to a threshold</source>
          <target state="translated">임계 값에 따라 데이터를 이진화 (기능 값을 0 또는 1로 설정)</target>
        </trans-unit>
        <trans-unit id="e6093920f16b7a0a90021c0a6f1b3ba3a339b537" translate="yes" xml:space="preserve">
          <source>Binarize each element of X</source>
          <target state="translated">X의 각 요소를 이진화</target>
        </trans-unit>
        <trans-unit id="0b5b7e155ae5a5495c55eaab6001751a5f50a1e4" translate="yes" xml:space="preserve">
          <source>Binarize labels in a one-vs-all fashion</source>
          <target state="translated">일대일 방식으로 레이블을 이진화</target>
        </trans-unit>
        <trans-unit id="44b97fdbb0f9ad559fc3a3794b99c9bc30004c1c" translate="yes" xml:space="preserve">
          <source>Binarizes labels in a one-vs-all fashion.</source>
          <target state="translated">일대 다 방식으로 레이블을 이진화합니다.</target>
        </trans-unit>
        <trans-unit id="2c6db33026290c5eceb25ad459dd62fbc5de5524" translate="yes" xml:space="preserve">
          <source>Binary and multiclass labels are supported. Only in the binary case does this relate to information about true and false positives and negatives. See references below.</source>
          <target state="translated">이진 및 멀티 클래스 레이블이 지원됩니다. 이진 경우에만 참 및 거짓 긍정 및 부정에 대한 정보와 관련이 있습니다. 아래 참조를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="76f6ce139477b2ff40abfa10ccd09f75138f8a5a" translate="yes" xml:space="preserve">
          <source>Binary array containing the code of each class.</source>
          <target state="translated">각 클래스의 코드를 포함하는 이진 배열.</target>
        </trans-unit>
        <trans-unit id="d5752168a3a19813275e09ded6a3644b6b365056" translate="yes" xml:space="preserve">
          <source>Binary indicators for missing values.</source>
          <target state="translated">결 측값에 대한 이진 표시기.</target>
        </trans-unit>
        <trans-unit id="d20fae796db569b25a51c85d3dbd034384870434" translate="yes" xml:space="preserve">
          <source>Binary probability estimates for loss=&amp;rdquo;modified_huber&amp;rdquo; are given by (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions it is necessary to perform proper probability calibration by wrapping the classifier with &lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">loss =&amp;rdquo;modified_huber&amp;rdquo;에 대한 이진 확률 추정치는 (clip (decision_function (X), -1, 1) + 1) / 2로 제공됩니다. 다른 손실 함수의 경우 분류기를 &lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt; 으로 감싸서 적절한 확률 보정을 수행해야합니다 . 교정 . 교정 된 분류기 CV.</target>
        </trans-unit>
        <trans-unit id="6ea0caebd3956174ca222001571ec7f4a771808d" translate="yes" xml:space="preserve">
          <source>Binary target values.</source>
          <target state="translated">이진 목표 값.</target>
        </trans-unit>
        <trans-unit id="e17841c821731bb47f4b19f4849e509ece4c5657" translate="yes" xml:space="preserve">
          <source>Binary targets transform to a column vector</source>
          <target state="translated">이진 대상은 열 벡터로 변환</target>
        </trans-unit>
        <trans-unit id="2a2767f039bc988b0f4c4e7163ccdacd052867c1" translate="yes" xml:space="preserve">
          <source>Binding of the cross-validation routine (low-level routine)</source>
          <target state="translated">교차 유효성 검사 루틴 바인딩 (낮은 수준의 루틴)</target>
        </trans-unit>
        <trans-unit id="64fb71b36aba4bcefad2e5d527659e300a2b2c01" translate="yes" xml:space="preserve">
          <source>Binomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio.</source>
          <target state="translated">이항 이탈 ( &lt;code&gt;'deviance'&lt;/code&gt; ) : 이진 분류에 대한 음의 이항 로그 우도 손실 함수 (확률 추정 제공). 초기 모델은 로그 승산 비에 의해 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="7eef6382001e9a152cc75ac79b202341870bb6db" translate="yes" xml:space="preserve">
          <source>Birch</source>
          <target state="translated">Birch</target>
        </trans-unit>
        <trans-unit id="c8804a672e163b7d85424df4080099668af078fd" translate="yes" xml:space="preserve">
          <source>Birch does not scale very well to high dimensional data. As a rule of thumb if &lt;code&gt;n_features&lt;/code&gt; is greater than twenty, it is generally better to use MiniBatchKMeans.</source>
          <target state="translated">자작 나무는 고차원 데이터에 적합하지 않습니다. 경우 엄지 손가락의 규칙으로 &lt;code&gt;n_features&lt;/code&gt; 는 스물보다 큰 경우, 일반적으로 잘 사용 MiniBatchKMeans이다.</target>
        </trans-unit>
        <trans-unit id="055a71a716f70327eef9729a95cd9e98a091a68c" translate="yes" xml:space="preserve">
          <source>Bishop, &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern recognition and machine learning&lt;/a&gt;, chapter 7 Sparse Kernel Machines</source>
          <target state="translated">Bishop, &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;패턴 인식 및 머신 러닝&lt;/a&gt; , 7 장 스파 스 커널 머신</target>
        </trans-unit>
        <trans-unit id="35b99b7929918827477b2be5f13db5f62d5bed94" translate="yes" xml:space="preserve">
          <source>Bishop, Christopher M. (2006). &amp;ldquo;Pattern recognition and machine learning&amp;rdquo;. Vol. 4 No. 4. New York: Springer.</source>
          <target state="translated">크리스토퍼 엠 감독 (2006). &amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;. Vol. 4. 뉴욕 : 스프링거.</target>
        </trans-unit>
        <trans-unit id="3ee00b8c0e8ff24c1a59dc452e853cf83ab3d2bf" translate="yes" xml:space="preserve">
          <source>Blei, David M. and Michael I. Jordan. (2006). &amp;ldquo;Variational inference for Dirichlet process mixtures&amp;rdquo;. Bayesian analysis 1.1</source>
          <target state="translated">Blei, David M. 및 Michael I. Jordan. (2006). &amp;ldquo;Drichlet 공정 혼합물에 대한 다양한 추론&amp;rdquo;. 베이지안 분석 1.1</target>
        </trans-unit>
        <trans-unit id="66370792731dff7f31408706eed1fd3ef5297d11" translate="yes" xml:space="preserve">
          <source>Blind source separation using FastICA</source>
          <target state="translated">FastICA를 사용한 맹인 소스 분리</target>
        </trans-unit>
        <trans-unit id="7d44bc449c2a26374800a503f10f3d8949505f40" translate="yes" xml:space="preserve">
          <source>Blue</source>
          <target state="translated">Blue</target>
        </trans-unit>
        <trans-unit id="d1411ae3cdd27fda5ea57577c408be223f586cf0" translate="yes" xml:space="preserve">
          <source>Body mass index</source>
          <target state="translated">체질량 지수</target>
        </trans-unit>
        <trans-unit id="eeb4978eef8f0138e90d13575223e62053c06fa3" translate="yes" xml:space="preserve">
          <source>Bonus point if the utility is able to give a confidence level for its predictions.</source>
          <target state="translated">유틸리티가 예측에 대한 신뢰 수준을 제공 할 수있는 경우 보너스 포인트.</target>
        </trans-unit>
        <trans-unit id="20ee87c5c904919ec390ea5b4c0a66ba0775ae58" translate="yes" xml:space="preserve">
          <source>BonusMalus</source>
          <target state="translated">BonusMalus</target>
        </trans-unit>
        <trans-unit id="3605ba73833c24e0fbda3c252c7bb0e601382c92" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating wether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 의 출력을 나타내는 부울 플래그 는 희소 행렬 또는 조밀 한 numpy 배열이며 개별 변환기의 출력과 &lt;code&gt;sparse_threshold&lt;/code&gt; 키워드에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="c9d24ce1f33e1a2da8b7d2e8d02488db3a1a0ff6" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating whether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 의 출력이 희소 행렬인지 조밀 한 numpy 배열 인지를 나타내는 부울 플래그입니다. 이는 개별 변환기의 출력과 &lt;code&gt;sparse_threshold&lt;/code&gt; 키워드에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="dd883a95149825523369daab7f0f8ca0aca6e0ac" translate="yes" xml:space="preserve">
          <source>Boolean mask of inliers classified as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 분류 된 부울 마스크입니다 .</target>
        </trans-unit>
        <trans-unit id="d2ef2625261226f9417113c581f3bab51f99fc11" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for test set.</source>
          <target state="translated">테스트 세트에 대한 부울 마스크 또는 인덱스</target>
        </trans-unit>
        <trans-unit id="30de559b973b97451b964227c7184d452eca2270" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for training set.</source>
          <target state="translated">훈련 세트에 대한 부울 마스크 또는 인덱스.</target>
        </trans-unit>
        <trans-unit id="a38162711f2499e7a5519304c06ab030205ccd3e" translate="yes" xml:space="preserve">
          <source>Boolean mask or list of indices (as returned by the get_support member of feature selectors).</source>
          <target state="translated">부울 마스크 또는 인덱스 목록 (기능 선택기의 get_support 멤버가 리턴 한대로).</target>
        </trans-unit>
        <trans-unit id="c3ff5ac4f1442b62c49b82219fbca08a9aaf6fc5" translate="yes" xml:space="preserve">
          <source>Boolean thresholding of array-like or scipy.sparse matrix</source>
          <target state="translated">배열 형 또는 scipy.sparse 행렬의 부울 임계 값</target>
        </trans-unit>
        <trans-unit id="52747050c0ad2a1de070c473421346e630cdac3f" translate="yes" xml:space="preserve">
          <source>Both &amp;lsquo;ascii&amp;rsquo; and &amp;lsquo;unicode&amp;rsquo; use NFKD normalization from &lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt;&lt;code&gt;unicodedata.normalize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">'ascii'및 'unicode'모두 &lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt; &lt;code&gt;unicodedata.normalize&lt;/code&gt; 의&lt;/a&gt; NFKD 정규화를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="cc08d18c56138ef4e57769f94344c2fcddc0c71c" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; create multiclass datasets by allocating each class one or more normally-distributed clusters of points. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; 은&lt;/a&gt; 모두 각 클래스에 하나 이상의 정규 분포 포인트 포인트를 할당하여 멀티 클래스 데이터 세트를 만듭니다. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt; 는 각 군집의 중심 및 표준 편차에 대한 제어 기능을 강화하고 군집화를 설명하는 데 사용됩니다. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt; 은 다음과 같은 방법으로 노이즈를 도입하는 것을 전문으로합니다. 클래스 당 여러 가우시안 클러스터; 그리고 특징 공간의 선형 변형.</target>
        </trans-unit>
        <trans-unit id="4b01e39314cd0b6e82319a49bae49952bbdf5790" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; support &lt;code&gt;warm_start=True&lt;/code&gt; which allows you to add more estimators to an already fitted model.</source>
          <target state="translated">두 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 지원 &lt;code&gt;warm_start=True&lt;/code&gt; 당신이 이미 장착 모델에 더 많은 추정량을 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0fdd261ec2b6eab839fe6617683b34303696fb85" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; can be used in a Pipeline as a way to build a composite estimator that supports imputation. See &lt;a href=&quot;../auto_examples/impute/plot_missing_values#sphx-glr-auto-examples-impute-plot-missing-values-py&quot;&gt;Imputing missing values before building an estimator&lt;/a&gt;.</source>
          <target state="translated">두 &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;IterativeImputer&lt;/code&gt; 는&lt;/a&gt; 지원의 전가하는 복합 추정을 구축하는 방법으로 파이프 라인에서 사용할 수 있습니다. &lt;a href=&quot;../auto_examples/impute/plot_missing_values#sphx-glr-auto-examples-impute-plot-missing-values-py&quot;&gt;추정기를 작성하기 전에 결 측값 대치를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="cf6c6c4310ce5d3e1dff50dd933fcb1d3d517f90" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt;&lt;code&gt;MLPClassifier&lt;/code&gt;&lt;/a&gt; use parameter &lt;code&gt;alpha&lt;/code&gt; for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt; &lt;code&gt;MLPRegressor&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt; &lt;code&gt;MLPClassifier&lt;/code&gt; &lt;/a&gt; 는 모두 정규화 (L2 정규화) 항에 매개 변수 &lt;code&gt;alpha&lt;/code&gt; 를 사용 하여 가중치를 크게하여 과적 합을 피할 수 있습니다. 다음 그림은 알파 값을 가진 다양한 결정 기능을 표시합니다.</target>
        </trans-unit>
        <trans-unit id="b53b3117e47a61020f59f56f0ab84443cf5a1cc4" translate="yes" xml:space="preserve">
          <source>Both &lt;strong&gt;tf&lt;/strong&gt; and &lt;strong&gt;tf&amp;ndash;idf&lt;/strong&gt; can be computed as follows using &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">두 &lt;strong&gt;TF&lt;/strong&gt; 와 &lt;strong&gt;TF는-IDF는&lt;/strong&gt; 사용하여 다음과 같이 계산 될 수있다 &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; 를&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="4bf93c5f6114d3860df00831377e70ae0b54b0c9" translate="yes" xml:space="preserve">
          <source>Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.</source>
          <target state="translated">얼굴 검증 및 얼굴 인식은 일반적으로 얼굴 검출을 수행하도록 훈련 된 모델의 출력에서 ​​수행되는 작업입니다. 얼굴 인식에 가장 많이 사용되는 모델은 Viola-Jones라고하며 OpenCV 라이브러리에서 구현됩니다. LFW 얼굴은 다양한 온라인 웹 사이트에서이 얼굴 탐지기로 추출되었습니다.</target>
        </trans-unit>
        <trans-unit id="ca7325084a8f64bcbbcfe176b54a08f8e59fb609" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule, for each training sample \(x \in \mathcal{R}^d\):</source>
          <target state="translated">LDA와 QDA는 모두 각 클래스 \ (k \)에 대한 데이터 \ (P (X | y = k) \)의 클래스 조건 분포를 모델링하는 단순 확률 모델에서 파생 될 수 있습니다. 그런 다음 각 훈련 샘플 \ (x \ in \ mathcal {R} ^ d \)에 대해 Bayes의 규칙을 사용하여 예측을 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="91e41139595583cca1f08b85e6f25218cb1d806a" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule:</source>
          <target state="translated">LDA와 QDA는 각각의 클래스 \ (k \)에 대한 데이터 \ (P (X | y = k) \)의 클래스 조건부 분포를 모델링하는 간단한 확률 모델로부터 도출 될 수 있습니다. 그런 다음 Bayes의 규칙을 사용하여 예측을 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="88ef4403ed65b61f6ce7d70445c003a80e194980" translate="yes" xml:space="preserve">
          <source>Both a large or small &lt;code&gt;leaf_size&lt;/code&gt; can lead to suboptimal query cost. For &lt;code&gt;leaf_size&lt;/code&gt; approaching 1, the overhead involved in traversing nodes can significantly slow query times. For &lt;code&gt;leaf_size&lt;/code&gt; approaching the size of the training set, queries become essentially brute force. A good compromise between these is &lt;code&gt;leaf_size = 30&lt;/code&gt;, the default value of the parameter.</source>
          <target state="translated">크거나 작은 &lt;code&gt;leaf_size&lt;/code&gt; 는 모두 최적의 쿼리 비용으로 이어질 수 있습니다. 들어 &lt;code&gt;leaf_size&lt;/code&gt; 가 1에 접근의 오버 헤드 수 상당히 느린 쿼리 시간 노드를 횡단에 참여. 들어 &lt;code&gt;leaf_size&lt;/code&gt; 훈련 집합의 크기에 접근, 쿼리는 본질적으로 무력된다. 이들 사이의 좋은 타협 은 매개 변수의 기본값 인 &lt;code&gt;leaf_size = 30&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="fa5698e1f194999c5b948025698f212260de6074" translate="yes" xml:space="preserve">
          <source>Both for the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; function it is possible to get an additional dimension with the RGB color channels by passing &lt;code&gt;color=True&lt;/code&gt;, in that case the shape will be &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; &lt;/a&gt; 함수 모두 &lt;code&gt;color=True&lt;/code&gt; 를 전달하여 RGB 색상 채널로 추가 차원을 얻을 수 있습니다. 이 경우 모양은 &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f849c057a89abeca98a91c38c141c4422d4b9044" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">KRR (kernel ridge regression)과 GPR은 내부적으로 &quot;커널 트릭&quot;을 사용하여 대상 기능을 학습합니다. KRR은 원래 공간에서 비선형 함수에 해당하는 각 커널에 의해 유도 된 공간에서 선형 함수를 학습합니다. 커널 공간의 선형 함수는 릿지 정규화에 따른 평균 제곱 오차 손실을 기반으로 선택됩니다. GPR은 커널을 사용하여 대상 함수에 대한 사전 분포의 공분산을 정의하고 관측 된 훈련 데이터를 사용하여 우도 함수를 정의합니다. 베이 즈 정리에 기초하여, 목표 함수들에 대한 (가우시안) 후방 분포가 정의되며, 그 평균은 예측에 사용된다.</target>
        </trans-unit>
        <trans-unit id="23db30f840724e288d4f8f1371ac596206d41d32" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and Gaussian process regression (GPR) learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">KRR (kernel ridge regression)과 GPR (Gussian process regression)은 내부적으로 &quot;커널 트릭&quot;을 사용하여 대상 기능을 학습합니다. KRR은 원래 공간에서 비선형 함수에 해당하는 각 커널에 의해 유도 된 공간에서 선형 함수를 학습합니다. 커널 공간의 선형 함수는 릿지 정규화에 따른 평균 제곱 오차 손실을 기반으로 선택됩니다. GPR은 커널을 사용하여 대상 함수에 대한 사전 분포의 공분산을 정의하고 관측 된 훈련 데이터를 사용하여 우도 함수를 정의합니다. 베이 즈 정리에 기초하여, 목표 함수들에 대한 (가우시안) 후방 분포가 정의되며, 그 평균은 예측에 사용된다.</target>
        </trans-unit>
        <trans-unit id="a2ae11bd288fcc4a889903f5cd55e2c7dc5062c9" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR at prediction-time.</source>
          <target state="translated">KRR (kernel ridge regression)과 SVR은 커널 트릭을 사용하여 비선형 함수를 학습합니다. 즉, 원래 공간의 비선형 함수에 해당하는 각 커널에 의해 유도 된 공간에서 선형 함수를 학습합니다. 손실 기능이 다릅니다 (리지 대 엡실론에 둔감 한 손실). SVR과 달리 KRR 피팅은 닫힌 형식으로 수행 할 수 있으며 일반적으로 중간 규모의 데이터 집합에 더 빠릅니다. 반면에, 학습 된 모델은 희소하지 않으므로 예측시 SVR보다 느립니다.</target>
        </trans-unit>
        <trans-unit id="9f6802fa6da263e373ed1a0b728e154415b2fd58" translate="yes" xml:space="preserve">
          <source>Both kinds of calibration can fix this issue and yield nearly identical results. This shows that sigmoid calibration can deal with situations where the calibration curve of the base classifier is sigmoid (e.g., for LinearSVC) but not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).</source>
          <target state="translated">두 종류의 교정 모두이 문제를 해결하고 거의 동일한 결과를 얻을 수 있습니다. 이것은 시그 모이 드 캘리브레이션이베이스 분류기의 캘리브레이션 곡선이 시그 모이 드 (예를 들어, LinearSVC의 경우)이지만 시그 모이 드가 트랜스 포지션 된 시그 모이 드 (예 : 가우시안 순진 베이 즈)가 아닌 상황을 처리 할 수 ​​있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="708135c11a370a819f586687e493e8e334b863bb" translate="yes" xml:space="preserve">
          <source>Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.</source>
          <target state="translated">두 선형 모델 모두 선형 결정 경계 (교차 하이퍼 플레인)를 가지고있는 반면 비선형 커널 모델 (다항식 또는 가우스 RBF)은 커널의 종류와 매개 변수에 따라 모양이 더 유연한 비선형 결정 경계를 갖습니다.</target>
        </trans-unit>
        <trans-unit id="26e0a0e806824f91854b61cb523786b2b1be2f09" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt; object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="translated">로더와 페처 함수는 둘 이상의 항목을 포함 하는 &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt; &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; &lt;/a&gt; 객체를 반환 합니다. 하나 는 키 &lt;code&gt;data&lt;/code&gt; &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; 형태의 배열 (20 개의 뉴스 그룹 제외)과 대상 값을 포함하는 &lt;code&gt;n_samples&lt;/code&gt; 길이의 numpy 배열 과 키 &lt;code&gt;target&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc56722e0a950bb51331dfd1a0000b2f92f9cd59" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="translated">로더 및 페처 함수는 둘 이상의 항목을 보유하는 사전과 유사한 오브젝트를 리턴합니다. &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; 배열의 키 &lt;code&gt;data&lt;/code&gt; (20newsgroups 제외) 및 &lt;code&gt;n_samples&lt;/code&gt; 길이의 배열 (null) 은 대상 값을 포함하고 키 &lt;code&gt;target&lt;/code&gt; 과 함께 값을 포함합니다 .</target>
        </trans-unit>
        <trans-unit id="c429ee44d91d629f81b84bf7db1e8151de09efd7" translate="yes" xml:space="preserve">
          <source>Both methods are compared in a regression problem using a BayesianRidge as supervised estimator.</source>
          <target state="translated">두 방법 모두 BayesianRidge를 감독 된 추정기로 사용하여 회귀 문제에서 비교됩니다.</target>
        </trans-unit>
        <trans-unit id="1f719d18cf26f246fcd8f594b634dc25f50e93c4" translate="yes" xml:space="preserve">
          <source>Both models are able to rank policyholders by risky-ness significantly better than chance although they are also both far from perfect due to the natural difficulty of the prediction problem from few features.</source>
          <target state="translated">두 모델 모두 몇 가지 기능으로 인한 예측 문제의 자연스러운 어려움으로 인해 완벽하지는 않지만 우연보다 훨씬 더 위험성에 따라 보험 계약자를 평가할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c567880fbb130b2f00326a7eed156cd91692677" translate="yes" xml:space="preserve">
          <source>Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;FastICA&lt;/code&gt;&lt;/a&gt;) if non-Gaussian priors on the latent variables are assumed.</source>
          <target state="translated">두 모델 모두 본질적으로 낮은 공분산 행렬을 가진 가우시안을 추정합니다. 두 모델 모두 확률론 적이므로 더 복잡한 모델 (예 : 혼합 요인 분석 도구)에 통합 될 수 있습니다. 잠재 변수에 대한 비 가우시안 사전이 가정 되면 매우 다른 모델 (예 : &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;FastICA&lt;/code&gt; &lt;/a&gt; )을 얻 습니다.</target>
        </trans-unit>
        <trans-unit id="a62e9a7de64eaa33971b5c3675cc0b062a65784c" translate="yes" xml:space="preserve">
          <source>Both models have access to five components with which to fit the data. Note that the Expectation Maximisation model will necessarily use all five components while the Variational Inference model will effectively only use as many as are needed for a good fit. Here we can see that the Expectation Maximisation model splits some components arbitrarily, because it is trying to fit too many components, while the Dirichlet Process model adapts it number of state automatically.</source>
          <target state="translated">두 모델 모두 데이터에 맞는 5 가지 구성 요소에 액세스 할 수 있습니다. 기대 최대화 모델은 반드시 5 가지 구성 요소를 모두 사용해야하는 반면, 변형 추론 모델은 적절한 적합에 필요한만큼만 효과적으로 사용합니다. 여기에서는 기대 최대화 모델이 너무 많은 구성 요소를 맞추려고하기 때문에 일부 구성 요소를 임의로 분할하는 것을 알 수 있으며, Dirichlet Process 모델은 자동으로 여러 개의 상태를 조정합니다.</target>
        </trans-unit>
        <trans-unit id="5b17a14614025b8fc55b61f491f7a477ebf75742" translate="yes" xml:space="preserve">
          <source>Both scores have positive values between 0.0 and 1.0, larger values being desirable.</source>
          <target state="translated">두 점수 모두 0.0과 1.0 사이의 양수 값을 가지며 더 큰 값이 바람직합니다.</target>
        </trans-unit>
        <trans-unit id="be67d7e7cbf7e3b7b92ab971146cc493ee899201" translate="yes" xml:space="preserve">
          <source>Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \(\lambda\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution:</source>
          <target state="translated">Box-Cox는 엄격히 긍정적 인 데이터에만 적용 할 수 있습니다. 두 방법 모두 변환은 최대 가능성 추정을 통해 결정되는 \ (\ lambda \)로 매개 변수화됩니다. 다음은 Box-Cox를 사용하여 로그 정규 분포에서 추출한 표본을 정규 분포에 매핑하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="3b31bca12137b60e3b02c5fa6fcbe997bf063055" translate="yes" xml:space="preserve">
          <source>Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</source>
          <target state="translated">Box-Cox는 입력 데이터가 반드시 양수 여야하지만 Yeo-Johnson은 양수 또는 음수 데이터를 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="3a33122410e3d55ce36b5d7a297f38716ddc9fb2" translate="yes" xml:space="preserve">
          <source>BrayCurtisDistance</source>
          <target state="translated">BrayCurtisDistance</target>
        </trans-unit>
        <trans-unit id="9567cb56bbf5a464a471c7e7c0603402f701c1b5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Arcing Classifiers&amp;rdquo;, Annals of Statistics 1998.</source>
          <target state="translated">Breiman,&amp;ldquo;레이싱 분류기&amp;rdquo;, Annals of Statistics 1998.</target>
        </trans-unit>
        <trans-unit id="b92ea166bdb68582b38ba5d28898e88166331fb5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001.</source>
          <target state="translated">Breiman,&amp;ldquo;랜덤 포레스트&amp;rdquo;, 기계 학습, 45 (1), 5-32, 2001.</target>
        </trans-unit>
        <trans-unit id="e706a24019de3d6255ea9f7b340895835c77d481" translate="yes" xml:space="preserve">
          <source>Brendan J. Frey and Delbert Dueck, &amp;ldquo;Clustering by Passing Messages Between Data Points&amp;rdquo;, Science Feb. 2007</source>
          <target state="translated">Brendan J. Frey와 Delbert Dueck,&amp;ldquo;데이터 포인트간에 메시지를 전달하여 클러스터링&amp;rdquo;, 2007 년 2 월 과학</target>
        </trans-unit>
        <trans-unit id="91c2780c7ce208d81dbc70c79b98b19c560e01f7" translate="yes" xml:space="preserve">
          <source>Breunig, Kriegel, Ng, and Sander (2000) &lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF: identifying density-based local outliers.&lt;/a&gt; Proc. ACM SIGMOD</source>
          <target state="translated">Breunig, Kriegel, Ng 및 Sander (2000) &lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF : 밀도 기반 지역 특이점 식별. &lt;/a&gt;Proc. ACM SIGMOD</target>
        </trans-unit>
        <trans-unit id="b62edbfb07bbbc25ba8cf8bbc25ed044eba7bbe5" translate="yes" xml:space="preserve">
          <source>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp;amp; Sander, J. (2000, May). LOF: identifying density-based local outliers. In ACM sigmod record.</source>
          <target state="translated">Breunig, MM, Kriegel, HP, Ng, RT, &amp;amp; Sander, J. (2000 년 5 월). LOF : 밀도 기반 로컬 특이 치를 식별합니다. ACM sigmod 레코드에서.</target>
        </trans-unit>
        <trans-unit id="f5333384a370e2b255c6a3cf2b193f53e9cafb20" translate="yes" xml:space="preserve">
          <source>Briefly, a first-order Taylor approximation says that \(l(z) \approx l(a) + (z - a) \frac{\partial l(a)}{\partial a}\). Here, \(z\) corresponds to \(F_{m - 1}(x_i) + h_m(x_i)\), and \(a\) corresponds to \(F_{m-1}(x_i)\)</source>
          <target state="translated">간단히 말해서, 1 차 Taylor 근사는 \ (l (z) \ approx l (a) + (z-a) \ frac {\ partial l (a)} {\ partial a} \)라고 말합니다. 여기서 \ (z \)는 \ (F_ {m-1} (x_i) + h_m (x_i) \)에 해당하고 \ (a \)는 \ (F_ {m-1} (x_i) \)에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="fb0165005c94e23be947347cfac674726ddcc8f3" translate="yes" xml:space="preserve">
          <source>Brier score</source>
          <target state="translated">브리 어 점수</target>
        </trans-unit>
        <trans-unit id="ce9563b2203c4e120af29d083264cdec661286f1" translate="yes" xml:space="preserve">
          <source>Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010). The balanced accuracy and its posterior distribution. Proceedings of the 20th International Conference on Pattern Recognition, 3121-24.</source>
          <target state="translated">Brodersen, KH; 옹, CS; 스테판, KE; JM 부만 (2010). 균형 잡힌 정확성과 사후 분포. 패턴 인식에 관한 제 20 차 국제 회의 절차, 3121-24.</target>
        </trans-unit>
        <trans-unit id="3a47350b7d7cb691a01ccff5ad5d3aca87290c28" translate="yes" xml:space="preserve">
          <source>Brown</source>
          <target state="translated">Brown</target>
        </trans-unit>
        <trans-unit id="5b66173631e06f3f28d6125b3cb26a28cb7fca86" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training</source>
          <target state="translated">훈련을 통해 배깅 평가자 앙상블을 구축하십시오</target>
        </trans-unit>
        <trans-unit id="794678e50f47205b9017d4e509f3518d6f5fadf5" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training set (X, y).</source>
          <target state="translated">트레이닝 세트 (X, y)에서 추정의 배깅 앙상블을 빌드하십시오.</target>
        </trans-unit>
        <trans-unit id="187f33f95926319c2456d01efd577acf94dfa6ae" translate="yes" xml:space="preserve">
          <source>Build a CF Tree for the input data.</source>
          <target state="translated">입력 데이터에 대한 CF 트리를 빌드하십시오.</target>
        </trans-unit>
        <trans-unit id="1367324c6e1505253779ffa63a9ab3c72dfbf04f" translate="yes" xml:space="preserve">
          <source>Build a HTML representation of an estimator.</source>
          <target state="translated">추정기의 HTML 표현을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="1e09fb3267f6b0d5980c9be2a9dffb892209f693" translate="yes" xml:space="preserve">
          <source>Build a boosted classifier from the training set (X, y).</source>
          <target state="translated">훈련 세트 (X, y)에서 강화 된 분류기를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="bb1580cf6761f3cb6ad38eda4b239ef5f0001320" translate="yes" xml:space="preserve">
          <source>Build a boosted regressor from the training set (X, y).</source>
          <target state="translated">훈련 세트 (X, y)에서 강화 회귀자를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="250228e6d05087ab14d0d947456ba0aa59ea7050" translate="yes" xml:space="preserve">
          <source>Build a contingency matrix describing the relationship between labels.</source>
          <target state="translated">레이블 간의 관계를 설명하는 우발성 매트릭스를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="d74890ceefdff850d4364b5427ecc2c4535dee78" translate="yes" xml:space="preserve">
          <source>Build a decision tree classifier from the training set (X, y).</source>
          <target state="translated">훈련 세트 (X, y)에서 의사 결정 트리 분류기를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="2b1b91e53bab7f86a619b51147d8ac61611972f5" translate="yes" xml:space="preserve">
          <source>Build a decision tree regressor from the training set (X, y).</source>
          <target state="translated">훈련 세트 (X, y)에서 의사 결정 트리 회귀자를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="255fad2483c531d4d8beb88f94f03694ee2b25aa" translate="yes" xml:space="preserve">
          <source>Build a forest of trees from the training set (X, y).</source>
          <target state="translated">훈련 세트 (X, y)에서 나무 숲을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="54ca7aba833ca6776a4b555adf51f52c37364032" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics</source>
          <target state="translated">주요 분류 지표를 보여주는 텍스트 보고서 작성</target>
        </trans-unit>
        <trans-unit id="0acb360cebaf298906fb4db3eabfa00d9453c9d3" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics.</source>
          <target state="translated">주요 분류 지표를 보여주는 텍스트 보고서를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="02e35795301c2dfa78447e621bba28f5e7d78fd9" translate="yes" xml:space="preserve">
          <source>Build a text report showing the rules of a decision tree.</source>
          <target state="translated">의사 결정 트리의 규칙을 보여주는 텍스트 보고서를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="657256d2b304ff4131eae140a5931256c38a3879" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list</source>
          <target state="translated">효과적인 단어 목록 작성 또는 가져 오기</target>
        </trans-unit>
        <trans-unit id="c274aea56fd427bf062945c347f3184c61ce0dda" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list.</source>
          <target state="translated">효과적인 불용어 목록을 작성하거나 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="1696170a9053ac1d87e48bfb4ce1d22a7e994777" translate="yes" xml:space="preserve">
          <source>Building a pipeline</source>
          <target state="translated">파이프 라인 구축</target>
        </trans-unit>
        <trans-unit id="d58a993cf5326c10970b9b4db170c35508c44151" translate="yes" xml:space="preserve">
          <source>Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, &lt;code&gt;bunch[&quot;value_key&quot;]&lt;/code&gt;, or by an attribute, &lt;code&gt;bunch.value_key&lt;/code&gt;.</source>
          <target state="translated">Bunch 객체는 때때로 함수 및 메서드의 출력으로 사용됩니다. 키, &lt;code&gt;bunch[&quot;value_key&quot;]&lt;/code&gt; 또는 속성, &lt;code&gt;bunch.value_key&lt;/code&gt; 로 값에 액세스 할 수 있도록하여 사전을 확장합니다 .</target>
        </trans-unit>
        <trans-unit id="6b9d2f152fc732c61d6802508ad1d27938fd60fa" translate="yes" xml:space="preserve">
          <source>By &lt;strong&gt;averaging&lt;/strong&gt; the estimates of predictive ability over several randomized trees one can &lt;strong&gt;reduce the variance&lt;/strong&gt; of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to &lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt; for more information on MDI and feature importance evaluation with Random Forests.</source>
          <target state="translated">여러 랜덤 트리에 대한 예측 능력의 추정치 를 &lt;strong&gt;평균&lt;/strong&gt; 함으로써 그러한 추정 &lt;strong&gt;의 분산&lt;/strong&gt; 을 &lt;strong&gt;줄이고&lt;/strong&gt; 특징 선택에 사용할 수 있습니다. 이것을 평균 불순물 감소 또는 MDI라고합니다. 참조 &lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt; 임의의 숲에 더 MDI에 대한 정보와 기능의 중요성을 평가합니다.</target>
        </trans-unit>
        <trans-unit id="4271aec9443f091b8f5d73284775ad1de7a16657" translate="yes" xml:space="preserve">
          <source>By contrast, in &lt;strong&gt;boosting methods&lt;/strong&gt;, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</source>
          <target state="translated">대조적으로, &lt;strong&gt;부스팅 방법에서&lt;/strong&gt; ,베이스 추정기는 순차적으로 구축되고 결합 추정기의 바이어스를 감소 시키려고 시도한다. 동기 부여는 몇 가지 약한 모델을 결합하여 강력한 앙상블을 생성하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="6c32bdf454c04c997b80ade2a7001005f06bf56a" translate="yes" xml:space="preserve">
          <source>By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="translated">기본적으로 \ (\ alpha_1 = \ alpha_2 = \ lambda_1 = \ lambda_2 = 10 ^ {-6} \).</target>
        </trans-unit>
        <trans-unit id="471aed0559b4b66da1109f2dd8bd4a3412768eaf" translate="yes" xml:space="preserve">
          <source>By default all available workers will be used (&lt;code&gt;n_jobs=-1&lt;/code&gt;) unless the caller passes an explicit value for the &lt;code&gt;n_jobs&lt;/code&gt; parameter.</source>
          <target state="translated">호출자가 &lt;code&gt;n_jobs&lt;/code&gt; 매개 변수에 대한 명시 적 값을 전달하지 않으면 기본적으로 사용 가능한 모든 작업자가 사용됩니다 ( &lt;code&gt;n_jobs=-1&lt;/code&gt; ) .</target>
        </trans-unit>
        <trans-unit id="5f6c87aa078eb9e4a070352402cf9ac4b38861b8" translate="yes" xml:space="preserve">
          <source>By default no shuffling occurs, including for the (stratified) K fold cross- validation performed by specifying &lt;code&gt;cv=some_integer&lt;/code&gt; to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, grid search, etc. Keep in mind that &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; still returns a random split.</source>
          <target state="translated">기본적으로 &lt;code&gt;cv=some_integer&lt;/code&gt; 를 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; , 그리드 검색 등 으로 지정하여 수행 된 (계층화 된) K 폴드 교차 검증을 포함하여 셔플 링이 발생하지 않습니다 . &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; 은&lt;/a&gt; 여전히 임의 분할을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e7a2fba0300cf0e1fb0a656c1e5e19ff70aee869" translate="yes" xml:space="preserve">
          <source>By default the data dir is set to a folder named &amp;lsquo;scikit_learn_data&amp;rsquo; in the user home folder.</source>
          <target state="translated">기본적으로 데이터 디렉토리는 사용자 홈 폴더에서 'scikit_learn_data'라는 폴더로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="78999f1442e19c854d03f2d5d120c491c2044f43" translate="yes" xml:space="preserve">
          <source>By default the estimator&amp;rsquo;s &lt;code&gt;score&lt;/code&gt; method is used to compute the individual scores.</source>
          <target state="translated">기본적으로 추정기의 &lt;code&gt;score&lt;/code&gt; 방법은 개별 점수를 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4b5dc2d778b41c185a986f8e9ad3bff91cf8ed7b" translate="yes" xml:space="preserve">
          <source>By default the following backends are available:</source>
          <target state="translated">기본적으로 다음과 같은 백엔드를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c9a567aacd54237b7723e5584e83694cffe0556c" translate="yes" xml:space="preserve">
          <source>By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=&amp;rsquo;exact&amp;rsquo; will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.</source>
          <target state="translated">기본적으로 그라디언트 계산 알고리즘은 O (NlogN) 시간으로 실행되는 Barnes-Hut 근사값을 사용합니다. method = 'exact'는 O (N ^ 2) 시간에 느리지 만 정확한 알고리즘에서 실행됩니다. 가장 가까운 이웃 오류가 3 % 이상이어야하는 경우 정확한 알고리즘을 사용해야합니다. 그러나 정확한 방법은 수백만 개의 예제로 확장 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="572245e5c8e088605558017246250c8893d52627" translate="yes" xml:space="preserve">
          <source>By default the order will be determined by the order of columns in the label matrix Y.:</source>
          <target state="translated">기본적으로 순서는 레이블 행렬 Y의 열 순서에 따라 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="cab46542ee630d0794253cdc9adb3976a1835662" translate="yes" xml:space="preserve">
          <source>By default the output is one-hot encoded into a sparse matrix (See &lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;Encoding categorical features&lt;/a&gt;) and this can be configured with the &lt;code&gt;encode&lt;/code&gt; parameter. For each feature, the bin edges are computed during &lt;code&gt;fit&lt;/code&gt; and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:</source>
          <target state="translated">기본적으로 출력은 희소 행렬로 one-hot 인코딩되며 ( &lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;카테고리 기능 인코딩&lt;/a&gt; 참조 ) 이것은 &lt;code&gt;encode&lt;/code&gt; 매개 변수를 사용하여 구성 할 수 있습니다 . 각 피처에 대해 구간 가장자리는 &lt;code&gt;fit&lt;/code&gt; 동안 및 구간 수와 함께 계산 되며 구간을 정의합니다. 따라서 현재 예에서 이러한 간격은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="7638d24983f6f0c8d02846709e02c43bfb3124b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt;&lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt;&lt;/a&gt; divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition.</source>
          <target state="translated">기본적으로 &lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt; &lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt; &lt;/a&gt; 은 데이터를 미니 배치로 나누고 지정된 반복 횟수 동안 미니 배치를 순환하여 온라인 방식으로 최적화합니다. 그러나 현재로서는 정지 조건을 구현하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2eb2435c3e98faf455297cd059d456431f056aed" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;float16&lt;/code&gt; results are computed using &lt;code&gt;float32&lt;/code&gt; intermediates for extra precision.</source>
          <target state="translated">기본적으로 &lt;code&gt;float16&lt;/code&gt; 결과는 정밀도 를 높이기 위해 &lt;code&gt;float32&lt;/code&gt; 중간체를 사용하여 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="06b991078a7a86c59d05afe16f66234f800e038c" translate="yes" xml:space="preserve">
          <source>By default, LocalOutlierFactor is only meant to be used for outlier detection (novelty=False). Set novelty to True if you want to use LocalOutlierFactor for novelty detection. In this case be aware that that you should only use predict, decision_function and score_samples on new unseen data and not on the training set.</source>
          <target state="translated">기본적으로 LocalOutlierFactor는 이상 값 탐지에만 사용됩니다 (novelty = False). 참신 탐지에 LocalOutlierFactor를 사용하려면 참신을 True로 설정하십시오. 이 경우 훈련 세트가 아닌 보이지 않는 새로운 데이터에 대해서만 predict, decision_function 및 score_samples를 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="2fb7ae3e457e09c80061e4212540f2949867d13f" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.</source>
          <target state="translated">기본적으로 효율적인 교차 교차 검증의 한 형태 인 일반화 된 교차 검증을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="76317967f826fa82a2b4a34c8b9f11ee88f99afc" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features &amp;gt; n_samples case is handled efficiently.</source>
          <target state="translated">기본적으로 효율적인 교차 교차 검증의 한 형태 인 일반화 된 교차 검증을 수행합니다. 현재 n_features&amp;gt; n_samples 케이스 만 효율적으로 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="c04cdaf41bfb70ed4b9c123ddaef4117557b0e3d" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;.</source>
          <target state="translated">기본적으로 &lt;code&gt;transformers&lt;/code&gt; 의 지정된 열만 변환되고 출력에서 ​​결합되며 지정되지 않은 열은 삭제됩니다. (기본값 &lt;code&gt;'drop'&lt;/code&gt; ). 지정하여 &lt;code&gt;remainder='passthrough'&lt;/code&gt; 에 지정되지 않은 모든 나머지 열 &lt;code&gt;transformers&lt;/code&gt; 자동으로 통과됩니다. 이 열 하위 집합은 변환기의 출력과 연결됩니다. &lt;code&gt;remainder&lt;/code&gt; 를 추정 자로 설정 하면 지정되지 않은 나머지 열은 &lt;code&gt;remainder&lt;/code&gt; 추정기를 사용합니다 . 추정자는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합&lt;/a&gt; 과 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;변환을&lt;/a&gt; 지원해야합니다 .</target>
        </trans-unit>
        <trans-unit id="15494fd5cbda86aa1f330192fb9882a6711f9c4e" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;. Note that using this feature requires that the DataFrame columns input at &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; have identical order.</source>
          <target state="translated">기본적으로 &lt;code&gt;transformers&lt;/code&gt; 의 지정된 열만 변환되고 출력에서 ​​결합되며 지정되지 않은 열은 삭제됩니다. (기본값 &lt;code&gt;'drop'&lt;/code&gt; ). 지정하여 &lt;code&gt;remainder='passthrough'&lt;/code&gt; 에 지정되지 않은 모든 나머지 열 &lt;code&gt;transformers&lt;/code&gt; 자동으로 통과됩니다. 이 열 하위 집합은 변환기의 출력과 연결됩니다. &lt;code&gt;remainder&lt;/code&gt; 를 추정 자로 설정 하면 지정되지 않은 나머지 열은 &lt;code&gt;remainder&lt;/code&gt; 추정기를 사용합니다 . 추정자는 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합&lt;/a&gt; 과 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;변환을&lt;/a&gt; 지원해야합니다 . 참고이 기능을 사용하는 것이 필요하다는 것을에서 DataFrame 열 입력 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;적합&lt;/a&gt;및 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;변환&lt;/a&gt; 동일한 순서를 갖는다.</target>
        </trans-unit>
        <trans-unit id="7750d963aa2e941a76e470078ac6c086adc6dcda" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">기본적으로 &lt;code&gt;transformers&lt;/code&gt; 에서 지정된 열만 출력에서 변환 및 결합되고 지정되지 않은 열은 삭제됩니다. (기본값은 &lt;code&gt;'drop'&lt;/code&gt; ). 지정하여 &lt;code&gt;remainder='passthrough'&lt;/code&gt; 에 지정되지 않은 모든 나머지 열 &lt;code&gt;transformers&lt;/code&gt; 자동으로 통과됩니다. 이 열의 하위 집합은 변압기의 출력과 연결됩니다. &lt;code&gt;remainder&lt;/code&gt; 를 추정기 로 설정 하면 나머지 지정되지 않은 열은 &lt;code&gt;remainder&lt;/code&gt; 추정기를 사용합니다 . 추정기는 &lt;code&gt;fit&lt;/code&gt; 및 &lt;code&gt;transform&lt;/code&gt; 지원해야합니다 .</target>
        </trans-unit>
        <trans-unit id="4a782d670d6fc36b236c4a8940c016b292a90c5d" translate="yes" xml:space="preserve">
          <source>By default, parameter search uses the &lt;code&gt;score&lt;/code&gt; function of the estimator to evaluate a parameter setting. These are the &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;&lt;/a&gt; for classification and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt;&lt;/a&gt; for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the &lt;code&gt;scoring&lt;/code&gt; parameter to &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; and many of the specialized cross-validation tools described below. See &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt; for more details.</source>
          <target state="translated">기본적으로 매개 변수 검색은 추정기 의 &lt;code&gt;score&lt;/code&gt; 함수를 사용하여 매개 변수 설정을 평가합니다. 이들은입니다 &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt; &lt;/a&gt; 분류 및 &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt; &lt;/a&gt; 회귀를 위해. 일부 응용 프로그램의 경우 다른 점수 매기기 기능이 더 적합합니다 (예 : 불균형 분류에서 정확도 점수는 종종 정보가 없습니다). 대체 스코어링 함수는 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 통해 &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt; 및 아래에 설명 된 많은 특수 교차 검증 도구에 지정할 수 있습니다 . 자세한 내용 &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;은 스코어링 매개 변수 : 모델 평가 규칙 정의&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="14b262313cef67a36d56fb94a78ec0ffe131d0f9" translate="yes" xml:space="preserve">
          <source>By default, the &amp;lsquo;recursion&amp;rsquo; method is used on tree-based estimators that support it, and &amp;lsquo;brute&amp;rsquo; is used for the rest.</source>
          <target state="translated">기본적으로 '재귀'방법은이를 지원하는 트리 기반 추정기에 사용되며 나머지에는 'brute'가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="cd89643870e89d7d33668ed08a7b04e11606d0f7" translate="yes" xml:space="preserve">
          <source>By default, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold. The default will change to a 5-fold cross-validation in version 0.22.</source>
          <target state="translated">기본적으로 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 는 3 배 교차 검증을 사용합니다. 그러나 회귀자가 아닌 분류 기가 전달 된 것을 발견하면 계층화 된 3 배를 사용합니다. 기본값은 0.22 버전에서 5 배 교차 검증으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="d01628e3c1ff7c3c146d98c4400e308b8d21e62e" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually.</source>
          <target state="translated">기본적으로 인코더는 각 기능의 고유 한 값을 기반으로 범주를 파생합니다. 또는 &lt;code&gt;categories&lt;/code&gt; 수동으로 지정할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="aabd7b3f7cca56e260a395d6492aceaf31d56d74" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated.</source>
          <target state="translated">기본적으로 인코더는 각 기능의 고유 한 값을 기준으로 범주를 파생합니다. 또는 &lt;code&gt;categories&lt;/code&gt; 수동으로 지정할 수도 있습니다. OneHotEncoder는 이전에 입력 기능이 [0, max (values)) 범위의 값을 사용한다고 가정했습니다. 이 동작은 더 이상 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e22c55145c4d94c4a5b62e5b6f40a60aa6e4907e" translate="yes" xml:space="preserve">
          <source>By default, the initial model \(F_{0}\) is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument.</source>
          <target state="translated">기본적으로 초기 모델 \ (F_ {0} \)은 손실을 최소화하는 상수로 선택됩니다. 최소 제곱 손실의 경우 이는 목표 값의 경험적 평균입니다. 초기 모델은 &lt;code&gt;init&lt;/code&gt; 인수 를 통해 지정할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e16205e1ed0a918834ac9076544d9991fc9c4ca6" translate="yes" xml:space="preserve">
          <source>By default, the input is checked to be a non-empty 2D array containing only finite values. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="translated">기본적으로 입력은 유한 값만 포함하는 비어 있지 않은 2D 배열로 확인됩니다. 배열의 dtype이 객체 인 경우 float로 변환하여 실패시 발생합니다.</target>
        </trans-unit>
        <trans-unit id="0397cff741cdca99d6793cf68ca8a230e768dce2" translate="yes" xml:space="preserve">
          <source>By default, the input is converted to an at least 2D numpy array. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="translated">기본적으로 입력은 2D 이상의 numpy 배열로 변환됩니다. 배열의 dtype이 object이면 float로 변환하여 실패를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="9dfabb9221fc44d77597df9d22d00887ce88c6cb" translate="yes" xml:space="preserve">
          <source>By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting &lt;code&gt;check_inverse&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:</source>
          <target state="translated">기본적으로 제공된 기능은 각각의 역에서 서로 맞는지 확인합니다. 그러나 &lt;code&gt;check_inverse&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정하여이 검사를 무시할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="761cb5c85c42ad01cf3595b928efe71009e68e6e" translate="yes" xml:space="preserve">
          <source>By default, the score computed at each CV iteration is the &lt;code&gt;score&lt;/code&gt; method of the estimator. It is possible to change this by using the scoring parameter:</source>
          <target state="translated">기본적으로 각 CV 반복에서 계산 된 &lt;code&gt;score&lt;/code&gt; 는 추정기 의 점수 방법입니다. 스코어링 매개 변수를 사용하여이를 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="61fe756b7e3e334c670a2007b6939f399986a294" translate="yes" xml:space="preserve">
          <source>By default, the values each feature can take is inferred automatically from the dataset and can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute:</source>
          <target state="translated">기본적으로 각 기능이 취할 수있는 값은 데이터 세트에서 자동으로 추론되며 &lt;code&gt;categories_&lt;/code&gt; 속성 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="588af344f8a3913fdf9179a25370573310fcee34" translate="yes" xml:space="preserve">
          <source>By default, zero-mean, unit-variance normalization is applied to the transformed data.</source>
          <target state="translated">기본적으로 평균이 0 인 단위 분산 정규화가 변환 된 데이터에 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="11243048cfb311c05ea90a5cca047fa81467c316" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) and predicted to be in group \(j\).</source>
          <target state="translated">정의에 따라 혼동 행렬 \ (C \)는 \ (C_ {i, j} \)가 \ (i \) 그룹에 있고 \ (j \) 그룹에있는 것으로 알려진 관측치의 수와 같도록합니다. ).</target>
        </trans-unit>
        <trans-unit id="c874644c0e92eeddd8de26271a422d136d288139" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) but predicted to be in group \(j\).</source>
          <target state="translated">정의에 따르면 혼동 행렬 \ (C \)는 \ (C_ {i, j} \)가 그룹 \ (i \)에있는 것으로 알려진 관측치 수와 같지만 그룹 \ (j \에있을 것으로 예상 됨) ).</target>
        </trans-unit>
        <trans-unit id="7edbd0cd8c2bac6d61783ee719a48e462c1cfcf6" translate="yes" xml:space="preserve">
          <source>By definition, entry \(i, j\) in a confusion matrix is the number of observations actually in group \(i\), but predicted to be in group \(j\). Here is an example:</source>
          <target state="translated">정의에 따르면 혼동 행렬의 \ (i, j \) 항목은 실제로 \ (i \) 그룹의 관측치 수이지만 \ (j \) 그룹에있을 것으로 예상됩니다. 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="70e3cb436db4ad1c973fa29cd922d3f24b1b4676" translate="yes" xml:space="preserve">
          <source>By imposing a positive (increasing) or negative (decreasing) constraint on the features during the learning process, the estimator is able to properly follow the general trend instead of being subject to the variations.</source>
          <target state="translated">학습 과정에서 특징에 긍정적 (증가) 또는 부정적 (감소) 제약 조건을 적용함으로써 추정자는 변동에 영향을받지 않고 일반적인 추세를 적절하게 따를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1dfab6b69e192f9e6ae185588957d2e4ef21b660" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification</source>
          <target state="translated">C- 지원 벡터 분류의 C 매개 변수</target>
        </trans-unit>
        <trans-unit id="cdda2bb3aa8ccb7b0f69433506f364d34d2e9052" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification. 1 by default.</source>
          <target state="translated">C- 지원 벡터 분류의 C 매개 변수. 기본적으로 1입니다.</target>
        </trans-unit>
        <trans-unit id="613d7a720c0f1c9138a31585cc88659b028aac73" translate="yes" xml:space="preserve">
          <source>C-Support Vector Classification.</source>
          <target state="translated">C- 지원 벡터 분류.</target>
        </trans-unit>
        <trans-unit id="bd2a390d1ac4f130ef2be0118bce6d064e5c8f80" translate="yes" xml:space="preserve">
          <source>C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.</source>
          <target state="translated">C. Kaynak (1995) 여러 분류기와 응용을 수기 인식으로 결합하는 방법, Bogazici University의 과학 및 공학 대학원 석사 논문, 석사 논문.</target>
        </trans-unit>
        <trans-unit id="233528e4f33123dff21e034b86ff445b99733c7a" translate="yes" xml:space="preserve">
          <source>C. Molnar, &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt;, Section 5.1, 2019.</source>
          <target state="translated">C. Molnar, &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;해석 가능한 기계 학습&lt;/a&gt; , 섹션 5.1, 2019.</target>
        </trans-unit>
        <trans-unit id="da3d99d58e8919ce7a794351fb460091679af9c1" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 118-120.</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Sch&amp;uuml;tze (2008). 정보 검색 소개. 캠브리지 대학 출판부, pp. 118-120.</target>
        </trans-unit>
        <trans-unit id="da9635593b5caf5d4585f5ff3fe3cc183bfd45ee" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265.</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Sch&amp;uuml;tze (2008). 정보 검색 소개. 케임브리지 대학 출판부, 234-265 쪽.</target>
        </trans-unit>
        <trans-unit id="14a7f63b48e4cd57491d39ae8fbc9659df0b4285" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Sch&amp;uuml;tze (2008). 정보 검색 소개. 케임브리지 대학 출판부. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d8731241db66672efece43e5d06ce590569a5d55" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Sch&amp;uuml;tze (2008). 정보 검색 소개. 캠브리지 대학 출판부. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7f2e75005cfe431b03b43e4219e3b46d7acd0ff4" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Schuetze (2008). 정보 검색 소개. 케임브리지 대학 출판부, 234-265 쪽. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec24d6932a118f83e8d64145c0a2ded87f2b67" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Schuetze (2008). 정보 검색 소개. 케임브리지 대학 출판부, 234-265 쪽. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01f3fb182680db69cebca5748ed6286feb50b1a9" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Schuetze (2008). 정보 검색 소개. 캠브리지 대학 출판부, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9600fefe6b9e4b354eca34f86431cfc60b66dab3" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="translated">CD Manning, P. Raghavan 및 H. Schuetze (2008). 정보 검색 소개. 캠브리지 대학 출판부, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d19ae811e7ffeb32597a550f21eb906e9869f42b" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="translated">CD Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;정보 검색 소개&lt;/a&gt; , 2008.</target>
        </trans-unit>
        <trans-unit id="015610fbd8f75c742ed46a08abb26c26092afe90" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="translated">CD Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;정보 검색 소개&lt;/a&gt; , 2008.</target>
        </trans-unit>
        <trans-unit id="ac35b04dfbcb3d2d718c41403e2829f9afacc158" translate="yes" xml:space="preserve">
          <source>C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.</source>
          <target state="translated">CM 주교 (2006). 패턴 인식 및 기계 학습. 스프링거, p. 209.</target>
        </trans-unit>
        <trans-unit id="4e28cad37c371942b28c3b9844d1c63eab0cd98c" translate="yes" xml:space="preserve">
          <source>C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule&amp;rsquo;s precondition if the accuracy of the rule improves without it.</source>
          <target state="translated">C4.5는 ID3의 후속 버전이며 연속 속성 값을 이산 간격 세트로 분할하는 이산 속성 (숫자 변수 기반)을 동적으로 정의하여 기능이 범주 형이어야한다는 제한을 제거했습니다. C4.5는 훈련 된 트리 (즉, ID3 알고리즘의 출력)를 if-then 규칙 세트로 변환합니다. 그런 다음 각 규칙의 정확성을 평가하여 적용 순서를 결정합니다. 제거하지 않고 규칙의 정확성이 향상되면 규칙의 전제 조건을 제거하여 제거를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="f38b90367c7c83e3cb66fb496411f772ac84712e" translate="yes" xml:space="preserve">
          <source>C5.0 is Quinlan&amp;rsquo;s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.</source>
          <target state="translated">C5.0은 독점 라이센스에 따른 Quinlan의 최신 버전입니다. C4.5보다 메모리를 적게 사용하고 더 작은 규칙 집합을 작성하는 것이 더 정확합니다.</target>
        </trans-unit>
        <trans-unit id="d173d3b539a344f61d41ecea5a62c323f7d19142" translate="yes" xml:space="preserve">
          <source>CCA Canonical Correlation Analysis.</source>
          <target state="translated">CCA 정식 상관 분석.</target>
        </trans-unit>
        <trans-unit id="b29de26d602ff110fad786ea8b03d97b6805af56" translate="yes" xml:space="preserve">
          <source>CCA inherits from PLS with mode=&amp;rdquo;B&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;.</source>
          <target state="translated">CCA는 mode =&amp;rdquo;B&amp;rdquo;및 deflation_mode =&amp;rdquo;canonical&amp;rdquo;인 PLS에서 상속합니다.</target>
        </trans-unit>
        <trans-unit id="3b8d88f34c322c835ae4b658c65d92bfd4b52c68" translate="yes" xml:space="preserve">
          <source>CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</source>
          <target state="translated">CHAS Charles River 더미 변수 (관이 하천을 향하면 1, 그렇지 않으면 0)</target>
        </trans-unit>
        <trans-unit id="65e7931e7d9586486554e7903a3ba08da00b37e4" translate="yes" xml:space="preserve">
          <source>CRIM per capita crime rate by town</source>
          <target state="translated">도시 별 1 인당 범죄율</target>
        </trans-unit>
        <trans-unit id="c1c39cb0e3578f55a7a5c64e876c1d8c4ac8dfba" translate="yes" xml:space="preserve">
          <source>CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are not supported.</source>
          <target state="translated">CSR, CSC 및 LIL 희소 행렬이 지원됩니다. COO 희소 행렬은 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="73f41c9b08913eecfdd5c30b35b021c1e2d148fd" translate="yes" xml:space="preserve">
          <source>Cache size for gram matrix columns (in megabytes). 100 by default.</source>
          <target state="translated">그램 매트릭스 열의 캐시 크기 (MB). 기본적으로 100입니다.</target>
        </trans-unit>
        <trans-unit id="596160dd9a9ac13e48baafc6fcbec2fd25ca71f9" translate="yes" xml:space="preserve">
          <source>Caching nearest neighbors</source>
          <target state="translated">최근 접 이웃 캐싱</target>
        </trans-unit>
        <trans-unit id="9ff21fb5eb8a4fa17d7e155397ec6ccfabcba0a4" translate="yes" xml:space="preserve">
          <source>Caching transformers within a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 내에서 변압기 캐싱</target>
        </trans-unit>
        <trans-unit id="bc2913cfd35cdeb7164299f7eca5904d9beb65fc" translate="yes" xml:space="preserve">
          <source>Calculate approximate log-likelihood as score.</source>
          <target state="translated">대략적인 로그 우도를 점수로 계산합니다.</target>
        </trans-unit>
        <trans-unit id="c1ebfc4b9bd037366318ba68c2c2cca59e1e8374" translate="yes" xml:space="preserve">
          <source>Calculate approximate perplexity for data X.</source>
          <target state="translated">데이터 X에 대한 대략적인 당황을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d43ab93e0150f4ef29d06ad998812dd6ba91c9c1" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">각 인스턴스에 대한 지표를 계산하고 평균을 찾으십시오 ( &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; &lt;/a&gt; 점수와 다른 다중 레이블 분류에만 의미가 있음 ).</target>
        </trans-unit>
        <trans-unit id="e282cf212dffcdd940b3d9463a8434aa0c623305" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).</source>
          <target state="translated">각 인스턴스에 대한 지표를 계산하고 평균을 찾습니다 (다중 레이블 분류에서만 의미가 있음).</target>
        </trans-unit>
        <trans-unit id="59762e6da26cf0edea79b5cf712b11b4295f7c3a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average.</source>
          <target state="translated">각 인스턴스에 대한 지표를 계산하고 평균을 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="b0449c6e69f12738c5fc9e626586315cc55b3e5c" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance; it can result in an F-score that is not between precision and recall.</source>
          <target state="translated">각 레이블에 대한 메트릭을 계산하고 지원에 의해 가중 된 평균 (각 레이블에 대한 실제 인스턴스 수)을 찾으십시오. 라벨 불균형을 설명하기 위해 '매크로'가 변경됩니다. 정밀도와 리콜 사이에 있지 않은 F- 점수가 발생할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bc72c4ff58beeced2951accfefb204124157b6da" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label).</source>
          <target state="translated">각 레이블에 대한 메트릭을 계산하고 지원에 의해 가중치가 부여 된 평균 (각 레이블에 대한 실제 인스턴스 수)을 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="a0980fad9587146eedd0aa21fe16b429b81ffb7a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance.</source>
          <target state="translated">각 레이블에 대한 메트릭을 계산하고 지원에 의해 가중치가 부여 된 평균을 찾습니다 (각 레이블의 실제 인스턴스 수). 이것은 레이블 불균형을 설명하기 위해 '매크로'를 변경합니다.</target>
        </trans-unit>
        <trans-unit id="c9463a17edbcd91794095643439dd340e0b094af" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</source>
          <target state="translated">각 레이블에 대한 메트릭을 계산하고 가중치가없는 평균을 찾으십시오. 레이블 불균형은 고려되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a029600dc316664ad0a95bc83365c7c5938aa93b" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by considering each element of the label indicator matrix as a label.</source>
          <target state="translated">레이블 표시기 매트릭스의 각 요소를 레이블로 고려하여 전체적으로 메트릭을 계산하십시오.</target>
        </trans-unit>
        <trans-unit id="734c6eed79b78874fafa14f9d8cbb9961b3a854c" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by counting the total true positives, false negatives and false positives.</source>
          <target state="translated">총 참 긍정, 거짓 부정 및 거짓 긍정을 계산하여 전 세계적으로 메트릭을 계산하십시오.</target>
        </trans-unit>
        <trans-unit id="23f0cbdf1f8f2eb83817496535820388353f4a46" translate="yes" xml:space="preserve">
          <source>Calculate the euclidean distances in the presence of missing values.</source>
          <target state="translated">결 측값이있는 경우 유클리드 거리를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="49fff0d8477b2f29bd766898572fb8e020082b5d" translate="yes" xml:space="preserve">
          <source>Calculates a covariance matrix shrunk on the diagonal</source>
          <target state="translated">대각선의 공분산 행렬 축소를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="f0383dd4fd81be714fa7344f5483fa52315c3c0b" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positive_rate&quot;&gt;fall out&lt;/a&gt; (also called the false positive rate) for each class:</source>
          <target state="translated">각 클래스에 대한 &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positive_rate&quot;&gt;폴 아웃&lt;/a&gt; (위양성 비율이라고도 함) 계산 :</target>
        </trans-unit>
        <trans-unit id="89465d26eac0f77a714d2eb41333a3104974a090" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positives_and_false_negatives&quot;&gt;miss rate&lt;/a&gt; (also called the false negative rate) for each class:</source>
          <target state="translated">각 클래스에 대한 &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positives_and_false_negatives&quot;&gt;실패율&lt;/a&gt; (위음성 비율이라고도 함) 계산 :</target>
        </trans-unit>
        <trans-unit id="485cdc580455f224386f0fdb1e7fd3b7d1083ba4" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;recall&lt;/a&gt; (also called the true positive rate or the sensitivity) for each class:</source>
          <target state="translated">각 클래스에 대한 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;재현율&lt;/a&gt; (진 양성 비율 또는 민감도라고도 함) 계산 :</target>
        </trans-unit>
        <trans-unit id="a9101242444ef9428e2eaa8ec947f98e495b1f80" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (also called the true negative rate) for each class:</source>
          <target state="translated">각 클래스에 대한 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;특이성&lt;/a&gt; (진 음성 비율이라고도 함) 계산 :</target>
        </trans-unit>
        <trans-unit id="c97307180f2f1c1353791c3b1ff02010b40d4cbc" translate="yes" xml:space="preserve">
          <source>Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation.</source>
          <target state="translated">그러나 밀도가 높은 표현에 대한 계산은 BLAS에서 고도로 최적화 된 벡터 연산 및 멀티 스레딩을 활용할 수 있으며 CPU 캐시 누락이 줄어 듭니다. 따라서 희소 한 입력 표현이 많은 CPU와 최적화 된 BLAS 구현이있는 머신의 밀도 높은 입력 표현보다 빠르기 위해서는 일반적으로 희소성이 매우 높아야합니다 (하드웨어에 따라 확인하려면 0이 아닌 최대 10 %).</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
