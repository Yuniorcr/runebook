<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="4057b3cb6bd141b135fafdb32fce3f5d0c92b8db" translate="yes" xml:space="preserve">
          <source>Sample matrix.</source>
          <target state="translated">샘플 매트릭스.</target>
        </trans-unit>
        <trans-unit id="04c4511b91c525e2da2ef7db41615c6ca0a3fd2e" translate="yes" xml:space="preserve">
          <source>Sample output:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc8d82180e352f6e4eb38618a9b77ce032b1c63f" translate="yes" xml:space="preserve">
          <source>Sample pipeline for text feature extraction and evaluation</source>
          <target state="translated">텍스트 기능 추출 및 평가를위한 샘플 파이프 라인</target>
        </trans-unit>
        <trans-unit id="0982f69f498171fb424746f3b46f588b79249d60" translate="yes" xml:space="preserve">
          <source>Sample usage of Nearest Centroid classification. It will plot the decision boundaries for each class.</source>
          <target state="translated">가장 가까운 중심 분류의 샘플 사용법. 각 클래스의 의사 결정 경계를 표시합니다.</target>
        </trans-unit>
        <trans-unit id="f22b9a32693422a5abcec4767410509915bc2f8f" translate="yes" xml:space="preserve">
          <source>Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class.</source>
          <target state="translated">가장 가까운 이웃 분류의 샘플 사용법. 각 클래스의 의사 결정 경계를 표시합니다.</target>
        </trans-unit>
        <trans-unit id="c4ad00c210ac74869e557ad5117f0c17f5204222" translate="yes" xml:space="preserve">
          <source>Sample usage of Neighborhood Components Analysis for dimensionality reduction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6f1c43a568cbfebecbc9c4a468d534512e56696" translate="yes" xml:space="preserve">
          <source>Sample vectors from which to compute variances.</source>
          <target state="translated">분산을 계산할 표본 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="4701e0099b8ff3338dac13ba80a9fd03a1b4e3e5" translate="yes" xml:space="preserve">
          <source>Sample vectors.</source>
          <target state="translated">샘플 벡터.</target>
        </trans-unit>
        <trans-unit id="e2a418c622901df3ef07f5cc98282eefd0e90959" translate="yes" xml:space="preserve">
          <source>Sample weight</source>
          <target state="translated">샘플 무게</target>
        </trans-unit>
        <trans-unit id="e69657285f6aad82f9a81418454c1c4adb873fb2" translate="yes" xml:space="preserve">
          <source>Sample weight.</source>
          <target state="translated">샘플 무게.</target>
        </trans-unit>
        <trans-unit id="b0d17c86dbd02471ac25031f76f6b9e62870f2a0" translate="yes" xml:space="preserve">
          <source>Sample weights</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03782597aeab2816675e9752810fe748c242aeef" translate="yes" xml:space="preserve">
          <source>Sample weights.</source>
          <target state="translated">샘플 무게.</target>
        </trans-unit>
        <trans-unit id="f828cc5ffdb9cf591696bfda2177ba993eb46afe" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, all samples are given the same weight.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="516c4850672ccbdd1765de8db9d3606a458e566e" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, the sample weights are initialized to 1 / n_samples.</source>
          <target state="translated">샘플 무게. None이면 샘플 가중치는 1 / n_samples로 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="06e56dd5257a783cd783e2275240a5cef38438c1" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, the sample weights are initialized to &lt;code&gt;1 / n_samples&lt;/code&gt;.</source>
          <target state="translated">샘플 무게. None이면, 샘플 가중치는 &lt;code&gt;1 / n_samples&lt;/code&gt; 로 초기화됩니다 .</target>
        </trans-unit>
        <trans-unit id="b28026c224fc212b3c8db8d7abcf83fe154d5aba" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="dd4e4922a0bf331287b6e7fdee69023638c0a9c2" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 이것은 모든 기본 추정기가 샘플 가중치를 지원하는 경우에만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="72b5ffbf428e4946121de0a0ea4fb4fa9205e66d" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 이는 기본 추정기가 샘플 가중치를 지원하는 경우에만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="44004435db67a9bbea489a279598bbde7cadacd2" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Only supported if the underlying classifier supports sample weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e07f10e1efb6d03970a11a668db91780f97be9f4" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 기본 회귀자가 샘플 가중치를 지원하는 경우에만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="d4811960b5c60cf1d2e21f4adf57777acf8cf860" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 순 제로 또는 음의 가중치로 하위 노드를 생성하는 분할은 각 노드에서 분할을 검색하는 동안 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="76ebb3a0858ee52c5cebb457753ce9dfb9f1fd6e" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 순 제로 또는 음의 가중치로 하위 노드를 생성하는 분할은 각 노드에서 분할을 검색하는 동안 무시됩니다. 분류의 경우 분할이 하위 노드 중 하나에서 음의 가중치를 갖는 단일 클래스를 생성하는 경우에도 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="317d3738fe42cf9aeb8c2e6052e8dee6f03e55e0" translate="yes" xml:space="preserve">
          <source>Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.</source>
          <target state="translated">샘플 무게. None이면 샘플에 동일한 가중치를 적용합니다. 순 제로 또는 음의 가중치로 하위 노드를 생성하는 분할은 각 노드에서 분할을 검색하는 동안 무시됩니다. 분할이 하위 노드 중 하나에서 음의 가중치를 갖는 단일 클래스를 생성하는 경우에도 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="a6e78b9afd27497df49aade34a36635be33138ac" translate="yes" xml:space="preserve">
          <source>Sample-weight support for Lasso and ElasticNet</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a90c0865394b45b5f31d5c73cc5fa995827fa090" translate="yes" xml:space="preserve">
          <source>Samples a subset of training points, computes kernel on these and computes normalization matrix.</source>
          <target state="translated">트레이닝 포인트의 서브 세트를 샘플링하고, 이것에 대한 커널을 계산하고 정규화 행렬을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="489527d2412e4e73b577f6c6fbbfa5b2fc34f813" translate="yes" xml:space="preserve">
          <source>Samples generator</source>
          <target state="translated">샘플 생성기</target>
        </trans-unit>
        <trans-unit id="984aa7241ddfaa0a1125ba76d49684d954091644" translate="yes" xml:space="preserve">
          <source>Samples may have several labels each (see &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html&quot;&gt;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html&lt;/a&gt;)</source>
          <target state="translated">샘플에는 각각 여러 개의 레이블이있을 수 있습니다 ( &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html&quot;&gt;http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html 참조&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="4d0b8d7411b8018b991a89ec2a4d88914d35dbf8" translate="yes" xml:space="preserve">
          <source>Samples may have several labels each (see &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html&quot;&gt;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html&lt;/a&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a729b3c883140152c8be9c9b913932e3054a28dd" translate="yes" xml:space="preserve">
          <source>Samples per class</source>
          <target state="translated">수업 당 샘플</target>
        </trans-unit>
        <trans-unit id="72ff32775331fbcdecdd7b50f2f019ca6fd41d2c" translate="yes" xml:space="preserve">
          <source>Samples random projection according to n_features.</source>
          <target state="translated">n_features에 따라 랜덤 프로젝션을 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="251d1d7b5c7f8793378b7d465a809b22ff0293ea" translate="yes" xml:space="preserve">
          <source>Samples to cluster.</source>
          <target state="translated">클러스터 할 샘플.</target>
        </trans-unit>
        <trans-unit id="1b35c86a656c810d2ffde5bec3bbb5716273c85d" translate="yes" xml:space="preserve">
          <source>Samples total</source>
          <target state="translated">총 샘플</target>
        </trans-unit>
        <trans-unit id="d94a358c32f7a1a8aa072b320513050f66fbf3bb" translate="yes" xml:space="preserve">
          <source>Samples.</source>
          <target state="translated">Samples.</target>
        </trans-unit>
        <trans-unit id="79b4194bd3e79bf7f3c58fb9c6afe5574c4f3465" translate="yes" xml:space="preserve">
          <source>Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.</source>
          <target state="translated">샘플. 각 샘플은 토큰 화되고 해시 될 텍스트 문서 (바이트 또는 유니 코드 문자열, 생성자 인수에 따라 파일 이름 또는 파일 객체) 여야합니다.</target>
        </trans-unit>
        <trans-unit id="4ecf733dec873b2818a96fff2c4d7137b5e9cce2" translate="yes" xml:space="preserve">
          <source>Samples. Each sample must be iterable an (e.g., a list or tuple) containing/generating feature names (and optionally values, see the input_type constructor argument) which will be hashed. raw_X need not support the len function, so it can be the result of a generator; n_samples is determined on the fly.</source>
          <target state="translated">샘플. 각 샘플은 해시 될 기능 이름 (및 선택적으로 값, input_type 생성자 인수 참조)을 포함 / 생성하는 반복 가능해야합니다 (예 : 목록 또는 튜플). raw_X는 len 함수를 지원할 필요가 없으므로 생성기의 결과 일 수 있습니다. n_samples는 즉시 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="475a3c12e7131d5c7c597d45cbde5d7c042c5697" translate="yes" xml:space="preserve">
          <source>Samples. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead a precomputed kernel matrix, shape = [n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for this estimator.</source>
          <target state="translated">샘플. kernel == &quot;precomputed&quot;인 경우, 이는 사전 계산 된 커널 행렬 대신 shape = [n_samples, n_samples_fitted]입니다. 여기서 n_samples_fitted는이 추정기의 피팅에 사용 된 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="786961f4d535734fe86dc46d23d2c4ae6643e27e" translate="yes" xml:space="preserve">
          <source>Sampling interval. Must be specified when sample_steps not in {1,2,3}.</source>
          <target state="translated">샘플링 간격. sample_steps가 {1,2,3}이 아닌 경우 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="fb8efe38000470eaaa9cdd2c1e100fa22c387ba8" translate="yes" xml:space="preserve">
          <source>Sampling more dimensions clearly leads to better classification results, but comes at a greater cost. This means there is a tradeoff between runtime and accuracy, given by the parameter n_components. Note that solving the Linear SVM and also the approximate kernel SVM could be greatly accelerated by using stochastic gradient descent via &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt;. This is not easily possible for the case of the kernelized SVM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b1ee52c21297b409f91752fd0536580b7df89b8" translate="yes" xml:space="preserve">
          <source>Sampling more dimensions clearly leads to better classification results, but comes at a greater cost. This means there is a tradeoff between runtime and accuracy, given by the parameter n_components. Note that solving the Linear SVM and also the approximate kernel SVM could be greatly accelerated by using stochastic gradient descent via &lt;a href=&quot;../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt;. This is not easily possible for the case of the kernelized SVM.</source>
          <target state="translated">더 많은 차원을 샘플링하면 더 나은 분류 결과로 이어지지 만 더 큰 비용이 듭니다. 이는 n_components 매개 변수가 제공하는 런타임과 정확도 사이에 상충 관계가 있음을 의미합니다. &lt;a href=&quot;../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt; 를 통해 확률 적 그라디언트 디센트 (stochastic gradient descent)를 사용하면 Linear SVM 및 대략적인 커널 SVM을 해결하는 것이 크게 가속화 될 수 있습니다 . 이것은 커널 화 된 SVM의 경우에는 불가능합니다.</target>
        </trans-unit>
        <trans-unit id="3e5ac0adafac21480f1a521f8334d2085c97ee0a" translate="yes" xml:space="preserve">
          <source>Sanjoy Dasgupta and Anupam Gupta, 1999, &amp;ldquo;An elementary proof of the Johnson-Lindenstrauss Lemma.&amp;rdquo; &lt;a href=&quot;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654&quot;&gt;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654&lt;/a&gt;</source>
          <target state="translated">Sanjoy Dasgupta와 Anupam Gupta, 1999,&amp;ldquo;Johnson-Lindenstrauss Lemma의 기본 증거.&amp;rdquo; &lt;a href=&quot;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654&quot;&gt;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="72e45a031f2d9ef687b450c7dbe04851bc78b888" translate="yes" xml:space="preserve">
          <source>Sanjoy Dasgupta and Anupam Gupta, 1999. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.3334&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;An elementary proof of the Johnson-Lindenstrauss Lemma.&lt;/a&gt;</source>
          <target state="translated">Sanjoy Dasgupta와 Anupam Gupta, 1999. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.3334&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Johnson-Lindenstrauss Lemma의 기본 증거.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="916673b66f20fa78c64c3896647266270d456625" translate="yes" xml:space="preserve">
          <source>Sanjoy Dasgupta. 2000. &lt;a href=&quot;http://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf&quot;&gt;Experiments with random projection.&lt;/a&gt; In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI&amp;lsquo;00), Craig Boutilier and Mois&amp;eacute;s Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.</source>
          <target state="translated">Sanjoy Dasgupta. 2000 년 &lt;a href=&quot;http://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf&quot;&gt;임의 투사와 실험. &lt;/a&gt;인공 지능의 불확실성 (UAI'00)에 관한 16 차 회의에서 Craig Boutilier와 Mois&amp;eacute;s Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., 미국 캘리포니아 주 샌프란시스코, 143-151.</target>
        </trans-unit>
        <trans-unit id="2894bd12dff71a351f3ecafd27a1d2a6b0bda89e" translate="yes" xml:space="preserve">
          <source>Sanjoy Dasgupta. 2000. &lt;a href=&quot;https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf&quot;&gt;Experiments with random projection.&lt;/a&gt; In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI&amp;rsquo;00), Craig Boutilier and Mois&amp;eacute;s Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dc12a8bd942b1cd9b3ab450a012927de384bfa6" translate="yes" xml:space="preserve">
          <source>Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score.</source>
          <target state="translated">내부 샘플 수가 최대 인 경우 적합 모델을 최상의 모델로 저장하십시오. 현재 추정 모델에 동일한 수의 이너가 있으면 점수가 더 좋은 경우에만 최상의 모델로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="94b03c70b196c58604c0a7faf7218bf6901b8e0c" translate="yes" xml:space="preserve">
          <source>Scalability</source>
          <target state="translated">Scalability</target>
        </trans-unit>
        <trans-unit id="461b60146605d928fa9b9a5fb416ca167c84c880" translate="yes" xml:space="preserve">
          <source>Scalability and stability improvements to KMeans</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="199d842d852910d77fdbfb1dc59f0d2885e1b21f" translate="yes" xml:space="preserve">
          <source>Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function.</source>
          <target state="translated">get_bin_seeds 함수에서 더 높은 값의 min_bin_freq를 사용하여 더 적은 시드를 사용하여 확장 성을 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="29f2c344812c53bdbb5e427694d6be2d492aaf47" translate="yes" xml:space="preserve">
          <source>Scalability, due to the sequential nature of boosting it can hardly be parallelized.</source>
          <target state="translated">부스팅의 순차적 특성으로 인해 확장 성이 거의 병렬화 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="f6484bee2609587949da674a2bdf0fe83ede7b2a" translate="yes" xml:space="preserve">
          <source>Scalability:</source>
          <target state="translated">Scalability:</target>
        </trans-unit>
        <trans-unit id="61dc05feb549eab6c07b7a94be612c538f71b094" translate="yes" xml:space="preserve">
          <source>Scalable Linear Support Vector Machine for classification implemented using liblinear. Check the See also section of LinearSVC for more comparison element.</source>
          <target state="translated">liblinear를 사용하여 구현 된 분류를위한 확장 가능한 선형 지원 벡터 머신. 자세한 비교 요소는 LinearSVC의 섹션도 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="eecb89d050bc91f7d55354e38239145e4acf15ef" translate="yes" xml:space="preserve">
          <source>Scalable Linear Support Vector Machine for regression implemented using liblinear.</source>
          <target state="translated">liblinear를 사용하여 구현 된 회귀를위한 확장 가능한 선형 지원 벡터 머신.</target>
        </trans-unit>
        <trans-unit id="22b700f6b9ee53c2fb9ac81cf84c12516aa516e1" translate="yes" xml:space="preserve">
          <source>Scalable linear Support Vector Machine for classification using liblinear.</source>
          <target state="translated">liblinear를 사용한 분류를위한 확장 가능한 선형 지원 벡터 머신.</target>
        </trans-unit>
        <trans-unit id="bdabc7bc958d2928d9827e9b54f6956c7bb824f2" translate="yes" xml:space="preserve">
          <source>Scale back the data to the original representation</source>
          <target state="translated">데이터를 원래 표현으로 축소</target>
        </trans-unit>
        <trans-unit id="561dee1ec35178a67790d71472d188769f1e1bd6" translate="yes" xml:space="preserve">
          <source>Scale each feature by its maximum absolute value.</source>
          <target state="translated">각 기능의 최대 절대 값을 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="2d3a1b9b18053dd9fb9c5426f583cf0f7d587a14" translate="yes" xml:space="preserve">
          <source>Scale each feature of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.</source>
          <target state="translated">(n_samples, n_features) 모양을 가정하여 호출자가 제공 한 특정 스케일을 곱하여 데이터 매트릭스의 각 기능을 스케일링하십시오.</target>
        </trans-unit>
        <trans-unit id="c6c81268e0182a1a807ca4c628dc76b97a0fa5dc" translate="yes" xml:space="preserve">
          <source>Scale each feature to the [-1, 1] range without breaking the sparsity.</source>
          <target state="translated">희소성을 해치지 않고 각 기능을 [-1, 1] 범위로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="5deb9ce023c33b22d107e28507be5494ec70764b" translate="yes" xml:space="preserve">
          <source>Scale each non zero row of X to unit norm</source>
          <target state="translated">X의 0이 아닌 각 행을 단위 규범으로 스케일</target>
        </trans-unit>
        <trans-unit id="617d6ad46bfccaf27b4ba0f374d21f46a99d594e" translate="yes" xml:space="preserve">
          <source>Scale each row of the data matrix by multiplying with specific scale provided by the caller assuming a (n_samples, n_features) shape.</source>
          <target state="translated">(n_samples, n_features) 모양을 가정하고 호출자가 제공 한 특정 스케일을 곱하여 데이터 매트릭스의 각 행을 스케일링하십시오.</target>
        </trans-unit>
        <trans-unit id="2febfe8f8ec8796f6ba7a4455156e82486b6b9ad" translate="yes" xml:space="preserve">
          <source>Scale factor between inner and outer circle.</source>
          <target state="translated">내부 원과 외부 원 사이의 배율.</target>
        </trans-unit>
        <trans-unit id="c9aadd325b03483aa8cbb4258dd4942f088c00d2" translate="yes" xml:space="preserve">
          <source>Scale features of X according to feature_range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="651dfd0de4ed652c75c33e720387c590f106c0bd" translate="yes" xml:space="preserve">
          <source>Scale features using statistics that are robust to outliers.</source>
          <target state="translated">특이 치에 강력한 통계를 사용하여 기능을 확장합니다.</target>
        </trans-unit>
        <trans-unit id="ea253b52225bff13ccf219a7ede865331cee2a5e" translate="yes" xml:space="preserve">
          <source>Scale input vectors individually to unit norm (vector length).</source>
          <target state="translated">입력 벡터를 개별적으로 단위 규범 (벡터 길이)으로 조정합니다.</target>
        </trans-unit>
        <trans-unit id="3db146d5ef4350db484651a2947cc4449aa1c920" translate="yes" xml:space="preserve">
          <source>Scale mixture parameter</source>
          <target state="translated">스케일 혼합물 파라미터</target>
        </trans-unit>
        <trans-unit id="214cf07e698e140c0ab386ee80be892f7e60d56f" translate="yes" xml:space="preserve">
          <source>Scale the data</source>
          <target state="translated">데이터 스케일</target>
        </trans-unit>
        <trans-unit id="a00231cc0fa6cda008f7de726dc3328122b68e67" translate="yes" xml:space="preserve">
          <source>Scaled data has zero mean and unit variance:</source>
          <target state="translated">스케일링 된 데이터에는 평균 및 단위 분산이 없습니다.</target>
        </trans-unit>
        <trans-unit id="28f5624ffdfd0dbb670e710c5400ff826061c8e3" translate="yes" xml:space="preserve">
          <source>Scalers are linear (or more precisely affine) transformers and differ from each other in the way to estimate the parameters used to shift and scale each feature.</source>
          <target state="translated">스케일러는 선형 (또는보다 정밀한) 변압기이며 각 기능을 이동 및 스케일링하는 데 사용되는 매개 변수를 추정하는 방식에서 서로 다릅니다.</target>
        </trans-unit>
        <trans-unit id="42fb0a5f800741efdeeef6c8d3f6efbb496929e6" translate="yes" xml:space="preserve">
          <source>Scaling a 1D array</source>
          <target state="translated">1D 배열 확장</target>
        </trans-unit>
        <trans-unit id="5e180a611580dedaac6cdcb57565a42487e31efa" translate="yes" xml:space="preserve">
          <source>Scaling features of X according to feature_range.</source>
          <target state="translated">feature_range에 따른 X의 스케일링 기능.</target>
        </trans-unit>
        <trans-unit id="faa375cb6d0913845d11a421f85f4fc1917244d4" translate="yes" xml:space="preserve">
          <source>Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.</source>
          <target state="translated">예를 들어 단위 규범에 대한 입력 스케일링은 텍스트 분류 또는 클러스터링에 대한 일반적인 작업입니다. 예를 들어, 2 개의 l2 정규화 된 TF-IDF 벡터의 내적은 벡터의 코사인 유사성이며 정보 검색 커뮤니티에서 일반적으로 사용하는 벡터 공간 모델의 기본 유사성 메트릭입니다.</target>
        </trans-unit>
        <trans-unit id="c9df043572b1169b126da4f0a95f811ab4322363" translate="yes" xml:space="preserve">
          <source>Scaling of the features in the space spanned by the class centroids.</source>
          <target state="translated">클래스 중심이 차지하는 공간의 기능 스케일링.</target>
        </trans-unit>
        <trans-unit id="b5c5a5ab3639fcbfa287bfb00c2baa97d840c68a" translate="yes" xml:space="preserve">
          <source>Scaling of the features in the space spanned by the class centroids. Only available for &amp;lsquo;svd&amp;rsquo; and &amp;lsquo;eigen&amp;rsquo; solvers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f91f363e9d78c82d08c38c5a1dbde0b091a85898" translate="yes" xml:space="preserve">
          <source>Scaling parameter of the chi2 kernel.</source>
          <target state="translated">chi2 커널의 스케일링 매개 변수.</target>
        </trans-unit>
        <trans-unit id="611f59db789837a47c8391146e294e88684d2aac" translate="yes" xml:space="preserve">
          <source>Scaling the regularization parameter for SVCs</source>
          <target state="translated">SVC의 정규화 매개 변수 스케일링</target>
        </trans-unit>
        <trans-unit id="8ca361aee1b505e96263673a562173e09064f7c8" translate="yes" xml:space="preserve">
          <source>Scaling vs Whitening</source>
          <target state="translated">스케일링 및 미백</target>
        </trans-unit>
        <trans-unit id="02ba5e6e4d2072a725717b83a0dcd2c4ccfb8e6e" translate="yes" xml:space="preserve">
          <source>Sch&amp;ouml;lkopf et. al &lt;a href=&quot;https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf&quot;&gt;New Support Vector Algorithms&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f575a7be2bbae52450cf10acab0f42ab527bed47" translate="yes" xml:space="preserve">
          <source>Schubert, E., Sander, J., Ester, M., Kriegel, H. P., &amp;amp; Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 19.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35b3e8e65e06fb91614a9faf2b4d8410e9e9072a" translate="yes" xml:space="preserve">
          <source>Schubert, Erich, Michael Gertz. &amp;ldquo;Improving the Cluster Structure Extracted from OPTICS Plots.&amp;rdquo; Proc. of the Conference &amp;ldquo;Lernen, Wissen, Daten, Analysen&amp;rdquo; (LWDA) (2018): 318-329.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ac4d036f5e40e9fc63fcd2b4959a2b29e290cfe" translate="yes" xml:space="preserve">
          <source>Scikit-learn 0.21 introduced two new experimental implementations of gradient boosting trees, namely &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, inspired by &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt; (See &lt;a href=&quot;#lightgbm&quot; id=&quot;id24&quot;&gt;[LightGBM]&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb84d52fa4915e02b24a271ab0aad024c4056d13" translate="yes" xml:space="preserve">
          <source>Scikit-learn 0.21 introduces two new experimental implementations of gradient boosting trees, namely &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, inspired by &lt;a href=&quot;https://github.com/Microsoft/LightGBM&quot;&gt;LightGBM&lt;/a&gt; (See &lt;a href=&quot;#lightgbm&quot; id=&quot;id14&quot;&gt;[LightGBM]&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b38453e586dafca0c0308eafbfda226dcf7f7c2" translate="yes" xml:space="preserve">
          <source>Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those image can be useful to test algorithms and pipeline on 2D data.</source>
          <target state="translated">Scikit-learn은 또한 작성자가 Creative Commons 라이센스로 게시 한 몇 가지 샘플 JPEG 이미지도 포함합니다. 이러한 이미지는 2D 데이터에서 알고리즘 및 파이프 라인을 테스트하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aad0f03512858b9b99ff0fe8b0c0fddc5ad0d78e" translate="yes" xml:space="preserve">
          <source>Scikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f09669bc19a45a6af1925c5dd7e5320d97b9858" translate="yes" xml:space="preserve">
          <source>Scikit-learn also permits evaluation of multiple metrics in &lt;code&gt;GridSearchCV&lt;/code&gt;, &lt;code&gt;RandomizedSearchCV&lt;/code&gt; and &lt;code&gt;cross_validate&lt;/code&gt;.</source>
          <target state="translated">Scikit-learn은 또한 &lt;code&gt;GridSearchCV&lt;/code&gt; , &lt;code&gt;RandomizedSearchCV&lt;/code&gt; 및 &lt;code&gt;cross_validate&lt;/code&gt; 의 여러 메트릭을 평가할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5427512908da9e885639e4f06d90d3fbb899fa1a" translate="yes" xml:space="preserve">
          <source>Scikit-learn deals with learning information from one or more datasets that are represented as 2D arrays. They can be understood as a list of multi-dimensional observations. We say that the first axis of these arrays is the &lt;strong&gt;samples&lt;/strong&gt; axis, while the second is the &lt;strong&gt;features&lt;/strong&gt; axis.</source>
          <target state="translated">Scikit-learn은 2D 배열로 표시되는 하나 이상의 데이터 집합의 학습 정보를 처리합니다. 그것들은 다차원 관측의 목록으로 이해 될 수 있습니다. 이 배열의 첫 번째 축은 &lt;strong&gt;샘플&lt;/strong&gt; 축이고 두 번째 축은 &lt;strong&gt;피처&lt;/strong&gt; 축이라고합니다.</target>
        </trans-unit>
        <trans-unit id="f9977ec0fe68c8f9bbc6588808c28cd22800a19b" translate="yes" xml:space="preserve">
          <source>Scikit-learn defines a simple API for creating visualizations for machine learning. The key features of this API is to allow for quick plotting and visual adjustments without recalculation. In this example, we will demonstrate how to use the visualization API by comparing ROC curves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68787e98ea90825b478bf4a016c311205a6818e9" translate="yes" xml:space="preserve">
          <source>Scikit-learn does some validation on data that increases the overhead per call to &lt;code&gt;predict&lt;/code&gt; and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable &lt;code&gt;SKLEARN_ASSUME_FINITE&lt;/code&gt; to a non-empty string before importing scikit-learn, or configure it in Python with &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;sklearn.set_config&lt;/code&gt;&lt;/a&gt;. For more control than these global settings, a &lt;code&gt;config_context&lt;/code&gt; allows you to set this configuration within a specified context:</source>
          <target state="translated">Scikit-learn은 &lt;code&gt;predict&lt;/code&gt; 및 유사한 기능에 대한 호출 당 오버 헤드를 증가시키는 데이터에 대한 일부 유효성 검사를 수행합니다. 특히, 피처가 유한한지 (NaN이 아니거나 무한한지) 확인하면 데이터를 완전히 통과해야합니다. 데이터가 허용 가능한지 확인하는 경우 scikit-learn을 가져 오기 전에 환경 변수 &lt;code&gt;SKLEARN_ASSUME_FINITE&lt;/code&gt; 를 비어 있지 않은 문자열 로 설정하여 유한 검사를 억제 하거나 &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;sklearn.set_config&lt;/code&gt; 를 사용&lt;/a&gt; 하여 Python에서 구성 할 수 있습니다. 이러한 전역 설정보다 더 많은 제어를 위해 &lt;code&gt;config_context&lt;/code&gt; 를 사용하면 지정된 컨텍스트 내에서이 구성을 설정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="56e4295ba33d7d0b066dcb5a29b6f828761a1e6b" translate="yes" xml:space="preserve">
          <source>Scikit-learn generally relies on the &lt;code&gt;loky&lt;/code&gt; backend, which is joblib&amp;rsquo;s default backend. Loky is a multi-processing backend. When doing multi-processing, in order to avoid duplicating the memory in each process (which isn&amp;rsquo;t reasonable with big datasets), joblib will create a &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html&quot;&gt;memmap&lt;/a&gt; that all processes can share, when the data is bigger than 1MB.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="562455165c34b7c83008b571c9bd46eceb879b92" translate="yes" xml:space="preserve">
          <source>Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies.</source>
          <target state="translated">Scikit-learn에는 인기있는 교차 유효성 검사 전략에 대한 기차 / 테스트 지수 목록을 생성하는 데 사용할 수있는 클래스 모음이 있습니다.</target>
        </trans-unit>
        <trans-unit id="b670925eafc995f1763e66f682772271e15a05e5" translate="yes" xml:space="preserve">
          <source>Scikit-learn implements different classes to estimate Gaussian mixture models, that correspond to different estimation strategies, detailed below.</source>
          <target state="translated">Scikit-learn은 서로 다른 추정 전략에 해당하는 가우시안 혼합 모델을 추정하기 위해 다양한 클래스를 구현합니다 (아래에 자세히 설명 됨).</target>
        </trans-unit>
        <trans-unit id="e9a530527422759264cd84546f6a0bd4a26252b3" translate="yes" xml:space="preserve">
          <source>Scikit-learn implements efficient kernel density estimation using either a Ball Tree or KD Tree structure, through the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator. The available kernels are shown in the second figure of this example.</source>
          <target state="translated">Scikit-learn은 &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; Estimator를 통해 Ball Tree 또는 KD Tree 구조를 사용하여 효율적인 커널 밀도 추정을 구현합니다 . 사용 가능한 커널은이 예제의 두 번째 그림에 나와 있습니다.</target>
        </trans-unit>
        <trans-unit id="b29477e8796624fa3eb37a4b942da5b84d872c57" translate="yes" xml:space="preserve">
          <source>Scikit-learn is a Python module integrating classic machine learning algorithms in the tightly-knit world of scientific Python packages (&lt;a href=&quot;http://www.scipy.org&quot;&gt;NumPy&lt;/a&gt;, &lt;a href=&quot;http://www.scipy.org&quot;&gt;SciPy&lt;/a&gt;, &lt;a href=&quot;http://matplotlib.org&quot;&gt;matplotlib&lt;/a&gt;).</source>
          <target state="translated">Scikit-learn은 밀접하게 짜여진 과학적 파이썬 패키지 ( &lt;a href=&quot;http://www.scipy.org&quot;&gt;NumPy&lt;/a&gt; , &lt;a href=&quot;http://www.scipy.org&quot;&gt;SciPy&lt;/a&gt; , &lt;a href=&quot;http://matplotlib.org&quot;&gt;matplotlib&lt;/a&gt; )에 고전적인 머신 러닝 알고리즘을 통합 한 Python 모듈입니다 .</target>
        </trans-unit>
        <trans-unit id="8efe5ffe4058ee23f32fcb7e77b39f35cffa4409" translate="yes" xml:space="preserve">
          <source>Scikit-learn is a Python module integrating classic machine learning algorithms in the tightly-knit world of scientific Python packages (&lt;a href=&quot;https://www.numpy.org/&quot;&gt;NumPy&lt;/a&gt;, &lt;a href=&quot;https://scipy.org/&quot;&gt;SciPy&lt;/a&gt;, &lt;a href=&quot;https://matplotlib.org/&quot;&gt;matplotlib&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5244e6b9c3228eb8fcf423888d9d9a31c68e2222" translate="yes" xml:space="preserve">
          <source>Scikit-learn offers a more efficient implementation for the construction of decision trees. A naive implementation (as above) would recompute the class label histograms (for classification) or the means (for regression) at for each new split point along a given feature. Presorting the feature over all relevant samples, and retaining a running label count, will reduce the complexity at each node to \(O(n_{features}\log(n_{samples}))\), which results in a total cost of \(O(n_{features}n_{samples}\log(n_{samples}))\). This is an option for all tree based algorithms. By default it is turned on for gradient boosting, where in general it makes training faster, but turned off for all other algorithms as it tends to slow down training when training deep trees.</source>
          <target state="translated">Scikit-learn은 의사 결정 트리 구성을위한보다 효율적인 구현을 제공합니다. (위와 같이) 순진한 구현은 주어진 지형지 물을 따라 각각의 새로운 분리 점에 대한 클래스 레이블 히스토그램 (분류) 또는 평균 (회귀)을 재 계산합니다. 모든 관련 샘플에 대해 기능을 미리 정렬하고 레이블 수를 유지하면 각 노드의 복잡성이 \ (O (n_ {features} \ log (n_ {samples})) \)로 감소하여 총 비용이 \ (O (n_ {기능} n_ {samples} \ log (n_ {samples})) \). 이것은 모든 트리 기반 알고리즘에 대한 옵션입니다. 기본적으로 그라디언트 부스팅을 위해 켜져 있으며 일반적으로 훈련 속도는 빨라지지만 다른 모든 알고리즘에서는 딥 트리를 훈련 할 때 훈련 속도가 느려질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4affb29f4cf970be26e1f3befb3e52980b3745a3" translate="yes" xml:space="preserve">
          <source>Scikit-learn provides 3 robust regression estimators: &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;, &lt;a href=&quot;#theil-sen-regression&quot;&gt;Theil Sen&lt;/a&gt; and &lt;a href=&quot;#huber-regression&quot;&gt;HuberRegressor&lt;/a&gt;</source>
          <target state="translated">Scikit-learn은 &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; , &lt;a href=&quot;#theil-sen-regression&quot;&gt;Theil Sen&lt;/a&gt; 및 &lt;a href=&quot;#huber-regression&quot;&gt;HuberRegressor의&lt;/a&gt; 3 가지 강력한 회귀 추정값을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="54ef66758fb920f45dd137b20e476f9003566169" translate="yes" xml:space="preserve">
          <source>Scikit-learn provides 3 robust regression estimators: &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;, &lt;a href=&quot;#theil-sen-regression&quot;&gt;Theil Sen&lt;/a&gt; and &lt;a href=&quot;#huber-regression&quot;&gt;HuberRegressor&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf6e9e896240b0867bdae025d2a37105852d3491" translate="yes" xml:space="preserve">
          <source>Scikit-learn relies heavily on NumPy and SciPy, which internally call multi-threaded linear algebra routines implemented in libraries such as MKL, OpenBLAS or BLIS.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="760c9dce2728b87b0e773a2a2023bd5ef6b1ebc6" translate="yes" xml:space="preserve">
          <source>Scikit-learn uses the &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/&quot;&gt;joblib&lt;/a&gt; library to enable parallel computing inside its estimators. See the joblib documentation for the switches to control parallel computing.</source>
          <target state="translated">Scikit-learn은 &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/&quot;&gt;joblib&lt;/a&gt; 라이브러리를 사용하여 추정기 내에서 병렬 컴퓨팅을 가능하게합니다. 병렬 컴퓨팅을 제어하는 ​​스위치에 대해서는 joblib 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="dbdfa5cbcc37d085da70cbad1d49bb4154a25ae3" translate="yes" xml:space="preserve">
          <source>Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you don&amp;rsquo;t store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse (&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;CSR or CSC&lt;/a&gt;) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6.</source>
          <target state="translated">Scipy는 희소 데이터 저장에 최적화 된 희소 행렬 데이터 구조를 제공합니다. 스파 스 형식의 주요 기능은 0을 저장하지 않으므로 데이터가 드문 경우 훨씬 적은 메모리를 사용한다는 것입니다. 희소 ( &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;CSR 또는 CSC&lt;/a&gt; ) 표현 에서 0이 아닌 값은 평균적으로 하나의 32 비트 정수 위치 + 64 비트 부동 소수점 값 + 행렬의 행 또는 열당 추가 32 비트 만 취합니다. 밀도가 높은 (또는 희소 한) 선형 모델에서 희소 입력을 사용하면 값이 0이 아닌 피처 만 내적과 모델 예측에 영향을 미치기 때문에 예측 속도를 약간 높일 수 있습니다. 따라서 1e6 차원 공간에 100이 아닌 0이있는 경우 100을 곱하고 1e6 대신 연산을 추가하면됩니다.</target>
        </trans-unit>
        <trans-unit id="41590fea612397630e8b90182fcd88974e84be1f" translate="yes" xml:space="preserve">
          <source>Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you don&amp;rsquo;t store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse (&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;CSR or CSC&lt;/a&gt;) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73ff9df76cca7434e9bdc94c5c539e98a711a167" translate="yes" xml:space="preserve">
          <source>Scipy sparse matrix formats documentation</source>
          <target state="translated">Scipy 희소 행렬 형식 설명서</target>
        </trans-unit>
        <trans-unit id="ec44ac6f635d9837f888fea19337ecd3bc1dc78d" translate="yes" xml:space="preserve">
          <source>Score function (or loss function) with signature &lt;code&gt;score_func(y, y_pred, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">서명과 점수 함수 (또는 손실 함수) &lt;code&gt;score_func(y, y_pred, **kwargs)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e8a42d4da772627d057fecc9d05d1c5e0de456df" translate="yes" xml:space="preserve">
          <source>Score of base estimator with best alpha.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4106362aa56af5a012e22c8aae43583defb47511" translate="yes" xml:space="preserve">
          <source>Score of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict (X) wrt의 점수 와이.</target>
        </trans-unit>
        <trans-unit id="269ca6f46a2303fa294fd49cbab7d3d725c81bb2" translate="yes" xml:space="preserve">
          <source>Score of the prediction.</source>
          <target state="translated">예측 점수.</target>
        </trans-unit>
        <trans-unit id="fee68e2ae75f4d785b563d7d07175bca2df697af" translate="yes" xml:space="preserve">
          <source>Score of the training dataset obtained using an out-of-bag estimate.</source>
          <target state="translated">가방 외부 견적을 사용하여 얻은 훈련 데이터 세트의 점수.</target>
        </trans-unit>
        <trans-unit id="d47762b2bcf100a0eee50ed9f7a0b022146b7037" translate="yes" xml:space="preserve">
          <source>Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when &lt;code&gt;oob_score&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a728e01859e5aacfa8054e6c7ac1cbc91bfd77d" translate="yes" xml:space="preserve">
          <source>Score of this parameter setting on given test split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fdd43514c35f028b5dfc878c7661a274717e7633" translate="yes" xml:space="preserve">
          <source>Score of this parameter setting on given training / test split.</source>
          <target state="translated">주어진 훈련 / 테스트 스플릿에서이 파라미터 설정의 점수.</target>
        </trans-unit>
        <trans-unit id="e14703c8615f59873dec25794c9dae3e6fdaa2e4" translate="yes" xml:space="preserve">
          <source>Score, and cross-validated scores</source>
          <target state="translated">점수 및 교차 검증 된 점수</target>
        </trans-unit>
        <trans-unit id="5ac699be69d4f6f03061e9fdf043eeb8d0ba0ed6" translate="yes" xml:space="preserve">
          <source>Scorer function used on the held out data to choose the best parameters for the model.</source>
          <target state="translated">보류 된 데이터에 사용 된 스코어러 기능은 모델에 가장 적합한 매개 변수를 선택합니다.</target>
        </trans-unit>
        <trans-unit id="2d120255d84deeb73e3a5484f87eef4818f21119" translate="yes" xml:space="preserve">
          <source>Scorer to use. It can be a single string (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;). If None, the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dafe5cdb100f4bad5185f8a89151e9de127407db" translate="yes" xml:space="preserve">
          <source>Scores of all outputs are averaged with uniform weight.</source>
          <target state="translated">모든 출력의 점수는 균일 한 가중치로 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="a879b57711ff565bb3c57b9cbdbf3f09144796d5" translate="yes" xml:space="preserve">
          <source>Scores of all outputs are averaged, weighted by the variances of each individual output.</source>
          <target state="translated">모든 산출물의 점수는 각 개별 산출의 분산에 의해 가중되어 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="bad96478d4d42d160afd8f51da6516a096f00968" translate="yes" xml:space="preserve">
          <source>Scores of features.</source>
          <target state="translated">기능의 점수.</target>
        </trans-unit>
        <trans-unit id="004421c31cff3e85919b0da3d9213b70c070211e" translate="yes" xml:space="preserve">
          <source>Scores on test set.</source>
          <target state="translated">시험 세트에 대한 점수.</target>
        </trans-unit>
        <trans-unit id="9cb2f72d484e4d0e25f5504cf3cb781f6831387a" translate="yes" xml:space="preserve">
          <source>Scores on training sets.</source>
          <target state="translated">훈련 세트에 대한 점수.</target>
        </trans-unit>
        <trans-unit id="a6e081bd4fc687e97f2a3c1767e01a47d8d0d090" translate="yes" xml:space="preserve">
          <source>Scoring</source>
          <target state="translated">Scoring</target>
        </trans-unit>
        <trans-unit id="1d35080d3b512f65a9672a3ec57c49c5c7d18fa7" translate="yes" xml:space="preserve">
          <source>Scoring parameter to use for early stopping. It can be a single string (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;). If None, the estimator&amp;rsquo;s default scorer is used. If &lt;code&gt;scoring='loss'&lt;/code&gt;, early stopping is checked w.r.t the loss value. Only used if early stopping is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88173ba266bcaafd44bc526091a11bf84a691634" translate="yes" xml:space="preserve">
          <source>Second example</source>
          <target state="translated">두 번째 예</target>
        </trans-unit>
        <trans-unit id="01969fb763d816468d958ddf55fbcfeb6492bbfe" translate="yes" xml:space="preserve">
          <source>Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter &lt;code&gt;n_jobs&lt;/code&gt;, which might not be available in all estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4e2db45443f0fd4ae50799405ca1ddce61eefb7" translate="yes" xml:space="preserve">
          <source>Second, when using a connectivity matrix, single, average and complete linkage are unstable and tend to create a few clusters that grow very quickly. Indeed, average and complete linkage fight this percolation behavior by considering all the distances between two clusters when merging them ( while single linkage exaggerates the behaviour by considering only the shortest distance between clusters). The connectivity graph breaks this mechanism for average and complete linkage, making them resemble the more brittle single linkage. This effect is more pronounced for very sparse graphs (try decreasing the number of neighbors in kneighbors_graph) and with complete linkage. In particular, having a very small number of neighbors in the graph, imposes a geometry that is close to that of single linkage, which is well known to have this percolation instability.</source>
          <target state="translated">둘째, 연결 매트릭스를 사용할 때 단일, 평균 및 전체 연결이 불안정하고 매우 빠르게 성장하는 몇 개의 클러스터를 만드는 경향이 있습니다. 실제로, 평균적이고 완전한 연계는 두 클러스터 사이의 모든 거리를 병합 할 때이 퍼콜 레이션 동작에 맞서 싸 웁니다 (단일 링크는 클러스터 사이의 최단 거리 만 고려하여 동작을 과장합니다). 연결성 그래프는 평균 및 전체 연결에 대해이 메커니즘을 분리하여보다 취약한 단일 연결과 유사합니다. 이 효과는 매우 드문 그래프 (kneighbors_graph의 이웃 수를 줄이십시오)와 완전한 연결에 대해 더욱 두드러집니다. 특히 그래프에 매우 적은 수의 이웃이 있으면 단일 링키지의 형상에 가까운 형상이 적용됩니다.이 침투 불안정성을 갖는 것으로 잘 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="d90e68437e39bd56831ac8f750d7707399d6fbb2" translate="yes" xml:space="preserve">
          <source>Secondly, the squared loss function is replaced by the unit deviance \(d\) of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM) &lt;a href=&quot;#id34&quot; id=&quot;id32&quot;&gt;11&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53553e63fb4cb7ed1ae88d54b317154b9777e613" translate="yes" xml:space="preserve">
          <source>Seconds used for refitting the best model on the whole dataset.</source>
          <target state="translated">전체 데이터 세트에서 최상의 모델을 다시 계산하는 데 사용되는 초입니다.</target>
        </trans-unit>
        <trans-unit id="a31532b7821dad04a81b3f8844e44fc4c4735bbf" translate="yes" xml:space="preserve">
          <source>Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa04a576bd56d584afacdf30a223dc3f1fde4eb3" translate="yes" xml:space="preserve">
          <source>Section 5.4.4, pp. 252-253.</source>
          <target state="translated">섹션 5.4.4, 252-253 페이지.</target>
        </trans-unit>
        <trans-unit id="d4628726ca2b8e9b183e1257c782a69297176123" translate="yes" xml:space="preserve">
          <source>Section contents</source>
          <target state="translated">섹션 내용</target>
        </trans-unit>
        <trans-unit id="978feab4a35a4b897d5316b48dac562d3091d0c5" translate="yes" xml:space="preserve">
          <source>See &amp;ldquo;Random Features for Large-Scale Kernel Machines&amp;rdquo; by A. Rahimi and Benjamin Recht.</source>
          <target state="translated">A. Rahimi와 Benjamin Recht의&amp;ldquo;대규모 커널 시스템의 임의 기능&amp;rdquo;을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="b31673babc80845a872201a18703583634f75350" translate="yes" xml:space="preserve">
          <source>See &amp;ldquo;Random Fourier Approximations for Skewed Multiplicative Histogram Kernels&amp;rdquo; by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.</source>
          <target state="translated">Fuxin Li, Catalin Ionescu 및 Cristian Sminchisescu의&amp;ldquo;비뚤어진 곱셈 히스토그램 커널에 대한 랜덤 푸리에 근사치&amp;rdquo;를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="03ac02c3ddabaad90ce0d4962bc965c10cba4eeb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#r95f74c4622c1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, Chapter 4, Section 4.2, for further details regarding the DotProduct kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0df8f05c9ec568b4cc1c4d54a3085f0ebf794bd9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#rw2006&quot; id=&quot;id6&quot;&gt;[RW2006]&lt;/a&gt;, pp84 for further details regarding the different variants of the Mat&amp;eacute;rn kernel.</source>
          <target state="translated">Mat&amp;eacute;rn 커널의 다양한 변형에 대한 자세한 내용 은 &lt;a href=&quot;#rw2006&quot; id=&quot;id6&quot;&gt;[RW2006]&lt;/a&gt; , pp84를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="dc2d327bd01f6b4c26633dc7230267bf2994fc43" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#rw2006&quot; id=&quot;id7&quot;&gt;[RW2006]&lt;/a&gt;, pp84 for further details regarding the different variants of the Mat&amp;eacute;rn kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd13548eb92de90d6303ee640236c96cda68888f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#svm-mathematical-formulation&quot;&gt;Mathematical formulation&lt;/a&gt; for a complete description of the decision function.</source>
          <target state="translated">결정 기능에 대한 자세한 설명은 &lt;a href=&quot;#svm-mathematical-formulation&quot;&gt;수학 공식&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="90eaaa0456af60c46de8c81dc16ee15dad424d81" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../auto_examples/compose/plot_transformed_target#sphx-glr-auto-examples-compose-plot-transformed-target-py&quot;&gt;examples/compose/plot_transformed_target.py&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/compose/plot_transformed_target#sphx-glr-auto-examples-compose-plot-transformed-target-py&quot;&gt;examples / compose / plot_transformed_target.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0d70be16f79f29cfb466970ca83989e73f6c00bb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../auto_examples/linear_model/plot_polynomial_interpolation#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py&quot;&gt;examples/linear_model/plot_polynomial_interpolation.py&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/linear_model/plot_polynomial_interpolation#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py&quot;&gt;examples / linear_model / plot_polynomial_interpolation.py를&lt;/a&gt; 참조하십시오</target>
        </trans-unit>
        <trans-unit id="66218b0f5237d32e41f01914713a47022cf20bb9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../auto_examples/model_selection/plot_learning_curve#sphx-glr-auto-examples-model-selection-plot-learning-curve-py&quot;&gt;examples/model_selection/plot_learning_curve.py&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/model_selection/plot_learning_curve#sphx-glr-auto-examples-model-selection-plot-learning-curve-py&quot;&gt;examples / model_selection / plot_learning_curve.py를&lt;/a&gt; 참조하십시오</target>
        </trans-unit>
        <trans-unit id="41261fbeb952dd5951bf36801866ce019db00dde" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../auto_examples/model_selection/plot_validation_curve#sphx-glr-auto-examples-model-selection-plot-validation-curve-py&quot;&gt;Plotting Validation Curves&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/model_selection/plot_validation_curve#sphx-glr-auto-examples-model-selection-plot-validation-curve-py&quot;&gt;검증 곡선 플로팅&lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="6a3b0e6c0d79b7c03e9dbb288652a52d5e7d7fd5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../modules/linear_model#bayesian-ridge-regression&quot;&gt;Bayesian Ridge Regression&lt;/a&gt; for more information on the regressor.</source>
          <target state="translated">&lt;a href=&quot;../../modules/linear_model#bayesian-ridge-regression&quot;&gt;회귀기에&lt;/a&gt; 대한 자세한 내용 은 베이지안 릿지 회귀 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="824c64b490bd5bd779a22eec474ba042707228d1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../modules/linear_model#theil-sen-regression&quot;&gt;Theil-Sen estimator: generalized-median-based estimator&lt;/a&gt; for more information on the regressor.</source>
          <target state="translated">참조 &lt;a href=&quot;../../modules/linear_model#theil-sen-regression&quot;&gt;Theil-센 추정을 : 일반 - 중간 기반의 추정을&lt;/a&gt; 회귀 변수에 대한 자세한 내용은.</target>
        </trans-unit>
        <trans-unit id="36bf338f19ee54a0199babdefd0eaf5b203f80f4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../modules/mixture#gmm&quot;&gt;Gaussian mixture models&lt;/a&gt; for more information on the estimator.</source>
          <target state="translated">참조 &lt;a href=&quot;../../modules/mixture#gmm&quot;&gt;가우시안 혼합 모델을&lt;/a&gt; 추에 대한 자세한 내용은.</target>
        </trans-unit>
        <trans-unit id="bda58af52a4d947e4c6e336facf5860c69c8478b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision tree&lt;/a&gt; for more information on the estimator.</source>
          <target state="translated">추정기에 대한 자세한 정보는 &lt;a href=&quot;../../modules/tree#tree&quot;&gt;의사 결정 트리&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1876d72641fc6bba23917c9ce1b023a0c5308e82" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of using ROC to model species distribution.</source>
          <target state="translated">ROC를 사용하여 &lt;a href=&quot;../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;종 분포를 모델링&lt;/a&gt; 하는 예는 종 분포 모델링 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3ca47154d5f58b185be5af00ff9392b2598d6abf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/calibration/plot_calibration#sphx-glr-auto-examples-calibration-plot-calibration-py&quot;&gt;Probability calibration of classifiers&lt;/a&gt; for an example of Brier score loss usage to perform probability calibration of classifiers.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/calibration/plot_calibration#sphx-glr-auto-examples-calibration-plot-calibration-py&quot;&gt;분류기의 확률 보정&lt;/a&gt; 을 수행하기위한 Brier 점수 손실 사용의 예 는 분류기의 확률 보정을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="282928c19fbfe87fe4167148ff3b6058bc018479" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;Recognizing hand-written digits&lt;/a&gt; for an example of classification report usage for hand-written digits.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;손으로 쓴 숫자에&lt;/a&gt; 대한 분류 보고서 사용법의 예는 손으로 쓴 숫자 인식을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="72d62eaa2236b9a2dd2c8a6bb04573291c57c36e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;Recognizing hand-written digits&lt;/a&gt; for an example of using a confusion matrix to classify hand-written digits.</source>
          <target state="translated">혼동 행렬을 사용하여 손으로 쓴 숫자를 분류하는 예는 &lt;a href=&quot;../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;손으로 쓴 숫자 인식을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="87880060115f16d38cb34f093520114ae1691683" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood&lt;/a&gt; for an example on how to fit a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of likelihood.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;&lt;/a&gt;&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; 객체를 데이터 에 맞추는 방법 및 Ledoit-Wolf 추정기의 성능을 가능성 측면에서 시각화하는 방법에 대한 예는 수축 공분산 추정 : LedoitWolf 대 OAS 및 최대 가능성 을 .</target>
        </trans-unit>
        <trans-unit id="2209166f8c957bd217f6d5f461ba4322d29b02c2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood&lt;/a&gt; for an example on how to fit a &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt;&lt;code&gt;ShrunkCovariance&lt;/code&gt;&lt;/a&gt; object to data.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;&lt;/a&gt;&lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt; &lt;code&gt;ShrunkCovariance&lt;/code&gt; &lt;/a&gt; 객체를 데이터에 맞추는 방법에 대한 예는 수축 공분산 추정 : LedoitWolf vs OAS 및 최대 가능성 을 .</target>
        </trans-unit>
        <trans-unit id="88a19b944edc191f5f7b057963b65a4e9112c1b2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood&lt;/a&gt; for an example on how to fit an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to data.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;&lt;/a&gt;&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 객체를 데이터에 맞추는 방법에 대한 예는 수축 공분산 추정 : LedoitWolf vs OAS 및 최대 가능성 을 .</target>
        </trans-unit>
        <trans-unit id="8a3f5dc7dd1289a56afbdf19c9d4415f754d0c74" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood&lt;/a&gt; for an example on how to fit an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to data.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/covariance/plot_covariance_estimation#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py&quot;&gt;&lt;/a&gt;&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; 객체를 데이터에 맞추는 방법에 대한 예는 수축 공분산 추정 : LedoitWolf vs OAS 및 최대 가능성 을 .</target>
        </trans-unit>
        <trans-unit id="a2ce4b68a58fc4d4775fc307d3337bfc6ba95951" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_lw_vs_oas#sphx-glr-auto-examples-covariance-plot-lw-vs-oas-py&quot;&gt;Ledoit-Wolf vs OAS estimation&lt;/a&gt; to visualize the Mean Squared Error difference between a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; and an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; estimator of the covariance.</source>
          <target state="translated">참조 &lt;a href=&quot;../auto_examples/covariance/plot_lw_vs_oas#sphx-glr-auto-examples-covariance-plot-lw-vs-oas-py&quot;&gt;OAS 추정 대 LedoitWolf을&lt;/a&gt; 사이의 평균 제곱 오류 차이 시각화 &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; 을&lt;/a&gt; 와 공분산 의 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; 추정기 .</target>
        </trans-unit>
        <trans-unit id="8878b5f91edc950d3f968af54f37c8ec353c6370" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_mahalanobis_distances#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py&quot;&gt;Robust covariance estimation and Mahalanobis distances relevance&lt;/a&gt; for an illustration of the difference between using a standard (&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;covariance.EmpiricalCovariance&lt;/code&gt;&lt;/a&gt;) or a robust estimate (&lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt;&lt;code&gt;covariance.MinCovDet&lt;/code&gt;&lt;/a&gt;) of location and covariance to assess the degree of outlyingness of an observation.</source>
          <target state="translated">표준 ( &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;covariance.EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 사용하는 것의 차이에 대한 설명은 &lt;a href=&quot;../auto_examples/covariance/plot_mahalanobis_distances#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py&quot;&gt;강력한 공분산 추정 및 Mahalanobis 거리 관련성&lt;/a&gt; 을 참조하십시오. ) 또는 강력한 추정치 ( &lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt; &lt;code&gt;covariance.MinCovDet&lt;/code&gt; &lt;/a&gt; 관찰 outlyingness의 정도를 평가하기 위해 위치 및 공분산을).</target>
        </trans-unit>
        <trans-unit id="030742fc8b624a6be4238fb99cd9f5a0e364d687" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_mahalanobis_distances#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py&quot;&gt;Robust covariance estimation and Mahalanobis distances relevance&lt;/a&gt; to visualize the difference between &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt;&lt;code&gt;MinCovDet&lt;/code&gt;&lt;/a&gt; covariance estimators in terms of Mahalanobis distance (so we get a better estimate of the precision matrix too).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 의 차이를 시각화하려면 &lt;a href=&quot;../auto_examples/covariance/plot_mahalanobis_distances#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py&quot;&gt;강력한 공분산 추정 및 Mahalanobis 거리 관련성&lt;/a&gt; 을 참조하십시오. 측면에서 와 &lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt; &lt;code&gt;MinCovDet&lt;/code&gt; &lt;/a&gt; 공분산 추정기 (따라서 정밀 행렬의 더 나은 추정값도 얻음).</target>
        </trans-unit>
        <trans-unit id="f9a056a85b47a4a6c1a6d704788263d39761f07a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/covariance/plot_robust_vs_empirical_covariance#sphx-glr-auto-examples-covariance-plot-robust-vs-empirical-covariance-py&quot;&gt;Robust vs Empirical covariance estimate&lt;/a&gt; for an example on how to fit a &lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt;&lt;code&gt;MinCovDet&lt;/code&gt;&lt;/a&gt; object to data and see how the estimate remains accurate despite the presence of outliers.</source>
          <target state="translated">적합 법에 대한 예는 &lt;a href=&quot;../auto_examples/covariance/plot_robust_vs_empirical_covariance#sphx-glr-auto-examples-covariance-plot-robust-vs-empirical-covariance-py&quot;&gt;견고성 대 경험적 공분산 추정치&lt;/a&gt; 를 참조하십시오 .&lt;a href=&quot;generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet&quot;&gt; &lt;code&gt;MinCovDet&lt;/code&gt; 의&lt;/a&gt; 추정은 이상치의 존재에도 불구하고 정확한 남아있는 데이터 객체를 볼.</target>
        </trans-unit>
        <trans-unit id="995b452653e2a34828a53fff1225e032a84a1ed7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/ensemble/plot_gradient_boosting_regression#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py&quot;&gt;Gradient Boosting regression&lt;/a&gt; for an example of mean squared error usage to evaluate gradient boosting regression.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/ensemble/plot_gradient_boosting_regression#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py&quot;&gt;그라디언트 부스팅 회귀&lt;/a&gt; 를 평가하기위한 평균 제곱 오류 사용법의 예는 그라디언트 부스팅 회귀 를 .</target>
        </trans-unit>
        <trans-unit id="c70bb54b9f9c6d372a94de7482636d40519b333f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/ensemble/plot_isolation_forest#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py&quot;&gt;IsolationForest example&lt;/a&gt; for an illustration of the use of IsolationForest.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/ensemble/plot_isolation_forest#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py&quot;&gt;IsolationForest&lt;/a&gt; 사용에 대한 그림은 IsolationForest 예 를 .</target>
        </trans-unit>
        <trans-unit id="462e8cdc93001fb7a3648b1195482e1f8b22fe44" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/feature_selection/plot_permutation_test_for_classification#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py&quot;&gt;Test with permutations the significance of a classification score&lt;/a&gt; for an example of accuracy score usage using permutations of the dataset.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/feature_selection/plot_permutation_test_for_classification#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py&quot;&gt;&lt;/a&gt;데이터 세트의 순열을 사용한 정확도 점수 사용의 예는 순열을 사용한 테스트로 분류 점수의 중요성을 .</target>
        </trans-unit>
        <trans-unit id="39b73a8956b97e37d2a1a56a5487c771ec6755e2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/feature_selection/plot_rfe_with_cross_validation#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py&quot;&gt;Recursive feature elimination with cross-validation&lt;/a&gt; for an example of zero one loss usage to perform recursive feature elimination with cross-validation.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/feature_selection/plot_rfe_with_cross_validation#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py&quot;&gt;교차 유효성 검사&lt;/a&gt; 로 재귀 기능 제거를 수행하기위한 무손실 사용의 예는 교차 유효성 검사 로 재귀 기능 제거를 .</target>
        </trans-unit>
        <trans-unit id="92f30204fbfed37d4520688e6847c0f1dec6d444" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/linear_model/plot_lasso_and_elasticnet#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py&quot;&gt;Lasso and Elastic Net for Sparse Signals&lt;/a&gt; for an example of R&amp;sup2; score usage to evaluate Lasso and Elastic Net on sparse signals.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/linear_model/plot_lasso_and_elasticnet#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py&quot;&gt;스파 스 신호&lt;/a&gt; 에서 Lasso 및 Elastic Net을 평가하기위한 R&amp;sup2; 점수 사용의 예는 스파 스 신호에 대한 Lasso 및 Elastic Net을 .</target>
        </trans-unit>
        <trans-unit id="01a78f9ef24e4fc8896bfa27a1c142f94096fbf4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/linear_model/plot_polynomial_interpolation#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py&quot;&gt;Polynomial interpolation&lt;/a&gt; for Ridge regression using created polynomial features.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/linear_model/plot_polynomial_interpolation#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py&quot;&gt;&lt;/a&gt;작성된 다항식 피처를 사용하여 릿지 회귀에 대한 다항식 보간 을 .</target>
        </trans-unit>
        <trans-unit id="6cc1903f7fccc8472139b491e9c35281e331be73" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/manifold/plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt; for an example of dimensionality reduction on a toy &amp;ldquo;S-curve&amp;rdquo; dataset.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/manifold/plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;&lt;/a&gt;장난감 &quot;S- 곡선&quot;데이터 세트의 차원 축소에 대한 예 는 매니 폴드 학습 방법 비교를 .</target>
        </trans-unit>
        <trans-unit id="c13f7bab27de634fe8533a5ffc3f4d5d442fb940" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/manifold/plot_lle_digits#sphx-glr-auto-examples-manifold-plot-lle-digits-py&quot;&gt;Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;&lt;/a&gt; for an example of dimensionality reduction on handwritten digits.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/manifold/plot_lle_digits#sphx-glr-auto-examples-manifold-plot-lle-digits-py&quot;&gt;필기 자릿수&lt;/a&gt; 의 차원 축소에 대한 예는 자필 자릿수에 대한 매니 폴드 학습 : 로컬 선형 임베딩, 아이소 맵&amp;hellip; 을 .</target>
        </trans-unit>
        <trans-unit id="2a32db7a757930b4f797df7e8d06a3cd21abcff6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/miscellaneous/plot_anomaly_comparison#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; (tuned to perform like an outlier detection method) and a covariance-based outlier detection with &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43f0e98d020880753d56a5cecd019501b6cb864a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/miscellaneous/plot_anomaly_comparison#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison of the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="add8ccecfab3f9c846b3f2682673366366734b83" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/miscellaneous/plot_anomaly_comparison#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison with other anomaly detection methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cee09c79ab5ccf2f8ea1b726ccad8beb5f01cbf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound#sphx-glr-auto-examples-miscellaneous-plot-johnson-lindenstrauss-bound-py&quot;&gt;The Johnson-Lindenstrauss bound for embedding with random projections&lt;/a&gt; for a theoretical explication on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba387b296d0109ccfdd27d435e72e0bda0b98a94" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/mixture/plot_concentration_prior#sphx-glr-auto-examples-mixture-plot-concentration-prior-py&quot;&gt;Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture&lt;/a&gt; for an example plotting the confidence ellipsoids for the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; with different &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; for different values of the parameter &lt;code&gt;weight_concentration_prior&lt;/code&gt;.</source>
          <target state="translated">매개 변수 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 의 다른 값에 대해 다른 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; 을 갖는 &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt; 의 신뢰 타원체를 플로팅하는 예제 &lt;a href=&quot;../auto_examples/mixture/plot_concentration_prior#sphx-glr-auto-examples-mixture-plot-concentration-prior-py&quot;&gt;는 변형 베이지안 가우스 혼합의 농도 사전 유형 분석을&lt;/a&gt; 참조하십시오. .</target>
        </trans-unit>
        <trans-unit id="17317cd9be65f918f2c9598fbac39ad346f5db56" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/mixture/plot_gmm#sphx-glr-auto-examples-mixture-plot-gmm-py&quot;&gt;Gaussian Mixture Model Ellipsoids&lt;/a&gt; for an example on plotting the confidence ellipsoids for both &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 에&lt;/a&gt; 대한 신뢰 타원체를 플로팅하는 방법에 대한 예제는 &lt;a href=&quot;../auto_examples/mixture/plot_gmm#sphx-glr-auto-examples-mixture-plot-gmm-py&quot;&gt;Gaussian Mixture Model Ellipsoids&lt;/a&gt; 를 참조하십시오. .</target>
        </trans-unit>
        <trans-unit id="a5877cbb374e3dc33496a562d7ff812fdb5db635" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/mixture/plot_gmm_covariances#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py&quot;&gt;GMM covariances&lt;/a&gt; for an example of using the Gaussian mixture as clustering on the iris dataset.</source>
          <target state="translated">보다 &lt;a href=&quot;../auto_examples/mixture/plot_gmm_covariances#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py&quot;&gt;&lt;/a&gt;홍채 데이터 세트에서 가우스 혼합을 클러스터링으로 사용하는 예는 GMM 공분산 을 .</target>
        </trans-unit>
        <trans-unit id="235746c777e5faff74a54e9f92e2aa47946dcca8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/mixture/plot_gmm_pdf#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py&quot;&gt;Density Estimation for a Gaussian mixture&lt;/a&gt; for an example on plotting the density estimation.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/mixture/plot_gmm_pdf#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py&quot;&gt;가우스 혼합에 대한 밀도 추정을&lt;/a&gt; 참조하십시오 플롯에 대한 예 대한 .</target>
        </trans-unit>
        <trans-unit id="1f45342ed85fb843511e05db7aa53da9e05cd4c8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/mixture/plot_gmm_selection#sphx-glr-auto-examples-mixture-plot-gmm-selection-py&quot;&gt;Gaussian Mixture Model Selection&lt;/a&gt; for an example of model selection performed with classical Gaussian mixture.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/mixture/plot_gmm_selection#sphx-glr-auto-examples-mixture-plot-gmm-selection-py&quot;&gt;가우스 혼합 모델 선택&lt;/a&gt; 참조고전 가우스 혼합으로 수행 된 모델 선택의 예는 을 .</target>
        </trans-unit>
        <trans-unit id="2675c64adf13cc79a9b07f8da3e0d3af0522fc69" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/grid_search_text_feature_extraction#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py&quot;&gt;Sample pipeline for text feature extraction and evaluation&lt;/a&gt; for an example of Grid Search coupling parameters from a text documents feature extractor (n-gram count vectorizer and TF-IDF transformer) with a classifier (here a linear SVM trained with SGD with either elastic net or L2 penalty) using a &lt;code&gt;pipeline.Pipeline&lt;/code&gt; instance.</source>
          <target state="translated">분류기 (여기서는 탄성 망이있는 SGD로 훈련 된 선형 SVM)의 텍스트 문서 기능 추출기 (n-gram count vectorizer 및 TF-IDF 변환기)의 그리드 검색 커플 링 매개 변수의 예는 &lt;a href=&quot;../auto_examples/model_selection/grid_search_text_feature_extraction#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py&quot;&gt;텍스트 기능 추출 및 평가에&lt;/a&gt; 대한 샘플 파이프 라인을 참조 하십시오. 또는 L2 페널티)를 사용하여 &lt;code&gt;pipeline.Pipeline&lt;/code&gt; 인스턴스를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="da92f3b35d742326e0c71721384ca2aaccbfcbb6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; for an example of using a confusion matrix to evaluate classifier output quality.</source>
          <target state="translated">분류기 출력 품질을 평가하기 위해 혼동 행렬을 사용하는 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;혼동 행렬&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="de7b9e4f40b1dbfe2688a95ace15280b606fa175" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;Parameter estimation using grid search with cross-validation&lt;/a&gt; for an example of &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; usage to estimate parameters using grid search with nested cross-validation.</source>
          <target state="translated">nested cross-validation이있는 그리드 검색을 사용하여 매개 변수를 추정 하는 &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt; 사용법 의 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;cross-validation&lt;/a&gt; 이있는 그리드 검색을 사용한 매개 변수 추정을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="907fb39fc7f1b14d7a7b9cc2b05542f6688793e4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;Parameter estimation using grid search with cross-validation&lt;/a&gt; for an example of Grid Search computation on the digits dataset.</source>
          <target state="translated">자릿수 데이터 세트에 대한 그리드 검색 계산의 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;교차 검증&lt;/a&gt; 을 통한 그리드 검색을 사용한 모수 추정을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="400cd83fcd5e25dceebfea56b549d5be061f5ba4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;Parameter estimation using grid search with cross-validation&lt;/a&gt; for an example of classification report usage for grid search with nested cross-validation.</source>
          <target state="translated">중첩 된 교차 유효성 검사를 사용한 그리드 검색에 대한 분류 보고서 사용 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_digits#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py&quot;&gt;교차 유효성 검사&lt;/a&gt; 를 사용한 그리드 검색을 사용한 모수 추정을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="588e2a7cc51b06cfa971b7f26bd8bf4053207cf1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_grid_search_refit_callable#sphx-glr-auto-examples-model-selection-plot-grid-search-refit-callable-py&quot;&gt;Balance model complexity and cross-validated score&lt;/a&gt; for an example of using &lt;code&gt;refit=callable&lt;/code&gt; interface in &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;. The example shows how this interface adds certain amount of flexibility in identifying the &amp;ldquo;best&amp;rdquo; estimator. This interface can also be used in multiple metrics evaluation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2735aad85c6c7c554984533b716de0827c908e17" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_multi_metric_evaluation#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py&quot;&gt;Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV&lt;/a&gt; for an example of &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; being used to evaluate multiple metrics simultaneously.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/model_selection/plot_multi_metric_evaluation#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py&quot;&gt;GridSearchCV&lt;/a&gt; 의 예는 &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 및 GridSearchCV에 대한 다중 메트릭 평가 데모를 참조하십시오.여러 메트릭을 동시에 평가하는 데 사용되는 .</target>
        </trans-unit>
        <trans-unit id="c2ffbe0f12938b51592ad9e927a7d22caaeca8af" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_multi_metric_evaluation#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py&quot;&gt;Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV&lt;/a&gt; for an example usage.</source>
          <target state="translated">사용법 예 &lt;a href=&quot;../auto_examples/model_selection/plot_multi_metric_evaluation#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py&quot;&gt;는 cross_val_score 및 GridSearchCV에&lt;/a&gt; 대한 다중 메트릭 평가 데모를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="8476e4237a00d9dd8e2ca35734caeb2c23a0697e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_nested_cross_validation_iris#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py&quot;&gt;Nested versus non-nested cross-validation&lt;/a&gt; for an example of Grid Search within a cross validation loop on the iris dataset. This is the best practice for evaluating the performance of a model with grid search.</source>
          <target state="translated">홍채 데이터 세트의 교차 유효성 검사 루프 내에서 그리드 검색의 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_nested_cross_validation_iris#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py&quot;&gt;중첩 된 교차 유효성 검사와 중첩되지 않은 교차 ​​유효성 검사&lt;/a&gt; 를 참조하십시오 . 그리드 검색을 사용하여 모델의 성능을 평가하는 가장 좋은 방법입니다.</target>
        </trans-unit>
        <trans-unit id="e8dfb2ba828b857661aeb6d59ff74f5c4db05d22" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_precision_recall#sphx-glr-auto-examples-model-selection-plot-precision-recall-py&quot;&gt;Precision-Recall&lt;/a&gt; for an example of &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt;&lt;code&gt;precision_recall_curve&lt;/code&gt;&lt;/a&gt; usage to evaluate classifier output quality.</source>
          <target state="translated">분류기 출력 품질을 평가하기위한 &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt; &lt;code&gt;precision_recall_curve&lt;/code&gt; &lt;/a&gt; 사용법 의 예는 &lt;a href=&quot;../auto_examples/model_selection/plot_precision_recall#sphx-glr-auto-examples-model-selection-plot-precision-recall-py&quot;&gt;Precision-Recall&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7914d54070b906eba7716c438acf436730af131e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_roc#sphx-glr-auto-examples-model-selection-plot-roc-py&quot;&gt;Receiver Operating Characteristic (ROC)&lt;/a&gt; for an example of using ROC to evaluate the quality of the output of a classifier.</source>
          <target state="translated">참조 &lt;a href=&quot;../auto_examples/model_selection/plot_roc#sphx-glr-auto-examples-model-selection-plot-roc-py&quot;&gt;수신기 작동 특성 (ROC)&lt;/a&gt;ROC를 사용하여 분류기 출력의 품질을 평가하는 예는 을 .</target>
        </trans-unit>
        <trans-unit id="61240d29f349f145d4e8cf44909bfb9cc2f016b2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/model_selection/plot_roc_crossval#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py&quot;&gt;Receiver Operating Characteristic (ROC) with cross validation&lt;/a&gt; for an example of using ROC to evaluate classifier output quality, using cross-validation.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/model_selection/plot_roc_crossval#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py&quot;&gt;교차 유효성 검사&lt;/a&gt; 를 사용하여 분류기 출력 품질을 평가하기 위해 ROC를 사용하는 예는 교차 유효성 검사 가있는 ROC (수신기 동작 특성)를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2194fe880eec73b4ed37b8700c0f77c050a462fc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/neighbors/plot_lof_outlier_detection#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py&quot;&gt;Outlier detection with Local Outlier Factor (LOF)&lt;/a&gt; for an illustration of the use of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">보기 &lt;a href=&quot;../auto_examples/neighbors/plot_lof_outlier_detection#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py&quot;&gt;지역 특이점 계수 (LOF)와 아웃 라이어 검출&lt;/a&gt; 의 사용의 그림은 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bb13fcb17f205551a70f65831a46478e0af0f276" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; with &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; (tuned to perform like an outlier detection method) and a covariance-based outlier detection with &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">참조 &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;장난감 데이터 세트에서 아웃 라이어 검출 용 이상 검출 알고리즘을 비교&lt;/a&gt; 하는 비교 &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; (특이점 검출 방법과 같이 수행하도록 조정)와 함께 공분산 기반의 아웃 라이어 검출 &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="64196936345ba93c97149a5b0ef386464e9766b9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison of the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">참조 &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;장난감 데이터 세트에 이상치 검출 이상 탐지 알고리즘을 비교&lt;/a&gt; 의 비교에 대한 &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 의 &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt; 의 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt; &lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="34db9fa5546a4563c9e371d735571ffbe05f0c7c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;Comparing anomaly detection algorithms for outlier detection on toy datasets&lt;/a&gt; for a comparison with other anomaly detection methods.</source>
          <target state="translated">다른 이상 탐지 방법과 비교하려면 &lt;a href=&quot;../auto_examples/plot_anomaly_comparison#sphx-glr-auto-examples-plot-anomaly-comparison-py&quot;&gt;장난감 데이터 세트&lt;/a&gt; 에서 이상 치를 탐지하기 위해 이상 탐지 알고리즘 비교를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="624945e6b02bb2ff2c43f28f85ed4813c5cbe96d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/plot_johnson_lindenstrauss_bound#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py&quot;&gt;The Johnson-Lindenstrauss bound for embedding with random projections&lt;/a&gt; for a theoretical explication on the Johnson-Lindenstrauss lemma and an empirical validation using sparse random matrices.</source>
          <target state="translated">Johnson-Lindenstrauss lemma에 대한 이론적 설명과 희소 난수 행렬을 사용한 경험적 검증에 대해서는 &lt;a href=&quot;../auto_examples/plot_johnson_lindenstrauss_bound#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py&quot;&gt;랜덤 프로젝션으로 임베드하는 Johnson-Lindenstrauss 바운드를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="9ca7b8e34286a27914e5e700a286fbed9102f4af" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/svm/plot_oneclass#sphx-glr-auto-examples-svm-plot-oneclass-py&quot;&gt;One-class SVM with non-linear kernel (RBF)&lt;/a&gt; for visualizing the frontier learned around some data by a &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 객체로 일부 데이터에 대해 학습 된 경계를 시각화하려면 &lt;a href=&quot;../auto_examples/svm/plot_oneclass#sphx-glr-auto-examples-svm-plot-oneclass-py&quot;&gt;비선형 커널 (RBF)이있는 1 클래스 SVM을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="76c663b6a0471e4c53ba854f9c09becf8ef59fc7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;Classification of text documents using sparse features&lt;/a&gt; for an example of &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt; usage to classify text documents.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;텍스트 문서를 분류&lt;/a&gt; 하기위한 &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt; 사용법 의 예는 스파 스 기능 을 사용하여 텍스트 문서 분류를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="188cc26ffe635b802535768a1802c77d30908617" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;Classification of text documents using sparse features&lt;/a&gt; for an example of classification report usage for text documents.</source>
          <target state="translated">&lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;텍스트 문서&lt;/a&gt; 의 분류 보고서 사용 예는 스파 스 기능 을 사용하여 텍스트 문서 분류를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="19118774a723324b6225f3d83bcf9761f94d3619" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;Classification of text documents using sparse features&lt;/a&gt; for an example of using a confusion matrix to classify text documents.</source>
          <target state="translated">혼동 행렬을 사용하여 텍스트 문서를 분류하는 예는 &lt;a href=&quot;../auto_examples/text/plot_document_classification_20newsgroups#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py&quot;&gt;스파 스 기능&lt;/a&gt; 을 사용하여 텍스트 문서 분류를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="81a2b2d1fb5035ca6b501c666e8a41c6286b60de" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../grid_search#multimetric-grid-search&quot;&gt;Specifying multiple metrics for evaluation&lt;/a&gt; for an example.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../grid_search#multimetric-grid-search&quot;&gt;평가할 여러 메트릭 지정을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="479250f0ad90f3ce8d5fd16594b9c17af606dc2c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; in the online documentation for a discussion of the choice of &lt;code&gt;algorithm&lt;/code&gt; and &lt;code&gt;leaf_size&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm&lt;/code&gt; 및 &lt;code&gt;leaf_size&lt;/code&gt; 선택에 대한 설명은 온라인 문서에서 &lt;a href=&quot;../neighbors#neighbors&quot;&gt;가장 가까운 이웃&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="91bb5f21ad92edefb7cd46dcdff8e31af1c98298" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../tree#minimal-cost-complexity-pruning&quot;&gt;Minimal Cost-Complexity Pruning&lt;/a&gt; for details on the pruning process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91032a83e07b0024745974d4bc72e492c7c9e85a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines and composite estimators&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;compose#combining-estimators&quot;&gt;파이프 라인 및 복합 추정기를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="516b0abd274bf0e8eb7951cfd8d017257e70560d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;feature_extraction#dict-feature-extraction&quot;&gt;Loading features from dicts&lt;/a&gt; for categorical features that are represented as a dict, not as scalars.</source>
          <target state="translated">스칼라가 아닌 dict로 표시되는 범주 형 기능에 대해서는 &lt;a href=&quot;feature_extraction#dict-feature-extraction&quot;&gt;dicts에서 기능로드를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5f5bdda151c122a6b0f331c022a3fe3419eba6e8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits&quot;&gt;here&lt;/a&gt; for more information about this dataset.</source>
          <target state="translated">이 데이터 세트에 대한 자세한 내용은 &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits&quot;&gt;여기&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="59cc7550d6ebef4a36dc7cbd048f831a5c0d0f3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf&quot;&gt;http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf&quot;&gt;http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf를&lt;/a&gt; 참조 하십시오</target>
        </trans-unit>
        <trans-unit id="58869cd4ecac8051c9d79dfaa9dbb2a543b85dfe" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf&quot;&gt;&amp;ldquo;Efficient additive kernels via explicit feature maps&amp;rdquo;&lt;/a&gt; A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, 2011</source>
          <target state="translated">&lt;a href=&quot;http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf&quot;&gt;&quot;명확한 기능 맵을 통한 효율적인 추가 커널&quot;&lt;/a&gt; 참조 A. Vedaldi 및 A. Zisserman, 패턴 분석 및 머신 인텔리전스, 2011</target>
        </trans-unit>
        <trans-unit id="8ccf3f09b07311e149a69a4c5ed81e792cfab2aa" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits&quot;&gt;here&lt;/a&gt; for more information about this dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4bdca7573215dd8d9f679d272cb9da9a534f1291" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">이 데이터 세트에 대한 자세한 내용 은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;여기&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="eb3a5aff676c4d5166ca0015430772938f3b0abf" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee1e742783ffe82079419a7f74e91fb7300f9192" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf&quot;&gt;https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7191cbc48e1c8b07d612d6ecdb398fe05677ce16" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt; for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.</source>
          <target state="translated">자세한 내용 &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;은 스코어링 매개 변수 : 모델 평가 규칙 정의&lt;/a&gt; 를 참조하십시오. Iris 데이터 세트의 경우, 샘플은 대상 클래스에 걸쳐 균형을 유지하므로 정확도와 F1- 점수가 거의 동일합니다.</target>
        </trans-unit>
        <trans-unit id="e083d29e5fc318a8e4c7186d224d71159333e317" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;outlier_detection#outlier-detection&quot;&gt;Novelty and Outlier Detection&lt;/a&gt; for the description and usage of OneClassSVM.</source>
          <target state="translated">&lt;a href=&quot;outlier_detection#outlier-detection&quot;&gt;OneClassSVM&lt;/a&gt; 의 설명 및 사용법은 참신 및 이상 값 탐지 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2bf27f073ae6fdc435309a3de7aa195bb3e33f73" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;predict_proba&lt;/code&gt; for details.</source>
          <target state="translated">자세한 내용은 &lt;code&gt;predict_proba&lt;/code&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="9a6c19ee072204bfa0caf7b0899caf92ad1a79e0" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;refit&lt;/code&gt; parameter for more information on allowed values.</source>
          <target state="translated">허용되는 값에 대한 자세한 정보는 &lt;code&gt;refit&lt;/code&gt; 매개 변수를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="fcd84460747232330056c00c1a39fd6ed47410b5" translate="yes" xml:space="preserve">
          <source>See &lt;code&gt;scoring&lt;/code&gt; parameter to know more about multiple metric evaluation.</source>
          <target state="translated">다중 메트릭 평가에 대한 자세한 내용은 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="61a08f389a25b863d0fca5015a2885ff6fd5c8cd" translate="yes" xml:space="preserve">
          <source>See Also:</source>
          <target state="translated">또한보십시오:</target>
        </trans-unit>
        <trans-unit id="dd75486b56d3a12e77b37b7ce59de88eb8618b01" translate="yes" xml:space="preserve">
          <source>See Rasmussen and Williams 2006, pp84 for details regarding the different variants of the Matern kernel.</source>
          <target state="translated">Matern 커널의 다양한 변형에 대한 자세한 내용은 Rasmussen 및 Williams 2006, pp84를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="2d8243a2c0e464492c9d563c4f92c56ae3421bcc" translate="yes" xml:space="preserve">
          <source>See also</source>
          <target state="translated">또한보십시오</target>
        </trans-unit>
        <trans-unit id="319ca132af6f206834626d4c0565d67f882f0bf4" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;Minimal Cost-Complexity Pruning&lt;/a&gt; for details on pruning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eddd8dfbe32e6e27a2f2d9692940c76ecde49164" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;neighbors#nca-dim-reduction&quot;&gt;Dimensionality reduction&lt;/a&gt; for dimensionality reduction with Neighborhood Components Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16d6db1c15da7e05f275e12f2b52c16bcc8dc1f7" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00556fb4b47eda5d6ddfa3723da8312c58733ada" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;plot_rfe_with_cross_validation#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py&quot;&gt;Recursive feature elimination with cross-validation&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;plot_rfe_with_cross_validation#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py&quot;&gt;교차 검증을 통한 재귀 기능 제거&lt;/a&gt; 참조</target>
        </trans-unit>
        <trans-unit id="9232d9babf570066106c095c7f9e14cb2421acee" translate="yes" xml:space="preserve">
          <source>See also &lt;a href=&quot;plot_roc_curve_visualization_api#sphx-glr-auto-examples-miscellaneous-plot-roc-curve-visualization-api-py&quot;&gt;ROC Curve with Visualization API&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="371a87eafb4de078ff674d69a5a89c186532eb49" translate="yes" xml:space="preserve">
          <source>See also:</source>
          <target state="translated">또한보십시오:</target>
        </trans-unit>
        <trans-unit id="bbbf1c8bb1bb44153dbb121ba5ff682161041559" translate="yes" xml:space="preserve">
          <source>See also: 1988 MLC Proceedings, 54-64. Cheeseman et al&amp;rdquo;s AUTOCLASS II conceptual clustering system finds 3 classes in the data.</source>
          <target state="translated">참조 : 1988 MLC Proceedings, 54-64 Cheeseman 등의 AUTOCLASS II 개념 클러스터링 시스템은 데이터에서 3 개의 클래스를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="96949ffbfe0e5b8e858a6b32bba0567fa5dcf8f9" translate="yes" xml:space="preserve">
          <source>See glossary entry for &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cross-validation-estimator&quot;&gt;cross-validation estimator&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ce8930e7f553fab96c5eb4cdba6059705f0a1b9" translate="yes" xml:space="preserve">
          <source>See section &lt;a href=&quot;preprocessing#preprocessing&quot;&gt;Preprocessing data&lt;/a&gt; for more details on scaling and normalization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b55a2c7dcbc914b3c4abb672c6103792d08facf" translate="yes" xml:space="preserve">
          <source>See sklearn.svm.predict for a complete list of parameters.</source>
          <target state="translated">전체 매개 변수 목록은 sklearn.svm.predict를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="8c30624bfa9869c6a4a63a1b052f17576abbc1b5" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;../../auto_examples/applications/svm_gui#sphx-glr-auto-examples-applications-svm-gui-py&quot;&gt;SVM GUI&lt;/a&gt; to download &lt;code&gt;svm_gui.py&lt;/code&gt;; add data points of both classes with right and left button, fit the model and change parameters and data.</source>
          <target state="translated">&lt;code&gt;svm_gui.py&lt;/code&gt; 를 다운로드 하려면 &lt;a href=&quot;../../auto_examples/applications/svm_gui#sphx-glr-auto-examples-applications-svm-gui-py&quot;&gt;SVM GUI&lt;/a&gt; 를 참조하십시오 . 오른쪽과 왼쪽 버튼으로 두 클래스의 데이터 포인트를 추가하고, 모델을 맞추고 매개 변수와 데이터를 변경하십시오.</target>
        </trans-unit>
        <trans-unit id="3a7904b44fb635dd1b8e25248a204eaa4fae3a1b" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering evaluation&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용 설명서 의 &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering 평가&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="68bd165827c5ef6d955b9032ff7e059af8a91538" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;Clustering performance evaluation&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용 설명서 의 &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;클러스터링 성능 평가&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="32e5d32ee9a61e6d932c5ca6a73a2f56a975703b" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;Visualizations&lt;/a&gt; section of the user guide for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8dd9c0a0a464abab54cd5ae3d828ecf303587e8d" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용자 안내서 의 &lt;a href=&quot;metrics#metrics&quot;&gt;쌍별 메트릭, 선호도 및 커널&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="a5b49cc34cb79ec02e159e95ecb887eb0b2cb87b" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#classification-metrics&quot;&gt;Classification metrics&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용자 안내서 의 &lt;a href=&quot;model_evaluation#classification-metrics&quot;&gt;분류 지표&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="a370028df0ac77d50f24c414c79435536844962d" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Metrics and scoring: quantifying the quality of predictions&lt;/a&gt; section and the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section of the user guide for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9391e01adcbcd6eab5489ba5c5bbde2b34c1c767" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Model evaluation: quantifying the quality of predictions&lt;/a&gt; section and the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;모델 평가 : 예측 품질 수량화&lt;/a&gt; 섹션 및 사용자 안내서 의 &lt;a href=&quot;metrics#metrics&quot;&gt;쌍별 메트릭, 친화도 및 커널&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="794b0f5d5def59a318bd787a4fccdd542a21003c" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#multilabel-ranking-metrics&quot;&gt;Multilabel ranking metrics&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용 설명서 의 &lt;a href=&quot;model_evaluation#multilabel-ranking-metrics&quot;&gt;다중 레이블 순위 메트릭&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="2e87795fde7a0f4562bca5bb85f3c7f5918727b2" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#regression-metrics&quot;&gt;Regression metrics&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용자 안내서 의 &lt;a href=&quot;model_evaluation#regression-metrics&quot;&gt;회귀 메트릭&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="e8449bc2e3e9fd1500a092c7a467f1094a642fdf" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt; section of the user guide for further details.</source>
          <target state="translated">자세한 내용은 사용자 안내서 &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;의 스코어링 매개 변수 : 모델 평가 규칙 정의&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bc10a00d51f81094bb499f5ba451c68f15309214" translate="yes" xml:space="preserve">
          <source>See the console&amp;rsquo;s output for further details about each model.</source>
          <target state="translated">각 모델에 대한 자세한 내용은 콘솔 출력을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="cd81e4d9092f6289f9eb153c5b671f6c9ec59b00" translate="yes" xml:space="preserve">
          <source>See the docstring of DistanceMetric for a list of available metrics.</source>
          <target state="translated">사용 가능한 메트릭 목록은 DistanceMetric의 문서 문자열을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bf132d2b0dc3be6b88274385f87b0ee3f849742c" translate="yes" xml:space="preserve">
          <source>See the documentation for scipy.spatial.distance for details on these metrics.</source>
          <target state="translated">이러한 메트릭에 대한 자세한 내용은 scipy.spatial.distance 설명서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="7af63b893a630b06c001dfe05c36e8144bafc593" translate="yes" xml:space="preserve">
          <source>See the documentation for scipy.spatial.distance for details on these metrics: &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/spatial.distance.html&quot;&gt;http://docs.scipy.org/doc/scipy/reference/spatial.distance.html&lt;/a&gt;</source>
          <target state="translated">이러한 메트릭에 대한 자세한 내용은 scipy.spatial.distance 설명서를 참조하십시오. &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/spatial.distance.html&quot;&gt;http://docs.scipy.org/doc/scipy/reference/spatial.distance.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26decb2493981e4fa0291aaddd53d2d9f9052651" translate="yes" xml:space="preserve">
          <source>See the documentation for scipy.spatial.distance for details on these metrics: &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/spatial.distance.html&quot;&gt;https://docs.scipy.org/doc/scipy/reference/spatial.distance.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c235949c8aa0b531ecb9dfa56db515940237097e" translate="yes" xml:space="preserve">
          <source>See the examples below and the doc string of &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier.fit&quot;&gt;&lt;code&gt;MLPClassifier.fit&lt;/code&gt;&lt;/a&gt; for further information.</source>
          <target state="translated">자세한 정보는 아래 예제 및 &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier.fit&quot;&gt; &lt;code&gt;MLPClassifier.fit&lt;/code&gt; &lt;/a&gt; 의 doc 문자열을 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="21e6c58453c3a42f9380294c0c23f1e8e3252a3a" translate="yes" xml:space="preserve">
          <source>See the examples below and the docstring of &lt;a href=&quot;generated/sklearn.neighbors.neighborhoodcomponentsanalysis#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit&quot;&gt;&lt;code&gt;NeighborhoodComponentsAnalysis.fit&lt;/code&gt;&lt;/a&gt; for further information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3733f36a35b17995708cf55601d9baf4bb2149b" translate="yes" xml:space="preserve">
          <source>See the examples below and the docstring of &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier.fit&quot;&gt;&lt;code&gt;MLPClassifier.fit&lt;/code&gt;&lt;/a&gt; for further information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f593defdf84cd9d54340a0b9eb8bdbba2164c5f" translate="yes" xml:space="preserve">
          <source>See the examples below for further information.</source>
          <target state="translated">자세한 내용은 아래 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="43e09506578d0fedfc6592860be1e23c1f8ef43b" translate="yes" xml:space="preserve">
          <source>See the examples for such an application.</source>
          <target state="translated">이러한 응용 프로그램의 예를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="e8c17521507d95b1954b9918e527f968af48b835" translate="yes" xml:space="preserve">
          <source>See. &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">보다. C. Bishop의&amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;, 12.2.1 p. 574 또는 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e4871111c8505b7690af20e518e753ae1b90a5ad" translate="yes" xml:space="preserve">
          <source>Seed for the random number generator used for probability estimates. 0 by default.</source>
          <target state="translated">확률 추정에 사용되는 난수 생성기의 시드입니다. 기본적으로 0입니다.</target>
        </trans-unit>
        <trans-unit id="72c84d205a7667dfdb05c6ebaaf98f08a838df92" translate="yes" xml:space="preserve">
          <source>Seeding is performed using a binning technique for scalability.</source>
          <target state="translated">시딩은 확장 성을 위해 비닝 기술을 사용하여 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="f1f3844996349c701e3db8be5a62a8ace310a543" translate="yes" xml:space="preserve">
          <source>Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters.</source>
          <target state="translated">커널을 초기화하는 데 사용되는 씨앗. 설정되지 않은 경우 시드는 그리드 크기 및 다른 매개 변수의 기본값으로 대역폭을 사용하여 clustering.get_bin_seeds에 의해 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="1a053c7e782c53a91d6d509bf6a99224f4b893d2" translate="yes" xml:space="preserve">
          <source>Segmenting the picture of greek coins in regions</source>
          <target state="translated">지역에서 그리스 동전 그림 세분화</target>
        </trans-unit>
        <trans-unit id="05c2c519388dfeab1d2da32ad0c3a55d08148aed" translate="yes" xml:space="preserve">
          <source>Select &lt;code&gt;min_samples&lt;/code&gt; random samples from the original data and check whether the set of data is valid (see &lt;code&gt;is_data_valid&lt;/code&gt;).</source>
          <target state="translated">원본 데이터에서 임의의 샘플을 &lt;code&gt;min_samples&lt;/code&gt; 선택 하고 데이터 세트가 유효한지 확인하십시오 ( &lt;code&gt;is_data_valid&lt;/code&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="4c8220c40092a5223a7519a4053e419620df4d58" translate="yes" xml:space="preserve">
          <source>Select eigensolver to use. If n_components is much less than the number of training samples, arpack may be more efficient than the dense eigensolver.</source>
          <target state="translated">사용할 고유 해석기를 선택하십시오. n_components가 학습 샘플 수보다 훨씬 적은 경우 arpack은 밀도가 높은 고유 솔버보다 더 효율적일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a8b26e8dd8c57424e2a0ff5f2b02e8bbc7be69eb" translate="yes" xml:space="preserve">
          <source>Select features according to a percentile of the highest scores.</source>
          <target state="translated">최고 점수의 백분위 수에 따라 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="d20a85a468318484f73da066702f203468ec7eb5" translate="yes" xml:space="preserve">
          <source>Select features according to the k highest scores.</source>
          <target state="translated">k 개의 최고 점수에 따라 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="c3952e3b9f6e5571ab9a9f69665172397cc254d2" translate="yes" xml:space="preserve">
          <source>Select features based on a false positive rate test.</source>
          <target state="translated">오 탐지 테스트를 기반으로 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="32515d7cef117ccce44afc2f4f0b98f43d0db807" translate="yes" xml:space="preserve">
          <source>Select features based on an estimated false discovery rate.</source>
          <target state="translated">추정 된 오 탐지 비율에 따라 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="6d7a0df88fc316c1f019dac960b65ab544487a37" translate="yes" xml:space="preserve">
          <source>Select features based on family-wise error rate.</source>
          <target state="translated">가족 별 오류율에 따라 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="ba417981f6009fc06cd3596ff9c6cb4c2bd25319" translate="yes" xml:space="preserve">
          <source>Select features based on percentile of the highest scores.</source>
          <target state="translated">최고 점수의 백분위 수를 기준으로 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="3532493330a0445601f2381a89fe492b8458f964" translate="yes" xml:space="preserve">
          <source>Select features based on the k highest scores.</source>
          <target state="translated">k 개의 최고 점수를 기준으로 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="d3166439a7b0a709dd15d8647b0a1e23526bb82a" translate="yes" xml:space="preserve">
          <source>Select from the model features with the higest score</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc6cce4211d0c67d4111ac2f86243aa83948ed07" translate="yes" xml:space="preserve">
          <source>Select n_samples integers from the set [0, n_population) without replacement.</source>
          <target state="translated">교체없이 세트 [0, n_population)에서 n_samples 정수를 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="e693da3619bc133d154aaf34e091d4f9f76e8468" translate="yes" xml:space="preserve">
          <source>Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples &amp;gt; n_features.</source>
          <target state="translated">알고리즘을 선택하여 이중 또는 원시 최적화 문제를 해결하십시오. n_samples&amp;gt; n_features 인 경우 dual = False를 선호하십시오.</target>
        </trans-unit>
        <trans-unit id="e09f39accd13c28376a1ebc78d546869197bec0f" translate="yes" xml:space="preserve">
          <source>Select the dataset to load: &amp;lsquo;train&amp;rsquo; for the development training set, &amp;lsquo;test&amp;rsquo; for the development test set, and &amp;lsquo;10_folds&amp;rsquo; for the official evaluation set that is meant to be used with a 10-folds cross validation.</source>
          <target state="translated">로드 할 데이터 세트를 선택하십시오 (개발 훈련 세트의 경우 'train', 개발 테스트 세트의 경우 'test', 10 배 교차 검증에 사용되는 공식 평가 세트의 경우 '10_folds').</target>
        </trans-unit>
        <trans-unit id="da5a2fc4086f03333558c16d8aba6a6ba8f98164" translate="yes" xml:space="preserve">
          <source>Select the dataset to load: &amp;lsquo;train&amp;rsquo; for the training set (23149 samples), &amp;lsquo;test&amp;rsquo; for the test set (781265 samples), &amp;lsquo;all&amp;rsquo; for both, with the training samples first if shuffle is False. This follows the official LYRL2004 chronological split.</source>
          <target state="translated">셔플이 False 인 경우 먼저 훈련 샘플을 사용하여 훈련 세트 (23149 샘플)에 대한 'train', 테스트 세트 (781265 샘플)에 대한 'test', 둘 모두에 대해 'all'을로드 할 데이터 세트를 선택합니다. 이것은 공식 LYRL2004 시간 분할을 따릅니다.</target>
        </trans-unit>
        <trans-unit id="a373737a4d85a4134f237dcaa76f506d35776152" translate="yes" xml:space="preserve">
          <source>Select the dataset to load: &amp;lsquo;train&amp;rsquo; for the training set, &amp;lsquo;test&amp;rsquo; for the test set, &amp;lsquo;all&amp;rsquo; for both, with shuffled ordering.</source>
          <target state="translated">로드 할 데이터 세트를 선택하십시오 (훈련 세트의 경우 'train', 테스트 세트의 경우 'test', 둘 다의 경우 'all').</target>
        </trans-unit>
        <trans-unit id="c5f861c6085a651c0ed9619f5012b02bb9a2d195" translate="yes" xml:space="preserve">
          <source>Select the parameters that minimises the impurity</source>
          <target state="translated">불순물을 최소화하는 파라미터를 선택하십시오</target>
        </trans-unit>
        <trans-unit id="776d7f86c8363c5c583ee4e086a4256b8464f6f6" translate="yes" xml:space="preserve">
          <source>Select the portion to load: &amp;lsquo;train&amp;rsquo;, &amp;lsquo;test&amp;rsquo; or &amp;lsquo;raw&amp;rsquo;</source>
          <target state="translated">로드 할 부분을 선택하십시오 : 'train', 'test'또는 'raw'</target>
        </trans-unit>
        <trans-unit id="b97f498920dc9d14793f9ec22705c51c7828c05e" translate="yes" xml:space="preserve">
          <source>Select whether the regularization affects the components (H), the transformation (W), both or none of them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09caeaab6645f9fa60776419a39bd350760c6b9f" translate="yes" xml:space="preserve">
          <source>Selecting &lt;code&gt;average=None&lt;/code&gt; will return an array with the score for each class.</source>
          <target state="translated">&lt;code&gt;average=None&lt;/code&gt; 을 선택하면 각 클래스의 점수가 포함 된 배열이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="09987abb5cb6e00639cc8ad149fcfc0ee4e216e7" translate="yes" xml:space="preserve">
          <source>Selecting dimensionality reduction with Pipeline and GridSearchCV</source>
          <target state="translated">파이프 라인 및 GridSearchCV를 사용하여 차원 축소 선택</target>
        </trans-unit>
        <trans-unit id="b619a7e9444390b8df9ed15ee53211d47286dc3c" translate="yes" xml:space="preserve">
          <source>Selecting the number of clusters with silhouette analysis on KMeans clustering</source>
          <target state="translated">KMeans 클러스터링에서 실루엣 분석으로 클러스터 수 선택</target>
        </trans-unit>
        <trans-unit id="1e3a867140ee60f287b6c8b807e610aee224839f" translate="yes" xml:space="preserve">
          <source>Selects the algorithm for finding singular vectors. May be &amp;lsquo;randomized&amp;rsquo; or &amp;lsquo;arpack&amp;rsquo;. If &amp;lsquo;randomized&amp;rsquo;, use &lt;a href=&quot;sklearn.utils.extmath.randomized_svd#sklearn.utils.extmath.randomized_svd&quot;&gt;&lt;code&gt;sklearn.utils.extmath.randomized_svd&lt;/code&gt;&lt;/a&gt;, which may be faster for large matrices. If &amp;lsquo;arpack&amp;rsquo;, use &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html#scipy.sparse.linalg.svds&quot;&gt;&lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;&lt;/a&gt;, which is more accurate, but possibly slower in some cases.</source>
          <target state="translated">특이 벡터를 찾기위한 알고리즘을 선택합니다. '무작위 화'또는 'arpack'일 수 있습니다. '랜덤 화'인 경우 &lt;a href=&quot;sklearn.utils.extmath.randomized_svd#sklearn.utils.extmath.randomized_svd&quot;&gt; &lt;code&gt;sklearn.utils.extmath.randomized_svd&lt;/code&gt; 를&lt;/a&gt; 사용 하십시오 . 큰 행렬의 경우 더 빠를 수 있습니다. 'arpack'인 경우 &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html#scipy.sparse.linalg.svds&quot;&gt; &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 를&lt;/a&gt; 사용하십시오. 보다 정확하지만 경우에 따라 속도가 느려질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fc3d3604d35f10ba0398acd746e6c18250ce369b" translate="yes" xml:space="preserve">
          <source>Selects the algorithm for finding singular vectors. May be &amp;lsquo;randomized&amp;rsquo; or &amp;lsquo;arpack&amp;rsquo;. If &amp;lsquo;randomized&amp;rsquo;, uses &lt;a href=&quot;sklearn.utils.extmath.randomized_svd#sklearn.utils.extmath.randomized_svd&quot;&gt;&lt;code&gt;randomized_svd&lt;/code&gt;&lt;/a&gt;, which may be faster for large matrices. If &amp;lsquo;arpack&amp;rsquo;, uses &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;, which is more accurate, but possibly slower in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba0796ade5894bc55073846864af8524b40634d7" translate="yes" xml:space="preserve">
          <source>Selects the algorithm for finding singular vectors. May be &amp;lsquo;randomized&amp;rsquo; or &amp;lsquo;arpack&amp;rsquo;. If &amp;lsquo;randomized&amp;rsquo;, uses &lt;code&gt;sklearn.utils.extmath.randomized_svd&lt;/code&gt;, which may be faster for large matrices. If &amp;lsquo;arpack&amp;rsquo;, uses &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;, which is more accurate, but possibly slower in some cases.</source>
          <target state="translated">특이 벡터를 찾기위한 알고리즘을 선택합니다. '무작위 화'또는 'arpack'일 수 있습니다. '랜덤 화'인 경우 &lt;code&gt;sklearn.utils.extmath.randomized_svd&lt;/code&gt; 를 사용 하면 큰 행렬의 경우 더 빠를 수 있습니다. 'arpack'인 경우 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 를 사용하면 보다 정확하지만 경우에 따라 속도가 느려질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90a62864642c982296e3e801c0e34e5e7f5e23e4" translate="yes" xml:space="preserve">
          <source>Semi Supervised Classification</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c634aac4fba33953abfb672747b23d137a6eb94" translate="yes" xml:space="preserve">
          <source>Sepal length</source>
          <target state="translated">분리 길이</target>
        </trans-unit>
        <trans-unit id="fb329e5a4491aa43414f15d76bffc8963ea0de09" translate="yes" xml:space="preserve">
          <source>Sepal width</source>
          <target state="translated">분리 폭</target>
        </trans-unit>
        <trans-unit id="e5dddf892a3efc8978d095e912cc4306a1e49804" translate="yes" xml:space="preserve">
          <source>Separating inliers from outliers using a Mahalanobis distance</source>
          <target state="translated">Mahalanobis 거리를 사용하여 특이 치에서 특이 치 분리</target>
        </trans-unit>
        <trans-unit id="aece656a00e1c7a3f193615a59fc1f63e6e58693" translate="yes" xml:space="preserve">
          <source>Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, &amp;ldquo;Decision Tree Construction Via Linear Programming.&amp;rdquo; Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.</source>
          <target state="translated">전술 한 분리 평면은 다중 표면 방법-트리 (MSM-T) [KP Bennett, &quot;선형 프로그래밍을 통한 의사 결정 트리 구성&quot;을 사용하여 얻어졌다. 4 차 중서부 인공 지능 및인지 과학 협회의 절차, pp. 97-101, 1992], 선형 프로그래밍을 사용하여 의사 결정 트리를 구성하는 분류 방법. 1-4 개의 피쳐와 1-3 개의 분리 평면에서 철저한 검색을 사용하여 관련 피쳐를 선택했습니다.</target>
        </trans-unit>
        <trans-unit id="749810666e6448d7103b8c1ba2bbfef3e451d983" translate="yes" xml:space="preserve">
          <source>Separator string used when constructing new features for one-hot coding.</source>
          <target state="translated">one-hot coding을위한 새로운 기능을 구성 할 때 사용되는 구분자 문자열입니다.</target>
        </trans-unit>
        <trans-unit id="70aafd2a89678b32332fcfe0ff93efd39c5a3c06" translate="yes" xml:space="preserve">
          <source>Sequence of integer labels or multilabel data to encode.</source>
          <target state="translated">인코딩 할 정수 레이블 또는 다중 레이블 데이터 시퀀스.</target>
        </trans-unit>
        <trans-unit id="63145e1c892f5c29b2a500cdc3f15222c0784b14" translate="yes" xml:space="preserve">
          <source>Sequence of resampled copies of the collections. The original arrays are not impacted.</source>
          <target state="translated">컬렉션의 재 샘플링 된 사본 시퀀스. 원래 배열에는 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="22f488ee4fca141470b8e2b188abe2e7275b3dc0" translate="yes" xml:space="preserve">
          <source>Sequence of shuffled copies of the collections. The original arrays are not impacted.</source>
          <target state="translated">컬렉션의 셔플 사본 순서. 원래 배열에는 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="05f31ec9564cc0e3d1b047a8eba0a9e234d2f0b3" translate="yes" xml:space="preserve">
          <source>Sequence of weights (&lt;code&gt;float&lt;/code&gt; or &lt;code&gt;int&lt;/code&gt;) to weight the occurrences of predicted class labels (&lt;code&gt;hard&lt;/code&gt; voting) or class probabilities before averaging (&lt;code&gt;soft&lt;/code&gt; voting). Uses uniform weights if &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">무게 (시퀀스 &lt;code&gt;float&lt;/code&gt; 또는 &lt;code&gt;int&lt;/code&gt; ) 예측 클래스 레이블 (의 발생 무게 &lt;code&gt;hard&lt;/code&gt; 평균 (전 투표) 또는 클래스 확률 &lt;code&gt;soft&lt;/code&gt; 투표). &lt;code&gt;None&lt;/code&gt; 이면 균일 한 가중치를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="1b2a96243ee03211fb7854a57171cf92c85f5687" translate="yes" xml:space="preserve">
          <source>Sequence of weights (&lt;code&gt;float&lt;/code&gt; or &lt;code&gt;int&lt;/code&gt;) to weight the occurrences of predicted values before averaging. Uses uniform weights if &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ea6d0fbbfa7ff1125b3f0dc0d1f0f204d3b725f" translate="yes" xml:space="preserve">
          <source>Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be &amp;lsquo;transforms&amp;rsquo;, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using &lt;code&gt;memory&lt;/code&gt; argument.</source>
          <target state="translated">변환 목록과 최종 추정기를 순차적으로 적용하십시오. 파이프 라인의 중간 단계는 '변환'이어야합니다. 즉, 적합 및 변환 방법을 구현해야합니다. 최종 추정기는 적합을 구현하기 만하면됩니다. 파이프 라인의 변환기는 &lt;code&gt;memory&lt;/code&gt; 인수를 사용하여 캐시 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5d912fff074ca31c7c82b5fde02f4d9656aba0e0" translate="yes" xml:space="preserve">
          <source>Set &lt;code&gt;kernel='precomputed'&lt;/code&gt; and pass the Gram matrix instead of X in the fit method. At the moment, the kernel values between &lt;em&gt;all&lt;/em&gt; training vectors and the test vectors must be provided.</source>
          <target state="translated">&lt;code&gt;kernel='precomputed'&lt;/code&gt; 설정 하고 fit 메소드에서 X 대신 Gram 행렬을 전달하십시오. 현재 &lt;em&gt;모든&lt;/em&gt; 훈련 벡터와 테스트 벡터 사이의 커널 값을 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="82c89924e3ff5aee8a7d762157ebec36e4bfb77e" translate="yes" xml:space="preserve">
          <source>Set &lt;code&gt;n_clusters&lt;/code&gt; to a required value using &lt;code&gt;brc.set_params(n_clusters=n_clusters)&lt;/code&gt;.</source>
          <target state="translated">집합 &lt;code&gt;n_clusters&lt;/code&gt; 하여 필요한 값 &lt;code&gt;brc.set_params(n_clusters=n_clusters)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="baf739c6d3f3081a673c9a62345f43337d9146af" translate="yes" xml:space="preserve">
          <source>Set an initial start configuration, randomly or not.</source>
          <target state="translated">초기 시작 구성을 임의로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="acdb8317b38cc1be24a0a9627d99a9301a5b666a" translate="yes" xml:space="preserve">
          <source>Set and validate the parameters of estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7115214e952526d911c94e2c07be9533a4c1bc42" translate="yes" xml:space="preserve">
          <source>Set global scikit-learn configuration</source>
          <target state="translated">전역 scikit-learn 구성 설정</target>
        </trans-unit>
        <trans-unit id="d973ce66011b9fa67c29ae92b31d59e39ce28a97" translate="yes" xml:space="preserve">
          <source>Set of samples, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">샘플 세트. 여기서 n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="859e800d4c0c95faf85e59d644290b4f9223483a" translate="yes" xml:space="preserve">
          <source>Set the parameter C of class i to &lt;code&gt;class_weight[i]*C&lt;/code&gt; for SVC. If not given, all classes are supposed to have weight one. The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">SVC 의 경우 클래스 i의 매개 변수 C를 &lt;code&gt;class_weight[i]*C&lt;/code&gt; 로 설정하십시오. 주어지지 않으면 모든 수업은 1을가집니다. &quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정합니다.</target>
        </trans-unit>
        <trans-unit id="291ade590fbb6a8638dd1afb8f1376626f33f6ea" translate="yes" xml:space="preserve">
          <source>Set the parameter C of class i to &lt;code&gt;class_weight[i]*C&lt;/code&gt; for SVC. If not given, all classes are supposed to have weight one. The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68197d507e5463b3337bc09b3b9761d9525e528a" translate="yes" xml:space="preserve">
          <source>Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">SVC의 경우 클래스 i의 매개 변수 C를 class_weight [i] * C로 설정하십시오. 주어지지 않으면 모든 수업은 1을가집니다. &amp;ldquo;balanced&amp;rdquo;모드는 y의 값을 사용하여 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정합니다.</target>
        </trans-unit>
        <trans-unit id="7ee0e3c771d52a3f4b21637b50de4b2fa144cadb" translate="yes" xml:space="preserve">
          <source>Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">SVC의 경우 클래스 i의 매개 변수 C를 class_weight [i] * C로 설정하십시오. 주어지지 않으면 모든 수업은 1을가집니다. &quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정합니다.</target>
        </trans-unit>
        <trans-unit id="02c4dadc552ae2f0292cf77b2c6f20b9175838e2" translate="yes" xml:space="preserve">
          <source>Set the parameters</source>
          <target state="translated">파라미터 설정</target>
        </trans-unit>
        <trans-unit id="3f30532fe6a7a61216e1f94a622dfc009651d7c9" translate="yes" xml:space="preserve">
          <source>Set the parameters of an estimator from the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72f87d2d27b1f0c322a530ad0dc9b59597d7be5b" translate="yes" xml:space="preserve">
          <source>Set the parameters of this estimator.</source>
          <target state="translated">이 추정기의 매개 변수를 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="57d60e82b45349e99163b5cb25f5c26dc09997fb" translate="yes" xml:space="preserve">
          <source>Set the parameters of this kernel.</source>
          <target state="translated">이 커널의 매개 변수를 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="e51dac5748f92b9a4d95a295db13667c4d900b8f" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace computation during transformation.</source>
          <target state="translated">변환 중에 인플레 이스 계산을 수행하려면 False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="c9e442dcb8465293c9e9b1ca26f1df573cbc8a5b" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace computation.</source>
          <target state="translated">전체 계산을 수행하려면 False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="66ebe619c3b8004252e57f6a28ff4945f1da8024" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).</source>
          <target state="translated">인플레 이스 행 정규화를 수행하고 복사를 피하려면 (입력이 이미 numpy 배열 인 경우) False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="00972eb158db00f5dae23773f938b4091d65b472" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).</source>
          <target state="translated">인플레 이스 스케일링을 수행하고 복사를 피하려면 (입력이 이미 numpy 배열 인 경우) False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="08f65c010729649e9d6102eb99a0ed3b76fe0859" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array).</source>
          <target state="translated">인플레 이스 변환을 수행하고 복사를 피하려면 (입력이 이미 numpy 배열 인 경우) False로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="bbfbd49ae916b95e446b3c89c15541e5023cd4ad" translate="yes" xml:space="preserve">
          <source>Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array). If True, a copy of &lt;code&gt;X&lt;/code&gt; is transformed, leaving the original &lt;code&gt;X&lt;/code&gt; unchanged</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70773d7b4f76452047259ed8e2e9af7169f13fc0" translate="yes" xml:space="preserve">
          <source>Set to True to apply zero-mean, unit-variance normalization to the transformed output.</source>
          <target state="translated">변환 된 출력에 제로 평균 단위 분산 정규화를 적용하려면 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="31263c2a03fef7d2c1e558ffe5e1f7ce22d5913e" translate="yes" xml:space="preserve">
          <source>Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7d505eb35ec97e19208554a83f21f5336e3915d" translate="yes" xml:space="preserve">
          <source>Set to true if output binary array is desired in CSR sparse format</source>
          <target state="translated">CSR 스파 스 형식으로 출력 이진 배열을 원하는 경우 true로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="e9779b7ab3cf4b478b4bfa7669bfc4f6492ae2ea" translate="yes" xml:space="preserve">
          <source>Sets the default value for the &lt;code&gt;assume_finite&lt;/code&gt; argument of &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;sklearn.set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">의 기본값으로 설정 &lt;code&gt;assume_finite&lt;/code&gt; 의 인수 &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;sklearn.set_config&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2047224d21bf76be06bd841e22b2f6efe81f5987" translate="yes" xml:space="preserve">
          <source>Sets the default value for the &lt;code&gt;working_memory&lt;/code&gt; argument of &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;sklearn.set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;sklearn.set_config&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;working_memory&lt;/code&gt; 인수에 대한 기본값을 설정합니다 .</target>
        </trans-unit>
        <trans-unit id="8fc661c1feefb08f484398590a9cfaa0d200700d" translate="yes" xml:space="preserve">
          <source>Sets the seed of the global random generator when running the tests, for reproducibility.</source>
          <target state="translated">테스트를 실행할 때 재현성을 위해 글로벌 랜덤 생성기의 시드를 설정합니다.</target>
        </trans-unit>
        <trans-unit id="75bad9ccfc79ab0c0bbe60c8449ed7b0c754d17f" translate="yes" xml:space="preserve">
          <source>Sets the value to return when there is a zero division, i.e. when all predictions and labels are negative. If set to &amp;ldquo;warn&amp;rdquo;, this acts as 0, but warnings are also raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5785bcff96fce8eedf96c87581cbc35bae5756d5" translate="yes" xml:space="preserve">
          <source>Sets the value to return when there is a zero division. If set to &amp;ldquo;warn&amp;rdquo;, this acts as 0, but warnings are also raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e057cbf5e19c7dcf916bf2fe2aa47295c10430c1" translate="yes" xml:space="preserve">
          <source>Sets the value to return when there is a zero division:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ceca261ca4bfaf0dac2e7a5f6879bae3049e05bd" translate="yes" xml:space="preserve">
          <source>Sets the verbosity amount</source>
          <target state="translated">상세 량을 설정합니다</target>
        </trans-unit>
        <trans-unit id="0f757b166230d0f61e44fc2003ab4f4a4d10043d" translate="yes" xml:space="preserve">
          <source>Setting &lt;code&gt;generate_only=True&lt;/code&gt; returns a generator that yields (estimator, check) tuples where the check can be called independently from each other, i.e. &lt;code&gt;check(estimator)&lt;/code&gt;. This allows all checks to be run independently and report the checks that are failing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41f97bb142955ba403db62394a8510aa45205b7b" translate="yes" xml:space="preserve">
          <source>Setting it to True gets the various classifiers and the parameters of the classifiers as well</source>
          <target state="translated">이를 True로 설정하면 다양한 분류기와 분류기의 매개 변수도 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="924da9eef84794e1bcb0c0c5d50e7650f0dfc881" translate="yes" xml:space="preserve">
          <source>Setting it to True gets the various classifiers and the parameters of the classifiers as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e750c0ef44cf143b57f79a94939ea31cd10193d" translate="yes" xml:space="preserve">
          <source>Setting print_changed_only to True will alternate the representation of estimators to only show the parameters that have been set to non-default values. This can be used to have more compact representations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2623b7b1ad6f2c3b5492832d70831c56aba6aac8" translate="yes" xml:space="preserve">
          <source>Setting the parameter by cross-validating the likelihood on three folds according to a grid of potential shrinkage parameters.</source>
          <target state="translated">잠재적 수축 매개 변수의 그리드에 따라 가능성을 3 배로 교차 검증하여 매개 변수를 설정합니다.</target>
        </trans-unit>
        <trans-unit id="edca67601d1a75cccde1deb24fddb7bb63088fcb" translate="yes" xml:space="preserve">
          <source>Setting the parameters for the voting classifier</source>
          <target state="translated">투표 분류기의 매개 변수 설정</target>
        </trans-unit>
        <trans-unit id="cb6261b9db86d6920a006098fc7538ed80a40df3" translate="yes" xml:space="preserve">
          <source>Several estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;) can cluster together only neighboring pixels of an image, thus forming contiguous patches:</source>
          <target state="translated">scikit-learn의 여러 추정기는 기능 또는 샘플간에 연결 정보를 사용할 수 있습니다. 예를 들어 Ward 클러스터링 ( &lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;계층 클러스터링&lt;/a&gt; )은 이미지의 인접 픽셀 만 함께 클러스터링 하여 연속적인 패치를 형성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b2ad224369c25dffec31e32504aa18be16f8d837" translate="yes" xml:space="preserve">
          <source>Several functions allow you to analyze the precision, recall and F-measures score:</source>
          <target state="translated">여러 기능을 통해 정밀도, 리콜 및 F 측정 점수를 분석 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="beccb29e29ebc99080f8c36d4203650a9f29b872" translate="yes" xml:space="preserve">
          <source>Several methods have been developed to compare two sets of biclusters. For now, only &lt;a href=&quot;generated/sklearn.metrics.consensus_score#sklearn.metrics.consensus_score&quot;&gt;&lt;code&gt;consensus_score&lt;/code&gt;&lt;/a&gt; (Hochreiter et. al., 2010) is available:</source>
          <target state="translated">두 세트의 biclusters를 비교하기위한 몇 가지 방법이 개발되었습니다. 현재는 &lt;a href=&quot;generated/sklearn.metrics.consensus_score#sklearn.metrics.consensus_score&quot;&gt; &lt;code&gt;consensus_score&lt;/code&gt; &lt;/a&gt; (Hochreiter et al., 2010) 만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bf696ed0c48f638295bdb050d71aac6a4287ef6f" translate="yes" xml:space="preserve">
          <source>Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.</source>
          <target state="translated">scikit-learn에서 여러 회귀 및 이진 분류 알고리즘을 사용할 수 있습니다. 이러한 알고리즘을 멀티 클래스 분류 사례로 확장하는 간단한 방법은 소위 one-vs-all 체계를 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="0ac410486b823defe3030785e8a86edcf2b2b7e4" translate="yes" xml:space="preserve">
          <source>Severity Model - Gamma distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e301dd6062f7e9a79975fe8e2d0ba91694c4dbc3" translate="yes" xml:space="preserve">
          <source>Sex</source>
          <target state="translated">Sex</target>
        </trans-unit>
        <trans-unit id="94351e57e5ad4d9a685a9e5e4a3a8ed2b422ed01" translate="yes" xml:space="preserve">
          <source>Shape of the data arrays</source>
          <target state="translated">데이터 배열의 모양</target>
        </trans-unit>
        <trans-unit id="6ce851a20ced87e3a45210428f1caa987910f68a" translate="yes" xml:space="preserve">
          <source>Shape of the i&amp;rsquo;th bicluster.</source>
          <target state="translated">i 번째 bicluster의 모양입니다.</target>
        </trans-unit>
        <trans-unit id="e14b35d505512b3adb2f8997ae35ca2be24040d8" translate="yes" xml:space="preserve">
          <source>Shape will be [n_samples, 1] for binary problems.</source>
          <target state="translated">이진 문제의 경우 형태는 [n_samples, 1]입니다.</target>
        </trans-unit>
        <trans-unit id="f4aa10e40109dde70a9d57a4c3969b16b2895540" translate="yes" xml:space="preserve">
          <source>Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep].</source>
          <target state="translated">지정된 값만큼 피처를 이동합니다. None 인 경우 [-class_sep, class_sep]에 그려진 임의의 값으로 기능이 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="89ec1dbbc8f85faf0ad282b8a6481e07a4785260" translate="yes" xml:space="preserve">
          <source>Shifted opposite of the Local Outlier Factor of X.</source>
          <target state="translated">X의 로컬 특이 치 계수와 반대로 이동했습니다.</target>
        </trans-unit>
        <trans-unit id="5433cd73ac014316d0b32695693eab5029601309" translate="yes" xml:space="preserve">
          <source>Shorthand</source>
          <target state="translated">Shorthand</target>
        </trans-unit>
        <trans-unit id="a8178c51c2cc3204c708328447fd16ef389ce9b6" translate="yes" xml:space="preserve">
          <source>Should be used when memory is inefficient to train all data. Chunks of data can be passed in several iteration, where the first call should have an array of all target variables.</source>
          <target state="translated">메모리가 모든 데이터를 훈련시키는 데 비효율적 일 때 사용해야합니다. 데이터 청크는 여러 번 반복하여 전달 될 수 있으며, 첫 번째 호출에는 모든 대상 변수의 배열이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="12700416ee0fef7fdd5157d1c27acbb9da13d5c9" translate="yes" xml:space="preserve">
          <source>Should be used when memory is inefficient to train all data. Chunks of data can be passed in several iteration.</source>
          <target state="translated">메모리가 모든 데이터를 훈련시키는 데 비효율적 일 때 사용해야합니다. 여러 개의 데이터를 여러 번 반복하여 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec934ba88e117c3577f933302800f3ab4b85705a" translate="yes" xml:space="preserve">
          <source>Show below is a logistic-regression classifiers decision boundaries on the first two dimensions (sepal length and width) of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; dataset. The datapoints are colored according to their labels.</source>
          <target state="translated">아래는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;홍채&lt;/a&gt; 데이터 세트 의 처음 두 차원 (sepal length 및 width)에 대한 로지스틱 회귀 분류기 결정 경계입니다 . 데이터 포인트는 레이블에 따라 색상이 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="c74e263b32d3703a876a54ba7cc367e3fb1c6bbb" translate="yes" xml:space="preserve">
          <source>Shown in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e. class one or two, using the logistic curve.</source>
          <target state="translated">이 합성 데이터 세트에서 로지스틱 회귀 분석을 사용하여 로지스틱 곡선을 사용하여 값을 0 또는 1, 즉 클래스 1 또는 2로 분류하는 방법이 도표에 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="ca5bc8cbcc9592e82a2ca132c00133d4ad37408e" translate="yes" xml:space="preserve">
          <source>Shows how shrinkage improves classification.</source>
          <target state="translated">수축이 분류를 개선하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="5dc7ad8809a977f328219d536276f520094e2981" translate="yes" xml:space="preserve">
          <source>Shows how to use a function transformer in a pipeline. If you know your dataset&amp;rsquo;s first principle component is irrelevant for a classification task, you can use the FunctionTransformer to select all but the first column of the PCA transformed data.</source>
          <target state="translated">파이프 라인에서 함수 변압기를 사용하는 방법을 보여줍니다. 데이터 집합의 첫 번째 기본 구성 요소가 분류 작업과 관련이 없다는 것을 알고 있으면 FunctionTransformer를 사용하여 PCA 변환 된 데이터의 첫 번째 열을 제외한 모든 열을 선택할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f535d0d4250bfadc5c1c6932476e7cb22e7db70e" translate="yes" xml:space="preserve">
          <source>Shows the effect of collinearity in the coefficients of an estimator.</source>
          <target state="translated">추정기의 계수에서 공선 성의 효과를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="1a78e7f7618436a20d69e64d9d5ffb3bc060c908" translate="yes" xml:space="preserve">
          <source>Shrinkage</source>
          <target state="translated">Shrinkage</target>
        </trans-unit>
        <trans-unit id="29ad8c0361eee52379ab28eb86f7303c232b073b" translate="yes" xml:space="preserve">
          <source>Shrinkage and sparsity with logistic regression</source>
          <target state="translated">로지스틱 회귀 분석을 통한 수축 및 희소성</target>
        </trans-unit>
        <trans-unit id="92e7e7782831a32d85f1f4adb6e6848b9931e9f2" translate="yes" xml:space="preserve">
          <source>Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</source>
          <target state="translated">수축 공분산 추정 : LedoitWolf vs OAS 및 최대 우도</target>
        </trans-unit>
        <trans-unit id="2e2068ed5693c53cc14ed41dc7c2ee819779a1f5" translate="yes" xml:space="preserve">
          <source>Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the &lt;code&gt;shrinkage&lt;/code&gt; parameter of the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt; class to &amp;lsquo;auto&amp;rsquo;. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf &lt;a href=&quot;#id6&quot; id=&quot;id4&quot;&gt;2&lt;/a&gt;. Note that currently shrinkage only works when setting the &lt;code&gt;solver&lt;/code&gt; parameter to &amp;lsquo;lsqr&amp;rsquo; or &amp;lsquo;eigen&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc56f7d6f334df6e1bf7e25fb6694a2b96d3283e" translate="yes" xml:space="preserve">
          <source>Shrinkage is a tool to improve estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator. Shrinkage LDA can be used by setting the &lt;code&gt;shrinkage&lt;/code&gt; parameter of the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt; class to &amp;lsquo;auto&amp;rsquo;. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf &lt;a href=&quot;#id5&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;. Note that currently shrinkage only works when setting the &lt;code&gt;solver&lt;/code&gt; parameter to &amp;lsquo;lsqr&amp;rsquo; or &amp;lsquo;eigen&amp;rsquo;.</source>
          <target state="translated">수축은 특징의 수에 비해 훈련 샘플의 수가 적은 상황에서 공분산 행렬의 추정을 향상시키는 도구입니다. 이 시나리오에서 경험적 샘플 공분산은 좋지 않은 추정기입니다. 수축 LDA는 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; 클래스 의 &lt;code&gt;shrinkage&lt;/code&gt; 매개 변수 를 'auto' 로 설정하여 사용할 수 있습니다 . 이것은 Ledoit과 Wolf에 의해 도입 된 음모에 따라 분석 방식으로 최적 수축 매개 변수를 자동으로 결정합니다 &lt;a href=&quot;#id5&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt; . 현재 수축은 &lt;code&gt;solver&lt;/code&gt; 매개 변수를 'lsqr'또는 'eigen'으로 설정 한 경우에만 작동합니다 .</target>
        </trans-unit>
        <trans-unit id="7e8136e1a5918ee41b1666fa514179c5cb22402c" translate="yes" xml:space="preserve">
          <source>Shrinkage parameter, possible values:</source>
          <target state="translated">수축 매개 변수, 가능한 값 :</target>
        </trans-unit>
        <trans-unit id="b2a27e6ba825492dec9776790877b64e516e75e0" translate="yes" xml:space="preserve">
          <source>Shrunk covariance.</source>
          <target state="translated">축소 공분산.</target>
        </trans-unit>
        <trans-unit id="4dcdf0ff13bd4f7b65e07eadf0216796b5d56197" translate="yes" xml:space="preserve">
          <source>Shuffle arrays or sparse matrices in a consistent way</source>
          <target state="translated">일관된 방식으로 배열 또는 희소 행렬 섞기</target>
        </trans-unit>
        <trans-unit id="c0ccd0261920fa2fccaab512e3420b322d650304" translate="yes" xml:space="preserve">
          <source>Shuffle the samples and the features.</source>
          <target state="translated">샘플과 기능을 섞습니다.</target>
        </trans-unit>
        <trans-unit id="372aba820bed6f2900292d1b119c1b7c02346b33" translate="yes" xml:space="preserve">
          <source>Shuffle the samples.</source>
          <target state="translated">샘플을 섞는다.</target>
        </trans-unit>
        <trans-unit id="bb741d2d7cb4e292767bcf7b4c4d2a7dcedf441d" translate="yes" xml:space="preserve">
          <source>Shuffle-Group(s)-Out cross-validation iterator</source>
          <target state="translated">셔플 그룹-교차 유효성 검사 반복기</target>
        </trans-unit>
        <trans-unit id="04a76dd0a6286b28de9940305c73988458741a00" translate="yes" xml:space="preserve">
          <source>Signed distance is positive for an inlier and negative for an outlier.</source>
          <target state="translated">부호있는 거리는 이너에 대해서는 양수이고 특이 치에 대해서는 음수입니다.</target>
        </trans-unit>
        <trans-unit id="175a8f49ca538859a1536806ea283ecf7546e18e" translate="yes" xml:space="preserve">
          <source>Signed distance to the separating hyperplane.</source>
          <target state="translated">분리 초평면까지의 서명 된 거리.</target>
        </trans-unit>
        <trans-unit id="bdea7e4b3b56af1c4dc44f101507e6d5fde4c3c5" translate="yes" xml:space="preserve">
          <source>Silhouette Coefficient for each samples.</source>
          <target state="translated">각 샘플에 대한 실루엣 계수.</target>
        </trans-unit>
        <trans-unit id="cef2e4d37f21e366fe4348cb5c8e3de442e95913" translate="yes" xml:space="preserve">
          <source>Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].</source>
          <target state="translated">실루엣 분석을 사용하여 결과 군집 간의 분리 거리를 연구 할 수 있습니다. 실루엣 플롯은 한 군집의 각 점이 인접 군집의 점에 얼마나 가까운 지에 대한 측정 값을 표시하므로 군집 수와 같은 매개 변수를 시각적으로 평가할 수있는 방법을 제공합니다. 이 측정 값의 범위는 [-1, 1]입니다.</target>
        </trans-unit>
        <trans-unit id="f28647d65c56d46a3ca67f993f108ac366d59691" translate="yes" xml:space="preserve">
          <source>Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.</source>
          <target state="translated">+1 근처의 실루엣 계수 (이러한 값이라고 함)는 샘플이 인접 군집에서 멀리 떨어져 있음을 나타냅니다. 값이 0이면 샘플이 인접한 두 군집 사이의 결정 경계에 있거나 매우 가깝다는 것을 나타내고 음수 값은 해당 샘플이 잘못된 군집에 할당되었을 수 있음을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="88d328be635604c256d2743bcb180fd1daab0b36" translate="yes" xml:space="preserve">
          <source>Similar feature extractors should be built for other kind of unstructured data input such as images, audio, video, &amp;hellip;</source>
          <target state="translated">이미지, 오디오, 비디오 등과 같은 다른 종류의 비정형 데이터 입력을 위해 유사한 기능 추출기를 구축해야합니다.</target>
        </trans-unit>
        <trans-unit id="9be20d5d3cad647d5b5693ebaedd1ee1a23948cc" translate="yes" xml:space="preserve">
          <source>Similar to &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; but only a single metric is permitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="319655ff7753a6199642b7bf6692dc2bf99bfe55" translate="yes" xml:space="preserve">
          <source>Similar to AgglomerativeClustering, but recursively merges features instead of samples.</source>
          <target state="translated">AgglomerativeClustering과 유사하지만 샘플 대신 기능을 재귀 적으로 병합합니다.</target>
        </trans-unit>
        <trans-unit id="133d603767b5dc043e4bab49b5255c4ddd0f05fe" translate="yes" xml:space="preserve">
          <source>Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.</source>
          <target state="translated">회귀 분석을 위해 NuSVC와 유사하게 매개 변수 nu를 사용하여 지원 벡터 수를 제어합니다. 그러나 nu가 C를 대체하는 NuSVC와 달리 여기서 nu는 epsilon-SVR의 매개 변수 epsilon을 대체합니다.</target>
        </trans-unit>
        <trans-unit id="d1a2b055f0753742d67fc90d1d4811e0a5d9ab30" translate="yes" xml:space="preserve">
          <source>Similar to SVC but uses a parameter to control the number of support vectors.</source>
          <target state="translated">SVC와 유사하지만 매개 변수를 사용하여 지원 벡터 수를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="367343dd50d61c27ddbb7a06df2fb9885bdf8a5f" translate="yes" xml:space="preserve">
          <source>Similar to SVC with parameter kernel=&amp;rsquo;linear&amp;rsquo;, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.</source>
          <target state="translated">kernel = 'linear'매개 변수를 사용하는 SVC와 유사하지만 libsvm이 아닌 liblinear의 측면에서 구현되므로 페널티 및 손실 함수를 선택할 때 더 많은 유연성을 가지며 많은 수의 샘플로 더 잘 확장되어야합니다.</target>
        </trans-unit>
        <trans-unit id="3ec63304462f4cbac1c3a261d38d9188bcded830" translate="yes" xml:space="preserve">
          <source>Similar to SVR with parameter kernel=&amp;rsquo;linear&amp;rsquo;, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.</source>
          <target state="translated">kernel = 'linear'매개 변수를 사용하는 SVR과 유사하지만 libsvm이 아닌 liblinear의 측면에서 구현되므로 페널티 및 손실 함수를 선택할 때 더 많은 유연성을 가지며 많은 수의 샘플로 더 잘 확장되어야합니다.</target>
        </trans-unit>
        <trans-unit id="997a429b680aeb0ca7576cadadd68b1d30fd4132" translate="yes" xml:space="preserve">
          <source>Similar to other boosting algorithms GBRT builds the additive model in a forward stagewise fashion:</source>
          <target state="translated">다른 부스팅 알고리즘과 유사하게 GBRT는 추가 단계 모델을 단계별로 빌드합니다.</target>
        </trans-unit>
        <trans-unit id="76a1a1878f09aac58b35d999b125160a70440000" translate="yes" xml:space="preserve">
          <source>Similar to other boosting algorithms, a GBRT is built in a greedy fashion:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9dba587c665016505432ed3d83545171f96e0b75" translate="yes" xml:space="preserve">
          <source>Similarity between individual biclusters is computed. Then the best matching between sets is found using the Hungarian algorithm. The final score is the sum of similarities divided by the size of the larger set.</source>
          <target state="translated">개별 biclusters 간의 유사성이 계산됩니다. 그런 다음 헝가리어 알고리즘을 사용하여 집합 간의 최상의 일치를 찾습니다. 최종 점수는 유사성의 합을 더 큰 세트의 크기로 나눈 것입니다.</target>
        </trans-unit>
        <trans-unit id="a365c849553e02aafca0dfedfc5010bc90d3ae71" translate="yes" xml:space="preserve">
          <source>Similarity score between -1.0 and 1.0. Random labelings have an ARI close to 0.0. 1.0 stands for perfect match.</source>
          <target state="translated">-1.0과 1.0 사이의 유사성 점수 임의 라벨링의 ARI는 0.0에 가깝습니다. 1.0은 완벽한 일치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="186186d91781f080c251077ac03fc20cf1639d0c" translate="yes" xml:space="preserve">
          <source>Similarly, &lt;a href=&quot;generated/sklearn.model_selection.repeatedstratifiedkfold#sklearn.model_selection.RepeatedStratifiedKFold&quot;&gt;&lt;code&gt;RepeatedStratifiedKFold&lt;/code&gt;&lt;/a&gt; repeats Stratified K-Fold n times with different randomization in each repetition.</source>
          <target state="translated">마찬가지로 &lt;a href=&quot;generated/sklearn.model_selection.repeatedstratifiedkfold#sklearn.model_selection.RepeatedStratifiedKFold&quot;&gt; &lt;code&gt;RepeatedStratifiedKFold&lt;/code&gt; &lt;/a&gt; 는 각 반복에서 다른 무작위 화로 Stratified K-Fold를 n 번 반복합니다.</target>
        </trans-unit>
        <trans-unit id="01f578d801e5d1ea722f26bf3985038251d17ab9" translate="yes" xml:space="preserve">
          <source>Similarly, L1 regularized logistic regression solves the following optimization problem</source>
          <target state="translated">마찬가지로 L1 정규화 된 로지스틱 회귀 분석은 다음 최적화 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="904046bd05dcb9fe24c66e7942e9a89936649854" translate="yes" xml:space="preserve">
          <source>Similarly, \(\ell_1\) regularized logistic regression solves the following optimization problem:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8ee1f0c1cff57e37c89253984120399103de69b" translate="yes" xml:space="preserve">
          <source>Similarly, a negative monotonic constraint is of the form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33d5515f485c474434afb456d44ce55ecb3a831d" translate="yes" xml:space="preserve">
          <source>Similarly, labels not present in the data sample may be accounted for in macro-averaging.</source>
          <target state="translated">유사하게, 데이터 샘플에 존재하지 않는 라벨은 매크로 평균화에서 설명 될 수있다.</target>
        </trans-unit>
        <trans-unit id="de3a0306d5f9d4f628a86437bd31f501c79f2495" translate="yes" xml:space="preserve">
          <source>Similarly, the precision recall curve can be plotted using &lt;code&gt;y_score&lt;/code&gt; from the prevision sections.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c868e091c0bbbbc855227dfcc9797f545ef094e" translate="yes" xml:space="preserve">
          <source>Simple 1D Kernel Density Estimation</source>
          <target state="translated">간단한 1D 커널 밀도 추정</target>
        </trans-unit>
        <trans-unit id="f5468d7aca1a86ccbbf784d0772796020bb33f7b" translate="yes" xml:space="preserve">
          <source>Simple to understand and to interpret. Trees can be visualised.</source>
          <target state="translated">이해하고 해석하기 간단합니다. 나무를 시각화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="954c17f332cbfd66dfa98282859837f21d64fd6a" translate="yes" xml:space="preserve">
          <source>Simple usage of Pipeline that runs successively a univariate feature selection with anova and then a C-SVM of the selected features.</source>
          <target state="translated">anova 및 선택한 기능의 C-SVM을 사용하여 일 변량 기능 선택을 연속적으로 실행하는 파이프 라인의 간단한 사용법.</target>
        </trans-unit>
        <trans-unit id="e7766eb7ad8cfdfa67609d5277fb9ac991ed77ce" translate="yes" xml:space="preserve">
          <source>Simple usage of Pipeline that runs successively a univariate feature selection with anova and then a SVM of the selected features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50f8d26df97413619de7bb6966a9aa041cc32e16" translate="yes" xml:space="preserve">
          <source>Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors.</source>
          <target state="translated">샘플을 분류하기위한 Support Vector Machines의 간단한 사용. 결정 표면과지지 벡터를 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="5b50d9c69163fc1e922706c7d40ea5de7c4c3507" translate="yes" xml:space="preserve">
          <source>Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA</source>
          <target state="translated">다양한 교차 분해 알고리즘의 간단한 사용법 :-PLSCanonical-PLSRegression, 다변량 반응, 일명 PLS2-PLSRegression, 일 변량 반응, 일명 PLS1-CCA</target>
        </trans-unit>
        <trans-unit id="da21ac2a81c42a0cc34c3a8c8243f3f710d2f668" translate="yes" xml:space="preserve">
          <source>SimpleImputer</source>
          <target state="translated">SimpleImputer</target>
        </trans-unit>
        <trans-unit id="0cd5c8d669edd41f72cf141b1f653ffc3a8f7d8a" translate="yes" xml:space="preserve">
          <source>Simply perform a svd on the crosscovariance matrix: X&amp;rsquo;Y There are no iterative deflation here.</source>
          <target state="translated">교차 공분산 행렬에서 간단히 svd를 수행하십시오. X'Y 여기에는 반복적 인 수축이 없습니다.</target>
        </trans-unit>
        <trans-unit id="f72f0eda605215d24ff9d1908550395272883fb4" translate="yes" xml:space="preserve">
          <source>Simulations</source>
          <target state="translated">Simulations</target>
        </trans-unit>
        <trans-unit id="2e04b6f26b355099b78d114e001527cec11f01b3" translate="yes" xml:space="preserve">
          <source>Since \(P(x_1, \dots, x_n)\) is constant given the input, we can use the following classification rule:</source>
          <target state="translated">입력이 주어지면 \ (P (x_1, \ dots, x_n) \)는 일정하므로 다음과 같은 분류 규칙을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4ed5170dfb2a4a5fe27dc284ef1f79230346d84b" translate="yes" xml:space="preserve">
          <source>Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.</source>
          <target state="translated">두 가지 다른 아키텍처에서는 모델 내부 표현이 다를 수 있으므로 한 아키텍처에서 모델을 덤프하고 다른 아키텍처에로드하는 것은 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="2e7dca0922f252e8bcb5dd7de62f190d029edf35" translate="yes" xml:space="preserve">
          <source>Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the &lt;code&gt;n_features&lt;/code&gt; parameter; otherwise the features will not be mapped evenly to the columns.</source>
          <target state="translated">간단한 모듈로가 해시 함수를 열 인덱스로 변환하는 데 사용되므로 &lt;code&gt;n_features&lt;/code&gt; 매개 변수 로 2의 거듭 제곱을 사용하는 것이 좋습니다 . 그렇지 않으면 기능이 열에 균등하게 매핑되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e94526c23963e4af6db5a385aa61965ac4ba9d0e" translate="yes" xml:space="preserve">
          <source>Since it requires to fit &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don&amp;rsquo;t scale well with &lt;code&gt;n_samples&lt;/code&gt;. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used &lt;code&gt;n_classes&lt;/code&gt; times.</source>
          <target state="translated">&lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; 분류기 에 맞아야하기 때문에이 방법은 일반적으로 O (n_classes ^ 2) 복잡성으로 인해 1 대 1보다 느립니다. 그러나이 방법은 &lt;code&gt;n_samples&lt;/code&gt; 와 함께 잘 확장되지 않는 커널 알고리즘과 같은 알고리즘에 유리할 수 있습니다 . 각 개별 학습 문제는 데이터의 작은 하위 집합 만 포함하고 나머지는 전체 데이터 세트가 &lt;code&gt;n_classes&lt;/code&gt; 시간 동안 사용 되기 때문 입니다.</target>
        </trans-unit>
        <trans-unit id="d70a1912c85de6341311b3ee0902966d17cafa41" translate="yes" xml:space="preserve">
          <source>Since it requires to fit &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don&amp;rsquo;t scale well with &lt;code&gt;n_samples&lt;/code&gt;. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used &lt;code&gt;n_classes&lt;/code&gt; times. The decision function is the result of a monotonic transformation of the one-versus-one classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="885cdc36b68d6b1fe527d22b8af012efa7ee88fc" translate="yes" xml:space="preserve">
          <source>Since our loss function is dependent on the amount of samples, the latter will influence the selected value of &lt;code&gt;C&lt;/code&gt;. The question that arises is &lt;code&gt;How do we optimally adjust C to account for the different amount of training samples?&lt;/code&gt;</source>
          <target state="translated">손실 함수는 샘플의 양에 따라 달라 지므로 후자는 선택한 &lt;code&gt;C&lt;/code&gt; 값에 영향을 미칩니다 . 발생하는 문제 &lt;code&gt;How do we optimally adjust C to account for the different amount of training samples?&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8414cbee67356c2ab2c8ba5220ad5c2247b09cc6" translate="yes" xml:space="preserve">
          <source>Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.</source>
          <target state="translated">재귀 분할은 트리 구조로 나타낼 수 있으므로 샘플을 분리하는 데 필요한 분할 수는 루트 노드에서 종료 노드까지의 경로 길이와 같습니다.</target>
        </trans-unit>
        <trans-unit id="e0effd5f72f2afc7c618332a9b819d624a406e57" translate="yes" xml:space="preserve">
          <source>Since the L1 norm promotes sparsity of features we might be interested in selecting only a subset of the most interesting features from the dataset. This example shows how to select two the most interesting features from the diabetes dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53564a374f656b4eebc264b9099b9cd10a25ad1a" translate="yes" xml:space="preserve">
          <source>Since the Poisson regressor internally models the log of the expected target value instead of the expected value directly (log vs identity link function), the relationship between X and y is not exactly linear anymore. Therefore the Poisson regressor is called a Generalized Linear Model (GLM) rather than a vanilla linear model as is the case for Ridge regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41606a5be005625c4678c2fc6968d98c5bcdacbb" translate="yes" xml:space="preserve">
          <source>Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature&amp;rsquo;s value is zero. This mechanism is enabled by default with &lt;code&gt;alternate_sign=True&lt;/code&gt; and is particularly useful for small hash table sizes (&lt;code&gt;n_features &amp;lt; 10000&lt;/code&gt;). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like &lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt;&lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;sklearn.feature_selection.chi2&lt;/code&gt;&lt;/a&gt; feature selectors that expect non-negative inputs.</source>
          <target state="translated">해시 함수가 (관련되지 않은) 기능간에 충돌을 일으킬 수 있으므로 서명 된 해시 함수가 사용되며 해시 값의 부호가 기능의 출력 행렬에 저장된 값의 부호를 결정합니다. 이런 식으로 충돌이 누적 오류가 아닌 취소 될 수 있으며 출력 기능 값의 예상 평균은 0입니다. 이 메커니즘은 기본적으로 &lt;code&gt;alternate_sign=True&lt;/code&gt; 로 활성화되며 작은 해시 테이블 크기 ( &lt;code&gt;n_features &amp;lt; 10000&lt;/code&gt; )에 특히 유용합니다 . 큰 해시 테이블 크기의 경우, 음이 아닌 입력을 기대하는 &lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt; &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;sklearn.feature_selection.chi2&lt;/code&gt; &lt;/a&gt; 기능 선택기 와 같은 추정기로 출력을 전달할 수 있도록 비활성화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="91020f655b0d3976000fc58a456fa0d94621c5a6" translate="yes" xml:space="preserve">
          <source>Since the kernel that is to be approximated is additive, the components of the input vectors can be treated separately. Each entry in the original space is transformed into 2*sample_steps+1 features, where sample_steps is a parameter of the method. Typical values of sample_steps include 1, 2 and 3.</source>
          <target state="translated">근사 될 커널은 부가 적이므로, 입력 벡터의 구성 요소는 개별적으로 처리 될 수 있습니다. 원래 공간의 각 항목은 2 * sample_steps + 1 기능으로 변환되며, 여기서 sample_steps는 메소드의 매개 변수입니다. sample_steps의 일반적인 값은 1, 2 및 3입니다.</target>
        </trans-unit>
        <trans-unit id="eab55792648442f25c33ef137afa5ea98f14550a" translate="yes" xml:space="preserve">
          <source>Since the linear predictor \(Xw\) can be negative and Poisson, Gamma and Inverse Gaussian distributions don&amp;rsquo;t support negative values, it is necessary to apply an inverse link function that guarantees the non-negativeness. For example with &lt;code&gt;link='log'&lt;/code&gt;, the inverse link function becomes \(h(Xw)=\exp(Xw)\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dac98c213a89fd4774cf1c1f95b63e10c23ed6ab" translate="yes" xml:space="preserve">
          <source>Since the posterior is intractable, variational Bayesian method uses a simpler distribution \(q(z,\theta,\beta | \lambda, \phi, \gamma)\) to approximate it, and those variational parameters \(\lambda\), \(\phi\), \(\gamma\) are optimized to maximize the Evidence Lower Bound (ELBO):</source>
          <target state="translated">후자는 다루기 어렵 기 때문에 변형 베이지안 방법은 더 간단한 분포 \ (q (z, \ theta, \ beta | \ lambda, \ phi, \ gamma) \)를 사용하여 근사값을 구하고 그 변형 매개 변수 \ (\ lambda \) , \ (\ phi \), \ (\ gamma \)는 ELBO (Evidence Lower Bound)를 최대화하도록 최적화되었습니다.</target>
        </trans-unit>
        <trans-unit id="ce93b96a689b29304c626bbff15b3c9ae5662f8f" translate="yes" xml:space="preserve">
          <source>Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both &lt;code&gt;fpr&lt;/code&gt; and &lt;code&gt;tpr&lt;/code&gt;, which are sorted in reversed order during their calculation.</source>
          <target state="translated">임계 값은 낮은 값에서 높은 값 으로 정렬되므로 계산시 역순으로 정렬 된 &lt;code&gt;fpr&lt;/code&gt; 및 &lt;code&gt;tpr&lt;/code&gt; 에 모두 해당하는지 확인하기 위해 반환시 역순으로 정렬됩니다.</target>
        </trans-unit>
        <trans-unit id="7f69d32984c3b4f143cfa37bb4e60432050ed0eb" translate="yes" xml:space="preserve">
          <source>Since there has not been much empirical work using approximate embeddings, it is advisable to compare results against exact kernel methods when possible.</source>
          <target state="translated">대략적인 임베딩을 사용한 경험적 작업이 많지 않았으므로 가능한 경우 정확한 커널 방법과 결과를 비교하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="181ed345f14e5249ac33bd9934643c4f9dd72c8f" translate="yes" xml:space="preserve">
          <source>Since v0.21, if &lt;code&gt;input&lt;/code&gt; is &lt;code&gt;filename&lt;/code&gt; or &lt;code&gt;file&lt;/code&gt;, the data is first read from the file and then passed to the given callable analyzer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa984969d0a90a5820c4fc022c64bfc47ca5a084" translate="yes" xml:space="preserve">
          <source>Single estimator versus bagging: bias-variance decomposition</source>
          <target state="translated">단일 추정기 대 배깅 : 바이어스-분산 분해</target>
        </trans-unit>
        <trans-unit id="c2fe1a00c3aef2fdadf0dd5e7ea7933f55e2ab1b" translate="yes" xml:space="preserve">
          <source>Single metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 를 사용한 단일 메트릭 평가</target>
        </trans-unit>
        <trans-unit id="78a22764fe4a9b48a649589e690300655f65c80d" translate="yes" xml:space="preserve">
          <source>Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance (&lt;em&gt;l2&lt;/em&gt;), Manhattan distance (or Cityblock, or &lt;em&gt;l1&lt;/em&gt;), cosine distance, or any precomputed affinity matrix.</source>
          <target state="translated">하나는 평균 완전 결합은 특히 유클리드 거리 (에서는 거리 (또는 유사성)의 다양한 사용될 수 &lt;em&gt;L2&lt;/em&gt; ), 맨하탄 거리 (또는 Cityblock 또는 &lt;em&gt;L1&lt;/em&gt; ), 코사인 거리, 또는 미리 계산 된 친 화성 기질.</target>
        </trans-unit>
        <trans-unit id="b1a526a4c2ab5cc879cbcf97bbdfc6e58be65f64" translate="yes" xml:space="preserve">
          <source>Singular values of &lt;code&gt;X&lt;/code&gt;. Only available when &lt;code&gt;X&lt;/code&gt; is dense.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="409f99dea48046b433f18bebd495f19a0bb51787" translate="yes" xml:space="preserve">
          <source>Singularities</source>
          <target state="translated">Singularities</target>
        </trans-unit>
        <trans-unit id="857a843ae0f540aecaddebae91ddc74b518d5cf4" translate="yes" xml:space="preserve">
          <source>Singularities:</source>
          <target state="translated">Singularities:</target>
        </trans-unit>
        <trans-unit id="2df59d349ec07bd33a82cdc2b0c4c3d4152244c6" translate="yes" xml:space="preserve">
          <source>Size of minibatches for stochastic optimizers. If the solver is &amp;lsquo;lbfgs&amp;rsquo;, the classifier will not use minibatch. When set to &amp;ldquo;auto&amp;rdquo;, &lt;code&gt;batch_size=min(200, n_samples)&lt;/code&gt;</source>
          <target state="translated">확률 적 최적화를위한 미니 배치 크기. 솔버가 'lbfgs'인 경우 분류기는 미니 배치를 사용하지 않습니다. &quot;auto&quot;로 설정하면 &lt;code&gt;batch_size=min(200, n_samples)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7241ed1ae94944f10d565b9b8502a2569d69bebc" translate="yes" xml:space="preserve">
          <source>Size of text font. If None, determined automatically to fit figure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa65aa30a853af1cf81f53119b6649c5aa5e2817" translate="yes" xml:space="preserve">
          <source>Size of the blocks into which the covariance matrix will be split during its Ledoit-Wolf estimation. This is purely a memory optimization and does not affect results.</source>
          <target state="translated">공분산 행렬이 Ledoit-Wolf 추정 중에 분할 될 블록의 크기입니다. 이것은 순전히 메모리 최적화이며 결과에 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="3bdf2049b3b05b0720764317c09e88bc19700eec" translate="yes" xml:space="preserve">
          <source>Size of the blocks into which the covariance matrix will be split. This is purely a memory optimization and does not affect results.</source>
          <target state="translated">공분산 행렬이 분할 될 블록의 크기입니다. 이것은 순전히 메모리 최적화이며 결과에 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="612eb8e8a229547855d2a4c02d66bbe2afb0395a" translate="yes" xml:space="preserve">
          <source>Size of the mini batches.</source>
          <target state="translated">미니 배치의 크기.</target>
        </trans-unit>
        <trans-unit id="aa636a80a54912b13ca3fa6029e0a40e02f80415" translate="yes" xml:space="preserve">
          <source>Size of the return array</source>
          <target state="translated">반환 배열의 크기</target>
        </trans-unit>
        <trans-unit id="08f603d3fe1f30df7982a9a08f592731c9eab73e" translate="yes" xml:space="preserve">
          <source>Size of the test sets.</source>
          <target state="translated">테스트 세트의 크기</target>
        </trans-unit>
        <trans-unit id="3ede3a7d9dde54c062e45da23a9dc65bbb39cbbf" translate="yes" xml:space="preserve">
          <source>Size of the test sets. Must be strictly less than the number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d7d650781fdf69336502b899ccd5c9f80ba4848" translate="yes" xml:space="preserve">
          <source>Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False.</source>
          <target state="translated">check_input = False 일 때 호출자가 처리한다고 가정 할 때 Gram 매트릭스를 포함하여 입력 유효성 검사를 건너 뜁니다.</target>
        </trans-unit>
        <trans-unit id="119077d89fb1cbe89db9591404feee43530ef290" translate="yes" xml:space="preserve">
          <source>Slides explaining PLS</source>
          <target state="translated">PLS를 설명하는 슬라이드</target>
        </trans-unit>
        <trans-unit id="c3d721c0cfe644c7ca720484ae796856345cb087" translate="yes" xml:space="preserve">
          <source>Small outliers</source>
          <target state="translated">작은 특이 치</target>
        </trans-unit>
        <trans-unit id="6cf8c0d1548c80d5c7b8e80adf89b56b8a30e60f" translate="yes" xml:space="preserve">
          <source>Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates. Alpha corresponds to &lt;code&gt;(2*C)^-1&lt;/code&gt; in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.</source>
          <target state="translated">작은 양의 알파 값은 문제의 컨디셔닝을 개선하고 추정값의 분산을 줄입니다. 알파 는 LogisticRegression 또는 LinearSVC와 같은 다른 선형 모델에서 &lt;code&gt;(2*C)^-1&lt;/code&gt; 에 해당합니다. 배열이 전달되면 페널티는 대상에 특정한 것으로 간주됩니다. 따라서 숫자와 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="8e135bd52bd2eb3356a694f0d8575402c5375bb6" translate="yes" xml:space="preserve">
          <source>Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space.</source>
          <target state="translated">값이 작을수록 대상 투영 공간에 더 나은 임베딩 및 더 많은 차원 수 (n_components)가 생깁니다.</target>
        </trans-unit>
        <trans-unit id="178a5fd9e6a787566f82c9ecbd118e48b0edcccd" translate="yes" xml:space="preserve">
          <source>Smallest value of alpha / alpha_max considered</source>
          <target state="translated">고려되는 alpha / alpha_max의 최소값</target>
        </trans-unit>
        <trans-unit id="9ec75e4c898141f811f9d6fe4e66f6da7a97bb9c" translate="yes" xml:space="preserve">
          <source>Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.</source>
          <target state="translated">컬렉션의 모든 용어가 정확히 한 번만 포함 된 추가 문서가있는 것처럼 문서 빈도에 1을 추가하여 idf 가중치를 매끄럽게합니다. 제로 나누기를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="8ec8b1649217676578a05802f05dc4dfdec72ebc" translate="yes" xml:space="preserve">
          <source>Smoothed empirical log probability for each class.</source>
          <target state="translated">각 클래스에 대한 경험적 로그 확률이 ​​평활화되었습니다.</target>
        </trans-unit>
        <trans-unit id="d5d952f65cbc310a8284a0aa676904d4b84a635c" translate="yes" xml:space="preserve">
          <source>Smoothed empirical log probability for each class. Only used in edge case with a single class in the training set.</source>
          <target state="translated">각 클래스에 대한 경험적 로그 확률이 ​​평활화되었습니다. 트레이닝 세트에서 단일 클래스가있는 엣지 케이스에서만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="fb1739757cbc4d2da964b132a46ededacd98a2aa" translate="yes" xml:space="preserve">
          <source>Soft Voting/Majority Rule classifier for unfitted estimators.</source>
          <target state="translated">적합하지 않은 추정량에 대한 소프트 투표 / 주요 규칙 분류기.</target>
        </trans-unit>
        <trans-unit id="3c16977f20db1a9e128b2062c246165103dd7921" translate="yes" xml:space="preserve">
          <source>Soft Voting/Majority Rule classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a33a62c21ea4043dbf31a4f6ee598307b73aa466" translate="yes" xml:space="preserve">
          <source>Soft hint to choose the default backend if no specific backend was selected with the parallel_backend context manager. The default process-based backend is &amp;lsquo;loky&amp;rsquo; and the default thread-based backend is &amp;lsquo;threading&amp;rsquo;.</source>
          <target state="translated">parallel_backend 컨텍스트 관리자로 특정 백엔드를 선택하지 않은 경우 기본 백엔드를 선택하는 힌트 기본 프로세스 기반 백엔드는 'loky'이고 기본 스레드 기반 백엔드는 'threading'입니다.</target>
        </trans-unit>
        <trans-unit id="9653b7a05f5df3e5d87561ce96e265c541ad8c31" translate="yes" xml:space="preserve">
          <source>SokalMichenerDistance</source>
          <target state="translated">SokalMichenerDistance</target>
        </trans-unit>
        <trans-unit id="01ed2fbc860294634b46d80d008798b47284ef75" translate="yes" xml:space="preserve">
          <source>SokalSneathDistance</source>
          <target state="translated">SokalSneathDistance</target>
        </trans-unit>
        <trans-unit id="7472593b6d35821b7f5c4104f85f3418ec74c28e" translate="yes" xml:space="preserve">
          <source>Solution to the non-negative least squares problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b76645291a4941abead277af175738d7e9485f1" translate="yes" xml:space="preserve">
          <source>Solution: &lt;a href=&quot;http://scikit-learn.org/stable/_downloads/plot_digits_classification_exercise.py&quot;&gt;&lt;code&gt;../../auto_examples/exercises/plot_digits_classification_exercise.py&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">솔루션 : &lt;a href=&quot;http://scikit-learn.org/stable/_downloads/plot_digits_classification_exercise.py&quot;&gt; &lt;code&gt;../../auto_examples/exercises/plot_digits_classification_exercise.py&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="97a1e21d1798b81e7cea434e741d7087aa2f4a99" translate="yes" xml:space="preserve">
          <source>Solution: &lt;a href=&quot;http://scikit-learn.org/stable/_downloads/plot_iris_exercise.py&quot;&gt;&lt;code&gt;../../auto_examples/exercises/plot_iris_exercise.py&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">솔루션 : &lt;a href=&quot;http://scikit-learn.org/stable/_downloads/plot_iris_exercise.py&quot;&gt; &lt;code&gt;../../auto_examples/exercises/plot_iris_exercise.py&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8dc243287f9af0458afdd79c7e208cb930e6e9d1" translate="yes" xml:space="preserve">
          <source>Solution: &lt;a href=&quot;https://scikit-learn.org/0.23/_downloads/91f0cd01beb5b964a5e1ece5bdd15499/plot_iris_exercise.py&quot;&gt;&lt;code&gt;../../auto_examples/exercises/plot_iris_exercise.py&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5a155c549be47525e0ef469c6ede18a502d5bc6" translate="yes" xml:space="preserve">
          <source>Solution: &lt;a href=&quot;https://scikit-learn.org/0.23/_downloads/bfcebce45024b267e8546d6914acfedc/plot_digits_classification_exercise.py&quot;&gt;&lt;code&gt;../../auto_examples/exercises/plot_digits_classification_exercise.py&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="908bab59b18307a14acc0f3d3e00d2c36c09b88e" translate="yes" xml:space="preserve">
          <source>Solve the isotonic regression model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d31ac38ecaa97cd7ca9cf4223578d60df63f89a9" translate="yes" xml:space="preserve">
          <source>Solve the isotonic regression model:</source>
          <target state="translated">등장 성 회귀 모델을 푸십시오.</target>
        </trans-unit>
        <trans-unit id="48c6334f59edac84435c4184f66b59babd6924b9" translate="yes" xml:space="preserve">
          <source>Solve the ridge equation by the method of normal equations.</source>
          <target state="translated">정규 방정식의 방법으로 릿지 방정식을 풉니 다.</target>
        </trans-unit>
        <trans-unit id="b5fa00edfa7fc06c7e99359e114af78ec006b205" translate="yes" xml:space="preserve">
          <source>Solver to use in the computational routines:</source>
          <target state="translated">계산 루틴에서 사용할 해 찾기 :</target>
        </trans-unit>
        <trans-unit id="5c136e6e68fedeebc5e62ea492bdc13f5c51a357" translate="yes" xml:space="preserve">
          <source>Solver to use, possible values:</source>
          <target state="translated">사용할 해 찾기 가능한 값 :</target>
        </trans-unit>
        <trans-unit id="577db6a48ff5a1db9c02bebc0320d90f752c60ef" translate="yes" xml:space="preserve">
          <source>Solves a dictionary learning matrix factorization problem online.</source>
          <target state="translated">온라인 사전 학습 매트릭스 분해 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="8820b686f5bac3c2f3bf8f93441f10523c0fe031" translate="yes" xml:space="preserve">
          <source>Solves a dictionary learning matrix factorization problem.</source>
          <target state="translated">사전 학습 매트릭스 인수 분해 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="b22d518aabf32cc9c9347bf653295056dc359f7f" translate="yes" xml:space="preserve">
          <source>Solves n_targets Orthogonal Matching Pursuit problems using only the Gram matrix X.T * X and the product X.T * y.</source>
          <target state="translated">그램 행렬 XT * X 및 제품 XT * y 만 사용하여 n_targets 직교 매칭 추구 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="80547cf29da9d4cc131f68d1680f3500976f9f6f" translate="yes" xml:space="preserve">
          <source>Solves n_targets Orthogonal Matching Pursuit problems. An instance of the problem has the form:</source>
          <target state="translated">n_targets 직교 매칭 추구 문제를 해결합니다. 문제의 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8d4fd8866d93aa1260a7a3d03ef9a9a9f9a2fc7d" translate="yes" xml:space="preserve">
          <source>Solves the optimization problem:</source>
          <target state="translated">최적화 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="e29fb180670f8bd6283a93ce616785d39e9b899f" translate="yes" xml:space="preserve">
          <source>Some advantages of decision trees are:</source>
          <target state="translated">의사 결정 트리의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="2b6f2b5ee645a40c14b49c77184129b10ec1567a" translate="yes" xml:space="preserve">
          <source>Some also work in the multilabel case:</source>
          <target state="translated">일부는 다중 레이블 사례에서도 작동합니다.</target>
        </trans-unit>
        <trans-unit id="8762622dcc16bf1560cb81f69bbd48256b77f9a4" translate="yes" xml:space="preserve">
          <source>Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;sklearn.set_config&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;config_context&lt;/code&gt;. The following suggests to limit temporary working memory to 128 MiB:</source>
          <target state="translated">표준 numpy 벡터화 된 연산을 사용하여 구현할 때 일부 계산에는 많은 양의 임시 메모리를 사용합니다. 시스템 메모리가 소진 될 수 있습니다. 고정 메모리 청크에서 계산을 수행 할 수있는 경우,이를 시도하고 &lt;a href=&quot;generated/sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;sklearn.set_config&lt;/code&gt; &lt;/a&gt; 또는 &lt;code&gt;config_context&lt;/code&gt; 를 사용 하여이 작업 메모리의 최대 크기 (기본값은 1GB)를 암시하도록 허용합니다 . 다음은 임시 작업 메모리를 128MiB로 제한하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f425af2b7289a6eb3b0699842a415a72e1141c04" translate="yes" xml:space="preserve">
          <source>Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.stratifiedshufflesplit#sklearn.model_selection.StratifiedShuffleSplit&quot;&gt;&lt;code&gt;StratifiedShuffleSplit&lt;/code&gt;&lt;/a&gt; to ensure that relative class frequencies is approximately preserved in each train and validation fold.</source>
          <target state="translated">일부 분류 문제는 대상 클래스의 분포에 큰 불균형을 나타낼 수 있습니다. 예를 들어 양성 샘플보다 몇 배 더 많은 음성 샘플이있을 수 있습니다. 이러한 경우 &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.stratifiedshufflesplit#sklearn.model_selection.StratifiedShuffleSplit&quot;&gt; &lt;code&gt;StratifiedShuffleSplit&lt;/code&gt; &lt;/a&gt; 에 구현 된 계층화 된 샘플링을 사용 하여 각 열차 및 검증 폴드에서 상대 클래스 주파수가 대략 보존되도록하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="2ea2f829ceb432ffa0e3b3d420b4273e01e30d41" translate="yes" xml:space="preserve">
          <source>Some cross validation iterators, such as &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;, have an inbuilt option to shuffle the data indices before splitting them. Note that:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 와 같은 일부 교차 유효성 검사 반복자 에는 데이터 인덱스를 셔플 링하기 전에 섞을 수있는 옵션이 내장되어 있습니다. 참고 :</target>
        </trans-unit>
        <trans-unit id="1b7524a4e391762865d52d4ee865a5a389b3ed4f" translate="yes" xml:space="preserve">
          <source>Some estimators expose a &lt;code&gt;transform&lt;/code&gt; method, for instance to reduce the dimensionality of the dataset.</source>
          <target state="translated">예를 들어 일부 추정기 는 데이터 집합의 차원을 줄이기 위해 &lt;code&gt;transform&lt;/code&gt; 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="abe93a33221be53771cefc563a6b553fa747a230" translate="yes" xml:space="preserve">
          <source>Some literature promotes alternative definitions of balanced accuracy. Our definition is equivalent to &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt; with class-balanced sample weights, and shares desirable properties with the binary case. See the &lt;a href=&quot;../model_evaluation#balanced-accuracy-score&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">일부 문헌은 균형 잡힌 정확도의 대체 정의를 장려합니다. 우리의 정의는 클래스-밸런싱 된 샘플 가중치를 갖는 &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; &lt;/a&gt; _ 점수와 동등 하며, 이진 경우와 바람직한 속성을 공유합니다. 사용 &lt;a href=&quot;../model_evaluation#balanced-accuracy-score&quot;&gt;설명서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="65e1dc1251c858f270c665ff61463afe65f478d7" translate="yes" xml:space="preserve">
          <source>Some metrics are essentially defined for binary classification tasks (e.g. &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled &lt;code&gt;1&lt;/code&gt; (though this may be configurable through the &lt;code&gt;pos_label&lt;/code&gt; parameter).</source>
          <target state="translated">일부 메트릭은 기본적으로 이진 분류 작업 (예 : &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt; )에 대해 정의됩니다 . 이 경우 기본적으로 양의 클래스가 &lt;code&gt;1&lt;/code&gt; 로 레이블링되어 있다고 가정하면 양의 레이블 만 평가됩니다 ( &lt;code&gt;pos_label&lt;/code&gt; 매개 변수를 통해 구성 할 수 있음 ).</target>
        </trans-unit>
        <trans-unit id="d5ba6b95ba600216ff9982f7a3dbaf94b6802f73" translate="yes" xml:space="preserve">
          <source>Some models allow for specialized, efficient parameter search strategies, &lt;a href=&quot;#alternative-cv&quot;&gt;outlined below&lt;/a&gt;. Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively considers all parameter combinations, while &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; can sample a given number of candidates from a parameter space with a specified distribution. After describing these tools we detail &lt;a href=&quot;#grid-search-tips&quot;&gt;best practice&lt;/a&gt; applicable to both approaches.</source>
          <target state="translated">일부 모델은 &lt;a href=&quot;#alternative-cv&quot;&gt;아래 설명 된&lt;/a&gt; 특수하고 효율적인 매개 변수 검색 전략을 허용 합니다 . 검색 후보 샘플링에 대한 두 가지 일반적인 접근 방식이 scikit-learn에서 제공됩니다. 주어진 값에 대해 &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 는 모든 매개 변수 조합을 철저히 고려하는 반면 &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt; 는 지정된 분포를 사용하여 매개 변수 공간에서 지정된 수의 후보를 샘플링 할 수 있습니다. 이러한 도구를 설명한 후 두 가지 방법 모두에 적용 할 수있는 &lt;a href=&quot;#grid-search-tips&quot;&gt;모범 사례를&lt;/a&gt; 자세히 설명합니다 .</target>
        </trans-unit>
        <trans-unit id="95196f8314df139319d7b42ff6d7520f5ecc9f1d" translate="yes" xml:space="preserve">
          <source>Some models also have &lt;code&gt;row_labels_&lt;/code&gt; and &lt;code&gt;column_labels_&lt;/code&gt; attributes. These models partition the rows and columns, such as in the block diagonal and checkerboard bicluster structures.</source>
          <target state="translated">일부 모델에는 &lt;code&gt;row_labels_&lt;/code&gt; 및 &lt;code&gt;column_labels_&lt;/code&gt; 속성도 있습니다. 이 모델은 블록 대각선 및 바둑판 bicluster 구조와 같이 행과 열을 분할합니다.</target>
        </trans-unit>
        <trans-unit id="89f290ef487f7e4f63f1b82009e209966dcbe74f" translate="yes" xml:space="preserve">
          <source>Some models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.</source>
          <target state="translated">일부 모델은 추정값을 단일 매개 변수 값에 적합시키는 것만 큼 효율적으로 일부 매개 변수 값 범위에 대한 데이터를 적합시킬 수 있습니다. 이 기능은이 매개 변수의 모델 선택에 사용되는보다 효율적인 교차 검증을 수행하는 데 활용 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d786744290bacd9a4dfc207be555be0e40c3853e" translate="yes" xml:space="preserve">
          <source>Some models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation).</source>
          <target state="translated">일부 모델은 단일 교차 경로를 계산하여 교차 검증을 사용하는 경우 대신 여러 정규화 경로를 계산하여 정규화 매개 변수의 최적 추정치에 대한 정보 이론적 폐쇄 형 공식을 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ad14a182695bef0a4c2fe22edd0961e8db73147c" translate="yes" xml:space="preserve">
          <source>Some of the clusters learned without connectivity constraints do not respect the structure of the swiss roll and extend across different folds of the manifolds. On the opposite, when opposing connectivity constraints, the clusters form a nice parcellation of the swiss roll.</source>
          <target state="translated">연결 제약 조건없이 학습 된 일부 클러스터는 스위스 롤의 구조를 고려하지 않고 매니 폴드의 다른 접힘에 걸쳐 확장됩니다. 반대로, 연결 제약 조건에 반대 할 때, 클러스터는 스위스 롤의 멋진 분할을 형성합니다.</target>
        </trans-unit>
        <trans-unit id="877cbb42097301f5a68339d0d9f55e4ca85ad3c3" translate="yes" xml:space="preserve">
          <source>Some of these are restricted to the binary classification case:</source>
          <target state="translated">이들 중 일부는 이진 분류 사례로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="bc828cde0b0d5dbd5e8ad77797297d4f6416ab76" translate="yes" xml:space="preserve">
          <source>Some other classifiers cope better with this harder version of the task. Try running &lt;a href=&quot;../auto_examples/model_selection/grid_search_text_feature_extraction#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py&quot;&gt;Sample pipeline for text feature extraction and evaluation&lt;/a&gt; with and without the &lt;code&gt;--filter&lt;/code&gt; option to compare the results.</source>
          <target state="translated">다른 분류기는이 어려운 버전의 작업에 더 잘 대처합니다. &lt;code&gt;--filter&lt;/code&gt; 옵션을 사용하거나 사용하지 않고 &lt;a href=&quot;../auto_examples/model_selection/grid_search_text_feature_extraction#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py&quot;&gt;텍스트 기능 추출 및 평가&lt;/a&gt; 를 위해 샘플 파이프 라인을 실행 하여 결과를 비교하십시오.</target>
        </trans-unit>
        <trans-unit id="1d754add1306692cb962c5467414be940979d0ab" translate="yes" xml:space="preserve">
          <source>Some parameter settings may result in a failure to &lt;code&gt;fit&lt;/code&gt; one or more folds of the data. By default, this will cause the entire search to fail, even if some parameter settings could be fully evaluated. Setting &lt;code&gt;error_score=0&lt;/code&gt; (or &lt;code&gt;=np.NaN&lt;/code&gt;) will make the procedure robust to such failure, issuing a warning and setting the score for that fold to 0 (or &lt;code&gt;NaN&lt;/code&gt;), but completing the search.</source>
          <target state="translated">일부 매개 변수 설정으로 인해 하나 이상의 데이터 접기에 &lt;code&gt;fit&lt;/code&gt; 않을 수 있습니다 . 기본적으로 일부 매개 변수 설정이 완전히 평가 되더라도 전체 검색이 실패합니다. 설정 &lt;code&gt;error_score=0&lt;/code&gt; (또는 &lt;code&gt;=np.NaN&lt;/code&gt; ) 경고를 발행 0 (또는 해당 배의 점수를 설정, 같은 실패 절차가 강력한 것 &lt;code&gt;NaN&lt;/code&gt; 의 ), 그러나 검색을 완료.</target>
        </trans-unit>
        <trans-unit id="2f1168d7fab23533f7c212613fc87fe5fb99b1b4" translate="yes" xml:space="preserve">
          <source>Some scikit-learn estimators and utilities can parallelize costly operations using multiple CPU cores, thanks to the following components:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f29c3cd2b5860fb787d236e9a466d0e36eb10659" translate="yes" xml:space="preserve">
          <source>Some tips and tricks:</source>
          <target state="translated">몇 가지 팁과 요령 :</target>
        </trans-unit>
        <trans-unit id="63fe20eae64c7864fd32162af52c1f81421bc7d2" translate="yes" xml:space="preserve">
          <source>Sometimes it may be useful to convert the data back into the original feature space. The &lt;code&gt;inverse_transform&lt;/code&gt; function converts the binned data into the original feature space. Each value will be equal to the mean of the two bin edges.</source>
          <target state="translated">때로는 데이터를 원래 피쳐 공간으로 다시 변환하는 것이 유용 할 수 있습니다. &lt;code&gt;inverse_transform&lt;/code&gt; 의 기능은 원래의 특징 공간으로 비닝 데이터를 변환한다. 각 값은 두 개의 빈 가장자리 평균과 같습니다.</target>
        </trans-unit>
        <trans-unit id="315bc72af841ed152a9aae1c3ac0990cdcb834ed" translate="yes" xml:space="preserve">
          <source>Sometimes looking at the learned coefficients of a neural network can provide insight into the learning behavior. For example if weights look unstructured, maybe some were not used at all, or if very large coefficients exist, maybe regularization was too low or the learning rate too high.</source>
          <target state="translated">때때로 신경망의 학습 계수를 보면 학습 행동에 대한 통찰력을 제공 할 수 있습니다. 예를 들어 가중치가 구조화되지 않은 것으로 보이거나 일부가 전혀 사용되지 않았거나 계수가 매우 큰 경우 정규화가 너무 낮거나 학습 속도가 너무 높을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fbff2a8532eac744fd3e459bcddf3fd34f40adee" translate="yes" xml:space="preserve">
          <source>Source URL: &lt;a href=&quot;http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&quot;&gt;http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&lt;/a&gt;</source>
          <target state="translated">소스 URL : &lt;a href=&quot;http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&quot;&gt;http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8173b56b5b02e6dc31d6c2059fb643537f89144d" translate="yes" xml:space="preserve">
          <source>Source URL: &lt;a href=&quot;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&quot;&gt;https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdef929636b0e2d76c9bcc79abe376f450451dd0" translate="yes" xml:space="preserve">
          <source>Sources, where n_samples is the number of samples and n_components is the number of components.</source>
          <target state="translated">소스, 여기서 n_samples는 샘플 수이고 n_components는 구성 요소 수입니다.</target>
        </trans-unit>
        <trans-unit id="3af0ce95ad1f86c8c1749c4ae38a0dba87aebcff" translate="yes" xml:space="preserve">
          <source>Sparse Principal Component Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcbe3516134bdf9c59a33ffb1c2e3120ebfb3eac" translate="yes" xml:space="preserve">
          <source>Sparse Principal Components Analysis (SparsePCA)</source>
          <target state="translated">희소 주성분 분석 (SparsePCA)</target>
        </trans-unit>
        <trans-unit id="e06472c25d26cda25191de9da3a114b1e469b208" translate="yes" xml:space="preserve">
          <source>Sparse coding</source>
          <target state="translated">희소 코딩</target>
        </trans-unit>
        <trans-unit id="32c8d921f9e1520199d1db9fe64aa6fb0f91121f" translate="yes" xml:space="preserve">
          <source>Sparse coding with a precomputed dictionary</source>
          <target state="translated">사전 계산 사전을 사용한 희소 코딩</target>
        </trans-unit>
        <trans-unit id="83cd17de4011d58af20829baa72e8c3c15415081" translate="yes" xml:space="preserve">
          <source>Sparse components extracted from the data.</source>
          <target state="translated">데이터에서 추출 된 스파 스 구성 요소.</target>
        </trans-unit>
        <trans-unit id="4aefb6aa7d814b8be3e6d2cb6f2931a1dadb31bc" translate="yes" xml:space="preserve">
          <source>Sparse input</source>
          <target state="translated">스파 스 입력</target>
        </trans-unit>
        <trans-unit id="935bf4a32a6f741a33bb9ac8e725575de63e795c" translate="yes" xml:space="preserve">
          <source>Sparse inverse covariance estimation</source>
          <target state="translated">희소 역공 분산 추정</target>
        </trans-unit>
        <trans-unit id="409d5a415f20eafd9b9f09c6ba22d21458394422" translate="yes" xml:space="preserve">
          <source>Sparse inverse covariance estimation with an l1-penalized estimator.</source>
          <target state="translated">l1 처벌 추정기로 스파 스 역공 분산 추정.</target>
        </trans-unit>
        <trans-unit id="2ee089f1e56c91604e7cab7d40e8b9f34677bb75" translate="yes" xml:space="preserve">
          <source>Sparse inverse covariance w/ cross-validated choice of the l1 penalty</source>
          <target state="translated">l1 페널티의 교차 검증 된 선택과 함께 스파 스 역공 분산</target>
        </trans-unit>
        <trans-unit id="0113e7df66bdaa38179e1b6944e6cd1aa2f5ffce" translate="yes" xml:space="preserve">
          <source>Sparse inverse covariance w/ cross-validated choice of the l1 penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92e371bb810dfdf353c195f32e5335e997df721c" translate="yes" xml:space="preserve">
          <source>Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples.</source>
          <target state="translated">주성분이 드물어 질수록 좀 더 이해하기 쉽고 해석하기 쉬운 표현이 만들어지며, 원래의 특징 중 어떤 것이 표본의 차이에 기여하는지 명확하게 강조합니다.</target>
        </trans-unit>
        <trans-unit id="19dbec98d9db7f8dccbc05e595e6dcc554502b17" translate="yes" xml:space="preserve">
          <source>Sparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.</source>
          <target state="translated">희소 랜덤 매트릭스는 밀도가 높은 가우시안 랜덤 프로젝션 매트릭스의 대안으로, 훨씬 더 효율적인 메모리를 제공하고 프로젝션 된 데이터의 빠른 계산을 허용하면서 유사한 임베딩 품질을 보장합니다.</target>
        </trans-unit>
        <trans-unit id="5ad88cbdaa9d7d5c176762a66b957d3aa9661770" translate="yes" xml:space="preserve">
          <source>Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.</source>
          <target state="translated">스파 스 랜덤 매트릭스는 밀도가 높은 랜덤 프로젝션 매트릭스의 대안으로, 훨씬 더 효율적인 메모리를 제공하고 프로젝션 된 데이터의 빠른 계산을 허용하면서 유사한 임베딩 품질을 보장합니다.</target>
        </trans-unit>
        <trans-unit id="a7a844fc75c56ce03e1afce70cb2355152140d0b" translate="yes" xml:space="preserve">
          <source>Sparsity</source>
          <target state="translated">Sparsity</target>
        </trans-unit>
        <trans-unit id="814e5a7e79ded720eafc96bc0232cca516d50079" translate="yes" xml:space="preserve">
          <source>Sparsity Example: Fitting only features 1 and 2</source>
          <target state="translated">희소성 예 : 피처 1과 2 만 피팅</target>
        </trans-unit>
        <trans-unit id="43cddceab3d136418b0e03e98d360e468cf1b8e5" translate="yes" xml:space="preserve">
          <source>Sparsity controlling parameter.</source>
          <target state="translated">희소성 제어 파라미터.</target>
        </trans-unit>
        <trans-unit id="647e2a8c2a361b51e5b69ed284ca76c83752d0f6" translate="yes" xml:space="preserve">
          <source>Sparsity controlling parameter. Higher values lead to sparser components.</source>
          <target state="translated">희소성 제어 파라미터. 값이 클수록 스파 스 구성 요소가 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="49f74e244eb107a1663f4aabb4f4ff9cbf7f050c" translate="yes" xml:space="preserve">
          <source>Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the &lt;code&gt;metric&lt;/code&gt; keyword.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a82be20d822213cd6b317bb5903a0613a76bafa" translate="yes" xml:space="preserve">
          <source>Species distribution modeling</source>
          <target state="translated">종 분포 모델링</target>
        </trans-unit>
        <trans-unit id="57d730dfe1585d0d57e966c58e133370714f1563" translate="yes" xml:space="preserve">
          <source>Specific parameters using e.g. &lt;code&gt;set_params(parameter_name=new_value)&lt;/code&gt;. In addition, to setting the parameters of the stacking estimator, the individual estimator of the stacking estimators can also be set, or can be removed by setting them to &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37195f9a92f1ed14e524a0ed92aab116b735c4ea" translate="yes" xml:space="preserve">
          <source>Specific parameters using e.g. set_params(parameter_name=new_value) In addition, to setting the parameters of the &lt;code&gt;VotingClassifier&lt;/code&gt;, the individual classifiers of the &lt;code&gt;VotingClassifier&lt;/code&gt; can also be set or replaced by setting them to None.</source>
          <target state="translated">예를 들어 set_params (parameter_name = new_value)를 사용하는 특정 매개 변수 또한 &lt;code&gt;VotingClassifier&lt;/code&gt; 의 매개 변수를 설정하는 것 외에도 VotingClassifier 의 개별 분류 기준 을 None으로 설정하여 설정하거나 &lt;code&gt;VotingClassifier&lt;/code&gt; 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="94e374689a4595b916bcf5e66713548e054c55c6" translate="yes" xml:space="preserve">
          <source>Specific weights can be assigned to each classifier via the &lt;code&gt;weights&lt;/code&gt; parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.</source>
          <target state="translated">&lt;code&gt;weights&lt;/code&gt; 매개 변수 를 통해 각 분류기에 특정 가중치를 지정할 수 있습니다 . 가중치가 제공되면 각 분류 자에 대해 예측 된 클래스 확률이 수집되고 분류 자 ​​가중치를 곱한 후 평균화됩니다. 그런 다음 최종 클래스 레이블은 평균 확률이 가장 높은 클래스 레이블에서 파생됩니다.</target>
        </trans-unit>
        <trans-unit id="68354cd532978d44f7b8a2998caa562af7db6244" translate="yes" xml:space="preserve">
          <source>Specifically, here the input variables are some gene sequences stored as variable-length strings consisting of letters &amp;lsquo;A&amp;rsquo;, &amp;lsquo;T&amp;rsquo;, &amp;lsquo;C&amp;rsquo;, and &amp;lsquo;G&amp;rsquo;, while the output variables are floating point numbers and True/False labels in the regression and classification tasks, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40c186465110b1329791125cf6cae8eb6f471442" translate="yes" xml:space="preserve">
          <source>Specifies a methodology to use to drop one of the categories per feature. This is useful in situations where perfectly collinear features cause problems, such as when feeding the resulting data into a neural network or an unregularized regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fbbf3fe05797ad1e4c717c36a5a204246056853" translate="yes" xml:space="preserve">
          <source>Specifies how multi-class classification problems are handled. Supported are &amp;ldquo;one_vs_rest&amp;rdquo; and &amp;ldquo;one_vs_one&amp;rdquo;. In &amp;ldquo;one_vs_rest&amp;rdquo;, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In &amp;ldquo;one_vs_one&amp;rdquo;, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that &amp;ldquo;one_vs_one&amp;rdquo; does not support predicting probability estimates.</source>
          <target state="translated">다중 클래스 분류 문제가 처리되는 방법을 지정합니다. &quot;one_vs_rest&quot;및 &quot;one_vs_one&quot;이 지원됩니다. &amp;ldquo;one_vs_rest&amp;rdquo;에서는 각 클래스마다 하나의 이진 가우시안 프로세스 분류 기가 장착되며이 클래스는 나머지 클래스와 분리되도록 훈련됩니다. &amp;ldquo;one_vs_one&amp;rdquo;에서 각 쌍의 클래스에 대해 하나의 이진 가우시안 프로세스 분류 기가 적합하며,이 두 클래스를 분리하도록 훈련됩니다. 이 이진 예측 변수의 예측은 다중 클래스 예측으로 결합됩니다. &amp;ldquo;one_vs_one&amp;rdquo;은 예측 확률 예측을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e7bd9f102fbfd80a2ad8d309cf0a9504bc2caee0" translate="yes" xml:space="preserve">
          <source>Specifies how multi-class classification problems are handled. Supported are &amp;lsquo;one_vs_rest&amp;rsquo; and &amp;lsquo;one_vs_one&amp;rsquo;. In &amp;lsquo;one_vs_rest&amp;rsquo;, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In &amp;lsquo;one_vs_one&amp;rsquo;, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that &amp;lsquo;one_vs_one&amp;rsquo; does not support predicting probability estimates.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9bddb5670bf596fd4f95945fb300823355f1c46f" translate="yes" xml:space="preserve">
          <source>Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.</source>
          <target state="translated">결정 함수에 상수 (일명 바이어스 또는 절편)를 추가해야하는지 여부를 지정합니다.</target>
        </trans-unit>
        <trans-unit id="9cc5cb93f212cfc9a20617982597c3fd3b14a01a" translate="yes" xml:space="preserve">
          <source>Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (X @ coef + intercept).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="032e8ae2185962af612ef54804c68499bb10a9e0" translate="yes" xml:space="preserve">
          <source>Specifies if the estimated precision is stored.</source>
          <target state="translated">추정 정밀도가 저장되는지 여부를 지정합니다.</target>
        </trans-unit>
        <trans-unit id="d38faf7dee61c2f434029c24d24417f5a7a63648" translate="yes" xml:space="preserve">
          <source>Specifies if the intercept should be fitted by the model. It must match the fit() method parameter.</source>
          <target state="translated">절편을 모형에 맞아야하는지 지정합니다. fit () 메소드 매개 변수와 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="21164bb750beb75acb9ae9d1f3fb116169b84f4e" translate="yes" xml:space="preserve">
          <source>Specifies the kernel type to be used in the algorithm. It must be one of &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo; or a callable. If none is given, &amp;lsquo;rbf&amp;rsquo; will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape &lt;code&gt;(n_samples, n_samples)&lt;/code&gt;.</source>
          <target state="translated">알고리즘에 사용될 커널 유형을 지정합니다. 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'또는 callable 중 하나 여야합니다. 아무 것도 지정하지 않으면 'rbf'가 사용됩니다. 콜 러블이 주어지면 데이터 행렬에서 커널 행렬을 사전 계산하는 데 사용됩니다. 해당 행렬은 모양의 배열이어야합니다 &lt;code&gt;(n_samples, n_samples)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7717364640e0a660ec81dfa33f675030f243fdf0" translate="yes" xml:space="preserve">
          <source>Specifies the kernel type to be used in the algorithm. It must be one of &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo; or a callable. If none is given, &amp;lsquo;rbf&amp;rsquo; will be used. If a callable is given it is used to precompute the kernel matrix.</source>
          <target state="translated">알고리즘에 사용될 커널 유형을 지정합니다. 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'또는 callable 중 하나 여야합니다. 아무 것도 지정하지 않으면 'rbf'가 사용됩니다. 콜 러블이 주어지면 커널 매트릭스를 사전 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b9435d1c3c2633287cc32557661450b6f00ca78e" translate="yes" xml:space="preserve">
          <source>Specifies the loss function. &amp;lsquo;hinge&amp;rsquo; is the standard SVM loss (used e.g. by the SVC class) while &amp;lsquo;squared_hinge&amp;rsquo; is the square of the hinge loss.</source>
          <target state="translated">손실 함수를 지정합니다. 'hinge'는 표준 SVM 손실 (예 : SVC 클래스에서 사용)이며 'squared_hinge'는 힌지 손실의 제곱입니다.</target>
        </trans-unit>
        <trans-unit id="68150facd38948e362a68f70eea508701955b6e7" translate="yes" xml:space="preserve">
          <source>Specifies the loss function. The epsilon-insensitive loss (standard SVR) is the L1 loss, while the squared epsilon-insensitive loss (&amp;lsquo;squared_epsilon_insensitive&amp;rsquo;) is the L2 loss.</source>
          <target state="translated">손실 함수를 지정합니다. 입실론에 둔감 한 손실 (표준 SVR)은 L1 손실이고, 입실론에 둔감 한 제곱 손실 ( 'squared_epsilon_insensitive')은 L2 손실입니다.</target>
        </trans-unit>
        <trans-unit id="1b5fabde85275fd0a5eb3f705ddd6c262b6e1ace" translate="yes" xml:space="preserve">
          <source>Specifies the loss function. With &amp;lsquo;squared_hinge&amp;rsquo; it is the squared hinge loss (a.k.a. L2 loss). With &amp;lsquo;log&amp;rsquo; it is the loss of logistic regression models.</source>
          <target state="translated">손실 함수를 지정합니다. 'squared_hinge'를 사용하면 제곱 된 힌지 손실 (일명 L2 손실)입니다. 'log'를 사용하면 로지스틱 회귀 모델이 손실됩니다.</target>
        </trans-unit>
        <trans-unit id="82fe667ff963f19359a5dba7024bcea48fa12322" translate="yes" xml:space="preserve">
          <source>Specifies the norm used in the penalization. The &amp;lsquo;l2&amp;rsquo; penalty is the standard used in SVC. The &amp;lsquo;l1&amp;rsquo; leads to &lt;code&gt;coef_&lt;/code&gt; vectors that are sparse.</source>
          <target state="translated">벌칙에 사용되는 표준을 지정합니다. 'l2'페널티는 SVC에서 사용되는 표준입니다. '1' 은 희소 한 &lt;code&gt;coef_&lt;/code&gt; 벡터로 이어진다 .</target>
        </trans-unit>
        <trans-unit id="5a337b6de37f25f0ac3016f29ff1f6486795a255" translate="yes" xml:space="preserve">
          <source>Specifies the returned model. Select &lt;code&gt;'lar'&lt;/code&gt; for Least Angle Regression, &lt;code&gt;'lasso'&lt;/code&gt; for the Lasso.</source>
          <target state="translated">반환 된 모델을 지정합니다. 선택 &lt;code&gt;'lar'&lt;/code&gt; 최소위한 각도 회귀, &lt;code&gt;'lasso'&lt;/code&gt; 올가미에 대해.</target>
        </trans-unit>
        <trans-unit id="ef557ab8128f60634f92721dd7b48ce0309e1238" translate="yes" xml:space="preserve">
          <source>Specifies whether to use &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; as the target response. For regressors this parameter is ignored and the response is always the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. By default, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; is tried first and we revert to &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; if it doesn&amp;rsquo;t exist. If &lt;code&gt;method&lt;/code&gt; is &amp;lsquo;recursion&amp;rsquo;, the response is always the output of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4eb03b762d72883fa3052fd37abd9dd63a6ceb00" translate="yes" xml:space="preserve">
          <source>Specifies whether to use &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; as the target response. If set to &amp;lsquo;auto&amp;rsquo;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt; is tried first and if it does not exist &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; is tried next.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ab0cdbec2fd77050a55d02bdf5982ebc80779f" translate="yes" xml:space="preserve">
          <source>Specify a download and cache folder for the datasets. If None, all scikit-learn data is stored in &amp;lsquo;~/scikit_learn_data&amp;rsquo; subfolders.</source>
          <target state="translated">데이터 세트의 다운로드 및 캐시 폴더를 지정하십시오. None 인 경우 모든 scikit-learn 데이터는 '~ / scikit_learn_data'하위 폴더에 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="5a7f883c69415d4b4614ca7ec1c25ea069f17592" translate="yes" xml:space="preserve">
          <source>Specify an download and cache folder for the datasets. If None, all scikit-learn data is stored in &amp;lsquo;~/scikit_learn_data&amp;rsquo; subfolders.</source>
          <target state="translated">데이터 세트의 다운로드 및 캐시 폴더를 지정하십시오. None 인 경우 모든 scikit-learn 데이터는 '~ / scikit_learn_data'하위 폴더에 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="bef377c44b3d810cadc5cf65c208d01924bfa02c" translate="yes" xml:space="preserve">
          <source>Specify another download and cache folder for the data sets. By default all scikit-learn data is stored in &amp;lsquo;~/scikit_learn_data&amp;rsquo; subfolders.</source>
          <target state="translated">데이터 세트에 다른 다운로드 및 캐시 폴더를 지정하십시오. 기본적으로 모든 scikit-learn 데이터는 '~ / scikit_learn_data'하위 폴더에 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="db707a2b2d7445205a990e044438fdad3fa08a72" translate="yes" xml:space="preserve">
          <source>Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in &amp;lsquo;~/scikit_learn_data&amp;rsquo; subfolders.</source>
          <target state="translated">데이터 세트에 다른 다운로드 및 캐시 폴더를 지정하십시오. 기본적으로 모든 scikit-learn 데이터는 '~ / scikit_learn_data'하위 폴더에 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="e0a4c5302feb0239f945ee7ba84757ae55a6d243" translate="yes" xml:space="preserve">
          <source>Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in &amp;lsquo;~/scikit_learn_data&amp;rsquo; subfolders. .. versionadded:: 0.19</source>
          <target state="translated">데이터 세트에 다른 다운로드 및 캐시 폴더를 지정하십시오. 기본적으로 모든 scikit-learn 데이터는 '~ / scikit_learn_data'하위 폴더에 저장됩니다. .. 버전 추가 :: 0.19</target>
        </trans-unit>
        <trans-unit id="3a104a4391c92d3464c7b1efaf07c449c778bcc9" translate="yes" xml:space="preserve">
          <source>Specify if the estimated precision is stored</source>
          <target state="translated">추정 정밀도가 저장되는지 지정</target>
        </trans-unit>
        <trans-unit id="68d4702fc734cffadd8a1ccf005f0aff7fa648f2" translate="yes" xml:space="preserve">
          <source>Specify if the estimated precision is stored.</source>
          <target state="translated">추정 정밀도가 저장되는지 여부를 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="4984f447cb8f4f521871d2431cae9f4220dfb519" translate="yes" xml:space="preserve">
          <source>Specify the column name in the data to use as target. If &amp;lsquo;default-target&amp;rsquo;, the standard target column a stored on the server is used. If &lt;code&gt;None&lt;/code&gt;, all columns are returned as data and the target is &lt;code&gt;None&lt;/code&gt;. If list (of strings), all columns with these names are returned as multi-target (Note: not all scikit-learn classifiers can handle all types of multi-output combinations)</source>
          <target state="translated">대상으로 사용할 데이터의 열 이름을 지정하십시오. 'default-target'인 경우 서버에 저장된 표준 대상 열 a가 사용됩니다. 경우 &lt;code&gt;None&lt;/code&gt; , 모든 열은 데이터로 반환 대상이 아니다 &lt;code&gt;None&lt;/code&gt; . (문자열) 목록 인 경우 이러한 이름을 가진 모든 열이 다중 대상으로 반환됩니다 (참고 : 모든 scikit-learn 분류 기가 모든 유형의 다중 출력 조합을 처리 할 수있는 것은 아닙니다)</target>
        </trans-unit>
        <trans-unit id="86eb3ad2a989c13695b9d85dcb05b7c45343a61d" translate="yes" xml:space="preserve">
          <source>Specify the desired relative and absolute tolerance of the result. If the true result is K_true, then the returned result K_ret satisfies &lt;code&gt;abs(K_true - K_ret) &amp;lt; atol + rtol * K_ret&lt;/code&gt; The default is zero (i.e. machine precision) for both.</source>
          <target state="translated">결과의 원하는 상대 및 절대 공차를 지정하십시오. 실제 결과가 K_true이면 반환 된 결과 K_ret은 &lt;code&gt;abs(K_true - K_ret) &amp;lt; atol + rtol * K_ret&lt;/code&gt; . 기본값은 모두 0입니다 (예 : 기계 정밀도).</target>
        </trans-unit>
        <trans-unit id="0742682745f85d107eacd49ea30a7e49015c565b" translate="yes" xml:space="preserve">
          <source>Specify the leaf size of the underlying tree. See &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for details. Default is 40.</source>
          <target state="translated">기본 트리의 리프 크기를 지정하십시오. 자세한 내용은 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 를 참조하십시오. 기본값은 40입니다.</target>
        </trans-unit>
        <trans-unit id="a3eb34c47a1823aab704036791431543ed289a4a" translate="yes" xml:space="preserve">
          <source>Specify the parallelization backend implementation. Supported backends are:</source>
          <target state="translated">병렬화 백엔드 구현을 지정하십시오. 지원되는 백엔드는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9ef7e5427d37487b864821803fe9613488fa8ce1" translate="yes" xml:space="preserve">
          <source>Specify the size of the kernel cache (in MB).</source>
          <target state="translated">커널 캐시의 크기를 MB 단위로 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="4329e4ac0b424da2818ac12cc4d13ce3581c4d3d" translate="yes" xml:space="preserve">
          <source>Specify what features are treated as categorical.</source>
          <target state="translated">범주로 취급되는 기능을 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="7d724db10f986282e8a5446f219cdacdbcddece6" translate="yes" xml:space="preserve">
          <source>Specify whether all or any of the given attributes must exist.</source>
          <target state="translated">주어진 속성 중 일부 또는 전부가 존재해야하는지 여부를 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="d079850de1341ae0792b165a2e4c64406a6bf6cd" translate="yes" xml:space="preserve">
          <source>Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the &lt;code&gt;n_iter&lt;/code&gt; parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:</source>
          <target state="translated">매개 변수의 샘플링 방법 지정은 &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 에&lt;/a&gt; 대한 매개 변수 지정과 매우 유사한 사전을 사용하여 수행됩니다 . 또한 샘플링 된 후보 또는 샘플링 반복의 수인 계산 예산은 &lt;code&gt;n_iter&lt;/code&gt; 매개 변수를 사용하여 지정 됩니다. 각 매개 변수에 대해 가능한 값에 대한 분포 또는 개별 선택 목록 (균일하게 샘플링 됨)을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="848f2bc6fd9feea5a4bd159161ff60b7b7f1ad05" translate="yes" xml:space="preserve">
          <source>Specifying the dataset by the name &amp;ldquo;iris&amp;rdquo; yields the lowest version, version 1, with the &lt;code&gt;data_id&lt;/code&gt; 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset &lt;code&gt;data_id&lt;/code&gt;. The other dataset, with &lt;code&gt;data_id&lt;/code&gt; 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:</source>
          <target state="translated">가장 낮은 버전, 버전 1이으로, 이름이 &quot;아이리스&quot;에 의해 데이터 집합을 산출 지정 &lt;code&gt;data_id&lt;/code&gt; 61 것은 당신이 항상 정확한 데이터 세트를 얻을 수 있도록하기 위해, 데이터 집합하여 지정하는 것이 가장 안전 &lt;code&gt;data_id&lt;/code&gt; . &lt;code&gt;data_id&lt;/code&gt; 가 969 인 다른 데이터 세트 는 버전 3 (버전 2는 비활성화 됨)이며 이진화 된 버전의 데이터를 포함합니다.</target>
        </trans-unit>
        <trans-unit id="a7781532fac864d69f63a26a8d414fddcb049d3b" translate="yes" xml:space="preserve">
          <source>Specifying the value of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; attribute will trigger the use of cross-validation with &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;, for example &lt;code&gt;cv=10&lt;/code&gt; for 10-fold cross-validation, rather than Generalized Cross-Validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3df7d54bd992cb63f5a75a6f7de49938e89cdde2" translate="yes" xml:space="preserve">
          <source>Spectral Clustering can also be used to cluster graphs by their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with &lt;code&gt;affinity=&amp;rsquo;precomputed&amp;rsquo;&lt;/code&gt;:</source>
          <target state="translated">스펙트럼 클러스터링을 사용하여 스펙트럼 임베딩으로 그래프를 클러스터링 할 수도 있습니다. 이 경우 선호도 행렬은 그래프의 인접 행렬이며 SpectralClustering은 &lt;code&gt;affinity=&amp;rsquo;precomputed&amp;rsquo;&lt;/code&gt; 초기화됩니다 .</target>
        </trans-unit>
        <trans-unit id="e698832cd5821ea0f1db5828f5b72aad63c46c00" translate="yes" xml:space="preserve">
          <source>Spectral Clustering can also be used to partition graphs via their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with &lt;code&gt;affinity='precomputed'&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="943c880ba7aef37194c447e5760436985518b340" translate="yes" xml:space="preserve">
          <source>Spectral Co-Clustering algorithm (Dhillon, 2001).</source>
          <target state="translated">스펙트럼 공동-클러스터링 알고리즘 (Dhillon, 2001).</target>
        </trans-unit>
        <trans-unit id="5c8a907562e6db42ff15e9df5a0d9b0b21be7b5f" translate="yes" xml:space="preserve">
          <source>Spectral Embedding (Laplacian Eigenmaps) is most useful when the graph has one connected component. If there graph has many components, the first few eigenvectors will simply uncover the connected components of the graph.</source>
          <target state="translated">스펙트럼 임베딩 (Laplacian Eigenmaps)은 그래프에 연결된 구성 요소가 하나있을 때 가장 유용합니다. 그래프에 많은 성분이있는 경우 처음 몇 개의 고유 벡터는 단순히 그래프의 연결된 성분을 찾아냅니다.</target>
        </trans-unit>
        <trans-unit id="7ff62a97384e4faf388cb99bbcc076cbdae4a5ec" translate="yes" xml:space="preserve">
          <source>Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function &lt;a href=&quot;generated/sklearn.manifold.spectral_embedding#sklearn.manifold.spectral_embedding&quot;&gt;&lt;code&gt;spectral_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.spectralembedding#sklearn.manifold.SpectralEmbedding&quot;&gt;&lt;code&gt;SpectralEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">스펙트럼 임베딩은 비선형 임베딩을 계산하는 방법입니다. Scikit-learn은 Laplacian Eigenmaps를 구현합니다. Laplacian Eigenmaps는 Laplacian 그래프의 스펙트럼 분해를 사용하여 데이터의 저 차원 표현을 찾습니다. 생성 된 그래프는 고차원 공간에서 저 차원 매니 폴드의 불연속 근사치로 간주 될 수 있습니다. 그래프를 기반으로 비용 함수를 최소화하면 매니 폴드에서 서로 가까이있는 점이 저 차원 공간에서 서로 가깝게 매핑되어 로컬 거리를 유지합니다. 스펙트럼은 매립 함수로 수행 될 수 &lt;a href=&quot;generated/sklearn.manifold.spectral_embedding#sklearn.manifold.spectral_embedding&quot;&gt; &lt;code&gt;spectral_embedding&lt;/code&gt; &lt;/a&gt; 또는 객체 지향 상대 &lt;a href=&quot;generated/sklearn.manifold.spectralembedding#sklearn.manifold.SpectralEmbedding&quot;&gt; &lt;code&gt;SpectralEmbedding&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="3266962963ccf3ff289e43e7853a5feea31fa6fe" translate="yes" xml:space="preserve">
          <source>Spectral biclustering (Kluger, 2003).</source>
          <target state="translated">스펙트럼 biclustering (Kluger, 2003).</target>
        </trans-unit>
        <trans-unit id="83334448105603952db5b041593dddc0f02ac19b" translate="yes" xml:space="preserve">
          <source>Spectral biclustering algorithms.</source>
          <target state="translated">스펙트럼 바이 블러스터 링 알고리즘.</target>
        </trans-unit>
        <trans-unit id="3fddf3521d69d12bc13710d54a4adc12aa85f512" translate="yes" xml:space="preserve">
          <source>Spectral clustering</source>
          <target state="translated">스펙트럼 클러스터링</target>
        </trans-unit>
        <trans-unit id="453e3a7c69660270eecfb13dabf16149c8b4512b" translate="yes" xml:space="preserve">
          <source>Spectral clustering for image segmentation</source>
          <target state="translated">이미지 세분화를위한 스펙트럼 클러스터링</target>
        </trans-unit>
        <trans-unit id="f9409615dd1103c73760717b8600df9e2157d615" translate="yes" xml:space="preserve">
          <source>Spectral embedding for non-linear dimensionality reduction.</source>
          <target state="translated">비선형 차원 축소를위한 스펙트럼 임베딩.</target>
        </trans-unit>
        <trans-unit id="8a0801a4fb2ecc40bcf6f04aa745ad2e1056e690" translate="yes" xml:space="preserve">
          <source>Spectral embedding of the training matrix.</source>
          <target state="translated">트레이닝 매트릭스의 스펙트럼 임베딩.</target>
        </trans-unit>
        <trans-unit id="2d2cb022bc3d26bd1407c4aa787d5e46e1ad4c3b" translate="yes" xml:space="preserve">
          <source>Speed</source>
          <target state="translated">Speed</target>
        </trans-unit>
        <trans-unit id="063a83567f47ad5f5679accf564d96c923566ee9" translate="yes" xml:space="preserve">
          <source>Speed:</source>
          <target state="translated">Speed:</target>
        </trans-unit>
        <trans-unit id="7d07f6cca3dbed6cdb804f0e2864e093c6647564" translate="yes" xml:space="preserve">
          <source>Split arrays or matrices into random train and test subsets</source>
          <target state="translated">배열 또는 행렬을 임의의 기차 및 테스트 하위 집합으로 분할</target>
        </trans-unit>
        <trans-unit id="5e854ececac820d9fb56cdde854f788365393cf5" translate="yes" xml:space="preserve">
          <source>Splits it into K folds, trains on K-1 and then tests on the left-out.</source>
          <target state="translated">그것을 K 폴드로 나누고 K-1을 훈련시킨 다음 왼쪽에서 테스트합니다.</target>
        </trans-unit>
        <trans-unit id="c2518ac986a45f6943dccb55ec28e7fc9787e8f9" translate="yes" xml:space="preserve">
          <source>Splitter Classes</source>
          <target state="translated">스플리터 클래스</target>
        </trans-unit>
        <trans-unit id="474933f1a999ce205b180d93539f6dbb5b05050e" translate="yes" xml:space="preserve">
          <source>Splitter Functions</source>
          <target state="translated">스플리터 기능</target>
        </trans-unit>
        <trans-unit id="01474e72e0404f40fd189e5ac7233925222e580d" translate="yes" xml:space="preserve">
          <source>Squared L2 norms of the lines of y. Required if tol is not None.</source>
          <target state="translated">y 선의 제곱 된 L2 규범. tol이 None이 아닌 경우 필수입니다.</target>
        </trans-unit>
        <trans-unit id="89cdcd77a950e009dab4164bc976d2f6ebb6b9e7" translate="yes" xml:space="preserve">
          <source>Squared Mahalanobis distances of the observations.</source>
          <target state="translated">관측치의 제곱 된 Mahalanobis 거리.</target>
        </trans-unit>
        <trans-unit id="a0b13f625123904866bd60e38bc7611ba95c992c" translate="yes" xml:space="preserve">
          <source>Squared Sum - Sum of the squared L2 norm of all samples.</source>
          <target state="translated">제곱합-모든 샘플의 제곱 된 L2 규범의 합입니다.</target>
        </trans-unit>
        <trans-unit id="1c1f19010d2ef30728a1e3cec08abc7bd4b0d974" translate="yes" xml:space="preserve">
          <source>Squared norm of the centroids.</source>
          <target state="translated">중심의 제곱 규범.</target>
        </trans-unit>
        <trans-unit id="ff4530f7332d92145f70c600e76bef65d08e2445" translate="yes" xml:space="preserve">
          <source>Stability path based on randomized Lasso estimates</source>
          <target state="translated">무작위 올가미 추정치에 기반한 안정성 경로</target>
        </trans-unit>
        <trans-unit id="9a5fecba5d8d30ecb602724233c6166d767b3036" translate="yes" xml:space="preserve">
          <source>Stability selection Nicolai Meinshausen, Peter Buhlmann Journal of the Royal Statistical Society: Series B Volume 72, Issue 4, pages 417-473, September 2010 DOI: 10.1111/j.1467-9868.2010.00740.x</source>
          <target state="translated">안정성 선택 Nicolai Meinshausen, Peter Buhlmann The Royal Statistical Society : Series B Volume 72, Issue 4, page 417-473, 2010 년 9 월 DOI : 10.1111 / j.1467-9868.2010.00740.x</target>
        </trans-unit>
        <trans-unit id="24ab98f7d3b4687c560036182f11b4e7733b1d68" translate="yes" xml:space="preserve">
          <source>Stack Exchange</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71b2c903e12bff4f98c474e759faf1146ab6ad92" translate="yes" xml:space="preserve">
          <source>Stack of estimators with a final classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8433d8b247979d86acd74f4143c89bb21831f7b9" translate="yes" xml:space="preserve">
          <source>Stack of estimators with a final regressor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a568e00cb92ef139342d099c3bf8234421058e98" translate="yes" xml:space="preserve">
          <source>Stack of predictors on a single data set</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8781e517c304621291a3255c93ced28430f5c0bd" translate="yes" xml:space="preserve">
          <source>Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f5f44a8c7d7cd4102991a0e07afa77f83e823da" translate="yes" xml:space="preserve">
          <source>Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d83774fb1981a771cc887192522bcd1ee8713fc" translate="yes" xml:space="preserve">
          <source>Stacked generalization is a method for combining estimators to reduce their biases &lt;a href=&quot;#w1992&quot; id=&quot;id32&quot;&gt;[W1992]&lt;/a&gt;&lt;a href=&quot;#htf&quot; id=&quot;id33&quot;&gt;[HTF]&lt;/a&gt;. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93dcc7ee10b4f7f50030c1b93ea7e60ca7979cd4" translate="yes" xml:space="preserve">
          <source>Stacking Classifier and Regressor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a37a65ea2247a4c331699e362c23755d1370e615" translate="yes" xml:space="preserve">
          <source>Stacking refers to a method to blend estimators. In this strategy, some estimators are individually fitted on some training data while a final estimator is trained using the stacked predictions of these base estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="891f1b1c9f204fa14cf72f5b45193c02a0d262be" translate="yes" xml:space="preserve">
          <source>Standard deviation of Gaussian noise added to the data.</source>
          <target state="translated">데이터에 가우스 잡음의 표준 편차가 추가되었습니다.</target>
        </trans-unit>
        <trans-unit id="a17025349c0cc77d7292705f3e0aace21538f53e" translate="yes" xml:space="preserve">
          <source>Standard deviation of predictive distribution at query points. Only returned when &lt;code&gt;return_std&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f806d5207c92015615b11e0738f918dc0548c864" translate="yes" xml:space="preserve">
          <source>Standard deviation of predictive distribution at query points. Only returned when return_std is True.</source>
          <target state="translated">쿼리 지점에서 예측 분포의 표준 편차입니다. return_std가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6edd185d8d7cdfc859bd82ca69e1fcc7af90edcd" translate="yes" xml:space="preserve">
          <source>Standard deviation of predictive distribution of query points.</source>
          <target state="translated">쿼리 지점의 예측 분포에 대한 표준 편차입니다.</target>
        </trans-unit>
        <trans-unit id="c37a2551fc59b4216e7ebb6941b4c543d13c64ce" translate="yes" xml:space="preserve">
          <source>Standard deviation over &lt;code&gt;n_repeats&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea11cb9dbd7c4fc006fb08938b76124b12688d69" translate="yes" xml:space="preserve">
          <source>StandardScaler</source>
          <target state="translated">StandardScaler</target>
        </trans-unit>
        <trans-unit id="9f96721b99a0217af973cbedbcbf7d1fa7440aeb" translate="yes" xml:space="preserve">
          <source>Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.</source>
          <target state="translated">데이터 세트의 표준화는 많은 머신 러닝 추정기의 공통 요구 사항입니다. 일반적으로 이것은 평균을 제거하고 단위 분산으로 스케일링하여 수행됩니다. 그러나 특이 치가 종종 표본 평균 / 분산에 부정적인 영향을 줄 수 있습니다. 이러한 경우, 중앙값과 사 분위수 범위는 종종 더 나은 결과를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="3845481860a037ebc5c39c64e8de0e95fe45e3fb" translate="yes" xml:space="preserve">
          <source>Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).</source>
          <target state="translated">데이터 셋의 표준화는 많은 머신 러닝 추정기의 공통 요구 사항입니다. 개별 기능이 표준 정규 분포 데이터 (예 : 평균이 0이고 단위 분산이있는 가우시안)와 다소 비슷하지 않은 경우 제대로 작동하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="781aef30981524e4bc3b3ab4682d7e1b6f686dcb" translate="yes" xml:space="preserve">
          <source>Standardize a dataset along any axis</source>
          <target state="translated">모든 축을 따라 데이터 세트 표준화</target>
        </trans-unit>
        <trans-unit id="8089cb9b9abb90199844c7f8d2ad6ef5ad6b9827" translate="yes" xml:space="preserve">
          <source>Standardize features by removing the mean and scaling to unit variance</source>
          <target state="translated">평균을 제거하고 단위 분산으로 스케일링하여 기능 표준화</target>
        </trans-unit>
        <trans-unit id="070fc0ca4dc6d3cb17aee36f0432a76e85e66a77" translate="yes" xml:space="preserve">
          <source>Start pointer to all the leaves.</source>
          <target state="translated">모든 나뭇잎에 대한 포인터를 시작하십시오.</target>
        </trans-unit>
        <trans-unit id="08a6668f9a564bddd6d8fa9fd4934eeea4b017c7" translate="yes" xml:space="preserve">
          <source>Starting configuration of the embedding to initialize the SMACOF algorithm. By default, the algorithm is initialized with a randomly chosen array.</source>
          <target state="translated">SMACOF 알고리즘을 초기화하기위한 임베딩 구성 시작 기본적으로 알고리즘은 임의로 선택된 배열로 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="7252947fdd6406b9475a3bf1b686e53848838289" translate="yes" xml:space="preserve">
          <source>Starting configuration of the embedding to initialize the algorithm. By default, the algorithm is initialized with a randomly chosen array.</source>
          <target state="translated">알고리즘을 초기화하기 위해 임베딩 구성을 시작합니다. 기본적으로 알고리즘은 임의로 선택된 배열로 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="08977c4568e04a737e6b3a87f7b6021573de1b1c" translate="yes" xml:space="preserve">
          <source>Starting from &lt;code&gt;joblib &amp;gt;= 0.14&lt;/code&gt;, when the &lt;code&gt;loky&lt;/code&gt; backend is used (which is the default), joblib will tell its child &lt;strong&gt;processes&lt;/strong&gt; to limit the number of threads they can use, so as to avoid oversubscription. In practice the heuristic that joblib uses is to tell the processes to use &lt;code&gt;max_threads
= n_cpus // n_jobs&lt;/code&gt;, via their corresponding environment variable. Back to our example from above, since the joblib backend of &lt;code&gt;GridSearchCV&lt;/code&gt; is &lt;code&gt;loky&lt;/code&gt;, each process will only be able to use 1 thread instead of 8, thus mitigating the oversubscription issue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91cd41feb47c4fc7679dfd69c834b11820027a8b" translate="yes" xml:space="preserve">
          <source>Starting from initial random weights, multi-layer perceptron (MLP) minimizes the loss function by repeatedly updating these weights. After computing the loss, a backward pass propagates it from the output layer to the previous layers, providing each weight parameter with an update value meant to decrease the loss.</source>
          <target state="translated">초기 임의 가중치부터 MLP (Multi-Layer Perceptron)는 이러한 가중치를 반복적으로 업데이트하여 손실 기능을 최소화합니다. 손실을 계산 한 후, 역방향 통과는 출력 계층에서 이전 계층으로 전달하여 각 가중치 매개 변수에 손실을 줄이기위한 업데이트 값을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="fcf350fa97b4ef940922ec2e36ae5accc928bb98" translate="yes" xml:space="preserve">
          <source>Starting node for path</source>
          <target state="translated">경로의 시작 노드</target>
        </trans-unit>
        <trans-unit id="bed5865b6136905da0496b8ae96a4873f78bef72" translate="yes" xml:space="preserve">
          <source>Stat Ass, 79:871, 1984.</source>
          <target state="translated">Stat Ass, 79 : 871, 1984.</target>
        </trans-unit>
        <trans-unit id="6493ce2cca639b99501821839727266114fab06b" translate="yes" xml:space="preserve">
          <source>Statistical learning</source>
          <target state="translated">통계 학습</target>
        </trans-unit>
        <trans-unit id="ff430697ec62291221833385a34a445a9ee9ecdf" translate="yes" xml:space="preserve">
          <source>Statistical learning: the setting and the estimator object in scikit-learn</source>
          <target state="translated">통계 학습 : scikit-learn의 설정 및 추정기 개체</target>
        </trans-unit>
        <trans-unit id="904a41f7fbe4f76d9e16b8a6416dab74831246a2" translate="yes" xml:space="preserve">
          <source>Stef van Buuren, Karin Groothuis-Oudshoorn (2011). &amp;ldquo;mice: Multivariate Imputation by Chained Equations in R&amp;rdquo;. Journal of Statistical Software 45: 1-67.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a673f9d4e8314126b08e8f81bb3a33f3d78b1e09" translate="yes" xml:space="preserve">
          <source>Still effective in cases where number of dimensions is greater than the number of samples.</source>
          <target state="translated">치수 수가 샘플 수보다 큰 경우에도 여전히 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="2473d40abe8e9fb2e7f524b8bad4d74240273fa9" translate="yes" xml:space="preserve">
          <source>Stochastic Gradient Descent</source>
          <target state="translated">확률 적 경사 하강</target>
        </trans-unit>
        <trans-unit id="195b32448a080f6c15b39de98057b7fe1bc4693b" translate="yes" xml:space="preserve">
          <source>Stochastic Gradient Descent is an optimization technique which minimizes a loss function in a stochastic fashion, performing a gradient descent step sample by sample. In particular, it is a very efficient method to fit linear models.</source>
          <target state="translated">확률 적 그라디언트 디센트는 확률 적 방식으로 손실 함수를 최소화하여 샘플별로 그라디언트 디센트 단계를 수행하는 최적화 기술입니다. 특히 선형 모델에 맞는 매우 효율적인 방법입니다.</target>
        </trans-unit>
        <trans-unit id="e3aa3ce34d4d1d755f86485089b5a4b4f435a8e2" translate="yes" xml:space="preserve">
          <source>Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the &lt;em&gt;same&lt;/em&gt; scaling must be applied to the test vector to obtain meaningful results. This can be easily done using &lt;code&gt;StandardScaler&lt;/code&gt;:</source>
          <target state="translated">확률 적 그라데이션 하강은 기능 스케일링에 민감하므로 데이터를 스케일링하는 것이 좋습니다. 예를 들어 입력 벡터 X의 각 속성을 [0,1] 또는 [-1, + 1]로 스케일링 하거나 평균 0과 분산 1을 갖도록 표준화하십시오 . 테스트 벡터에도 &lt;em&gt;동일한&lt;/em&gt; 스케일링을 적용해야합니다. 의미있는 결과를 얻습니다. 이는 &lt;code&gt;StandardScaler&lt;/code&gt; 를 사용하여 쉽게 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ee01e77469d8f50feb7b1b4735d433d85aa0d812" translate="yes" xml:space="preserve">
          <source>Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute &lt;code&gt;oob_improvement_&lt;/code&gt;. &lt;code&gt;oob_improvement_[i]&lt;/code&gt; holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming.</source>
          <target state="translated">확률 적 그래디언트 부스팅을 사용하면 부트 스트랩 샘플에 포함되지 않은 예 (예 : 가방 외부 예)에 대한 편차 개선을 계산하여 테스트 편차의 가방 외부 추정값을 계산할 수 있습니다. 개선 사항은 &lt;code&gt;oob_improvement_&lt;/code&gt; 속성에 저장됩니다 . &lt;code&gt;oob_improvement_[i]&lt;/code&gt; 는 i 번째 단계를 현재 예측에 추가하면 OOB 샘플의 손실 측면에서 개선점을 유지합니다. 최적의 반복 횟수를 결정하기 위해 백 아웃 추정을 모델 선택에 사용할 수 있습니다. OOB 추정치는 일반적으로 매우 비관적이므로 교차 유효성 검사를 대신 사용하고 교차 유효성 검사에 너무 많은 시간이 소요되는 경우에만 OOB를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="5823a3f0a9a6ed167c583e77307156305a3e83ac" translate="yes" xml:space="preserve">
          <source>Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The &lt;code&gt;partial_fit&lt;/code&gt; method allows online/out-of-core learning.</source>
          <target state="translated">확률 적 그라디언트 디센트는 선형 모델에 맞는 간단하면서도 매우 효율적인 방법입니다. 샘플 수 (및 피처 수)가 매우 큰 경우에 특히 유용합니다. &lt;code&gt;partial_fit&lt;/code&gt; 의 방법은 온라인으로 할 수 있습니다 / 아웃 - 오브 - 핵심 학습.</target>
        </trans-unit>
        <trans-unit id="88c2b7432b4c70aedd301d6441f9f686fac99afd" translate="yes" xml:space="preserve">
          <source>Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of \(E(w,b)\) by considering a single training example at a time.</source>
          <target state="translated">확률 적 경사 하강은 제약이없는 최적화 문제에 대한 최적화 방법입니다. (배치) 그래디언트 디센트와 달리 SGD는 한 번에 단일 교육 예제를 고려하여 \ (E (w, b) \)의 실제 그래디언트를 근사합니다.</target>
        </trans-unit>
        <trans-unit id="3bc9b5e942f6c3298a3799e63fea9d4e51700363" translate="yes" xml:space="preserve">
          <source>Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree.</source>
          <target state="translated">n_clusters에서 트리 생성을 일찍 중지하십시오. 기능 수에 비해 군집 수가 적지 않은 경우 계산 시간을 줄이는 데 유용합니다. 이 옵션은 연결 매트릭스를 지정할 때만 유용합니다. 또한 클러스터 수를 변경하고 캐싱을 사용하는 경우 전체 트리를 계산하는 것이 유리할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="27fa813a99ee0ee872f0a0fdd316cf176619503c" translate="yes" xml:space="preserve">
          <source>Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. It must be &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;. By default &lt;code&gt;compute_full_tree&lt;/code&gt; is &amp;ldquo;auto&amp;rdquo;, which is equivalent to &lt;code&gt;True&lt;/code&gt; when &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt; or that &lt;code&gt;n_clusters&lt;/code&gt; is inferior to the maximum between 100 or &lt;code&gt;0.02 * n_samples&lt;/code&gt;. Otherwise, &amp;ldquo;auto&amp;rdquo; is equivalent to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dc26590d142d1e5e73aaec1d67524322b86dfd8" translate="yes" xml:space="preserve">
          <source>Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the &amp;lsquo;children&amp;rsquo; output is of limited use, and the &amp;lsquo;parents&amp;rsquo; output should rather be used. This option is valid only when specifying a connectivity matrix.</source>
          <target state="translated">n_clusters에서 트리 생성을 일찍 중지하십시오. 이는 클러스터 수가 샘플 수에 비해 작지 않은 경우 계산 시간을 줄이는 데 유용합니다. 이 경우 전체 트리가 계산되지 않으므로 '자식'출력의 사용이 제한적이며 '부모'출력이 사용되어야합니다. 이 옵션은 연결 매트릭스를 지정할 때만 유효합니다.</target>
        </trans-unit>
        <trans-unit id="44e9e4435e20e453549059a3ee4002b032ad438a" translate="yes" xml:space="preserve">
          <source>Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree.</source>
          <target state="translated">n_clusters에서 트리 생성을 일찍 중지하십시오. 이는 클러스터 수가 샘플 수에 비해 작지 않은 경우 계산 시간을 줄이는 데 유용합니다. 이 옵션은 연결 매트릭스를 지정할 때만 유용합니다. 또한 클러스터 수를 변경하고 캐싱을 사용하는 경우 전체 트리를 계산하는 것이 유리할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cec77383bf3b11f332bd0e653f87f2dc409b1dee" translate="yes" xml:space="preserve">
          <source>Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. It must be &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;. By default &lt;code&gt;compute_full_tree&lt;/code&gt; is &amp;ldquo;auto&amp;rdquo;, which is equivalent to &lt;code&gt;True&lt;/code&gt; when &lt;code&gt;distance_threshold&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt; or that &lt;code&gt;n_clusters&lt;/code&gt; is inferior to the maximum between 100 or &lt;code&gt;0.02 * n_samples&lt;/code&gt;. Otherwise, &amp;ldquo;auto&amp;rdquo; is equivalent to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84a6e50b1119dc5648f3c425a6b5dee68899e309" translate="yes" xml:space="preserve">
          <source>Stop iteration if at least this number of inliers are found.</source>
          <target state="translated">이 개수 이상의 이너가 발견되면 반복을 중지하십시오.</target>
        </trans-unit>
        <trans-unit id="538193aed5fb2f898d909880cd3e81469e15df67" translate="yes" xml:space="preserve">
          <source>Stop iteration if score is greater equal than this threshold.</source>
          <target state="translated">점수가이 임계 값보다 큰 경우 반복을 중지하십시오.</target>
        </trans-unit>
        <trans-unit id="12517d0c8ade549d252ae4e535d57433e9861479" translate="yes" xml:space="preserve">
          <source>Stop solver after this many iterations regardless of accuracy (XXX Currently there is no API to know whether this kicked in.) -1 by default.</source>
          <target state="translated">정확도에 관계없이이 반복을 많이 수행 한 후에 솔버를 중지하십시오 (XXX 현재이 기능이 시작되었는지 여부를 알 수있는 API는 없습니다) -1 기본적으로.</target>
        </trans-unit>
        <trans-unit id="356700453d0a0d54f3f7d4b7b513913c4b6ecef8" translate="yes" xml:space="preserve">
          <source>Stop the algorithm if w has converged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df6a1587df9722b5b493e30cfc313089ab29220d" translate="yes" xml:space="preserve">
          <source>Stop the algorithm if w has converged. Default is 1.e-3.</source>
          <target state="translated">w가 수렴되면 알고리즘을 중지하십시오. 기본값은 1.e-3입니다.</target>
        </trans-unit>
        <trans-unit id="7cf46a1e9b2d853c73253eb4c96cbe1ea1bb5aa6" translate="yes" xml:space="preserve">
          <source>Stop words are words like &amp;ldquo;and&amp;rdquo;, &amp;ldquo;the&amp;rdquo;, &amp;ldquo;him&amp;rdquo;, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.</source>
          <target state="translated">중지 단어는 &quot;및&quot;, &quot;the&quot;, &quot;him&quot;과 같은 단어로, 텍스트 내용을 나타내는 데 도움이되지 않는 것으로 추정되며 예측을위한 신호로 해석되지 않도록 제거 될 수 있습니다. 그러나 때로는 글쓰기 스타일 또는 성격 분류와 같은 유사한 단어가 예측에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="7eb24af52d1aa9e6b8d6715fd2fd646422f9b535" translate="yes" xml:space="preserve">
          <source>Stopping criteria.</source>
          <target state="translated">기준을 중지합니다.</target>
        </trans-unit>
        <trans-unit id="2eda661dab2cf19600424ed9df7c9d0563861dff" translate="yes" xml:space="preserve">
          <source>Stopping criterion for eigendecomposition of the Laplacian matrix when &lt;code&gt;eigen_solver='arpack'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51b3a0bb0b4d84ebe8a65cc9b8c0c2b3279fba93" translate="yes" xml:space="preserve">
          <source>Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver.</source>
          <target state="translated">arpack eigen_solver를 사용할 때 라플라시안 행렬의 고유 분해 기준을 중지합니다.</target>
        </trans-unit>
        <trans-unit id="a798cb65fb942fe2386149a92d6481d384d3e392" translate="yes" xml:space="preserve">
          <source>Stopping criterion. For the lbfgs solver, the iteration will stop when &lt;code&gt;max{|g_j|, j = 1, ..., d} &amp;lt;= tol&lt;/code&gt; where &lt;code&gt;g_j&lt;/code&gt; is the j-th component of the gradient (derivative) of the objective function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68414daf9a6e27622678aff82460baea3a51326c" translate="yes" xml:space="preserve">
          <source>Stopping criterion. For the newton-cg and lbfgs solvers, the iteration will stop when &lt;code&gt;max{|g_i | i = 1, ..., n} &amp;lt;= tol&lt;/code&gt; where &lt;code&gt;g_i&lt;/code&gt; is the i-th component of the gradient.</source>
          <target state="translated">기준을 중지합니다. newton-cg 및 lbfgs 솔버의 경우 &lt;code&gt;max{|g_i | i = 1, ..., n} &amp;lt;= tol&lt;/code&gt; 여기서 &lt;code&gt;g_i&lt;/code&gt; 는 그래디언트의 i 번째 성분입니다.</target>
        </trans-unit>
        <trans-unit id="fe10af6492740b92517388e940d4c53ee7e65f2c" translate="yes" xml:space="preserve">
          <source>Stopping tolerance for EM algorithm.</source>
          <target state="translated">EM 알고리즘에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="54709da56f5bb7c428a618dd7d85fb0e4421bcb5" translate="yes" xml:space="preserve">
          <source>Stopping tolerance for log-likelihood increase.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3afb8412732c53a6b3ed5f0cf3d5d66e9526d993" translate="yes" xml:space="preserve">
          <source>Stopping tolerance for updating document topic distribution in E-step.</source>
          <target state="translated">E-step에서 문서 주제 분배를 업데이트하기위한 허용 오차를 중지합니다.</target>
        </trans-unit>
        <trans-unit id="5745be1c1a529a40ad5578990283b7d9ed5dfe16" translate="yes" xml:space="preserve">
          <source>Store n output values in leaves, instead of 1;</source>
          <target state="translated">n 대신에 n 개의 출력 값을 잎에 저장하십시오.</target>
        </trans-unit>
        <trans-unit id="0fac441919e9594e005f5d0571a0bfbc255133fa" translate="yes" xml:space="preserve">
          <source>Stored sampling interval. Specified as a parameter if sample_steps not in {1,2,3}.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a6ab3415c7252ce3e941fe62193814f0365a625" translate="yes" xml:space="preserve">
          <source>Stores nearest neighbors instance, including BallTree or KDtree if applicable.</source>
          <target state="translated">해당되는 경우 BallTree 또는 KDtree를 포함하여 가장 가까운 인접 인스턴스를 저장합니다.</target>
        </trans-unit>
        <trans-unit id="f9d028499b97399f71598e9bb920fa52d5ec8313" translate="yes" xml:space="preserve">
          <source>Stores the affinity matrix used in &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 에 사용 된 선호도 매트릭스를 저장합니다 .</target>
        </trans-unit>
        <trans-unit id="781b70a9f7d2aabdccb43059620f25863827cacd" translate="yes" xml:space="preserve">
          <source>Stores the embedding vectors</source>
          <target state="translated">임베딩 벡터를 저장합니다</target>
        </trans-unit>
        <trans-unit id="9fe548f57e57cace725aa47be0bac94930f35de7" translate="yes" xml:space="preserve">
          <source>Stores the embedding vectors.</source>
          <target state="translated">임베딩 벡터를 저장합니다.</target>
        </trans-unit>
        <trans-unit id="e7019b0e2126237169f8ccc84f1dacd8599b7b63" translate="yes" xml:space="preserve">
          <source>Stores the geodesic distance matrix of training data.</source>
          <target state="translated">훈련 데이터의 측지 거리 행렬을 저장합니다.</target>
        </trans-unit>
        <trans-unit id="7bfa0d6921b4ff07d4718354c3a4168af9b3a946" translate="yes" xml:space="preserve">
          <source>Stores the position of the dataset in the embedding space.</source>
          <target state="translated">임베드 공간에서 데이터 세트의 위치를 ​​저장합니다.</target>
        </trans-unit>
        <trans-unit id="8197f80c6163117652499db82ad63b22aa5b87b2" translate="yes" xml:space="preserve">
          <source>Stores the training data.</source>
          <target state="translated">훈련 데이터를 저장합니다.</target>
        </trans-unit>
        <trans-unit id="6485fe8179de6b50a8b0db7cf302477ffee4cf50" translate="yes" xml:space="preserve">
          <source>Strategy to use to generate predictions.</source>
          <target state="translated">예측을 생성하는 데 사용할 전략.</target>
        </trans-unit>
        <trans-unit id="9ba291b4721c49cd83c83d224ae746db45b32e1d" translate="yes" xml:space="preserve">
          <source>Strategy used to define the widths of the bins.</source>
          <target state="translated">빈의 너비를 정의하는 데 사용되는 전략.</target>
        </trans-unit>
        <trans-unit id="890ad0feded21dbb4c68bfca3e2d7cdbb498d411" translate="yes" xml:space="preserve">
          <source>Stratified K-Folds cross-validator</source>
          <target state="translated">계층화 된 K- 폴드 교차 검증기</target>
        </trans-unit>
        <trans-unit id="078f2e04c72cf2c2cef672d9cd530d809895796a" translate="yes" xml:space="preserve">
          <source>Stratified ShuffleSplit cross-validator</source>
          <target state="translated">계층화 된 셔플</target>
        </trans-unit>
        <trans-unit id="10ef227c2ccc54bd522a7229f1c708e11ce5e295" translate="yes" xml:space="preserve">
          <source>Strehl, Alexander, and Joydeep Ghosh (2002). &amp;ldquo;Cluster ensembles &amp;ndash; a knowledge reuse framework for combining multiple partitions&amp;rdquo;. Journal of Machine Learning Research 3: 583&amp;ndash;617. &lt;a href=&quot;http://strehl.com/download/strehl-jmlr02.pdf&quot;&gt;doi:10.1162/153244303321897735&lt;/a&gt;.</source>
          <target state="translated">Strehl, Alexander 및 Joydeep Ghosh (2002). &amp;ldquo;클러스터 앙상블 &amp;ndash; 여러 파티션을 결합하기위한 지식 재사용 프레임 워크&amp;rdquo;. 기계 학습 연구 저널 3 : 583&amp;ndash;617. &lt;a href=&quot;http://strehl.com/download/strehl-jmlr02.pdf&quot;&gt;doi : 10.1162 / 153244303321897735&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="738d614dd3bdb5fdb2eecf8a98b676a349ac24fe" translate="yes" xml:space="preserve">
          <source>Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a &lt;em&gt;way&lt;/em&gt; to train a model. Often, an instance of &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using &lt;code&gt;SGDClassifier(loss='log')&lt;/code&gt; results in logistic regression, i.e. a model equivalent to &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; which is fitted via SGD instead of being fitted by one of the other solvers in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. Similarly, &lt;code&gt;SGDRegressor(loss='squared_loss', penalty='l2')&lt;/code&gt; and &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; solve the same optimization problem, via different means.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f823045d1b6e6e3e2566fad8b86b9a7f7274035" translate="yes" xml:space="preserve">
          <source>String describing the type of covariance parameters to use. Must be one of:</source>
          <target state="translated">사용할 공분산 모수의 유형을 설명하는 문자열입니다. 다음 중 하나 여야합니다.</target>
        </trans-unit>
        <trans-unit id="24715b349c9d19241a871e75b2e65bf44d424eee" translate="yes" xml:space="preserve">
          <source>String describing the type of the weight concentration prior. Must be one of:</source>
          <target state="translated">이전의 중량 농도 유형을 설명하는 문자열 다음 중 하나 여야합니다.</target>
        </trans-unit>
        <trans-unit id="8c721819e7f297061a3852981915f8763538ca5d" translate="yes" xml:space="preserve">
          <source>String identifier for kernel function to use or the kernel function itself. Only &amp;lsquo;rbf&amp;rsquo; and &amp;lsquo;knn&amp;rsquo; strings are valid inputs. The function passed should take two inputs, each of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="428566ee279a0d9edb0dac9070b52a231b258cad" translate="yes" xml:space="preserve">
          <source>String identifier for kernel function to use or the kernel function itself. Only &amp;lsquo;rbf&amp;rsquo; and &amp;lsquo;knn&amp;rsquo; strings are valid inputs. The function passed should take two inputs, each of shape [n_samples, n_features], and return a [n_samples, n_samples] shaped weight matrix</source>
          <target state="translated">사용할 커널 함수 또는 커널 함수 자체의 문자열 식별자. 'rbf'및 'knn'문자열 만 유효한 입력입니다. 전달 된 함수는 각각 모양 [n_samples, n_features]의 두 입력을 가져 와서 [n_samples, n_samples] 모양의 가중치 행렬을 반환해야합니다.</target>
        </trans-unit>
        <trans-unit id="af724a0a2167e8652dc92f95eace643e40894f38" translate="yes" xml:space="preserve">
          <source>String identifier for kernel function to use or the kernel function itself. Only &amp;lsquo;rbf&amp;rsquo; and &amp;lsquo;knn&amp;rsquo; strings are valid inputs. The function passed should take two inputs, each of shape [n_samples, n_features], and return a [n_samples, n_samples] shaped weight matrix.</source>
          <target state="translated">사용할 커널 함수 또는 커널 함수 자체의 문자열 식별자. 'rbf'및 'knn'문자열 만 유효한 입력입니다. 전달 된 함수는 각각 모양 [n_samples, n_features]의 두 입력을 가져 와서 [n_samples, n_samples] 모양의 가중치 행렬을 반환해야합니다.</target>
        </trans-unit>
        <trans-unit id="62948e7b4671e9ca0f3cde3750c969c3432c4224" translate="yes" xml:space="preserve">
          <source>String identifier of the dataset. Note that OpenML can have multiple datasets with the same name.</source>
          <target state="translated">데이터 세트의 문자열 식별자입니다. OpenML은 이름이 같은 여러 데이터 세트를 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="df0679bd93bc3d3699b427e726c60dd8e54a8049" translate="yes" xml:space="preserve">
          <source>String inputs, &amp;ldquo;absolute_loss&amp;rdquo; and &amp;ldquo;squared_loss&amp;rdquo; are supported which find the absolute loss and squared loss per sample respectively.</source>
          <target state="translated">샘플 당 절대 손실과 제곱 손실을 각각 찾는 문자열 입력 &quot;absolute_loss&quot;및 &quot;squared_loss&quot;가 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="0e35f8f4354526879dda20784b410a6fffd10219" translate="yes" xml:space="preserve">
          <source>String must be in {&amp;lsquo;frobenius&amp;rsquo;, &amp;lsquo;kullback-leibler&amp;rsquo;, &amp;lsquo;itakura-saito&amp;rsquo;}. Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from &amp;lsquo;frobenius&amp;rsquo; (or 2) and &amp;lsquo;kullback-leibler&amp;rsquo; (or 1) lead to significantly slower fits. Note that for beta_loss &amp;lt;= 0 (or &amp;lsquo;itakura-saito&amp;rsquo;), the input matrix X cannot contain zeros. Used only in &amp;lsquo;mu&amp;rsquo; solver.</source>
          <target state="translated">문자열은 { 'frobenius', 'kullback-leibler', 'itakura-saito'}에 있어야합니다. X와 내적 WH 사이의 거리를 측정하여 베타 발산을 최소화합니다. 'frobenius'(또는 2) 및 'kullback-leibler'(또는 1)와 다른 값은 피팅 속도가 상당히 느려집니다. beta_loss &amp;lt;= 0 (또는 'itakura-saito')의 경우 입력 행렬 X는 0을 포함 할 수 없습니다. 'mu'솔버에서만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9f2d9e288ea5ff4eb7ea1abaab0c518bb3979797" translate="yes" xml:space="preserve">
          <source>String names for input features if available. By default, &amp;ldquo;x0&amp;rdquo;, &amp;ldquo;x1&amp;rdquo;, &amp;hellip; &amp;ldquo;xn_features&amp;rdquo; is used.</source>
          <target state="translated">사용 가능한 경우 입력 기능의 문자열 이름. 기본적으로&amp;ldquo;x0&amp;rdquo;,&amp;ldquo;x1&amp;rdquo;,&amp;hellip;&amp;ldquo;xn_features&amp;rdquo;가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6eb302a1a8353d21c585781076747cec5064ac6a" translate="yes" xml:space="preserve">
          <source>String representation of the input tree in GraphViz dot format. Only returned if &lt;code&gt;out_file&lt;/code&gt; is None.</source>
          <target state="translated">GraphViz 도트 형식으로 입력 트리의 문자열 표현. &lt;code&gt;out_file&lt;/code&gt; 이 None 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="a25a8a192fb86c92debb43e92001c859f89e3fcf" translate="yes" xml:space="preserve">
          <source>String[s] representing allowed sparse matrix formats, such as &amp;lsquo;csc&amp;rsquo;, &amp;lsquo;csr&amp;rsquo;, etc. If the input is sparse but not in the allowed format, it will be converted to the first listed format. True allows the input to be any format. False means that a sparse matrix input will raise an error.</source>
          <target state="translated">'csc', 'csr'등과 같이 허용 된 희소 행렬 형식을 나타내는 문자열입니다. 입력이 희소하지만 허용 된 형식이 아닌 경우, 첫 번째로 나열된 형식으로 변환됩니다. True는 입력을 모든 형식으로 허용합니다. False는 희소 행렬 입력으로 인해 오류가 발생 함을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="5110c05a58c323fc1a90d8e9c2d4e3215bf368fd" translate="yes" xml:space="preserve">
          <source>Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b91b90e5e49a63cce92ec827bcf2ae012d9565f3" translate="yes" xml:space="preserve">
          <source>Subsequently, the object is created as:</source>
          <target state="translated">결과적으로 객체는 다음과 같이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="858f5a05f03a44bd2c0d7ffef4c39076939797fb" translate="yes" xml:space="preserve">
          <source>Subset of X on axis 0 or 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8cf7e7f541f13164e6f0420a446eeb6e92a09d1" translate="yes" xml:space="preserve">
          <source>Subset of X on first axis</source>
          <target state="translated">첫 번째 축에서 X의 하위 집합</target>
        </trans-unit>
        <trans-unit id="6ae596021e773a90882ea646d69c3ae9bc66f60f" translate="yes" xml:space="preserve">
          <source>Subset of target values</source>
          <target state="translated">목표 값의 부분 집합</target>
        </trans-unit>
        <trans-unit id="71578b0f6daa48f798b6ba24f599e04480076227" translate="yes" xml:space="preserve">
          <source>Subset of the target values</source>
          <target state="translated">목표 값의 부분 집합</target>
        </trans-unit>
        <trans-unit id="b6f57836197cd859fcd7456747a897dac9249a7c" translate="yes" xml:space="preserve">
          <source>Subset of the target values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="223f88ba981735506f55650c24adc2c0be541ac7" translate="yes" xml:space="preserve">
          <source>Subset of the training data</source>
          <target state="translated">훈련 데이터의 부분 집합</target>
        </trans-unit>
        <trans-unit id="a36f00d869bab77d370e6340b596e76174606ee7" translate="yes" xml:space="preserve">
          <source>Subset of the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bc315a85db741490d46c866dcdf3685f245d4e2" translate="yes" xml:space="preserve">
          <source>Subset of training data</source>
          <target state="translated">교육 데이터의 하위 세트</target>
        </trans-unit>
        <trans-unit id="19abeb39c58b2714170ce5a2488e41705eacf825" translate="yes" xml:space="preserve">
          <source>Subset of training points used to construct the feature map.</source>
          <target state="translated">기능 맵을 구성하는 데 사용되는 교육 지점의 하위 집합입니다.</target>
        </trans-unit>
        <trans-unit id="af82dc274666b6dae18b2b0a4a918322786e1ec9" translate="yes" xml:space="preserve">
          <source>Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.</source>
          <target state="translated">이러한 데이터 그룹은 도메인별로 다릅니다. 예를 들어 여러 환자에서 수집 한 의료 데이터가 있고 각 환자에서 여러 개의 샘플을 채취 한 경우가 있습니다. 이러한 데이터는 개별 그룹에 따라 달라질 수 있습니다. 이 예에서 각 샘플의 환자 ID는 그룹 식별자입니다.</target>
        </trans-unit>
        <trans-unit id="116040368f9a04b617abdb5e80205920c1827d88" translate="yes" xml:space="preserve">
          <source>Such integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).</source>
          <target state="translated">그러나 이러한 정수 표현은 연속적인 입력을 기대하고 범주가 정렬되는 것으로 해석하기 때문에 모든 scikit-learn 추정기와 함께 직접 사용할 수는 없습니다 (예 : 브라우저 세트는 임의로 정렬 됨).</target>
        </trans-unit>
        <trans-unit id="db90c3a55b9a44b531c3ac8e926f4bff46ae5000" translate="yes" xml:space="preserve">
          <source>Sum of squared distances of samples to their closest cluster center.</source>
          <target state="translated">가장 가까운 군집 중심까지의 표본의 제곱 거리의 합.</target>
        </trans-unit>
        <trans-unit id="854e30e5027349dd68053e9c07e26612b753dfb3" translate="yes" xml:space="preserve">
          <source>Sum of the impurities of the subtree leaves for the corresponding alpha value in &lt;code&gt;ccp_alphas&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d0794742200525b0f1825276d5e113f8014eaee" translate="yes" xml:space="preserve">
          <source>Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c895953cca3a7be3ad68f35cb10044d9d62303c" translate="yes" xml:space="preserve">
          <source>Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount. Then divide by the best possible score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51853ebee0d0437a819288d394e52f2825e89e10" translate="yes" xml:space="preserve">
          <source>Sum-kernel k1 + k2 of two kernels k1 and k2.</source>
          <target state="translated">두 커널 k1과 k2의 합 커널 k1 + k2.</target>
        </trans-unit>
        <trans-unit id="bb594232250fde950a53bc755dc305c05d6d4d31" translate="yes" xml:space="preserve">
          <source>Summary Statistics</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70ee3e3bff0af30ecffa237a657d140e21c08452" translate="yes" xml:space="preserve">
          <source>Summary Statistics:</source>
          <target state="translated">요약 통계 :</target>
        </trans-unit>
        <trans-unit id="fd64088007de4ee1ec90faddb61b9fabe7591dbe" translate="yes" xml:space="preserve">
          <source>Supervised learning algorithms will require a category label for each document in the training set. In this case the category is the name of the newsgroup which also happens to be the name of the folder holding the individual documents.</source>
          <target state="translated">지도 학습 알고리즘에는 학습 세트의 각 문서에 대한 카테고리 레이블이 필요합니다. 이 경우 범주는 뉴스 그룹의 이름이며 개별 문서가 들어있는 폴더의 이름이기도합니다.</target>
        </trans-unit>
        <trans-unit id="a76d63a44e8696360e974f3be74fa9ede463ccb8" translate="yes" xml:space="preserve">
          <source>Supervised learning: predicting an output variable from high-dimensional observations</source>
          <target state="translated">지도 학습 : 고차원 관측치에서 출력 변수 예측</target>
        </trans-unit>
        <trans-unit id="a3f901fb4e96c309b27de60d3fa05a2f6c90ddfd" translate="yes" xml:space="preserve">
          <source>Support Vector Classification (SVC) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eafe7087c2917502cf9a105460eb618a5158ac5" translate="yes" xml:space="preserve">
          <source>Support Vector Classification (SVC) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">SVC (Support Vector Classification)는 RandomForestClassifier로서 훨씬 더 많은 시그 모이 드 곡선을 보여줍니다. 이는 최대 한계 방법 (Niculescu-Mizil 및 Caruana &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1] 비교&lt;/a&gt; )에서 일반적으로 결정 경계에 가까운 단단한 샘플 ( 지원 벡터).</target>
        </trans-unit>
        <trans-unit id="ed5eaa4e09c1fde40caa79c99d658b1a804b4ecf" translate="yes" xml:space="preserve">
          <source>Support Vector Machine algorithms are not scale invariant, so &lt;strong&gt;it is highly recommended to scale your data&lt;/strong&gt;. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the &lt;em&gt;same&lt;/em&gt; scaling must be applied to the test vector to obtain meaningful results. See section &lt;a href=&quot;preprocessing#preprocessing&quot;&gt;Preprocessing data&lt;/a&gt; for more details on scaling and normalization.</source>
          <target state="translated">Support Vector Machine 알고리즘은 스케일이 변하지 않으므로 &lt;strong&gt;데이터를 스케일하는 것이 좋습니다&lt;/strong&gt; . 예를 들어 입력 벡터 X의 각 속성을 [0,1] 또는 [-1, + 1]로 스케일링 하거나 평균 0과 분산 1을 갖도록 표준화합니다 . 테스트 벡터에 &lt;em&gt;동일한&lt;/em&gt; 스케일링을 적용하여 의미있는 결과를 얻습니다. 스케일링 및 정규화에 대한 자세한 내용은 &lt;a href=&quot;preprocessing#preprocessing&quot;&gt;전처리 데이터&lt;/a&gt; 섹션을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="6759d5fe5cd02546f72e19999514f8d8dd0c2afc" translate="yes" xml:space="preserve">
          <source>Support Vector Machine algorithms are not scale invariant, so &lt;strong&gt;it is highly recommended to scale your data&lt;/strong&gt;. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the &lt;em&gt;same&lt;/em&gt; scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7db4fe2bb2b495808d702cc828d549eda5ecd6dd" translate="yes" xml:space="preserve">
          <source>Support Vector Machine for Regression implemented using libsvm.</source>
          <target state="translated">libsvm을 사용하여 구현 된 회귀 벡터 시스템 지원</target>
        </trans-unit>
        <trans-unit id="f893d85d40edb954e6730df0c490772b3e2a0229" translate="yes" xml:space="preserve">
          <source>Support Vector Machine for classification implemented with libsvm with a parameter to control the number of support vectors.</source>
          <target state="translated">지원 벡터 수를 제어하는 ​​매개 변수와 함께 libsvm으로 구현 된 분류 용 지원 벡터 시스템.</target>
        </trans-unit>
        <trans-unit id="5051cb5a7ebb600b6c87a0405fb53ae2a929b69a" translate="yes" xml:space="preserve">
          <source>Support Vector Machine for classification using libsvm.</source>
          <target state="translated">libsvm을 사용한 분류를위한 Vector Machine을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="c9c0030d3280fdd6faa0de4e8ec40fd895c7cc05" translate="yes" xml:space="preserve">
          <source>Support Vector Machine for regression implemented using libsvm using a parameter to control the number of support vectors.</source>
          <target state="translated">지원 벡터 수를 제어하는 ​​매개 변수를 사용하여 libsvm을 사용하여 구현 된 회귀 용 지원 벡터 머신.</target>
        </trans-unit>
        <trans-unit id="0e9e942139034d62a386d593445cc7ca0b8119c8" translate="yes" xml:space="preserve">
          <source>Support Vector Machines</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09b4534def5c091569acb02092fe6cf8bbcb767a" translate="yes" xml:space="preserve">
          <source>Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based implementation scales between \(O(n_{features} \times n_{samples}^2)\) and \(O(n_{features} \times n_{samples}^3)\) depending on how efficiently the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; cache is used in practice (dataset dependent). If the data is very sparse \(n_{features}\) should be replaced by the average number of non-zero features in a sample vector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56158ab7bc33e3424017c0101c6f6a1afb88a3a9" translate="yes" xml:space="preserve">
          <source>Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by this &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based implementation scales between \(O(n_{features} \times n_{samples}^2)\) and \(O(n_{features} \times n_{samples}^3)\) depending on how efficiently the &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; cache is used in practice (dataset dependent). If the data is very sparse \(n_{features}\) should be replaced by the average number of non-zero features in a sample vector.</source>
          <target state="translated">Support Vector Machine은 강력한 도구이지만 훈련 벡터 수에 따라 컴퓨팅 및 스토리지 요구 사항이 빠르게 증가합니다. SVM의 핵심은 2 차 프로그래밍 문제 (QP)로 나머지 훈련 데이터에서 지원 벡터를 분리합니다. 이 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 기반 구현에서 사용되는 QP 솔버 는 \ (O (n_ {features} \ times n_ {samples} ^ 2) \)와 \ (O (n_ {features} \ times n_ {samples} ^ 3) \ 사이에서 확장됩니다. ) &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 캐시가 실제로 얼마나 효율적으로 사용되는지에 따라 (데이터 집합에 따라 다름) 데이터가 매우 드문 경우 \ (n_ {features} \)는 샘플 벡터에서 0이 아닌 평균의 수로 대체해야합니다.</target>
        </trans-unit>
        <trans-unit id="5aeba1d5d3764ce4f342c9f3c5c4d98c95831ef3" translate="yes" xml:space="preserve">
          <source>Support Vector Regression (SVR) using linear and non-linear kernels</source>
          <target state="translated">선형 및 비선형 커널을 사용하여 SVR (Vector Regression) 지원</target>
        </trans-unit>
        <trans-unit id="c38caf86bc0ea77939ed0d559b2d4f17cae05de8" translate="yes" xml:space="preserve">
          <source>Support Vector Regression implemented using libsvm.</source>
          <target state="translated">libsvm을 사용하여 구현 된 벡터 회귀 지원</target>
        </trans-unit>
        <trans-unit id="9f57f9c660b4dd2f0deaa4ba97e0c878c516d5af" translate="yes" xml:space="preserve">
          <source>Support vector machines (SVMs)</source>
          <target state="translated">지원 벡터 머신 (SVM)</target>
        </trans-unit>
        <trans-unit id="bbbf41eb38c0c6ebc14c9776b74f3b7c7223e260" translate="yes" xml:space="preserve">
          <source>Support vectors.</source>
          <target state="translated">지원 벡터.</target>
        </trans-unit>
        <trans-unit id="a54e8408d47bb6e31202d1c04b19dcb2a41dd085" translate="yes" xml:space="preserve">
          <source>Supports sparse matrices, as long as they are nonnegative.</source>
          <target state="translated">음수가 아닌 한 희소 행렬을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="3d69897cfb127444947f0af512011088c32f7842" translate="yes" xml:space="preserve">
          <source>Suppose there are \(n\) training samples, \(m\) features, \(k\) hidden layers, each containing \(h\) neurons - for simplicity, and \(o\) output neurons. The time complexity of backpropagation is \(O(n\cdot m \cdot h^k \cdot o \cdot i)\), where \(i\) is the number of iterations. Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.</source>
          <target state="translated">\ (n \) 훈련 샘플, \ (m \) 기능, \ (k \) 숨겨진 레이어 (각각 \ (h \) 뉴런 포함-단순함) 및 \ (o \) 출력 뉴런이 있다고 가정합니다. 역 전파의 시간 복잡도는 \ (O (n \ cdot m \ cdot h ^ k \ cdot o \ cdot i) \)이며 여기서 \ (i \)는 반복 횟수입니다. 역전 파는 시간 복잡성이 높기 때문에 적은 수의 숨겨진 뉴런과 적은 수의 숨겨진 레이어로 시작하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="cd4ffcedd7e0903b657852f1e48effcf51cd6dba" translate="yes" xml:space="preserve">
          <source>Suppose you have a machine with 8 CPUs. Consider a case where you&amp;rsquo;re running a &lt;code&gt;GridSearchCV&lt;/code&gt; (parallelized with joblib) with &lt;code&gt;n_jobs=8&lt;/code&gt; over a &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; (parallelized with OpenMP). Each instance of &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; will spawn 8 threads (since you have 8 CPUs). That&amp;rsquo;s a total of &lt;code&gt;8 * 8 = 64&lt;/code&gt; threads, which leads to oversubscription of physical CPU resources and to scheduling overhead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="717b26aef2df5c03a35ae859cfcbb420ec45f953" translate="yes" xml:space="preserve">
          <source>Swaps two columns of a CSC/CSR matrix in-place.</source>
          <target state="translated">CSC / CSR 매트릭스의 두 열을 제자리에 교환합니다.</target>
        </trans-unit>
        <trans-unit id="1069fce64f91499526a54ca2a920222a7a6a7b20" translate="yes" xml:space="preserve">
          <source>Swaps two rows of a CSC/CSR matrix in-place.</source>
          <target state="translated">CSC / CSR 매트릭스의 두 행을 제자리에 교환합니다.</target>
        </trans-unit>
        <trans-unit id="fe072010fa51f4d65d4b1c57510d1adce13a6e7b" translate="yes" xml:space="preserve">
          <source>Swiss Roll reduction with LLE</source>
          <target state="translated">LLE를 사용한 스위스 롤 감소</target>
        </trans-unit>
        <trans-unit id="2230299d58c6b8fd7778e9806246c78a89ba5d37" translate="yes" xml:space="preserve">
          <source>Symmetrized version of the input array, i.e. the average of array and array.transpose(). If sparse, then duplicate entries are first summed and zeros are eliminated.</source>
          <target state="translated">입력 배열의 대칭 버전, 즉 배열과 array.transpose ()의 평균. 드문 경우 중복 항목을 먼저 합산하고 0을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="5617e20da29f8f9d1be80cd4e8da4f2cca7d87a9" translate="yes" xml:space="preserve">
          <source>Symmetry: d(x, y) = d(y, x)</source>
          <target state="translated">대칭 : d (x, y) = d (y, x)</target>
        </trans-unit>
        <trans-unit id="5c4b58b32e84506455d7badad68c3391e5ed62f8" translate="yes" xml:space="preserve">
          <source>Synthetic example</source>
          <target state="translated">합성 예</target>
        </trans-unit>
        <trans-unit id="a2f05b63d3eed62a3d034f7470902282f6f3879f" translate="yes" xml:space="preserve">
          <source>T. Calinski and J. Harabasz, 1974. &amp;ldquo;A dendrite method for cluster analysis&amp;rdquo;. Communications in Statistics</source>
          <target state="translated">T. Calinski and J. Harabasz, 1974.&amp;ldquo;클러스터 분석을위한 덴 드라이트 방법&amp;rdquo;. 통계 커뮤니케이션</target>
        </trans-unit>
        <trans-unit id="cfd0a0e6ed4317c498cad6dff52fb64a880cf1fc" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman,&amp;ldquo;통계 학습의 요소 Ed. 2&amp;rdquo;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="7f7943ebfea41ffafd05b1021989488d98e43338" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, p592-593, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman,&amp;ldquo;통계 학습의 요소 Ed. 2&amp;rdquo;, p592-593, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="70c29954ab4c3cb7794dae94a173c1937d4eb172" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman,&amp;ldquo;통계학의 요소&amp;rdquo;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="1a36ad5ec8c80f7993b9fd82eb2686d2da21883a" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman, &lt;a href=&quot;https://web.stanford.edu/~hastie/ElemStatLearn//&quot;&gt;The Elements of Statistical Learning&lt;/a&gt;, Second Edition, Section 10.13.2, Springer, 2009.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83fcdb4c642340f440ec3c76643b0ff4a3e4d907" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman. &amp;ldquo;Elements of Statistical Learning&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman. &quot;통계학 학습 요소&quot;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="cb7835aaacc19565f84e186774c00699f37d4811" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman. 통계 학습의 요소 Ed. 2, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="d4c99bc2ba4c28ec35fea9e0c8535d7da602917b" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.</source>
          <target state="translated">T. Hastie, R. Tibshirani 및 J. Friedman. 통계 학습의 요소, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="080b47cb3536b08b8d64a0132c354c0e69235639" translate="yes" xml:space="preserve">
          <source>T. Hastie, R. Tibshirani, J. Friedman, &lt;a href=&quot;https://web.stanford.edu/~hastie/ElemStatLearn/&quot;&gt;The Elements of Statistical Learning&lt;/a&gt;, Springer 2009</source>
          <target state="translated">T. Hastie, R. Tibshirani, J. Friedman, &lt;a href=&quot;https://web.stanford.edu/~hastie/ElemStatLearn/&quot;&gt;통계 학습의 요소&lt;/a&gt; , Springer 2009</target>
        </trans-unit>
        <trans-unit id="caa676716fee5a5c020ff878a7d0616f2b82e015" translate="yes" xml:space="preserve">
          <source>T. Ho, &amp;ldquo;The random subspace method for constructing decision forests&amp;rdquo;, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.</source>
          <target state="translated">T. Ho,&amp;ldquo;의사 결정 포리스트 구성을위한 무작위 부분 공간 방법&amp;rdquo;, 패턴 분석 및 머신 인텔리전스, 20 (8), 832-844, 1998.</target>
        </trans-unit>
        <trans-unit id="12a252b4085c50c08e5600b6a2ace31faa3ef960" translate="yes" xml:space="preserve">
          <source>T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou &amp;ldquo;Nystroem Method vs Random Fourier Features: A Theoretical and Empirical Comparison&amp;rdquo;, Advances in Neural Information Processing Systems 2012</source>
          <target state="translated">T. Yang, Y. Li, M. Mahdavi, R. Jin 및 Z. Zhou&amp;ldquo;Nystroem 방법 대 랜덤 푸리에 특징 : 이론적 및 경험적 비교&amp;rdquo;, 신경 정보 처리 시스템의 발전 2012</target>
        </trans-unit>
        <trans-unit id="01f0642e8e9ab9a87342728e5ceb8bbd9d2f4ab3" translate="yes" xml:space="preserve">
          <source>TAX full-value property-tax rate per $10,000</source>
          <target state="translated">$ 10,000 당 세금 전체 가치 재산 세율</target>
        </trans-unit>
        <trans-unit id="dd1b5c68340d106d37b309522fe8b393cb21ad39" translate="yes" xml:space="preserve">
          <source>TF-IDF vectors of text documents crawled from the web</source>
          <target state="translated">웹에서 크롤링 된 텍스트 문서의 TF-IDF 벡터</target>
        </trans-unit>
        <trans-unit id="45e8bc91482fcb8372ecf82971819bb3adf7f455" translate="yes" xml:space="preserve">
          <source>TODO: implement zip dataset loading too</source>
          <target state="translated">TODO : zip 데이터 셋 로딩도 구현</target>
        </trans-unit>
        <trans-unit id="8ab0e32d1d047cd892b558c9b2f078b6857615c4" translate="yes" xml:space="preserve">
          <source>Takes a group array to group observations.</source>
          <target state="translated">그룹 배열을 사용하여 관측치를 그룹화합니다.</target>
        </trans-unit>
        <trans-unit id="3fd5fa24212eb9a6093f6fb3922373c2e928c57e" translate="yes" xml:space="preserve">
          <source>Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks).</source>
          <target state="translated">불균형 한 클래스 배포 (이진 또는 멀티 클래스 분류 작업)로 폴드를 작성하지 않도록 그룹 정보를 고려합니다.</target>
        </trans-unit>
        <trans-unit id="d9f2745c15759b2e07e7b8ac9dcbd8d3eb7f1df5" translate="yes" xml:space="preserve">
          <source>Talks given, slide-sets and other information relevant to scikit-learn.</source>
          <target state="translated">scikit-learn과 관련된 대화, 슬라이드 세트 및 기타 정보.</target>
        </trans-unit>
        <trans-unit id="61ad50a9b9189cc3cf1874568e35e7901ff4c982" translate="yes" xml:space="preserve">
          <source>Target</source>
          <target state="translated">Target</target>
        </trans-unit>
        <trans-unit id="27a0909e2e214e16b84c188da9b6e36fbb24a75c" translate="yes" xml:space="preserve">
          <source>Target Domain</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0810f8f564f353b71a4c98cca217fcf1b526a4f" translate="yes" xml:space="preserve">
          <source>Target cardinality</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a92a80f8cb5657b9d712fa7c0bc7d1998153a6b8" translate="yes" xml:space="preserve">
          <source>Target names used for plotting. By default, &lt;code&gt;labels&lt;/code&gt; will be used if it is defined, otherwise the unique labels of &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; will be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3566560919d090e98a5bc58e40d68ba478487e60" translate="yes" xml:space="preserve">
          <source>Target number of non-zero coefficients. Use &lt;code&gt;np.inf&lt;/code&gt; for no limit.</source>
          <target state="translated">0이 아닌 계수의 대상 수입니다. &lt;code&gt;np.inf&lt;/code&gt; 를 제한없이 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="de81f661c0a5f66b2eb62d654cf5ee97c42a462f" translate="yes" xml:space="preserve">
          <source>Target relative to X for classification or regression; None for unsupervised learning.</source>
          <target state="translated">분류 또는 회귀 분석을 위해 X를 기준으로하는 목표; 비지도 학습에는 해당되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d9d3de45f60123470c229b29def5cf613978229f" translate="yes" xml:space="preserve">
          <source>Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by &amp;ldquo;decision_function&amp;rdquo; on some classifiers).</source>
          <target state="translated">목표 점수는 포지티브 클래스의 확률 추정치, 신뢰도 값 또는 임계 값이 아닌 결정 측정치 (일부 분류기의 &quot;decision_function&quot;에 의해 반환 됨) 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2da36130db1b72e7220423e41225d3cfbecbf96b" translate="yes" xml:space="preserve">
          <source>Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by &amp;ldquo;decision_function&amp;rdquo; on some classifiers). For binary y_true, y_score is supposed to be the score of the class with greater label.</source>
          <target state="translated">목표 점수는 포지티브 클래스의 확률 추정치, 신뢰도 값 또는 임계 값이 아닌 결정 측정치 (일부 분류기의 &quot;decision_function&quot;에 의해 반환 됨) 일 수 있습니다. 이진 y_true의 경우 y_score는 레이블이 더 큰 클래스의 점수 여야합니다.</target>
        </trans-unit>
        <trans-unit id="e311afc7eeab469d8890dd7eea87d765736badbd" translate="yes" xml:space="preserve">
          <source>Target scores, can either be probability estimates, confidence values, or non-thresholded measure of decisions (as returned by &amp;ldquo;decision_function&amp;rdquo; on some classifiers).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b0d9d264d1c088751e6e40c1e8e25db9f44f02f" translate="yes" xml:space="preserve">
          <source>Target scores. In the binary and multilabel cases, these can be either probability estimates or non-thresholded decision values (as returned by &lt;code&gt;decision_function&lt;/code&gt; on some classifiers). In the multiclass case, these must be probability estimates which sum to 1. The binary case expects a shape (n_samples,), and the scores must be the scores of the class with the greater label. The multiclass and multilabel cases expect a shape (n_samples, n_classes). In the multiclass case, the order of the class scores must correspond to the order of &lt;code&gt;labels&lt;/code&gt;, if provided, or else to the numerical or lexicographical order of the labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c1467eb9fce38ab3f431a143b7b4099a3d2d978" translate="yes" xml:space="preserve">
          <source>Target values</source>
          <target state="translated">목표 값</target>
        </trans-unit>
        <trans-unit id="1eb29851ae3516c30efee3683f12f4c58d29d5ce" translate="yes" xml:space="preserve">
          <source>Target values (class labels in classification, real numbers in regression)</source>
          <target state="translated">목표 값 (분류의 클래스 레이블, 회귀의 실수)</target>
        </trans-unit>
        <trans-unit id="9236c7bb185e41917cc98485dd0c1b72938dc4f1" translate="yes" xml:space="preserve">
          <source>Target values (integers for classification, real numbers for regression).</source>
          <target state="translated">목표 값 (분류를위한 정수, 회귀를위한 실수).</target>
        </trans-unit>
        <trans-unit id="abfa5417a6d6ee53dab20f6c0ef952512e0950c9" translate="yes" xml:space="preserve">
          <source>Target values (integers)</source>
          <target state="translated">목표 값 (정수)</target>
        </trans-unit>
        <trans-unit id="030d74b88e6ada2cc611aa05819c6875ad926cb6" translate="yes" xml:space="preserve">
          <source>Target values (integers). Will be cast to X&amp;rsquo;s dtype if necessary</source>
          <target state="translated">목표 값 (정수). 필요한 경우 X의 dtype으로 캐스팅됩니다.</target>
        </trans-unit>
        <trans-unit id="489913dd1ba6e89f6fe19c6ab73a6d0ba1572bbc" translate="yes" xml:space="preserve">
          <source>Target values (strings or integers in classification, real numbers in regression) For classification, labels must correspond to classes.</source>
          <target state="translated">목표 값 (분류의 문자열 또는 정수, 회귀의 실수) 분류의 경우 레이블은 클래스와 일치해야합니다.</target>
        </trans-unit>
        <trans-unit id="20f1907edd6deff55545e2c6878457953ca23f05" translate="yes" xml:space="preserve">
          <source>Target values in training data (also required for prediction)</source>
          <target state="translated">훈련 데이터의 목표 값 (예측에도 필요)</target>
        </trans-unit>
        <trans-unit id="5698f85443295556a3231f89aa327fa20aab0ad9" translate="yes" xml:space="preserve">
          <source>Target values of shape = [n_samples] or [n_samples, n_outputs]</source>
          <target state="translated">shape의 목표 값 = [n_samples] 또는 [n_samples, n_outputs]</target>
        </trans-unit>
        <trans-unit id="da9e802f308bd36e270eb5bda433836f08ce2390" translate="yes" xml:space="preserve">
          <source>Target values, array of float values, shape = [n_samples]</source>
          <target state="translated">목표 값, 부동 값의 배열, 모양 = [n_samples]</target>
        </trans-unit>
        <trans-unit id="9d4acf064ddb5b23ed0c4bdf7cb1ed1c86e0cce4" translate="yes" xml:space="preserve">
          <source>Target values, must be binary</source>
          <target state="translated">대상 값, 이진이어야합니다</target>
        </trans-unit>
        <trans-unit id="3784ae1e62853f0d4899c1eb47f9d650c50e4292" translate="yes" xml:space="preserve">
          <source>Target values.</source>
          <target state="translated">목표 값.</target>
        </trans-unit>
        <trans-unit id="7a264b43381dcd3fa803548587f56b48ebe73a21" translate="yes" xml:space="preserve">
          <source>Target values. All sparse matrices are converted to CSR before inverse transformation.</source>
          <target state="translated">목표 값. 모든 희소 행렬은 역변환 전에 CSR로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="338b0342f37ab2a3243f471f75fbb6b8060759e1" translate="yes" xml:space="preserve">
          <source>Target values. Class labels must be an integer or float, or array-like objects of integer or float for multilabel classifications.</source>
          <target state="translated">목표 값. 클래스 레이블은 정수 또는 부동 소수점이거나 다중 레이블 분류의 경우 정수 또는 부동 소수점 배열과 같은 객체 여야합니다.</target>
        </trans-unit>
        <trans-unit id="d95ddd0372c185ada4f873880d8cf72b3e972b79" translate="yes" xml:space="preserve">
          <source>Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification.</source>
          <target state="translated">목표 값. 2 차원 행렬은 0과 1 만 포함해야하며 다중 레이블 분류를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="7d68da7045b7713975074def112516b22e3d548e" translate="yes" xml:space="preserve">
          <source>Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification. Sparse matrix can be CSR, CSC, COO, DOK, or LIL.</source>
          <target state="translated">목표 값. 2 차원 행렬은 0과 1 만 포함해야하며 다중 레이블 분류를 나타냅니다. 희소 행렬은 CSR, CSC, COO, DOK 또는 LIL 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5ca333a3206ea1ae310cc995419dd5b579b0311c" translate="yes" xml:space="preserve">
          <source>Target values. Will be cast to X&amp;rsquo;s dtype if necessary</source>
          <target state="translated">목표 값. 필요한 경우 X의 dtype으로 캐스팅됩니다.</target>
        </trans-unit>
        <trans-unit id="26a7504c7e3fd5624ea9f7504b20b691f54baa64" translate="yes" xml:space="preserve">
          <source>Target values. Will be cast to X&amp;rsquo;s dtype if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a73f8fffa45e4edde85ae5cfe9a7df8f5b0ddf64" translate="yes" xml:space="preserve">
          <source>Target vector (class labels).</source>
          <target state="translated">대상 벡터 (클래스 레이블)</target>
        </trans-unit>
        <trans-unit id="fbbf7212be6c75614582f7683105e9b145317ffe" translate="yes" xml:space="preserve">
          <source>Target vector relative to X</source>
          <target state="translated">X에 상대적인 목표 벡터</target>
        </trans-unit>
        <trans-unit id="9b9ad2408038efc60fe0697bee9336f5ceee389a" translate="yes" xml:space="preserve">
          <source>Target vector relative to X.</source>
          <target state="translated">X에 상대적인 목표 벡터.</target>
        </trans-unit>
        <trans-unit id="cb81b6b3ab32530c4b31b59fded732e0bc1457db" translate="yes" xml:space="preserve">
          <source>Target vector.</source>
          <target state="translated">목표 벡터.</target>
        </trans-unit>
        <trans-unit id="86747748812222a9af434d10160edcea63797259" translate="yes" xml:space="preserve">
          <source>Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.</source>
          <target state="translated">n_samples는 샘플 수이고 n_targets는 반응 변수 수입니다.</target>
        </trans-unit>
        <trans-unit id="bb444a37f78059e3557a89e3cec7d30ee2a0e255" translate="yes" xml:space="preserve">
          <source>Target. Will be cast to X&amp;rsquo;s dtype if necessary</source>
          <target state="translated">표적. 필요한 경우 X의 dtype으로 캐스팅됩니다.</target>
        </trans-unit>
        <trans-unit id="652ac2cbbafccc62d55637f20bfa949ef565ffbd" translate="yes" xml:space="preserve">
          <source>Target:</source>
          <target state="translated">Target:</target>
        </trans-unit>
        <trans-unit id="d35260a00f655f27edcc35a7eb16da44a4f671a6" translate="yes" xml:space="preserve">
          <source>Targets</source>
          <target state="translated">Targets</target>
        </trans-unit>
        <trans-unit id="bae347ef05fa5719d83860ee11ad8e50b4550a95" translate="yes" xml:space="preserve">
          <source>Targets for input data.</source>
          <target state="translated">입력 데이터의 대상입니다.</target>
        </trans-unit>
        <trans-unit id="25e14b664fd8a2e3008eacd528868e3512f875a8" translate="yes" xml:space="preserve">
          <source>Targets for supervised learning.</source>
          <target state="translated">지도 학습을위한 목표.</target>
        </trans-unit>
        <trans-unit id="135d4c14ef59d437ba9b3977ff9548aa96a57252" translate="yes" xml:space="preserve">
          <source>Targets for supervised or &lt;code&gt;None&lt;/code&gt; for unsupervised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e907b7e300146da06f6bd372dfec64398cc10d60" translate="yes" xml:space="preserve">
          <source>Targets used for scoring. Must fulfill label requirements for all steps of the pipeline.</source>
          <target state="translated">득점에 사용되는 목표. 파이프 라인의 모든 단계에 대한 레이블 요구 사항을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="12f7c88d38da9108a78eb595ada57372e18cdd00" translate="yes" xml:space="preserve">
          <source>Technically the Lasso model is optimizing the same objective function as the Elastic Net with &lt;code&gt;l1_ratio=1.0&lt;/code&gt; (no L2 penalty).</source>
          <target state="translated">기술적으로 Lasso 모델은 &lt;code&gt;l1_ratio=1.0&lt;/code&gt; (L2 페널티 없음)으로 Elastic Net과 동일한 목적 함수를 최적화합니다 .</target>
        </trans-unit>
        <trans-unit id="7c21757d6dba7765c9b420762d607df44985a57d" translate="yes" xml:space="preserve">
          <source>Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.</source>
          <target state="translated">n = 442 명의 당뇨병 환자 각각에 대해 10 개의 기준 변수, 연령, 성별, 체질량 지수, 평균 혈압 및 6 개의 혈청 측정 값을 얻었으며, 관심 반응, 기준선 1 년 후 질병 진행의 정량적 척도 .</target>
        </trans-unit>
        <trans-unit id="faacbc438202f94eb51c4c27efecd77f5a804a90" translate="yes" xml:space="preserve">
          <source>Tenenbaum, J.B.; De Silva, V.; &amp;amp; Langford, J.C. A global geometric framework for nonlinear dimensionality reduction. Science 290 (5500)</source>
          <target state="translated">테네 바움, JB; 데 실바, V .; &amp;amp; Langford, JC 비선형 차원 축소를위한 글로벌 기하학적 프레임 워크. 과학 290 (5500)</target>
        </trans-unit>
        <trans-unit id="2a1358959d0f2f819085e4aa4680265c467cbf33" translate="yes" xml:space="preserve">
          <source>Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">Tenenhaus, M. (1998). La regression PLS : 이론과 실용. 파리 : 에디션 테크닉.</target>
        </trans-unit>
        <trans-unit id="0a268d2f62458299ec67330e170374c2cecaa669" translate="yes" xml:space="preserve">
          <source>Terms that were ignored because they either:</source>
          <target state="translated">다음과 같은 이유로 무시 된 용어 :</target>
        </trans-unit>
        <trans-unit id="9f2d4d3a12b50c0b296af732b3fa3674c7292a32" translate="yes" xml:space="preserve">
          <source>Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).</source>
          <target state="translated">우도를 계산할 테스트 데이터. 여기서 n_samples는 샘플 수이고 n_features는 피처 수입니다. X_test는 적합에 사용 된 데이터 (중심 포함)와 동일한 분포에서 도출 된 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="ed7e95a0302971bec5a032415d69927921186679" translate="yes" xml:space="preserve">
          <source>Test data to be transformed, must have the same number of features as the data used to train the model.</source>
          <target state="translated">변환 할 테스트 데이터는 모델 학습에 사용 된 데이터와 동일한 수의 기능을 가져야합니다.</target>
        </trans-unit>
        <trans-unit id="65eaa1a409cbf0736a7b1da17a35a153fc9af91f" translate="yes" xml:space="preserve">
          <source>Test samples</source>
          <target state="translated">테스트 샘플</target>
        </trans-unit>
        <trans-unit id="29446ed524d3e237184352cbcf6e1c5aaeb464e4" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator. Passing None as test samples gives the same result as passing real test samples, since DummyRegressor operates independently of the sampled observations.</source>
          <target state="translated">shape = (n_samples, n_features) 또는 None으로 샘플을 테스트합니다. 일부 추정기의 경우, 이는 사전 계산 된 커널 매트릭스 일 수 있습니다. 대신 shape = (n_samples, n_samples_fitted], 여기서 n_samples_fitted는 추정기 피팅에 사용 된 샘플 수입니다. DummyRegressor는 샘플링 된 관찰과 독립적으로 작동하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="12cad09d9d4837878fb37fd506e5f7fb71a801c9" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. Passing None as test samples gives the same result as passing real test samples, since DummyClassifier operates independently of the sampled observations.</source>
          <target state="translated">shape = (n_samples, n_features) 또는 None으로 샘플을 테스트합니다. DummyClassifier가 샘플링 된 관측치와 독립적으로 작동하므로 테스트 샘플로 없음을 전달하면 실제 테스트 샘플을 전달하는 것과 동일한 결과가 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="0c1d5bbb82f5cfc66b7e35e84179b02f4b13f8b1" translate="yes" xml:space="preserve">
          <source>Test samples.</source>
          <target state="translated">테스트 샘플.</target>
        </trans-unit>
        <trans-unit id="bdec5057d32ebe97bd4c525fff2435009f4be0a0" translate="yes" xml:space="preserve">
          <source>Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.</source>
          <target state="translated">테스트 샘플. 일부 추정기의 경우, 이는 사전 계산 된 커널 매트릭스 일 수 있습니다. 대신 shape = (n_samples, n_samples_fitted]입니다. 여기서 n_samples_fitted는 추정기 피팅에 사용 된 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="a3c0fad25d001ac0a0589946fd03f6f0be795bec" translate="yes" xml:space="preserve">
          <source>Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead, shape = (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f9baccc70399290d29568f9812594e6335c4ae0" translate="yes" xml:space="preserve">
          <source>Test with permutations the significance of a classification score</source>
          <target state="translated">분류 점수의 중요성을 순열로 테스트</target>
        </trans-unit>
        <trans-unit id="c67f73aee0c3dc1034cba236cfe8c8526d7a9123" translate="yes" xml:space="preserve">
          <source>Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.</source>
          <target state="translated">텍스트 분석은 기계 학습 알고리즘의 주요 응용 분야입니다. 그러나 원시 데이터, 일련의 기호는 가변 길이의 원시 텍스트 문서가 아닌 고정 크기의 숫자 피처 벡터를 기대하기 때문에 알고리즘 자체에 직접 공급 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="990226708ed5e3f7319c13e5c3e22d22fd438346" translate="yes" xml:space="preserve">
          <source>Text is made of characters, but files are made of bytes. These bytes represent characters according to some &lt;em&gt;encoding&lt;/em&gt;. To work with text files in Python, their bytes must be &lt;em&gt;decoded&lt;/em&gt; to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist.</source>
          <target state="translated">텍스트는 문자로 구성되지만 파일은 바이트로 구성됩니다. 이 바이트는 일부 &lt;em&gt;인코딩&lt;/em&gt; 에 따라 문자를 나타냅니다 . Python에서 텍스트 파일로 작업하려면 바이트를 유니 코드라고하는 문자 세트 로 &lt;em&gt;디코딩&lt;/em&gt; 해야합니다 . 일반적인 인코딩은 ASCII, Latin-1 (서유럽), KOI8-R (러시아어) 및 범용 인코딩 UTF-8 및 UTF-16입니다. 다른 많은 존재합니다.</target>
        </trans-unit>
        <trans-unit id="2a2f9f7e298485c4a85bf6b791046a1b63087cf9" translate="yes" xml:space="preserve">
          <source>Text preprocessing, tokenizing and filtering of stopwords are all included in &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;, which builds a dictionary of features and transforms documents to feature vectors:</source>
          <target state="translated">텍스트 사전 처리, 토큰 화 및 중지 어 필터링은 모두 &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 에&lt;/a&gt; 포함되어 있습니다. 이 기능은 피처 사전을 작성하고 문서를 피처 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="81ed5011593c32dbca614ea281c1292f374dff62" translate="yes" xml:space="preserve">
          <source>Text summary of all the rules in the decision tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79142cb36f8945e4d341c82cf5dc060dc02c8f0f" translate="yes" xml:space="preserve">
          <source>Text summary of the precision, recall, F1 score for each class. Dictionary returned if output_dict is True. Dictionary has the following structure:</source>
          <target state="translated">각 클래스의 정밀도, 리콜, F1 점수에 대한 텍스트 요약. output_dict가 True 인 경우 사전이 반환되었습니다. 사전의 구조는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f042ff208f7aff2fdebc20ebdcfe3611681222ed" translate="yes" xml:space="preserve">
          <source>Tf is &amp;ldquo;n&amp;rdquo; (natural) by default, &amp;ldquo;l&amp;rdquo; (logarithmic) when &lt;code&gt;sublinear_tf=True&lt;/code&gt;. Idf is &amp;ldquo;t&amp;rdquo; when use_idf is given, &amp;ldquo;n&amp;rdquo; (none) otherwise. Normalization is &amp;ldquo;c&amp;rdquo; (cosine) when &lt;code&gt;norm='l2'&lt;/code&gt;, &amp;ldquo;n&amp;rdquo; (none) when &lt;code&gt;norm=None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;sublinear_tf=True&lt;/code&gt; 인 경우 Tf는 기본적으로 &quot;n&quot;(자연), &quot;l&quot;(대수) 입니다. use_idf가 제공되면 Idf는 &quot;t&quot;이고, 그렇지 않으면 &quot;n&quot;(없음)입니다. &lt;code&gt;norm='l2'&lt;/code&gt; 인 경우 정규화는 &quot;c&quot;(코사인) 이고 &lt;code&gt;norm=None&lt;/code&gt; 인 경우 &quot;n&quot;(없음) 입니다.</target>
        </trans-unit>
        <trans-unit id="97730bbab5383bbe19dd65de91be719c29295304" translate="yes" xml:space="preserve">
          <source>Tf means &lt;strong&gt;term-frequency&lt;/strong&gt; while tf&amp;ndash;idf means term-frequency times &lt;strong&gt;inverse document-frequency&lt;/strong&gt;: \(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).</source>
          <target state="translated">TF 수단 &lt;strong&gt;용어 주파수&lt;/strong&gt; 동안 TF-IDF 수단 용어 주파수 회 &lt;strong&gt;문서 주파수 역&lt;/strong&gt; : \ (\ 텍스트 {TF-IDF (t, d)} = \ 텍스트 {TF (t, d)} \ 시간 \ 텍스트 {IDF (티)}\).</target>
        </trans-unit>
        <trans-unit id="f1cc0d39fa695e88ce231644990ae2b503602ddb" translate="yes" xml:space="preserve">
          <source>Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.</source>
          <target state="translated">Tf는 항-주파수를 의미하고 tf-idf는 항-주파수 시간과 문서 주파수의 역수를 의미합니다. 이것은 정보 검색에서 일반적으로 사용되는 가중치 체계이며 문서 분류에도 유용합니다.</target>
        </trans-unit>
        <trans-unit id="771178a448f62a1d367a9045d97d08b26eb6869c" translate="yes" xml:space="preserve">
          <source>Tf-idf-weighted document-term matrix.</source>
          <target state="translated">tf-idf 가중 문서 용어 행렬.</target>
        </trans-unit>
        <trans-unit id="daaa1c74bc4f107e3ab2cba2520cc29b4809af00" translate="yes" xml:space="preserve">
          <source>TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.</source>
          <target state="translated">TfidfVectorizer는 메모리 내 어휘 (python dict)를 사용하여 가장 빈번한 단어를 피처 인덱스에 매핑하고 단어 발생 빈도 (스파 스) 행렬을 계산합니다. 그런 다음 단어 빈도는 말뭉치에 걸쳐 기능별로 수집 된 IDF (Inverse Document Frequency) 벡터를 사용하여 다시 가중치가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="2c1e749a66bcce49e10daf1b4a981af3bebb9284" translate="yes" xml:space="preserve">
          <source>That this function takes time at least quadratic in n_samples. For large datasets, it&amp;rsquo;s wise to set that parameter to a small value.</source>
          <target state="translated">이 함수는 n_samples에서 적어도 2 차 시간이 걸립니다. 큰 데이터 집합의 경우 해당 매개 변수를 작은 값으로 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="fb20b5f965f5eb1a2ab7f1c1219e1e8adbfdfc00" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001.</source>
          <target state="translated">&quot;균형&quot;휴리스틱은 2001 년 King, Zen, Rare Events Data의 Logistic Regression에서 영감을 얻었습니다.</target>
        </trans-unit>
        <trans-unit id="f4d692dace6b9f963c8a80ff6fb54e77b0936bf5" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정합니다.</target>
        </trans-unit>
        <trans-unit id="ef34b0ee7fbdfc2770447dcdf0759da38193f229" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정 합니다.</target>
        </trans-unit>
        <trans-unit id="66610aa2288acbb0ecf69897c27cb9799d361b2f" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 자동으로 조정합니다 : &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ef1bbf84c17d648e39cb34085672c6faaeb47082" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced_subsample&amp;rdquo; mode is the same as &amp;ldquo;balanced&amp;rdquo; except that weights are computed based on the bootstrap sample for every tree grown.</source>
          <target state="translated">&amp;ldquo;balanced_subsample&amp;rdquo;모드는 가중 된 모든 트리에 대한 부트 스트랩 샘플을 기반으로 가중치가 계산된다는 점을 제외하면&amp;ldquo;balanced_subsample&amp;rdquo;모드는&amp;ldquo;balanced&amp;rdquo;와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="ed963145b52986c215cb0664e22ee6755071701e" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo; is an optimization algorithm that approximates the Broyden&amp;ndash;Fletcher&amp;ndash;Goldfarb&amp;ndash;Shanno algorithm &lt;a href=&quot;#id28&quot; id=&quot;id23&quot;&gt;8&lt;/a&gt;, which belongs to quasi-Newton methods. The &amp;ldquo;lbfgs&amp;rdquo; solver is recommended for use for small data-sets but for larger datasets its performance suffers. &lt;a href=&quot;#id29&quot; id=&quot;id24&quot;&gt;9&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd9d2df3102671da4d90b6a16223e62bcf13a40d" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo; solver is used by default for its robustness. For large datasets the &amp;ldquo;saga&amp;rdquo; solver is usually faster. For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss, which might be even faster but requires more tuning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9570d1ba54b75e486c6a8fe70ab0af402d1567cc" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;newton-cg&amp;rdquo; solvers only support L2 penalization and are found to converge faster for some high dimensional data. Setting &lt;code&gt;multi_class&lt;/code&gt; to &amp;ldquo;multinomial&amp;rdquo; with these solvers learns a true multinomial logistic regression model &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]&lt;/a&gt;, which means that its probability estimates should be better calibrated than the default &amp;ldquo;one-vs-rest&amp;rdquo; setting.</source>
          <target state="translated">&quot;lbfgs&quot;, &quot;sag&quot;및 &quot;newton-cg&quot;솔버는 L2 불이익 만 지원하며 일부 고차원 데이터의 경우 더 빨리 수렴됩니다. 이러한 솔버를 사용하여 &lt;code&gt;multi_class&lt;/code&gt; 를 &quot;다항식&quot;으로 설정 하면 진정한 다항 로지스틱 회귀 모델 &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]을&lt;/a&gt; 알게 되므로 확률 추정값이 기본 &quot;1 대 1 대&quot;설정보다 더 잘 교정되어야합니다.</target>
        </trans-unit>
        <trans-unit id="5921538cf9d0b305a04fb55e5723f201454da417" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;newton-cg&amp;rdquo; solvers only support \(\ell_2\) regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting &lt;code&gt;multi_class&lt;/code&gt; to &amp;ldquo;multinomial&amp;rdquo; with these solvers learns a true multinomial logistic regression model &lt;a href=&quot;#id25&quot; id=&quot;id20&quot;&gt;5&lt;/a&gt;, which means that its probability estimates should be better calibrated than the default &amp;ldquo;one-vs-rest&amp;rdquo; setting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c76cf618fdea0e603c8990092e5d960628df7c0c" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;new&amp;rdquo; data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.</source>
          <target state="translated">&quot;새로운&quot;데이터는 입력 데이터의 선형 조합으로 구성되며 KDE 모델에 따라 가중 적으로 가중치가 그려집니다.</target>
        </trans-unit>
        <trans-unit id="6e028a15cea05a7ec04768381b1c3e0f7d725291" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;sag&amp;rdquo; solver uses Stochastic Average Gradient descent &lt;a href=&quot;#id26&quot; id=&quot;id21&quot;&gt;6&lt;/a&gt;. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="420229f24d72cfc948f72b9aaf53e46dfcb25b62" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;sag&amp;rdquo; solver uses a Stochastic Average Gradient descent &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.</source>
          <target state="translated">&quot;새그&quot;솔버는 확률 평균 그라디언트 디센트 (Stochastic Average Gradient descent)를 사용합니다 &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt; . 샘플 수와 피처 수가 모두 많은 경우 대규모 데이터 세트의 경우 다른 솔버보다 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="2dbe4d8b05b25b3ecc8447f4c7fa6494b583e905" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver &lt;a href=&quot;#id27&quot; id=&quot;id22&quot;&gt;7&lt;/a&gt; is a variant of &amp;ldquo;sag&amp;rdquo; that also supports the non-smooth &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt;. This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports &lt;code&gt;penalty=&quot;elasticnet&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf55bf4220bbf6e5ad8c38b76c827b53a1e3f193" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; is a variant of &amp;ldquo;sag&amp;rdquo; that also supports the non-smooth &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; option. This is therefore the solver of choice for sparse multinomial logistic regression.</source>
          <target state="translated">&quot;사가&quot;솔버 &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; 는 &quot;sag&quot;의 변형으로 부드러운 &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; 옵션 도 지원합니다 . 따라서 이것은 희소 다항 로지스틱 회귀 분석을 위해 선택하는 솔버입니다.</target>
        </trans-unit>
        <trans-unit id="77bf2f7306c562160b3a78c9199a57470ad6395e" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver is often the best choice. The &amp;ldquo;liblinear&amp;rdquo; solver is used by default for historical reasons.</source>
          <target state="translated">&quot;사가&quot;솔버가 종종 최선의 선택입니다. 역사적인 이유로 &quot;liblinear&quot;솔버가 기본적으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="068bc43bd479e1422a1e2139866c2ca587dbb3ad" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;steepness&amp;rdquo; of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.</source>
          <target state="translated">ROC 곡선의 &quot;스티프니스&quot;도 중요합니다. 위양성 비율을 최소화하면서 실제 양의 비율을 최대화하는 것이 이상적입니다.</target>
        </trans-unit>
        <trans-unit id="0eb5d5532023d8acbeeebff557bc347056c3c6a7" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;target&amp;rdquo; for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.</source>
          <target state="translated">이 데이터베이스의 &quot;대상&quot;은 사진을 찍는 사람의 신원을 나타내는 0에서 39 사이의 정수입니다. 그러나 클래스 당 10 개의 예제 만있는이 상대적으로 작은 데이터 세트는 감독되지 않은 또는 반 감독 된 관점에서 더 흥미 롭습니다.</target>
        </trans-unit>
        <trans-unit id="457f2fe1e264c1b033671c931911858da10508e3" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;auto&amp;rsquo; mode is the default and is intended to pick the cheaper option of the two depending on the shape of the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6893a2ecba3f5b3ceba43b94c7037a23940a0678" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;auto&amp;rsquo; mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data.</source>
          <target state="translated">'자동'모드는 기본값이며 훈련 데이터의 모양과 형식에 따라 두 가지 중 더 저렴한 옵션을 선택하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="c686d2e453ba3e5377730ccb9178f9e3372548ba" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;brute&amp;rsquo; method is a generic method that works with any estimator. It approximates the above integral by computing an average over the data &lt;code&gt;X&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f80449b3a36a9645d51b541d6ac4415080a7df2" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;cd&amp;rsquo; solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.</source>
          <target state="translated">'cd'솔버는 Frobenius 표준 만 최적화 할 수 있습니다. NMF의 근본적인 비 볼록성으로 인해 동일한 거리 함수를 최적화 할 때에도 서로 다른 솔버가 서로 다른 최소 점으로 수렴 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="370b11b6ae177f24cc2d42a049dda5c0d7e30775" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;eigen&amp;rsquo; solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the &amp;lsquo;eigen&amp;rsquo; solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</source>
          <target state="translated">'고유'솔버는 클래스 스 캐터 비율 사이의 클래스 스 캐터 간 최적화를 기반으로합니다. 분류 및 변환에 모두 사용할 수 있으며 수축을 지원합니다. 그러나 '고 유량'솔버는 공분산 행렬을 계산해야하므로 많은 피쳐가있는 상황에는 적합하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="969c56b312ffc26d2c3d7a44ecbd20546b358e11" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;log&amp;rsquo; loss gives logistic regression, a probabilistic classifier. &amp;lsquo;modified_huber&amp;rsquo; is another smooth loss that brings tolerance to outliers as well as probability estimates. &amp;lsquo;squared_hinge&amp;rsquo; is like hinge but is quadratically penalized. &amp;lsquo;perceptron&amp;rsquo; is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; for a description.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccda076fda793672987d7568e3ca12c3047fb684" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;log&amp;rsquo; loss gives logistic regression, a probabilistic classifier. &amp;lsquo;modified_huber&amp;rsquo; is another smooth loss that brings tolerance to outliers as well as probability estimates. &amp;lsquo;squared_hinge&amp;rsquo; is like hinge but is quadratically penalized. &amp;lsquo;perceptron&amp;rsquo; is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.</source>
          <target state="translated">'로그'손실은 확률 적 분류 기인 로지스틱 회귀를 제공합니다. 'modified_huber'는 확률 추정치뿐만 아니라 특이 치에 대한 내성을 제공하는 또 다른 부드러운 손실입니다. 'squared_hinge'는 힌지와 비슷하지만 2 차적으로 불이익을받습니다. 'perceptron'은 perceptron 알고리즘에서 사용되는 선형 손실입니다. 다른 손실은 회귀를 위해 설계되었지만 분류에도 유용 할 수 있습니다. 설명은 SGDRegressor를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="960f213c3988b3643a9995192d8dbe5d183a7506" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;lsqr&amp;rsquo; solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix \(\Sigma\), and supports shrinkage. This solver computes the coefficients \(\omega_k = \Sigma^{-1}\mu_k\) by solving for \(\Sigma \omega = \mu_k\), thus avoiding the explicit computation of the inverse \(\Sigma^{-1}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5302138e8a256151a982f3c737747f8db1fec2f7" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;lsqr&amp;rsquo; solver is an efficient algorithm that only works for classification. It supports shrinkage.</source>
          <target state="translated">'lsqr'솔버는 분류에만 작동하는 효율적인 알고리즘입니다. 수축을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="920a5500dd530a18d71ed258f87f05c8340a0987" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, and &amp;lsquo;lbfgs&amp;rsquo; solvers support only L2 regularization with primal formulation, or no regularization. The &amp;lsquo;liblinear&amp;rsquo; solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the &amp;lsquo;saga&amp;rsquo; solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2a91334301a1b93c477cd479a707ce044fefdf3" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, and &amp;lsquo;lbfgs&amp;rsquo; solvers support only L2 regularization with primal formulation. The &amp;lsquo;liblinear&amp;rsquo; solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">'newton-cg', 'sag'및 'lbfgs'솔버는 기본 공식을 사용한 L2 정규화 만 지원합니다. 'liblinear'솔버는 L2 페널티에 대해서만 이중 공식을 사용하여 L1 및 L2 정규화를 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="f2d81bdc600d48b918368b0b02059bc57b808de5" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;recursion&amp;rsquo; method is faster than the &amp;lsquo;brute&amp;rsquo; method, but it is only supported by some tree-based estimators. It is computed as follows. For a given point \(x_S\), a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ade2e6c6872bcfb8e63408411f739881d7395764" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;squared_loss&amp;rsquo; refers to the ordinary least squares fit. &amp;lsquo;huber&amp;rsquo; modifies &amp;lsquo;squared_loss&amp;rsquo; to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. &amp;lsquo;epsilon_insensitive&amp;rsquo; ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. &amp;lsquo;squared_epsilon_insensitive&amp;rsquo; is the same but becomes squared loss past a tolerance of epsilon.</source>
          <target state="translated">'squared_loss'는 일반적인 최소 제곱 적합을 나타냅니다. 'huber'는 엡실론 거리를지나 제곱에서 선형 손실로 전환하여 특이 치를 올바르게 얻는 데 덜 집중하도록 'squared_loss'를 수정합니다. 'epsilon_insensitive'는 epsilon보다 작은 오류를 무시하고 그 직전의 선형입니다. SVR에서 사용되는 손실 기능입니다. 'squared_epsilon_insensitive'는 동일하지만 엡실론 공차를지나 제곱 손실이됩니다.</target>
        </trans-unit>
        <trans-unit id="d846a2b9506766851ba4d72a28fde6b068825be1" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;svd&amp;rsquo; solver is the default solver used for &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;, and it is the only available solver for &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;. It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the &amp;lsquo;svd&amp;rsquo; solver may be preferable in situations where the number of features is large. The &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix \(\Sigma_k\) is, by definition, equal to \(\frac{1}{n - 1} X_k^tX_k = V S^2 V^t\) where \(V\) comes from the SVD of the (centered) matrix: \(X_k = U S V^t\). It turns out that we can compute the log-posterior above without having to explictly compute \(\Sigma\): computing \(S\) and \(V\) via the SVD of \(X\) is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix \(X\) and the SVD of the class-wise mean vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="388443bd992c152f7c80a788085a15982e280e0a" translate="yes" xml:space="preserve">
          <source>The (scaled) interquartile range for each feature in the training set.</source>
          <target state="translated">훈련 세트의 각 기능에 대한 (확장 된) 사 분위수 범위.</target>
        </trans-unit>
        <trans-unit id="1db16517be9cf545f06172e2c55188fa78cfa7fa" translate="yes" xml:space="preserve">
          <source>The (sometimes surprising) observation is that this is &lt;em&gt;still a linear model&lt;/em&gt;: to see this, imagine creating a new set of features</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3533a4edec1fdb05f12a2a421dc320606ca77c6" translate="yes" xml:space="preserve">
          <source>The (sometimes surprising) observation is that this is &lt;em&gt;still a linear model&lt;/em&gt;: to see this, imagine creating a new variable</source>
          <target state="translated">(때로는 놀라운) 관찰은 이것이 &lt;em&gt;여전히 선형 모델이라는 것입니다&lt;/em&gt; . 이것을 보려면 새 변수를 만드는 것을 상상해보십시오.</target>
        </trans-unit>
        <trans-unit id="ae37fbc1863417aba870f086fd7dcb7d12932667" translate="yes" xml:space="preserve">
          <source>The (x,y) position of the lower-left corner, in degrees</source>
          <target state="translated">왼쪽 아래 모서리의 (x, y) 위치 (도)</target>
        </trans-unit>
        <trans-unit id="bcd6ca42c3472afbe27069a62710b5c531496d9b" translate="yes" xml:space="preserve">
          <source>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper &amp;ldquo;Newsweeder: Learning to filter netnews,&amp;rdquo; though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트는 약 20,000 개의 뉴스 그룹 문서 모음으로, 20 개의 다른 뉴스 그룹에서 (거의) 균등하게 분할됩니다. 우리가 아는 한, Ken Lang은 원래 그의 논문 &quot;Newsweeder : Learn to filter netnews&quot;를 위해 수집 한 것이지만,이 컬렉션을 명시 적으로 언급하지는 않습니다. 20 개의 뉴스 그룹 모음은 텍스트 분류 및 텍스트 클러스터링과 같은 기계 학습 기술의 텍스트 응용 프로그램 실험에 널리 사용되는 데이터 세트가되었습니다.</target>
        </trans-unit>
        <trans-unit id="4b2a042059fffe007deb9ebabf02d8062c1e6bda" translate="yes" xml:space="preserve">
          <source>The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트는 교육 (또는 개발)과 테스트 (또는 성능 평가)를위한 두 개의 하위 세트로 분할 된 20 개의 주제에 대한 약 18000 개의 뉴스 그룹 게시물로 구성됩니다. 열차와 테스트 세트의 분할은 특정 날짜 전후에 게시 된 메시지를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="6f66371b5ad2b199bde3ecde676da8dfe31ce717" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#ht2001&quot; id=&quot;id25&quot;&gt;[HT2001]&lt;/a&gt; multiclass AUC metric can be extended to be weighted by the prevalence:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c380ecdb017c04631da3ca1753b6ddf07ce8267f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.cluster&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; module gathers popular unsupervised clustering algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.cluster&quot;&gt; &lt;code&gt;sklearn.cluster&lt;/code&gt; 의&lt;/a&gt; 모듈은 인기 자율 클러스터링 알고리즘을 수집합니다.</target>
        </trans-unit>
        <trans-unit id="6a798b177e574d6ff4ac12be7b93e3b8f8d74b71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; module includes methods and algorithms to robustly estimate the covariance of features given a set of points. The precision matrix defined as the inverse of the covariance is also estimated. Covariance estimation is closely related to the theory of Gaussian Graphical Models.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; 의&lt;/a&gt; 모듈 견고 점들의 세트를 주어진 기능의 공분산을 추정하는 방법 및 알고리즘을 포함한다. 공분산의 역으로 ​​정의 된 정밀 행렬도 추정됩니다. 공분산 추정은 가우스 그래픽 모델 이론과 밀접한 관련이 있습니다.</target>
        </trans-unit>
        <trans-unit id="d1230decfda989b60168bc88df7b70ef79122b2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.datasets&quot;&gt;&lt;code&gt;sklearn.datasets&lt;/code&gt;&lt;/a&gt; module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.datasets&quot;&gt; &lt;code&gt;sklearn.datasets&lt;/code&gt; 의&lt;/a&gt; 모듈로드 인기 기준 데이터 세트를 페치하는 방법을 포함하는 데이터 세트에로드 유틸리티를 포함한다. 또한 일부 인공 데이터 생성기가 있습니다.</target>
        </trans-unit>
        <trans-unit id="94bdb0abc615359801b0dde8f5ed432fa774aae6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; module includes matrix decomposition algorithms, including among others PCA, NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; 의&lt;/a&gt; 모듈은, 행렬 분해 알고리즘을 포함하는 다른 PCA, ICA 중 NMF 또는 포함된다. 이 모듈의 대부분의 알고리즘은 차원 축소 기술로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cce83af2900332bbe995457713d2e3977e6cea91" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes ensemble-based methods for classification, regression and anomaly detection.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 분류, 회귀 이상 검출을위한 앙상블 기반 방법을 포함한다.</target>
        </trans-unit>
        <trans-unit id="a3b34965d608c8571221686c4eaa7dd2128cda34" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.exceptions&quot;&gt;&lt;code&gt;sklearn.exceptions&lt;/code&gt;&lt;/a&gt; module includes all custom warnings and error classes used across scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.exceptions&quot;&gt; &lt;code&gt;sklearn.exceptions&lt;/code&gt; 의&lt;/a&gt; 모듈에 걸쳐 사용되는 모든 사용자 정의 경고 및 오류 클래스 포함 scikit을 배우기.</target>
        </trans-unit>
        <trans-unit id="88149e0dc35a9af4a24d012611fba0cc883c2d66" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.experimental&quot;&gt;&lt;code&gt;sklearn.experimental&lt;/code&gt;&lt;/a&gt; module provides importable modules that enable the use of experimental features or estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="953e85b8304fe86126d3f8d4d49e2c347def818a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module deals with feature extraction from raw data. It currently includes methods to extract features from text and images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; 의&lt;/a&gt; 모듈은 원시 데이터에서 특징 추출 다루고있다. 현재 텍스트와 이미지에서 기능을 추출하는 방법이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="2ff97b1fa019f5f400e3468860cd96aea04f63dc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to extract features from images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt; 의&lt;/a&gt; 서브 모듈 집결 유틸리티는 이미지에서 특징을 추출합니다.</target>
        </trans-unit>
        <trans-unit id="5fb7f21374928a29d973d39c8eed205507941559" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to build feature vectors from text documents.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; &lt;/a&gt; 서브 모듈 집결 유틸리티는 텍스트 문서로부터 특징 벡터를 구축합니다.</target>
        </trans-unit>
        <trans-unit id="f8894b12541a4b0b591ccbf9c1c5e42bf4d0c13b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module implements feature selection algorithms. It currently includes univariate filter selection methods and the recursive feature elimination algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; 의&lt;/a&gt; 모듈 구현을 선택 알고리즘 기능. 현재는 일 변량 필터 선택 방법과 재귀 기능 제거 알고리즘을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="af392a06a08e896e0c1f9a845ceba81c0151ed14" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt;&lt;code&gt;sklearn.gaussian_process&lt;/code&gt;&lt;/a&gt; module implements Gaussian Process based regression and classification.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt; &lt;code&gt;sklearn.gaussian_process&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 가우시안 프로세스를 기반으로 회귀 및 분류.</target>
        </trans-unit>
        <trans-unit id="385213737ec5d4067fe933c56afb4e9039eb2d7c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.inspection&quot;&gt;&lt;code&gt;sklearn.inspection&lt;/code&gt;&lt;/a&gt; module includes tools for model inspection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e25959a779b184ae02a906c2808f68686c73aab5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt;&lt;code&gt;sklearn.kernel_approximation&lt;/code&gt;&lt;/a&gt; module implements several approximate kernel feature maps base on Fourier transforms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt; &lt;code&gt;sklearn.kernel_approximation&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 몇 가지 대략적인 커널 기능은 푸리에 변환에 기반을 매핑합니다.</target>
        </trans-unit>
        <trans-unit id="311a58f7516f87097f4b239ae388620a5fb8155d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; module implements a variety of linear models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf3dc31fd9ef458aef6de4af32bf51a7e61106a1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; module implements generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; 의&lt;/a&gt; 모듈을 구현하는 선형 모델 일반화. 여기에는 최소 회귀 및 좌표 하강으로 계산 된 릿지 회귀, 베이지안 회귀, 올가미 및 탄성 망 추정기가 포함됩니다. 또한 확률 적 그라디언트 디센트 관련 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="75e43c84de91a4a1d643ca88db0beef9e86494b8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.manifold&quot;&gt;&lt;code&gt;sklearn.manifold&lt;/code&gt;&lt;/a&gt; module implements data embedding techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.manifold&quot;&gt; &lt;code&gt;sklearn.manifold&lt;/code&gt; &lt;/a&gt; 모듈을 구현하는 데이터 임베딩 기술.</target>
        </trans-unit>
        <trans-unit id="f55aa3d6c230d41fe62ec5929fa59e00645b5d62" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module includes score functions, performance metrics and pairwise metrics and distance computations.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 의&lt;/a&gt; 모듈 점수 기능, 성능 메트릭 및 페어 메트릭 및 거리 계산을 포함한다.</target>
        </trans-unit>
        <trans-unit id="90553131dabe004a613ea2c87be35b6b6db9a1ae" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt;&lt;code&gt;sklearn.metrics.cluster&lt;/code&gt;&lt;/a&gt; submodule contains evaluation metrics for cluster analysis results. There are two forms of evaluation:</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt; &lt;code&gt;sklearn.metrics.cluster&lt;/code&gt; 의&lt;/a&gt; 서브 모듈은 클러스터 분석 결과에 대한 평가 지표가 포함되어 있습니다. 평가에는 두 가지 형태가 있습니다.</target>
        </trans-unit>
        <trans-unit id="537333336506a029d4e76c0c5320f3e14636c908" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.mixture&quot;&gt;&lt;code&gt;sklearn.mixture&lt;/code&gt;&lt;/a&gt; module implements mixture modeling algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.mixture&quot;&gt; &lt;code&gt;sklearn.mixture&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 혼합물 모델링 알고리즘.</target>
        </trans-unit>
        <trans-unit id="1b9bcfe9136ee1328197e574e66457c49aa39ded" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt;&lt;code&gt;sklearn.naive_bayes&lt;/code&gt;&lt;/a&gt; module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes&amp;rsquo; theorem with strong (naive) feature independence assumptions.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt; &lt;code&gt;sklearn.naive_bayes&lt;/code&gt; 는&lt;/a&gt; 구현을 나이브 베이 즈 알고리즘 모듈을 포함한다. 이것들은 강력한 (순진한) 특징 독립성 가정으로 베이 즈 정리를 적용하는 것에 기초한지도 학습 방법입니다.</target>
        </trans-unit>
        <trans-unit id="31da4b6c2407f749b7e6e441bc1101265b243a97" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module implements the k-nearest neighbors algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 는&lt;/a&gt; 구현에게 K-가장 가까운 이웃 알고리즘을 모듈.</target>
        </trans-unit>
        <trans-unit id="637db5b82af4c4775ad8c11b2cc086c407ac4adc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neural_network&quot;&gt;&lt;code&gt;sklearn.neural_network&lt;/code&gt;&lt;/a&gt; module includes models based on neural networks.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neural_network&quot;&gt; &lt;code&gt;sklearn.neural_network&lt;/code&gt; 의&lt;/a&gt; 모듈은 신경 네트워크를 기반으로 모델을 포함하고 있습니다.</target>
        </trans-unit>
        <trans-unit id="97c84f48ddcbce9eff5bb423de61ca9bed7742a5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline&lt;/code&gt;&lt;/a&gt; module implements utilities to build a composite estimator, as a chain of transforms and estimators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline&lt;/code&gt; &lt;/a&gt; 모듈 구현 유틸리티 변환하여 추정기의 체인과 같은 복합 추정기를 구축.</target>
        </trans-unit>
        <trans-unit id="3e4a3abf94a63259dfe9d5546d6b613a02821c2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt;&lt;code&gt;sklearn.preprocessing&lt;/code&gt;&lt;/a&gt; module includes scaling, centering, normalization, binarization and imputation methods.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt; &lt;code&gt;sklearn.preprocessing&lt;/code&gt; &lt;/a&gt; 모듈은 스케일링 중심 정규화 이진화 전가 및 방법을 포함한다.</target>
        </trans-unit>
        <trans-unit id="f8f509d37a71bbf7a86f10aff8bf2d2fdd437066" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt;&lt;code&gt;sklearn.preprocessing&lt;/code&gt;&lt;/a&gt; module includes scaling, centering, normalization, binarization methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fd91efb13a21a364a66a195be3f60dbc3429cc3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt;&lt;code&gt;sklearn.semi_supervised&lt;/code&gt;&lt;/a&gt; module implements semi-supervised learning algorithms. These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks. This module includes Label Propagation.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt; &lt;code&gt;sklearn.semi_supervised&lt;/code&gt; &lt;/a&gt; 모듈의 구현은 반지도 학습 알고리즘. 이러한 알고리즘은 분류 작업에 소량의 레이블이 지정된 데이터와 대량의 레이블이없는 데이터를 사용했습니다. 이 모듈에는 레이블 전파가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="6b1e5de562db4c7ae4499c2a4fcb3f75a3027318" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; module includes Support Vector Machine algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; 의&lt;/a&gt; 모듈은 지원 벡터 기계 알고리즘을 포함하고 있습니다.</target>
        </trans-unit>
        <trans-unit id="ef3c16856f883650f7c10c3b8b62a045804fc7da" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module includes decision tree-based models for classification and regression.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; &lt;/a&gt; 모듈은 분류와 회귀를위한 의사 결정 트리 기반 모델이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="fd392963cb16a5b60813f22af8246fa4065eb565" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.utils&quot;&gt;&lt;code&gt;sklearn.utils&lt;/code&gt;&lt;/a&gt; module includes various utilities.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.utils&quot;&gt; &lt;code&gt;sklearn.utils&lt;/code&gt; 의&lt;/a&gt; 모듈은 다양한 유틸리티가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c614be243d558a71ca4ede548ecf4767e4adc7c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;simple example on this dataset&lt;/a&gt; illustrates how starting from the original problem one can shape the data for consumption in scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;이 데이터 세트에&lt;/a&gt; 대한 간단한 예 는 원래 문제에서 시작하여 scikit-learn에서 소비 할 데이터를 구성하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="1173339ea4209f3d2d9f369da23b0882a1bab947" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; estimator was entirely re-worked, and it is now significantly faster and more stable. In addition, the Elkan algorithm is now compatible with sparse matrices. The estimator uses OpenMP based parallelism instead of relying on joblib, so the &lt;code&gt;n_jobs&lt;/code&gt; parameter has no effect anymore. For more details on how to control the number of threads, please refer to our &lt;a href=&quot;../../modules/computing#parallelism&quot;&gt;Parallelism&lt;/a&gt; notes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30870f8f1c3b8d652d87325d159164cb4897186f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;ensemble.HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;ensemble.HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; now have native support for missing values (NaNs). This means that there is no need for imputing data when training or predicting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d52c41feb33f6dcb3543db8b050b747b486d3d88" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; class is very flexible - it can be used with a variety of estimators to do round-robin regression, treating every variable as an output in turn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20080456681e7e51efd596990b6fe9c5442d5451" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;inspection.permutation_importance&lt;/code&gt;&lt;/a&gt; can be used to get an estimate of the importance of each feature, for any fitted estimator:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ce39f7d5e0bf9b7a13f0705f1c0e1a4677070b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; function returns a &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; object that can be used for plotting without needing to recalculate the partial dependence. In this example, we show how to plot partial dependence plots and how to quickly customize the plot with the visualization API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa56488b2914a0f1ce166cbc607c029c9e711d0f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.metrics.mean_tweedie_deviance#sklearn.metrics.mean_tweedie_deviance&quot;&gt;&lt;code&gt;sklearn.metrics.mean_tweedie_deviance&lt;/code&gt;&lt;/a&gt; depends on a &lt;code&gt;power&lt;/code&gt; parameter. As we do not know the true value of the &lt;code&gt;power&lt;/code&gt; parameter, we here compute the mean deviances for a grid of possible values, and compare the models side by side, i.e. we compare them at identical values of &lt;code&gt;power&lt;/code&gt;. Ideally, we hope that one model will be consistently better than the other, regardless of &lt;code&gt;power&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14b4e05e10f859ded14445d95cdfc3e13fffcfe1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;sklearn.metrics.roc_auc_score&lt;/code&gt;&lt;/a&gt; function can be used for multi-class classification. The multi-class One-vs-One scheme compares every unique pairwise combination of classes. In this section, we calculate the AUC using the OvR and OvO schemes. We report a macro average, and a prevalence-weighted average.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f876970ad7209ceedd7d12cc65c7506766be528" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a4f7d548c6a3daf45cbc9aca2408b6479c48a9f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; provides parameters such as &lt;code&gt;min_samples_leaf&lt;/code&gt; and &lt;code&gt;max_depth&lt;/code&gt; to prevent a tree from overfiting. Cost complexity pruning provides another option to control the size of a tree. In &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt;, this pruning technique is parameterized by the cost complexity parameter, &lt;code&gt;ccp_alpha&lt;/code&gt;. Greater values of &lt;code&gt;ccp_alpha&lt;/code&gt; increase the number of nodes pruned. Here we only show the effect of &lt;code&gt;ccp_alpha&lt;/code&gt; on regularizing the trees and how to choose a &lt;code&gt;ccp_alpha&lt;/code&gt; based on validation scores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="424aa7402b9869b036306a671e3630b4177e36b0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;의사 결정 트리가&lt;/a&gt; 추가 시끄러운 관찰과 사인 곡선에 맞게 사용된다. 결과적으로 사인 곡선에 가까운 국소 선형 회귀를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="eb2cbae46431d84a4889d55d659950b594e78664" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;의사 결정 나무는&lt;/a&gt; 동시에 시끄러운 x와 하나의 기본 기능 주어진 원의 Y 관찰을 예측하는 데 사용됩니다. 결과적으로 원에 근사한 국소 선형 회귀를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="4ea0ad8f51ec5bec92f088b272fa90a6ac2d5b55" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function is a data fetching / caching functions that downloads the data archive from the original &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20 newsgroups website&lt;/a&gt;, extracts the archive contents in the &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; folder and calls the &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; on either the training or testing set folder, or both of them:</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; 의&lt;/a&gt; 기능은 데이터 원본에서 보관 다운로드하는 것이 기능을 캐시 / 데이터 가져 오는 것입니다 &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;(20) 뉴스 그룹 웹 사이트&lt;/a&gt; 에서 아카이브 내용을 추출 &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; 폴더와 통화 &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; 을&lt;/a&gt; 하거나 훈련에 또는 테스트 세트 폴더 또는 둘 다 :</target>
        </trans-unit>
        <trans-unit id="26c038b3ea935758dab579b3237ee5588d78f251" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; datasets is subdivided into 3 subsets: the development &lt;code&gt;train&lt;/code&gt; set, the development &lt;code&gt;test&lt;/code&gt; set and an evaluation &lt;code&gt;10_folds&lt;/code&gt; set meant to compute performance metrics using a 10-folds cross validation scheme.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; 의&lt;/a&gt; 데이터 세트가 3 개 하위 집합으로 세분화되어 개발 &lt;code&gt;train&lt;/code&gt; 세트, 개발 &lt;code&gt;test&lt;/code&gt; 세트 및 평가 &lt;code&gt;10_folds&lt;/code&gt; 는 10 폴드 교차 검증 방식을 사용 컴퓨팅 성능 측정에 의미 설정합니다.</target>
        </trans-unit>
        <trans-unit id="d14958ad2582740fd909337c2882b7ba18717e2a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes two averaging algorithms based on randomized &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈은 무작위에 따라 두 평균 알고리즘이 포함되어 &lt;a href=&quot;tree#tree&quot;&gt;의사 결정 트리&lt;/a&gt; 랜덤 포레스트 알고리즘과 엑스트라 나무 방법 :. 두 알고리즘 모두 나무를 위해 특별히 설계된 섭동 및 결합 기법이다 &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; . 이는 분류기 구성에 임의성을 도입하여 다양한 분류기 집합이 생성됨을 의미합니다. 앙상블의 예측은 개별 분류기의 평균 예측으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="fbeef59e0313a7e281a500dd36152abed677fa2e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; 의&lt;/a&gt; 모듈은 텍스트 및 이미지 등의 형식으로 구성된 데이터 세트에서 기계 학습 알고리즘에 의해 지원되는 형식으로 특징을 추출 할 수있다.</target>
        </trans-unit>
        <trans-unit id="ff270d1b4e640c5be645e763d862de5cbdc56019" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.inspection&quot;&gt;&lt;code&gt;sklearn.inspection&lt;/code&gt;&lt;/a&gt; module provides a convenience function &lt;a href=&quot;generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="565412031e53246181e593ab56b9ab7f3accb362" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the &lt;code&gt;sample_weight&lt;/code&gt; parameter.</source>
          <target state="translated">그만큼 &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 은&lt;/a&gt; , 구현을 여러 손실을 모듈 점수 및 유틸리티 기능 분류 성능을 측정 할 수 있습니다. 일부 메트릭에는 양의 클래스, 신뢰도 값 또는 이진 결정 값의 확률 추정값이 필요할 수 있습니다. 대부분의 구현에서는 각 샘플이 &lt;code&gt;sample_weight&lt;/code&gt; 매개 변수를통해 전체 점수에 가중치를 부여 할 수있습니다.</target>
        </trans-unit>
        <trans-unit id="6986be647f522d4ad92a86deccdacfb4588f163b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 은&lt;/a&gt; , 구현을 여러 손실을 모듈 점수 및 유틸리티 함수는 회귀 성능을 측정 할 수 있습니다. 그중 일부는 다중 출력 사례를 처리하도록 향상되었습니다 : &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt; ,&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt; 및&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e3af9dc32993fb04e5c47da4dea690da48a6baa4" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions. For more information see the &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;Clustering performance evaluation&lt;/a&gt; section for instance clustering, and &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering evaluation&lt;/a&gt; for biclustering.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; 구현을 여러 손실, 점수 및 유틸리티 기능을 모듈. 자세한 정보 는 인스턴스 클러스터링에 대한 &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;클러스터링 성능 평가&lt;/a&gt; 섹션 및 &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;바이&lt;/a&gt; 클러스터링에 대한 Biclustering 평가 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="095cb4e1ad7cf586616a563cdbf95404fbb2310e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.</source>
          <target state="translated">그만큼 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; 의&lt;/a&gt; 서브 모듈 구현 유틸리티는 페어 거리 또는 샘플 세트의 친 화성을 평가.</target>
        </trans-unit>
        <trans-unit id="8e4a825968b52124826a5f75996abc15ec7f1a2c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module implements &lt;em&gt;meta-estimators&lt;/em&gt; to solve &lt;code&gt;multiclass&lt;/code&gt; and &lt;code&gt;multilabel&lt;/code&gt; classification problems by decomposing such problems into binary classification problems. &lt;code&gt;multioutput&lt;/code&gt; regression is also supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60f0063776d96ccddba5880841f7defdb7f0d5d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module implements &lt;em&gt;meta-estimators&lt;/em&gt; to solve &lt;code&gt;multiclass&lt;/code&gt; and &lt;code&gt;multilabel&lt;/code&gt; classification problems by decomposing such problems into binary classification problems. Multitarget regression is also supported.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; 의&lt;/a&gt; 모듈 구현의 &lt;em&gt;메타 - 추정기&lt;/em&gt; 해결 &lt;code&gt;multiclass&lt;/code&gt; 및 &lt;code&gt;multilabel&lt;/code&gt; 이진 분류 문제에 대한 이러한 문제를 분해하여 분류 문제. 다중 대상 회귀도 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="8db5d205727541fd60809b9d143967244bf8e79b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt;&lt;code&gt;sklearn.random_projection&lt;/code&gt;&lt;/a&gt; module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: &lt;a href=&quot;#gaussian-random-matrix&quot;&gt;Gaussian random matrix&lt;/a&gt; and &lt;a href=&quot;#sparse-random-matrix&quot;&gt;sparse random matrix&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt; &lt;code&gt;sklearn.random_projection&lt;/code&gt; 의&lt;/a&gt; 모듈 구현하는 간단하고 계산적으로 효율적인 방법은 빠른 처리 시간과 작은 모델 크기에 (추가의 편차 등) 정도의 양을 조절하여 거래 데이터의 차원을 감소시킨다. 이 모듈은 두 가지 유형의 비정형 랜덤 매트릭스를 구현합니다.&lt;a href=&quot;#gaussian-random-matrix&quot;&gt; 가우시안 랜덤 매트릭스&lt;/a&gt; 와 &lt;a href=&quot;#sparse-random-matrix&quot;&gt;희소 랜덤 매트릭스의&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="a34e8e8e9bf3ccf9c2fa51aff7ed58e5e45bbffa" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; class is used to calibrate a classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaa03339275413a44471cce8ed9110742f7e29fb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; 의&lt;/a&gt; 객체가 수행하는 상향식 접근 방식을 사용하여 계층 적 클러스터링 : 자신의 클러스터의 각 관측을 시작하고, 클러스터는 연속적으로 함께 병합됩니다. 연결 기준에 따라 병합 전략에 사용되는 메트릭이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="913b5a9805377fabb258d2653b5b70e8adeffb2c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt;&lt;code&gt;SpectralBiclustering&lt;/code&gt;&lt;/a&gt; algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt; &lt;code&gt;SpectralBiclustering&lt;/code&gt; 의&lt;/a&gt; 알고리즘이 입력 데이터 행렬은 숨겨진 바둑판 구조를 갖는 것으로 가정한다. 이러한 구조를 갖는 매트릭스의 행 및 열은 분할되어 행 클러스터 및 열 클러스터의 데카르트 곱에서 임의의 bicluster의 엔트리가 대략 일정하도록 할 수있다. 예를 들어, 2 개의 행 파티션과 3 개의 열 파티션이있는 경우 각 행은 3 개의 biclusters에 속하고 각 열은 2 개의 biclusters에 속합니다.</target>
        </trans-unit>
        <trans-unit id="96812a842015efa920168397038c120e6e957561" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt;&lt;code&gt;SpectralCoclustering&lt;/code&gt;&lt;/a&gt; algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt; &lt;code&gt;SpectralCoclustering&lt;/code&gt; 의&lt;/a&gt; 알고리즘은 높은 대응 다른 행과 열의 값보다 biclusters를 찾는다. 각 행과 각 열은 정확히 하나의 bicluster에 속하므로 행과 열을 다시 정렬하여 파티션을 연속적으로 만들면 대각선을 따라 이러한 높은 값이 나타납니다.</target>
        </trans-unit>
        <trans-unit id="9debcd56df8be7e32ea091b79dc8e313d63ea1d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt;&lt;code&gt;Birch&lt;/code&gt;&lt;/a&gt; builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt; &lt;code&gt;Birch&lt;/code&gt; &lt;/a&gt; 주어진 데이터에 대한 특징 트리 (CFT)라는 나무를 구축합니다. 데이터는 본질적으로 일련의 특성 피쳐 노드 (CF 노드)로 손실 압축됩니다. CF 노드에는 특성 기능 서브 클러스터 (CF 서브 클러스터)라고하는 많은 서브 클러스터가 있으며 비 터미널 CF 노드에있는 이러한 CF 서브 클러스터는 CF 노드를 자식으로 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5792059d2ce9d3e99380df43abdafaacd52cf188" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt;&lt;code&gt;Birch&lt;/code&gt;&lt;/a&gt; builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8b115edebda7f7bf86445503d0ce08900b4f8de" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of &lt;em&gt;core samples&lt;/em&gt;, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, &lt;code&gt;min_samples&lt;/code&gt; and &lt;code&gt;eps&lt;/code&gt;, which define formally what we mean when we say &lt;em&gt;dense&lt;/em&gt;. Higher &lt;code&gt;min_samples&lt;/code&gt; or lower &lt;code&gt;eps&lt;/code&gt; indicate higher density necessary to form a cluster.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt; 고밀도 영역이 저밀도 영역으로 구분로서 알고리즘은 클러스터를 플레이. 이 다소 일반적인 관점으로 인해 DBSCAN에서 찾은 클러스터는 k-means와 달리 클러스터가 볼록한 모양이라고 가정하는 것과는 달리 어떤 모양이든 될 수 있습니다. DBSCAN의 핵심 구성 요소는 &lt;em&gt;핵심 샘플&lt;/em&gt; 개념이며 , 이는 밀도가 높은 영역에있는 샘플입니다. 따라서 클러스터는 서로 가까이있는 (일부 거리 측정으로 측정 된) 코어 샘플 세트와 코어 샘플에 가깝지만 코어 샘플이 아닌 비 코어 샘플 세트입니다. 알고리즘, 두 개의 매개 변수가 &lt;code&gt;min_samples&lt;/code&gt; 및 &lt;code&gt;eps&lt;/code&gt; 공식적으로 정의, 우리가 말할 때 우리가 무엇을 의미하는지에 관해 이하 &lt;code&gt;eps&lt;/code&gt; &lt;em&gt; 밀도&lt;/em&gt; 합니다. 더 높은 &lt;code&gt;min_samples&lt;/code&gt; 는 클러스터를 형성하는 데 필요한 높은 밀도를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="844222980d29de5ed47698200091f13bdd09a284" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt;&lt;code&gt;FeatureAgglomeration&lt;/code&gt;&lt;/a&gt; uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;Unsupervised dimensionality reduction&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt; &lt;code&gt;FeatureAgglomeration&lt;/code&gt; 은&lt;/a&gt; 함께 그룹으로 응집성 클러스터링을 사용하여 기능의 수를 감소, 매우 비슷한 표정을 갖추고 있습니다. 차원 축소 도구 입니다. &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;감독되지 않은 차원 축소를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3ccd68ae912b5d8e7b609345a756782aaea1f1b3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the &lt;a href=&quot;inertia&quot;&gt;inertia&lt;/a&gt; or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; 은&lt;/a&gt; 으로 알려진 기준 최소화 등분 n 개의 그룹으로 분리 된 샘플을 시도하여 클러스터 데이터 산법 &lt;a href=&quot;inertia&quot;&gt;관성&lt;/a&gt; 또는 클러스터 내 제곱합한다. 이 알고리즘을 사용하려면 클러스터 수를 지정해야합니다. 그것은 많은 수의 샘플로 잘 확장되며 다양한 분야의 광범위한 응용 분야에서 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="1ffab773fe637da04ea4c04490bc4a37e92e75f6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the &lt;em&gt;inertia&lt;/em&gt; or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8f9ba49e304c2e7e84cbf4122c9838a65e0d463" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt; is a variant of the &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 은&lt;/a&gt; 의 변종이다 &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt; 여전히 동일한 목적 함수를 최적화하는 동안, 계산 시간을 줄이기 위해 미니 일괄 알고리즘을 사용. 미니-배치는 입력 데이터의 하위 집합으로, 각 트레이닝 반복에서 무작위로 샘플링됩니다. 이러한 미니 배치는 로컬 솔루션으로 수렴하는 데 필요한 계산량을 크게 줄입니다. k- 평균의 수렴 시간을 줄이는 다른 알고리즘과 달리 미니 배치 k- 평균은 일반적으로 표준 알고리즘보다 약간 더 나쁜 결과를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="13f492ae552c222beb8a78a6fea9613c4b7f22e2" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt;&lt;code&gt;OPTICS&lt;/code&gt;&lt;/a&gt; algorithm shares many similarities with the &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; algorithm, and can be considered a generalization of DBSCAN that relaxes the &lt;code&gt;eps&lt;/code&gt; requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a &lt;em&gt;reachability&lt;/em&gt; graph, which assigns each sample both a &lt;code&gt;reachability_&lt;/code&gt; distance, and a spot within the cluster &lt;code&gt;ordering_&lt;/code&gt; attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of &lt;em&gt;inf&lt;/em&gt; set for &lt;code&gt;max_eps&lt;/code&gt;, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given &lt;code&gt;eps&lt;/code&gt; value using the &lt;code&gt;cluster_optics_dbscan&lt;/code&gt; method. Setting &lt;code&gt;max_eps&lt;/code&gt; to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="381def8c4d001638003d40e7acf9264b0a49ea0f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; helps performing different transformations for different columns of the data, within a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; that is safe from data leakage and that can be parametrized. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; works on arrays, sparse matrices, and &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; 는&lt;/a&gt; 내에서 데이터의 다른 열의 다른 변환을 수행하는 데 도움이 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 데이터 유출로부터 안전하게되어 있고 그 매개 변수화 될 수있다. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; &lt;/a&gt; 는 배열, 희소 행렬 및 &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;팬더 DataFrames에서 작동&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="d5b3cf1eea0426995e81b4c182953586d3dd6af5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; helps performing different transformations for different columns of the data, within a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; that is safe from data leakage and that can be parametrized. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; works on arrays, sparse matrices, and &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37d727244bb97826f98eb0365b95bf6cd4afb239" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; class is experimental and the API is subject to change.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;compose.ColumnTransformer&lt;/code&gt; 의&lt;/a&gt; 클래스는 실험이며, API는 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36f50b6d2de06293da3e162b3eb7e342568ccd05" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.make_column_transformer#sklearn.compose.make_column_transformer&quot;&gt;&lt;code&gt;make_column_transformer&lt;/code&gt;&lt;/a&gt; function is available to more easily create a &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; object. Specifically, the names will be given automatically. The equivalent for the above example would be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83e5137b54932bec66ccc36542bace6834598b69" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt;&lt;code&gt;GraphicalLasso&lt;/code&gt;&lt;/a&gt; estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its &lt;code&gt;alpha&lt;/code&gt; parameter, the more sparse the precision matrix. The corresponding &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt;&lt;code&gt;GraphicalLassoCV&lt;/code&gt;&lt;/a&gt; object uses cross-validation to automatically set the &lt;code&gt;alpha&lt;/code&gt; parameter.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt; &lt;code&gt;GraphicalLasso&lt;/code&gt; 의&lt;/a&gt; 추정기 정밀도 행렬 희소성을 적용하는 L1 페널티를 사용하여 상위의 &lt;code&gt;alpha&lt;/code&gt; 매개 더 희박한 정확도 행렬. 해당 &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt; &lt;code&gt;GraphicalLassoCV&lt;/code&gt; &lt;/a&gt; 객체는 교차 검증을 사용하여 &lt;code&gt;alpha&lt;/code&gt; 매개 변수 를 자동으로 설정합니다 .</target>
        </trans-unit>
        <trans-unit id="4d547fe6e0c2b8c31056c1efceecbd9d17b8cbf1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt; method that can be used in cross-validation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9447390bf3cfd368da76e6282f132428b32dfff8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a &lt;code&gt;score&lt;/code&gt; method that can be used in cross-validation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 의&lt;/a&gt; 객체는 또한 설명 분산의 양에 따라 데이터의 가능성을 줄 수있는 PCA의 확률 적 해석을 제공합니다. 따라서 교차 유효성 검사에 사용할 수 있는 &lt;code&gt;score&lt;/code&gt; 방법을 구현합니다 .</target>
        </trans-unit>
        <trans-unit id="de1125bcd2177e15b5b35e281621b5bbf18681e1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object is very useful, but has certain limitations for large datasets. The biggest limitation is that &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; only supports batch processing, which means all of the data to be processed must fit in main memory. The &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; object uses a different form of processing and allows for partial computations which almost exactly match the results of &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; while processing the data in a minibatch fashion. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; makes it possible to implement out-of-core Principal Component Analysis either by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 의&lt;/a&gt; 객체는 매우 유용하지만, 대규모 데이터 세트에 대한 특정 제한이 있습니다. 가장 큰 제한은 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 가 일괄 처리 만 지원한다는 것입니다. 즉, 처리 할 모든 데이터가 기본 메모리에 맞아야합니다. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; 의&lt;/a&gt; 목적은 처리의 다른 형태를 사용하는 거의 정확한 결과와 일치하는 부분의 계산을 허용 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 를&lt;/a&gt; minibatch 방식으로 데이터를 처리하는 동안. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; 를&lt;/a&gt; 사용하면 다음 중 하나를 통해 코어 외부 주요 구성 요소 분석을 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ecd6b33d3bd2199aadcf263cff0e5246cde4bd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;SparseCoder&lt;/code&gt;&lt;/a&gt; object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a &lt;code&gt;fit&lt;/code&gt; method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the &lt;code&gt;transform_method&lt;/code&gt; initialization parameter:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;SparseCoder&lt;/code&gt; 의&lt;/a&gt; 목적은 고정 된 발 원자 희소 선형 조합으로 신호를 변환하기 위해 사용될 수있는 추정 장치이며, 이산 웨이블릿 기반으로 사전 등을 미리 계산. 따라서이 객체는 &lt;code&gt;fit&lt;/code&gt; 메소드를 구현하지 않습니다 . 변환은 희소 한 코딩 문제에 해당한다 : 가능한 적은 수의 사전 원자의 선형 조합으로서 데이터의 표현을 찾는 것. 사전 학습의 모든 변형은 다음의 변환 메소드를 구현하며, &lt;code&gt;transform_method&lt;/code&gt; 초기화 매개 변수 를 통해 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e3da78a60195e1f6e094f14916e2b477c6f8356b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; supports &lt;code&gt;warm_start=True&lt;/code&gt; which allows you to add more trees to an already fitted model:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="541c2e4d790ae9e327bfbba2dc5bd507a6c1e503" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt; provide such strategies which can be applied to classification and regression problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8482134b0c48ad8bcdfc22624a585a7b569b1ee" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; can also be used together with &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; in order to tune the hyperparameters of the individual estimators:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51b760d25143490b212d3dce7b63a475beffe57d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt; function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt;&lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt;&lt;/a&gt;. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt; 이미지로부터 추출 기능 패치는 2 차원 어레이로서 저장하거나, 제 축을 따라 색 정보와 입체. 모든 패치에서 이미지를 다시 작성하려면 &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt; &lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 . 예를 들어 3 가지 색상 채널 (예 : RGB 형식)로 4x4 픽셀 사진을 생성 해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="13de12da96c62cdbe967814b1ded04e8c85eef13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt;&lt;code&gt;PatchExtractor&lt;/code&gt;&lt;/a&gt; class works in the same way as &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt;, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt; &lt;code&gt;PatchExtractor&lt;/code&gt; 의&lt;/a&gt; 와 동일한 방식으로 클래스 작품 &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; 은&lt;/a&gt; , 만 입력으로 여러 이미지를 지원합니다. 추정기로 구현되므로 파이프 라인에서 사용할 수 있습니다. 보다:</target>
        </trans-unit>
        <trans-unit id="5f1d9b11617dc21530d4a8e947334af50d14c144" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; also comes with the following limitations:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 은&lt;/a&gt; 또한 다음과 같은 제한 사항이 포함되어 있습니다 :</target>
        </trans-unit>
        <trans-unit id="dec1e79879a530cf5d8d2ea5bf189d8118bb1961" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \(f\), which is then squashed through a link function to obtain the probabilistic classification. The latent function \(f\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \(f\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; 의&lt;/a&gt; 구체적 테스트 클래스 예측 확률의 형태를 취할 확률적인 분류를위한 분류 목적을 위해 구현 가우시안 프로세스 (GP). GaussianProcessClassifier는 잠재 함수 \ (f \)보다 먼저 GP를 배치 한 다음 링크 함수를 통해 스쿼드되어 확률 적 분류를 얻습니다. 잠재 함수 \ (f \)는 소위 방해 함수로, 그 값은 관찰되지 않고 자체적으로 관련이 없습니다. 그것의 목적은 모델의 편리한 공식화를 허용하고, 예측 동안 \ (f \)가 제거 (통합)된다. GaussianProcessClassifier는 적분을 분석적으로 계산할 수 없지만 이진 경우에 쉽게 근사되는 로지스틱 링크 함수를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="2a70b80f4163a2c6bfd08f3c8b84b458a9627cc7" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt;&lt;code&gt;GaussianProcessRegressor&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for &lt;code&gt;normalize_y=False&lt;/code&gt;) or the training data&amp;rsquo;s mean (for &lt;code&gt;normalize_y=True&lt;/code&gt;). The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt; &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; 의&lt;/a&gt; 회귀 목적으로 구현 가우시안 프로세스 (GP). 이를 위해서는 GP의 이전을 지정해야합니다. 이전 평균은 상수 및 0 ( &lt;code&gt;normalize_y=False&lt;/code&gt; 의 경우 ) 또는 훈련 데이터의 평균 ( &lt;code&gt;normalize_y=True&lt;/code&gt; 의 경우 )으로 가정합니다. 사전의 공분산은 &lt;a href=&quot;#gp-kernels&quot;&gt;커널&lt;/a&gt; 객체를 전달하여 지정됩니다 . 커널의 하이퍼 파라미터는 전달 &lt;code&gt;optimizer&lt;/code&gt; 기반으로 LLM (log-marginal-likelihood)을 최대화하여 GaussianProcessRegressor를 피팅하는 동안 최적화 됩니다. LML에 여러 개의 로컬 옵티마가있을 수 있으므로 다음을 지정하여 옵티 마이저를 반복적으로 시작할 수 있습니다. &lt;code&gt;n_restarts_optimizer&lt;/code&gt; . 첫 번째 실행은 항상 커널의 초기 하이퍼 파라미터 값에서 시작하여 수행됩니다. 후속 실행은 허용 된 값의 범위에서 무작위로 선택된 하이퍼 파라미터 값에서 수행됩니다. 초기 하이퍼 파라미터를 고정 상태로 유지해야하는 경우 최적화 프로그램으로 &lt;code&gt;None&lt;/code&gt; 을 전달할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="cef90bbcd0a36066d60f0a9fa46fd7add704859a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt;&lt;code&gt;GaussianProcessRegressor&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for &lt;code&gt;normalize_y=False&lt;/code&gt;) or the training data&amp;rsquo;s mean (for &lt;code&gt;normalize_y=True&lt;/code&gt;). The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a013933edad2184a36ef1d15fcbf21a794c0c7f5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt;&lt;code&gt;ConstantKernel&lt;/code&gt;&lt;/a&gt; kernel can be used as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel where it scales the magnitude of the other factor (kernel) or as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \(constant\_value\). It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt; &lt;code&gt;ConstantKernel&lt;/code&gt; 의&lt;/a&gt; 커널은의 일부로 사용할 수있는 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt; 는 다른 요소 (커널)의 크기 또는의 한 부분으로 확장 커널 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; 는 가우시안 프로세스의 평균 수정 커널. 매개 변수 \ (constant \ _value \)에 따라 다릅니다. 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a1a78e3b1d5985ce1973c8989ff0075d7d078b79" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt; 커널은 일반적으로 지수와 결합된다. 지수가 2 인 예는 다음 그림에 표시되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="299194d50f816028e01666a91e5aaf031d88d5c0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; 의&lt;/a&gt; 커널 \ (N (0, 1) \)의 계수에 전과 \ (x_d (d = 1..., D) \) 비 정지하고 바꾸어 선형 회귀로부터 획득 될 수 있고, 종래 바이어스에서 \ (N (0, \ sigma_0 ^ 2) \) &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; 의&lt;/a&gt; 커널은 기원에 대한 좌표가 아니라 번역의 회전에 불변이다. 매개 변수는 \ (\ sigma_0 ^ 2 \)에 의해 매개 변수화됩니다. \ (\ sigma_0 ^ 2 = 0 \)의 경우 커널을 동종 선형 커널이라고하며, 그렇지 않으면 동종이 아닙니다. 커널은</target>
        </trans-unit>
        <trans-unit id="9b254fd92e2584b8ca8c7f30ca756b0bac24ec2b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt;&lt;code&gt;ExpSineSquared&lt;/code&gt;&lt;/a&gt; kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt; &lt;code&gt;ExpSineSquared&lt;/code&gt; &lt;/a&gt; 커널은주기 함수 모델링을 허용한다. 길이 스케일 매개 변수 \ (l&amp;gt; 0 \) 및 주기성 매개 변수 \ (p&amp;gt; 0 \)에 의해 매개 변수화됩니다. 현재 \ (l \)가 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="4fa9c3925ee31fe17ddb7d4f95d9aa563f446e7b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt;&lt;code&gt;Matern&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel and a generalization of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt; &lt;code&gt;Matern&lt;/code&gt; &lt;/a&gt; 커널은 고정 커널과의 일반화 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 커널. 결과 함수의 부드러움을 제어하는 ​​추가 매개 변수 \ (\ nu \)가 있습니다. 스칼라 (커널의 등방성 변형) 또는 입력 값과 동일한 수의 벡터 (이방성 변형) 일 수있는 길이 스케일 매개 변수 ({l&amp;gt; 0 \)로 매개 변수화됩니다. 커널). 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="9cc391d0a3f24c35e3e834b704611ffc08dfc23c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel can be seen as a scale mixture (an infinite sum) of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\) Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt; 커널의 규모 혼합물 (무한 합)으로 볼 수 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 다양한 특성 길이 비늘 커널. 길이 스케일 모수 \ (l&amp;gt; 0 \) 및 스케일 혼합 모수 \ (\ alpha&amp;gt; 0 \)에 의해 모수화됩니다. \ (l \)가 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="a0d4e8e5df8534d7f797dec945fa5951797b46d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 커널은 고정 된 커널입니다. &quot;제곱 지수&quot;커널이라고도합니다. 스칼라 (커널의 등방성 변형) 또는 입력 값과 동일한 수의 벡터 (이방성 변형) 일 수있는 길이 스케일 매개 변수 ({l&amp;gt; 0 \)로 매개 변수화됩니다. 커널). 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="82f3fd9dd57bf0bc3cf4c6bc458ae2db6dc1ab71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.knnimputer#sklearn.impute.KNNImputer&quot;&gt;&lt;code&gt;KNNImputer&lt;/code&gt;&lt;/a&gt; class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, &lt;code&gt;nan_euclidean_distances&lt;/code&gt;, is used to find the nearest neighbors. Each missing feature is imputed using values from &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than &lt;code&gt;n_neighbors&lt;/code&gt; and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during &lt;code&gt;transform&lt;/code&gt;. For more information on the methodology, see ref. &lt;a href=&quot;#ol2001&quot; id=&quot;id5&quot;&gt;[OL2001]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f4b1aa7c1e397df865fdc8d1fd65546b5eaaf2f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;MissingIndicator&lt;/code&gt; &lt;/a&gt; 유용 변환기는 데이터 세트에서 누락 값의 존재를 나타내는 이진 행렬에 대응하는 데이터 세트를 변환한다. 이 변환은 대치와 함께 유용합니다. 대치 사용시 누락 된 값에 대한 정보를 보존하는 것이 유익 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a7157e982e0dbd339518050ff3330356266a7d9d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. Note that both the &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; have the boolean parameter &lt;code&gt;add_indicator&lt;/code&gt; (&lt;code&gt;False&lt;/code&gt; by default) which when set to &lt;code&gt;True&lt;/code&gt; provides a convenient way of stacking the output of the &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer with the output of the imputer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0c9736c8ad276e3deabee46eb181026e0204e8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports categorical data represented as string values or pandas categoricals when using the &lt;code&gt;'most_frequent'&lt;/code&gt; or &lt;code&gt;'constant'&lt;/code&gt; strategy:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 사용하는 경우 문자열 값 또는 팬더 categoricals로 표현 범주 형 데이터 지원 &lt;code&gt;'most_frequent'&lt;/code&gt; 또는 &lt;code&gt;'constant'&lt;/code&gt; 전략을 :</target>
        </trans-unit>
        <trans-unit id="4d17103c250c5ab6ac126fd9a857f81de53fed6f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports sparse matrices:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 스파 스 매트릭스를 지원합니다 :</target>
        </trans-unit>
        <trans-unit id="618df5d6360d655fcf582933900e1cb3bcf02379" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 누락 된 값을 전가하기위한 기본 전략을 제공합니다. 결 측값은 제공된 상수 값으로 결측되거나 결 측값이있는 각 열의 통계 (평균, 중간 또는 가장 빈번한)를 사용하여 대치 될 수 있습니다. 이 클래스는 다른 결 측값 인코딩도 허용합니다.</target>
        </trans-unit>
        <trans-unit id="68b4ceed0ca0d6a56ff0860f4ead39fbe0f8312e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt; function calculates the feature importance of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-estimators&quot;&gt;estimators&lt;/a&gt; for a given dataset. The &lt;code&gt;n_repeats&lt;/code&gt; parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f59968771d1dbbd03a744853045d3c0b7aa414b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; constructs an approximate mapping for the radial basis function kernel, also known as &lt;em&gt;Random Kitchen Sinks&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007]&lt;/a&gt;. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; 은&lt;/a&gt; 라고도 방사형 기저 함수 커널 대략 매핑 구축해 &lt;em&gt;랜덤 부엌 싱크 &lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007를]&lt;/a&gt; . 이 변환은 선형 알고리즘 (예 : 선형 SVM)을 적용하기 전에 커널 맵을 명시 적으로 모델링하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="44948166b7a6695399209dd8e1e1f1af0b0058e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; differs from using &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with loss set to &lt;code&gt;huber&lt;/code&gt; in the following ways.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 의&lt;/a&gt; 사용과 다른 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; 을&lt;/a&gt; 분실 세트 &lt;code&gt;huber&lt;/code&gt; 다음과 같은 방법으로한다.</target>
        </trans-unit>
        <trans-unit id="7ab9e90f2b3f808e98761c90cab46994be6820bb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; is different to &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt;&lt;code&gt;RANSACRegressor&lt;/code&gt;&lt;/a&gt; because it does not ignore the effect of the outliers but gives a lesser weight to them.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 가&lt;/a&gt; 다릅니다 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 가 이상치로 분류된다 샘플 선형 손실을 적용하기 때문이다. 샘플의 절대 오차가 특정 임계 값보다 작은 경우 샘플은 inlier로 분류됩니다. &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt; &lt;code&gt;RANSACRegressor&lt;/code&gt; &lt;/a&gt; 는 특이 치의 영향을 무시하지는 않지만 가중치는 적기 때문에 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="6c9667130f9758bb96a7c943daf0a6616eced1f9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;Compressive sensing: tomography reconstruction with L1 prior (Lasso)&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70ef4a25a40856b26fd987533d68c36540345c20" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights (see &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;Compressive sensing: tomography reconstruction with L1 prior (Lasso)&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 스파 스 계수를 추정하는 선형 모델이다. 매개 변수 값이 적은 솔루션을 선호하는 경향이 있기 때문에 특정 상황에서 유용하며, 주어진 솔루션이 의존하는 변수의 수를 효과적으로 줄입니다. 이러한 이유로 Lasso와 그 변형은 압축 감지 분야의 기본입니다. 특정 조건 하에서 정확한 0이 아닌 가중치 세트를 복구 할 수 있습니다 ( &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;압축 감지 : L1 이전의 단층 촬영 재구성 (올가미) 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="eec44edb57da642e68d30eaaa1191346c0a6b209" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;Y&lt;/code&gt; is a 2D array of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c1095adf7bd87312f73373efdee9c54e778b1be" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;Y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; 는&lt;/a&gt; 공동 회귀 문제 성긴 계수를 추정하는 탄성 네트 모델이다 : &lt;code&gt;Y&lt;/code&gt; 는 2 차원 어레이의 형태이다 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; . 제약 조건은 선택한 기능이 작업이라고도하는 모든 회귀 문제에 대해 동일하다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="0855f24dbbabd45aa8775e800911f6fb3f3411c3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; 는&lt;/a&gt; 공동 회귀 문제 성긴 계수를 추정하는 선형 모델이다 : &lt;code&gt;y&lt;/code&gt; 2 차원 어레이의 형태이다 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; . 제약 조건은 선택한 기능이 작업이라고도하는 모든 회귀 문제에 대해 동일하다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="d927ce91bac1b6df649580c487bdf34ce5f21e13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt;&lt;code&gt;Perceptron&lt;/code&gt;&lt;/a&gt; is another simple classification algorithm suitable for large scale learning. By default:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt; &lt;code&gt;Perceptron&lt;/code&gt; &lt;/a&gt; 대규모 학습에 적합한 또 다른 간단한 분류 알고리즘이다. 기본적으로:</target>
        </trans-unit>
        <trans-unit id="114bd6c0908df3918d68db503a6c5b185beb59c6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; regressor has a classifier variant: &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt;. This classifier first converts binary targets to &lt;code&gt;{-1, 1}&lt;/code&gt; and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressor&amp;rsquo;s prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97c2977337c286541956e3848f1e0d89e23d036d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt; can be significantly faster than e.g. &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with a high number of classes, because it is able to compute the projection matrix \((X^T X)^{-1} X^T\) only once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc6941408ce5829c04ee30dacb5eec313120d42e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It looses its robustness properties and becomes no better than an ordinary least squares in high dimension.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; 의&lt;/a&gt; 추정기는 다중 차원의 중간의 일반화를 이용한다. 따라서 다변량 이상치에 강합니다. 그러나 추정의 견고성은 문제의 차원에 따라 빠르게 감소합니다. 견고성이 떨어지고 높은 치수의 보통 최소 제곱보다 낫지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6c80a0d39e7d7595f1867b8a38b3ed5fa33131bb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="497dd0db8e84137ac4b140cff3317a3423c09e22" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;accuracy&lt;/a&gt;, either the fraction (default) or the count (normalize=False) of correct predictions.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; 의&lt;/a&gt; 함수 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;정밀도&lt;/a&gt; , 분획 (기본) 또는 정확한 예측 계수 (표준화 = 거짓)를 하나.</target>
        </trans-unit>
        <trans-unit id="734b9a83298cb0e5bb404a0eb951bb89a38c30c1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;average precision&lt;/a&gt; (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; 기능은 계산 &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;평균 정밀도&lt;/a&gt; 예측 점수에서 (AP)를. 값은 0과 1 사이이며 높을수록 좋습니다. AP는 다음과 같이 정의됩니다</target>
        </trans-unit>
        <trans-unit id="e82eb3ff042c4b3a5e1b920dfccc7718db2306b1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;average precision&lt;/a&gt; (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c1e547613cb8b25282a0a1d2e2d0fa8b86fab4a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt;&lt;code&gt;balanced_accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;balanced accuracy&lt;/a&gt;, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt; &lt;code&gt;balanced_accuracy_score&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;균형 정확성&lt;/a&gt; 불균형 데이터 세트에 비정상적으로 실적 추정을 피한다. 이는 클래스 당 리콜 점수의 거시 평균 또는 각 클래스가 실제 클래스의 역 유병률에 따라 가중치를 적용하는 원시 정확도입니다. 따라서 균형 잡힌 데이터 세트의 경우 점수는 정확도와 같습니다.</target>
        </trans-unit>
        <trans-unit id="6c29a08df4ccee7316d3d3b84f8a1be99122d005" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;Brier score&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;찔레 점수&lt;/a&gt; 바이너리 클래스를. 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="446d2ff4c24d7bf4bbe8a92416191f1e2b3de72b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;sklearn.metrics.brier_score_loss&lt;/code&gt;&lt;/a&gt; may be used to evaluate how well a classifier is calibrated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80ef899573ebb3be6112621f7df5aadf4a0d99d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt;&lt;code&gt;classification_report&lt;/code&gt;&lt;/a&gt; function builds a text report showing the main classification metrics. Here is a small example with custom &lt;code&gt;target_names&lt;/code&gt; and inferred labels:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt; &lt;code&gt;classification_report&lt;/code&gt; 의&lt;/a&gt; 기능은 주요 분류 통계를 보여주는 텍스트 보고서를 작성합니다. 다음은 맞춤 &lt;code&gt;target_names&lt;/code&gt; 및 유추 된 레이블 이 포함 된 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="297cbd08a7b9d6cdee2db0fac772b51c60ecb158" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt; function evaluates classification accuracy by computing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;confusion matrix&lt;/a&gt; with each row corresponding to the true class (Wikipedia and other references may use different convention for axes).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="646495d784f725b3203da7b1895753c47a46c957" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt; function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class &amp;lt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt;&amp;gt;`_. (Wikipedia and other references may use different convention for axes.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; 의&lt;/a&gt; 기능은 각 행 &amp;lt;실제 클래스에 대응 혼란 매트릭스 연산에 의한 분류의 정확도를 평가 &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt; &amp;gt;`_한다. (Wikipedia 및 기타 참조는 축에 대해 다른 규칙을 사용할 수 있습니다.)</target>
        </trans-unit>
        <trans-unit id="627c762ce2611d603b6cf9dd93706bacfe9a64ab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt;&lt;code&gt;coverage_error&lt;/code&gt;&lt;/a&gt; function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt; &lt;code&gt;coverage_error&lt;/code&gt; 의&lt;/a&gt; 기능은 모든 사실 레이블이 예측되도록 최종 예측에 포함되어야하는 라벨의 평균 수를 계산합니다. 이것은 실제 레이블을 잃지 않고 평균적으로 얼마나 많은 최상위 레이블을 예측해야하는지 알고 싶을 때 유용합니다. 따라서이 측정 항목의 최상의 가치는 평균 실제 라벨 수입니다.</target>
        </trans-unit>
        <trans-unit id="365c5eb64f0dc2e33be205a551210f568e81546d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;explained variance regression score&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; 는&lt;/a&gt; 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;설명 분산 회귀 점수를&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="16accfb21d784810c328541c85b1894b818cde8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt;&lt;code&gt;hamming_loss&lt;/code&gt;&lt;/a&gt; computes the average Hamming loss or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;Hamming distance&lt;/a&gt; between two sets of samples.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt; &lt;code&gt;hamming_loss&lt;/code&gt; 는&lt;/a&gt; 평균 해밍 손실 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;해밍 거리가&lt;/a&gt; 샘플들의 두 세트 사이에있다.</target>
        </trans-unit>
        <trans-unit id="6d1238c9791f472ba1850e50c0898d87bebfa2d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function computes the average distance between the model and the data using &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;hinge loss&lt;/a&gt;, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수는 모델 및 사용 데이터 사이의 평균 거리 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;힌지 손실&lt;/a&gt; 하는 경우에만 예측 에러를 고려한 해당 측정 한 양면. 힌지 손실은 서포트 벡터 머신과 같은 최대 마진 분류기에서 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8897c326f42ba94a97047f2763d3dfdfd57b8bb8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.jaccard_score#sklearn.metrics.jaccard_score&quot;&gt;&lt;code&gt;jaccard_score&lt;/code&gt;&lt;/a&gt; function computes the average of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity coefficients&lt;/a&gt;, also called the Jaccard index, between pairs of label sets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29930da8eb2c0b1ff7129cc1cbfbb0416883031e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt;&lt;code&gt;jaccard_similarity_score&lt;/code&gt;&lt;/a&gt; function computes the average (default) or sum of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity coefficients&lt;/a&gt;, also called the Jaccard index, between pairs of label sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt; &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 의&lt;/a&gt; 함수의 평균 (기본) 또는 합 연산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;인 Jaccard 유사도 계수&lt;/a&gt; 라벨 세트들의 쌍들 사이에는 인 Jaccard 인덱스라고 불리는,.</target>
        </trans-unit>
        <trans-unit id="cbf6f35f94b93c090ec2e48a35d245f91dcfca65" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt;&lt;code&gt;label_ranking_average_precision_score&lt;/code&gt;&lt;/a&gt; function implements label ranking average precision (LRAP). This metric is linked to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function, but is based on the notion of label ranking instead of precision and recall.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt; &lt;code&gt;label_ranking_average_precision_score&lt;/code&gt; 의&lt;/a&gt; 기능 구현은 순위 평균 정밀도 (LRAP를) 레이블을 붙입니다. 이 메트릭은 &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; 함수에 연결되어 있지만 정밀도 및 호출 대신 레이블 순위의 개념을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="79620a85c10a9be19922ad33cc7395bed79c9e4e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt;&lt;code&gt;label_ranking_loss&lt;/code&gt;&lt;/a&gt; function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt; &lt;code&gt;label_ranking_loss&lt;/code&gt; &lt;/a&gt; 기능은 거짓과 진실 라벨의 순서쌍의 숫자의 역으로 가중 샘플에 걸쳐 평균, 즉 잘못 정렬 레이블 쌍의 수, 진정한 라벨이 거짓 라벨보다 낮은 점수가 순위 손실을 계산합니다. 달성 가능한 최저 순위 손실은 0입니다.</target>
        </trans-unit>
        <trans-unit id="4b0810d3cef1ca02b990e21053d774c24a0e430b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt;&lt;code&gt;log_loss&lt;/code&gt;&lt;/a&gt; function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator&amp;rsquo;s &lt;code&gt;predict_proba&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt; &lt;code&gt;log_loss&lt;/code&gt; 의&lt;/a&gt; 함수로 계산은의 추정기에 의해 반환 손실은 지상 진실 라벨 확률 매트릭스의리스트를 소정 로그온 &lt;code&gt;predict_proba&lt;/code&gt; 의 방법.</target>
        </trans-unit>
        <trans-unit id="3661b0b19cd7cbd747b2bf1ce7b4a383102a7abe" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;Matthew&amp;rsquo;s correlation coefficient (MCC)&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; 의&lt;/a&gt; 함수 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;매튜의 상관 계수 (MCC)를&lt;/a&gt; 이진 클래스. 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="61afc8dcca66cd062b78b06b9f6f1f6a381f8368" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt;&lt;code&gt;max_error&lt;/code&gt;&lt;/a&gt; does not support multioutput.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d67557cc032f22cb8c3e56bc9812e04ade8f8ad" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt;&lt;code&gt;max_error&lt;/code&gt;&lt;/a&gt; function computes the maximum &lt;a href=&quot;https://en.wikipedia.org/wiki/Errors_and_residuals&quot;&gt;residual error&lt;/a&gt; , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, &lt;code&gt;max_error&lt;/code&gt; would be &lt;code&gt;0&lt;/code&gt; on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8af6e2db7d7843522a3d6755e0b8d1e9cbd1e8f1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;mean absolute error&lt;/a&gt;, a risk metric corresponding to the expected value of the absolute error loss or \(l1\)-norm loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;평균 절대 오차&lt;/a&gt; 규범 손실 - 절대 에러 또는 손실 \ (L1 \)의 기대 값에 대응하는 메트릭 위험.</target>
        </trans-unit>
        <trans-unit id="798074cb4600d0906a9ad4975942309f6067f034" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean square error&lt;/a&gt;, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;평균 제곱 에러&lt;/a&gt; 의 제곱 (이차) 에러 또는 손실의 기대 값에 대응하는 메트릭 위험.</target>
        </trans-unit>
        <trans-unit id="e4bbccf6d12a691e935e91e2611be809f1bb4329" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; 의&lt;/a&gt; 함수 제곱 대수 (이차) 에러 또는 손실의 기대 값에 대응하는 위험 메트릭 계산한다.</target>
        </trans-unit>
        <trans-unit id="9206d6989b69732d0750b8f7b0f0cca35f16a502" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_tweedie_deviance#sklearn.metrics.mean_tweedie_deviance&quot;&gt;&lt;code&gt;mean_tweedie_deviance&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tweedie_distribution#The_Tweedie_deviance&quot;&gt;mean Tweedie deviance error&lt;/a&gt; with a &lt;code&gt;power&lt;/code&gt; parameter (\(p\)). This is a metric that elicits predicted expectation values of regression targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39f48c0bbd67ae6ad01f06368c176486b0da9e3b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; does not support multioutput.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; 는&lt;/a&gt; 다중 출력을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4c03eab2aa446025510f65f3a7e796a70c97a820" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; 는&lt;/a&gt; 이 이상치에 대한 강력한 있기 때문에 특히 흥미 롭다. 손실은 목표와 예측 사이의 모든 절대 차이의 중앙값을 취하여 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9e08c2b58ff3ad56580920cccb403426006e1ddc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function computes class-wise (default) or sample-wise (samplewise=True) multilabel confusion matrix to evaluate the accuracy of a classification. multilabel_confusion_matrix also treats multiclass data as if it were multilabel, as this is a transformation commonly applied to evaluate multiclass problems with binary classification metrics (such as precision, recall, etc.).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f5631acff2238ea5bcb79376fef6d2e143756a6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt;&lt;code&gt;precision_recall_curve&lt;/code&gt;&lt;/a&gt; computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt; &lt;code&gt;precision_recall_curve&lt;/code&gt; 는&lt;/a&gt; 지상 진실 라벨에서 정밀 리콜 곡선과 결정 임계 값을 변화시킴으로써 분류에 의해 주어진 점수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d16f1c84f45a895677960253a6c6c45cf8b671e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; accept an additional value &lt;code&gt;'variance_weighted'&lt;/code&gt; for the &lt;code&gt;multioutput&lt;/code&gt; parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; is the default value for &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; for backward compatibility. This will be changed to &lt;code&gt;uniform_average&lt;/code&gt; in the future.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; 은&lt;/a&gt; 추가로 값을 받아 &lt;code&gt;'variance_weighted'&lt;/code&gt; 에 대한 &lt;code&gt;multioutput&lt;/code&gt; 매개 변수를. 이 옵션을 사용하면 해당 대상 변수의 분산에 따라 각 개별 점수의 가중치가 적용됩니다. 이 설정은 전체적으로 캡처 된 비 스케일 분산을 정량화합니다. 목표 변수가 다른 척도 인 경우이 점수는 높은 분산 변수를 잘 설명하는 데 더 중요합니다. 이전 버전과의 호환성을 위해 &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 의 기본값은 multioutput &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; 입니다 . 앞으로는 &lt;code&gt;uniform_average&lt;/code&gt; 로 변경 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="093b69e0a2a3ccbb33c0abc5ad459d307bcf6553" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function computes R&amp;sup2;, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;coefficient of determination&lt;/a&gt;. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 의&lt;/a&gt; 함수 R&amp;sup2; 상기 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;판정 계수&lt;/a&gt; . 모델이 미래의 샘플을 얼마나 잘 예측할 수 있는지 측정합니다. 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="38084709322f1f775abaac153e1595951318cede" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;coefficient of determination&lt;/a&gt;, usually denoted as R&amp;sup2;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fdc4c3e65c0a2a9ffbf753fef92ef0300417867" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the predicted labels are provided in an array with values from 0 to &lt;code&gt;n_classes&lt;/code&gt;, and the scores correspond to the probability estimates that a sample belongs to a particular class. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and by prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cdf15299db25ef1f3b3628cdbe3b0de1b0d0ab3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;Wikipedia article on AUC&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; 의&lt;/a&gt; 기능은 AUC 또는 AUROC로 표시되는 수신기 동작 특성 (ROC) 곡선 아래의 면적을 계산한다. roc 곡선 아래의 면적을 계산함으로써, 곡선 정보는 하나의 숫자로 요약됩니다. 자세한 내용은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;AUC Wikipedia 기사를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="fbc2259a8dc85fa7ed24780d0f0e2ac678fbcad9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; function computes the sum or the average of the 0-1 classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By default, the function normalizes over the sample. To get the sum of the \(L_{0-1}\), set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt; 함수 합 또는 0-1 분류 손실 (\ (L_ {0-1} \)) 위에 \ (N _ {\ 텍스트 샘플 {}} \)의 평균을 계산한다. 기본적으로 함수는 샘플에서 정규화됩니다. \ (L_ {0-1} \)의 합계를 얻으려면 &lt;code&gt;normalize&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="0ce78ef531a28f4df4b9620cf7454901e00bf965" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 의&lt;/a&gt; 객체 구현 변분 추론 알고리즘 가우시안 혼합 모델의 변형. API는 &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; 에&lt;/a&gt; 의해 정의 된 것과 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="7755b185d3bd9567bc77a31c3d84e8be2c332f90" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; 는&lt;/a&gt; 차이 클래스의 공분산을 제한하는 다양한 옵션을 제공하는 것은 추정 : 구형 대각선, 연결 또는 전체 공분산을.</target>
        </trans-unit>
        <trans-unit id="9cb1ef419e28ff804b54eef9fc18747641996ac9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; object implements the &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt;&lt;code&gt;GaussianMixture.fit&lt;/code&gt;&lt;/a&gt; method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt;&lt;code&gt;GaussianMixture.predict&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt; 객체 는 가우스 혼합 모델에 적합하도록 EM ( &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; ) 알고리즘을 구현합니다 . 또한 다변량 모델에 대한 신뢰 타원체를 그리고 베이지안 정보 기준을 계산하여 데이터의 군집 수를 평가할 수 있습니다. &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt; &lt;code&gt;GaussianMixture.fit&lt;/code&gt; 에&lt;/a&gt; 있어서 열차 데이터 배운다 가우시안 혼합 모델에 제공된다. 테스트 데이터가 주어지면 &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt; &lt;code&gt;GaussianMixture.predict&lt;/code&gt; &lt;/a&gt; 메소드 를 사용하여 주로 속하는 가우스를 각 샘플에 할당 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0716de4b023c33cd11d454f2784cb63c7a79bbcc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; function differs from &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; in two ways:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0de55d1a8cabbad74e59064d298db364a4949211" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; instance implements the usual estimator API: when &amp;ldquo;fitting&amp;rdquo; it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 의&lt;/a&gt; 인스턴스를 구현 일반적인 추정의 API : 데이터 세트에 그것을 &quot;피팅&quot;모든 매개 변수 값의 가능한 조합을 평가하고 최적의 조합은 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="45a8a9b77f54ed1a1a49e1eebdf058e73c1ad33a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator behaves as a combination of &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt; 반복자의 조합으로서 동작 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt; 및 그룹들의 서브 세트가 각각의 분할에 대해 내밀하는 랜덤 분할의 시퀀스를 생성한다.</target>
        </trans-unit>
        <trans-unit id="ec15ef26f9a075815cd5139e68468c23802a4936" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 는&lt;/a&gt; 반복자는 독립적 인 기차 / 테스트 데이터 세트 분할의 사용자 정의 번호를 생성합니다. 샘플을 먼저 섞은 다음 기차와 테스트 세트로 나눕니다.</target>
        </trans-unit>
        <trans-unit id="9d19931407bdaf26c2091f0ff552c9705b61d84b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; (LOF) 알고리즘은 관찰의 이상 정도를 반영 (현지 특이 요인이라고 함) 점수를 계산합니다. 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 측정합니다. 아이디어는 이웃보다 밀도가 실질적으로 낮은 샘플을 탐지하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="7ee66eef276fb6deecd16d4b6508f3c8417f58ed" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier has a &lt;code&gt;shrink_threshold&lt;/code&gt; parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by &lt;code&gt;shrink_threshold&lt;/code&gt;. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt; 분류는이 &lt;code&gt;shrink_threshold&lt;/code&gt; 하는 구현하는 가장 가까운 수축 중심 분류 매개 변수를. 실제로 각 중심에 대한 각 지형지 물의 값은 해당 지형지 물의 클래스 내 분산으로 나뉩니다. 그런 다음 기능 값은 &lt;code&gt;shrink_threshold&lt;/code&gt; 만큼 줄어 듭니다 . 특히, 특정 기능 값이 0을 초과하면 0으로 설정됩니다. 실제로, 이는 분류에 영향을주는 기능을 제거합니다. 예를 들어 노이즈 기능을 제거하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="80f0d0589fbf3ca83c23f0b6ee7bc1939986e528" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;sklearn.cluster.KMeans&lt;/code&gt;&lt;/a&gt; algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) for more complex methods that do not make this assumption. Usage of the default &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; is simple:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43771e48f74cd166aa989881024956f81686fd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the &lt;code&gt;sklearn.KMeans&lt;/code&gt; algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) for more complex methods that do not make this assumption. Usage of the default &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; is simple:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt; 분류기는 부재의 중심으로 각각의 클래스를 나타내는 간단한 알고리즘이다. 실제로 이것은 &lt;code&gt;sklearn.KMeans&lt;/code&gt; 알고리즘 의 레이블 업데이트 단계와 유사 합니다. 또한 선택할 매개 변수가 없으므로 훌륭한 기준 분류 기준이됩니다. 그러나 볼록하지 않은 클래스뿐만 아니라 모든 차원에서 동일한 분산이 가정되므로 클래스가 크게 다른 분산을 갖는 경우에는 문제가 발생합니다. 이 가정을하지 않는보다 복잡한 방법 은 선형 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) 및 2 차 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; )을 참조하십시오. 기본 &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; 사용&lt;/a&gt; 간단하다 :</target>
        </trans-unit>
        <trans-unit id="39bfe25102ea45948a285842eddb73a441e962a3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; is built using a list of &lt;code&gt;(key, value)&lt;/code&gt; pairs, where the &lt;code&gt;key&lt;/code&gt; is a string containing the name you want to give this step and &lt;code&gt;value&lt;/code&gt; is an estimator object:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 목록을 사용하여 구축 &lt;code&gt;(key, value)&lt;/code&gt; 쌍, &lt;code&gt;key&lt;/code&gt; 이 단계를주고 싶어하고 이름이 포함 된 문자열 &lt;code&gt;value&lt;/code&gt; 추정량 개체입니다 :</target>
        </trans-unit>
        <trans-unit id="c923ad3a865326ec6c72af48e5305bcc89674896" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \(N(0, \frac{1}{n_{components}})\).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt; 이&lt;/a&gt; 성분은 다음 분배 \에서 도출되는 랜덤하게 생성 행렬에 원래의 입력 공간 (N (0, \ FRAC {1} {N_ {성분}}) \) 돌출하여 차원을 줄인다.</target>
        </trans-unit>
        <trans-unit id="bf402b0deb2f6872588357c0d1a4ee6663793993" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space using a sparse random matrix.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; 는&lt;/a&gt; 희소 랜덤 행렬을 이용하여 원래의 입력 공간으로 돌출하여 차원을 줄인다.</target>
        </trans-unit>
        <trans-unit id="bfc6d74a43acae56b2f26724a2d5ae8338eaf262" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt;&lt;code&gt;export_graphviz&lt;/code&gt;&lt;/a&gt; exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt; &lt;code&gt;export_graphviz&lt;/code&gt; 의&lt;/a&gt; 수출 또한 클래스 노드를 착색 (또는 값 회귀를위한) 원하는 경우 명시 적 변수와 클래스 이름을 사용하여 포함 미적 옵션의 다양한 지원합니다. Jupyter 노트북은 이러한 플롯을 자동으로 인라인으로 렌더링합니다.</target>
        </trans-unit>
        <trans-unit id="7c175b2e5f3e3958b57eb43bc3177612df200e71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/%20Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt; states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a15d0b16a3ea1ca7d8280fbccb678c643c12770" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-measure&lt;/a&gt; (\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \(F_\beta\) measure reaches its best value at 1 and its worst score at 0. With \(\beta = 1\), \(F_\beta\) and \(F_1\) are equivalent, and the recall and the precision are equally important.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-계수&lt;/a&gt; (\ (F_ \ 베타 \)와 \ (F_1 \) 방법)은 정밀 리콜의 가중 조화 평균으로서 해석 될 수있다. \ (F_ \ beta \) 측정 값은 1에서 가장 높은 값에 도달하고 최악의 점수는 0에 도달합니다. 리콜과 정밀도도 똑같이 중요합니다.</target>
        </trans-unit>
        <trans-unit id="88cf2fa597f50207550c56a5d725499fc42ad462" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt; states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;존슨 Lindenstrauss 표제어는&lt;/a&gt; 페어 와이즈 거리의 왜곡을 조절하면서 모든 고차원 세트 임의로 낮은 차원 유클리드 공간에 투영 될 수 있다는 것을 말한다.</target>
        </trans-unit>
        <trans-unit id="6b941988cfc8d08eb5daa76d4a5974b4197ff783" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-estimator&quot;&gt;estimator&lt;/a&gt; is required to be a fitted estimator. &lt;code&gt;X&lt;/code&gt; can be the data set used to train the estimator or a hold-out set. The permutation importance of a feature is calculated as follows. First, a baseline metric, defined by &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scoring&quot;&gt;scoring&lt;/a&gt;, is evaluated on a (potentially different) dataset defined by the &lt;code&gt;X&lt;/code&gt;. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96678c00449216bcbe65a0961a8b25b8baf7a396" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class can adapt its number of mixture components automatically. The parameter &lt;code&gt;weight_concentration_prior&lt;/code&gt; has a direct link with the resulting number of components with non-zero weights. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.</source>
          <target state="translated">&lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 의 클래스는 자동으로 혼합 구성 요소의 수를 적용 할 수 있습니다. &lt;code&gt;weight_concentration_prior&lt;/code&gt; 매개 변수 는 가중치가 0이 아닌 결과 구성 요소 수와 직접 링크됩니다. 이전 농도에 대해 낮은 값을 지정하면 모델이 소수의 구성 요소에 대부분의 가중치를 적용하여 나머지 구성 요소 가중치를 0에 매우 가깝게 설정합니다. 이전 농도의 높은 값은 혼합물에서 더 많은 성분이 활성화 될 수있게한다.</target>
        </trans-unit>
        <trans-unit id="c17c439e95320993d0276d174b035cd14b7ce3b3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter controls the amount of regularization in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; object: a large value for &lt;code&gt;C&lt;/code&gt; results in less regularization. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; gives &lt;a href=&quot;#shrinkage&quot;&gt;Shrinkage&lt;/a&gt; (i.e. non-sparse coefficients), while &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; gives &lt;a href=&quot;#sparsity&quot;&gt;Sparsity&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 의 파라미터를 제어에서 정규화 양 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 목적 :위한 큰 값 &lt;code&gt;C&lt;/code&gt; 를 적게 정규화 결과. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 는 &lt;a href=&quot;#shrinkage&quot;&gt;수축&lt;/a&gt; (즉, 희소 계수)을 제공하고 &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; 은 &lt;a href=&quot;#sparsity&quot;&gt;희소성을&lt;/a&gt; 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="9164d9a9144eaecf5fe284f2e40277e82f0b8068" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter trades off correct classification of training examples against maximization of the decision function&amp;rsquo;s margin. For larger values of &lt;code&gt;C&lt;/code&gt;, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower &lt;code&gt;C&lt;/code&gt; will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 의 결정 함수의 수익성의 극대화에 대한 사례를 교육의 올바른 분류 오프 파라미터 거래. &lt;code&gt;C&lt;/code&gt; 값이 클수록 결정 기능이 모든 교육 지점을 올바르게 분류하는 것이 더 좋으면 더 작은 마진이 허용됩니다. &lt;code&gt;C&lt;/code&gt; 가 낮을수록 훈련 정확도를 높이 면서 더 큰 마진을 가지므로 더 간단한 결정 기능을 얻을 수 있습니다. 즉,``C ''는 SVM에서 정규화 매개 변수로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="9aa2de3f6ced8022ed53d959fe2e18d24e70ecf1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;DESCR&lt;/code&gt; contains a free-text description of the data, while &lt;code&gt;details&lt;/code&gt; contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML documentation&lt;/a&gt; The &lt;code&gt;data_id&lt;/code&gt; of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:</source>
          <target state="translated">&lt;code&gt;DESCR&lt;/code&gt; 는 동안, 데이터의 자유 텍스트 설명을 포함 &lt;code&gt;details&lt;/code&gt; 데이터 세트 ID처럼 openml에 저장된 메타 데이터의 사전을 포함하고 있습니다. 자세한 내용은 참조 &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML 문서 &lt;/a&gt; &lt;code&gt;data_id&lt;/code&gt; 쥐의 단백질 데이터 세트는 40966입니다, 당신은 openml 웹 사이트의 데이터 세트에 대한 자세한 정보를 얻을이 (또는 이름)를 사용할 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="388bd8e84c98bb0ce4ff6564cc35fb4e0e8f41db" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; estimator has the most flexibility and is able to predict higher expected values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81a0037eca65e4ed69e2a49ca4871b0ce138bc10" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Normalizer&lt;/code&gt; rescales the vector for each sample to have unit norm, independently of the distribution of the samples. It can be seen on both figures below where all samples are mapped onto the unit circle. In our example the two selected features have only positive values; therefore the transformed data only lie in the positive quadrant. This would not be the case if some original features had a mix of positive and negative values.</source>
          <target state="translated">&lt;code&gt;Normalizer&lt;/code&gt; 독립적 샘플의 분포의 각각의 샘플을 단위 놈을위한 벡터를 다시 조정. 아래의 두 그림에서 모든 샘플이 단위 원에 매핑되어 있습니다. 이 예에서 선택한 두 피처는 양수 값만 갖습니다. 따라서 변환 된 데이터는 양의 사분면에만 있습니다. 일부 원래 기능에 양수 값과 음수 값이 혼합되어있는 경우에는 그렇지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c108938c180fba7834742cb26f12054acc7a2184" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;PCA&lt;/code&gt; fitting is only computed at the evaluation of the first configuration of the &lt;code&gt;C&lt;/code&gt; parameter of the &lt;code&gt;LinearSVC&lt;/code&gt; classifier. The other configurations of &lt;code&gt;C&lt;/code&gt; will trigger the loading of the cached &lt;code&gt;PCA&lt;/code&gt; estimator data, leading to save processing time. Therefore, the use of caching the pipeline using &lt;code&gt;memory&lt;/code&gt; is highly beneficial when fitting a transformer is costly.</source>
          <target state="translated">&lt;code&gt;PCA&lt;/code&gt; 의 결합 만의 제 1 구성의 평가에서 계산되는 &lt;code&gt;C&lt;/code&gt; 에서 의 파라미터 &lt;code&gt;LinearSVC&lt;/code&gt; 의 분류. &lt;code&gt;C&lt;/code&gt; 의 다른 구성은 캐시 된 &lt;code&gt;PCA&lt;/code&gt; 추정기 데이터 의로드를 트리거하여 처리 시간을 절약합니다. 따라서 &lt;code&gt;memory&lt;/code&gt; 사용하여 파이프 라인 캐싱을 사용 하면 변압기를 장착하는 데 많은 비용이 듭니다.</target>
        </trans-unit>
        <trans-unit id="c287f0a262d74ac1807987cdaae52227ef4012e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Product&lt;/code&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a3cb6faf35a31966ca4f91d6d7c341a997f291b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;RandomForestClassifier&lt;/code&gt; is trained using &lt;em&gt;bootstrap aggregation&lt;/em&gt;, where each new tree is fit from a bootstrap sample of the training observations \(z_i = (x_i, y_i)\). The &lt;em&gt;out-of-bag&lt;/em&gt; (OOB) error is the average error for each \(z_i\) calculated using predictions from the trees that do not contain \(z_i\) in their respective bootstrap sample. This allows the &lt;code&gt;RandomForestClassifier&lt;/code&gt; to be fit and validated whilst being trained &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17baab07595bc9a1b9010bf52fc5ebb7c1555943" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;RandomForestClassifier&lt;/code&gt; is trained using &lt;em&gt;bootstrap aggregation&lt;/em&gt;, where each new tree is fit from a bootstrap sample of the training observations \(z_i = (x_i, y_i)\). The &lt;em&gt;out-of-bag&lt;/em&gt; (OOB) error is the average error for each \(z_i\) calculated using predictions from the trees that do not contain \(z_i\) in their respective bootstrap sample. This allows the &lt;code&gt;RandomForestClassifier&lt;/code&gt; to be fit and validated whilst being trained &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;RandomForestClassifier&lt;/code&gt; 을 사용하여 훈련 &lt;em&gt;부트 스트랩 집계&lt;/em&gt; 각각의 새로운 나무 훈련 관측 \ (z_i = (x_i로부터, y_i) \)의 부트 스트랩 샘플에서 적합합니다. &lt;em&gt;밖으로의 가방&lt;/em&gt; (OOB) 오류가 각의 평균 오류입니다 \ (z_i \)가 포함되지 않은 나무에서 예측을 사용하여 계산 \ (z_i \) 각각의 부트 스트랩 샘플입니다. 이를 통해 &lt;code&gt;RandomForestClassifier&lt;/code&gt; 를 학습 하는 동안 적합하고 검증 할 수 있습니다 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b88cefdaf9f96b3cdafb06bfae8af6e9f9c7d591" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Ridge&lt;/code&gt; regression model can predict very low expected frequencies that do not match the data. It can therefore severly under-estimate the risk for some policyholders.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fcb1f40315317ee430343ccf6429ac412e1cf20" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;SpectralBiclustering&lt;/code&gt; algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68d62b9583d10d4cdba8ebe8ca76f4a0cc3bafcf" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;SpectralCoclustering&lt;/code&gt; algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2977d18fbb46e84c7bb310b8e5bfcc96a3f259f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Sum&lt;/code&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="093b9af7ff877738a1c191bf8fe58f666ce1b93c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;VotingClassifier&lt;/code&gt; can also be used together with &lt;code&gt;GridSearch&lt;/code&gt; in order to tune the hyperparameters of the individual estimators:</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 은 또한 함께 사용될 수있다 &lt;code&gt;GridSearch&lt;/code&gt; 개별 추정기의 하이퍼 파라미터 조정 순서 :</target>
        </trans-unit>
        <trans-unit id="a92f24d7703ca3728952e82f17755a0b7604fe2c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;alpha&lt;/code&gt; parameter controls the degree of sparsity of the coefficients estimated.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; 매개 변수 추정 계수의 희박의 정도를 제어한다.</target>
        </trans-unit>
        <trans-unit id="315ca80415fed5e99f9417365418d048de6cb5f9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;alpha&lt;/code&gt; parameter controls the degree of sparsity of the estimated coefficients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d22b7c9a5ce685adf8551f77a733c13cab8fe2d4" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;best_estimator_&lt;/code&gt;, &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; correspond to the scorer (key) that is set to the &lt;code&gt;refit&lt;/code&gt; attribute.</source>
          <target state="translated">&lt;code&gt;best_estimator_&lt;/code&gt; , &lt;code&gt;best_index_&lt;/code&gt; , &lt;code&gt;best_score_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 받는 설정 기록원 (키)에 대응하는 &lt;code&gt;refit&lt;/code&gt; 속성.</target>
        </trans-unit>
        <trans-unit id="ea04fcbf02a8de4070a990d8e4f932cdf0278b0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;beta&lt;/code&gt; parameter determines the weight of precision in the combined score. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; lends more weight to precision, while &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; favors recall (&lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; considers only precision, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; only recall).</source>
          <target state="translated">&lt;code&gt;beta&lt;/code&gt; 파라미터는 정밀 결합 점수의 중량을 결정한다. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; 은 정밀도에 더 많은 가중치를 부여하는 반면, &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; 은 리콜을 선호합니다 ( &lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; 은 정밀도, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; 리콜 만 고려 ).</target>
        </trans-unit>
        <trans-unit id="baaa5bd4c537184b8165bf82e50573a6954bd1a3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;beta&lt;/code&gt; parameter determines the weight of recall in the combined score. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; lends more weight to precision, while &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; favors recall (&lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; considers only precision, &lt;code&gt;beta -&amp;gt; +inf&lt;/code&gt; only recall).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cbc8f71751f51b8523a564fa7c56816a950df46" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;clf&lt;/code&gt; (for classifier) estimator instance is first fitted to the model; that is, it must &lt;em&gt;learn&lt;/em&gt; from the model. This is done by passing our training set to the &lt;code&gt;fit&lt;/code&gt; method. For the training set, we&amp;rsquo;ll use all the images from our dataset, except for the last image, which we&amp;rsquo;ll reserve for our predicting. We select the training set with the &lt;code&gt;[:-1]&lt;/code&gt; Python syntax, which produces a new array that contains all but the last item from &lt;code&gt;digits.data&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;clf&lt;/code&gt; 추정기 인스턴스 (분류기)를 제 모델에 장착되고; 즉 , 모델에서 &lt;em&gt;배워야&lt;/em&gt; 합니다 . 훈련 세트를 &lt;code&gt;fit&lt;/code&gt; 방법 으로 전달하면됩니다 . 훈련 세트의 경우 예측을 위해 예약 할 마지막 이미지를 제외하고 데이터 세트의 모든 이미지를 사용합니다. &lt;code&gt;[:-1]&lt;/code&gt; Python 구문으로 훈련 세트를 선택합니다 .이 구문은 &lt;code&gt;digits.data&lt;/code&gt; 에서 마지막 항목을 제외한 모든 항목을 포함하는 새 배열을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="0655de3e2b717fc73c590188fdaa9881c37414a7" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cross_validate&lt;/code&gt; function differs from &lt;code&gt;cross_val_score&lt;/code&gt; in two ways -</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 에서 기능 다릅니다 &lt;code&gt;cross_val_score&lt;/code&gt; 두 가지 방법 -</target>
        </trans-unit>
        <trans-unit id="f3aad90428722c87b3422d97c1856aa8204966ed" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cv_results_&lt;/code&gt; parameter can be easily imported into pandas as a &lt;code&gt;DataFrame&lt;/code&gt; for further inspection.</source>
          <target state="translated">&lt;code&gt;cv_results_&lt;/code&gt; 의 매개 변수는 쉽게로 팬더로 가져올 수 있습니다 &lt;code&gt;DataFrame&lt;/code&gt; 추가 검사합니다.</target>
        </trans-unit>
        <trans-unit id="6d0144f231c5dfa868fda545c176e4a6eca95147" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;data_id&lt;/code&gt; also uniquely identifies a dataset from OpenML:</source>
          <target state="translated">&lt;code&gt;data_id&lt;/code&gt; 또한 고유 OpenML에서 데이터 세트를 식별합니다 :</target>
        </trans-unit>
        <trans-unit id="7bfd5d978dedbc7159d3a401f357dd25ad25428a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers:</source>
          <target state="translated">&lt;code&gt;decision_function&lt;/code&gt; 의 방법은 음의 값이 특이점이 아닌 부정적인 라이어가있는 것과 같은 방법에있어서, 스코어링 함수로 정의된다 :</target>
        </trans-unit>
        <trans-unit id="bc22b4069c85e3e1ecbf3c942877625aea87d185" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option &lt;code&gt;probability&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, class membership probability estimates (from the methods &lt;code&gt;predict_proba&lt;/code&gt; and &lt;code&gt;predict_log_proba&lt;/code&gt;) are enabled. In the binary case, the probabilities are calibrated using Platt scaling &lt;a href=&quot;#id11&quot; id=&quot;id2&quot;&gt;9&lt;/a&gt;: logistic regression on the SVM&amp;rsquo;s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per &lt;a href=&quot;#id12&quot; id=&quot;id3&quot;&gt;10&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62eacbcbc4132ef1f5317a0a939d640165e31df3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option &lt;code&gt;probability&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, class membership probability estimates (from the methods &lt;code&gt;predict_proba&lt;/code&gt; and &lt;code&gt;predict_log_proba&lt;/code&gt;) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM&amp;rsquo;s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;decision_function&lt;/code&gt; 메소드는 각 샘플에 대한 클래스 별 점수 (또는 이진 경우 샘플 당 단일 점수)를 제공합니다. 생성자 옵션 &lt;code&gt;probability&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 로 설정되면 ( &lt;code&gt;predict_proba&lt;/code&gt; 및 &lt;code&gt;predict_log_proba&lt;/code&gt; 메소드에서) 클래스 멤버쉽 확률 추정 이 사용됩니다. 이진 경우 확률은 Platt scaling : SVM의 점수에 대한 로지스틱 회귀를 사용하여 훈련 데이터에 대한 추가 교차 검증에 따라 교정됩니다. 멀티 클래스의 경우, 이것은 Wu et al. (2004).</target>
        </trans-unit>
        <trans-unit id="cacd73f2ee1bdb21ccf547bfa27f031af8d1c84f" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;estimators&lt;/code&gt; parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="912eed5f20931cd928be8a8b3c3891f77622f73b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter can be set to &lt;code&gt;'all'&lt;/code&gt; to return all features whether or not they contain missing values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac9899847d23144b2277382b5ec5bf6360733941" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter can be set to &lt;code&gt;'all'&lt;/code&gt; to returned all features whether or not they contain missing values:</source>
          <target state="translated">는 &lt;code&gt;features&lt;/code&gt; 매개 변수가 설정 될 수있다 &lt;code&gt;'all'&lt;/code&gt; 가 누락 된 값을 포함할지 여부를 반환 모든 기능에 :</target>
        </trans-unit>
        <trans-unit id="0e29ac108f496200ac17f2eb1912fca379623586" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter is used to choose the features for which the mask is constructed. By default, it is &lt;code&gt;'missing-only'&lt;/code&gt; which returns the imputer mask of the features containing missing values at &lt;code&gt;fit&lt;/code&gt; time:</source>
          <target state="translated">&lt;code&gt;features&lt;/code&gt; 마스크가 구성되어있는 기능을 선택하는 데 사용되는 매개 변수입니다. 기본적으로 &lt;code&gt;'missing-only'&lt;/code&gt; 으로 &lt;code&gt;fit&lt;/code&gt; 시간에 결 측값이 포함 된 피처의 마스크를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="9c67ef88019a1b7e1168b9363c0a3a8856eb06c6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;final_estimator&lt;/code&gt; will use the predictions of the &lt;code&gt;estimators&lt;/code&gt; as input. It needs to be a classifier or a regressor when using &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt;, respectively:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b06298ca8347e48ebdf3df08a4c5d05e9d2dcbb" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;fit&lt;/code&gt; function takes two arguments: &lt;code&gt;n_components&lt;/code&gt;, which is the target dimensionality of the feature transform, and &lt;code&gt;gamma&lt;/code&gt;, the parameter of the RBF-kernel. A higher &lt;code&gt;n_components&lt;/code&gt; will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that &amp;ldquo;fitting&amp;rdquo; the feature function does not actually depend on the data given to the &lt;code&gt;fit&lt;/code&gt; function. Only the dimensionality of the data is used. Details on the method can be found in &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; : 함수는 두 인자 얻어 &lt;code&gt;n_components&lt;/code&gt; 기능의 목표 차원 변환을하고, 그리고 &lt;code&gt;gamma&lt;/code&gt; 상기 RBF 커널의 파라미터. &lt;code&gt;n_components&lt;/code&gt; 가 높을수록 커널 근사값이 향상되고 커널 SVM에서 생성 한 결과와 더 유사한 결과가 나타납니다. 기능 기능은 실제로 주어진 데이터에 의존하지 않는다 &quot;피팅&quot;고주의 &lt;code&gt;fit&lt;/code&gt; 기능. 데이터의 차원 만 사용됩니다. 방법에 대한 자세한 내용은 &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="24f4fe27df1296a0eb0115a0bb0e2832f2225923" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;id&lt;/code&gt; of each check is set to be a pprint version of the estimator and the name of the check with its keyword arguments. This allows to use &lt;code&gt;pytest -k&lt;/code&gt; to specify which tests to run:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8f8ea902ec10f43c9cca575477617a393bf1406" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;increasing&lt;/code&gt; parameter changes the constraint to \(\hat{y}_i \ge \hat{y}_j\) whenever \(X_i \le X_j\). Setting it to &amp;lsquo;auto&amp;rsquo; will automatically choose the constraint based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;&gt;Spearman&amp;rsquo;s rank correlation coefficient&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f460a2e1f3d0337b4c58660913081d08e87c0f52" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;init&lt;/code&gt; attribute determines the initialization method applied, which has a great impact on the performance of the method. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; implements the method Nonnegative Double Singular Value Decomposition. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;4&lt;/a&gt; is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="759d68cbc1b0e0c960b6f5234a5fe3b985174684" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;init&lt;/code&gt; attribute determines the initialization method applied, which has a great impact on the performance of the method. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; implements the method Nonnegative Double Singular Value Decomposition. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.</source>
          <target state="translated">&lt;code&gt;init&lt;/code&gt; 특성은 상기 방법의 성능에 큰 영향을인가하는 초기화 방법을 결정한다. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 는 음이 아닌 이중 특이 값 분해 방법을 구현합니다. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; 는 두 개의 SVD 프로세스를 기반으로하며, 하나는 데이터 매트릭스에 근사하고, 다른 하나는 단위 랭크 매트릭스의 대수적 특성을 이용하는 결과적인 부분 SVD 요인의 양의 섹션에 근사합니다. 기본 NNDSVD 알고리즘은 희소 인수 분해에 더 적합합니다. 변형 NNDSVDa (모든 0이 데이터의 모든 요소의 평균과 동일하게 설정 됨) 및 NNDSVDar (0이 데이터의 평균을 100으로 나눈 값보다 작은 임의의 섭동으로 설정 됨)이 밀도가 높을 때 권장 됨 케이스.</target>
        </trans-unit>
        <trans-unit id="dae4c344a8a96bf6754b4ff3002f4e9350562dbc" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;intercept_&lt;/code&gt; attribute holds the intercept (aka offset or bias):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aaf2ff68858d9c24eece58235794e4a322e1ce9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;intercept_&lt;/code&gt; member is not converted.</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 회원은 변환되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0e6b2b935d052640da205c359a0d82666ebb9942" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, &lt;code&gt;is_data_valid&lt;/code&gt; should be used as it is called prior to fitting the model and thus leading to better computational performance.</source>
          <target state="translated">&lt;code&gt;is_data_valid&lt;/code&gt; 및 &lt;code&gt;is_model_valid&lt;/code&gt; 기능을 식별 할 수 있도록 임의의 서브 샘플 축퇴 조합을 거부한다. 퇴화 사례를 식별하기 위해 추정 된 모델이 필요하지 않은 경우 &lt;code&gt;is_data_valid&lt;/code&gt; 는 모델을 피팅하기 전에 호출되므로 계산 성능이 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="dee80932cbc9425c512fa33535be444f8c4ddfa6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;l2_regularization&lt;/code&gt; parameter is a regularizer on the loss function and corresponds to \(\lambda\) in equation (2) of &lt;a href=&quot;#xgboost&quot; id=&quot;id26&quot;&gt;[XGBoost]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1aa8bc8d7f393abce9beb6161257c50a1665624" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;len(features)&lt;/code&gt; plots are arranged in a grid with &lt;code&gt;n_cols&lt;/code&gt; columns. Two-way partial dependence plots are plotted as contour plots.</source>
          <target state="translated">&lt;code&gt;len(features)&lt;/code&gt; 플롯과 격자 배열되어 &lt;code&gt;n_cols&lt;/code&gt; 의 열입니다. 양방향 부분 의존도 플롯은 등고선 플롯으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="da451cbcbf87abb0e9a3cde3e39db4fedec8991d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;len(features)&lt;/code&gt; plots are arranged in a grid with &lt;code&gt;n_cols&lt;/code&gt; columns. Two-way partial dependence plots are plotted as contour plots. The deciles of the feature values will be shown with tick marks on the x-axes for one-way plots, and on both axes for two-way plots.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d39915194cee376ca662b61de9924274942d60a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;make_columntransformer&lt;/code&gt; function is available to more easily create a &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; object. Specifically, the names will be given automatically. The equivalent for the above example would be:</source>
          <target state="translated">&lt;code&gt;make_columntransformer&lt;/code&gt; 의 기능을 더 쉽게 만들 수 있습니다 &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; 의&lt;/a&gt; 개체를. 특히 이름이 자동으로 주어집니다. 위의 예와 동등한 것은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">&lt;code&gt;mean_fit_time&lt;/code&gt; , &lt;code&gt;std_fit_time&lt;/code&gt; , &lt;code&gt;mean_score_time&lt;/code&gt; 및 &lt;code&gt;std_score_time&lt;/code&gt; 는 초 모두.</target>
        </trans-unit>
        <trans-unit id="5067a7dbb7441ffa442bc35298e784d3bbb5d81c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how &lt;code&gt;X&lt;/code&gt; values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predictions will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predictions will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo; a &lt;code&gt;ValueError&lt;/code&gt; is raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">&lt;code&gt;out_of_bounds&lt;/code&gt; 훈련 도메인의 x 값의 외부를 처리하는 방법을 핸들 매개 변수입니다. &quot;nan&quot;으로 설정하면 예측 된 y 값은 NaN이됩니다. &quot;clip&quot;으로 설정하면 예측 된 y 값이 가장 가까운 열차 간격 끝점에 해당하는 값으로 설정됩니다. &amp;ldquo;raise&amp;rdquo;로 설정되면 &lt;code&gt;interp1d&lt;/code&gt; 가 ValueError를 발생 시키 도록하십시오.</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">순진 Bayes 모델 의 &lt;code&gt;partial_fit&lt;/code&gt; 메소드 호출은 일부 계산 오버 헤드를 발생시킵니다. 사용 가능한 RAM이 허용하는 한 큰 데이터 청크 크기를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="12fb3214c445789e7d1fb007eb13c0f9c87b3271" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;penalty&lt;/code&gt; parameter determines the regularization to be used (see description above in the classification section).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; 모듈은 상기 유틸리티 클래스 제공 &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt; 구현하여 동일한 동작한다는 &lt;code&gt;Transformer&lt;/code&gt; (짝수 비록 API가 &lt;code&gt;fit&lt;/code&gt; : 클래스이 조작 취급 샘플 무국적 독립적 인 방법은이 경우 쓸모 참조).</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
