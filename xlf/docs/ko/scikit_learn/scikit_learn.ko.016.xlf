<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="20a9e2ae79f377ec69f0ec221a6bcb99fe892698" translate="yes" xml:space="preserve">
          <source>Inputs: fitted predictive model \(m\), tabular dataset (training or validation) \(D\).</source>
          <target state="translated">입력 : 적합 예측 모델 \ (m \), 테이블 형식 데이터 세트 (훈련 또는 검증) \ (D \).</target>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">이미 장착 된 LSH 포리스트에 새 데이터를 삽입합니다.</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">이미 장착 된 LSH 포리스트에 새 데이터를 삽입합니다. 비용은 새로운 총 크기에 비례하므로 추가를 일괄 처리해야합니다.</target>
        </trans-unit>
        <trans-unit id="aa15440a6446ecaf7603f9c0287316507f4328a1" translate="yes" xml:space="preserve">
          <source>Inspecting coefficients across the folds of a cross-validation loop gives an idea of their stability.</source>
          <target state="translated">교차 검증 루프의 주름에 걸쳐 계수를 검사하면 안정성에 대한 아이디어를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d3ce4618efaae8bf391d0768eaf2c4b834adfb9b" translate="yes" xml:space="preserve">
          <source>Inspection</source>
          <target state="translated">Inspection</target>
        </trans-unit>
        <trans-unit id="4c0fbc7b0ca330086776985f409e7f037b2f9494" translate="yes" xml:space="preserve">
          <source>Instance of the estimator.</source>
          <target state="translated">추정 자의 인스턴스입니다.</target>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">카디널리티 세트 'n choose k'를 사용하여 계산하는 대신, 여기서 n은 샘플 수이고 k는 서브 샘플 수 (적어도 피처 수)입니다. 'n을 선택한 경우 주어진 최대 크기의 확률 적 하위 모집단 만 고려하십시오. k '는 max_subpopulation보다 큽니다. 작은 문제 크기 이외의 경우이 매개 변수는 n_subsamples가 변경되지 않은 경우 메모리 사용량 및 런타임을 판별합니다.</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">벡터 결과를 제공하는 대신 LARS 솔루션은 매개 변수 벡터의 L1 표준의 각 값에 대한 솔루션을 나타내는 곡선으로 구성됩니다. 계수의 전체 경로는 어레이에 저장 &lt;code&gt;coef_path_&lt;/code&gt; (n_features max_features + 1)의 크기를 갖는다. 첫 번째 열은 항상 0입니다.</target>
        </trans-unit>
        <trans-unit id="848ef7b85f04c1e0179836725b124a8c68948c34" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \(\ell_1\) norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">벡터 결과를 제공하는 대신 LARS 솔루션은 매개 변수 벡터의 \ (\ ell_1 \) 노름의 각 값에 대한 솔루션을 나타내는 곡선으로 구성됩니다. 전체 계수 경로는 크기 (n_features, max_features + 1)를 갖는 배열 &lt;code&gt;coef_path_&lt;/code&gt; 에 저장됩니다 . 첫 번째 열은 항상 0입니다.</target>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">체인의 다양한 구성 요소의 매개 변수를 조정하는 대신 가능한 값의 그리드에서 최상의 매개 변수를 철저히 검색 할 수 있습니다. 우리는 idf의 유무에 관계없이 선형 SVM에 대해 0.01 또는 0.001의 페널티 매개 변수를 사용하여 단어 또는 bigrams에서 모든 분류자를 시도합니다.</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">대신 호출자는 명시 적으로 &lt;code&gt;with_centering=False&lt;/code&gt; 로 설정하고 (이 경우 CSR 매트릭스의 기능에서 분산 스케일링 만 수행됨 &lt;code&gt;X.toarray()&lt;/code&gt; 구체화 된 밀도 배열이 적합 할 것으로 예상되는 경우 X.toarray () 를 호출 해야합니다. 메모리에.</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">대신 호출자는 명시 적으로 &lt;code&gt;with_mean=False&lt;/code&gt; 로 설정하고 (이 경우 CSC 매트릭스의 기능에서 분산 스케일링 만 수행됨 &lt;code&gt;X.toarray()&lt;/code&gt; 구체화 된 밀도 배열이 적합 할 것으로 예상되는 경우 X.toarray () 를 호출 해야합니다. 메모리에.</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">대신, \ (w \)를 통한 분포는 축 평행, 타원형 가우스 분포 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">주어진 &lt;code&gt;encoding&lt;/code&gt; 아닌 문자를 포함하는 분석을 위해 바이트 시퀀스가 ​​제공되는 경우 수행 할 작업에 대한 지침 . 기본적으로 '엄격'이므로 UnicodeDecodeError가 발생합니다. 다른 값은 '무시'및 '바꾸기'입니다.</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">주어진 &lt;code&gt;encoding&lt;/code&gt; 아닌 문자를 포함하는 분석을 위해 바이트 시퀀스가 ​​제공되는 경우 수행 할 작업에 대한 지침 . bytes.decode에 키워드 인수 'errors'로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">레이블의 정수 배열. 제공하지 않으면 레이블은 y_true 및 y_pred에서 유추됩니다.</target>
        </trans-unit>
        <trans-unit id="e031a894709099be1ecbe448974105f94db94157" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to linear predictor.</source>
          <target state="translated">선형 예측 자에 추가 된 절편 (일명 편향).</target>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">의사 결정 기능에 절편 (일명 바이어스)이 추가되었습니다.</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">차단 기간.</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">알고리즘에 의해 유지되는 충분한 내부 통계. 그것들을 유지하는 것은 진화의 역사를 잃어 버리지 않기 위해 온라인 환경에서 유용하지만, 최종 사용자에게는 사용해서는 안됩니다. A (n_components, n_components)는 사전 공분산 행렬입니다. B (n_features, n_components)는 데이터 근사 행렬입니다</target>
        </trans-unit>
        <trans-unit id="e1895bccbde849f2ce31dc6715c34549e1152575" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid losing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">알고리즘에 의해 유지되는 충분한 내부 통계. 온라인 설정에서는 진화의 역사를 잃지 않도록 유지하는 것이 유용하지만 최종 사용자는 사용할 수 없습니다. A (n_components, n_components)는 사전 공분산 행렬입니다. B (n_features, n_components)는 데이터 근사 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">내부적으로 Laplace 근사법은 가우시안이 아닌 가우스 뒤를 근사화하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">내부적으로 목표 &lt;code&gt;y&lt;/code&gt; 는 항상 2 차원 배열로 변환되어 사이 킷 학습 변압기에서 사용됩니다. 예측시 출력은 &lt;code&gt;y&lt;/code&gt; 와 동일한 수의 차원을 갖도록 재구성됩니다 .</target>
        </trans-unit>
        <trans-unit id="465a9fa03a440d5f1b8441512ea129ccebe5933c" translate="yes" xml:space="preserve">
          <source>Internally, this method uses &lt;code&gt;max_iter = 1&lt;/code&gt;. Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence and early stopping should be handled by the user.</source>
          <target state="translated">내부적으로이 메서드는 &lt;code&gt;max_iter = 1&lt;/code&gt; 합니다. 따라서 한 번 호출 한 후 최소 비용 함수에 도달한다는 보장은 없습니다. 객관적 수렴 및 조기 중지 등의 사항은 사용자가 처리해야합니다.</target>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">내부적으로는 모든 계산을 처리하기 위해 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 과 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 를 사용합니다. 이 라이브러리는 C와 Cython을 사용하여 랩핑됩니다.</target>
        </trans-unit>
        <trans-unit id="921b6b42e9e212246385b90b6e2081ffae4bdd4d" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt; and &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython. For a description of the implementation and details of the algorithms used, please refer to their respective papers.</source>
          <target state="translated">내부적으로 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm &lt;/a&gt;&lt;a href=&quot;#id14&quot; id=&quot;id9&quot;&gt;12&lt;/a&gt; 및 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear &lt;/a&gt;&lt;a href=&quot;#id13&quot; id=&quot;id10&quot;&gt;11&lt;/a&gt; 을 사용하여 모든 계산을 처리합니다. 이러한 라이브러리는 C 및 Cython을 사용하여 래핑됩니다. 사용 된 알고리즘의 구현 및 세부 사항에 대한 설명은 해당 문서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="a02157db035ff864370a2c436b6c81a38e8d8a3c" translate="yes" xml:space="preserve">
          <source>Interpreting coefficients: scale matters</source>
          <target state="translated">계수 해석 : 척도 문제</target>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">'Ctrl-C'를 사용하여 다중 프로세스 작업 중단</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">직관적 인 해석 : 불량 V 측정을 사용한 군집화 &lt;strong&gt;는 동종 성과 완전성 측면에서 질적으로 분석되어&lt;/strong&gt; 과제에 의해 발생하는 '종종'실수를 더 잘 느낄 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">직관적으로, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;정밀도&lt;/a&gt; 는 분류기에서 음성 인 샘플을 양성으로 표시하지 않는 능력이며, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;리콜&lt;/a&gt; 은 분류기에서 모든 양성 샘플을 찾는 능력입니다.</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">직관적으로, 히스토그램은 포인트 당 한 블록 씩 블록 스택으로 생각할 수 있습니다. 적절한 그리드 공간에 블록을 쌓아서 히스토그램을 복구합니다. 그러나 블록을 일반 그리드에 쌓는 대신 각 블록을 나타내는 점의 중심에 놓고 각 위치의 총 높이를 합하면 어떻게 될까요? 이 아이디어는 왼쪽 하단 시각화로 이어집니다. 히스토그램만큼 깨끗하지는 않지만 데이터가 블록 위치를 구동한다는 것은 기본 데이터를 훨씬 더 잘 표현한다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">직관적으로, &lt;code&gt;gamma&lt;/code&gt; 매개 변수는 단일 학습 예제의 영향이 도달하는 정도를 정의합니다. 낮은 값은 'far'를 의미하고 높은 값은 'close'를 의미합니다. &lt;code&gt;gamma&lt;/code&gt; 파라미터 지원 벡터로서 모델에 의해 선택된 샘플들의 영향 반경의 역으로 볼 수있다.</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">직관적으로,이 행렬은 의사 특징 (일부 거듭 제곱 된 점)의 행렬로 해석 될 수 있습니다. 행렬은 다항식 커널에 의해 유도 된 행렬과 유사하지만 (이와는 다릅니다).</target>
        </trans-unit>
        <trans-unit id="d0136f60343b9ddfe4e95ff298a3f11e6a44a13d" translate="yes" xml:space="preserve">
          <source>Intuitively, we&amp;rsquo;re trying to maximize the margin (by minimizing \(||w||^2 = w^Tw\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \(y_i (w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from their correct margin boundary. The penalty term &lt;code&gt;C&lt;/code&gt; controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter (see note below).</source>
          <target state="translated">직관적으로, 우리는 (\ (|| w || ^ 2 = w ^ Tw \)를 최소화함으로써) 마진을 최대화하려고 노력하고 있으며, 샘플이 잘못 분류되거나 마진 경계 내에있을 때 패널티가 발생합니다. 이상적으로, \ (y_i (w ^ T \ phi (x_i) + b) \) 값은 모든 샘플에 대해 \ (\ geq 1 \)이며, 이는 완벽한 예측을 나타냅니다. 그러나 문제는 일반적으로 초평면으로 항상 완벽하게 분리 할 수있는 것은 아니므로 일부 샘플은 올바른 마진 경계에서 \ (\ zeta_i \) 거리에있을 수 있습니다. 페널티 항 &lt;code&gt;C&lt;/code&gt; 는이 페널티의 강도를 제어하고 결과적으로 역 정규화 매개 변수 역할을합니다 (아래 참고 참조).</target>
        </trans-unit>
        <trans-unit id="fcf37d79a0d7f3a40e6e7bdc86aa285b256f5c04" translate="yes" xml:space="preserve">
          <source>Inverse Gaussian</source>
          <target state="translated">역 가우스</target>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">정규화 강도의 역수; 양의 부동 소수점이어야합니다. 서포트 벡터 머신과 마찬가지로 값이 작을수록 정규화가 더 강력 해집니다.</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">변형을 거꾸로하십시오.</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">변형을 거꾸로하십시오. 각 피처 그룹에 할당 된 Xred 값으로 nb_features 크기의 벡터를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">역변환 행렬. &lt;code&gt;fit_inverse_transform&lt;/code&gt; 이 True 인 경우에만 사용 가능 합니다.</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">역변환 된 배열.</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">전달 된 추정기의 전달 된 메소드 이름을 호출합니다. method = 'predict_proba'의 경우 열은 정렬 된 순서로 클래스에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="93ce645e781a1eaea74d358c1f7aa54ffb25426b" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">호출 &lt;code&gt;fit&lt;/code&gt; 온 방법을 &lt;code&gt;VotingClassifier&lt;/code&gt; 은 클래스 속성에 저장됩니다 그 원래의 추정량의 클론 맞는 &lt;code&gt;self.estimators_&lt;/code&gt; 을 . 추정기는 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 &lt;code&gt;'drop'&lt;/code&gt; 으로 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">호출 &lt;code&gt;fit&lt;/code&gt; 온 방법을 &lt;code&gt;VotingClassifier&lt;/code&gt; 은 클래스 속성에 저장됩니다 그 원래의 추정량의 클론 맞는 &lt;code&gt;self.estimators_&lt;/code&gt; 을 . 추정기는 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 &lt;code&gt;None&lt;/code&gt; 으로 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0f0eee1c2a0f3878912e931a58a1d92e46d13c5a" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingRegressor&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;'drop'&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">호출 &lt;code&gt;fit&lt;/code&gt; 온 방법을 &lt;code&gt;VotingRegressor&lt;/code&gt; 은 클래스 속성에 저장됩니다 그 원래의 추정량의 클론 맞는 &lt;code&gt;self.estimators_&lt;/code&gt; 을 . 추정기는 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 &lt;code&gt;'drop'&lt;/code&gt; 으로 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">가우스 프로세스 분류 (GPC)를위한 등 확률 라인</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">격리 숲 알고리즘</target>
        </trans-unit>
        <trans-unit id="90b7e1d9dae263e13bf54b6eb5bbce295b25c458" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm.</source>
          <target state="translated">격리 포리스트 알고리즘.</target>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">포레스트 예제</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">아이소 맵 임베딩</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">R의 동위 원소 최적화 : PAVA (Pool-Adjacent Violators Algorithm) 및 활성 설정 방법 Leeuw, Hornik, Mair Journal of Statistical Software 2009</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">등장 중간 회귀 : 선형 프로그래밍 접근법 Nilotpal Chakravarti 연산 수학 연구 Vol. 14, No. 2 (1989 년 5 월), pp. 303-308</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">등장 성 회귀</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">y의 등장 성.</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">등장 성 회귀 모델.</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">함수가 호출되거나 클래스가 인스턴스화 될 때 경고를 발행하고 docstring에 경고를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">현재 데이터에 적응합니다.</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">평가를 위해 여러 메트릭을 지정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">또한 설정된 카디널리티 공식으로 표현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">&lt;code&gt;kernel&lt;/code&gt; 인수 로 직접 사용할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">또한 부울 랜덤 변수를 고려하는 추정기 (예 : 베이지안 설정에서 Bernoulli 분포를 사용하여 모델링)를위한 전처리 단계로 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">또한 숫자가 아닌 레이블을 해시 가능하고 비교 가능한 한 숫자 레이블로 변환하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">숫자가 아닌 레이블을 (해시 가능하고 비교 가능한 한) 숫자 레이블로 변환하는 데 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">또한 손실 함수에 정규화 항을 추가하여 모델 매개 변수를 축소하여 과적 합을 방지 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">잘린 SVD의 scipy.sparse.linalg ARPACK 구현을 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">매개 변수 &lt;code&gt;(estimator, X, y)&lt;/code&gt; 로 호출 할 수 있습니다 . 여기서 &lt;code&gt;estimator&lt;/code&gt; 는 평가해야하는 모델이고, &lt;code&gt;X&lt;/code&gt; 는 유효성 검증 데이터이며, &lt;code&gt;y&lt;/code&gt; 는 &lt;code&gt;X&lt;/code&gt; (감독 된 경우) 또는 &lt;code&gt;None&lt;/code&gt; (감독되지 않은 경우)의 기본 목표입니다. 케이스).</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 다운로드 /로드 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">항목 당 가중치 차이로 해석 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">k- 평균 (및 미니 배치 k- 평균)은 피처 스케일링에 매우 민감하며이 경우 IDF 가중치는 다음에 의해 제공된 &quot;지상 진실&quot;에 대해 측정 된만큼 클러스터링의 품질을 향상시키는 데 도움이됩니다. 20 개 뉴스 그룹 데이터 세트의 클래스 레이블 지정</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">두 개의 0이 아닌 계수를 가진 &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;OMP (Orthogonal Matching Pursuit)&lt;/a&gt; 의 결과는 하나만 유지할 때보 다 편향이 약간 적습니다 (가장자리가 덜 두드러짐). 또한 Frobenius 규범의 지상 진실과 더 가깝습니다.</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\ (\ nu \)-SVC 공식은 \ (C \)-SVC의 매개 변수화이므로 수학적으로 동등하다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">일 변량 기능 선택에 사용할 수 있습니다 ( &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;사용자 안내서 참조)&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">추정 절차에 정규화 매개 변수를 포함시키는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">학습 속도가 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">클러스터링 최적화의 목표로 사용할 단일 메트릭을 제공하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="53127b98db145a1107f55ea45dbcbf9eb40fb387" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">[Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;2&lt;/a&gt; 에서주의 깊게 제한하면 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 데이터 세트의 부품 기반 표현을 생성하여 해석 가능한 모델을 생성 할 수 있음 이 관찰되었습니다 . 다음 예제는 PCA 고유면과 비교하여 Olivetti 얼굴 데이터 세트의 이미지에서 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 찾은 16 개의 희소 성분을 표시합니다 .</target>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">[Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; 에서 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 신중하게 구속 될 때 데이터 기반의 부품 기반 표현을 생성하여 해석 가능한 모델을 생성 할 수 있음 이 관찰되었습니다 . 다음 예는 PCA 고유면과 비교하여 Olivetti면 데이터 세트의 이미지에서 &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 가 찾은 16 개의 희소 성분을 표시합니다 .</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">컴퓨터 비전에서 자주 사용되는 지수 카이 제곱 커널과 유사한 속성을 갖지만, 기능 맵의 간단한 Monte Carlo 근사를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">노이즈없는 데이터에 적용되는 ML에서 유용한 것으로 입증되었습니다. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;간단히 말해서 양자 역학을위한 기계 학습을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="bbe0780585e0153715a866cbcbfa3d1d5e8429c4" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">무소음 데이터에 적용되는 ML에 유용함이 입증되었습니다. 예를 들어 &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;양자 역학에 대한 기계 학습을 간단히&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">랜덤 키친 싱크의 변형을 구현합니다. [1]</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">정규화 기 이전에 L1으로 훈련 된 선형 모델입니다.</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt; 의 대안으로 제공되는 메모리 효율적인 온라인 학습 알고리즘 입니다. 그것은 잎 중심에서 클러스터 중심이 판독되는 트리 데이터 구조를 구성합니다. 이것들은 최종 군집 중심이거나 &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt; 과 같은 다른 군집 알고리즘에 대한 입력으로 제공 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">온라인 학습 방법에서 학습 속도를 제어하는 ​​매개 변수입니다. 점근 적 수렴을 보장하려면 값을 (0.5, 1.0]으로 설정해야합니다. 값이 0.0이고 batch_size가 &lt;code&gt;n_samples&lt;/code&gt; 인 경우 업데이트 방법은 배치 학습과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">95 % 통계 효율을 달성 하려면 매개 변수 &lt;code&gt;epsilon&lt;/code&gt; 을 1.35 로 설정하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">텍스트 처리 커뮤니티에서는 정규화 된 카운트 (일명 용어 빈도) 또는 TF-IDF 값이 지정된 기능이 실제로 약간 더 자주 수행되는 경우에도 이진 기능 값 (아마도 확률 론적 추론을 단순화하기 위해)을 사용하는 것이 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">분산 비율 기준이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="657bf821e2dc05fc87b192deecf1d4c429b7d563" translate="yes" xml:space="preserve">
          <source>It is also possible to compute the permutation importances on the training set. This reveals that &lt;code&gt;random_num&lt;/code&gt; gets a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use that random numerical feature to overfit. You can further confirm this by re-running this example with constrained RF with min_samples_leaf=10.</source>
          <target state="translated">훈련 세트에 대한 순열 중요도를 계산할 수도 있습니다. 이것은 &lt;code&gt;random_num&lt;/code&gt; 이 테스트 세트에서 계산 될 때보 다 훨씬 더 높은 중요도 순위를 갖는다 는 것을 보여줍니다 . 이 두 플롯의 차이점은 RF 모델이 임의의 수치 기능을 사용하여 과적 합할 수있을만큼 충분한 용량을 가지고 있다는 확인입니다. min_samples_leaf = 10을 사용하여 제한된 RF로이 예제를 다시 실행하여이를 추가로 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">사전 및 / 또는 코드를 데이터에 존재할 수있는 제약 조건과 일치하도록 긍정적으로 제한 할 수도 있습니다. 다음은 서로 다른 양의 구속 조건이 적용된면입니다. 빨간색은 음수 값을, 파란색은 양수 값을, 흰색은 0을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">이웃 지점 사이의 연결을 보여주는 희소 그래프를 효율적으로 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="0396c69b921d6f190c09b79532ebbdc31b35e115" translate="yes" xml:space="preserve">
          <source>It is also possible to encode each column into &lt;code&gt;n_categories - 1&lt;/code&gt; columns instead of &lt;code&gt;n_categories&lt;/code&gt; columns by using the &lt;code&gt;drop&lt;/code&gt; parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (&lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;), since co-linearity would cause the covariance matrix to be non-invertible. When this parameter is not None, &lt;code&gt;handle_unknown&lt;/code&gt; must be set to &lt;code&gt;error&lt;/code&gt;:</source>
          <target state="translated">로 각 열을 부호화하는 것도 가능하다 &lt;code&gt;n_categories - 1&lt;/code&gt; 열 대신 &lt;code&gt;n_categories&lt;/code&gt; 의 열을 이용하여 &lt;code&gt;drop&lt;/code&gt; 파라미터. 이 매개 변수를 사용하면 사용자가 삭제할 각 기능에 대한 카테고리를 지정할 수 있습니다. 이는 일부 분류기에서 입력 행렬의 공선 성을 피하는 데 유용합니다. 이러한 기능은 예를 들어 비정규 화 회귀 ( &lt;a href=&quot;generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt; &lt;code&gt;LinearRegression&lt;/code&gt; &lt;/a&gt; )를 사용할 때 유용합니다. 공선 성은 공분산 행렬을 비가 역적으로 만들 수 있기 때문입니다. 이 매개 변수가 None이 아니면 &lt;code&gt;handle_unknown&lt;/code&gt; 을 &lt;code&gt;error&lt;/code&gt; 로 설정해야합니다 .</target>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">&lt;code&gt;output_distribution='normal'&lt;/code&gt; 을 설정 하여 &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하여 데이터를 정규 분포에 매핑 할 수도 있습니다 . 홍채 데이터 세트와 함께 앞의 예제를 사용하여 :</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">교차 검증 반복자를 대신 전달하여 다른 교차 검증 전략을 사용할 수도 있습니다. 예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">아래 예와 같이 혼합 신호를 분리하는 데 일반적으로 사용됩니다 ( &lt;em&gt;블라인드 소스 분리&lt;/em&gt; 로 알려진 문제 ).</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">순방향 선택만큼 계산이 빠르며 일반 최소 제곱과 동일한 복잡도를 갖습니다.</target>
        </trans-unit>
        <trans-unit id="eaad2537722d5ddb17252eb65683de60a4e9ec00" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares.</source>
          <target state="translated">계산적으로 순방향 선택만큼 빠르며 일반 최소 제곱과 동일한 복잡성 순서를 갖습니다.</target>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">F 점수로 변환 된 다음 p- 값으로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">올가미와 같은 다른 추정기에 대한 솔루션을 생성하도록 쉽게 수정됩니다.</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">분류자가 뉴스 그룹 헤더와 같이 20 개의 뉴스 그룹 데이터에 나타나는 특정 내용에 쉽게 맞출 수 있습니다. 많은 분류 기준은 매우 높은 F 점수를 얻지 만 결과는이 시대에 속하지 않은 다른 문서로는 일반화되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f3493e2a2c4e4ad9e265942ad2fd137cc9804a32" translate="yes" xml:space="preserve">
          <source>It is generally recommended to avoid using significantly more processes or threads than the number of CPUs on a machine. Over-subscription happens when a program is running too many threads at the same time.</source>
          <target state="translated">일반적으로 시스템의 CPU 수보다 훨씬 많은 프로세스 또는 스레드를 사용하지 않는 것이 좋습니다. 초과 구독은 프로그램이 동시에 너무 많은 스레드를 실행할 때 발생합니다.</target>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">형상 수가 매우 많은 경우 다른 차원 축소 방법 (예 : 밀도가 높은 데이터의 경우 PCA 또는 희소 데이터의 경우 TruncatedSVD)을 사용하여 차원 수를 적절한 양 (예 : 50)으로 줄이는 것이 좋습니다. 이것은 약간의 노이즈를 억제하고 샘플 사이의 페어 단위 거리 계산 속도를 높입니다. 자세한 내용은 Laurens van der Maaten의 FAQ [2]를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 방법으로 모형을 학습 할 때 레이블이없는 데이터와 함께 레이블이없는 점에 식별자를 지정하는 것이 중요합니다 . 이 구현이 사용하는 식별자는 정수 값 \ (-1 \)입니다.</target>
        </trans-unit>
        <trans-unit id="643f8f6ee250eb138ea3f0ff80df853cb01ed9c1" translate="yes" xml:space="preserve">
          <source>It is important to keep in mind that the coefficients that have been dropped may still be related to the outcome by themselves: the model chose to suppress them because they bring little or no additional information on top of the other features. Additionnaly, this selection is unstable for correlated features, and should be interpreted with caution.</source>
          <target state="translated">삭제 된 계수는 여전히 그 자체로 결과와 관련이있을 수 있다는 점을 명심하는 것이 중요합니다. 모델은 다른 기능 위에 추가 정보를 거의 또는 전혀 제공하지 않기 때문에이를 억제하기로 선택했습니다. 또한이 선택은 상관 된 기능에 대해 불안정하므로주의해서 해석해야합니다.</target>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">샘플 수가 피처 수보다 훨씬 클 경우 수축이 필요하지 않을 것입니다. 이에 대한 직감은 모집단 공분산이 전체 순위 인 경우 표본 수가 증가하면 표본 공분산도 양의 명확한 값이된다는 것입니다. 결과적으로 수축이 필요하지 않으며 방법이 자동으로 수행해야합니다.</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">그것은 홍채에 대한 150 가지 관찰로 이루어지며, 각각 4 가지 특징, 즉 홍채와 꽃잎의 길이와 너비는 &lt;code&gt;iris.DESCR&lt;/code&gt; 에 자세히 설명되어 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">총 수에 비해 적은 수의 피처 만 선택하는 경우 (예 : 피처 수에 비해 샘플 수가 거의없는 경우) LassoCV보다 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">이러한 예측을 평가 지표에 전달하는 것은 적절하지 않습니다. &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt; 를 사용 하여 일반화 오류를 측정 하십시오 .</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">라이브러리에서 Parallel을 호출 할 때 백엔드 이름을 하드 코딩하지 않는 것이 좋습니다. 대신 라이브러리 사용자가 parallel_backend 컨텍스트 관리자를 사용하여 외부에서 백엔드를 변경할 수 있도록 소프트 힌트 (권장) 또는 하드 제한 조건 (필수)을 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">정규화 (벌칙)되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="3023247377e7882a0cbda1c2d8280926be6aa8ba" translate="yes" xml:space="preserve">
          <source>It is now possible to prune most tree-based estimators once the trees are built. The pruning is based on minimal cost-complexity. Read more in the &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;User Guide&lt;/a&gt; for details.</source>
          <target state="translated">이제 나무가 만들어지면 대부분의 나무 기반 추정기를 잘라내는 것이 가능합니다. 가지 치기는 최소한의 비용 복잡성을 기반으로합니다. 자세한 내용은 사용 &lt;a href=&quot;../../modules/tree#minimal-cost-complexity-pruning&quot;&gt;설명서&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">p &amp;gt;&amp;gt; n 인 문맥에서 수치 적으로 효율적입니다 (즉, 치수의 수가 점의 수보다 훨씬 큰 경우).</target>
        </trans-unit>
        <trans-unit id="3387956abcbcbc8c7f2e1b04a07247bbce69a743" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where the number of features is significantly greater than the number of samples.</source>
          <target state="translated">피처 수가 샘플 수보다 훨씬 많은 상황에서 수치 적으로 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">낮은 특이 값과 관련된 성분의 특이 벡터를 제거하여 대부분의 분산을 유지하는 저 차원 공간에 데이터를 투영하는 것이 종종 흥미 롭습니다.</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">최상의 &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 점수를 얻기 위해 하이퍼 파라미터 공간을 검색하는 것이 가능하며 권장됩니다 .</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">이진 화기의 임계 값을 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">평균화 대신 레이블 별 정밀도, 리콜, F1- 점수 및 지원을 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; 의사 난수 생성기 를 명시 적으로 시드하여 결과의 ​​재현성을 위해 난수를 제어 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">벡터화 생성자에 콜 러블을 전달하여 동작을 사용자 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;with_mean=False&lt;/code&gt; 또는 &lt;code&gt;with_std=False&lt;/code&gt; 를 &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; 생성자에 전달하여 센터링 또는 스케일링을 비활성화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">다음과 같이 범주 이름을 다시 가져올 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">학습 데이터에서 학습 한 변환의 정확한 특성을 찾기 위해 스케일러 속성을 조사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">로드 할 카테고리 목록을 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt; 함수 에 전달하여 카테고리의 하위 선택 만로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">동일한 실행에서 스파 스 배열과 밀도 배열을 혼합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="54adadb321f18cc462f6fa41bc0c4a25f1f6b29d" translate="yes" xml:space="preserve">
          <source>It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The &lt;code&gt;statsmodels
package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; natively supports this. Within sklearn, one could use bootstrapping instead as well.</source>
          <target state="translated">페널티없이 회귀하는 경우 계수에 대한 p- 값과 신뢰 구간을 얻을 수 있습니다. &lt;code&gt;statsmodels package &amp;lt;https://pypi.org/project/statsmodels/&amp;gt;&lt;/code&gt; 기본적으로이 작업을 지원합니다. sklearn 내에서 대신 부트 스트랩을 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&quot;해시 트릭&quot;(결합하여 그 한계를 극복 할 수 &lt;a href=&quot;#feature-hashing&quot;&gt;기능 해싱&lt;/a&gt; 에 의해 구현) &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; 의&lt;/a&gt; 클래스와 전처리 텍스트와의 토큰 기능 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Python의 내장 지속성 모델 &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; 을 사용하여 모델을 scikit-learn에 저장할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Python의 내장 지속성 모델, 즉 &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt; 을 사용하여 모델을 scikit-learn에 저장할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c1dfe5304fca32594b4f7b15a0ed1671355448d1" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">Python의 내장 지속성 모델, 즉 &lt;a href=&quot;https://docs.python.org/3/library/pickle.html&quot;&gt;pickle&lt;/a&gt; 을 사용하여 scikit-learn에 모델을 저장할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">매개 변수 &lt;code&gt;categories&lt;/code&gt; 사용하여이를 명시 적으로 지정할 수 있습니다 . 데이터 셋에는 성별, 대륙 4 개, 웹 브라우저 4 개가 있습니다.</target>
        </trans-unit>
        <trans-unit id="7291e604dadcc649d0d77ccb4ebf5e3c457ba713" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt;&lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.metrics.ConfusionMatrixDisplay&quot;&gt; &lt;code&gt;ConfusionMatrixDisplay&lt;/code&gt; &lt;/a&gt; 를 생성 하려면 &lt;a href=&quot;sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt; &lt;code&gt;plot_confusion_matrix&lt;/code&gt; &lt;/a&gt; 를 사용하는 것이 좋습니다 . 모든 매개 변수는 속성으로 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="ab8a32e7f4d09197b24de455e36e8e5fe3df1e84" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">시각화 &lt;a href=&quot;sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt; &lt;code&gt;plot_precision_recall_curve&lt;/code&gt; &lt;/a&gt; 를 만들려면 plot_precision_recall_curve 를 사용하는 것이 좋습니다 . 모든 매개 변수는 속성으로 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="80b150a43cd5aba26116528fb4f30933db56b582" translate="yes" xml:space="preserve">
          <source>It is recommend to use &lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt; to create a visualizer. All parameters are stored as attributes.</source>
          <target state="translated">시각화 &lt;a href=&quot;sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt; &lt;code&gt;plot_roc_curve&lt;/code&gt; &lt;/a&gt; 를 만들려면 plot_roc_curve 를 사용하는 것이 좋습니다 . 모든 매개 변수는 속성으로 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="f18c0fc60439524a8f745b0ef99ce87a71814897" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create a &lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt;. All parameters are stored as attributes.</source>
          <target state="translated">&lt;a href=&quot;sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt; &lt;code&gt;plot_partial_dependence&lt;/code&gt; &lt;/a&gt; 를 사용하여 &lt;a href=&quot;#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; &lt;/a&gt; 를 생성하는 것이 좋습니다 . 모든 매개 변수는 속성으로 저장됩니다.</target>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">다운 스트림 모델이 피쳐의 선형 독립성을 추가로 가정 할 수 있으므로 피쳐를 독립적으로 중앙에 배치하고 크기를 조정하는 것만으로는 충분하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b49f552a6383ab2c79a28eb8ae358eb905c8df59" translate="yes" xml:space="preserve">
          <source>It is sometimes tedious to find the model which will best perform on a given dataset. Stacking provide an alternative by combining the outputs of several learners, without the need to choose a model specifically. The performance of stacking is usually close to the best model and sometimes it can outperform the prediction performance of each individual model.</source>
          <target state="translated">주어진 데이터 세트에서 가장 잘 수행 될 모델을 찾는 것은 때때로 지루한 일입니다. 스태킹은 모델을 구체적으로 선택할 필요없이 여러 학습자의 출력을 결합하여 대안을 제공합니다. 스태킹의 성능은 일반적으로 최상의 모델에 가깝고 때로는 각 개별 모델의 예측 성능을 능가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">특정 트랜스포머의 상태를 다시 사용할 수 있기 때문에 때때로 상태를 저장하는 것이 좋습니다. &lt;code&gt;GridSearchCV&lt;/code&gt; 에서 파이프 라인을 사용하면 이러한 상황이 트리거됩니다. 따라서 인수 &lt;code&gt;memory&lt;/code&gt; 를 사용하여 캐싱을 활성화합니다.</target>
        </trans-unit>
        <trans-unit id="9d1619fcc011ef5a461992b135770b43d5982f12" translate="yes" xml:space="preserve">
          <source>It is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.</source>
          <target state="translated">사용자가 누락 된 값으로 인한 불확실성을 측정하는 데 관심이 없을 때 예측 및 분류의 맥락에서 단일 대 다중 대치가 얼마나 유용한 지에 대해서는 여전히 공개 된 문제입니다.</target>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">혼합 모델 학습을위한 가장 빠른 알고리즘입니다</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">큰 값이 클수록 반대입니다. 즉 큰 값은 이너에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="eec2b41e384c85c1e4587a1c1f6fa007f24ac337" translate="yes" xml:space="preserve">
          <source>It is the opposite as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">큰 값이 더 좋기 때문에 반대입니다. 즉, 큰 값은 inlier에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">각 트리가 다른 트리와 독립적으로 구축되므로 RandomForests와 ExtraTrees를 여러 코어에 병렬로 장착 할 수 있습니다. AdaBoost의 샘플은 순차적으로 구축되므로 여러 코어를 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="9876d3b6328cf0e41b8f18a6a35d45d86ad7b5f1" translate="yes" xml:space="preserve">
          <source>It is worth noting that more than 93% of policyholders have zero claims. If we were to convert this problem into a binary classification task, it would be significantly imbalanced, and even a simplistic model that would only predict mean can achieve an accuracy of 93%.</source>
          <target state="translated">보험 계약자의 93 % 이상이 클레임이 없습니다. 이 문제를 이진 분류 작업으로 변환하면 상당한 불균형이 발생하고 평균 만 예측하는 단순한 모델도 93 %의 정확도를 달성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="86441e9f0bfca4823b62f4a6d4cecce7c1b80a8e" translate="yes" xml:space="preserve">
          <source>It might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting &lt;code&gt;min_samples_leaf=5&lt;/code&gt; or &lt;code&gt;min_samples_leaf=10&lt;/code&gt;) so as to limit overfitting while not introducing too much underfitting.</source>
          <target state="translated">트리의 용량을 제한하여 (예를 들어 &lt;code&gt;min_samples_leaf=5&lt;/code&gt; 또는 &lt;code&gt;min_samples_leaf=10&lt;/code&gt; 을 설정하여) 테스트 세트에서 약간 더 나은 정확도를 위해 훈련 세트에서 약간의 정확도를 교환 하여 너무 도입하지 않으면 서 과적 합을 제한 할 수 있습니다. 훨씬 과소 적합.</target>
        </trans-unit>
        <trans-unit id="c1e3cdc828409a9c2610db18343fc252165aed81" translate="yes" xml:space="preserve">
          <source>It might seem questionable to use a (penalized) Least Squares loss to fit a classification model instead of the more traditional logistic or hinge losses. However in practice all those models can lead to similar cross-validation scores in terms of accuracy or precision/recall, while the penalized least squares loss used by the &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt;&lt;code&gt;RidgeClassifier&lt;/code&gt;&lt;/a&gt; allows for a very different choice of the numerical solvers with distinct computational performance profiles.</source>
          <target state="translated">보다 전통적인 물류 또는 힌지 손실 대신 분류 모델에 적합하도록 (페널티가 부여 된) 최소 제곱 손실을 사용하는 것은 의심스러워 보일 수 있습니다. 그러나 실제로 이러한 모든 모델은 정확도 또는 정밀도 / 재현율 측면에서 유사한 교차 검증 점수로 &lt;a href=&quot;generated/sklearn.linear_model.ridgeclassifier#sklearn.linear_model.RidgeClassifier&quot;&gt; &lt;code&gt;RidgeClassifier&lt;/code&gt; &lt;/a&gt; 수 있으며, RidgeClassifier에서 사용하는 불이익을받는 최소 제곱 손실을 사용 하면 고유 한 계산 성능 프로필을 사용하여 매우 다른 수치 솔버를 선택할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">전체 조각 별 선형 솔루션 경로를 생성하는데, 이는 교차 검증 또는 유사한 모델 조정 시도에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="dec67b5f65557893043d8c253cdd2dab65f3a96a" translate="yes" xml:space="preserve">
          <source>It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.</source>
          <target state="translated">모델의 독립 변수에 의해 설명 된 분산 비율 (y의)을 나타냅니다. 이는 적합도의 표시를 제공하므로 설명 된 분산의 비율을 통해 모델에서 보이지 않는 샘플을 얼마나 잘 예측할 수 있는지에 대한 척도를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">테스트 점수 외에 적합 시간, 점수 시간 (및 선택적으로 훈련 점수 및 적합 추정량)이 포함 된 dict를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">다음과 같은 속성을 가진 사전과 유사한 객체를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">&lt;code&gt;y&lt;/code&gt; 를 참조하여 &lt;code&gt;X&lt;/code&gt; 의 &lt;code&gt;estimator&lt;/code&gt; 예측 품질 을 정량화하는 부동 소수점 숫자를 반환합니다 . 다시 말하지만, 일반적으로 숫자가 높을수록 득점자가 손실을 반환하면 해당 값을 무시해야합니다.</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">Johnson-Lindenstrauss lemma는 데이터 세트의 구조에 대한 가정을하지 않기 때문에 필요한 수의 구성 요소를 매우 보수적으로 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="af7347a7c717add0101a2649bad5550dc47a184a" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 을 사용 하여 숫자 데이터 세트에서 SVM으로 분류하기 위해 RBF 커널의 기능 맵을 근사화하는 방법을 보여줍니다 . 원래 공간에서 선형 SVM을 사용한 결과, 근사 매핑을 사용한 선형 SVM 및 커널 화 된 SVM을 사용한 결과가 비교됩니다. 다양한 양의 몬테카를로 샘플링 ( 임의 푸리에 특성을 사용하는 &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 의 경우 ) 및 근사 매핑에 대한 훈련 세트 ( &lt;a href=&quot;../../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; 의 경우&lt;/a&gt; ) 의 다른 크기 하위 집합에 대한 타이밍 및 정확도 가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 을 사용 하여 숫자 데이터 세트에서 SVM으로 분류하기위한 RBF 커널의 기능 맵을 근사화하는 방법을 보여줍니다 . 원래 공간에서 선형 SVM을 사용한 결과, 근사 매핑을 사용하고 커널 화 된 SVM을 사용하는 선형 SVM을 비교합니다. 대략적인 매핑을 위해 다양한 양의 Monte Carlo 샘플링 ( &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 의 경우 임의 푸리에 기능을 사용 하는 경우 ) 및 트레이닝 세트의 다른 크기의 하위 집합 ( &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; )에 대한 타이밍 및 정확도 가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">텍스트 문서 모음을 토큰 발생 횟수 (또는 이진 발생 정보)를 보유하는 scipy.sparse 행렬로 변환합니다. norm = 'l1'인 경우 토큰 빈도로 정규화되거나 norm = 'l2'인 경우 유클리드 단위 영역에 투사 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">실수로 모델을 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">Halko 등의 방법으로 Full SVD의 LAPACK 구현 또는 임의 절단 된 SVD를 사용합니다. 입력 데이터의 모양과 추출 할 구성 요소의 수에 따라 2009 년.</target>
        </trans-unit>
        <trans-unit id="74ae47bdcf2723d7a82146ada9f167c02a150388" translate="yes" xml:space="preserve">
          <source>It will plot the class decision boundaries given by a Nearest Neighbors classifier when using the Euclidean distance on the original features, versus using the Euclidean distance after the transformation learned by Neighborhood Components Analysis. The latter aims to find a linear transformation that maximises the (stochastic) nearest neighbor classification accuracy on the training set.</source>
          <target state="translated">Neighborhood Components Analysis에서 학습 한 변환 후 Euclidean 거리를 사용하는 것과 비교하여 원래 기능에서 Euclidean 거리를 사용할 때 Nearest Neighbors 분류 기가 제공하는 클래스 결정 경계를 플로팅합니다. 후자는 훈련 세트에서 (확률 적) 가장 가까운 이웃 분류 정확도를 최대화하는 선형 변환을 찾는 것을 목표로합니다.</target>
        </trans-unit>
        <trans-unit id="80b76c72ce07f72ff1bcc8279eceffb68f3a73b2" translate="yes" xml:space="preserve">
          <source>It would be possible to get even higher predictive performance with a larger neural network but the training would also be significantly more expensive.</source>
          <target state="translated">더 큰 신경망으로 더 높은 예측 성능을 얻을 수 있지만 훈련 비용도 훨씬 더 비쌉니다.</target>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;return_X_y&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정하여 거의 모든 함수에서 데이터와 대상 만 포함하는 튜플로 출력을 제한 할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">커널 모양이 결과 분포의 부드러움에 어떤 영향을 미치는지 분명합니다. scikit-learn 커널 밀도 추정기는 다음과 같이 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="35600165765d17d14f9a53e3a40d6c087a8e15cc" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes.</source>
          <target state="translated">클러스터의 계층 적 병합을 나타내는 트리를 덴드로 그램으로 시각화 할 수 있습니다. 육안 검사는 데이터 구조를 이해하는 데 유용 할 수 있지만 표본 크기가 작은 경우에는 더욱 그렇습니다.</target>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">수렴 될 때까지 2와 3을 반복하십시오.</target>
        </trans-unit>
        <trans-unit id="e39adff24d3659cea88912062bf2425d11890a07" translate="yes" xml:space="preserve">
          <source>Iterative imputation of the missing values</source>
          <target state="translated">결 측값의 반복 대치</target>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">증거를 극대화하기위한 반복 절차</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">듀얼은</target>
        </trans-unit>
        <trans-unit id="ce6398892ce7bfa57c8075a52d29f534a23469a6" translate="yes" xml:space="preserve">
          <source>Its validation performance, measured via the \(R^2\) score, is significantly larger than the chance level. This makes it possible to use the &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt;&lt;code&gt;permutation_importance&lt;/code&gt;&lt;/a&gt; function to probe which features are most predictive:</source>
          <target state="translated">\ (R ^ 2 \) 점수를 통해 측정 된 유효성 검사 성능은 기회 수준보다 훨씬 큽니다. 이렇게하면 &lt;a href=&quot;generated/sklearn.inspection.permutation_importance#sklearn.inspection.permutation_importance&quot;&gt; &lt;code&gt;permutation_importance&lt;/code&gt; &lt;/a&gt; 함수 를 사용하여 가장 예측 가능한 기능을 조사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="07da5fcab12f57a49e864df0a2dcb43ec8cd118b" translate="yes" xml:space="preserve">
          <source>J&amp;oslash;rgensen, B. (1992). The theory of exponential dispersion models and analysis of deviance. Monografias de matem&amp;aacute;tica, no. 51. See also &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;Exponential dispersion model.&lt;/a&gt;</source>
          <target state="translated">J&amp;oslash;rgensen, B. (1992). 지수 분산 모델 이론 및 이탈도 분석. Monografias de matem&amp;aacute;tica, no. 51. &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_dispersion_model&quot;&gt;지수 분산 모델을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d7e5d74ebe16b65b422b57dd9089de563aa7d4b5" translate="yes" xml:space="preserve">
          <source>J. Cohen (1960). &amp;ldquo;A coefficient of agreement for nominal scales&amp;rdquo;. Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.</source>
          <target state="translated">J. Cohen (1960). &quot;공칭 척도에 대한 일치 계수&quot;. 교육 및 심리 측정 20 (1) : 37-46. doi : 10.1177 / 001316446002000104.</target>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;정밀 리콜과 ROC 곡선의 관계&lt;/a&gt; , ICML 2006.</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J. Friedman,&amp;ldquo;다변량 적응 회귀 스플라인&amp;rdquo;, 통계 연보 19 (1), 1-67, 1991 페이지.</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J. Friedman, Greedy Function Approximation : 그라디언트 부스팅 머신, 통계 분석, Vol. 29, No. 5, 2001.</target>
        </trans-unit>
        <trans-unit id="f06167e7b529cb39087bd6b97f521b456419a7cd" translate="yes" xml:space="preserve">
          <source>J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov. &amp;ldquo;Neighbourhood Components Analysis&amp;rdquo;. Advances in Neural Information Processing Systems. 17, 513-520, 2005. &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</source>
          <target state="translated">J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov. &amp;ldquo;주변 구성 요소 분석&amp;rdquo;. 신경 정보 처리 시스템의 발전. 17, 513-520, 2005. &lt;a href=&quot;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&quot;&gt;http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 : 희소 코딩을위한 온라인 사전 학습 ( &lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="8417a497b98a8d480d1cb9818d1f322bb7565268" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 : 스파 스 코딩을위한 온라인 사전 학습 ( &lt;a href=&quot;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;https://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman, H. Qin 및 R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;무료 오픈 소스 소프트웨어 패키지에서 단어 목록 중지&amp;rdquo;&lt;/a&gt; . 에서 &lt;em&gt;발동. NLP 오픈 소스 소프트웨어 워크숍&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="40bd5fef8dd5af699d79639063d7832cf1e45e49" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">J. Nothman, H. Qin 및 R. Yurchak (2018). &lt;a href=&quot;https://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;무료 오픈 소스 소프트웨어 패키지의 중지 단어 목록&amp;rdquo;&lt;/a&gt; . 에서 &lt;em&gt;발동. NLP 오픈 소스 소프트웨어 워크숍&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;멀티 클래스 AdaBoost&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">JR 퀸란. C4. 5 : 기계 학습을위한 프로그램. 1993 년 모건 카우프만</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;2 블록 사건에 중점을 둔 부분 최소 제곱 (PLS) 방법에 대한 조사&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">Jaccard 유사성 계수 점수</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">Jacob A. Wegelin. 2 블록 경우에 중점을 둔 부분 최소 제곱 (PLS) 방법에 대한 조사. 기술 보고서 ​​371, 시애틀 워싱턴 대학 통계학과, 2000.</target>
        </trans-unit>
        <trans-unit id="b9a5b6145a558ec82725430f200fe46fa35f6aac" translate="yes" xml:space="preserve">
          <source>Jarvelin, K., &amp;amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4), 422-446.</source>
          <target state="translated">Jarvelin, K., &amp;amp; Kekalainen, J. (2002). IR 기술의 누적 이득 기반 평가. 정보 시스템에 대한 ACM 거래 (TOIS), 20 (4), 422-446.</target>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">Jesse Read, Bernhard Pfahringer, Geoff Holmes, 아이브 프랭크,</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,&amp;ldquo;다중 분류 분류를위한 분류기 체인&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="43121fdb3391941437c9f79e4fe2a93d4f69bed7" translate="yes" xml:space="preserve">
          <source>Joblib also tries to limit the oversubscription by limiting the number of threads usable in some third-party library threadpools like OpenBLAS, MKL or OpenMP. The default limit in each worker is set to &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; but this limit can be overwritten with the &lt;code&gt;inner_max_num_threads&lt;/code&gt; argument which will be used to set this limit in the child processes.</source>
          <target state="translated">Joblib는 또한 OpenBLAS, MKL 또는 OpenMP와 같은 일부 타사 라이브러리 스레드 풀에서 사용할 수있는 스레드 수를 제한하여 초과 구독을 제한하려고합니다. 각 작업자의 기본 제한은 &lt;code&gt;max(cpu_count() // effective_n_jobs, 1)&lt;/code&gt; 되지만이 제한은 자식 프로세스에서이 제한을 설정하는 데 사용되는 &lt;code&gt;inner_max_num_threads&lt;/code&gt; 인수 로 덮어 쓸 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bbd429df4e4127b1956a8217cca140ac1f4576de" translate="yes" xml:space="preserve">
          <source>Joblib is able to support both multi-processing and multi-threading. Whether joblib chooses to spawn a thread or a process depends on the &lt;strong&gt;backend&lt;/strong&gt; that it&amp;rsquo;s using.</source>
          <target state="translated">Joblib는 다중 처리 및 다중 스레딩을 모두 지원할 수 있습니다. joblib가 스레드 또는 프로세스를 생성할지 여부는 사용중인 &lt;strong&gt;백엔드에&lt;/strong&gt; 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="dbbc7351b3326fa09a2af62a2a8c482a7f498e4a" translate="yes" xml:space="preserve">
          <source>Joblib is currently unable to avoid oversubscription in a multi-threading context. It can only do so with the &lt;code&gt;loky&lt;/code&gt; backend (which spawns processes).</source>
          <target state="translated">Joblib는 현재 다중 스레딩 컨텍스트에서 초과 구독을 피할 수 없습니다. 프로세스를 생성 하는 &lt;code&gt;loky&lt;/code&gt; 백엔드 에서만 가능 합니다.</target>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">요한나 하딘, 데이비드 엠 로크 강력한 거리 분포. 전산 및 그래픽 통계 저널. 2005 년 12 월 1 일, 14 (4) : 928-946.</target>
        </trans-unit>
        <trans-unit id="3cd3820aa7670cf9157f83a7b518b28ca957a3fc" translate="yes" xml:space="preserve">
          <source>John K. Dixon, &amp;ldquo;Pattern Recognition with Partly Missing Data&amp;rdquo;, IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue: 10, pp. 617 - 621, Oct. 1979. &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;http://ieeexplore.ieee.org/abstract/document/4310090/&lt;/a&gt;</source>
          <target state="translated">John K. Dixon, &quot;부분적으로 누락 된 데이터가있는 패턴 인식&quot;, IEEE Transactions on Systems, Man 및 Cybernetics, Volume : 9, Issue : 10, pp. 617-621, Oct. 1979. &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/4310090/&quot;&gt;http://ieeexplore.ieee. 조직 / 추상 / 문서 / 4310090 /&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">남자. D. Kelleher, Brian Mac Namee, Aoife D' Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;예측 데이터 분석을위한 머신 러닝의 기초 : 알고리즘, 실제 사례 및 사례 연구&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">남자. D. Kelleher, Brian Mac Namee, Aoife D' Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;예측 데이터 분석을위한 기계 학습의 기초 : 알고리즘,&lt;/a&gt; 실제 사례 및 사례 연구 , 2015.</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">멀티 태스킹 올가미를 통한 공동 피처 선택</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">조인트 파라미터 선택</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">기계 학습 연구 저널 15 (10 월) : 3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7eaac587d1f40d409b66183976f554af82049338" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">Journal of Machine Learning Research 15 (10 월) : 3221-3245, 2014. &lt;a href=&quot;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">1988 년 7 월</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">칠월; 1998</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman, Riccadonna, Furlanello, (2012). 멀티 클래스 예측에서 MCC와 CEN 에러 측정의 비교</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">교육, 사전 처리 (예 : 표준화, 기능 선택 등) 및 유사한 &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;데이터 변환&lt;/a&gt; 에서 보류 된 데이터에 대해 예측 변수를 테스트하는 것이 중요 하듯이 마찬가지로 트레이닝 세트에서 학습하고 예측을 위해 보류 데이터에 적용해야합니다. :</target>
        </trans-unit>
        <trans-unit id="bb93e8a53cb9de4ad35209178664abb04cf0e5f7" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">훈련에서 보류 된 데이터에 대해 예측자를 테스트하는 것이 중요 하듯이, 전처리 (예 : 표준화, 특징 선택 등) 및 유사한 &lt;a href=&quot;https://scikit-learn.org/0.23/data_transforms.html#data-transforms&quot;&gt;데이터 변환&lt;/a&gt; 도 훈련 세트에서 학습하고 예측을 위해 보류 된 데이터에 적용해야합니다. :</target>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue (a in b)와 같지만 더 좋은 기본 메시지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">self.assertTrue (a가 아닌 b)와 비슷하지만 더 좋은 기본 메시지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inen 및 S. &amp;Auml;yr&amp;auml;m&amp;ouml; : &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;강력한 데이터 마이닝을위한 공간 중앙값 계산&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K (X, Y) = &amp;lt;X, Y&amp;gt; / (|| X || * || Y ||)</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">K- 폴드 교차 검증기</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="57bc3e1ca5db2c1c7f140e0c9654a7e1bd0c4cd4" translate="yes" xml:space="preserve">
          <source>K-Means clustering.</source>
          <target state="translated">K- 평균 클러스터링.</target>
        </trans-unit>
        <trans-unit id="4dcab3f446cd66684df43251a7a52921a5b665f1" translate="yes" xml:space="preserve">
          <source>K-dimensional tree for fast generalized N-point problems.</source>
          <target state="translated">빠른 일반화 된 N 점 문제를위한 K 차원 트리.</target>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">겹치지 않는 그룹이있는 K- 폴드 반복기 변형.</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K- 평균 군집화</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">사용할 K- 평균 알고리즘. 고전적인 EM 스타일 알고리즘은 &quot;풀&quot;입니다. &amp;ldquo;elkan&amp;rdquo;변형은 삼각형 부등식을 사용하여보다 효율적이지만 현재 희소 데이터는 지원하지 않습니다. &quot;auto&quot;는 밀도가 높은 데이터에 대해서는 &quot;elkan&quot;을 선택하고 희소 데이터에 대해서는 &quot;full&quot;을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="c00998c8eabed1e931fa81c6f69faf59aad07506" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient on data with well-defined clusters, by using the triangle inequality. However it&amp;rsquo;s more memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).</source>
          <target state="translated">사용할 K- 평균 알고리즘입니다. 고전적인 EM 스타일 알고리즘은 &quot;full&quot;입니다. &quot;elkan&quot;변형은 삼각형 부등식을 사용하여 클러스터가 잘 정의 된 데이터에서 더 효율적입니다. 그러나 추가 형태 배열 (n_samples, n_clusters) 할당으로 인해 메모리 집약적입니다.</target>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">K- 평균은 벡터 양자화에 사용될 수있다. 이것은 훈련 된 &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt; 모델의 변환 방법을 사용하여 달성됩니다 .</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K- 평균 군집 알고리즘.</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K- 평균은 작고 동일한 대각선 공분산 행렬을 사용하는 기대 최대화 알고리즘과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K- 평균은 종종 로이드의 알고리즘으로 지칭된다. 기본적으로 알고리즘에는 세 단계가 있습니다. 첫 번째 단계는 초기 중심을 선택하며 가장 기본적인 방법은 데이터 세트 \ (X \)에서 \ (k \) 샘플을 선택하는 것입니다. 초기화 후 K- 평균은 다른 두 단계 사이에서 반복됩니다. 첫 번째 단계는 각 샘플을 가장 가까운 중심에 할당합니다. 두 번째 단계는 각각의 이전 중심에 할당 된 모든 샘플의 평균값을 취하여 새로운 중심을 생성합니다. 이전 중심과 새로운 중심 사이의 차이가 계산되고 알고리즘은이 값이 임계 값보다 작을 때까지 마지막 두 단계를 반복합니다. 즉, 중심이 크게 움직이지 않을 때까지 반복됩니다.</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K- 평균 양자화</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">빠른 일반화 된 N- 포인트 문제를위한 KDTree</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree (X, leaf_size = 40, metric = 'minkowski', ** kwargs)</target>
        </trans-unit>
        <trans-unit id="e6f48940c5e34202cb03c9567fde221973b85eb8" translate="yes" xml:space="preserve">
          <source>KNN Based Imputation</source>
          <target state="translated">KNN 기반 대치</target>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">Kappa 점수는 이진 또는 멀티 클래스 문제에 대해 계산할 수 있지만 다중 레이블 문제 (레이블 당 점수를 수동으로 계산하는 경우 제외)에는 적용되지 않으며 두 개 이상의 어노 테이터에는 적용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="7642aa421288e310d04c4d2f1c88ac24e935cd02" translate="yes" xml:space="preserve">
          <source>Ke et. al. &lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&amp;ldquo;LightGBM: A Highly Efficient Gradient BoostingDecision Tree&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">Ke et. al. &lt;a href=&quot;https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree&quot;&gt;&quot;LightGBM : 고효율 그라디언트 부스팅 결정 트리&quot;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">3 개의 RGB 채널을 단일 회색 레벨 채널로 평균하는 대신 유지하십시오. 색상이 True 인 경우 데이터의 모양은 color = False 인 모양보다 차원이 하나 더 있습니다.</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">Ken Tang과 Ponnuthurai N. Suganthan 및 Xi Yao 및 A. Kai Qin. 관련 가중치 LDA를 사용한 선형 차원 감소 전기 전자 공학부 남양 기술 대학교 2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">종 분포의 커널 밀도 추정</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">커널 밀도 추정</target>
        </trans-unit>
        <trans-unit id="9837c7505d0f3a8c028c3a0430177597bb56e3e2" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation.</source>
          <target state="translated">커널 밀도 추정.</target>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">커널 PCA</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">커널 PCA는 다음에서 소개되었습니다.</target>
        </trans-unit>
        <trans-unit id="2064482c4c2332e23a8df06a915448e7e780cd46" translate="yes" xml:space="preserve">
          <source>Kernel Principal Component Analysis.</source>
          <target state="translated">커널 주성분 분석.</target>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">커널 주성분 분석 (KPCA)</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">커널 대역폭.</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">'rbf', 'poly'및 'sigmoid'에 대한 커널 계수.</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">rbf 커널에 대한 커널 계수.</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">rbf, poly 및 sigmoid 커널에 대한 커널 계수. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">rbf, poly, sigmoid, laplacian 및 chi2 커널에 대한 커널 계수. &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; 에 대해 무시되었습니다 .</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">scikit-learn의 커널 밀도 추정은 &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt; 추정기 에서 구현되며, 효율적인 쿼리에 볼 트리 또는 KD 트리를 사용합니다 (자세한 내용은 &lt;a href=&quot;neighbors#neighbors&quot;&gt;가장 가까운 이웃&lt;/a&gt; 참조 ). 위의 예에서는 단순성을 위해 1D 데이터 세트를 사용하지만 실제로 차원의 저주로 인해 높은 차원에서 성능이 저하되지만 커널 밀도 추정은 여러 차원에서 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">로그-마진 가능성이 평가되는 커널 하이퍼 파라미터. None이면 self.kernel_.theta의 사전 계산 된 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">로그-마진 가능성이 평가되는 커널 하이퍼 파라미터. 멀티 클래스 분류의 경우, 세타는 복합 커널 또는 개별 커널의 하이퍼 파라미터 일 수 있습니다. 후자의 경우 모든 개별 커널에 동일한 세타 값이 할당됩니다. None이면 self.kernel_.theta의 사전 계산 된 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">커널 k (X, Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">대략적인 커널 맵. callable은 두 개의 인수와이 객체에 kernel_params로 전달 된 키워드 인수를 허용해야하며 부동 소수점 숫자를 리턴해야합니다.</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">내부적으로 사용되는 커널 매핑. callable은 두 개의 인수와이 객체에 kernel_params로 전달 된 키워드 인수를 허용해야하며 부동 소수점 숫자를 리턴해야합니다. 사전 계산 된 커널 매트릭스를 샘플 대신 추정기 메소드로 전달하려면 &quot;사전 계산&quot;으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="438349cc7f220c5c2d499eba1d3d6c5414057ba4" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. This parameter is directly passed to &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is a string, it must be one of the metrics in &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt;. If &lt;code&gt;kernel&lt;/code&gt; is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if &lt;code&gt;kernel&lt;/code&gt; is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.</source>
          <target state="translated">내부적으로 사용되는 커널 매핑. 이 매개 변수는 &lt;code&gt;sklearn.metrics.pairwise.pairwise_kernel&lt;/code&gt; 에 직접 전달됩니다 . 경우 &lt;code&gt;kernel&lt;/code&gt; 문자열, 그것의 측정 기준 중 하나 여야합니다 &lt;code&gt;pairwise.PAIRWISE_KERNEL_FUNCTIONS&lt;/code&gt; . 경우 &lt;code&gt;kernel&lt;/code&gt; &quot;미리 계산&quot;되어, X는 커널 행렬로 간주됩니다. 또는 &lt;code&gt;kernel&lt;/code&gt; 이 호출 가능한 함수 인 경우 각 인스턴스 쌍 (행)에서 호출되고 결과 값이 기록됩니다. 콜 러블은 X에서 두 행을 입력으로 가져와 해당 커널 값을 단일 숫자로 반환해야합니다. 즉, &lt;a href=&quot;../classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; 의&lt;/a&gt; 콜 러블 은 단일 샘플이 아닌 행렬에서 작동하므로 허용되지 않습니다. 대신 커널을 식별하는 문자열을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">커널 매트릭스.</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">벡터 시스템 지원 또는 커널 PCA와 같은 커널 방법은 커널 힐버트 공간을 재현하는 속성에 의존합니다. 긍정적 인 확실한 커널 함수 \ (k \) (소위 Mercer 커널)의 경우, Hilbert 공간 \ (\ mathcal {H} \)에 대한 \ (\ phi \) 매핑이 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">대체 차원 공간으로 데이터를 투사하는 커널 방법</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">커널 운영자는 하나 또는 두 개의 기본 커널을 가져 와서 새로운 커널로 결합합니다. &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; 커널 개의 커널을 얻어 \ (K1 \)와 \ (K2 \)와 비아 콤바인들을 \ (K_ {합} (X, Y) = (K1) (X, Y) + K2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt; 커널이 커널을한다 \ (K1 \)와 \ (K2 \)와 통해 콤바인을 \ (K_ {제품} (X, Y) = K1 (X, Y) * K2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt; 커널이 하나의 기본 커널 통해 스칼라 파라미터 \ (지수 \) 및 콤바인들을 얻어 \ (K_ {EXP} (X, Y) = K (X, Y) ^ \ 텍스트 {지수} \).</target>
        </trans-unit>
        <trans-unit id="6d4564c4032221fcc796901a96d1a6cf81d6aa2f" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k_1\) and \(k_2\) and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(p\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^p\). Note that magic methods &lt;code&gt;__add__&lt;/code&gt;, &lt;code&gt;__mul___&lt;/code&gt; and &lt;code&gt;__pow__&lt;/code&gt; are overridden on the Kernel objects, so one can use e.g. &lt;code&gt;RBF() + RBF()&lt;/code&gt; as a shortcut for &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt;.</source>
          <target state="translated">커널 연산자는 하나 또는 두 개의 기본 커널을 가져 와서 새 커널로 결합합니다. &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; 커널 개의 커널을 얻어 \ (k_1 \)와 \ (k_2 \)와 비아 콤바인들을 \ (K_ {합} (X, Y) = k_1 (X, Y) + k_2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt; 커널이 커널을한다 \ (k_1 \)와 \ (k_2 \)와 통해 콤바인을 \ (K_ {제품} (X, Y) = k_1 (X, Y) * k_2 (X, Y) \). &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt; 커널이 하나의 기본 커널 \ 스칼라 파라미터를 통해 \ (p \) 및 콤바인들을 취 (EXP K_ {} (X, Y) = K (X, Y) P ^ \). 매직 메서드 &lt;code&gt;__add__&lt;/code&gt; , &lt;code&gt;__mul___&lt;/code&gt; 및 &lt;code&gt;__pow__&lt;/code&gt; 는 커널 객체에서 재정의되므로 &lt;code&gt;Sum(RBF(), RBF())&lt;/code&gt; 대한 바로 가기로 &lt;code&gt;RBF() + RBF()&lt;/code&gt; 를 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;KRR&lt;/a&gt; (Kernel Ridge Regression) [M2012] 는 &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (선형 최소 제곱과 l2-norm 정규화)을 커널 트릭과 결합합니다. 따라서 각 커널과 데이터에 의해 유도 된 공간에서 선형 함수를 학습합니다. 비선형 커널의 경우 원래 공간의 비선형 함수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="3fcbd037e0e31c66e26fa73fca7d9588d56b4cd8" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge regression and classification&lt;/a&gt; (linear least squares with l2-norm regularization) with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;kernel trick&lt;/a&gt;. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">Kernel ridge regression ( &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;KRR&lt;/a&gt; ) [M2012] 는 &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge 회귀 및 분류&lt;/a&gt; (l2-norm 정규화를 사용한 선형 최소 제곱)를 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_method&quot;&gt;커널 트릭&lt;/a&gt; 과 결합합니다 . 따라서 각 커널과 데이터에 의해 유도 된 공간에서 선형 함수를 학습합니다. 비선형 커널의 경우 이것은 원래 공간의 비선형 함수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">KRR (Kernel ridge Regression)은 릿지 회귀 (리니어 최소 제곱과 l2-norm 정규화)를 커널 트릭과 결합합니다. 따라서 각 커널과 데이터에 의해 유도 된 공간에서 선형 함수를 학습합니다. 비선형 커널의 경우 원래 공간의 비선형 함수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">커널 능선 회귀는 능선 회귀와 커널 트릭을 결합</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">커널 능선 회귀</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">모델에 사용할 커널 : 선형, 다항식, RBF, 시그 모이 드 또는 사전 계산.</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">모델에 사용할 커널 : 선형, 다항식, RBF, 시그 모이 드 또는 사전 계산. 기본적으로 'rbf'.</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">다른 커널 세트로 구성된 커널.</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">핵심. 기본값은&amp;ldquo;선형&amp;rdquo;입니다.</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">커널 (GP와 관련하여 &quot;공분산 함수&quot;라고도 함)은 GP의 앞뒤에있는 모양을 결정하는 GP의 중요한 구성 요소입니다. 이들은 유사한 데이터 포인트가 유사한 목표 값을 가져야한다는 가정과 결합 된 두 데이터 포인트의 &quot;유사성&quot;을 정의하여 학습중인 기능에 대한 가정을 인코딩합니다. 두 가지 범주의 커널을 구별 할 수 있습니다. 고정 커널은 두 데이터 포인트의 거리에만 의존하고 절대 값 \ (k (x_i, x_j) = k (d (x_i, x_j)) \)가 아니라 변환에 따라 변하지 않습니다 고정되지 않은 커널은 데이터 포인트의 특정 값에도 의존합니다. 정지형 커널은 등방성 및 이방성 커널로 세분 될 수 있으며, 등방성 커널도 ​​입력 공간의 회전에 영향을받지 않습니다. 상세 사항은,우리는 4 장을 참조한다&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98cdf159fe1dcd2fc6aef984c01878bfe7d52c10" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;. For guidance on how to best combine different kernels, we refer to &lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]&lt;/a&gt;.</source>
          <target state="translated">커널 (GP의 맥락에서 &quot;공분산 함수&quot;라고도 함)은 GP의 이전 및 이후 모양을 결정하는 GP의 중요한 요소입니다. 유사한 데이터 포인트가 유사한 목표 값을 가져야한다는 가정과 결합 된 두 데이터 포인트의 &quot;유사성&quot;을 정의하여 학습중인 기능에 대한 가정을 인코딩합니다. 두 가지 범주의 커널을 구별 할 수 있습니다. 고정 커널은 절대 값이 아닌 두 데이터 포인트의 거리에만 의존하므로 변환에 불변합니다. 입력 공간에서 비정상 커널은 데이터 포인트의 특정 값에도 의존합니다. 고정 커널은 등방성 커널과 이방성 커널로 더 세분화 될 수 있으며, 여기서 등방성 커널은 입력 공간의 회전에도 변하지 않습니다. 상세 사항은,우리는 4 장을 참조하십시오&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt; . 서로 다른 커널을 가장 잘 결합하는 방법에 대한 지침은 &lt;a href=&quot;#duv2014&quot; id=&quot;id6&quot;&gt;[Duv2014]를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">커널 &lt;code&gt;b&lt;/code&gt; 는 객체 &lt;code&gt;a&lt;/code&gt; 와 b 가 객체 &lt;code&gt;a&lt;/code&gt; 와 &lt;code&gt;c&lt;/code&gt; 보다&amp;ldquo;더 유사한&amp;rdquo;것으로 간주 되는 경우 유사성의 척도입니다 &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; 예 : s (a, b)&amp;gt; s (a, c)) . 커널은 또한 양의 반 한정이어야합니다.</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">커널은 벡터 \ (\ theta \) 하이퍼 파라미터로 매개 변수화됩니다. 이러한 하이퍼 파라미터는 예를 들어 커널의 길이 또는주기를 제어 할 수 있습니다 (아래 참조). 모든 커널 은 &lt;code&gt;__call__&lt;/code&gt; 메소드 에서 &lt;code&gt;eval_gradient=True&lt;/code&gt; 설정을 통해 \ (\ theta \)와 관련하여 커널의 자동 공분산 분석 그라디언트 계산을 지원합니다 . 이 그라디언트는 로그-마진 우도의 그라디언트를 계산할 때 가우시안 프로세스 (회귀 및 분류기)에 의해 사용되며, 이는 \ (\ theta \)의 값을 결정하는 데 사용됩니다. 그래디언트 상승을 통한 가능성. 각 하이퍼 파라미터에 대해 커널 인스턴스를 만들 때 초기 값과 범위를 지정해야합니다. \ (\ theta \)의 현재 값은 속성을 통해 가져오고 설정할 수 있습니다 &lt;code&gt;theta&lt;/code&gt; 커널 객체의 세타 . 또한 하이퍼 파라미터의 &lt;code&gt;bounds&lt;/code&gt; 는 커널 의 속성 범위 에 의해 액세스 될 수 있습니다 . 두 속성 (세타 및 경계)은 일반적으로 그래디언트 기반 최적화에 더 적합하기 때문에 내부적으로 사용되는 값의 로그 변환 된 값을 반환합니다. 각 hyperparameter의 사양의 인스턴스의 형태로 저장된다 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt; 각 커널이다. 이름이 &quot;x&quot;인 하이퍼 파라미터를 사용하는 커널에는 self.x 및 self.x_bounds 속성이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="4aed51cbfb6629b3c22c46504375ed74bc60a033" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">커널은 하이퍼 파라미터의 벡터 \ (\ theta \)로 매개 변수화됩니다. 이러한 하이퍼 파라미터는 예를 들어 커널의 길이 스케일 또는 주기성을 제어 할 수 있습니다 (아래 참조). 모든 커널 은 &lt;code&gt;__call__&lt;/code&gt; 메서드 에서 &lt;code&gt;eval_gradient=True&lt;/code&gt; 를 설정하여 \ (\ theta \)에 대한 커널의 자기 공분산의 분석적 기울기 계산을 지원합니다 . 이 그래디언트는 로그 마진 가능성의 그래디언트를 계산할 때 가우스 프로세스 (회귀 및 분류기 모두)에 의해 사용되며, 로그 마진 가능성을 최대화하는 \ (\ theta \)의 값을 결정하는 데 사용됩니다. 기울기 상승을 통한 가능성. 각 하이퍼 파라미터에 대해 커널 인스턴스를 만들 때 초기 값과 경계를 지정해야합니다. \ (\ theta \)의 현재 값은 속성을 통해 가져오고 설정할 수 있습니다. &lt;code&gt;theta&lt;/code&gt; 커널 객체의 세타 . 또한 하이퍼 파라미터의 &lt;code&gt;bounds&lt;/code&gt; 는 커널 의 속성 경계 에 의해 액세스 할 수 있습니다 . 두 속성 (세타 및 경계)은 일반적으로 그래디언트 기반 최적화에 더 적합하기 때문에 내부적으로 사용 된 값의 로그 변환 값을 반환합니다. 각 hyperparameter의 사양의 인스턴스의 형태로 저장된다 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt; 각 커널이다. 이름이 &quot;x&quot;인 하이퍼 파라미터를 사용하는 커널에는 self.x 및 self.x_bounds 속성이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">Kevin P. Murphy&amp;ldquo;기계 학습 : 확률 론적 관점&amp;rdquo;, MIT Press chapter 14.4.3, pp. 492-493</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">키는 &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; &lt;/a&gt; 로 전달할 수있는 매개 변수 이름입니다 .</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">키워드 인수를 사용하면 이러한 기본값을 특정 데이터 세트에 맞출 수 있습니다 (매개 변수 &lt;code&gt;target_name&lt;/code&gt; , &lt;code&gt;data_name&lt;/code&gt; , &lt;code&gt;transpose_data&lt;/code&gt; 및 아래 예 참조).</target>
        </trans-unit>
        <trans-unit id="6a687df4f73e66be23d8d5cd9810da872c9b92e2" translate="yes" xml:space="preserve">
          <source>Keyword arguments passed to the coordinate descent solver.</source>
          <target state="translated">좌표 하강 솔버에 전달 된 키워드 인수입니다.</target>
        </trans-unit>
        <trans-unit id="ee035295632767669037b1fd1546556e8af6cebd" translate="yes" xml:space="preserve">
          <source>Keyword arguments to be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="translated">matplotlib의 &lt;code&gt;plot&lt;/code&gt; 에 전달할 키워드 인수 입니다.</target>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">지정된 메트릭 함수에 전달할 키워드 인수</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola 및 Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;대규모 멀티 태스킹 학습을위한 기능 해싱&lt;/a&gt; . Proc. ICML.</target>
        </trans-unit>
        <trans-unit id="aaf2909b07b71367a7207c2f93060ee37cc58e6c" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola 및 Josh Attenberg (2009). &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;대규모 멀티 태스킹 학습을위한 기능 해싱&lt;/a&gt; . Proc. ICML.</target>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma, Diederik 및 Jimmy Ba. &amp;ldquo;아담 : 확률 론적 방법</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger, Y., Basri, R., Chang, JT, &amp;amp; Gerstein, M. (2003). 마이크로 어레이 데이터의 스펙트럼 biclustering : coclustering 유전자 및 조건. 게놈 연구, 13 (4), 703-716.</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">Kluger, Yuval 등 al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;microarray 데이터의 스펙트럼 biclustering : 유전자와 조건을 coclustering&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">샘플 수만 알면 &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt; 은 랜덤 투영에 의해 발생하는 경계 왜곡을 보장하기 위해 랜덤 서브 스페이스의 최소 크기를 보수적으로 추정합니다.</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">코비 크 래머, 요람 싱어. 멀티 클래스 커널 기반 벡터 머신의 알고리즘 구현 기계 학습 연구 2 (2001), 265-292</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">최적화 후 Kullback-Leibler 발산.</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS는 함수의 2 차 부분 미분을 나타내는 헤 시안 행렬에 근사한 솔버입니다. 또한 Hessian 행렬의 역수에 근사하여 매개 변수 업데이트를 수행합니다. 구현은 Scipy 버전의 &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS를 사용&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="554ef38240e48c4335936815621409310d3aac71" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS는 함수의 2 차 편도 함수를 나타내는 헤 시안 행렬을 근사하는 솔버입니다. 또한 매개 변수 업데이트를 수행하기 위해 헤세 행렬의 역을 근사합니다. 구현은 Scipy 버전의 &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS를 사용&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="7bcf28acc035a046a1884f5a68a7e0642aed3a3f" translate="yes" xml:space="preserve">
          <source>L-BFGS-B &amp;ndash; Software for Large-scale Bound-constrained Optimization</source>
          <target state="translated">L-BFGS-B &amp;ndash; 대규모 바운드 제약 최적화를위한 소프트웨어</target>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L. Breiman,&amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, 123-140, 1996 쪽.</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L. Breiman,&amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24 (2), 123-140, 1996.</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L. Breiman.</target>
        </trans-unit>
        <trans-unit id="97e482bcc046e44b1e543a4a852a328e802cd962" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001. &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</source>
          <target state="translated">L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45 (1), 5-32, 2001. &lt;a href=&quot;https://doi.org/10.1023/A:1010933404324&quot;&gt;https://doi.org/10.1023/A:1010933404324&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman, J. Friedman, R. Olshen 및 C. Stone,&amp;ldquo;분류 및 회귀 트리&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman, J. Friedman, R. Olshen 및 C. Stone. 분류 및 회귀 트리. 1984 년 캘리포니아 벨몬트의 워즈워스</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;하위 모델 선택 및 회귀 평가 : X- 랜덤 사례&lt;/a&gt; , 국제 통계 검토 1992;</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman 및 A. Cutler,&amp;ldquo;랜덤 포레스트&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c982df17d29f8aba32aa6a03bfa726201c066e60" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman 및 A. Cutler, &quot;Random Forests&quot;, &lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ecce2961bb8a3f0ec10fc321ad8811dfc6312ac" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector&amp;rdquo;, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko, NN Leonenko, &quot;무작위 벡터 엔트로피의 샘플 추정치&quot;, Probl. Peredachi Inf., 23 : 2 (1987), 9-16</target>
        </trans-unit>
        <trans-unit id="d97aac3a80efb6d42fc11cefc484a2c681583627" translate="yes" xml:space="preserve">
          <source>L. F. Kozachenko, N. N. Leonenko, &amp;ldquo;Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16</source>
          <target state="translated">LF Kozachenko, NN Leonenko,&amp;ldquo;무작위 벡터 엔트로피의 샘플 추정치 :, Probl. Peredachi Inf., 23 : 2 (1987), 9-16</target>
        </trans-unit>
        <trans-unit id="f170f61c9bead94cf287d881c88071c0e3a5501e" translate="yes" xml:space="preserve">
          <source>L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</source>
          <target state="translated">L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01908075&quot;&gt;https://link.springer.com/article/10.1007%2FBF01908075&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9026b644be7a2edc54521dca8f44e2af501befa2" translate="yes" xml:space="preserve">
          <source>L. Mosley, &lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;A balanced approach to the multi-class imbalance problem&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">L. Mosley, &lt;a href=&quot;https://lib.dr.iastate.edu/etd/13537/&quot;&gt;다중 클래스 불균형 문제에 대한 균형 잡힌 접근&lt;/a&gt; , IJCV 2010.</target>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">Robert C. Moore, John DeNero의 멀티 클래스 힌지 손실 모델에 대한 L1 및 L2 정규화.</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">로지스틱 회귀 분석에서의 L1 페널티 및 희소성</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">L1 규범 : \ (R (w) : = \ sum_ {i = 1} ^ {n} | w_i | \), 드문 드문 솔루션으로 이어집니다.</target>
        </trans-unit>
        <trans-unit id="b5bea6bb158694d1df1789af92353788b4accacc" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse solutions.</source>
          <target state="translated">L1 규범 : \ (R (w) : = \ sum_ {j = 1} ^ {m} | w_j | \), 희소 해로 이어집니다.</target>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">L2 규범 : \ (R (w) : = \ fra {{1} {2} \ sum_ {i = 1} ^ {n} w_i ^ 2 \),</target>
        </trans-unit>
        <trans-unit id="2f4d6f15c348b07a6c4412e8ecac45c72eb1770b" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),</source>
          <target state="translated">L2 표준 : \ (R (w) : = \ frac {1} {2} \ sum_ {j = 1} ^ {m} w_j ^ 2 = || w || _2 ^ 2 \),</target>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">L2 페널티 (규정 화 용어) 매개 변수.</target>
        </trans-unit>
        <trans-unit id="512ddf6d4bbcf9517a6def433a9acfdaefb1e3cd" translate="yes" xml:space="preserve">
          <source>LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This reduces the log posterior to:</source>
          <target state="translated">LDA는 QDA의 특별한 경우로, 각 클래스의 가우스는 모든 \ (k \)에 대해 동일한 공분산 행렬 \ (\ Sigma_k = \ Sigma \)를 공유한다고 가정합니다. 이렇게하면 로그 사후가 다음과 같이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR &amp;ndash; 큰 선형 분류를위한 라이브러리</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR : 큰 선형 분류를위한 라이브러리</target>
        </trans-unit>
        <trans-unit id="919f2c891fd7b6ae4005ef3cab68511f9b26c031" translate="yes" xml:space="preserve">
          <source>LIBSVM: A Library for Support Vector Machines</source>
          <target state="translated">LIBSVM : 서포트 벡터 머신 용 라이브러리</target>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">LSA는 잠재 시맨틱 인덱싱 (LSI)으로도 알려져 있지만, 정보 검색 목적으로 영구 인덱스에서 사용하는 것을 엄격하게 의미합니다.</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">LSH Forest는 대략적인 방법이므로 인덱싱 된 데이터 집합에서 일부 실제 인접 항목이 결과에서 누락 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">LSH Forest : 지역 민감성 해싱 포레스트 [1]은 바닐라 근사치의 가장 가까운 이웃 검색 방법에 대한 대체 방법입니다. LSH 포리스트 데이터 구조는 정렬 된 배열과 이진 검색 및 32 비트 고정 길이 해시를 사용하여 구현되었습니다. 코사인 거리에 근접한 해시 패밀리로 랜덤 프로젝션이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">인구의 LSTAT % 낮은 상태</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">라벨 전파 분류기</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">라벨 전파 숫자 활성 학습</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">레이블 전파 숫자 : 성능 시연</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">복잡한 구조를 배우는 레이블 전파</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">변환을 통해 각 항목에 할당 된 레이블.</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">긍정적 인 것으로 간주되는 라벨과 부정적인 것으로 간주되는 라벨.</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">LOF 점수 및 오염 매개 변수에 따라 레이블은 내부자에 대해 1이고 특이 치에 대해 -1입니다.</target>
        </trans-unit>
        <trans-unit id="0facd2ec455a2234134eaa8ce0e172bf077ca396" translate="yes" xml:space="preserve">
          <source>Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.</source>
          <target state="translated">포지티브 클래스의 레이블입니다. y_true가 모두 0이거나 모두 -1이 아니면 pos_label의 기본값이 1이 아닌 경우 기본값은 더 큰 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">긍정적 클래스의 레이블. 없음 인 경우 최대 레이블이 양의 클래스로 사용됩니다</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">레이블 전파는 반 감독 그래프 추론 알고리즘의 몇 가지 변형을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">레이블 전파 모델에는 두 가지 기본 제공 커널 메소드가 있습니다. 커널의 선택은 알고리즘의 확장 성과 성능 모두에 영향을줍니다. 다음을 사용할 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">LRAP (Label Ranking Average Precision)는 다음 질문에 대한 답을 표본에 대해 평균화합니다. 각 근거리 레이블에 대해, 상위 레이블의 일부는 실제 레이블입니까? 각 샘플과 관련된 레이블에 더 나은 순위를 부여 할 수있는 경우이 성능 측정 값이 높아집니다. 획득 한 점수는 항상 0보다 엄격하고 최상의 값은 1입니다. 샘플 당 정확히 하나의 관련 레이블이있는 경우 레이블 순위 평균 정밀도는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;평균 역수 순위&lt;/a&gt; 와 같습니다 .</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">LRAP (Label Ranking Average Precision)는 점수가 낮은 실제 레이블과 전체 레이블의 비율을 기준으로 각 샘플에 할당 된 각 기본 진리 레이블에 대한 평균입니다.</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">특이 치 샘플 (주어진 반경에 이웃이없는 샘플)에 대해 제공되는 레이블입니다. None으로 설정하면 특이 치가 감지 될 때 ValueError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">Semi-supervised learning을위한 LabelSpreading 모델</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">모든 클래스 멤버를 동일한 클러스터에 할당하는 레이블이 항상 완전한 것은 아니므로 처벌을받습니다.</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">모든 클래스 멤버를 동일한 클러스터에 할당하는 레이블은 균질하지 않으므로 불이익을받습니다.</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">같은 클래스에서 온 멤버를 가진 순수한 클러스터가있는 레이블은 균질하지만 불필요하게 분할하면 완성도에 해를 끼치며 V 측정에도 불이익을줍니다.</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">라벨이 지정된 데이터.</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">주어진 샘플에 가장 가까운 중심을 찾아서 새로운 샘플에 라벨을 붙입니다.</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">첫 번째 어노 테이터가 지정한 레이블.</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">두 번째 어노 테이터가 지정한 레이블. 카파 통계량은 대칭이므로 &lt;code&gt;y1&lt;/code&gt; 과 &lt;code&gt;y2&lt;/code&gt; 를 바꾸어도 값이 변경되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">서브 클러스터가 전체적으로 클러스터 된 후 서브 클러스터의 중심에 지정된 레이블.</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">각 얼굴 이미지와 관련된 레이블. 이 레이블의 범위는 0-39이며 주제 ID에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">각 얼굴 이미지와 관련된 레이블. 이 레이블의 범위는 0-5748이며 개인 ID에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">각 이미지 쌍과 관련된 레이블. 두 레이블 값은 다른 사람이거나 같은 사람입니다.</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">X 라벨.</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">각 포인트의 라벨</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">각 포인트의 레이블 (comput_labels가 True로 설정된 경우).</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">각 지점의 레이블.</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">그룹 내 순열을 제한하는 레이블, 즉 &lt;code&gt;y&lt;/code&gt; 값은 동일한 그룹 식별자를 가진 샘플간에 순열됩니다. 지정하지 않으면 &lt;code&gt;y&lt;/code&gt; 값이 모든 샘플에서 순열됩니다.</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">큰 &lt;code&gt;n_clusters&lt;/code&gt; 및 &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">큰 &lt;code&gt;n_samples&lt;/code&gt; 및 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">큰 데이터 세트, 이상치 제거, 데이터 축소.</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">큰 특이 치</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Lars는 경로의 각 꼬임에 대해서만 경로 솔루션을 계산합니다. 결과적으로 꼬임이 적을 때 매우 효율적이며, 기능이나 샘플이 거의없는 경우입니다. 또한 메타 매개 변수를 설정하지 않고 전체 경로를 계산할 수 있습니다. 반대로 좌표 하강은 미리 지정된 그리드에서 경로 점을 계산합니다 (여기서는 기본값 사용). 따라서 그리드 점의 수가 경로의 꼬임 수보다 작은 경우 더 효율적입니다. 기능의 수가 실제로 많고 많은 양을 선택하기에 충분한 샘플이있는 경우 이러한 전략은 흥미로울 수 있습니다. 수치 적으로 오차가 큰 변수의 경우 Lars는 더 많은 오차를 누적하는 반면 좌표 하강 알고리즘은 그리드의 경로 만 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">올가미와 탄성 그물</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">희소 신호를위한 올가미 및 탄성 망</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">올가미 및 탄성 그물 (L1 및 L2 벌칙)은 좌표 하강을 사용하여 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">최소 각 회귀로 계산 된 올가미</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">정규화 경로를 따라 반복 피팅이있는 올가미 선형 모델</target>
        </trans-unit>
        <trans-unit id="47657b9ded4cc2a2b0764459c9d492e6cc3f4eb7" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path.</source>
          <target state="translated">정규화 경로를 따라 반복 피팅이있는 올가미 선형 모델.</target>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">모델 선택을 위해 BIC 또는 AIC를 사용하여 Lars에 맞는 올가미 모델</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">최소 각도 회귀에 적합한 올가미 모델</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">최소 각도 회귀에 적합한 올가미 모델</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">올가미 모델 선택 : 교차 검증 / AIC / BIC</target>
        </trans-unit>
        <trans-unit id="5632592b831e91b94b8f3294c0115a52d64872b7" translate="yes" xml:space="preserve">
          <source>Lasso models (see the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; User Guide section) estimates sparse coefficients. LassoCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for the model estimation.</source>
          <target state="translated">올가미 모델 ( &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;올가미 사용&lt;/a&gt; 안내서 섹션 참조)은 희소 계수를 추정합니다. LassoCV는 모델 추정에 가장 적합한 정규화 매개 변수 ( &lt;code&gt;alpha&lt;/code&gt; ) 값을 결정하기 위해 교차 검증을 적용 합니다.</target>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">조밀하고 드문 데이터에 올가미</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">LARS를 사용한 올가미 경로</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">좌표 하강을 이용한 올가미 ( &lt;a href=&quot;linear_model#lasso&quot;&gt;올가미&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">Latent Dirichlet Allocation은 텍스트 코포 라와 같은 개별 데이터 집합의 수집을위한 생성 확률 모델입니다. 또한 문서 콜렉션에서 추상 주제를 발견하는 데 사용되는 주제 모델입니다.</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">온라인 변형 베이 즈 알고리즘을 사용한 Latent Dirichlet Allocation</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">데이터의 잠재 표현.</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">나중에 다음을 사용하여 절인 모델 (다른 Python 프로세스에서 가능)을 다시로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">나중에 다음을 사용하여 절인 모델을 다시로드 할 수 있습니다 (다른 Python 프로세스에서 가능).</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">위도 하우스 블록 위도</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; 로&lt;/a&gt; 전달되었습니다 . 이는 트리를 저장하는 데 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="90341c46ba90925b69433ce5faecb3e1b8c85d8c" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;code&gt;BallTree&lt;/code&gt; or &lt;code&gt;KDTree&lt;/code&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">&lt;code&gt;BallTree&lt;/code&gt; 또는 &lt;code&gt;KDTree&lt;/code&gt; 에 전달 된 리프 크기 입니다. 이는 트리를 저장하는 데 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 미칠 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 BallTree 또는 KDTree로 전달되었습니다. 이는 트리 저장에 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">잎 크기는 BallTree 또는 cKDTree로 전달되었습니다. 이는 트리 저장에 필요한 메모리뿐만 아니라 구성 및 쿼리 속도에 영향을 줄 수 있습니다. 최적의 값은 문제의 특성에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">데이터 X에 대한 NMF 모델을 학습하고 변환 된 데이터를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">데이터 X에 대한 NMF 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">기능 이름 목록-&amp;gt; 인덱스 맵핑을 배우고 X를 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">기능 이름-&amp;gt; 인덱스 맵핑 목록을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">원시 문서에서 모든 토큰의 어휘 사전을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">열차 데이터의 치수 축소를 배우고 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">X에서 경험적 차이를 배웁니다.</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">Varial Bayes 방법으로 데이터 X에 대한 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">IDF 벡터 (전세계 항 가중치) 알아보기</target>
        </trans-unit>
        <trans-unit id="fb7b507c119ba0834ef410110951b80a51da9b63" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights).</source>
          <target state="translated">idf 벡터 (전역 용어 가중치)를 알아 봅니다.</target>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">사전 계산되지 않은 커널에 대한 역변환을 배우십시오. (즉, 포인트의 사전 이미지를 찾는 법을 배웁니다)</target>
        </trans-unit>
        <trans-unit id="351ef97b73f57653764681dfe2a94603d5d1cbab" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return document-term matrix.</source>
          <target state="translated">어휘 사전을 배우고 문서 용어 행렬을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">어휘 사전을 배우고 용어 문서 매트릭스를 반환하십시오.</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">훈련 세트에서 어휘와 IDF를 배우십시오.</target>
        </trans-unit>
        <trans-unit id="51c9b9cb5d5b207d3afa5b6e39e7500c1ff633ed" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return document-term matrix.</source>
          <target state="translated">어휘와 idf를 배우고 문서 용어 행렬을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">어휘와 IDF를 배우고 용어 문서 매트릭스를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">그래프 구조 학습</target>
        </trans-unit>
        <trans-unit id="4ecad9f15b8037b0486e20e0cb489ddd61caeca5" translate="yes" xml:space="preserve">
          <source>Learning an embedding</source>
          <target state="translated">임베딩 학습</target>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">학습과 예측</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">학습 곡선.</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">학습 문제는 몇 가지 범주로 나뉩니다.</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">체중 업데이트에 대한 학습 속도 일정.</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;learning_rate&lt;/code&gt; 은 learning_rate 로 각 분류 자의 기여를 줄입니다 . &lt;code&gt;learning_rate&lt;/code&gt; 와 &lt;code&gt;n_estimators&lt;/code&gt; 사이에는 균형이 있습니다 .</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;learning_rate&lt;/code&gt; 은 learning_rate 로 각 회귀 변수의 기여를 줄입니다 . &lt;code&gt;learning_rate&lt;/code&gt; 와 &lt;code&gt;n_estimators&lt;/code&gt; 사이에는 균형이 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">예측 함수의 매개 변수를 학습하고 동일한 데이터에서 테스트하는 것은 방법 론적 실수입니다. 방금 본 샘플의 레이블을 반복하는 모델은 완벽한 점수를 얻지 만 아직 유용한 것은 예측하지 못합니다. 보이지 않는 데이터. 이 상황을 &lt;strong&gt;과적 합&lt;/strong&gt; 이라고 &lt;strong&gt;합니다&lt;/strong&gt; . 이를 피하기 위해 사용 가능한 데이터의 일부를 &lt;strong&gt;테스트 세트 &lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 로 유지하는 (감독 된) 기계 학습 실험을 수행 할 때 일반적입니다 . 상업적 실험에서도 기계 학습은 일반적으로 실험적으로 시작되기 때문에 &quot;실험&quot;이라는 단어는 학문적 용도만을 나타내는 것이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="7ed56c1456833faed4202c795166989f5307ee69" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; techniques.</source>
          <target state="translated">예측 함수의 매개 변수를 학습하고 동일한 데이터에 대해 테스트하는 것은 방법 론적 실수입니다. 방금 본 샘플의 레이블을 반복하는 모델은 완벽한 점수를 받았지만 아직 유용한 것은 예측하지 못합니다. 보이지 않는 데이터. 이 상황을 &lt;strong&gt;과적 합&lt;/strong&gt; 이라고 &lt;strong&gt;합니다&lt;/strong&gt; . 이를 방지하기 위해 (지도 된) 머신 러닝 실험을 수행 할 때 사용 가능한 데이터의 일부를 &lt;strong&gt;테스트 세트 &lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 로 유지하는 것이 일반적인 관행입니다 . '실험'이라는 단어는 학문적 용도로만 사용되는 것이 아닙니다. 상업적 환경에서도 머신 러닝은 일반적으로 실험적으로 시작되기 때문입니다. 다음은 모델 학습에서 일반적인 교차 검증 워크 플로의 순서도입니다. 최상의 매개 변수는 다음에 의해 결정될 수 있습니다.&lt;a href=&quot;grid_search#grid-search&quot;&gt;그리드 검색&lt;/a&gt; 기술.</target>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">연결하지 않고 각 출력을 독립적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">최소 각 회귀 모형</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">최소 각 회귀 모형 (일명 LAR)</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">희소 성분에 대한 데이터의 최소 제곱 투영.</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">최소 절대 편차 ( &lt;code&gt;'lad'&lt;/code&gt; ) : 회귀에 대한 강력한 손실 함수. 초기 모델은 목표 값의 중앙값으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">최소 제곱 ( &lt;code&gt;'ls'&lt;/code&gt; ) : 우수한 계산 특성으로 인해 회귀에 대한 자연스러운 선택입니다. 초기 모델은 목표 값의 평균으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="972ad47a68ab0dd02c8d4f9c32f25ef79120c408" translate="yes" xml:space="preserve">
          <source>Least-Squares: Linear regression (Ridge or Lasso depending on \(R\)). \(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).</source>
          <target state="translated">최소 제곱 : 선형 회귀 (\ (R \)에 따라 능선 또는 올가미). \ (L (y_i, f (x_i)) = \ frac {1} {2} (y_i-f (x_i)) ^ 2 \).</target>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">최소 제곱 : 릿지 회귀.</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">최소 각 회귀 ( &lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;최소 각 회귀&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="8427893bdf84ecb2b84085547aa1cfdd8fd0e807" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.</source>
          <target state="translated">최소 각도 회귀 (LARS)는 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 개발 한 고차원 데이터에 대한 회귀 알고리즘입니다. LARS는 전진 단계적 회귀와 유사합니다. 각 단계에서 대상과 가장 관련이있는 기능을 찾습니다. 동일한 상관 관계를 갖는 여러 피처가있는 경우 동일한 피처를 따라가는 대신 피처간에 등각 방향으로 진행됩니다.</target>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">최소 각도 회귀 (LARS)는 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani가 개발 한 고차원 데이터에 대한 회귀 알고리즘입니다. LARS는 순차 단계적 회귀와 유사합니다. 각 단계에서 반응과 가장 관련성이 높은 예측 변수를 찾습니다. 동일한 예측자를 따라 계속 진행하는 대신 동일한 상관 관계를 갖는 여러 예측 변수가있는 경우 예측 변수간에 동일한 방향으로 진행됩니다.</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">하나의 그룹 아웃 교차 유효성 검사기 나가기</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">P 그룹을 교차 유효성 검사기에서 제외</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">P 그룹을 제외하십시오.</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">P 관측 값을 남겨 두십시오.</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">하나의 관찰을 남겨 두십시오.</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">Leave-One-Out 교차 유효성 검사기</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">Leave-P-Out 교차 검증기</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O, Wolf M. Honey, 표본 공분산 행렬을 축소했습니다. 포트폴리오 관리 저널 30 (4), 110-119, 2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolf는 수축률을 계산하는 특수한 형태의 수축 계수로,&amp;ldquo;대형 공분산 행렬에 대한 잘 추정 된 추정기&amp;rdquo;, Ledoit 및 Wolf, 다변량 분석 저널에 설명 된 O. Ledoit 및 M. Wolf의 공식을 사용하여 수축 계수를 계산합니다. , Volume 88, Issue 2, 2004 년 2 월, 365-411 쪽.</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Ledoit-Wolf 및 OAS 추정</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">LedoitWolf Estimator</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">반환 된 커널 k (X, Y)의 왼쪽 인수</target>
        </trans-unit>
        <trans-unit id="db46139863e59ff060f10e61ce009637ab35e3fe" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y).</source>
          <target state="translated">반환 된 커널 k (X, Y)의 왼쪽 인수.</target>
        </trans-unit>
        <trans-unit id="9bf558c9b2f1ab98bdd863d46c825f77b8bcb622" translate="yes" xml:space="preserve">
          <source>Left to right.</source>
          <target state="translated">좌에서 우로.</target>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">경로의 길이입니다. &lt;code&gt;eps=1e-3&lt;/code&gt; 은 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 의미합니다.</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">경로의 길이입니다. &lt;code&gt;eps=1e-3&lt;/code&gt; 은 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="817bbec7d68f2acac91e1c4383d071e50b618e53" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters</source>
          <target state="translated">매개 변수 수에 대한 민감도가 낮음</target>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">매개 변수 수에 대한 민감도 감소 :</target>
        </trans-unit>
        <trans-unit id="5f1b602bd58c70400c6cc3123702692e92eee6c0" translate="yes" xml:space="preserve">
          <source>Lessons learned</source>
          <target state="translated">교훈</target>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">K (x, z)를 phi (x) ^ T phi (z)에 의해 정의 된 커널이라고하자. 여기서 phi는 x를 힐버트 공간에 매핑하는 함수입니다. KernelCenterer는 phi (x)를 명시 적으로 계산하지 않고 데이터를 중심에 둡니다 (즉, 평균을 0으로 정규화). sklearn.preprocessing.StandardScaler (with_std = False)를 사용하여 phi (x)를 센터링하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="821c4d001d9578c562e1b0af64f8be6da39e632f" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(\sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">\ (S \)는 유사성 행렬이고 \ (X \)는 \ (n \) 입력 포인트의 좌표입니다. 불일치 \ (\ hat {d} _ {ij} \)는 최적의 방식으로 선택된 유사성의 변형입니다. 스트레스라고하는 목표는 \ (\ sum_ {i &amp;lt;j} d_ {ij} (X)-\ hat {d} _ {ij} (X) \)로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">\ (S \)를 유사도 행렬로하고 \ (X \)를 \ (n \) 입력 점의 좌표로 지정하십시오. 불일치 \ (\ hat {d} _ {ij} \)는 최적의 방법으로 선택한 유사성의 변형입니다. 스트레스라고하는 목표는 \ (sum_ {i &amp;lt;j} d_ {ij} (X)-\ hat {d} _ {ij} (X) \)에 의해 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="5ab8a1b7961470064849295cd500b6579b4fa397" translate="yes" xml:space="preserve">
          <source>Let \(X_S\) be the set of target features (i.e. the &lt;code&gt;features&lt;/code&gt; parameter) and let \(X_C\) be its complement.</source>
          <target state="translated">\ (X_S \)를 대상 기능 집합 (즉, &lt;code&gt;features&lt;/code&gt; 매개 변수)으로하고 \ (X_C \)를 보완 요소로 지정합니다.</target>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">노드 \ (m \)의 데이터를 \ (Q \)로 표시하십시오. 피처 \ (j \)와 임계 값 \ (t_m \)으로 구성된 각 후보 분할 \ (\ theta = (j, t_m) \)에 대해 데이터를 \ (Q_ {left} (\ theta) \)로 분할하고 \ (Q_ {right} (\ theta) \) 하위 집합</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">이제 겹쳐진 영역을 평균하여 패치에서 원본 이미지를 재구성 해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="b5fecaca6e2e29d4dbee3904d66ce06eade247b5" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s compute the performance of this constant prediction baseline with 3 different regression metrics:</source>
          <target state="translated">세 가지 회귀 측정 항목을 사용하여이 상수 예측 기준선의 성능을 계산해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="253db59ca1134838191e01d302a48262e9ba6374" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s consider the following trained regression model:</source>
          <target state="translated">다음과 같은 훈련 된 회귀 모델을 고려해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="b44b9f209d8c4175d9e9b8f1619b5304447365c9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s fit a MLPRegressor and compute single-variable partial dependence plots</source>
          <target state="translated">MLPRegressor를 피팅하고 단일 변수 부분 의존도를 계산해 봅시다.</target>
        </trans-unit>
        <trans-unit id="c338f438f84e4277f756f203f24830c86a10546d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load data from the newsgroups dataset which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).</source>
          <target state="translated">20 개의 주제에 대한 약 18,000 개의 뉴스 그룹 게시물을 두 개의 하위 집합으로 구성하는 뉴스 그룹 데이터 세트에서 데이터를로드 해 보겠습니다. 하나는 교육용 (또는 개발 용)이고 다른 하나는 테스트 용 (또는 성능 평가 용)입니다.</target>
        </trans-unit>
        <trans-unit id="b852fb92508c2f8d11c716ed1054a5db1d558210" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s load the motor claim dataset from OpenML: &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https://www.openml.org/d/41214&lt;/a&gt;</source>
          <target state="translated">OpenML에서 모터 클레임 데이터 세트를로드 해 보겠습니다 : &lt;a href=&quot;https://www.openml.org/d/41214&quot;&gt;https://www.openml.org/d/41214&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ed3c266c74a01e549482a05ad99384adcdbf59a9" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s make the same partial dependence plot for the 2 features interaction, this time in 3 dimensions.</source>
          <target state="translated">이번에는 3 차원에서 두 기능 상호 작용에 대해 동일한 부분 종속성 플롯을 만들어 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="2f393e9b32e94a400250ae55bd302c219802fcb3" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now compute the partial dependence plots for this neural network using the model-agnostic (brute-force) method:</source>
          <target state="translated">이제 모델에 구애받지 않는 (brute-force) 방법을 사용하여이 신경망에 대한 부분 종속성 플롯을 계산해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="c401ba9464abf901fd51a8e8666504a0d45adbfb" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s now fit a GradientBoostingRegressor and compute the partial dependence plots either or one or two variables at a time.</source>
          <target state="translated">이제 GradientBoostingRegressor를 피팅하고 한 번에 하나 또는 두 개의 변수를 부분 의존성 플롯으로 계산해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">첫 번째로로드 된 파일의 첫 번째 줄을 인쇄 해 봅시다 :</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">샘플 10, 25 및 50에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">샘플 10, 50 및 85에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">샘플 10, 80 및 140에 관심이 있고 클래스 이름을 알고 싶다고 가정 해 봅시다.</target>
        </trans-unit>
        <trans-unit id="e8b31884f71e52e12b72fcb054664d8ab80d318b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation object:</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 교차 유효성 검사 개체를 찾는 방법을 살펴 보겠습니다 .</target>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">&lt;code&gt;KFold&lt;/code&gt; 교차 검증 객체를 찾는 방법을 살펴 보겠습니다 .</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">가장 유익한 기능이 무엇인지 살펴 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">다음과 같은 카운트를 예로 들어 봅시다. 첫 번째 용어는 시간의 100 %이므로 그다지 흥미롭지 않습니다. 다른 두 가지 기능은 시간의 50 % 미만에 불과하므로 문서 내용을 더 잘 나타낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">기본 설정으로 다시 시도하십시오 :</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">이것을 사용하여 텍스트 문서의 최소한의 말뭉치의 단어 발생을 토큰 화하고 계산하십시오.</target>
        </trans-unit>
        <trans-unit id="292ee05108cd151612c92edea6da34acc9a56662" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.</source>
          <target state="translated">팬더를 사용하여 타이타닉 데이터 세트의 사본을로드 해 보겠습니다. 다음은 숫자 및 범주 기능에 별도의 전처리를 적용하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">많은 scikit-learn 교차 유효성 검사 개체에 대한 교차 유효성 검사 동작을 시각적으로 비교해 보겠습니다. 아래에서는 몇 가지 일반적인 교차 유효성 검사 개체를 반복하여 각각의 동작을 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">자세한 수준.</target>
        </trans-unit>
        <trans-unit id="339e81226696b46d7377960244af7f4dcb540176" translate="yes" xml:space="preserve">
          <source>Lewis, D. D., Yang, Y., Rose, T. G., &amp;amp; Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.</source>
          <target state="translated">Lewis, DD, Yang, Y., Rose, TG, &amp;amp; Li, F. (2004). RCV1 : 텍스트 분류 연구를위한 새로운 벤치 마크 컬렉션입니다. 기계 학습 연구 저널, 5, 361-397.</target>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="87b3d037e844d0b272ce5bfc0fb0a06e31f13827" translate="yes" xml:space="preserve">
          <source>License: BSD 3 clause</source>
          <target state="translated">라이센스 : BSD 3 조항</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">3 절 BSD 라이센스에 따라 라이센스가 부여됩니다.</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman, M. (2013). UCI 머신 러닝 리포지토리 [ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]. 캘리포니아 어바인 : 캘리포니아 대학교, 정보 및 컴퓨터 과학부.</target>
        </trans-unit>
        <trans-unit id="e76bfa901cdd22255b64ebacf70ec6f76b7f04ab" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman, M. (2013). UCI 기계 학습 저장소 [ &lt;a href=&quot;https://archive.ics.uci.edu/ml&quot;&gt;https://archive.ics.uci.edu/ml&lt;/a&gt; ]. 캘리포니아 어바인 : 캘리포니아 대학교, 정보 및 컴퓨터 과학 학교.</target>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">&lt;a href=&quot;tree#tree&quot;&gt;의사 결정 트리&lt;/a&gt; 와 마찬가지로 트리 의 포리스트도 &lt;a href=&quot;tree#tree-multioutput&quot;&gt;다중 출력 문제로&lt;/a&gt; 확장됩니다 (Y가 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 크기의 배열 인 경우 ).</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 과 마찬가지로 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 개별 단계를 대체 할 수 있으며 &lt;code&gt;'drop'&lt;/code&gt; 으로 설정하면 무시됩니다 .</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">MultinomialNB와 마찬가지로이 분류기는 이산 데이터에 적합합니다. 차이점은 MultinomialNB가 발생 횟수와 작동하지만 BernoulliNB는 이진 / 부울 기능을 위해 설계되었다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">fit (X) 다음에 transform (X)이 있지만 메모리에 X를 구체화 할 필요는 없습니다.</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">파이프 라인 및 FeatureUnion에서와 같이 트랜스포머 및 해당 매개 변수를 &lt;code&gt;set_params&lt;/code&gt; 를 사용하여 설정 하고 그리드 검색에서 검색 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">파이프 라인과 마찬가지로 피처 유니온에는 구성 요소를 명시 적으로 명명 할 필요가없는 &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt; 이라는 속기 생성자 가 있습니다.</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">스케일러와 마찬가지로 &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; 는&lt;/a&gt; 모든 기능을 동일하고 알려진 범위 또는 분포에 넣습니다. 그러나 순위 변환을 수행하면 비정상적인 분포가 완화되고 스케일링 방법보다 특이 치의 영향을 덜받습니다. 그러나 피처 내에서 또는 피처간에 거리와 상관 관계가 왜곡됩니다.</target>
        </trans-unit>
        <trans-unit id="73666b411bce492e14d6ba5736c0ca5eebf6eb1b" translate="yes" xml:space="preserve">
          <source>Like the Poisson GLM above, the gradient boosted trees model minimizes the Poisson deviance. However, because of a higher predictive power, it reaches lower values of Poisson deviance.</source>
          <target state="translated">위의 Poisson GLM과 마찬가지로 Gradient Boosted Tree 모델은 Poisson 편차를 최소화합니다. 그러나 더 높은 예측력으로 인해 더 낮은 포아송 이탈도 값에 도달합니다.</target>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">캐시 크기의 바이트 수로 제한합니다.</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">돌아 오는 이웃의 거리를 제한합니다. (기본값은 생성자에 전달 된 값입니다).</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">선형 판별 분석</target>
        </trans-unit>
        <trans-unit id="bb7eb231c96859c6082b4ce0d3b796b4329f6e8f" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">선형 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) 및 2 차 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; )은 이름에서 알 수 있듯이 각각 선형 및 2 차 결정 표면이있는 두 가지 고전적인 분류기입니다.</target>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">선형 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) 및 2 차 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; )은 이름에서 알 수 있듯이 선형 및 2 차 결정 표면이있는 두 개의 클래식 분류기입니다.</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">LDA (Linear Discriminant Analysis)는 &lt;em&gt;클래스 간&lt;/em&gt; 차이 &lt;em&gt;가&lt;/em&gt; 가장 큰 속성을 식별하려고합니다 . 특히 LDA는 PCA와 달리 알려진 클래스 레이블을 사용하는 감독 방법입니다.</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">선형 판별 분석 및 2 차 판별 분석</target>
        </trans-unit>
        <trans-unit id="37e8c1f7f3b5f52be8ccec8de8de9cd593660b24" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt;&lt;/a&gt; module, and Neighborhood Components Analysis, from the &lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module, are supervised dimensionality reduction method, i.e. they make use of the provided labels, contrary to other methods.</source>
          <target state="translated">로부터 선형 판별 분석, &lt;a href=&quot;../../modules/classes#module-sklearn.discriminant_analysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis&lt;/code&gt; 의&lt;/a&gt; 모듈 및 환경 요소 분석은에서 &lt;a href=&quot;../../modules/classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 의&lt;/a&gt; 모듈, 즉 그들이 다른 방법으로 제공된 라벨의 사용, 반대로을, 관리 감독 차원 축소 방법입니다.</target>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">L1로 정규화 기 (일명 올가미)로 훈련 된 선형 모델</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">선형 회귀 예제</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">Linear SVC는 기본적으로 확률 적 분류 기가 아니지만이 예제에서 사용 가능한 기본 제공 보정 옵션이 있습니다 ( &lt;code&gt;probability=True&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">선형 SVM</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">선형 합-모든 표본의 합을 보유하는 n 차원 벡터</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">선형 서포트 벡터 분류 ( &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; ) 방송 최대 마진 방법 (비교 니쿨 레스 쿠-Mizil 및 카루 아나 전형적이다 RandomForestClassifier 같은 더욱 S 자형 곡선 &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt; ), 근접 결정 경계한다 어려운 샘플들에있는 초점 ( 지원 벡터).</target>
        </trans-unit>
        <trans-unit id="aa48807728eba9fefe9fd82435afe3ea7ac48643" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; (선형 지원 벡터 분류 )는 RandomForestClassifier와 같이 훨씬 더 많은 시그 모이 드 곡선을 보여줍니다. 이는 최대 마진 방법 (Niculescu-Mizil 및 Caruana &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;1&lt;/a&gt; 비교 )에 일반적으로 사용되며, 이는 결정 경계에 가까운 하드 샘플에 중점을 둡니다 (지원 벡터).</target>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">선형 지원 벡터 분류.</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">선형 지원 벡터 회귀.</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">공분산 타원체를 사용한 선형 및 2 차 판별 분석</target>
        </trans-unit>
        <trans-unit id="f5530856b3075f323ba02b6ac9992d5aae8eee6e" translate="yes" xml:space="preserve">
          <source>Linear classifiers</source>
          <target state="translated">선형 분류기</target>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">SGD 교육이 포함 된 선형 분류기 (SVM, 로지스틱 회귀, ao)</target>
        </trans-unit>
        <trans-unit id="9a198808a208105876aff5b3459a1743d02b9e6c" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, etc.) with SGD training.</source>
          <target state="translated">SGD 교육이 포함 된 선형 분류기 (SVM, 로지스틱 회귀 등).</target>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">특이 값 중심의 데이터 분해를 사용한 선형 차원 축소 가장 중요한 특이 벡터 만 데이터를 더 낮은 차원 공간에 투영하도록 유지합니다.</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">더 작은 차원 공간으로 데이터를 투영하기 위해 데이터의 특이 값 분해를 사용한 선형 차원 축소.</target>
        </trans-unit>
        <trans-unit id="6451e9b1aa60579e3e911ed4aef57d459ed7cbf1" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">데이터의 특이 값 분해를 사용하여 더 낮은 차원 공간에 투영하는 선형 차원 감소. 입력 데이터는 중앙에 있지만 SVD를 적용하기 전에 각 기능에 대해 조정되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c7a4096bc58f13af1ea98cce08fd73c62fb91596" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.</source>
          <target state="translated">데이터의 특이 값 분해를 사용하여 선형 차원 감소, 가장 중요한 특이 벡터 만 유지하여 데이터를 더 낮은 차원 공간에 투영합니다. 입력 데이터는 중앙에 있지만 SVD를 적용하기 전에 각 기능에 대해 조정되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">선형 커널 ( &lt;code&gt;kernel = 'linear'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">l2 정규화를 갖는 선형 최소 제곱.</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">SGD를 사용하여 정기적 인 경험적 손실을 최소화하여 장착 된 선형 모델</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">많은 회귀 분석기 각각의 개별 효과를 테스트하기위한 선형 모형. 이 기능은 독립형 기능 선택 절차가 아닌 기능 선택 절차에 사용되는 스코어링 기능입니다.</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">선형 모형 : 회귀에서 희소성까지</target>
        </trans-unit>
        <trans-unit id="0b1d2caa3dbccbb7fc7a7a3c7fac23bacc286b81" translate="yes" xml:space="preserve">
          <source>Linear models with regularization</source>
          <target state="translated">정규화를 사용하는 선형 모델</target>
        </trans-unit>
        <trans-unit id="46d0fcb8f937066fa349aa94a3c710921424dd9b" translate="yes" xml:space="preserve">
          <source>Linear models with sparse coefficients</source>
          <target state="translated">희소 계수가있는 선형 모델</target>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">선형 모델 : \ (y = X \ beta + \ epsilon \)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">선형 회귀</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">특이 치에 강한 선형 회귀 모델.</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">L1 및 L2 사전을 정규화기로 결합한 선형 회귀</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">선형 능선 회귀</target>
        </trans-unit>
        <trans-unit id="1dc397a187bb8fae755995aa069f79a9d565d581" translate="yes" xml:space="preserve">
          <source>Linear support vector classification.</source>
          <target state="translated">선형 지원 벡터 분류.</target>
        </trans-unit>
        <trans-unit id="5959458e20a73276c12d61f1d64604d66daab52d" translate="yes" xml:space="preserve">
          <source>LinearRegression</source>
          <target state="translated">LinearRegression</target>
        </trans-unit>
        <trans-unit id="6ff9599a07718d87fb124205a5a88e8262436582" translate="yes" xml:space="preserve">
          <source>LinearRegression fits a linear model with coefficients w = (w1, &amp;hellip;, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.</source>
          <target state="translated">LinearRegression은 계수 w = (w1,&amp;hellip;, wp)를 사용하여 선형 모델을 피팅하여 데이터 세트에서 관찰 된 목표와 선형 근사에 의해 예측 된 목표 사이의 잔차 제곱합을 최소화합니다.</target>
        </trans-unit>
        <trans-unit id="daf34c391f43051e2982c2bbeba34bf1a7727132" translate="yes" xml:space="preserve">
          <source>List containing the artists for the annotation boxes making up the tree.</source>
          <target state="translated">트리를 구성하는 주석 상자의 아티스트가 포함 된 목록입니다.</target>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">열차 테스트 입력을 포함하는 목록.</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">(name, class)의 목록. 여기서 &lt;code&gt;name&lt;/code&gt; 은 문자열 인 클래스 이름이고 &lt;code&gt;class&lt;/code&gt; 는 실제 클래스 유형입니다.</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">체인으로 연결된 순서대로 (이름, 변환) 튜플 (구현 적합 / 변형)의 목록이며 마지막 오브젝트는 추정기입니다.</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">데이터의 서브 세트에 적용 할 변환기 오브젝트를 지정하는 (이름, 변환기, 열 (들)) 튜플의 목록.</target>
        </trans-unit>
        <trans-unit id="d6a17bcffaea2ea2f18b9842614672499d0a26c6" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">데이터의 하위 집합에 적용 할 변환기 개체를 지정하는 (이름, 변환기, 열) 튜플 목록입니다.</target>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">&lt;code&gt;n_features&lt;/code&gt; 차원 데이터 포인트 목록 . 각 행은 단일 데이터 포인트에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">y 벡터에 나타날 수있는 모든 클래스 목록.</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 알파가 자동으로 설정 &lt;code&gt;None&lt;/code&gt; 경우</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 알파가 자동으로 설정되지 않은 경우</target>
        </trans-unit>
        <trans-unit id="a5422f4e0e412f7e68186f61afde0578ebd8abd7" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically.</source>
          <target state="translated">모델을 계산할 알파 목록입니다. None 알파가 자동으로 설정되는 경우.</target>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">모델을 계산할 알파의 목록입니다. 제공되지 않으면 자동으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">용어 배열 목록.</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">내장 커널 목록.</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">로지스틱 회귀 모형의 계수 목록입니다. fit_intercept를 True로 설정하면 두 번째 치수는 n_features + 1이되며 마지막 항목은 절편을 나타냅니다. 용 &lt;code&gt;multiclass='multinomial'&lt;/code&gt; , 형상 (n_classes, n_cs, n_features) 또는 (n_classes, n_cs, n_features + 1)이다.</target>
        </trans-unit>
        <trans-unit id="d090495d2c127f32b67f3946c4bdcba721a89fcd" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If &lt;code&gt;None&lt;/code&gt; is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">행렬을 인덱싱 할 레이블 목록입니다. 레이블의 하위 집합을 재정렬하거나 선택하는 데 사용할 수 있습니다. 경우 &lt;code&gt;None&lt;/code&gt; 주어지지에 한 번 이상에서 나타나는 그 &lt;code&gt;y_true&lt;/code&gt; 또는 &lt;code&gt;y_pred&lt;/code&gt; 은 정렬 된 순서로 사용된다.</target>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">행렬을 인덱싱 할 레이블 목록입니다. 레이블의 하위 집합을 다시 정렬하거나 선택하는 데 사용할 수 있습니다. 아무 것도 지정하지 않으면 &lt;code&gt;y_true&lt;/code&gt; 또는 &lt;code&gt;y_pred&lt;/code&gt; 에 적어도 한 번 나타나는 항목 이 정렬 된 순서로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">행렬을 인덱싱 할 레이블 목록입니다. 레이블 하위 집합을 선택하는 데 사용할 수 있습니다. 없음 인 경우 &lt;code&gt;y1&lt;/code&gt; 또는 &lt;code&gt;y2&lt;/code&gt; 에 적어도 한 번 나타나는 모든 레이블 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">n_ 특징 차원 데이터 포인트 목록. 각 행은 단일 데이터 포인트에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">n_ 특징 차원 데이터 포인트 목록. 각 행은 단일 쿼리에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">슬라이스 가능성을 보장하는 객체 목록.</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">데이터 X에 첨부 된 샘플 중량 목록</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">샘플 목록.</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">데이터에 적용 할 변환기 객체 목록입니다. 각 튜플의 처음 절반은 변압기의 이름입니다.</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">정규화 매개 변수의 값 목록 또는 사용해야하는 정규화 매개 변수 수를 지정하는 정수 이 경우 매개 변수는 1e-4와 1e4 사이의 로그 스케일로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">점수를 계산할 가중치 유형 목록입니다. 없음은 가중치가 없음을 의미합니다. &quot;선형&quot;은 선형 가중을 의미하고; &quot;2 차&quot;는 2 차 가중을 의미한다.</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming 및 Zhou, Zhi-Hua. &amp;ldquo;격리 숲&amp;rdquo; 데이터 마이닝, 2008. ICDM'08. 여덟 번째 IEEE 국제 회의.</target>
        </trans-unit>
        <trans-unit id="ef6455b6e1e2fee9a705bb5d5a6878e2089d032f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;rsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming 및 Zhou, Zhi-Hua. &quot;고립의 숲.&quot; 데이터 마이닝, 2008. ICDM'08. 여덟 번째 IEEE 국제 회의.</target>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">Liu, Fei Tony, Ting, Kai Ming 및 Zhou, Zhi-Hua. &amp;ldquo;격리 기반 이상 탐지.&amp;rdquo; 데이터의 지식 발견에 대한 ACM 거래 (TKDD) 6.1 (2012) : 3.</target>
        </trans-unit>
        <trans-unit id="d7cbf66ae3940637cf4feeef412f2113fd5144b3" translate="yes" xml:space="preserve">
          <source>Load Data and Train a SVC</source>
          <target state="translated">데이터로드 및 SVC 훈련</target>
        </trans-unit>
        <trans-unit id="5a977facf077b843c15be4adf2b27656870bbc8b" translate="yes" xml:space="preserve">
          <source>Load Data and train model</source>
          <target state="translated">데이터로드 및 모델 학습</target>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt; 에서 다운로드 한 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">보스턴 주택 가격 데이터 집합을로드하고 반환합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">유방암 위스콘신 데이터 세트를로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">당뇨병 데이터 집합을로드하고 반환합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">숫자 데이터 집합을로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">홍채 데이터 세트를로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">linnerud 데이터 세트를로드하고 반환합니다 (다변량 회귀).</target>
        </trans-unit>
        <trans-unit id="24c66578782cdc888ab71f9a25a06214508e7234" translate="yes" xml:space="preserve">
          <source>Load and return the physical excercise linnerud dataset.</source>
          <target state="translated">실제 운동 linnerud 데이터 세트를로드하고 반환합니다.</target>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">와인 데이터 셋을로드하고 반환합니다 (분류).</target>
        </trans-unit>
        <trans-unit id="34956b05e013a2f3d8d5817783733a9d69ea9a29" translate="yes" xml:space="preserve">
          <source>Load data from the training set</source>
          <target state="translated">훈련 세트에서 데이터로드</target>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">SVMlight 형식으로 여러 파일에서 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">svmlight / libsvm 형식의 데이터 세트를 스파 스 CSR 매트릭스에로드</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">이미지 조작을 위해 샘플 이미지를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">카테고리가있는 텍스트 파일을 하위 폴더 이름으로로드하십시오.</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트를로드하고이를 토큰 수 (분류)로 벡터화하십시오.</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">캘리포니아 주택 데이터 세트를로드합니다 (회귀).</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">와일드 한 (LFW) 쌍 데이터 세트 (분류)에 레이블이있는면을로드합니다.</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">Wild (LFW) people 데이터 세트 (분류)에 레이블이있는면을로드합니다.</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">AT &amp;amp; T (분류)에서 Olivetti면 데이터 세트를로드합니다.</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">RCV1 다중 레이블 데이터 세트 (분류)를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">표지 유형 데이터 세트 (분류)를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="ad3fd711e27424bfdcf7677e93b2ded911f0bbc0" translate="yes" xml:space="preserve">
          <source>Load the data</source>
          <target state="translated">데이터로드</target>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트 (분류)에서 파일 이름 및 데이터를로드하십시오.</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">kddcup99 데이터 세트를로드하십시오 (분류).</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">단일 샘플 이미지의 numpy 배열을로드합니다</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">Phillips 등의 종 분포 데이터 세트 로더</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">Phillips 등의 종 분포 데이터 세트 로더 알. (2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">예제 데이터 셋로드</target>
        </trans-unit>
        <trans-unit id="caf6cb93de1911a37a3c4f9c173bcddd3283525a" translate="yes" xml:space="preserve">
          <source>Loading datasets, basic feature extraction and target definitions</source>
          <target state="translated">데이터 세트, 기본 기능 추출 및 대상 정의로드</target>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">외부 데이터 세트에서로드</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">20 개 뉴스 그룹 데이터 세트로드</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;china&lt;/code&gt; 과 &lt;code&gt;flower&lt;/code&gt; 모두로드합니다 .</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">국부적으로 선형 임베딩</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">LLE (Locally Linear Embeding)는 로컬 주변 거리를 유지하는 데이터의보다 낮은 차원의 투영을 찾습니다. 최고의 비선형 임베딩을 찾기 위해 전 세계적으로 비교되는 일련의 로컬 주성분 분석으로 생각할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 함수 또는 객체 지향 대응 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 로컬 선형 임베딩을 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">X가 주어진 가우스 혼합의 로그 가능성.</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">p = 0 또는 p = 1에 대해 로그 손실이 정의되지 않으므로 확률은 max (eps, min (1-eps, p))로 잘립니다.</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">로그 손실, 일명 물류 손실 또는 교차 엔트로피 손실.</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">로지스틱 회귀 손실 또는 교차 엔트로피 손실이라고도하는 로그 손실은 확률 추정치에 정의됩니다. 그것은 (다항식) 로지스틱 회귀 및 신경망과 기대 최대화의 일부 변형에서 일반적으로 사용되며, 이산 예측 대신 분류기 의 확률 출력 ( &lt;code&gt;predict_proba&lt;/code&gt; ) 을 평가하는 데 사용될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">확률 추정치의 로그입니다.</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">X에서 각 데이터 포인트의 확률을 기록합니다.</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">각 클래스의 로그 확률 (부드럽게)</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">현재 모델에서 각 샘플의 로그 우도</target>
        </trans-unit>
        <trans-unit id="f4a66282780599e98e06ef51f0d5990ef29ec975" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model.</source>
          <target state="translated">현재 모델에서 각 샘플의 로그 가능도입니다.</target>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">접힌 데이터에 대한 로그 아웃 가능성 점수</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">훈련 데이터에 대한 로그의 한계 확률.</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">로그 : 로지스틱 회귀.</target>
        </trans-unit>
        <trans-unit id="710d6654ab013b68e75b864460870bdb608ee833" translate="yes" xml:space="preserve">
          <source>Log: equivalent to Logistic Regression. \(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).</source>
          <target state="translated">로그 : 로지스틱 회귀와 동일합니다. \ (L (y_i, f (x_i)) = \ log (1 + \ exp (-y_i f (x_i))) \).</target>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">로지스틱 회귀 (일명 logit, MaxEnt) 분류기.</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">로지스틱 회귀 3 클래스 분류기</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">로지스틱 회귀 CV (일명 logit, MaxEnt) 분류기.</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">물류 기능</target>
        </trans-unit>
        <trans-unit id="90723c3f54f2127002d5e8087f3fd75704debc54" translate="yes" xml:space="preserve">
          <source>Logistic regression is implemented in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net regularization.</source>
          <target state="translated">로지스틱 회귀는 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 에서 구현됩니다 . 이 구현은 선택적 \ (\ ell_1 \), \ (\ ell_2 \) 또는 Elastic-Net 정규화를 사용하여 이진, 일 대 나머지 또는 다항 로지스틱 회귀에 맞출 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">원시 픽셀 값에 대한 로지스틱 회귀가 비교를 위해 제공됩니다. 이 예는 BernoulliRBM에서 추출한 기능이 분류 정확도를 향상시키는 데 도움이된다는 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">내장 된 교차 검증을 통한 로지스틱 회귀</target>
        </trans-unit>
        <trans-unit id="96304f46c8b5deeee00fad5a320967f13f13e69f" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation.</source>
          <target state="translated">기본 제공 교차 검증을 통한 로지스틱 회귀.</target>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">로지스틱 회귀는 이름에도 불구하고 회귀가 아닌 분류에 대한 선형 모형입니다. 로지스틱 회귀는 문헌에서 로짓 회귀, 최대 엔트로피 분류 (MaxEnt) 또는 로그 선형 분류기로도 알려져 있습니다. 이 모델에서 단일 시도의 가능한 결과를 설명하는 확률은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;로지스틱 함수를&lt;/a&gt; 사용하여 모델링 됩니다 .</target>
        </trans-unit>
        <trans-unit id="8e225015f4826ba9beac040479565ae8cd20d1cd" translate="yes" xml:space="preserve">
          <source>Logistic regression.</source>
          <target state="translated">로지스틱 회귀.</target>
        </trans-unit>
        <trans-unit id="9cb16d85d5ce6d29fbfeaf0784f0b2e2c61ca591" translate="yes" xml:space="preserve">
          <source>LogisticRegression</source>
          <target state="translated">LogisticRegression</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegression은 로그 손실을 직접 최적화하므로 잘 보정 된 예측을 반환합니다. 반대로, 다른 방법은 방법마다 다른 바이어스를 사용하여 바이어스 된 확률을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="25965de326ab0450ae299a566621b897c09262e7" translate="yes" xml:space="preserve">
          <source>Long-awaited Generalized Linear Models with non-normal loss functions are now available. In particular, three new regressors were implemented: &lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt;&lt;code&gt;PoissonRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt;&lt;code&gt;GammaRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt;&lt;code&gt;TweedieRegressor&lt;/code&gt;&lt;/a&gt;. The Poisson regressor can be used to model positive integer counts, or relative frequencies. Read more in the &lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;User Guide&lt;/a&gt;. Additionally, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; supports a new &amp;lsquo;poisson&amp;rsquo; loss as well.</source>
          <target state="translated">이제 비정규 손실 함수를 사용하여 오랫동안 기다려온 일반화 선형 모델을 사용할 수 있습니다. ㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ ㅇㅇㅇ 특히 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.poissonregressor#sklearn.linear_model.PoissonRegressor&quot;&gt; &lt;code&gt;PoissonRegressor&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;../../modules/generated/sklearn.linear_model.gammaregressor#sklearn.linear_model.GammaRegressor&quot;&gt; &lt;code&gt;GammaRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.tweedieregressor#sklearn.linear_model.TweedieRegressor&quot;&gt; &lt;code&gt;TweedieRegressor&lt;/code&gt; 의&lt;/a&gt; 세 가지 새로운 회귀 변수가 구현 되었습니다 . 포아송 회귀 분석기는 양의 정수 수 또는 상대 빈도를 모델링하는 데 사용할 수 있습니다. 자세한 내용은 &lt;a href=&quot;../../modules/linear_model#generalized-linear-regression&quot;&gt;사용자 가이드를 참조하십시오&lt;/a&gt; . 또한 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 는 새로운 ' 푸 아송 '손실도 지원합니다.</target>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">경도 집 블록 경도</target>
        </trans-unit>
        <trans-unit id="900f1aa8e30fd6bc29b8e8d4cae98f421ef462cd" translate="yes" xml:space="preserve">
          <source>Looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others, like AGE, varies a lot more, several decades.</source>
          <target state="translated">기능 중요도를 측정하기 위해 계수 플롯을 살펴보면 일부는 작은 규모로 다르지만 AGE와 같은 다른 것들은 수십 년에 걸쳐 훨씬 더 다양하기 때문에 오해의 소지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="65fb0d7c39bbac75b47a97a31a7e2974bcfa2753" translate="yes" xml:space="preserve">
          <source>Looking closely at the WAGE distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models such as ridge or lasso work best for a normal distribution of error).</source>
          <target state="translated">임금 분포를 자세히 살펴보면 꼬리가 길다는 것을 알 수 있습니다. 이러한 이유로, 우리는 로그를 대략 정규 분포로 바꾸어야합니다. (리지나 올가미와 같은 선형 모델은 오차의 정규 분포를 위해 가장 잘 작동합니다).</target>
        </trans-unit>
        <trans-unit id="e9aa6c9b011905252d25083008c9809acc3f4160" translate="yes" xml:space="preserve">
          <source>Loss function used by the algorithm.</source>
          <target state="translated">알고리즘에서 사용하는 손실 함수입니다.</target>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">저수준 방법</target>
        </trans-unit>
        <trans-unit id="43b8e239b3dbfa96de18c459c0e716f889fbf1ff" translate="yes" xml:space="preserve">
          <source>Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.</source>
          <target state="translated">최저 예측 값에 대한 하한 (최소값이 여전히 더 높을 수 있음). 설정하지 않으면 기본값은 -inf입니다.</target>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">추론에 가장 적합한 가능성 (모델에 대한 훈련 데이터의)에 대한 하한값.</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">EM에 가장 잘 맞는 (모델에 대한 훈련 데이터의) 로그 우도에 대한 하한값.</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;X_train_&lt;/code&gt; 에서 커널의 삼각 삼각형 Cholesky 분해</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M. Bawa, T. Condie 및 P. Ganesan,&amp;ldquo;LSH Forest : 유사 검색을위한 자체 조정 색인&amp;rdquo;, WWW '05 World Wide Web에 관한 제 14 차 국제 회의 절차, 651-660, 2005.</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumont et al., &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;무작위 서브 윈도우 및 다중 출력 무작위 트리가있는 고속 멀티 클래스 이미지 주석&lt;/a&gt; , 2009 국제 컴퓨터 비전 이론 및 응용 프로그램 컨퍼런스</target>
        </trans-unit>
        <trans-unit id="4f0f168494bf38e0f99da8c5a97b71596101a871" translate="yes" xml:space="preserve">
          <source>M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, Journal of Machine Learning Research, Vol. 1, 2001.</source>
          <target state="translated">ME Tipping, Sparse Bayesian Learning and the Relevance Vector Machine, Journal of Machine Learning Research, Vol. 2001 년 1 월 1 일.</target>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M. Everingham, L. Van Gool, CKI Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt; , IJCV 2010.</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">MAE 출력은 음이 아닌 부동 소수점입니다. 가장 좋은 값은 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="dcac13e5386ab7554d99f42d2774ea6dec7dc033" translate="yes" xml:space="preserve">
          <source>MARR</source>
          <target state="translated">MARR</target>
        </trans-unit>
        <trans-unit id="203b5e4f3efe3b38e1b9f876ab3923dffadb1fa1" translate="yes" xml:space="preserve">
          <source>MARR_Unmarried</source>
          <target state="translated">MARR_Unmarried</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV 주택 소유 주택의 평균 가치 $ 1000</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLP는 비선형 모델을 훈련 데이터에 맞출 수 있습니다. &lt;code&gt;clf.coefs_&lt;/code&gt; 는 모델 매개 변수를 구성하는 가중치 행렬을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">MLP는 기능 스케일링에 민감합니다.</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">MLP는 숨겨진 뉴런, 레이어 및 반복 횟수와 같은 여러 하이퍼 파라미터를 조정해야합니다.</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">MLP는 2 개의 배열에 대해 학습한다 : 크기의 배열 X (n_samples, n_features)는 부동 소수점 특징 벡터로 표현 된 학습 샘플을 보유하고; 및 훈련 샘플에 대한 목표 값 (클래스 라벨)을 보유하는 크기 (n_samples)의 배열 y :</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">사용 MLP 열차 &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;확률 그라데이션 하강&lt;/a&gt; , &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;아담&lt;/a&gt; , 또는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS를&lt;/a&gt; . Stochastic Gradient Descent (SGD)는 적응이 필요한 매개 변수와 관련하여 손실 함수의 구배를 사용하여 매개 변수를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="4fc94508d8948819a05a769d08877751f90a9958" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;확률 적 경사 하강 법&lt;/a&gt; , &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt; 또는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS를&lt;/a&gt; 사용하는 MLP 훈련 . 확률 적 경사 하강 법 (SGD)은 적응이 필요한 매개 변수와 관련하여 손실 함수의 경사를 사용하여 매개 변수를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">역 전파를 사용하는 MLP 훈련 보다 정확하게는, 어떤 형태의 그라디언트 디센트를 사용하여 학습하고 그라디언트는 역 전파를 사용하여 계산됩니다. 분류를 위해 교차 엔트로피 손실 함수를 최소화하여 샘플 \ (x \) 당 확률 추정값 \ (P (y | x) \) 벡터를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">MLP는 문제 유형에 따라 다른 손실 기능을 사용합니다. 분류에 대한 손실 함수는 교차 엔트로피이며, 이진 경우에는</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">숨겨진 레이어가있는 MLP에는 로컬 최소값이 두 개 이상인 볼록하지 않은 손실 기능이 있습니다. 따라서 다른 임의 가중치 초기화는 다른 유효성 검사 정확도로 이어질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPClassifier는 각 시간 단계에서 모델 파라미터에 대한 손실 함수의 부분 미분 값이 계산되어 파라미터를 업데이트하기 때문에 반복적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="a1925f1c916c80accddbe48b0d0e8da75d102083" translate="yes" xml:space="preserve">
          <source>MLPRegressor</source>
          <target state="translated">MLPRegressor</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPRegressor는 각 시간 단계에서 모델 매개 변수에 대한 손실 함수의 부분 미분 값이 계산되어 매개 변수를 업데이트하므로 반복적으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">다항 로지스틱 + L1을 사용한 MNIST 분류</target>
        </trans-unit>
        <trans-unit id="fbb70ceca40f03de1a52a87cdebacfd0c2426668" translate="yes" xml:space="preserve">
          <source>MNIST classification using multinomial logistic + L1</source>
          <target state="translated">다항 로지스틱 + L1을 사용한 MNIST 분류</target>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">머신 러닝 알고리즘에는 데이터가 필요합니다. 각 &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; 하위 폴더로 이동 하여 &lt;code&gt;fetch_data.py&lt;/code&gt; 스크립트를 실행하십시오 (먼저 읽은 후).</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">기계 학습은 데이터 세트의 일부 특성을 학습 한 다음 다른 데이터 세트에 대해 해당 특성을 테스트하는 것입니다. 머신 러닝의 일반적인 방법은 데이터 세트를 2 개로 분할하여 알고리즘을 평가하는 것입니다. 우리는 그 세트 중 하나를 &lt;strong&gt;훈련 세트&lt;/strong&gt; 라고 부릅니다 . 우리는 다른 세트를 &lt;strong&gt;테스트 세트&lt;/strong&gt; 라고 부릅니다.이 세트 에서 학습 된 속성을 테스트합니다.</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">기계 학습 : 문제 설정</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="ca3ae45b6eafdd0a6dfea23df60842bdb389e9fc" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">훈련 세트의 마할 라 노비스 거리 ( &lt;a href=&quot;#sklearn.covariance.EllipticEnvelope.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 이라고 함) 관측치입니다.</target>
        </trans-unit>
        <trans-unit id="55e9152468183c8a16be469ac6ea3dbb0e42dd62" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is called) observations.</source>
          <target state="translated">훈련 세트의 마할 라 노비스 거리 ( &lt;a href=&quot;#sklearn.covariance.MinCovDet.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 이라고 함) 관측치입니다.</target>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">훈련 세트의 Mahalanobis 거리 ( &lt;code&gt;fit&lt;/code&gt; ) 관찰.</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">마할 라 노비스 중심까지 거리</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="7a412cc8631eaef465ac98b25829da66ced3b43d" translate="yes" xml:space="preserve">
          <source>Main takeaways</source>
          <target state="translated">주요 요점</target>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">입력 데이터를 복사하십시오.</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">2d에서 더 작은 원을 포함하는 큰 원을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">성과 지표 또는 손실 함수로 득점자를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="723bb825ac5b7b14f789cce44f7b667d39fe6543" translate="yes" xml:space="preserve">
          <source>Make and use a deep copy of X and Y (if Y exists)</source>
          <target state="translated">X 및 Y의 전체 복사본을 만들고 사용합니다 (Y가있는 경우).</target>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">교차 유효성 검사를 위해 배열을 색인 가능하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="5f9f781fbc2f44502a26f1a8945369508ac6ebd5" translate="yes" xml:space="preserve">
          <source>Make pipeline to preprocess the data</source>
          <target state="translated">데이터 전처리를위한 파이프 라인 만들기</target>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">X의 첫 번째 축에 최소 개수의 샘플이 있는지 확인하십시오 (2D 배열의 경우 행).</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">배열이 2D, 정사각형 및 대칭인지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D 어레이에 최소 개수의 기능 (열)이 있는지 확인하십시오. 기본값 1은 빈 데이터 세트를 거부합니다. 이 검사는 X가 효과적으로 2 차원이거나 원래 1D이고 &lt;code&gt;ensure_2d&lt;/code&gt; 가 True 인 경우에만 적용됩니다 . 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">2D 어레이에 최소 개수의 기능 (열)이 있는지 확인하십시오. 기본값 1은 빈 데이터 세트를 거부합니다. 이 확인은 입력 데이터가 실제로 2 차원이거나 원래 1D이고 &lt;code&gt;ensure_2d&lt;/code&gt; 가 True 인 경우에만 적용됩니다 . 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">배열의 첫 번째 축에 최소 개수의 샘플이 있는지 확인하십시오 (2D 배열의 경우 행). 0으로 설정하면이 검사가 비활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">모든 기능에서 동일한 스케일이 사용되는지 확인하십시오. 매니 폴드 학습 방법은 가장 가까운 이웃 검색을 기반으로하기 때문에 알고리즘이 제대로 수행되지 않을 수 있습니다. 이기종 데이터를 확장하는 편리한 방법은 &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="bd58d70c4390b61fe411e7823adf47cfb5052e9e" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; (see &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt;).</source>
          <target state="translated">모델을 피팅하기 전에 훈련 데이터를 퍼 뮤팅 (셔플)하거나 각 반복 후 셔플 하려면 &lt;code&gt;shuffle=True&lt;/code&gt; 를 사용하십시오 (기본적으로 사용됨). 또한 이상적으로는 &lt;code&gt;make_pipeline(StandardScaler(), SGDClassifier())&lt;/code&gt; 사용하여 기능을 표준화해야합니다 ( &lt;a href=&quot;compose#combining-estimators&quot;&gt;Pipelines&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">모델을 피팅하기 전에 학습 데이터를 퍼 뮤트 (셔플)하거나 &lt;code&gt;shuffle=True&lt;/code&gt; 를 사용 하여 각 반복 후에 셔플하십시오.</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">두 개의 인터리빙 반원 만들기</target>
        </trans-unit>
        <trans-unit id="69015b0f2ce90a52d27e40a08b160550cfb23a90" translate="yes" xml:space="preserve">
          <source>Making predictions</source>
          <target state="translated">예측하기</target>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">능금산:</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">능금산</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">매니 폴드 학습은 PCA와 같은 선형 프레임 워크를 데이터의 비선형 구조에 민감하게 일반화하려는 시도로 생각할 수 있습니다. 감독되는 변형이 존재하지만 일반적인 매니 폴드 학습 문제는 감독되지 않습니다. 미리 정해진 분류를 사용하지 않고 데이터 자체에서 데이터의 고차원 구조를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">잘린 영역의 매니 폴드 학습 방법</target>
        </trans-unit>
        <trans-unit id="5f7bca3c10846eb5854c536c3448fedf998ffbcf" translate="yes" xml:space="preserve">
          <source>Manifold learning</source>
          <target state="translated">매니 폴드 학습</target>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">매니 폴드 학습은 비선형 차원 축소 방법입니다. 이 작업의 알고리즘은 많은 데이터 세트의 차원이 인위적으로 만 높다는 아이디어를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">자필 자릿수에 대한 매니 폴드 학습 : 로컬 선형 임베딩, 아이소 맵&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="ad97dd09eb7e528a0b4debe1b9c745e650b7307a" translate="yes" xml:space="preserve">
          <source>Manually setting one of the environment variables (&lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt;, &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt;, or &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt;) will take precedence over what joblib tries to do. The total number of threads will be &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Note that setting this limit will also impact your computations in the main process, which will only use &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt;. Joblib exposes a context manager for finer control over the number of threads in its workers (see joblib docs linked below).</source>
          <target state="translated">환경 변수 ( &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt; , &lt;code&gt;MKL_NUM_THREADS&lt;/code&gt; , &lt;code&gt;OPENBLAS_NUM_THREADS&lt;/code&gt; 또는 &lt;code&gt;BLIS_NUM_THREADS&lt;/code&gt; ) 중 하나를 수동으로 설정하면 joblib가 수행하려는 작업보다 우선합니다. 총 스레드 수는 &lt;code&gt;n_jobs * &amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; 입니다. 이 제한을 설정하면 &lt;code&gt;&amp;lt;LIB&amp;gt;_NUM_THREADS&lt;/code&gt; 만 사용하는 기본 프로세스의 계산에도 영향을줍니다 . Joblib는 작업자의 스레드 수를보다 세밀하게 제어하기 위해 컨텍스트 관리자를 노출합니다 (아래 링크 된 joblib 문서 참조).</target>
        </trans-unit>
        <trans-unit id="0471386edfe0bf057e3ed471b66689189fdc892c" translate="yes" xml:space="preserve">
          <source>Manufacturing</source>
          <target state="translated">Manufacturing</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">많은 응용 프로그램에서 새 관측치가 기존 관측치와 동일한 분포에 속하는지 ( &lt;em&gt;inlier&lt;/em&gt; ) 또는 다른 것으로 간주되어야하는지 ( &lt;em&gt;outlier&lt;/em&gt; ) 결정할 수 있어야합니다 . 종종이 기능은 실제 데이터 세트를 정리하는 데 사용됩니다. 두 가지 중요한 차이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">많은 클러스터, 가능한 연결 제한</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">많은 클러스터, 연결 제약 조건, 비 유클리드 거리</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">많은 클러스터, 고르지 않은 클러스터 크기, 평평하지 않은 기하학</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">많은 데이터 세트에는 텍스트, 부동 및 날짜와 같은 다양한 유형의 기능이 포함되어 있으며 각 유형의 기능에는 별도의 전처리 또는 기능 추출 단계가 필요합니다. scikit-learn 메소드를 적용하기 전에 (예 : &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; 사용) 데이터를 사전 처리하는 것이 가장 쉬운 경우가 많습니다 . scikit-learn으로 데이터를 전달하기 전에 데이터를 처리하는 것은 다음 이유 중 하나로 인해 문제가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d589144c9193e129ab4721a317a3dc3eaa909106" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">많은 데이터 세트에는 텍스트, 부동 소수점 및 날짜와 같은 다양한 유형의 기능이 포함되어 있으며 각 기능 유형에는 별도의 전처리 또는 기능 추출 단계가 필요합니다. 종종 scikit-learn 메서드를 적용하기 전에 데이터를 사전 처리하는 것이 가장 쉽습니다 (예 : &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt; 사용) . 데이터를 scikit-learn에 전달하기 전에 처리하는 것은 다음 이유 중 하나로 인해 문제가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt; 와 같은 추가 매개 변수가 필요하기 때문에 많은 메트릭에 &lt;code&gt;scoring&lt;/code&gt; 값 으로 사용할 이름이 제공되지 않습니다 . 이러한 경우 적절한 점수 매기기 개체를 생성해야합니다. 스코어링을 위해 호출 가능한 객체를 생성하는 가장 간단한 방법은 &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; 를 사용하는 것 입니다. 이 기능은 메트릭을 모델 평가에 사용할 수있는 호출 가능으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="c6eff6cfb2b81378a219da9247d45ca528d18b55" translate="yes" xml:space="preserve">
          <source>Many scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt;&lt;code&gt;KNeighborsRegressor&lt;/code&gt;&lt;/a&gt;, but also some clustering methods such as &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;, and some manifold embeddings such as &lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt;&lt;code&gt;TSNE&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;Isomap&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">많은 scikit-learn 추정기는 &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.neighbors.kneighborsregressor#sklearn.neighbors.KNeighborsRegressor&quot;&gt; &lt;code&gt;KNeighborsRegressor&lt;/code&gt; &lt;/a&gt; 와 같은 여러 분류기 및 회귀 자 뿐만 아니라 &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;SpectralClustering&lt;/code&gt; &lt;/a&gt; 과 같은 일부 클러스터링 방법과 &lt;a href=&quot;generated/sklearn.manifold.tsne#sklearn.manifold.TSNE&quot;&gt; &lt;code&gt;TSNE&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;Isomap&lt;/code&gt; &lt;/a&gt; 과 같은 일부 매니 폴드 임베딩과 같은 최근 접 이웃에 의존합니다 .</target>
        </trans-unit>
        <trans-unit id="3a89fb6cdb80688c2fbf1190407f9abec89dd4f3" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">많은 통계 문제에는 모집단의 공분산 행렬 추정이 필요하며, 이는 데이터 세트 산점도 모양의 추정으로 볼 수 있습니다. 대부분의 경우 이러한 추정은 속성 (크기, 구조, 동질성)이 추정 품질에 큰 영향을 미치는 샘플에 대해 수행되어야합니다. &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; 의&lt;/a&gt; 패키지는 정확하게 다양한 설정에서 인구의 공분산 행렬을 추정 할 수있는 도구를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">많은 통계적 문제는 모집단 공분산 행렬의 추정을 필요로하며, 이는 데이터 세트 산점도 형태의 추정으로 볼 수 있습니다. 대부분의 경우, 이러한 추정은 특성 (크기, 구조, 동질성)이 추정 품질에 큰 영향을 미치는 샘플에서 수행해야합니다. &lt;code&gt;sklearn.covariance&lt;/code&gt; 의 패키지는 정확하게 다양한 설정에서 인구의 공분산 행렬을 추정 할 수있는 도구를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">많은, 더 많은&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">정규 분포에 데이터 매핑</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">전력 변환을 사용하여 데이터를 정규 분포에 매핑합니다.</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; 매개 변수를 사용하여 데이터를 표준 정규 분포에 매핑 합니다.</target>
        </trans-unit>
        <trans-unit id="12f61571505f27a87deaf94928434363c3add704" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution='normal'&lt;/code&gt;.</source>
          <target state="translated">매개 변수 &lt;code&gt;output_distribution='normal'&lt;/code&gt; 을 사용하여 데이터를 표준 정규 분포에 매핑 합니다.</target>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">i로 인코딩 된 값이 목록에서 i 번째가되도록 각 범주 형 기능 이름을 값 목록에 맵핑합니다.</target>
        </trans-unit>
        <trans-unit id="67b185421bf5d928c6a7d5bf74ee20d677f50228" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list. If &lt;code&gt;as_frame&lt;/code&gt; is True, this is None.</source>
          <target state="translated">i로 인코딩 된 값이 목록에서 i 번째가되도록 각 범주 기능 이름을 값 목록에 매핑합니다. 경우 &lt;code&gt;as_frame&lt;/code&gt; 가 True 인이은 없음입니다.</target>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">변환 된 데이터의 한계 분포. 선택 사항은 'uniform'(기본값) 또는 'normal'입니다.</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">Mark Schmidt, Nicolas Le Roux 및 Francis Bach : &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;확률 평균 그라디언트를 사용하여 유한 합계 최소화&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c75a2b42a0ab364e58b54559c45fcda50d1973f4" translate="yes" xml:space="preserve">
          <source>Married</source>
          <target state="translated">Married</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">X에서 사용할 마스크</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">Matern 커널.</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로 정규화 기 이전에 \ (\ ell_1 \)로 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로, 이것은 정규화 자로 \ (\ ell_1 \) \ (\ ell_2 \)와 \ (\ ell_2 \)를 혼합하여 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">수학적으로 정규화 기 이전에 혼합 된 \ (\ ell_1 \) \ (\ ell_2 \)로 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="85bbf115a5892290f61e157be8a21f18c7185291" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm and \(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">수학적으로는 정규화를 위해 \ (\ ell_1 \) \ (\ ell_2 \)-norm 및 \ (\ ell_2 \)-norm이 혼합 된 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a22bf31bc3080adb481367d05d21298b6681e5ee" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\)-norm for regularization. The objective function to minimize is:</source>
          <target state="translated">수학적으로 정규화를 위해 혼합 된 \ (\ ell_1 \) \ (\ ell_2 \)-norm으로 훈련 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47b46d2e2bec7b475ce40ef0c30d61c9f27a62f1" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:</source>
          <target state="translated">수학적으로는 정규화 항이 추가 된 선형 모델로 구성됩니다. 최소화 할 목적 함수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">수학적으로이 수축은 경험적 공분산 행렬의 최소값과 최대 값 사이의 비율을 줄이는 것으로 구성됩니다. 주어진 오프셋에 따라 모든 고유 값을 간단히 이동하여 수행 할 수 있습니다. 이는 공분산 행렬의 l2-penalized Maximum Likelihood Estimator를 찾는 것과 같습니다. 실제로 수축은 간단한 볼록 변환으로 요약됩니다. \ (\ Sigma _ {\ rm shrunk} = (1- \ alpha) \ hat {\ Sigma} + \ alpha \ frac {{\ rm Tr} \ hat {\ 시그마}} {p} \ rm ID \).</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">수학적으로, 학습 샘플 \ (X \)에 적용된 잘린 SVD는 낮은 순위 근사화 \ (X \)를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">\ (C_ {i, j} \)가 실제 클래스 \ (i \) 및 예측 된 클래스 \ (j \)의 샘플 수인 행렬 \ (C \). 경우 &lt;code&gt;eps is None&lt;/code&gt; ,이 배열의 DTYPE은 정수가됩니다. 경우 &lt;code&gt;eps&lt;/code&gt; 주어진다는 DTYPE은 플로트 될 것입니다. 수 있을까요 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 경우 &lt;code&gt;sparse=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">점 사이의 유사성의 행렬</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">스케일링 할 매트릭스.</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">분해 할 행렬</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">피처의 분산을 사용하여 정규화 할 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">두 개의 열을 교환 할 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">두 개의 행을 바꾸는 행렬.</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">E- 단계에서 문서 주제 분배를 업데이트하기위한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">ELBO를 최대화하는 것은 \ (q (z, \ theta, \ beta) \)와 실제 후부 \ (p (z, \ theta, \ beta | w, \ alpha, \ eta) \).</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">목표 평균을 뺀 후 로그 한계 우도를 최대화하면 LML이 -83.214 인 다음 커널이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">Johnson-Lindenstrauss lemma에 의해 정의 된 최대 왜곡률. 배열이 제공되면 안전한 배열의 구성 요소를 배열별로 계산합니다.</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">최대 우도 공분산 추정기</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">잔차의 최대 표준입니다. None이 아닌 경우 n_nonzero_coefs를 대체합니다.</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">각 노드에서 최대 CF 서브 클러스터 수 서브 클러스터 수가 branching_factor를 초과하도록 새 샘플이 입력되면 해당 노드는 서브 클러스터가 각각 재분배 된 두 개의 노드로 분할됩니다. 해당 노드의 부모 하위 클러스터가 제거되고 두 개의 새 하위 클러스터가 두 개의 분할 노드의 부모로 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">&lt;code&gt;tol&lt;/code&gt; 개선을 충족시키지 못하는 최대 에포크 수 . solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="7d6dde474581154e61a4c14f0f2c50aef90aa524" translate="yes" xml:space="preserve">
          <source>Maximum number of imputation rounds to perform before returning the imputations computed during the final round. A round is a single imputation of each feature with missing values. The stopping criterion is met once &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt; tol, where &lt;code&gt;X_t&lt;/code&gt; is &lt;code&gt;X&lt;/code&gt; at iteration &lt;code&gt;t. Note that early stopping is only
applied if ``sample_posterior=False`&lt;/code&gt;.</source>
          <target state="translated">최종 라운드 동안 계산 된 대치를 반환하기 전에 수행 할 대치 라운드의 최대 수입니다. 라운드는 결 측값이있는 각 특성의 단일 대치입니다. 중단 기준은 &lt;code&gt;abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))&lt;/code&gt; &amp;lt;tol 일 때 &lt;code&gt;X_t&lt;/code&gt; 됩니다. 여기서 X_t 는 반복 &lt;code&gt;t. Note that early stopping is only applied if ``sample_posterior=False`&lt;/code&gt; 에서 &lt;code&gt;X&lt;/code&gt; 입니다. 조기 중지는``sample_posterior = False` 인 경우에만 적용됩니다 .</target>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">최대 반복 횟수</target>
        </trans-unit>
        <trans-unit id="ec0c7c7cfd1a4dd46776f2839b3e2ee30db0ceb0" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations allowed.</source>
          <target state="translated">허용되는 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">시간이 초과되기 전의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">맞추는 동안 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">arpack의 최대 반복 횟수입니다. None이면 arpack에서 최적의 값을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 'sparse_cg'및 'lsqr'솔버의 경우 기본값은 scipy.sparse.linalg에 의해 결정됩니다. 'sag'솔버의 경우 기본값은 1000입니다.</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 'sparse_cg'및 'lsqr'솔버의 경우 기본값은 scipy.sparse.linalg에 의해 결정됩니다. 'sag'및 saga 솔버의 경우 기본값은 1000입니다.</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">켤레 그라디언트 솔버의 최대 반복 횟수입니다. 기본값은 scipy.sparse.linalg에 의해 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">무작위 샘플 선택에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">arpack 솔버의 최대 반복 횟수입니다. eigen_solver == 'dense'인 경우 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">공간 중앙값 계산을위한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">최적화를위한 최대 반복 횟수입니다. 250 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">솔버에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="23888143df53d4407b7d5db42e819ad8fc40bfb6" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations in the optimization.</source>
          <target state="translated">최적화의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">단일 실행에 대한 SMACOF 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">단일 실행에 대한 k- 평균 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">실행할 k- 평균 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">최적화 알고리즘의 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">초기 중지 기준 추론과 독립적으로 중지하기 전에 전체 데이터 세트에 대한 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="a1da02f682252ffa124e7544bc23e0152d7ce8c8" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations performed on each seed.</source>
          <target state="translated">각 시드에서 수행되는 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="ff75c0f1937b00a0d18d3456b10469179a13bfd5" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations run across all classes.</source>
          <target state="translated">모든 클래스에서 실행되는 최대 반복 수입니다.</target>
        </trans-unit>
        <trans-unit id="5d873ede9710528d1162698e1b4121133119149c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">솔버가 수렴하는 데 걸리는 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="0f2d5ee22794f6faeed2ed56167e69832301d9fb" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that &lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; should run for.</source>
          <target state="translated">&lt;code&gt;scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)&lt;/code&gt; 가 실행되어야하는 최대 반복 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">될 수 반복의 최대 수는 0 라이어 또는에 의해 정의 된 유효하지 않은 데이터 발견으로 인해 생략 &lt;code&gt;is_data_valid&lt;/code&gt; 에 의해 정의되거나 잘못된 모델 &lt;code&gt;is_model_valid&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">scipy.optimize.fmin_l_bfgs_b를 실행해야하는 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 경우 수행 할 최대 반복 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="f8b9c126c90c88245cf150185cad9a95bb9ec437" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; or &lt;code&gt;lasso_lars&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;algorithm='lasso_cd'&lt;/code&gt; 또는 &lt;code&gt;lasso_lars&lt;/code&gt; 인 경우 수행 할 최대 반복 횟수입니다 .</target>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">Lars 알고리즘에서 수행 할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="67f387c33c051b181969f68bc7a00e313e621f7a" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform when solving the lasso problem.</source>
          <target state="translated">올가미 문제를 해결할 때 수행 할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">수행 할 최대 반복 횟수로, 무제한으로 무한대로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">수행 할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">수행 할 최대 반복 횟수입니다. 조기 정지에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">최적화를 중단하기 전에 진행되지 않은 최대 반복 횟수로, 과장된 초기 250 번의 반복 이후에 사용됩니다. 진행률은 50 번 반복 될 때마다 확인되므로이 값은 50의 다음 배수로 반올림됩니다.</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">아직 수렴되지 않은 경우 클러스터링 작업이 종료되기 전 (해당 시드 지점에 대해) 시드 지점 당 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">최대 반복 횟수입니다. 기본값은 300</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">최대 반복 횟수입니다. 기본값은 300입니다.</target>
        </trans-unit>
        <trans-unit id="78adce28aba1dcec634d8308251161f963f008c1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Should be greater than or equal to 1.</source>
          <target state="translated">최대 반복 횟수입니다. 1보다 크거나 같아야합니다.</target>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">최대 반복 횟수입니다. 솔버는 수렴 ( 'tol'으로 결정) 또는이 반복 횟수까지 반복됩니다. 확률 적 솔버 ( 'sgd', 'adam')의 경우, 그래디언트 단계 수가 아니라 에포크 수 (각 데이터 포인트가 몇 번 사용되는지)를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">계산 효율의 Quantile을 추정하는 데 사용되는 최대 샘플 수입니다. 서브 샘플링 절차는 값이 동일한 희소 행렬과 밀도가 높은 행렬에 따라 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">수행 할 최대 반복 횟수이므로 포함 할 최대 기능입니다. &lt;code&gt;n_features&lt;/code&gt; 10 % 이지만 사용 가능한 경우 5 이상입니다.</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; 또는 &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; 인 경로의 노드 수 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;max_iter&lt;/code&gt; , &lt;code&gt;n_features&lt;/code&gt; 또는 &lt;code&gt;alpha&lt;/code&gt; 보다 큰 상관 관계가있는 경로의 노드 수 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">각 반복에서 최대 공분산 (절대 값으로). &lt;code&gt;n_alphas&lt;/code&gt; 는 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 또는 &lt;code&gt;n_features&lt;/code&gt; 중 작은 값입니다.</target>
        </trans-unit>
        <trans-unit id="2519f5f4a0d8bcad0dcee25fb3c2cfe0d8efda04" translate="yes" xml:space="preserve">
          <source>Maximum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one max value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to np.inf.</source>
          <target state="translated">가능한 최대 대치 값입니다. 스칼라 인 경우 모양 (n_features)으로 브로드 캐스팅됩니다. 배열과 유사한 경우 각 기능에 대해 하나의 최대 값 인 모양 (n_features,)을 예상합니다. &lt;code&gt;None&lt;/code&gt; (기본값)은 np.inf로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">데이터 샘플이 inlier로 분류되는 최대 잔차입니다. 기본적으로 임계 값은 목표 값 &lt;code&gt;y&lt;/code&gt; 의 MAD (중앙 절대 편차)로 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">단일 트레이닝 세트의 최대 크기.</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">샘플에 대한 X의 최대 제곱합. SAG 솔버에서만 사용됩니다. None이면 모든 샘플을 통과하여 계산됩니다. 교차 검증 속도를 높이려면 값을 미리 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">최대 단계 크기 (규정 화). 기본값은 1.0입니다.</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">담즙의 최대 값.</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">오른쪽 경계에 대한 입력 배열 &lt;code&gt;X_&lt;/code&gt; 의 최대 값 .</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">Jaccard 계수를 사용하는 문자열&amp;ldquo;jaccard&amp;rdquo;또는 4 개의 인수를 사용하는 함수일 수 있으며 각 인수는 1d 표시기 벡터입니다 (a_rows, a_columns, b_rows, b_columns).</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">( 'headers', 'footers', 'quotes')의 하위 집합을 포함 할 수 있습니다. 이들 각각은 뉴스 그룹 게시물에서 감지 및 제거되어 분류자가 메타 데이터에 과적 합하지 못하게하는 일종의 텍스트입니다.</target>
        </trans-unit>
        <trans-unit id="5b986034f147ad60f742da340ca1bd20b5f08ce7" translate="yes" xml:space="preserve">
          <source>McCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.</source>
          <target state="translated">McCullagh, Peter; Nelder, John (1989). 일반화 선형 모형, 제 2 판. Boca Raton : Chapman 및 Hall / CRC. ISBN 0-412-31760-5.</target>
        </trans-unit>
        <trans-unit id="72639b42074abb6c8ea20684337c75e653e13eb2" translate="yes" xml:space="preserve">
          <source>McSherry, F., &amp;amp; Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.</source>
          <target state="translated">McSherry, F., &amp;amp; Najork, M. (2008, 3 월). 컴퓨팅 정보 검색 성능은 공동 점수가있을 때 효율적으로 측정합니다. 정보 검색에 관한 유럽 회의 (pp. 414-421). Springer, Berlin, Heidelberg.</target>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">평균 절대 오차 :</target>
        </trans-unit>
        <trans-unit id="90a417c7a65441a9ebd4508d554460a1437266a0" translate="yes" xml:space="preserve">
          <source>Mean Gamma deviance regression loss.</source>
          <target state="translated">평균 감마 이탈도 회귀 손실.</target>
        </trans-unit>
        <trans-unit id="b5e71e9559d855f0bb974ab566c48b140de9b95a" translate="yes" xml:space="preserve">
          <source>Mean Poisson deviance regression loss.</source>
          <target state="translated">평균 포아송 이탈도 회귀 손실입니다.</target>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">모든 샘플에 대한 평균 실루엣 계수.</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">평균 제곱 오차 :</target>
        </trans-unit>
        <trans-unit id="04bada6d1e51be3ec0e69f413da51c5f62d2f9fc" translate="yes" xml:space="preserve">
          <source>Mean Tweedie deviance regression loss.</source>
          <target state="translated">평균 Tweedie 이탈도 회귀 손실.</target>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">평균 절대 오차 회귀 손실</target>
        </trans-unit>
        <trans-unit id="640f16564a014e166473dc43ba616e7e2ca59c7f" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) w.r.t. y.</source>
          <target state="translated">self.predict (X) wrty의 평균 정확도</target>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict (X) wrt의 평균 정확도. 와이.</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">best_estimator의 평균 교차 검증 된 점수</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">best_estimator의 교차 검증 된 평균 점수입니다.</target>
        </trans-unit>
        <trans-unit id="905638cd4671af1b6b218d93d25fb805a7548993" translate="yes" xml:space="preserve">
          <source>Mean of feature importance over &lt;code&gt;n_repeats&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;n_repeats&lt;/code&gt; 보다 중요한 기능의 평균입니다 .</target>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">쿼리 포인트 예측 예측 수단</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">쿼리 지점의 예측 분포 평균입니다.</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">훈련 목표의 평균 또는 중앙값 또는 Quantile 또는 사용자가 제공 한 일정한 값.</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">평균 이동 클러스터링은 매끄러운 샘플 밀도에서 &quot;블롭&quot;을 발견하는 것을 목표로합니다. 중심 기반 알고리즘으로, 중심에 대한 후보를 지정된 지역 내 포인트의 평균으로 업데이트하여 작동합니다. 그런 다음 이러한 후보를 후 처리 단계에서 필터링하여 거의 중복되는 부분을 제거하여 최종 중심 세트를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">플랫 커널을 사용한 평균 교대 클러스터링.</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">각 접힘의 검정 세트에 대한 평균 제곱 오차는 l1_ratio 및 alpha에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">평균 제곱 오차 회귀 손실</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">평균 제곱 로그 오류 회귀 손실</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="896bb25ed00af769d9d9cdb21af7655bd0a22654" translate="yes" xml:space="preserve">
          <source>Measure and plot the results</source>
          <target state="translated">결과 측정 및 플로팅</target>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">포인트 집합의 두 군집의 유사성을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">X의 측정 오류</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">y의 측정 오류</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">블록 단위의 MedInc 중간 소득</target>
        </trans-unit>
        <trans-unit id="ca6bd4b635d61f1c13fd0394e55000bb30738881" translate="yes" xml:space="preserve">
          <source>Median absolute error output is non-negative floating point. The best value is 0.0. Read more in the &lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">중앙값 절대 오류 출력은 음이 아닌 부동 소수점입니다. 가장 좋은 값은 0.0입니다. 자세한 내용은 &lt;a href=&quot;../model_evaluation#median-absolute-error&quot;&gt;사용자 가이드를 참조하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">절대 오차 회귀 손실의 중앙값</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">중간 &lt;code&gt;n_samples&lt;/code&gt; , 작은 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 회원 은 가중치를 보유합니다 (\ (w \)</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 멤버 는 \ (b \)를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 멤버 는 인터셉트 (일명 오프셋 또는 바이어스)를 보유합니다.</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">numpy 배열에 대한 젬핑 모드가 작업자에게 전달되었습니다. 자세한 내용은 'max_nbytes'매개 변수 설명서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">큰 샘플 크기를위한 메모리 소비</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">변환 된 대상에서 회귀하는 메타 추정기</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">변압기로 복합 모델을 구축하기위한 메타 추정기</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">중요도 가중치를 기반으로 기능을 선택하기위한 메타 트랜스포머.</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="bebb8fb7cc6768e9928f39fcd0184089c7f19d03" translate="yes" xml:space="preserve">
          <source>Method for initialization</source>
          <target state="translated">초기화 방법</target>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">k- 평균 알고리즘의 초기화 방법; 기본값은 'k- 평균 ++'입니다.</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">초기화 방법, 기본값은 'k- 평균 ++'입니다.</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">초기화 방법은 기본적으로 'k-means ++'입니다.</target>
        </trans-unit>
        <trans-unit id="0e3feb124243dbfe777425d1e7de652a8a95432b" translate="yes" xml:space="preserve">
          <source>Method for initialization:</source>
          <target state="translated">초기화 방법 :</target>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">방법 이름</target>
        </trans-unit>
        <trans-unit id="f2ddbb16b4269f001b2886168e6ee73674985a74" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;.</source>
          <target state="translated">특이 벡터를 정규화하고 이중 군집으로 변환하는 방법입니다. 'scale', 'bistochastic'또는 'log'중 하나 일 수 있습니다. 저자는 '로그'사용을 권장합니다. 그러나 데이터가 희소하면 로그 정규화가 작동하지 않으므로 기본값이 'bistochastic'입니다.</target>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">특이 벡터를 biclusters로 정규화하고 변환하는 방법. 'scale', 'bistochastic'또는 'log'중 하나 일 수 있습니다. 저자는 'log'를 사용하는 것이 좋습니다. 그러나 데이터가 드문 경우 로그 정규화가 작동하지 않으므로 기본값이 'bistochastic'입니다. 주의 : &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; 인 경우 데이터가 희박하지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">최단 경로를 찾는 데 사용하는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">변환 된 결과를 인코딩하는 데 사용되는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">절차를 초기화하는 데 사용되는 방법. 기본값 : n_components &amp;lt;n_features 인 경우 'nndsvd', 그렇지 않은 경우 무작위. 유효한 옵션 :</target>
        </trans-unit>
        <trans-unit id="2babbe784e2e75bf6bc5c5da588a447b4ed201c9" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None.</source>
          <target state="translated">프로 시저를 초기화하는 데 사용되는 방법입니다. 기본값 : 없음.</target>
        </trans-unit>
        <trans-unit id="7506fd4c85f1f80b13ff77585a3eea447916c5af" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: None. Valid options:</source>
          <target state="translated">프로 시저를 초기화하는 데 사용되는 방법입니다. 기본값 : 없음. 유효한 옵션 :</target>
        </trans-unit>
        <trans-unit id="6e9cbcdd1d5058381751f008677bca0c5dd1dd9b" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">&lt;code&gt;_component&lt;/code&gt; 를 업데이트하는 데 사용되는 방법 입니다. &lt;a href=&quot;#sklearn.decomposition.LatentDirichletAllocation.fit&quot;&gt; &lt;code&gt;fit&lt;/code&gt; &lt;/a&gt; 방법 에서만 사용됩니다 . 일반적으로 데이터 크기가 크면 온라인 업데이트가 일괄 업데이트보다 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">&lt;code&gt;_component&lt;/code&gt; 를 업데이트하는 데 사용되는 방법 . &lt;code&gt;fit&lt;/code&gt; 방법으로 만 사용하십시오 . 일반적으로 데이터 크기가 크면 온라인 업데이트가 배치 업데이트보다 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="8de2b023f4bb12cf6f4720283ae55f1dda2214ee" translate="yes" xml:space="preserve">
          <source>Methods called for each base estimator. It can be:</source>
          <target state="translated">각 기본 추정량에 대해 호출되는 메소드. 그것은 될 수 있습니다:</target>
        </trans-unit>
        <trans-unit id="66a03315f429c2912a3083f4347509dcbe2b4de3" translate="yes" xml:space="preserve">
          <source>Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.</source>
          <target state="translated">거리 계산에 사용할 미터법입니다. scikit-learn 또는 scipy.spatial.distance의 모든 측정 항목을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a01ea489bdc9c9d6a182edc027242ff94374f848" translate="yes" xml:space="preserve">
          <source>Metric used to compute distances to neighbors.</source>
          <target state="translated">이웃까지의 거리를 계산하는 데 사용되는 메트릭입니다.</target>
        </trans-unit>
        <trans-unit id="223dc067df313c03c1e792b17b98d4c951add488" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;ldquo;precomputed&amp;rdquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted. If &amp;ldquo;precomputed&amp;rdquo;, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.</source>
          <target state="translated">연결을 계산하는 데 사용되는 메트릭입니다. &quot;유클리드&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;맨해튼&quot;, &quot;코사인&quot;또는 &quot;사전 계산&quot;일 수 있습니다. 연결이 &quot;ward&quot;인 경우 &quot;유클리드&quot;만 허용됩니다. &quot;미리 계산 된&quot;경우, 적합 방법에 대한 입력으로 거리 행렬 (유사성 행렬 대신)이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">연결을 계산하는 데 사용되는 메트릭입니다. &quot;유클리드&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;맨해튼&quot;, &quot;코사인&quot;또는 &quot;사전 계산 된&quot;일 수 있습니다. 연결이 &quot;병동&quot;이면 &quot;유클리드&quot;만 허용됩니다.</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">다양한 머신 러닝 작업에 사용할 수있는 메트릭은 아래 섹션에 자세히 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="6333551e93ef2383df0508df89bd1ca423b74bc9" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping, &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;, 2001.</source>
          <target state="translated">Michael E. Tipping, &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;, 2001.</target>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">마이클 E. 팁 : &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;스파 스 베이지안 학습 및 관련성 벡터 머신&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">마이클 마샬 (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">미니 배치 K- 평균 군집</target>
        </trans-unit>
        <trans-unit id="f81af70401b9fbaba1cc8f3d806f31408afbc851" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering.</source>
          <target state="translated">미니 배치 K- 평균 클러스터링.</target>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">미니 배치 스파 스 주요 구성 요소 분석</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">미니 배치 사전 학습</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">미니 배치 스파 스 PCA ( &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt; ) 는 더 빠르지 만 정확성이 &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt; 의 변형입니다 . 주어진 반복 횟수에 대해 기능 집합의 작은 덩어리를 반복하여 속도를 높입니다.</target>
        </trans-unit>
        <trans-unit id="acc629f9bc13af6fe4ccc30d47949b9a29f4708a" translate="yes" xml:space="preserve">
          <source>Minimal cost complexity pruning recursively finds the node with the &amp;ldquo;weakest link&amp;rdquo;. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. To get an idea of what values of &lt;code&gt;ccp_alpha&lt;/code&gt; could be appropriate, scikit-learn provides &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt;&lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt;&lt;/a&gt; that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.</source>
          <target state="translated">최소 비용 복잡성 정리는 &quot;가장 약한 링크&quot;가있는 노드를 재귀 적으로 찾습니다. 가장 약한 링크는 유효 알파가 특징이며, 유효 알파가 가장 작은 노드가 먼저 제거됩니다. &lt;code&gt;ccp_alpha&lt;/code&gt; 의 값이 적절할 수 있는지 파악하기 위해 scikit-learn은 가지 치기 프로세스의 각 단계에서 유효 알파와 해당하는 총 잎 불순물을 반환하는 &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path&quot;&gt; &lt;code&gt;DecisionTreeClassifier.cost_complexity_pruning_path&lt;/code&gt; &lt;/a&gt; 를 제공 합니다. 알파가 증가하면 더 많은 나무가 가지 치기되어 잎의 총 불순물이 증가합니다.</target>
        </trans-unit>
        <trans-unit id="8f8dae4007afe0299bfaa99f8bc74594de701fdf" translate="yes" xml:space="preserve">
          <source>Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of &lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]&lt;/a&gt;. This algorithm is parameterized by \(\alpha\ge0\) known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of a given tree \(T\):</source>
          <target state="translated">최소 비용 복잡도 가지 치기는 과적 합을 피하기 위해 트리를 가지 치기하는 데 사용되는 알고리즘으로 &lt;a href=&quot;#bre&quot; id=&quot;id2&quot;&gt;[BRE]의&lt;/a&gt; 3 장에 설명되어 있습니다. 이 알고리즘은 복잡도 매개 변수로 알려진 \ (\ alpha \ ge0 \)에 의해 매개 변수화됩니다. 복잡성 매개 변수는 주어진 트리 \ (T \)의 비용 복잡성 측정 \ (R_ \ alpha (T) \)을 정의하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">목적 함수를 최소화합니다 :</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">확률 평균 그라디언트를 사용하여 유한 합계 최소화 &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">최소 공분산 결정기 (MCD) : 공분산의 강력한 추정량.</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">최소 공분산 결정 인자 추정기</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">경로를 따라 최소 상관 관계. 올가미의 정규화 매개 변수 alpha 매개 변수에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">충분한 항목이 &lt;code&gt;min_hash_match&lt;/code&gt; 제약 조건을 충족한다고 가정하면 추정 기당 평가 된 최소 후보 수입니다 .</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">원본 데이터에서 무작위로 선택된 최소 샘플 수. &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; 대한 절대 샘플 수로 처리되고 &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; 대한 상대 수 &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; )로 처리 됩니다. 이것은 일반적으로 주어진 &lt;code&gt;base_estimator&lt;/code&gt; 를 추정하는 데 필요한 최소 샘플 수로 선택됩니다 . 기본적으로 &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; 추정기가 가정되고 &lt;code&gt;min_samples&lt;/code&gt; 가 &lt;code&gt;X.shape[1] + 1&lt;/code&gt; 로 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="7055b8e6fba3c22d096f09a773f8fa2a0f2c2a45" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead.</source>
          <target state="translated">OPTICS 클러스터의 최소 샘플 수로, 절대 수 또는 샘플 수의 일부 (최소 2로 반올림)로 표시됩니다. 경우 &lt;code&gt;None&lt;/code&gt; 의 값 &lt;code&gt;min_samples&lt;/code&gt; 이 대신 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="143330e48296c9730a85d5042f3f4108bc450e1a" translate="yes" xml:space="preserve">
          <source>Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If &lt;code&gt;None&lt;/code&gt;, the value of &lt;code&gt;min_samples&lt;/code&gt; is used instead. Used only when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">OPTICS 클러스터의 최소 샘플 수로, 절대 수 또는 샘플 수의 일부 (최소 2로 반올림)로 표시됩니다. 경우 &lt;code&gt;None&lt;/code&gt; 의 값 &lt;code&gt;min_samples&lt;/code&gt; 이 대신 사용됩니다. &lt;code&gt;cluster_method='xi'&lt;/code&gt; 인 경우에만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="1c28e0e254bdc137eed30061d47cd3e6720cfe75" translate="yes" xml:space="preserve">
          <source>Minimum possible imputed value. Broadcast to shape (n_features,) if scalar. If array-like, expects shape (n_features,), one min value for each feature. &lt;code&gt;None&lt;/code&gt; (default) is converted to -np.inf.</source>
          <target state="translated">가능한 최소 대치 값입니다. 스칼라 인 경우 모양 (n_features)으로 브로드 캐스팅됩니다. 배열과 유사한 경우 각 기능에 대해 최소값 하나 인 모양 (n_features,)을 예상합니다. &lt;code&gt;None&lt;/code&gt; (기본값)은 -np.inf로 변환됩니다.</target>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">담즙의 최소값.</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">왼쪽 경계에 대한 입력 배열 &lt;code&gt;X_&lt;/code&gt; 의 최소값 입니다.</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="8984ca78ae6f645a8da0469517e3e68a7c22986d" translate="yes" xml:space="preserve">
          <source>Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between &lt;code&gt;1e0&lt;/code&gt; and &lt;code&gt;1e3&lt;/code&gt;:</source>
          <target state="translated">그리드 검색에서 위의 예를 미러링하면 &lt;code&gt;1e0&lt;/code&gt; 과 &lt;code&gt;1e3&lt;/code&gt; 사이에 로그 균일하게 분포 된 연속 랜덤 변수를 지정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">&lt;code&gt;class_log_prior_&lt;/code&gt; 를 선형 모델로 해석하기 위해 class_log_prior_ 를 미러링 합니다.</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">MultinomialNB를 선형 모델로 해석하기 위해 &lt;code&gt;feature_log_prob_&lt;/code&gt; 를 미러링 합니다.</target>
        </trans-unit>
        <trans-unit id="5f2cbd107037ed23248e5058a7a64cd6bae05468" translate="yes" xml:space="preserve">
          <source>Miscellaneous</source>
          <target state="translated">Miscellaneous</target>
        </trans-unit>
        <trans-unit id="0d2ecb69e7b12979a3e40a5ab8b5183911f1a3c3" translate="yes" xml:space="preserve">
          <source>Miscellaneous and introductory examples for scikit-learn.</source>
          <target state="translated">scikit-learn에 대한 기타 및 입문 예제입니다.</target>
        </trans-unit>
        <trans-unit id="26655e342820eb1000893c259228858eef67a34d" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values</source>
          <target state="translated">누락 된 속성 값</target>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">누락 된 속성 값 :</target>
        </trans-unit>
        <trans-unit id="905705cdb93f7b29194485a892d91da6c1cdc874" translate="yes" xml:space="preserve">
          <source>Missing Value Imputation</source>
          <target state="translated">결 측값 대치</target>
        </trans-unit>
        <trans-unit id="67cc34b1cd58b9ef03f7ff376e8541732aac180b" translate="yes" xml:space="preserve">
          <source>Missing information</source>
          <target state="translated">빠진 정보</target>
        </trans-unit>
        <trans-unit id="0e00e76132a4d6b917901e5526c250a336a29108" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">누락 된 값은 기본 &lt;a href=&quot;../../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; 를&lt;/a&gt; 사용하여 평균, 중앙값 또는 가장 빈번한 값으로 대체 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt; 은 기본 sklearn.impute.SimpleImputer를 사용하여 평균, 중간 값 또는 가장 빈번한 값으로 대체 할 수 있습니다 . 중앙값은 결과를 지배 할 수있는 높은 크기의 변수가있는 데이터에 대한보다 강력한 추정값입니다 ( '롱테일'이라고도 함).</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">'데이터'에서 결 측값은 NaN으로 표시됩니다. '대상'에서 결 측값은 NaN (숫자 대상) 또는 없음 (범주 대상)으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">Scikit-learn의 모든 bicluster 추정기에 대한 믹스 인 클래스</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 분류자를위한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 클러스터 추정기에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 밀도 추정 기용 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 회귀 추정량에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">scikit-learn의 모든 변압기에 대한 믹스 인 클래스.</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">모델 복잡성 영향</target>
        </trans-unit>
        <trans-unit id="9c567347b8af7d91b331f07af5f77ec0a361f505" translate="yes" xml:space="preserve">
          <source>Model Selection</source>
          <target state="translated">모델 선택</target>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">모델 선택 인터페이스</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">모델 블렌딩 : 하나의 감독 된 추정기의 예측이 앙상블 방법으로 다른 추정기를 훈련시키는 데 사용될 때.</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">모델 복잡성</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">scikit-learn의 모델 압축은 현재 선형 모델에만 해당됩니다. 이와 관련하여 모델 희소성 (즉, 모델 벡터에서 0이 아닌 좌표의 수)을 제어하려고 함을 의미합니다. 일반적으로 모델 희소성을 희소 입력 데이터 표현과 결합하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">모델 지속성</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">모델 재구성은 사용 가능한 기능 중 일부만 모델에 맞도록 선택하는 것으로 구성됩니다. 다시 말해, 학습 단계에서 모델이 피쳐를 버린 경우 입력에서 피쳐를 제거 할 수 있습니다. 이것은 몇 가지 이점이 있습니다. 먼저 모델 자체의 메모리 (및 시간) 오버 헤드가 줄어 듭니다. 또한 이전 실행에서 유지할 기능을 알고 있으면 파이프 라인에서 명시 적 기능 선택 구성 요소를 버릴 수도 있습니다. 마지막으로, 모델에 의해 폐기 된 기능을 수집 및 구축하지 않음으로써 데이터 액세스 및 기능 추출 계층에서 업스트림 처리 시간 및 I / O 사용을 줄일 수 있습니다. 예를 들어 원시 데이터가 데이터베이스에서 온 경우 쿼리가 더 가벼운 레코드를 반환하도록하여 더 간단하고 빠른 쿼리를 작성하거나 I / O 사용을 줄일 수 있습니다. 현재재구성은 scikit-learn에서 수동으로 수행해야합니다. 드문 드문 입력의 경우 (특히 &lt;code&gt;CSR&lt;/code&gt; 형식)의 경우 일반적으로 관련 기능을 생성하지 않고 열을 비워두면 충분합니다.</target>
        </trans-unit>
        <trans-unit id="28eeecfcba5c4e3a6c993b8bf2c6736730dfbd15" translate="yes" xml:space="preserve">
          <source>Model selection</source>
          <target state="translated">모델 선택</target>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; &lt;/a&gt; 와 같은 도구를 사용한 모델 선택 및 평가는 평가 된 평가자에 적용되는 메트릭을 제어 하는 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">다양한 파라미터 설정을 평가하여 모델을 선택하면 레이블이 지정된 데이터를 사용하여 그리드의 파라미터를 &quot;트레이닝&quot;할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">확률 적 PCA 및 요인 분석 (FA)을 사용한 모델 선택</target>
        </trans-unit>
        <trans-unit id="96389a0f02a3826ae6017961d0301435c315a116" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">중첩 된 CV가없는 모델 선택은 동일한 데이터를 사용하여 모델 매개 변수를 조정하고 모델 성능을 평가합니다. 따라서 정보는 모델에 &quot;누설&quot;되어 데이터에 과적 합 될 수 있습니다. 이 효과의 크기는 주로 데이터 세트의 크기와 모델의 안정성에 따라 달라집니다. 이러한 문제에 대한 분석은 Cawley와 Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">중첩 된 CV가없는 모델 선택은 동일한 데이터를 사용하여 모델 매개 변수를 조정하고 모델 성능을 평가합니다. 따라서 정보가 모델에 &quot;누설&quot;되어 데이터에 과적 합할 수 있습니다. 이 효과의 크기는 주로 데이터 세트의 크기와 모델의 안정성에 따라 다릅니다. 이러한 문제에 대한 분석은 Cawley 및 Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">모델 선택 : 추정기 및 매개 변수 선택</target>
        </trans-unit>
        <trans-unit id="ed1ba8eabae7e8d7de3d25705f2020f1428a1a11" translate="yes" xml:space="preserve">
          <source>Model the number of claims with a Poisson distribution, and the average claim amount per claim, also known as severity, as a Gamma distribution and multiply the predictions of both in order to get the total claim amount.</source>
          <target state="translated">Poisson 분포로 클레임 수와 심각도라고도하는 클레임 ​​당 평균 클레임 금액을 감마 분포로 모델링하고 총 클레임 금액을 얻기 위해 두 예측을 곱합니다.</target>
        </trans-unit>
        <trans-unit id="7c9e82e3e8aa374bb01cb1f0583b83a3f30a27a0" translate="yes" xml:space="preserve">
          <source>Model the total claim amount per exposure directly, typically with a Tweedie distribution of Tweedie power \(p \in (1, 2)\).</source>
          <target state="translated">일반적으로 Tweedie power \ (p \ in (1, 2) \)의 Tweedie 분포를 사용하여 노출 당 총 청구 금액을 직접 모델링합니다.</target>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">모델 검증</target>
        </trans-unit>
        <trans-unit id="44e8839819f969bae0c352bffb18b8d9aae37a64" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">종의 지리적 분포를 모델링하는 것은 보전 생물학에서 중요한 문제입니다. 이 예에서 우리는 과거 관찰과 14 개의 환경 변수가 주어진 두 남미 포유류의 지리적 분포를 모델링합니다. 긍정적 인 예 (실패한 관찰이 없음) 만 &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 문제를 밀도 추정 문제로 던지고 sklearn.svm.OneClassSVM 을 모델링 도구로 사용합니다 . 데이터 세트는 Phillips et. al. (2006). 가능한 경우 예제에서는 &lt;a href=&quot;https://matplotlib.org/basemap/&quot;&gt;베이스 맵&lt;/a&gt; 을 사용 하여 남미의 해안선과 국경을 표시합니다.</target>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">종의 지리적 분포를 모델링하는 것은 보존 생물학에서 중요한 문제입니다. 이 예에서 우리는 과거의 관측치와 14 개의 환경 변수가 주어진 2 개의 남미 포유류의 지리적 분포를 모델링합니다. 우리는 긍정적 인 예만 가지고 있기 때문에 (실패한 관찰은 없다),이 문제를 밀도 추정 문제로 캐스트하고 &lt;code&gt;OneClassSVM&lt;/code&gt; 패키지에서 제공하는 &lt;code&gt;sklearn.svm&lt;/code&gt; 을 모델링 도구로 사용합니다. 데이터 세트는 Phillips et. 알. (2006). 가능한 &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;경우이&lt;/a&gt; 예에서는 베이스 맵 을 사용 하여 남미의 해안선과 국가 경계를 그립니다.</target>
        </trans-unit>
        <trans-unit id="3432d8d9b44052d02847746ae08d1ac381072a89" translate="yes" xml:space="preserve">
          <source>Modified Huber: \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) &amp;gt; 1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.</source>
          <target state="translated">수정 된 Huber : \ (L (y_i, f (x_i)) = \ max (0, 1-y_i f (x_i)) ^ 2 \) if \ (y_i f (x_i)&amp;gt; 1 \) 및 \ (L ( y_i, f (x_i)) = -4 y_i f (x_i) \) 그렇지 않으면.</target>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt; 모듈 은 커널 릿지 회귀를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">그라데이션 하강 업데이트를위한 추진력. 0과 1 사이 여야합니다. solver = 'sgd'인 경우에만 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="08837633f9d15f78a0ca0401b5e29aae44a3cb25" translate="yes" xml:space="preserve">
          <source>Monotonic Constraints</source>
          <target state="translated">단조 적 제약</target>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann, F. and Triggs, B. 및 Jurie, F.&amp;ldquo;무작위 클러스터링 포리스트를 사용하는 빠른 차별적 시각적 코드북&amp;rdquo;NIPS 2007</target>
        </trans-unit>
        <trans-unit id="3f683b2b5fe59dc7e9963e0b844a2be959abe1bd" translate="yes" xml:space="preserve">
          <source>More details about the losses formulas can be found in the &lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">손실 공식에 대한 자세한 내용은 &lt;a href=&quot;../sgd#sgd-mathematical-formulation&quot;&gt;사용자 안내서&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">자세한 내용은 MacKay, David JC의 &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;베이지안 보간&lt;/a&gt; 기사를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3dd8319d03052df7074a6e0643293f5dc781d510" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;http://scikit-learn.org/stable/modules/sgd.html&quot;&gt;SGD&lt;/a&gt; 문서에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">자세한 내용은 &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt; 설명서를 참조 하십시오.</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">모델 선택에 사용할 수있는 도구에 대한 자세한 내용은 &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증 : 추정기 성능 평가&lt;/a&gt; 및 추정기 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;의 하이퍼 파라미터 조정&lt;/a&gt; 섹션에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">보다 공식적으로, 샘플 \ (k \)가 샘플 \ (i \)의 모범이되는 책임은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">보다 공식적으로, 코어 샘플을 데이터 세트의 샘플 인 것으로 정의하여 &lt;code&gt;eps&lt;/code&gt; 거리 내에 &lt;code&gt;min_samples&lt;/code&gt; 다른 샘플 이 존재 하도록 하는데 , 코어 샘플의 &lt;em&gt;이웃&lt;/em&gt; 으로 정의됩니다 . 이것은 핵심 샘플이 벡터 공간의 밀집된 영역에 있음을 알려줍니다. 클러스터는 재귀 코어 샘플을 복용 핵심 샘플입니다 이웃 모두를 찾는 모든 찾는하여 구축 할 수 코어 샘플의 집합입니다 &lt;em&gt;그들의&lt;/em&gt; 등 핵심 샘플있는 이웃을합니다. 또한 클러스터에는 비 핵심 샘플 세트가 있습니다.이 샘플은 클러스터의 코어 샘플에 인접하지만 자체 샘플은 아닙니다. 직관적으로 이러한 샘플은 클러스터의 가장자리에 있습니다.&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">보다 일반적으로 분류기의 정확도가 임의에 너무 가까운 경우에는 문제가 발생했음을 의미 할 수 있습니다. 기능이 도움이되지 않거나 하이퍼 파라미터가 올바르게 조정되지 않았으며 분류 기가 클래스 불균형으로 인해 어려움을 겪고 있습니다.</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">자세한 정보는 &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy 설치 페이지&lt;/a&gt; 와 데비안 / 우분투에 대한 단계별 설치 지침이있는 Daniel Nouri 의이 &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;블로그 게시물&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c3cc4720813506dbf91cef9b68d3a09728559160" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">자세한 정보는 &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy 설치 페이지&lt;/a&gt; 와 Daniel Nouri 의이 &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;블로그 게시물&lt;/a&gt; 에서 찾을 수 있습니다 .이 블로그 게시물 에는 Debian / Ubuntu에 대한 단계별 설치 지침이 있습니다.</target>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">OpenML의 추가 메타 데이터</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">보다 정확하게는 초기 모델을 설명한 후 목표 대응에 대한 기대; 부분 의존도에는 &lt;code&gt;init&lt;/code&gt; 모델이 포함되지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">더 읽기 쉬운 코드, 특히 인수 목록 구성을 피하기 때문에.</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">보다 구체적으로, 선형 및 2 차 판별 분석의 경우, \ ​​(P (X | y) \)는 밀도를 갖는 다변량 가우스 분포로 모델링됩니다.</target>
        </trans-unit>
        <trans-unit id="2b3cb69cb34819cd8834522c11758ddc50e1f474" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(x|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">보다 구체적으로, 선형 및 2 차 판별 분석의 경우 \ (P (x | y) \)는 밀도가있는 다변량 가우스 분포로 모델링됩니다.</target>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">가장 일반적으로 시차는 \ (\ hat {d} _ {ij} = b S_ {ij} \)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="242544fc56d0b5c7cafd51d576a4b175219e1770" translate="yes" xml:space="preserve">
          <source>Most estimators based on nearest neighbors graphs now accept precomputed sparse graphs as input, to reuse the same graph for multiple estimator fits. To use this feature in a pipeline, one can use the &lt;code&gt;memory&lt;/code&gt; parameter, along with one of the two new transformers, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The precomputation can also be performed by custom estimators to use alternative implementations, such as approximate nearest neighbors methods. See more details in the &lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">최근 접 이웃 그래프를 기반으로하는 대부분의 추정기는 이제 미리 계산 된 희소 그래프를 입력으로 받아 여러 추정기 적합에 대해 동일한 그래프를 재사용합니다. 파이프 라인에서이 기능을 사용하려면 두 개의 새로운 변환기 중 하나 인 &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.KNeighborsTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt; &lt;code&gt;neighbors.RadiusNeighborsTransformer&lt;/code&gt; &lt;/a&gt; 와 함께 &lt;code&gt;memory&lt;/code&gt; 매개 변수를 사용할 수 있습니다 . 사용자 지정 추정기에 의해 사전 계산을 수행하여 근사 근사치 방법과 같은 대체 구현을 사용할 수도 있습니다. 자세한 내용은 &lt;a href=&quot;../../modules/neighbors#neighbors-transformer&quot;&gt;사용자 안내서를 참조하십시오&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="44814fa11b1099a09b484290756712e9a8679a34" translate="yes" xml:space="preserve">
          <source>Most of the parameters are unchanged from &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;. One exception is the &lt;code&gt;max_iter&lt;/code&gt; parameter that replaces &lt;code&gt;n_estimators&lt;/code&gt;, and controls the number of iterations of the boosting process:</source>
          <target state="translated">대부분의 매개 변수는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 에서 변경되지 않습니다 . 한 가지 예외는있다 &lt;code&gt;max_iter&lt;/code&gt; 의 대체 매개 변수 &lt;code&gt;n_estimators&lt;/code&gt; 및 제어 강화 과정의 반복 횟수를 :</target>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">분산의 대부분은 종 유효 곡선의 종 모양 곡선으로 설명 할 수 있습니다. 특이 값 프로파일의 하위 순위 부분은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">대부분의 scikit-learn 모델은 컴파일 된 Cython 확장 또는 최적화 된 컴퓨팅 라이브러리로 구현되므로 일반적으로 매우 빠릅니다. 반면, 많은 실제 응용 프로그램에서 기능 추출 프로세스 (예 : 데이터베이스 행 또는 네트워크 패킷과 같은 원시 데이터를 numpy 배열로 변환)는 전체 예측 시간을 제어합니다. 예를 들어 Reuters 텍스트 분류 작업에서 선택한 모델에 따라 전체 준비 (SGML 파일 읽기 및 구문 분석, 텍스트 토큰 화 및 공통 벡터 공간으로 해싱)는 실제 예측 코드보다 100 ~ 500 배 더 많은 시간이 걸립니다.</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">자연어 처리 (NLP) 및 정보 검색 (IR) 문헌에서 대부분의 LSA 처리는 &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; 모양을 갖도록 행렬의 축을 교환합니다 . 우리는 scikit-learn API와 더 잘 일치하는 다른 방식으로 LSA를 제시하지만 발견 된 특이 값은 동일합니다.</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">다중 대상 분류</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">다중 대상 회귀</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">멀티 클래스 Ada 부스트 결정 트리</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">멀티 클래스 대상.</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">멀티 클래스 대상. 표시기 매트릭스는 다중 레이블 분류를 켭니다.</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">다차원 스케일링</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">다층 퍼셉트론 분류기.</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">다층 퍼셉트론은 기능 스케일링에 민감하므로 데이터를 스케일링하는 것이 좋습니다. 예를 들어 입력 벡터 X의 각 속성을 [0, 1] 또는 [-1, +1]로 스케일링 하거나 평균 0과 분산 1을 갖도록 표준화하십시오 . 테스트 세트에 &lt;em&gt;동일한&lt;/em&gt; 스케일링을 적용해야합니다. 의미있는 결과. &lt;code&gt;StandardScaler&lt;/code&gt; 를 사용하여 표준화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">다층 퍼셉트론 회귀 기.</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">다중 출력 결정 트리 회귀</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">여러 예측 변수에서 예측 된 다중 출력 대상 참고 : 예측 변수마다 별도의 모델이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">다중 출력 대상.</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">다중 출력 대상. 표시기 행렬은 다중 레이블 추정을 켭니다.</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1 / L2 혼합 규범을 정규화기로 학습 한 멀티 태스크 ElasticNet 모델</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 ElasticNet</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 올가미</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">교차 검증이 내장 된 멀티 태스킹 L1 / L2 올가미.</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">L1 / L2 혼합 규범을 정규화기로 훈련 한 다중 작업 올가미 모델</target>
        </trans-unit>
        <trans-unit id="162889b9c309e59105e244387d111bbb75ed4cc7" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</source>
          <target state="translated">정규화기로 L1 / L2 혼합 노름으로 훈련 된 멀티 태스킹 Lasso 모델.</target>
        </trans-unit>
        <trans-unit id="0119eef45392f9d57273a8cd6ca2fcc5f0003969" translate="yes" xml:space="preserve">
          <source>Multi-task linear regressors with variable selection</source>
          <target state="translated">변수 선택이있는 다중 작업 선형 회귀 분석기</target>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">멀티 클래스 및 멀티 라벨 분류 전략</target>
        </trans-unit>
        <trans-unit id="8ddfaa46f2a114c89c7cad99fd0ddf4dc7314399" translate="yes" xml:space="preserve">
          <source>Multiclass case:</source>
          <target state="translated">다중 클래스 케이스 :</target>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">멀티 클래스 분류</target>
        </trans-unit>
        <trans-unit id="868117baea7dbed0e92972aac1d890c07c8ae48f" translate="yes" xml:space="preserve">
          <source>Multiclass data will be treated as if binarized under a one-vs-rest transformation. Returned confusion matrices will be in the order of sorted unique labels in the union of (y_true, y_pred).</source>
          <target state="translated">다중 클래스 데이터는 일대 나머지 변환에서 이진화 된 것처럼 처리됩니다. 반환 된 혼동 행렬은 (y_true, y_pred)의 합집합에서 정렬 된 고유 레이블의 순서를 따릅니다.</target>
        </trans-unit>
        <trans-unit id="2d8780a18f5ba3e6cb5bb5a579ea67a8f2550bb3" translate="yes" xml:space="preserve">
          <source>Multiclass only. Determines the type of configuration to use. The default value raises an error, so either &lt;code&gt;'ovr'&lt;/code&gt; or &lt;code&gt;'ovo'&lt;/code&gt; must be passed explicitly.</source>
          <target state="translated">다중 클래스 전용입니다. 사용할 구성 유형을 결정합니다. 기본값은 오류를 발생 시키므로 &lt;code&gt;'ovr'&lt;/code&gt; 또는 &lt;code&gt;'ovo'&lt;/code&gt; 를 명시 적으로 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="f6acefc7f4185f0f4df5b1178aaed55afe1147ed" translate="yes" xml:space="preserve">
          <source>Multiclass only. List of labels that index the classes in &lt;code&gt;y_score&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, the numerical or lexicographical order of the labels in &lt;code&gt;y_true&lt;/code&gt; is used.</source>
          <target state="translated">다중 클래스 전용입니다. &lt;code&gt;y_score&lt;/code&gt; 의 클래스를 인덱싱하는 레이블 목록입니다 . 경우 &lt;code&gt;None&lt;/code&gt; 에있는 라벨의 숫자 또는 사전 순 &lt;code&gt;y_true&lt;/code&gt; 이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">멀티 클래스 확률 추정치는 Zadrozny와 Elkan이 권장하는 간단한 정규화에 의한 이진 (1 대 나머지) 추정치에서 파생됩니다.</target>
        </trans-unit>
        <trans-unit id="39d0ca41499d6b7f83a17678aedf5fae33705c06" translate="yes" xml:space="preserve">
          <source>Multiclass problems are binarized and treated like the corresponding multilabel problem:</source>
          <target state="translated">다중 클래스 문제는 이진화되고 해당 다중 레이블 문제처럼 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">멀티 클래스 설정</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">새로운 그룹에 대한 다중 클래스 희소 로지스틱 회귀</target>
        </trans-unit>
        <trans-unit id="abed0a03e9d2180975b0a68aad7dbbccddc0d2d0" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logistic regression on 20newgroups</source>
          <target state="translated">20 개의 새 그룹에 대한 다중 클래스 희소 로지스틱 회귀</target>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">멀티 클래스 스펙트럼 클러스터링, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4386a05880a13b860ce6d4571568b773373f22e3" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">다중 클래스 스펙트럼 클러스터링, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">멀티 클래스 vs. 멀티 라벨 피팅</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">다차원 스케일링</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">멀티 라벨 분류</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">다중 라벨 순위 측정 항목</target>
        </trans-unit>
        <trans-unit id="ce79d912af81c5a374445c22e6e3cfe9ecb9a93c" translate="yes" xml:space="preserve">
          <source>Multilabel-indicator case:</source>
          <target state="translated">다중 라벨 표시기 케이스 :</target>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">멀티 메트릭 스코어링은 사전 정의 된 스코어 이름의 문자열 목록으로 지정되거나 스코어러 이름을 스코어러 기능 및 / 또는 사전 정의 된 스코어러 이름에 맵핑하는 dict입니다. 자세한 내용은 &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;다중 메트릭 평가 사용&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">다항식 + L1 페널티</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">다항식 + L2 페널티</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">다항 이탈 ( &lt;code&gt;'deviance'&lt;/code&gt; ) : &lt;code&gt;n_classes&lt;/code&gt; 가 서로 배타적 인 클래스를 갖는 다중 클래스 분류를위한 음의 다항 로그 우도 손실 함수 . 확률 추정치를 제공합니다. 초기 모델은 각 클래스의 사전 확률로 제공됩니다. 각 반복에서 &lt;code&gt;n_classes&lt;/code&gt; 회귀 트리를 구성해야하므로 클래스 수가 많은 데이터 세트에 GBRT를 비효율적으로 만들 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">다중 출력 분류 지원은 어떤 분류에 추가 할 수 있습니다 &lt;code&gt;MultiOutputClassifier&lt;/code&gt; . 이 전략은 대상 당 하나의 분류기를 맞추는 것으로 구성됩니다. 이를 통해 여러 대상 변수 분류가 가능합니다. 이 클래스의 목적은 추정값을 확장하여 일련의 응답 (y1, y2, y3)을 예측하기 위해 단일 X 예측 행렬에서 학습 된 일련의 목표 함수 (f1, f2, f3&amp;hellip;, fn)를 추정 할 수 있도록하는 것입니다. &amp;hellip;, yn).</target>
        </trans-unit>
        <trans-unit id="8ec2d1e390ee85463a8b9edc1df8f6a33597454a" translate="yes" xml:space="preserve">
          <source>Multioutput methods</source>
          <target state="translated">다중 출력 방법</target>
        </trans-unit>
        <trans-unit id="8e7bfb83db794fa15648cd7ab3b23c509cc8018c" translate="yes" xml:space="preserve">
          <source>Multioutput regression</source>
          <target state="translated">다중 출력 회귀</target>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">다중 출력 회귀 지원은 어떤 회귀에 추가 할 수 있습니다 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; . 이 전략은 대상 당 하나의 회귀자를 피팅하는 것으로 구성됩니다. 각 대상은 정확히 하나의 회귀 자로 표시되므로 해당 회귀를 검사하여 대상에 대한 지식을 얻을 수 있습니다. 으로 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 가 대상 당 하나의 회귀를 맞는이 목표 사이의 상관 관계를 이용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="96d87119823da5637cea208be6276fad5c922737" translate="yes" xml:space="preserve">
          <source>Multioutput- multiclass classification</source>
          <target state="translated">다중 출력-다중 클래스 분류</target>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 를 사용한 다중 메트릭 평가 (자세한 내용은 &lt;code&gt;scoring&lt;/code&gt; 매개 변수 문서를 참조하십시오)</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">&lt;code&gt;scoring&lt;/code&gt; 매개 변수를 메트릭 스코어러 이름 목록 으로 설정 하거나 스코어러 이름을 스코어러 호출 가능 항목에 맵핑하는 dict를 사용하여 여러 메트릭 매개 변수 검색을 수행 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="24f6a3478d65f4dead755fd18d3792f81fa6260d" translate="yes" xml:space="preserve">
          <source>Multiple stacking layers can be achieved by assigning &lt;code&gt;final_estimator&lt;/code&gt; to a &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt;&lt;code&gt;StackingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.ensemble.stackingregressor#sklearn.ensemble.StackingRegressor&quot;&gt; &lt;code&gt;StackingRegressor&lt;/code&gt; 에&lt;/a&gt; &lt;code&gt;final_estimator&lt;/code&gt; 를 할당 하면 다중 스태킹 레이어를 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">변압기 당 기능에 대한 곱하기 가중치. 키는 변압기 이름이며 무게 값입니다.</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">변압기 당 기능에 대한 곱하기 가중치. 변압기의 출력에는 이러한 가중치가 곱해집니다. 키는 변압기 이름이며 무게 값입니다.</target>
        </trans-unit>
        <trans-unit id="afa1ae58a55a69631c4c27e76dfc260c619f73c7" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C for each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">각 클래스에 대한 매개 변수 C의 승수. &lt;code&gt;class_weight&lt;/code&gt; 매개 변수를 기반으로 계산 됩니다.</target>
        </trans-unit>
        <trans-unit id="b0e900a51b93880c89d198a9d719bd1f20cdb329" translate="yes" xml:space="preserve">
          <source>Multipliers of parameter C of each class. Computed based on the &lt;code&gt;class_weight&lt;/code&gt; parameter.</source>
          <target state="translated">각 클래스의 매개 변수 C의 승수. &lt;code&gt;class_weight&lt;/code&gt; 매개 변수를 기반으로 계산 됩니다.</target>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">피처에 지정된 값을 곱합니다. 없음 인 경우 [1, 100]에 그려진 임의의 값으로 피처를 스케일링합니다. 이동 후 스케일링이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d54881ba1eca5e77240b1b917c4b4af86c4398ba" translate="yes" xml:space="preserve">
          <source>Multiplying the coefficients by the standard deviation of the related feature would reduce all the coefficients to the same unit of measure. As we will see &lt;a href=&quot;#scaling-num&quot;&gt;after&lt;/a&gt; this is equivalent to normalize numerical variables to their standard deviation, as \(y = \sum{coef_i \times X_i} = \sum{(coef_i \times std_i) \times (X_i / std_i)}\).</source>
          <target state="translated">계수에 관련 기능의 표준 편차를 곱하면 모든 계수가 동일한 측정 단위로 줄어 듭니다. 우리시피 &lt;a href=&quot;#scaling-num&quot;&gt;후&lt;/a&gt; \ 이것으로 그 표준 편차 수치 변수를 정상화 동등 (Y = \ 합 {coef_i \ 시간 x_i로부터} = \ 합 {(coef_i \ 시간 std_i) \ 시간 (x_i로부터 / std_i)} \) .</target>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">멀티 스레드 BLAS 라이브러리는 때때로 Python의 &lt;code&gt;multiprocessing&lt;/code&gt; 모듈 과 충돌 합니다. 예를 들어 &lt;code&gt;GridSearchCV&lt;/code&gt; 및 &lt;code&gt;n_jobs&lt;/code&gt; 인수 를 사용하는 대부분의 다른 추정기 ( &lt;code&gt;SGDClassifier&lt;/code&gt; , &lt;code&gt;SGDRegressor&lt;/code&gt; , &lt;code&gt;Perceptron&lt;/code&gt; , &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; 및 임의 포리스트와 같은 트리 기반 메소드 제외)가 사용합니다. 이것은 OpenMP 지원으로 빌드 된 Apple의 Accelerate 및 OpenBLAS에 해당됩니다.</target>
        </trans-unit>
        <trans-unit id="3483a919f49e511ca829411c285a74281e005ef3" translate="yes" xml:space="preserve">
          <source>Multivariate imputation of missing values.</source>
          <target state="translated">결 측값의 다변량 대치.</target>
        </trans-unit>
        <trans-unit id="3e386b49678343bab915e96a29e16b2a1aa09993" translate="yes" xml:space="preserve">
          <source>Multivariate imputer that estimates each feature from all the others.</source>
          <target state="translated">다른 모든 특성에서 각 특성을 추정하는 다변량 대치 자입니다.</target>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">partial_fit을 처음 호출 할 때 제공해야하며 후속 호출에서는 생략 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">기본 추정기의 입력 가정을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">상호 정보 (기회에 맞게 조정되지 않음)</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">두 군집 간의 상호 정보.</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">두 랜덤 변수 사이의 상호 정보 (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 은 음이 아닌 값으로, 변수 간의 종속성을 측정합니다. 두 개의 임의 변수가 독립적이고 값이 클수록 종속성이 높은 경우에만 0입니다.</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">두 랜덤 변수 사이의 상호 정보 (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 은 음이 아닌 값으로, 변수 간의 종속성을 측정합니다. 두 개의 임의 변수가 독립적이고 값이 클수록 종속성이 높은 경우에만 0입니다.</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">피처와 대상 간의 상호 정보.</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">연속 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">연속적인 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">개별 대상에 대한 상호 정보.</target>
        </trans-unit>
        <trans-unit id="2555b04ef28112b324874c1cc2f3bf2b5b5c384e" translate="yes" xml:space="preserve">
          <source>Mutual information, a non-negative value</source>
          <target state="translated">음수가 아닌 상호 정보</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="4e1221dedd7ee34eb6931a44dc15d9a84ca69a81" translate="yes" xml:space="preserve">
          <source>N : number of dimensions</source>
          <target state="translated">N : 치수 수</target>
        </trans-unit>
        <trans-unit id="8daf5ce04352d841160e980446540ca50cce58e4" translate="yes" xml:space="preserve">
          <source>N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.</source>
          <target state="translated">구조에 N 그램! 유니 그램 (n = 1)의 간단한 모음을 만드는 대신 연속 단어 쌍의 발생 횟수를 계산하는 bigram (n = 2) 모음을 선호 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0d5c48bb908535393359e08969f6193dc43fff44" translate="yes" xml:space="preserve">
          <source>NCA can be seen as learning a (squared) Mahalanobis distance metric:</source>
          <target state="translated">NCA는 (제곱) Mahalanobis 거리 측정법을 학습하는 것으로 볼 수 있습니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
