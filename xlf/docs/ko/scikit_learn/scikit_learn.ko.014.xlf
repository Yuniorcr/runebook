<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="d1e3e38243b5c0275e9fe55822527a766daf84e5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="cd66ce686ba75391d858a270576652221e9366cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_validate&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.cross_validate&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="64bdbb73303147fbfb7b4c72e2c5b9f862e6846a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="058189237019f568402a807ff0f37d6659f730bd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="a35d588868114f2ff75fd130567f7e47c8648990" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="c436bb41de4b1b0cbaf9f2a30c84739212eaf00c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="8c90ea159f2ef33465cc5505a41ebcb13d8fa556" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="3b803ea3410b546ac8080c64d5a283e65161ec36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt; 을 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="175a335aedce67fab5414504b5b10913957177f2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="7e5276c814e14e2874f1abc67268ecba0a9384b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="07d68dd4fe4a915ea6b5e32e2eca6d299d857bb7" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="af3404bd8b46bee3672ba86cfbae1ef5ccc5adfa" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="141d21773d99f3975d70ea9e9ce62f28b143b5ad" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="5d999f3df66c782268c5cd0602cb79c507903292" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="f26682456d970fbdab48470c970d2b2cf9d0be01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="da4ee64191a33554442daf81f4b75c328c16de8c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsTransformer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.KNeighborsTransformer&lt;/code&gt; 를 사용한 예제</target>
        </trans-unit>
        <trans-unit id="2b0ca2d2cfcae04b6e8f648035a263800eba906a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="135f507596f50f4128ab7f674fa719851e06ba93" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="2d07ff0ebbbb5614d7d55c03e06e43f1f3f6d8b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="0749621ad934aca64b6d4bb597f488ca85f63f5a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NeighborhoodComponentsAnalysis&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.NeighborhoodComponentsAnalysis&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="9e409523b85d3bf8b35de4b9a79e58e19412df01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="92b259d24e226f9725b7a2251b317d28f69ae1de" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="bc26545aa247592ad5cae0c82982049aecb04005" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="439d6a7e65183ef0435c18b6b323c74a5c51b635" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPRegressor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.neural_network.MLPRegressor&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="aa6ee186b720b58c48c82ddfe44e7546cce24778" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="998ec3583b26e3fb4a4ed0aff8fa32c106d8d3fd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="ce16abd7de545850afcb200375d7496f51a99f86" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="22b8f1c3009cd0b959254500e33f171fb13ddc5e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="2542a7119b1badd4a5e17e03f5405355cc7a6b4e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="c5ebd1b718c2b56f55f671f8d4b702c99b6f3049" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="c4381d9fd377759510c6b6b975b0aef8ecf5740c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="6c598f0cbf4e71a390fc3aa76e817310e670b681" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="c1bc49d9937f0e2a5f4b0d4692671125b0d008b2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="149136e5bc700b2e0a7747b2cd498f47536f44b4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="11c249d2a654ff2f1c2cc110185680b161c2af36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt; 를 사용한 예제</target>
        </trans-unit>
        <trans-unit id="56c4939b9d0afea5d1cebbe13cfe733258c34c00" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="3a155bf7a30ca61ce1f29447001d55507068d346" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="deb84a64d1425831a8777aa592bd6255271f75f8" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="6a98bd8bd28f139beba8d2222e8707ab286c5640" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="cc6025242e9ac27ec2143cf98f98385064f91436" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="be97d0bdf7a32b2c8b6fa3970798bb89bf783b34" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="36fa08e3b378f54ed6dffaaf0e419793c6ba101a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="69e893f40e9fcd420f194c86a5ec119b1f003520" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="f1b108e5d448ff373a393340cccb912e8ffb72af" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="955e7e901699a2db4e4172aa9ab76068eea1b92f" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="4c4748cf4849c5fc863bc575791af8141a34cef5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="88c1a4bb06d05e08ee20fd615bb4b53da175dc3d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="737ca8cb1ccf3eeb065178263662a6be9c3d9649" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.set_config&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.set_config&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="7cd962b7c530c80e05b210277444c2bad820ca1e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="edbf5922b6f27be5c5403e10197b447705e6f9cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="12c9b218e18b941578bc9aca3c23747e42f54c1b" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="dea2aede79f19fb95e5148ee5c11b3c7f223323a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="6bc3b4bf10642b79be6b77851a23e142b0ca36cf" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.SVC&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="c4dcb2fd7042cd90b5cd989b10ea50e9efa48d50" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVR&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.SVR&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="2a2669f574a679afac893b53b03bdb1bd3060d05" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="15e7f4eb0d23fc3ecea22ca0e3a3ebdefcef3322" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="65c5a0a1817f47dc4a24c72d932486e31f108484" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt; 를 사용하는 예제</target>
        </trans-unit>
        <trans-unit id="607ad6107d22cda5b789ada19b3e439fe3038b27" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.plot_tree&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.tree.plot_tree&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="5b4d8799f6ce5100bf207b65e66619307f668cac" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; 를 사용한 예제</target>
        </trans-unit>
        <trans-unit id="668a018e39621dfc09781d5e1a018ed078089b72" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Memory&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.Memory&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="7213df901109f2c1652eb65fa0f35fe0ff18cd9d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="e9dab354869145a323333bb1a6f4abfc5f1cb7b1" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.estimator_checks.parametrize_with_checks&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.estimator_checks.parametrize_with_checks&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="2865372d64705a746f4e393e76c01c0a1fae3b37" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="da99fb3fb0ad21b853976b89d77d08cdf2ec4f84" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.gen_even_slices&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.gen_even_slices&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="851edad00347cd3937bd7ea24424c348d471aca4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.metaestimators.if_delegate_has_method&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.metaestimators.if_delegate_has_method&lt;/code&gt; 를 사용하는 예</target>
        </trans-unit>
        <trans-unit id="08628ee68b552a536a7e40670b9091a9987d81dc" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.shuffle&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;sklearn.utils.shuffle&lt;/code&gt; 을 사용하는 예</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="2c25bc9aea0135b8a44078eaa686262d861b8d2c" translate="yes" xml:space="preserve">
          <source>Exception class to raise if estimator is used before fitting.</source>
          <target state="translated">피팅 전에 추정기가 사용되는 경우 발생하는 예외 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="7233f2cfa7da608e08349effe3a0829359d064c0" translate="yes" xml:space="preserve">
          <source>Exception.with_traceback(tb) &amp;ndash; set self.__traceback__ to tb and return self.</source>
          <target state="translated">Exception.with_traceback (tb) &amp;ndash; self .__ traceback__을 tb로 설정하고 self를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="adb63819e55094a1c321051667ef69bc97910b3c" translate="yes" xml:space="preserve">
          <source>Exercise 1: Language identification</source>
          <target state="translated">연습 1 : 언어 식별</target>
        </trans-unit>
        <trans-unit id="f2d31e2f590f63884cdac3b6e0353c56e95636fd" translate="yes" xml:space="preserve">
          <source>Exercise 2: Sentiment Analysis on movie reviews</source>
          <target state="translated">실습 2 : 영화 리뷰에 대한 감정 분석</target>
        </trans-unit>
        <trans-unit id="f076754d245dfa0bb055ce293bbe18bfa8d62689" translate="yes" xml:space="preserve">
          <source>Exercise 3: CLI text classification utility</source>
          <target state="translated">연습 3 : CLI 텍스트 분류 유틸리티</target>
        </trans-unit>
        <trans-unit id="4dc503dafcf231e8065504c4cd9f19a0dcdfc147" translate="yes" xml:space="preserve">
          <source>Exercises</source>
          <target state="translated">Exercises</target>
        </trans-unit>
        <trans-unit id="9b4e2cce8211934c05426343255dbbe40e6fc29d" translate="yes" xml:space="preserve">
          <source>Exercises for the tutorials</source>
          <target state="translated">튜토리얼을위한 연습</target>
        </trans-unit>
        <trans-unit id="e172501d8e170f5e501dbb7e10f621b359347edb" translate="yes" xml:space="preserve">
          <source>Exhaustive search over specified parameter values for an estimator.</source>
          <target state="translated">추정기의 지정된 매개 변수 값을 철저히 검색합니다.</target>
        </trans-unit>
        <trans-unit id="827e74ef83aecac9c6f9fc861a3a010bc5589266" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel (aka periodic kernel).</source>
          <target state="translated">Exp-Sine-Squared 커널 (일명 주기적 커널).</target>
        </trans-unit>
        <trans-unit id="a9607bbaaa4524856cf2140445f92e407ff55546" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel.</source>
          <target state="translated">Exp-Sine-Squared 커널.</target>
        </trans-unit>
        <trans-unit id="3b0c107b231b7c7d2dd1b666566c6604385763d3" translate="yes" xml:space="preserve">
          <source>Expected results for the top 5 most represented people in the dataset:</source>
          <target state="translated">데이터 세트에서 가장 많이 대표되는 상위 5 명에 대한 예상 결과 :</target>
        </trans-unit>
        <trans-unit id="98312dc3857136b93e4f949a4e72a6712170156c" translate="yes" xml:space="preserve">
          <source>Explained variance regression score function</source>
          <target state="translated">설명 된 분산 회귀 점수 함수</target>
        </trans-unit>
        <trans-unit id="31162dbfd1baf8644ae388945633979e4d4b742f" translate="yes" xml:space="preserve">
          <source>Explicit feature map approximation for RBF kernels</source>
          <target state="translated">RBF 커널에 대한 명시 적 기능 맵 근사치</target>
        </trans-unit>
        <trans-unit id="2139158fefd69dd7900f572f929357923b2e9c08" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">adam에서 첫 번째 모멘트 벡터의 추정치에 대한 지수 붕괴율은 [0, 1)에 있어야합니다. solver = 'adam'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="3b517f2d5130d355e5abbd42b6b2d0de0b8022fe" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">adam에서 두 번째 모멘트 벡터 추정치에 대한 지수 붕괴율은 [0, 1)에 있어야합니다. solver = 'adam'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="5be074c21803e5b45a92c260ef448b3876757aad" translate="yes" xml:space="preserve">
          <source>Exponential kernel (&lt;code&gt;kernel = 'exponential'&lt;/code&gt;)</source>
          <target state="translated">지수 커널 ( &lt;code&gt;kernel = 'exponential'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="4e6d09cb3f7c099f340e3cd011063b05dd20e736" translate="yes" xml:space="preserve">
          <source>Exponential loss (&lt;code&gt;'exponential'&lt;/code&gt;): The same loss function as &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt;. Less robust to mislabeled examples than &lt;code&gt;'deviance'&lt;/code&gt;; can only be used for binary classification.</source>
          <target state="translated">지수 손실 ( &lt;code&gt;'exponential'&lt;/code&gt; ) : &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt; 와 동일한 손실 함수 입니다. &lt;code&gt;'deviance'&lt;/code&gt; 보다 레이블이 잘못 지정된 예에 덜 견고합니다 . 이진 분류에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="82a5349d4a42ae4b58ce233c1fe754cf20695efd" translate="yes" xml:space="preserve">
          <source>Exponentiate kernel by given exponent.</source>
          <target state="translated">주어진 지수로 커널을 지수화하십시오.</target>
        </trans-unit>
        <trans-unit id="6a1fb392b4816003f7cab8689b69b73b81fd13d1" translate="yes" xml:space="preserve">
          <source>Export a decision tree in DOT format.</source>
          <target state="translated">의사 결정 트리를 DOT 형식으로 내 보냅니다.</target>
        </trans-unit>
        <trans-unit id="862ee3b17a826818107e616215cf3c3a954115aa" translate="yes" xml:space="preserve">
          <source>Exposure</source>
          <target state="translated">Exposure</target>
        </trans-unit>
        <trans-unit id="9eb0650b6756b55d46beda1492ea11acfe7e3431" translate="yes" xml:space="preserve">
          <source>Expresses to what extent the local structure is retained.</source>
          <target state="translated">로컬 구조가 유지되는 정도를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="03b256629a71913d7df02c71e2c067b3de2f6cf2" translate="yes" xml:space="preserve">
          <source>External Resources, Videos and Talks</source>
          <target state="translated">외부 자료, 비디오 및 대화</target>
        </trans-unit>
        <trans-unit id="6a53253fde7542b58764813563b294277a2499e0" translate="yes" xml:space="preserve">
          <source>External Tutorials</source>
          <target state="translated">외부 튜토리얼</target>
        </trans-unit>
        <trans-unit id="d29859b915eecd05832dcd65405884b6cc0d4c40" translate="yes" xml:space="preserve">
          <source>Extra keyword arguments will be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="translated">추가 키워드 인수는 matplotlib의 &lt;code&gt;plot&lt;/code&gt; 에 전달됩니다 .</target>
        </trans-unit>
        <trans-unit id="8d6114c0a06ffd4fdec489d603f3c2293f085d8e" translate="yes" xml:space="preserve">
          <source>Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the &lt;code&gt;max_features&lt;/code&gt; randomly selected features and the best split among those is chosen. When &lt;code&gt;max_features&lt;/code&gt; is set 1, this amounts to building a totally random decision tree.</source>
          <target state="translated">엑스트라 트리는 빌드 방식이 기존 의사 결정 트리와 다릅니다. 두 그룹으로 노드의 샘플을 분리하는 가장 스플릿 보면, 랜덤 분할은 각각 그려 &lt;code&gt;max_features&lt;/code&gt; 임의로 선택된 기능 및 선택된 것 중에서 가장 분할. &lt;code&gt;max_features&lt;/code&gt; 가 1로 설정 되면 이는 완전히 임의의 의사 결정 트리를 작성하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="9955e456c4c0c464fbdd3775651693383fea52c1" translate="yes" xml:space="preserve">
          <source>Extract an ordered array of unique labels</source>
          <target state="translated">순서가 지정된 고유 레이블 배열 추출</target>
        </trans-unit>
        <trans-unit id="73640fcc3fc33a24884d5d906a4df64cbbe3523c" translate="yes" xml:space="preserve">
          <source>Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor.</source>
          <target state="translated">추출은 적합 또는 생성자에 제공된 단어를 사용하여 원시 텍스트 문서에서 카운트를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="752e876b40a502b1de5591e926917e4bc7914d7e" translate="yes" xml:space="preserve">
          <source>Extracting features from text files</source>
          <target state="translated">텍스트 파일에서 기능 추출</target>
        </trans-unit>
        <trans-unit id="d28d7a25a071634693f533b81722275ae4a2006b" translate="yes" xml:space="preserve">
          <source>Extracting the clusters runs in linear time. Note that this results in &lt;code&gt;labels_&lt;/code&gt; which are close to a &lt;a href=&quot;sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; with similar settings and &lt;code&gt;eps&lt;/code&gt;, only if &lt;code&gt;eps&lt;/code&gt; is close to &lt;code&gt;max_eps&lt;/code&gt;.</source>
          <target state="translated">클러스터 추출은 선형 시간으로 실행됩니다. 참고이 결과 그 &lt;code&gt;labels_&lt;/code&gt; 가까이에 있습니다 &lt;a href=&quot;sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt; 유사한 설정과 함께 &lt;code&gt;eps&lt;/code&gt; 경우에만, &lt;code&gt;eps&lt;/code&gt; 에 가까운 &lt;code&gt;max_eps&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f1c8b18c9f903ec74b12bbc8b3c4f7151c923f8b" translate="yes" xml:space="preserve">
          <source>Extracts an ordered list of points and reachability distances, and performs initial clustering using &lt;code&gt;max_eps&lt;/code&gt; distance specified at OPTICS object instantiation.</source>
          <target state="translated">정렬 된 지점 및 도달 거리 목록을 추출하고 OPTICS 개체 인스턴스화에 지정된 &lt;code&gt;max_eps&lt;/code&gt; 거리를 사용하여 초기 클러스터링을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="4d1d5b7b89c16041c19e08ba2433ff6ccea2c09b" translate="yes" xml:space="preserve">
          <source>Extracts patches from a collection of images</source>
          <target state="translated">이미지 모음에서 패치를 추출합니다.</target>
        </trans-unit>
        <trans-unit id="20b1f470ded66ddce46bb1f3957f49e776657ce0" translate="yes" xml:space="preserve">
          <source>F values of features.</source>
          <target state="translated">피처의 F 값.</target>
        </trans-unit>
        <trans-unit id="94361d25a9823c7799f46133208e1994c901281f" translate="yes" xml:space="preserve">
          <source>F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</source>
          <target state="translated">이진 분류에서 양성 클래스의 F- 베타 점수 또는 멀티 클래스 작업에 대한 각 클래스의 F- 베타 점수의 가중 평균.</target>
        </trans-unit>
        <trans-unit id="b88bc0b15f26e6bb0b6f496fb42c5c9bb95eea78" translate="yes" xml:space="preserve">
          <source>F-value between label/feature for regression tasks.</source>
          <target state="translated">회귀 작업에 대한 레이블 / 기능 간의 F- 값.</target>
        </trans-unit>
        <trans-unit id="3ca92f821d80e2b3e5b34951989fdd6926d62950" translate="yes" xml:space="preserve">
          <source>F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task.</source>
          <target state="translated">이진 분류에서 양수 클래스의 F1 점수 또는 멀티 클래스 작업에 대한 각 클래스의 F1 점수의 가중 평균.</target>
        </trans-unit>
        <trans-unit id="03688ba6aa340b87549088aa5739944cb6b1dc73" translate="yes" xml:space="preserve">
          <source>FAQ</source>
          <target state="translated">FAQ</target>
        </trans-unit>
        <trans-unit id="761451a93e0c15b8090688d5167bdaf6518e982d" translate="yes" xml:space="preserve">
          <source>FPR test stands for False Positive Rate test. It controls the total amount of false detections.</source>
          <target state="translated">FPR 테스트는 False Positive Rate 테스트를 나타냅니다. 오 탐지의 총량을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="385b798ea2337dc2cc46e2c01984384fa2b4a883" translate="yes" xml:space="preserve">
          <source>F_beta</source>
          <target state="translated">F_beta</target>
        </trans-unit>
        <trans-unit id="272ad30c6a89ef4d06060249f8d92df03b3df406" translate="yes" xml:space="preserve">
          <source>Face completion with a multi-output estimators</source>
          <target state="translated">다중 출력 추정기로 얼굴 완성</target>
        </trans-unit>
        <trans-unit id="0507c7e4982962e832b85d06191a2b4d2bebd20a" translate="yes" xml:space="preserve">
          <source>Face recognition with eigenfaces</source>
          <target state="translated">고유면을 이용한 얼굴 인식</target>
        </trans-unit>
        <trans-unit id="2a0151d21d57d7f913fb01048c891608a6dbd1dd" translate="yes" xml:space="preserve">
          <source>Face, a 1024 x 768 size image of a raccoon face, is used here to illustrate how &lt;code&gt;k&lt;/code&gt;-means is used for vector quantization.</source>
          <target state="translated">너구리 얼굴의 1024 x 768 크기 이미지 인 Face는 &lt;code&gt;k&lt;/code&gt; - 평균이 벡터 양자화에 어떻게 사용 되는지를 설명 하기 위해 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="e470f0e1bd32044d0ac0cbb19036da58e0de1f71" translate="yes" xml:space="preserve">
          <source>Faces dataset decompositions</source>
          <target state="translated">데이터 셋 분해에 직면</target>
        </trans-unit>
        <trans-unit id="05a6c8a2b029e9776dffd4a4e031ba78fd52aae9" translate="yes" xml:space="preserve">
          <source>Faces recognition example using eigenfaces and SVMs</source>
          <target state="translated">고유면 및 SVM을 사용한 얼굴 인식 예</target>
        </trans-unit>
        <trans-unit id="1395bbf4ff3a0e8184103b7a1548ed436935fe5c" translate="yes" xml:space="preserve">
          <source>Factor Analysis (FA)</source>
          <target state="translated">요인 분석 (FA)</target>
        </trans-unit>
        <trans-unit id="ef94831c8deb9ad37fe90ab4fff5e44828bcfdae" translate="yes" xml:space="preserve">
          <source>Factor analysis &lt;em&gt;can&lt;/em&gt; produce similar components (the columns of its loading matrix) to &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;. However, one can not make any general statements about these components (e.g. whether they are orthogonal):</source>
          <target state="translated">요인 분석 &lt;em&gt;은 &lt;/em&gt;&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 와 유사한 구성 요소 (적재 행렬 열)를 생성 &lt;em&gt;할 수 있습니다&lt;/em&gt; . 그러나 이러한 구성 요소에 대한 일반적인 설명은 할 수 없습니다 (예 : 직각인지 여부).</target>
        </trans-unit>
        <trans-unit id="d48954e3f19174382e76ae0104e93e844b15de73" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using SVD based approach.</source>
          <target state="translated">FactorAnalysis는 SVD 기반 접근 방식을 사용하여 잠재 변수를 관찰 된 변수로 변환하는 소위 &lt;code&gt;loading&lt;/code&gt; 행렬 의 최대 가능성 추정을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="6452044c700752d1602ef512b59b643cc8ae2400" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using expectation-maximization (EM).</source>
          <target state="translated">FactorAnalysis는 기대 최대화 (EM)를 사용하여 소위 &lt;code&gt;loading&lt;/code&gt; 매트릭스의 잠재 가능성 추정 , 잠재 변수를 관측 된 변수로 변환 하는 최대 가능성 추정을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="cf3867c5acb3e93b6681ae294efcb69608ed1285" translate="yes" xml:space="preserve">
          <source>Factorization matrix, sometimes called &amp;lsquo;dictionary&amp;rsquo;.</source>
          <target state="translated">때때로 '사전'이라고하는 인수 분해 행렬.</target>
        </trans-unit>
        <trans-unit id="c37fb3c7cf083e46f6a90c90a7e2709ff0a25468" translate="yes" xml:space="preserve">
          <source>False : never precompute distances</source>
          <target state="translated">False : 거리를 미리 계산하지 마십시오</target>
        </trans-unit>
        <trans-unit id="4031377a355ac026bbc01c0a8c66abf927d5347f" translate="yes" xml:space="preserve">
          <source>False : never precompute distances.</source>
          <target state="translated">False : 거리를 미리 계산하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9f59f0377a675667ec374342cc81b9550d388560" translate="yes" xml:space="preserve">
          <source>False positive rate.</source>
          <target state="translated">거짓 양성률.</target>
        </trans-unit>
        <trans-unit id="25cefd866e6e705dad47ce327a0915fc7b301c18" translate="yes" xml:space="preserve">
          <source>False when &lt;code&gt;y&lt;/code&gt;&amp;rsquo;s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.</source>
          <target state="translated">거짓 때 &lt;code&gt;y&lt;/code&gt; 의 형태는 달리 진정한 맞춤 동안 (N_SAMPLES,) 또는 (N_SAMPLES, 1)입니다.</target>
        </trans-unit>
        <trans-unit id="3b5c093eaf163ad057d3c3eb085ce8a982ff3356" translate="yes" xml:space="preserve">
          <source>False: accept both np.inf and np.nan in X.</source>
          <target state="translated">False : X에서 np.inf와 np.nan을 모두 허용합니다.</target>
        </trans-unit>
        <trans-unit id="e8392e19182108180157ea67d85038a25937bf1d" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in X.</source>
          <target state="translated">False : X에서 np.inf, np.nan, pd.NA를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="ed9971d141108d2bfea275398777be51a58308ae" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in array.</source>
          <target state="translated">False : 배열에 np.inf, np.nan, pd.NA를 허용합니다.</target>
        </trans-unit>
        <trans-unit id="804378d4f4edbfd6d3a924fd7282c07b3c8233b0" translate="yes" xml:space="preserve">
          <source>False: the results is casted to a signed int</source>
          <target state="translated">False : 결과가 부호있는 정수로 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="b73e8a734794b535117e86ab6a6ba9f9a65b9a31" translate="yes" xml:space="preserve">
          <source>Fan, Rong-En, et al., &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;&amp;ldquo;LIBLINEAR: A library for large linear classification.&amp;rdquo;&lt;/a&gt;, Journal of machine learning research 9.Aug (2008): 1871-1874.</source>
          <target state="translated">Fan, Rong-En 등, &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;&quot;LIBLINEAR : 대규모 선형 분류를위한 라이브러리&quot; &lt;/a&gt;, Journal of machine learning research 9. August (2008) : 1871-1874.</target>
        </trans-unit>
        <trans-unit id="c87c316f44a57817f64366d0bec154e1f9523fd3" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="translated">형태소 분석, lemmatizing, 복합 분할, 품사 기반 필터링 등의 고급 토큰 수준 분석은 scikit-learn 코드베이스에 포함되지 않지만 토크 나이저 또는 분석기를 사용자 지정하여 추가 할 수 있습니다. 다음은 &lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK를&lt;/a&gt; 사용하는 토크 나이저와 &lt;code&gt;CountVectorizer&lt;/code&gt; 가 있는 CountVectorizer 입니다 .</target>
        </trans-unit>
        <trans-unit id="47db1ac61ed9cca1bf16f757f65fe37b19da02d2" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="translated">형태소 분석, lemmatizing, 복합 분할, 품사 기반 필터링 등과 같은 멋진 토큰 수준 분석은 scikit-learn 코드베이스에 포함되지 않지만 토크 나이저 또는 분석기를 사용자 지정하여 추가 할 수 있습니다. &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK를&lt;/a&gt; 사용하는 토크 나이저와 &lt;code&gt;CountVectorizer&lt;/code&gt; 있는 CountVectorizer 는 다음과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="000599c940f606b6105f7d1916791a94094aca12" translate="yes" xml:space="preserve">
          <source>Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \(N\) samples in \(D\) dimensions, this approach scales as \(O[D N^2]\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \(N\) grows, the brute-force approach quickly becomes infeasible. In the classes within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt;, brute-force neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'brute'&lt;/code&gt;, and are computed using the routines available in &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">가장 가까운 이웃을 빠르게 계산하는 것은 기계 학습 분야에서 활발한 연구 분야입니다. 가장 순진한 이웃 검색 구현에는 데이터 세트의 모든 점 쌍 사이의 거리에 대한 무차별 계산이 포함됩니다. \ (D \) 차원의 \ (N \) 샘플의 경우이 방법은 \ (O [DN ^ 2] \). 효율적인 무차별 이웃 검색은 소규모 데이터 샘플에 대해 매우 경쟁력이 있습니다. 그러나, 샘플의 수가 증가함에 따라 무차별 대입 방식은 빠르게 실현 될 수 없게된다. &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; 내의 클래스에서 brute-force 이웃 검색은 키워드 &lt;code&gt;algorithm = 'brute'&lt;/code&gt; 사용하여 지정 되며 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt; 에서 사용 가능한 루틴을 사용하여 계산됩니다 .</target>
        </trans-unit>
        <trans-unit id="2e91ff193b9320036698a113daf46d8118a44e7d" translate="yes" xml:space="preserve">
          <source>FastICA on 2D point clouds</source>
          <target state="translated">2D 포인트 클라우드의 FastICA</target>
        </trans-unit>
        <trans-unit id="6921319b009c74978f9e5104f25e9063c7cf2c6b" translate="yes" xml:space="preserve">
          <source>FastICA: a fast algorithm for Independent Component Analysis.</source>
          <target state="translated">FastICA : 독립 성분 분석을위한 빠른 알고리즘.</target>
        </trans-unit>
        <trans-unit id="c2070bedc34b06cdbf740c2fed5e122d3efb93a7" translate="yes" xml:space="preserve">
          <source>Faster for large datasets</source>
          <target state="translated">대규모 데이터 세트에 대해 더 빠름</target>
        </trans-unit>
        <trans-unit id="6fa15d231f996464c9e743e456c3d6855ab0d8f6" translate="yes" xml:space="preserve">
          <source>Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition Letters, 2006, 27(8):861-874.</source>
          <target state="translated">Fawcett T. ROC 분석 소개 [J]. 패턴 인식 편지, 2006, 27 (8) : 861-874.</target>
        </trans-unit>
        <trans-unit id="0724b7da2e7893e2aa61b560332f137d5b0c3174" translate="yes" xml:space="preserve">
          <source>Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874.</source>
          <target state="translated">Fawcett, T. (2006). ROC 분석에 대한 소개입니다. 패턴 인식 편지, 27 (8), 861-874.</target>
        </trans-unit>
        <trans-unit id="e6d44f9610f58495afc33ead5474258ef3212417" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2001. &lt;a href=&quot;http://ieeexplore.ieee.org/document/989510/&quot;&gt;Using rule sets to maximize ROC performance&lt;/a&gt; In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138.</source>
          <target state="translated">Fawcett, T., 2001. 데이터 마이닝에서 &lt;a href=&quot;http://ieeexplore.ieee.org/document/989510/&quot;&gt;ROC 성능을 최대화하기 위해 규칙 집합 사용&lt;/a&gt; , 2001. Proceedings IEEE International Conference, pp. 131-138.</target>
        </trans-unit>
        <trans-unit id="777dc3527e71dea0036f160bc11c11f47afad8e4" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2006. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;An introduction to ROC analysis.&lt;/a&gt; Pattern Recognition Letters, 27(8), pp. 861-874.</source>
          <target state="translated">Fawcett, T., 2006. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;ROC 분석 소개. &lt;/a&gt;패턴 인식 편지, 27 (8), pp. 861-874.</target>
        </trans-unit>
        <trans-unit id="77834b29664ccb5e5cc9173718797e7ec674b1ed" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="translated">&lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;캘리포니아 주택 데이터 집합의&lt;/a&gt; 지형지 물 0 (블록의 중간 소득)과 지형지 물 5 (가구 수)는 규모가 매우 다르고 일부 특이 치가 있습니다. 이 두 가지 특성으로 인해 데이터를 시각화하는 데 어려움이 있으며, 더 중요한 것은 많은 머신 러닝 알고리즘의 예측 성능을 저하시킬 수 있습니다. 스케일링되지 않은 데이터는 많은 그라디언트 기반 추정기의 수렴을 느리게하거나 심지어 방해 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="d43b2b4b53fe06f630d6e6fea5ae39ec97ef2464" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="translated">&lt;a href=&quot;https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;캘리포니아 주택 데이터 세트의&lt;/a&gt; 기능 0 (블록 내 중간 소득) 및 기능 5 (가구 수) 는 매우 다른 척도를 가지며 매우 큰 특이 치를 포함합니다. 이 두 가지 특성은 데이터를 시각화하는 데 어려움을 겪고 더 중요한 것은 많은 기계 학습 알고리즘의 예측 성능을 저하시킬 수 있다는 것입니다. 스케일되지 않은 데이터는 또한 많은 그래디언트 기반 추정기의 수렴 속도를 늦추거나 방지 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="1e2bfc0d574ebb3c0866208d70d9a4defc9229e8" translate="yes" xml:space="preserve">
          <source>Feature Selection</source>
          <target state="translated">기능 선택</target>
        </trans-unit>
        <trans-unit id="3290cbe981e79caf9ab6f89a8812507aa114a727" translate="yes" xml:space="preserve">
          <source>Feature agglomeration</source>
          <target state="translated">기능 응집</target>
        </trans-unit>
        <trans-unit id="81f8ec148b187c97182bf2c56b5ce15909bcbae4" translate="yes" xml:space="preserve">
          <source>Feature agglomeration vs. univariate selection</source>
          <target state="translated">피처 집합 대 일 변량 선택</target>
        </trans-unit>
        <trans-unit id="49e1dcf68a4912e92e5e7f68710255b3ca05471e" translate="yes" xml:space="preserve">
          <source>Feature discretization</source>
          <target state="translated">특징 이산화</target>
        </trans-unit>
        <trans-unit id="3c43171269f5f432d869bba89f82c3b54ca0debf" translate="yes" xml:space="preserve">
          <source>Feature extraction</source>
          <target state="translated">특징 추출</target>
        </trans-unit>
        <trans-unit id="df5954aef1e3ea02a4e2fe61d1e0e10b9f726b5a" translate="yes" xml:space="preserve">
          <source>Feature extraction and normalization.</source>
          <target state="translated">특징 추출 및 정규화.</target>
        </trans-unit>
        <trans-unit id="aedfee8f7e4b62695a0b89930709eed1e654b00f" translate="yes" xml:space="preserve">
          <source>Feature extraction is very different from &lt;a href=&quot;feature_selection#feature-selection&quot;&gt;Feature selection&lt;/a&gt;: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features.</source>
          <target state="translated">특징 추출은 &lt;a href=&quot;feature_selection#feature-selection&quot;&gt;특징 선택&lt;/a&gt; 과 매우 다릅니다 . 전자는 텍스트 나 이미지와 같은 임의의 데이터를 기계 학습에 사용할 수있는 수치 적 특징으로 변환합니다. 후자는 이러한 기능에 적용되는 기계 학습 기술입니다.</target>
        </trans-unit>
        <trans-unit id="4c3a870a4e311a31f848b5314c8f3949d8e31e78" translate="yes" xml:space="preserve">
          <source>Feature hashing can be employed in document classification, but unlike &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see &lt;a href=&quot;#hashing-vectorizer&quot;&gt;Vectorizing a large text corpus with the hashing trick&lt;/a&gt;, below, for a combined tokenizer/hasher.</source>
          <target state="translated">기능 해시는 문서 분류에 이용 될 수 있지만, 달리 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;text.CountVectorizer&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; 는&lt;/a&gt; 단어 분할 또는 유니 코드 - 투 - UTF-8 인코딩 제외한 다른 전처리를하지 않는다; 결합 된 토크 나이저 / 해셔에 대해서는 아래 &lt;a href=&quot;#hashing-vectorizer&quot;&gt;의 해싱 트릭으로 큰 텍스트 코퍼스 벡터화를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7bb8776f4e73559816ad10cf154be669f7df47cf" translate="yes" xml:space="preserve">
          <source>Feature importances with forests of trees</source>
          <target state="translated">나무 숲과 기능의 중요성</target>
        </trans-unit>
        <trans-unit id="561e2555a0c1659bb31a6148c1abe250489ca708" translate="yes" xml:space="preserve">
          <source>Feature mappings for the samples in X.</source>
          <target state="translated">X의 샘플에 대한 기능 매핑</target>
        </trans-unit>
        <trans-unit id="b96ff43d78f2a1f5a4e1d891c2cc36e4c380c32b" translate="yes" xml:space="preserve">
          <source>Feature matrix, for use with estimators or further transformers.</source>
          <target state="translated">추정기 또는 추가 변압기와 함께 사용하기위한 기능 매트릭스.</target>
        </trans-unit>
        <trans-unit id="66d052b121f901629f4ce88124bd7cb2635fe497" translate="yes" xml:space="preserve">
          <source>Feature matrix.</source>
          <target state="translated">피처 매트릭스.</target>
        </trans-unit>
        <trans-unit id="c4587a739696f602e34f3bba74d9106ccf3fff49" translate="yes" xml:space="preserve">
          <source>Feature names corresponding to the indices in &lt;code&gt;features&lt;/code&gt;.</source>
          <target state="translated">의 인덱스에 해당하는 이름 기능 &lt;code&gt;features&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="975d25794ea94f600d607356398714d33388d870" translate="yes" xml:space="preserve">
          <source>Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.</source>
          <target state="translated">바이트 문자열 유형의 기능 이름은있는 그대로 사용됩니다. 유니 코드 문자열은 먼저 UTF-8로 변환되지만 유니 코드 정규화는 수행되지 않습니다. 기능 값은 (유한) 숫자 여야합니다.</target>
        </trans-unit>
        <trans-unit id="9509bbe5f8817e2ca0562d6baac6f0ad1787e1d6" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.</source>
          <target state="translated">재귀 적 기능 제거 및 최상의 기능 수를 교차 검증 한 기능 순위.</target>
        </trans-unit>
        <trans-unit id="f706081a8ad25e74e9a88b9e19a48d5a46a52012" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination.</source>
          <target state="translated">재귀 적 기능 제거를 통한 기능 순위.</target>
        </trans-unit>
        <trans-unit id="7f17a87c5de04e39e04129e9c0737edfeafaee92" translate="yes" xml:space="preserve">
          <source>Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.</source>
          <target state="translated">표준화 (또는 Z- 점수 정규화)를 통한 기능 스케일링은 많은 머신 러닝 알고리즘에 중요한 전처리 단계가 될 수 있습니다. 표준화는 평균의 평균이 0이고 표준 편차가 1 인 표준 정규 분포의 속성을 갖도록 피쳐의 크기를 조정합니다.</target>
        </trans-unit>
        <trans-unit id="3ac99400fe4169f40a977ea7007419c98db61771" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1 for all values of the regularization parameter. The reference article suggests &lt;code&gt;scores_&lt;/code&gt; is the max of &lt;code&gt;all_scores_&lt;/code&gt;.</source>
          <target state="translated">정규화 매개 변수의 모든 값에 대해 0과 1 사이의 피처 스코어입니다. 참조 기사에서는 &lt;code&gt;scores_&lt;/code&gt; 가 최대 &lt;code&gt;all_scores_&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="aa75d746547685bebfdc2a7e366361e478fa27f0" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1.</source>
          <target state="translated">기능 점수는 0과 1 사이입니다.</target>
        </trans-unit>
        <trans-unit id="afb880f1a910829f871ab84d5509ba79bf00b111" translate="yes" xml:space="preserve">
          <source>Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">기능 선택은 일반적으로 실제 학습을 수행하기 전에 전처리 단계로 사용됩니다. scikit-learn에서 권장되는 방법은 sklearn.pipeline.Pipeline을 사용하는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5a515063a6bfb2324e04a5cb7dd1fe0bf9c9b6c2" translate="yes" xml:space="preserve">
          <source>Feature selection mode.</source>
          <target state="translated">기능 선택 모드.</target>
        </trans-unit>
        <trans-unit id="c9b5854c848ad0aec62ba5b490b140e3c7a1bb7d" translate="yes" xml:space="preserve">
          <source>Feature selection using SelectFromModel and LassoCV</source>
          <target state="translated">SelectFromModel 및 LassoCV를 사용한 기능 선택</target>
        </trans-unit>
        <trans-unit id="d13bfd588897e09202c5ea9fa5c11da3f65c72ad" translate="yes" xml:space="preserve">
          <source>Feature selection with sparse data</source>
          <target state="translated">희소 데이터로 기능 선택</target>
        </trans-unit>
        <trans-unit id="8a9e4457b64c8313be96ab86a00a0aa32201d58b" translate="yes" xml:space="preserve">
          <source>Feature selector that removes all low-variance features.</source>
          <target state="translated">저 분산 기능을 모두 제거하는 기능 선택기.</target>
        </trans-unit>
        <trans-unit id="e1b0ac4f2f2d7f5b2a31bb18d47b68f8979e7db4" translate="yes" xml:space="preserve">
          <source>Feature transformations with ensembles of trees</source>
          <target state="translated">앙상블 트리를 사용한 기능 변환</target>
        </trans-unit>
        <trans-unit id="9a6ded29908e935164e678c6c6abd840802b0c72" translate="yes" xml:space="preserve">
          <source>Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.</source>
          <target state="translated">이보다 작거나 같은 피처 값은 0으로, 그 위의 1로 대체됩니다. 희소 행렬에 대한 연산의 임계 값은 0보다 작을 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="0bf4742e81f7f65623aaef302013934b401b3eb5" translate="yes" xml:space="preserve">
          <source>Feature values in training data (also required for prediction)</source>
          <target state="translated">트레이닝 데이터의 피처 값 (예측에도 필요)</target>
        </trans-unit>
        <trans-unit id="3252ec9f22752fab5d0e2a9197cc3c5847a58e00" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data (also required for prediction).</source>
          <target state="translated">훈련 데이터의 특징 벡터 또는 기타 표현 (예측에도 필요).</target>
        </trans-unit>
        <trans-unit id="1ed37149ccf99f3ffe537e50f5c181c973cd258e" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data.</source>
          <target state="translated">훈련 데이터의 특징 벡터 또는 기타 표현.</target>
        </trans-unit>
        <trans-unit id="b7599b9ef6a2c582b4507f1b87929e181ce07276" translate="yes" xml:space="preserve">
          <source>Feature vectors; always 2-d.</source>
          <target state="translated">특징 벡터; 항상 2-d.</target>
        </trans-unit>
        <trans-unit id="8425332e36244b1448079ae72c3edeebff1ff3df" translate="yes" xml:space="preserve">
          <source>Feature-wise means</source>
          <target state="translated">기능적 수단</target>
        </trans-unit>
        <trans-unit id="4dfce3abcd43918524b8cd8b96300d3efd01d1fb" translate="yes" xml:space="preserve">
          <source>Feature-wise transformation of the data.</source>
          <target state="translated">기능별 데이터 변환.</target>
        </trans-unit>
        <trans-unit id="5ce13d491431cc736ca395a68aa77c25024d518b" translate="yes" xml:space="preserve">
          <source>Feature-wise variances</source>
          <target state="translated">기능별 차이</target>
        </trans-unit>
        <trans-unit id="8c7ad0b456fa9bf3ae09e270340fb831f15c3f18" translate="yes" xml:space="preserve">
          <source>FeatureHasher and DictVectorizer Comparison</source>
          <target state="translated">특징 해 셔 및 DictVectorizer 비교</target>
        </trans-unit>
        <trans-unit id="fc338f87a058158eb824b53705961801516a9460" translate="yes" xml:space="preserve">
          <source>Features</source>
          <target state="translated">Features</target>
        </trans-unit>
        <trans-unit id="7b144c2dee4c701c6fc61ec2c427f38a9b6c6529" translate="yes" xml:space="preserve">
          <source>Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding &lt;code&gt;y&lt;/code&gt; when compared to just feature 1</source>
          <target state="translated">당뇨병 데이터 세트의 특징 1 및 2는 아래에 적합하고 그려져 있습니다. 특징 2는 전체 모형에 대해 강한 계수를 갖지만, 단지 특징 1과 비교할 때 &lt;code&gt;y&lt;/code&gt; 에 관한 많은 정보를 제공하지는 않습니다.</target>
        </trans-unit>
        <trans-unit id="27bf161ea6af475b73d710eebe7f66e368f22dbd" translate="yes" xml:space="preserve">
          <source>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.</source>
          <target state="translated">특징은 유방 덩어리의 미세 바늘 흡 인물 (FNA)의 디지털화 된 이미지로부터 계산된다. 그들은 이미지에 존재하는 세포핵의 특성을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="7dc73780307913b147af45ec74f5c7491dc77d15" translate="yes" xml:space="preserve">
          <source>Features got by optimizing the Huber loss.</source>
          <target state="translated">Huber 손실을 최적화하여 얻은 기능.</target>
        </trans-unit>
        <trans-unit id="0a06ba45f44f1716120206612e20d59fa0c2d9b4" translate="yes" xml:space="preserve">
          <source>Features that are deemed of &lt;strong&gt;low importance for a bad model&lt;/strong&gt; (low cross-validation score) could be &lt;strong&gt;very important for a good model&lt;/strong&gt;. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but &lt;strong&gt;how important this feature is for a particular model&lt;/strong&gt;.</source>
          <target state="translated">&lt;strong&gt;불량 모델에 대해 중요도&lt;/strong&gt; 가 &lt;strong&gt;낮은&lt;/strong&gt; 것으로 간주되는 기능 (교차 검증 점수가 낮음)은 &lt;strong&gt;좋은 모델에 매우 중요&lt;/strong&gt; 할 수 있습니다 . 따라서 중요도를 계산하기 전에 홀드 아웃 세트 (또는 교차 검증이 더 좋음)를 사용하여 모델의 예측력을 평가하는 것이 항상 중요합니다. 순열 중요도는 기능 자체의 내재적 예측 값을 반영하는 것이 아니라이 &lt;strong&gt;기능이 특정 모델에 대해 얼마나 중요한지&lt;/strong&gt; 반영합니다 .</target>
        </trans-unit>
        <trans-unit id="285c7b098890786c959265d1fe4e4933df08392b" translate="yes" xml:space="preserve">
          <source>Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.</source>
          <target state="translated">샘플 (매핑)에서 발생하지 않는 기능은 결과 배열 / 매트릭스에서 0 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="e9ec5e9ca7278a6df71a586ca47e81c93d4d7336" translate="yes" xml:space="preserve">
          <source>Features which contain all missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">모든 누락 된 값을 포함하는 기능 &lt;code&gt;fit&lt;/code&gt; 에 버려진 &lt;code&gt;transform&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="71724a67f7aa53e34d06b737e24512de12116bad" translate="yes" xml:space="preserve">
          <source>Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.</source>
          <target state="translated">이 임계 값보다 낮은 학습 설정 편차를 가진 기능은 제거됩니다. 기본값은 분산이 0이 아닌 모든 피처를 유지하는 것입니다. 즉, 모든 표본에서 동일한 값을 갖는 피처를 제거합니다.</target>
        </trans-unit>
        <trans-unit id="c6023b69815535fce0c52cfebfa0b836b623efcf" translate="yes" xml:space="preserve">
          <source>Ferri, C&amp;egrave;sar &amp;amp; Hernandez-Orallo, Jose &amp;amp; Modroiu, R. (2009). &lt;a href=&quot;https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf&quot;&gt;An Experimental Comparison of Performance Measures for Classification.&lt;/a&gt; Pattern Recognition Letters. 30. 27-38.</source>
          <target state="translated">Ferri, C&amp;egrave;sar &amp;amp; Hernandez-Orallo, Jose &amp;amp; Modroiu, R. (2009). &lt;a href=&quot;https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf&quot;&gt;분류를위한 성능 측정의 실험적 비교. &lt;/a&gt;패턴 인식 편지. 30. 27-38.</target>
        </trans-unit>
        <trans-unit id="7609c48291c2fa7cc57f1dc4a85a3683c0749b4f" translate="yes" xml:space="preserve">
          <source>Fetch an mldata.org data set</source>
          <target state="translated">mldata.org 데이터 세트 가져 오기</target>
        </trans-unit>
        <trans-unit id="2b29d907c2ea7e85ef894ade96f5a9788aa198c9" translate="yes" xml:space="preserve">
          <source>Fetch dataset from openml by name or dataset id.</source>
          <target state="translated">이름 또는 데이터 세트 ID로 openml에서 데이터 세트를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="76c8d1e0086c4e650fc514d650a93664c1ba4aa6" translate="yes" xml:space="preserve">
          <source>Fevotte, C., &amp;amp; Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9).</source>
          <target state="translated">Fevotte, C., &amp;amp; Idier, J. (2011). 베타-분산에 의한 음이 아닌 행렬 분해를위한 알고리즘. 신경 계산, 23 (9).</target>
        </trans-unit>
        <trans-unit id="07da69c9120fa48d1a8831ddfdb2e22d5f38a14c" translate="yes" xml:space="preserve">
          <source>Few clusters, even cluster size, non-flat geometry</source>
          <target state="translated">몇몇 클러스터, 심지어 클러스터 크기, 평평하지 않은 기하학</target>
        </trans-unit>
        <trans-unit id="ba82bf0fe9bdb74f2de9056ff08a208a0201f2e5" translate="yes" xml:space="preserve">
          <source>Field &lt;code&gt;support_vectors_&lt;/code&gt; is now empty, only indices of support vectors are stored in &lt;code&gt;support_&lt;/code&gt;</source>
          <target state="translated">이제 &lt;code&gt;support_vectors_&lt;/code&gt; 필드 가 비어 있으며 지원 벡터의 인덱스 만 &lt;code&gt;support_&lt;/code&gt; 에 저장됩니다</target>
        </trans-unit>
        <trans-unit id="e6b7eea98ef60c86409d4e4dd54b3796ed56341b" translate="yes" xml:space="preserve">
          <source>Figure containing partial dependence plots.</source>
          <target state="translated">부분 의존도를 포함하는 그림.</target>
        </trans-unit>
        <trans-unit id="bf29ad109219929951ed38da3e672e781ba3fade" translate="yes" xml:space="preserve">
          <source>Figure containing the confusion matrix.</source>
          <target state="translated">혼동 행렬을 포함하는 그림.</target>
        </trans-unit>
        <trans-unit id="65bafae58e0d2ce7524ef55e10fcc5ed53a13aa4" translate="yes" xml:space="preserve">
          <source>Figure containing the curve.</source>
          <target state="translated">곡선을 포함하는 그림.</target>
        </trans-unit>
        <trans-unit id="63c41242bc3de30c7fa64bf90cc3aa34e5d08243" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values corresponding to Family-wise error rate</source>
          <target state="translated">필터 : 패밀리 별 오류율에 해당하는 p- 값을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="d601967c960c718fae8a27ae10382e904525e519" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values for an estimated false discovery rate</source>
          <target state="translated">필터 : 추정 된 잘못된 발견 비율에 대한 p- 값을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="f9d7c51c4e21ffa778934e88c4ada6f78e753e59" translate="yes" xml:space="preserve">
          <source>Filter: Select the pvalues below alpha based on a FPR test.</source>
          <target state="translated">필터 : FPR 테스트를 기반으로 알파 아래의 p 값을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="fa74263b1f44e0e368feef4eccc429e274602a1d" translate="yes" xml:space="preserve">
          <source>Final perplexity score on training set.</source>
          <target state="translated">훈련 세트에 대한 최종 난이도 점수.</target>
        </trans-unit>
        <trans-unit id="e832a73dedeb0259ac793f9ac425f8e8ee172602" translate="yes" xml:space="preserve">
          <source>Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using &lt;a href=&quot;decomposition#nmf&quot;&gt;Non-negative matrix factorization (NMF or NNMF)&lt;/a&gt;:</source>
          <target state="translated">마지막으로 클러스터링의 하드 할당 제약 조건을 완화함으로써, 예를 들어 &lt;a href=&quot;decomposition#nmf&quot;&gt;NMF 또는 NNMF (Non-negative matrix factorization)&lt;/a&gt; 를 사용하여 코퍼스의 주요 주제를 발견 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="3879f3b348e8db024fb12750b7a6d38d6bbf41c3" translate="yes" xml:space="preserve">
          <source>Finally one can also observe that for some intermediate values of &lt;code&gt;gamma&lt;/code&gt; we get equally performing models when &lt;code&gt;C&lt;/code&gt; becomes very large: it is not necessary to regularize by enforcing a larger margin. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to simplify the decision function with a lower value of &lt;code&gt;C&lt;/code&gt; so as to favor models that use less memory and that are faster to predict.</source>
          <target state="translated">마지막으로 &lt;code&gt;gamma&lt;/code&gt; 일부 중간 값에 대해 &lt;code&gt;C&lt;/code&gt; 가 매우 커지면 동등하게 모델을 수행한다는 것을 알 수 있습니다. 더 큰 마진을 적용하여 정규화 할 필요는 없습니다. RBF 커널의 반경만으로도 우수한 구조적 정규화 기 역할을합니다. 실제로 적은 메모리를 사용하고 예측이 더 빠른 모델을 선호하기 위해 더 낮은 &lt;code&gt;C&lt;/code&gt; 값으로 결정 기능을 단순화하는 것이 여전히 흥미로울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="023b5a681b65ffd6304d113bea1d054693e2974b" translate="yes" xml:space="preserve">
          <source>Finally one should highlight that the Compound Poisson Gamma model that is directly fit on the pure premium is operationally simpler to develop and maintain as it consists in a single scikit-learn estimator instead of a pair of models, each with its own set of hyperparameters.</source>
          <target state="translated">마지막으로, 순수 프리미엄에 직접 맞는 Compound Poisson Gamma 모델은 각각 고유 한 하이퍼 파라미터 세트가있는 한 쌍의 모델 대신 단일 scikit-learn 추정기로 구성되므로 운영상 개발 및 유지 관리가 더 간단하다는 점을 강조해야합니다.</target>
        </trans-unit>
        <trans-unit id="132cf9317ef371444593d6b5ea568f7dd88cd93f" translate="yes" xml:space="preserve">
          <source>Finally we are going to visualize the score:</source>
          <target state="translated">마지막으로 점수를 시각화 할 것입니다.</target>
        </trans-unit>
        <trans-unit id="7c7dc3c27466a01b4ea9d811b8dcf76d3f27d658" translate="yes" xml:space="preserve">
          <source>Finally we will plot the selected two features from the data.</source>
          <target state="translated">마지막으로 데이터에서 선택한 두 기능을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="968616cc558da3fce60b0fa70563b382cf880868" translate="yes" xml:space="preserve">
          <source>Finally, &lt;a href=&quot;#dummy-estimators&quot;&gt;Dummy estimators&lt;/a&gt; are useful to get a baseline value of those metrics for random predictions.</source>
          <target state="translated">마지막으로 &lt;a href=&quot;#dummy-estimators&quot;&gt;더미 추정기&lt;/a&gt; 는 임의 예측에 대한 해당 메트릭의 기준값을 얻는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="f6d5bddccec3511395edb0437e03e1a9559b2146" translate="yes" xml:space="preserve">
          <source>Finally, as we will see next, computing partial dependence plots tree-based models is also orders of magnitude faster making it cheap to compute partial dependence plots for pairs of interacting features:</source>
          <target state="translated">마지막으로, 다음에 볼 수 있듯이 부분 의존성 플롯 트리 기반 모델을 계산하는 것도 몇 배 더 빠르기 때문에 상호 작용하는 특징 쌍에 대한 부분 의존성 플롯을 계산하는 것이 저렴합니다.</target>
        </trans-unit>
        <trans-unit id="bac8fe99c0a1d10b0495b7da851a20ddd76d68ac" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="translated">마지막으로 3.의 경우 scikit-learn 내에 여러 옵션이 있습니다. 모든 알고리즘이 점진적으로 학습 할 수있는 것은 아니지만 (즉, 모든 인스턴스를 한 번에 보지 않고) &lt;code&gt;partial_fit&lt;/code&gt; API를 구현하는 모든 추정 자가 후보입니다. 실제로 미니 배치 인스턴스 ( &quot;온라인 학습&quot;이라고도 함)에서 점진적으로 학습 할 수있는 기능은 특정 시간에 적은 양의 인스턴스 만 메인 메모리. 관련성과 메모리 풋 프린트의 균형을 맞추는 미니 배치에 적합한 크기를 선택하려면 약간의 조정이 필요할 수 있습니다 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8f153367033a26a7113472eddd66aa5516b2f784" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">마지막으로 3의 경우 scikit-learn 내부에 여러 옵션이 있습니다. 모든 알고리즘이 점진적으로 학습 될 수있는 것은 아니지만 (즉, 모든 인스턴스를 한 번에 보지 않아도) &lt;code&gt;partial_fit&lt;/code&gt; API를 구현하는 모든 추정기 는 후보입니다. 실제로, 소량의 인스턴스 (때때로 &quot;온라인 학습&quot;이라고 함)에서 점진적으로 학습하는 기능은 특정 시점에 소량의 인스턴스 만 존재 함을 보장하므로 코어 외부 학습의 핵심입니다. 메인 메모리. 관련성과 메모리 풋 프린트의 균형을 맞추는 미니 배치에 적합한 크기를 선택하려면 약간의 조정이 필요할 수 있습니다 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2ea772d9a2706e63fd635b713dfc5a265ad9f7ca" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="translated">마지막으로 마지막 데이터 세트의 경우 한 샘플이 하이퍼 큐브에 균일하게 분포되어있어 다른 샘플보다 비정상적이라고 말하기는 어렵습니다. 약간 오 &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 되는 sklearn.svm.OneClassSVM을 제외하고 모든 추정자는이 상황에 대한 적절한 솔루션을 제시합니다. 이 경우 좋은 평가자가 모든 샘플에 유사한 점수를 할당해야하므로 샘플의 이상 점수를 더 자세히 살펴 보는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="02251ecbb666e2a1b9f431635bb03c5b54eb0a09" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;code&gt;svm.OneClassSVM&lt;/code&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="translated">마지막으로, 마지막 데이터 세트의 경우, 하나의 샘플이 하이퍼 큐브에 균일하게 분포되어 있기 때문에 하나의 샘플이 다른 샘플보다 더 비정상적이라고 말하기 어렵습니다. 약간 &lt;code&gt;svm.OneClassSVM&lt;/code&gt; 되는 svm.OneClassSVM을 제외하고 모든 추정기는이 상황에 적합한 솔루션을 제시합니다. 이러한 경우, 좋은 추정자가 모든 표본에 유사한 점수를 할당해야하므로 표본의 비정상 점수를 더 자세히 살펴 보는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="ced44a1e1c24bc2a905bacd41c580fdb8b398c7b" translate="yes" xml:space="preserve">
          <source>Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the &lt;code&gt;toarray&lt;/code&gt; method of sparse matrices is another option.</source>
          <target state="translated">마지막으로 중심 데이터가 충분히 작을 것으로 예상되는 경우 희소 행렬 의 &lt;code&gt;toarray&lt;/code&gt; 방법을 사용하여 입력을 배열로 명시 적으로 변환하는 것이 또 다른 옵션입니다.</target>
        </trans-unit>
        <trans-unit id="3e454243da6a98ce545408a4ef381fa38d1b1d45" translate="yes" xml:space="preserve">
          <source>Finally, many parts of the implementation of &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are parallelized.</source>
          <target state="translated">마지막으로 &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt; &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 구현의 많은 부분 이 병렬화됩니다.</target>
        </trans-unit>
        <trans-unit id="9b957bcce911d726f82907b30c1a02070fcadf91" translate="yes" xml:space="preserve">
          <source>Finally, note that parameters of the models have been here handpicked but that in practice they need to be adjusted. In the absence of labelled data, the problem is completely unsupervised so model selection can be a challenge.</source>
          <target state="translated">마지막으로, 모델의 매개 변수는 여기에서 수동으로 선택되었지만 실제로는 조정해야합니다. 레이블이 지정된 데이터가없는 경우 문제는 완전히 감독되지 않으므로 모델 선택이 어려울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="657149d8f47b73d795165e7f97ca51bcb04565c9" translate="yes" xml:space="preserve">
          <source>Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt; needs to be formatted as in &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt; output:</source>
          <target state="translated">마지막으로, 사용자 지정 추정기에 의해 사전 계산을 수행하여 근사 근사치 방법 또는 특수 데이터 유형을 사용한 구현과 같은 다양한 구현을 사용할 수 있습니다. 미리 계산 된 이웃 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;희소 그래프&lt;/a&gt; 는 &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;radius_neighbors_graph&lt;/code&gt; &lt;/a&gt; 출력 에서와 같이 형식화 되어야 합니다.</target>
        </trans-unit>
        <trans-unit id="5d0d4410d99fc712aa6cde836179153d36f8849c" translate="yes" xml:space="preserve">
          <source>Finally, the preprocessing pipeline is integrated in a full prediction pipeline using &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;, together with a simple classification model.</source>
          <target state="translated">마지막으로 전처리 파이프 라인은 간단한 분류 모델과 함께 &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; 을&lt;/a&gt; 사용하여 전체 예측 파이프 라인에 통합됩니다 .</target>
        </trans-unit>
        <trans-unit id="88dd3e3888504f660d94f91abf7147882c40ab2a" translate="yes" xml:space="preserve">
          <source>Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the &lt;code&gt;n_jobs&lt;/code&gt; parameter. If &lt;code&gt;n_jobs=k&lt;/code&gt; then computations are partitioned into &lt;code&gt;k&lt;/code&gt; jobs, and run on &lt;code&gt;k&lt;/code&gt; cores of the machine. If &lt;code&gt;n_jobs=-1&lt;/code&gt; then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using &lt;code&gt;k&lt;/code&gt; jobs will unfortunately not be &lt;code&gt;k&lt;/code&gt; times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).</source>
          <target state="translated">마지막으로이 모듈은 트리의 병렬 구성과 &lt;code&gt;n_jobs&lt;/code&gt; 매개 변수를 통한 예측의 병렬 계산 기능도 제공합니다 . 경우 &lt;code&gt;n_jobs=k&lt;/code&gt; 다음의 계산을들로 분할되는 &lt;code&gt;k&lt;/code&gt; 작업 및 실행 &lt;code&gt;k&lt;/code&gt; 시스템의 코어. 경우 &lt;code&gt;n_jobs=-1&lt;/code&gt; 다음 컴퓨터에서 사용할 수있는 모든 코어가 사용됩니다. 프로세스 간 통신 오버 헤드로 인해 속도가 선형 적이 지 않을 수 있습니다 (즉, &lt;code&gt;k&lt;/code&gt; 작업 사용이 불행히도 &lt;code&gt;k&lt;/code&gt; 배 빠르지 는 않습니다 ). 많은 수의 나무를 만들 때 또는 단일 트리를 만들 때 상당한 시간이 필요한 경우 (예 : 큰 데이터 집합)에도 상당한 속도 향상이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="ac4279287fdb399e92fa8f401fc1a571b7df3622" translate="yes" xml:space="preserve">
          <source>Finally, we can compare the two models using a plot of cumulated claims: for each model, the policyholders are ranked from safest to riskiest and the fraction of observed total cumulated claims is plotted on the y axis. This plot is often called the ordered Lorenz curve of the model.</source>
          <target state="translated">마지막으로, 누적 클레임 플롯을 사용하여 두 모델을 비교할 수 있습니다. 각 모델에 대해 보험 계약자는 가장 안전한 것부터 가장 위험한 것으로 순위가 매겨지고 관찰 된 총 누적 클레임의 비율은 y 축에 표시됩니다. 이 플롯은 종종 모델의 정렬 된 로렌츠 곡선이라고합니다.</target>
        </trans-unit>
        <trans-unit id="abb5440e13032f9c21233aa604593a79c839b358" translate="yes" xml:space="preserve">
          <source>Finally, we fit our pipeline on the training data and use it to predict topics for &lt;code&gt;X_test&lt;/code&gt;. Performance metrics of our pipeline are then printed.</source>
          <target state="translated">마지막으로 훈련 데이터에 파이프 라인을 맞추고이를 사용하여 &lt;code&gt;X_test&lt;/code&gt; 에 대한 주제를 예측합니다 . 그런 다음 파이프 라인의 성능 메트릭이 인쇄됩니다.</target>
        </trans-unit>
        <trans-unit id="f50b898f991b3ec8dadce88abad6749cb3fd8140" translate="yes" xml:space="preserve">
          <source>Finally, we have a full-fledged example of &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;. It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above.</source>
          <target state="translated">마지막으로, 우리는 &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;텍스트 문서&lt;/a&gt; 의 Out-of-core 분류에 대한 본격적인 예를 가지고 있습니다 . 핵심 학습 시스템을 구축하려는 사람들에게 시작점을 제공하고 위에서 논의한 대부분의 개념을 보여주기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="67b1b42b3e66107ae3b505ef41aac41ee3327ff3" translate="yes" xml:space="preserve">
          <source>Finally, we will consider a non-linear model, namely Gradient Boosting Regression Trees. Tree-based models do not require the categorical data to be one-hot encoded: instead, we can encode each category label with an arbitrary integer using &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. With this encoding, the trees will treat the categorical features as ordered features, which might not be always a desired behavior. However this effect is limited for deep enough trees which are able to recover the categorical nature of the features. The main advantage of the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt; over the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt; is that it will make training faster.</source>
          <target state="translated">마지막으로, 비선형 모델, 즉 Gradient Boosting Regression Trees를 고려할 것입니다. 트리 기반 모델은 범주 형 데이터가 원-핫 인코딩 될 필요가 없습니다. 대신 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; 를&lt;/a&gt; 사용하여 임의의 정수로 각 범주 레이블을 인코딩 할 수 있습니다 . 이 인코딩을 사용하면 트리는 범주 형 기능을 순서가 지정된 기능으로 처리하므로 항상 원하는 동작이 아닐 수 있습니다. 그러나이 효과는 기능의 범주 적 특성을 복구 할 수있는 충분히 깊은 트리에 제한됩니다. 의 가장 큰 장점 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; &lt;/a&gt; 오버 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;OneHotEncoder&lt;/code&gt; 은&lt;/a&gt; 이 훈련을 더 빨리 할 것입니다.</target>
        </trans-unit>
        <trans-unit id="15e943be721eab8a2c2612f45c392677826a2d6c" translate="yes" xml:space="preserve">
          <source>Finally, we will plot the predictions made by all models for comparison.</source>
          <target state="translated">마지막으로 비교를 위해 모든 모델에서 만든 예측을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="e9d6cf373d4d749e0936e6f64e1c533e8fd8ee41" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the 20 predictions. The red stars show the average prediction made by &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">마지막으로 20 개의 예측을 시각화합니다. 빨간색 별표는 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; &lt;/a&gt; 의 평균 예측을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="9f642cdc0abe545be7ddd1dd012ea6198c659512" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the results. To do that we will first compute the test set deviance and then plot it against boosting iterations.</source>
          <target state="translated">마지막으로 결과를 시각화합니다. 이를 위해 먼저 테스트 세트 편차를 계산 한 다음 부스팅 반복에 대해 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="a40631d3e46670650e2553ca58294d12cd46b8ee" translate="yes" xml:space="preserve">
          <source>Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]&lt;/a&gt;.</source>
          <target state="translated">마지막으로,베이스 추정기가 샘플과 피처의 서브셋을 기반으로 할 때,이 방법을 랜덤 패치 &lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="7185a548932a7d63b1a08d591559f9d8821b9404" translate="yes" xml:space="preserve">
          <source>Find a &amp;lsquo;safe&amp;rsquo; number of components to randomly project to</source>
          <target state="translated">무작위로 투사 할 '안전한'수의 구성 요소 찾기</target>
        </trans-unit>
        <trans-unit id="e40e118fb652d4dd06f307746b620728b2345e85" translate="yes" xml:space="preserve">
          <source>Find a good set of parameters using grid search.</source>
          <target state="translated">그리드 검색을 사용하여 적절한 매개 변수 세트를 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="022a840c39dad4bac6c36ba2a156531a2e97ca25" translate="yes" xml:space="preserve">
          <source>Find importance of the features</source>
          <target state="translated">기능의 중요성 찾기</target>
        </trans-unit>
        <trans-unit id="2ff57da6a2f73ea1d0dfc969be6516b83c61b137" translate="yes" xml:space="preserve">
          <source>Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from.</source>
          <target state="translated">텍스트의 실제 인코딩이 무엇인지 알아보십시오. 파일에는 인코딩을 알려주는 헤더 또는 README가 제공되거나 텍스트의 출처에 따라 가정 할 수있는 표준 인코딩이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c317277c718283dc0dcc7baf2d812226e24cc453" translate="yes" xml:space="preserve">
          <source>Find the minimum value of an array over positive values</source>
          <target state="translated">양수 값보다 배열의 최소값 찾기</target>
        </trans-unit>
        <trans-unit id="d95b4144f33d10b131b37a33f2e7ad7cc55860b7" translate="yes" xml:space="preserve">
          <source>Find the optimal separating hyperplane using an SVC for classes that are unbalanced.</source>
          <target state="translated">균형이 맞지 않는 클래스에 대해 SVC를 사용하여 최적의 분리 초평면을 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="1c17584736b51c43c88ae3dfa46ea2817cd1a339" translate="yes" xml:space="preserve">
          <source>Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</source>
          <target state="translated">곱이 음이 아닌 행렬 X에 근사한 두 개의 음이 아닌 행렬 (W, H)을 찾으십시오.이 인수 분해는 예를 들어 차원 축소, 소스 분리 또는 주제 추출에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4daca12ed4dca9ca21325e05dde77793902b89f7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization parameter \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt;.</source>
          <target state="translated">합리적인 정규화 매개 변수 \ (\ alpha \)를 찾는 것은 &lt;code&gt;GridSearchCV&lt;/code&gt; 를 사용하여 수행하는 것이 가장 좋습니다 . 일반적으로 &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="d0410c73baea9b092d4edb9802ae4c10c09f4696" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="translated">합리적인 정규화 용어 \ (\ alpha \)를 찾는 것은 일반적으로 &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt; 범위의 &lt;code&gt;GridSearchCV&lt;/code&gt; 를 사용하여 수행하는 것이 가장 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="37e657d78a9dafeda1b142a17aaad5659fab59b7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using automatic hyper-parameter search, e.g. &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="translated">합리적인 정규화 용어 \ (\ alpha \)를 찾는 것은 일반적으로 &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt; 범위의 &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt; 와 같은 자동 하이퍼 매개 변수 검색을 사용하여 수행하는 것이 가장 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="3c5a640c918941c9e6a6ecbfe984c3f4bfbed0d7" translate="yes" xml:space="preserve">
          <source>Finding help</source>
          <target state="translated">도움 찾기</target>
        </trans-unit>
        <trans-unit id="71cc727880d53ab3c238ec955595a0ded28610b7" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">무작위 구조 찾기 : 대략적인 행렬 분해 구성을위한 확률 론적 알고리즘 Halko, et al., 2009 (arXiv : 909) http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="de5cdfe8ddd63c8c4fb1f7f388d421a16cbbf828" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="translated">무작위로 구조 찾기 : 근사 행렬 분해를 구성하기위한 확률 알고리즘 Halko, et al., 2009 (arXiv : 909) https://arxiv.org/pdf/0909.4061.pdf</target>
        </trans-unit>
        <trans-unit id="50ccb6d4c8a3bfa0951293912c0c5132106a8c89" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt;http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</source>
          <target state="translated">임의성을 가진 구조 찾기 : 대략적인 행렬 분해 구성을위한 확률 적 알고리즘 Halko, et al., 2009 &lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt; http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="09110546ddb637f3471f1e7efbbe318b3963484e" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;https://arxiv.org/abs/0909.4061&lt;/a&gt;</source>
          <target state="translated">임의성 구조 찾기 : 근사 행렬 분해를 구성하기위한 확률 알고리즘 Halko, et al., 2009 &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;https://arxiv.org/abs/0909.4061&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26becafaa69af10fe9097be8fd6bf298839f5095" translate="yes" xml:space="preserve">
          <source>Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.</source>
          <target state="translated">희소 코드를 사용하여 데이터를 나타내는 데 가장 적합한 사전 (원자 집합)을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="bdd5a22a4b66cac66c670f6f1297e40cb6d2aae2" translate="yes" xml:space="preserve">
          <source>Finds a sparse representation of data against a fixed, precomputed dictionary.</source>
          <target state="translated">사전 계산 된 고정 사전에 대해 데이터의 희소 한 표현을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="a5dbd88babc750758342dfe447e24f6c260c3097" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them.</source>
          <target state="translated">고밀도 코어 샘플을 찾아 클러스터를 확장합니다.</target>
        </trans-unit>
        <trans-unit id="e0fb0de42fddb601e63b5dedf2b446e9b6fdd66e" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them. This example uses data that is generated so that the clusters have different densities. The &lt;a href=&quot;../../modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt;&lt;code&gt;sklearn.cluster.OPTICS&lt;/code&gt;&lt;/a&gt; is first used with its Xi cluster detection method, and then setting specific thresholds on the reachability, which corresponds to &lt;a href=&quot;../../modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;sklearn.cluster.DBSCAN&lt;/code&gt;&lt;/a&gt;. We can see that the different clusters of OPTICS&amp;rsquo;s Xi method can be recovered with different choices of thresholds in DBSCAN.</source>
          <target state="translated">고밀도의 핵심 샘플을 찾아서 클러스터를 확장합니다. 이 예에서는 클러스터의 밀도가 서로 다르도록 생성 된 데이터를 사용합니다. &lt;a href=&quot;../../modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt; &lt;code&gt;sklearn.cluster.OPTICS&lt;/code&gt; 는&lt;/a&gt; 먼저 클러스터 사이 검출 방법에서 사용하고있는 대응에 도달에 특정 임계 값을 설정한다 &lt;a href=&quot;../../modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;sklearn.cluster.DBSCAN&lt;/code&gt; &lt;/a&gt; . OPTICS의 Xi 방법의 다양한 클러스터가 DBSCAN에서 다양한 임계 값 선택으로 복구 될 수 있음을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0ac333cc4bd6a4e85a5691bb991f9cd4f114c12" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point.</source>
          <target state="translated">점의 K- 이웃을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="e7def6740e556221f1aa42003fab4e2365f3483e" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point.</source>
          <target state="translated">점의 K- 이웃을 찾습니다. 각 점의 인접 지표와 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="011362db8e96e3edcb95f8fcdf5f1e0d92d5e3e2" translate="yes" xml:space="preserve">
          <source>Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving:</source>
          <target state="translated">다음을 해결하여 데이터 행렬 X를 근사화하는 데 가장 적합한 사전과 해당 스파 스 코드를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="b6f70d43205a979d509f018cfbe88e93d812b3ec" translate="yes" xml:space="preserve">
          <source>Finds the neighbors within a given radius of a point or points.</source>
          <target state="translated">주어진 지점의 반경 내에서 이웃을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="dd8ddfd214a6d88017dd4006d52b1774d72e0be7" translate="yes" xml:space="preserve">
          <source>Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.</source>
          <target state="translated">데이터를 최적으로 재구성 할 수있는 희소 구성 요소 집합을 찾습니다. Sparseness의 양은 매개 변수 alpha에 의해 주어진 L1 페널티의 계수에 의해 제어 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9ce397e0f2c9f33bd1911b665e99384e26321100" translate="yes" xml:space="preserve">
          <source>Finite Gaussian mixture fit with EM.</source>
          <target state="translated">유한 가우스 혼합은 EM에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="fd9ae1ff1bfcac14b05b92d277d3f6074f078a31" translate="yes" xml:space="preserve">
          <source>First 10 columns are numeric predictive values</source>
          <target state="translated">처음 10 개의 열은 숫자 예측값입니다.</target>
        </trans-unit>
        <trans-unit id="30250bc7520fcdac140dbcf0c3820bc484ca88d7" translate="yes" xml:space="preserve">
          <source>First example</source>
          <target state="translated">첫 번째 예</target>
        </trans-unit>
        <trans-unit id="7874170ffefbed5d0d1c4372827ea2aeb194c3ba" translate="yes" xml:space="preserve">
          <source>First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion.</source>
          <target state="translated">먼저 훈련 세트에 나무의 앙상블 (완전히 임의의 나무, 임의의 숲 또는 그라디언트 강화 나무)을 맞추십시오. 그런 다음 앙상블에서 각 트리의 각 리프에는 새 기능 공간에서 고정 된 임의의 기능 인덱스가 할당됩니다. 이 리프 인덱스는 원-핫 방식으로 인코딩됩니다.</target>
        </trans-unit>
        <trans-unit id="c9c47ba90ccfdfceca4eb5b8199939d35b815802" translate="yes" xml:space="preserve">
          <source>First note that the K means \(\mu_k\) are vectors in \(\mathcal{R}^d\), and they lie in an affine subspace \(H\) of dimension at least \(K - 1\) (2 points lie on a line, 3 points lie on a plane, etc).</source>
          <target state="translated">먼저 K는 \ (\ mu_k \)가 \ (\ mathcal {R} ^ d \)의 벡터이며, 적어도 \ (K-1 \) 차원의 아핀 부분 공간 \ (H \)에 있다는 것을 의미합니다. (2 점은 선에, 3 점은 평면에 있습니다.)</target>
        </trans-unit>
        <trans-unit id="5b7d36dc535373f63b2b8c97af1496035ebdccf8" translate="yes" xml:space="preserve">
          <source>First of all, we can take a look to the values of the coefficients of the regressor we have fitted.</source>
          <target state="translated">우선, 우리가 피팅 한 회귀 변수의 계수 값을 살펴볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="af1667b67f8a74831d9cbccc10fec01c4bb8c75d" translate="yes" xml:space="preserve">
          <source>First we check which value of \(\alpha\) has been selected.</source>
          <target state="translated">먼저 \ (\ alpha \)의 어떤 값이 선택되었는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="31c524afbf18465a1ae6f80767e0e696d536b875" translate="yes" xml:space="preserve">
          <source>First we create a data set of 9 samples from 3 classes, and plot the points in the original space. For this example, we focus on the classification of point no. 3. The thickness of a link between point no. 3 and another point is proportional to their distance.</source>
          <target state="translated">먼저 3 개 클래스에서 9 개 샘플로 구성된 데이터 세트를 만들고 원래 공간에 점을 플로팅합니다. 이 예에서는 포인트 번호의 분류에 중점을 둡니다. 3. 지점 번호 사이의 링크 두께. 3과 다른 점은 거리에 비례합니다.</target>
        </trans-unit>
        <trans-unit id="66d3e97d3998fff9804546e983728130f9a1d48d" translate="yes" xml:space="preserve">
          <source>First we download the two datasets. Diabetes dataset is shipped with scikit-learn. It has 442 entries, each with 10 features. California Housing dataset is much larger with 20640 entries and 8 features. It needs to be downloaded. We will only use the first 400 entries for the sake of speeding up the calculations but feel free to use the whole dataset.</source>
          <target state="translated">먼저 두 개의 데이터 세트를 다운로드합니다. 당뇨병 데이터 세트는 scikit-learn과 함께 제공됩니다. 여기에는 각각 10 개의 기능이있는 442 개의 항목이 있습니다. California Housing 데이터 세트는 20640 개의 항목과 8 개의 기능으로 훨씬 더 큽니다. 다운로드가 필요합니다. 계산 속도를 높이기 위해 처음 400 개의 항목 만 사용하지만 전체 데이터 세트를 자유롭게 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="c2e4b6f9c83c90558d9df87e3572e1d8ff690d21" translate="yes" xml:space="preserve">
          <source>First we need to load the data.</source>
          <target state="translated">먼저 데이터를로드해야합니다.</target>
        </trans-unit>
        <trans-unit id="8d180e35a78f80e45c6959bdc2213230723a6ec9" translate="yes" xml:space="preserve">
          <source>First we verify which value of \(\alpha\) has been selected.</source>
          <target state="translated">먼저 \ (\ alpha \)의 어떤 값이 선택되었는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="da6f0be97a3f9bc23cb9968f6dd764558635185b" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s get some insights by looking at the variable distributions and at the pairwise relationships between them. Only numerical variables will be used. In the following plot, each dot represents a sample.</source>
          <target state="translated">먼저, 변수 분포와 그 사이의 쌍별 관계를 살펴봄으로써 몇 가지 통찰력을 얻으십시오. 숫자 변수 만 사용됩니다. 다음 그림에서 각 점은 샘플을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="1c794cffa5e7e2eb61589bb3349c992e76e5c222" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s load the diabetes dataset which is available from within sklearn. Then, we will look what features are collected for the diabates patients:</source>
          <target state="translated">먼저 sklearn 내에서 사용할 수있는 당뇨병 데이터 세트를로드 해 보겠습니다. 그런 다음, 당뇨병 환자를 위해 수집 된 기능을 살펴볼 것입니다.</target>
        </trans-unit>
        <trans-unit id="cac3b241d42030e20b4b1109edf6cf6d51db7345" translate="yes" xml:space="preserve">
          <source>First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline:</source>
          <target state="translated">첫째, 사전 계산 된 그래프는 예를 들어 추정기의 매개 변수를 변경하면서 여러 번 재사용 할 수 있습니다. 사용자가 수동으로 수행하거나 scikit-learn 파이프 라인의 캐싱 속성을 사용하여 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="97be8aa0250f0cdac7301eb79bf3b2a478185ccc" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;GaussianNB&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="translated">먼저, 세 가지 예시 분류기 ( &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;../../modules/generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;GaussianNB&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt; )가 초기화 되고 가중치 &lt;code&gt;[1, 1, 5]&lt;/code&gt; 로 소프트 투표 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; 를 초기화하는 데 사용됩니다. 즉, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt; 의 예측 확률 이 평균 확률이 계산 될 때 다른 분류기의 가중치.</target>
        </trans-unit>
        <trans-unit id="16a8cd51addf2278ba02a88d4c3026fabfe281d9" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;code&gt;LogisticRegression&lt;/code&gt;, &lt;code&gt;GaussianNB&lt;/code&gt;, and &lt;code&gt;RandomForestClassifier&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;RandomForestClassifier&lt;/code&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="translated">먼저, 3 개의 시험 분류기 ( &lt;code&gt;LogisticRegression&lt;/code&gt; , &lt;code&gt;GaussianNB&lt;/code&gt; 및 &lt;code&gt;RandomForestClassifier&lt;/code&gt; )가 초기화 되고 가중치가 &lt;code&gt;[1, 1, 5]&lt;/code&gt; 인 소프트 투표 투표 &lt;code&gt;VotingClassifier&lt;/code&gt; 를 초기화하는 데 사용됩니다. 즉, &lt;code&gt;RandomForestClassifier&lt;/code&gt; 의 예상 확률이 5 배로 계산됩니다. 평균 확률이 계산 될 때 다른 분류기의 가중치.</target>
        </trans-unit>
        <trans-unit id="f50c60bee958db3de20cd6c08f2c3fe9b2fb6e88" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[2,
1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; each count 2 times as much as the weights of the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; classifier when the averaged probability is calculated.</source>
          <target state="translated">먼저, 세 가지 예시적인 분류기 ( &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; )가 초기화 되고 가중치가 &lt;code&gt;[2, 1, 2]&lt;/code&gt; 인 소프트 투표 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; 를 초기화하는 데 사용됩니다 . 이는 &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 의 예측 확률이 각각 다음 과 같이 2 번 계산 됨을 의미합니다. 평균 확률을 계산할 때 &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; 분류기 의 가중치만큼 .</target>
        </trans-unit>
        <trans-unit id="1ea180eeb1f820255090eaa2e746ff697919b622" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;, &lt;code&gt;KNeighborsClassifier&lt;/code&gt;, and &lt;code&gt;SVC&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[2, 1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; and &lt;code&gt;SVC&lt;/code&gt; count 5 times as much as the weights of the &lt;code&gt;KNeighborsClassifier&lt;/code&gt; classifier when the averaged probability is calculated.</source>
          <target state="translated">먼저, 3 개의 예시적인 분류기 ( &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; , &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 및 &lt;code&gt;SVC&lt;/code&gt; )가 초기화 되고 가중치가 &lt;code&gt;[2, 1, 2]&lt;/code&gt; 인 소프트 투표 투표 &lt;code&gt;VotingClassifier&lt;/code&gt; 를 초기화하는 데 사용됩니다. 즉, &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; 및 &lt;code&gt;SVC&lt;/code&gt; 의 예상 확률이 5 배로 계산됩니다. 평균 확률이 계산 될 때 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 분류기 의 가중치로 .</target>
        </trans-unit>
        <trans-unit id="5fdc5502e4f55e2ed52afffb452568aea16c1218" translate="yes" xml:space="preserve">
          <source>First, we fit the model.</source>
          <target state="translated">먼저 모델을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="ef9c4b59f96926f6f0e5f166376f8408083d95db" translate="yes" xml:space="preserve">
          <source>First, we load the wine dataset and convert it to a binary classification problem. Then, we train a support vector classifier on a training dataset.</source>
          <target state="translated">먼저 wine 데이터 세트를로드하고 이진 분류 문제로 변환합니다. 그런 다음 훈련 데이터 세트에서 지원 벡터 분류기를 훈련합니다.</target>
        </trans-unit>
        <trans-unit id="9d07cc681f72e7314cfb570248652c85d831875b" translate="yes" xml:space="preserve">
          <source>First, we must understand the structure of our data. It has 100 randomly generated input datapoints, 3 classes split unevenly across datapoints, and 10 &amp;ldquo;groups&amp;rdquo; split evenly across datapoints.</source>
          <target state="translated">먼저 데이터의 구조를 이해해야합니다. 임의로 생성 된 100 개의 입력 데이터 포인트, 3 개의 클래스가 데이터 포인트에 고르지 않게 분할되고 10 개의 &quot;그룹&quot;이 데이터 포인트에 고르게 분할됩니다.</target>
        </trans-unit>
        <trans-unit id="cc0ba427a614a821f465ac72c31717d2cbd7abed" translate="yes" xml:space="preserve">
          <source>First, we train a decision tree and a multi-layer perceptron on the diabetes dataset.</source>
          <target state="translated">먼저, 당뇨병 데이터 세트에 대한 의사 결정 트리와 다층 퍼셉트론을 훈련합니다.</target>
        </trans-unit>
        <trans-unit id="f123dfa67a1788d2150ed770bdb351ff3e4fb8cc" translate="yes" xml:space="preserve">
          <source>First, we train a random forest on the breast cancer dataset and evaluate its accuracy on a test set:</source>
          <target state="translated">먼저 유방암 데이터 세트에서 무작위 포리스트를 훈련시키고 테스트 세트에서 정확도를 평가합니다.</target>
        </trans-unit>
        <trans-unit id="2b8f70566f670b04c1ac522dc7b0f129462badb9" translate="yes" xml:space="preserve">
          <source>First, we want to estimate the score on the original data:</source>
          <target state="translated">먼저 원본 데이터에 대한 점수를 추정하려고합니다.</target>
        </trans-unit>
        <trans-unit id="fab091b4d5d2bc522f0ed86af86bd56d5c7a2d19" translate="yes" xml:space="preserve">
          <source>First, we will load the diabetes dataset and initiate a gradient boosting regressor, a random forest regressor and a linear regression. Next, we will use the 3 regressors to build the voting regressor:</source>
          <target state="translated">먼저 당뇨병 데이터 세트를로드하고 경사 부스팅 회귀, 랜덤 포레스트 회귀 및 선형 회귀를 시작합니다. 다음으로 3 개의 회귀자를 사용하여 투표 회귀자를 구축합니다.</target>
        </trans-unit>
        <trans-unit id="5918b4ffaa61bc5b2d97e424d8740b3562535f07" translate="yes" xml:space="preserve">
          <source>First, we would like a transformer that extracts the subject and body of each post. Since this is a stateless transformation (does not require state information from training data), we can define a function that performs the data transformation then use &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to create a scikit-learn transformer.</source>
          <target state="translated">먼저 각 게시물의 제목과 본문을 추출하는 변환기를 원합니다. 이것은 상태 비 저장 변환이므로 (학습 데이터의 상태 정보가 필요하지 않음) 데이터 변환을 수행하는 함수를 정의한 다음 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하여 scikit-learn 변환기를 만들 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b01058ecfb06b2978590da1239a0a81618b1465b" translate="yes" xml:space="preserve">
          <source>Fisher transformation. Wikipedia. &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://en.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</source>
          <target state="translated">피셔 변환. 위키 백과. &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://ko.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec02df98bcfaf21ee3dd0abe872b47c664ba1c" translate="yes" xml:space="preserve">
          <source>Fisher, R.A. &amp;ldquo;The use of multiple measurements in taxonomic problems&amp;rdquo; Annual Eugenics, 7, Part II, 179-188 (1936); also in &amp;ldquo;Contributions to Mathematical Statistics&amp;rdquo; (John Wiley, NY, 1950).</source>
          <target state="translated">Fisher, RA&amp;ldquo;분류학 문제에서 다중 측정의 사용&amp;rdquo;Annual Eugenics, 7, Part II, 179-188 (1936); 또한&amp;ldquo;수학적 통계에 대한 기여&amp;rdquo;(John Wiley, NY, 1950)에도 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d49737491d2fdff73fcfce78bfa3496db9aead4" translate="yes" xml:space="preserve">
          <source>Fit Gaussian Naive Bayes according to X, y</source>
          <target state="translated">X, y에 따라 가우스 나이브 베이에 적합</target>
        </trans-unit>
        <trans-unit id="5697d81dd45cef7c7ddb8bf2ab425085bcc0125d" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process classification model</source>
          <target state="translated">가우스 프로세스 분류 모델 적합</target>
        </trans-unit>
        <trans-unit id="87cac59f89b2fa13ce5de11220e3eeec4bd8c5cf" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process regression model.</source>
          <target state="translated">가우스 프로세스 회귀 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="8eb1d823143ba03b3eb9b823b5dab8b0cf44511a" translate="yes" xml:space="preserve">
          <source>Fit Kernel Ridge regression model</source>
          <target state="translated">커널 릿지 회귀 모형 적합</target>
        </trans-unit>
        <trans-unit id="62df27bb0a01d202786ab5c8f2652f91558551a4" translate="yes" xml:space="preserve">
          <source>Fit KernelCenterer</source>
          <target state="translated">커널 센터 너 맞추기</target>
        </trans-unit>
        <trans-unit id="f97a5239f74bba940e2a0890df2d8120afcfa03c" translate="yes" xml:space="preserve">
          <source>Fit LSI model on training data X.</source>
          <target state="translated">훈련 데이터 X에 LSI 모델을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="c2f5d79cd7108f6536b92a8ab72454a376b81e9e" translate="yes" xml:space="preserve">
          <source>Fit LSI model to X and perform dimensionality reduction on X.</source>
          <target state="translated">LSI 모델을 X에 맞추고 X에서 차원 축소를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="f79f31a7bd81775029053b8e229553093e280715" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given</source>
          <target state="translated">주어진에 따라 LinearDiscriminantAnalysis 모델을 적합</target>
        </trans-unit>
        <trans-unit id="5a914df7976c81fad1a03ac324a4632a19d9e602" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 LinearDiscriminantAnalysis 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="cb91a62a58b24a52b6e32fdd160e489a55cfbac1" translate="yes" xml:space="preserve">
          <source>Fit MultiTaskElasticNet model with coordinate descent</source>
          <target state="translated">좌표 하강으로 MultiTaskElasticNet 모델 맞추기</target>
        </trans-unit>
        <trans-unit id="07e793af99a651a8f0a47822a080e4cf194bac9b" translate="yes" xml:space="preserve">
          <source>Fit Naive Bayes classifier according to X, y</source>
          <target state="translated">X, y에 따라 Naive Bayes 분류기에 적합</target>
        </trans-unit>
        <trans-unit id="afc10844f54e485a835b4e52b36e0ebefe70c8de" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X, then transform X.</source>
          <target state="translated">OneHotEncoder를 X에 맞추고 X를 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="6cbc8c577214883d5f9bdf864d2a7d76e95b5a74" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X.</source>
          <target state="translated">OneHotEncoder를 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="7a3e15c6e4bf063bec4aa435d4748fb868286ed7" translate="yes" xml:space="preserve">
          <source>Fit Ridge and HuberRegressor on a dataset with outliers.</source>
          <target state="translated">특이 치가있는 데이터 세트에 Ridge 및 HuberRegressor를 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="9157b13407894777185a7ac8935a66fc19422ca1" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier model.</source>
          <target state="translated">Ridge 분류기 모델에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="e744f084cdfc69f98e0c6bdc1ef85d9a5fdf52b4" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier with cv.</source>
          <target state="translated">cv와 Ridge 분류기를 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="8f8e3b0b4710ae51d90dcd69d91e33d123946d3a" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model</source>
          <target state="translated">릿지 회귀 모형 적합</target>
        </trans-unit>
        <trans-unit id="47da48f063cc42a3f008d1e6c416d90a29ea0591" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model with cv.</source>
          <target state="translated">cv를 사용하여 Ridge 회귀 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2e3e7ebdde04653523f01d0d804ccc0c533f3460" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model.</source>
          <target state="translated">릿지 회귀 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="74fbf563cb041ef637760be7f01def498fc1300e" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space and return that transformed output.</source>
          <target state="translated">X를 포함 된 공간에 맞추고 변환 된 출력을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="97ca0bc1677dbeaefd600411062ca557eacc6754" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space.</source>
          <target state="translated">X를 포함 된 공간에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="9b21c283121f9ec299302b1b3016dbcdb3391466" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="translated">베이지안 릿지 모델을 피팅하고 정규화 매개 변수 람다 (무게의 정밀도)와 알파 (소음의 정밀)를 최적화합니다.</target>
        </trans-unit>
        <trans-unit id="dd6f625705b26516fc692deb07fa9f7046f6035b" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="translated">베이지안 능선 모델을 적합합니다. 이 구현과 정규화 매개 변수 람다 (가중 정밀도) 및 알파 (노이즈 정밀도)의 최적화에 대한 자세한 내용은 참고 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="dac39fc69b5b06ff2bcc2ce8d7b8984bce5d49fc" translate="yes" xml:space="preserve">
          <source>Fit a Generalized Linear Model.</source>
          <target state="translated">일반화 선형 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="8bec80135f416f6c22f260fc3b1ac04b4cff59ba" translate="yes" xml:space="preserve">
          <source>Fit a model to the random subset (&lt;code&gt;base_estimator.fit&lt;/code&gt;) and check whether the estimated model is valid (see &lt;code&gt;is_model_valid&lt;/code&gt;).</source>
          <target state="translated">모형을 임의의 부분 집합 ( &lt;code&gt;base_estimator.fit&lt;/code&gt; )에 맞추고 추정 된 모형이 유효한지 확인하십시오 ( &lt;code&gt;is_model_valid&lt;/code&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="ebd8c04ad061ef008e394e38c512b2d8b58287fa" translate="yes" xml:space="preserve">
          <source>Fit a semi-supervised label propagation model based</source>
          <target state="translated">반 감독 라벨 전파 모델 기반</target>
        </trans-unit>
        <trans-unit id="deba03b3f9d00ab1e647b43aa626a17c324cec03" translate="yes" xml:space="preserve">
          <source>Fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.</source>
          <target state="translated">모든 변환을 차례로 맞추고 데이터를 변환 한 다음 최종 추정기를 사용하여 변환 된 데이터를 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="114f8ea42731308cb5abe991bcd972d570944b6a" translate="yes" xml:space="preserve">
          <source>Fit all transformers using X.</source>
          <target state="translated">X를 사용하여 모든 변압기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="246e8826a6bd7a4f56f132c591c766db9ce2d6b0" translate="yes" xml:space="preserve">
          <source>Fit all transformers, transform the data and concatenate results.</source>
          <target state="translated">모든 변압기를 장착하고 데이터를 변환하며 결과를 연결하십시오.</target>
        </trans-unit>
        <trans-unit id="922528f3171c2d46b470779ae89446b3062cf8a4" translate="yes" xml:space="preserve">
          <source>Fit estimator and transform dataset.</source>
          <target state="translated">추정기에 적합하고 데이터 세트를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="54af94e49eb61f7670851d6ef57ae7193fdff2da" translate="yes" xml:space="preserve">
          <source>Fit estimator to data.</source>
          <target state="translated">추정량을 데이터에 맞추십시오.</target>
        </trans-unit>
        <trans-unit id="85fa5fe8b9795ca2b319f392f4a4756a8b584c81" translate="yes" xml:space="preserve">
          <source>Fit estimator using RANSAC algorithm.</source>
          <target state="translated">RANSAC 알고리즘을 사용하여 추정기를 적합합니다.</target>
        </trans-unit>
        <trans-unit id="820a840249b34711a41e746dbc8ea00e0bef6043" translate="yes" xml:space="preserve">
          <source>Fit estimator.</source>
          <target state="translated">추정량을 맞추십시오.</target>
        </trans-unit>
        <trans-unit id="c7654cec30bb319e2781e513c9f9ca93708de9b2" translate="yes" xml:space="preserve">
          <source>Fit is on grid of alphas and best alpha estimated by cross-validation.</source>
          <target state="translated">적합은 교차 검증에 의해 추정 된 알파 및 최상의 알파 그리드에 있습니다.</target>
        </trans-unit>
        <trans-unit id="b8cbf7d1420444207c14a8b65b5fcf478860f130" translate="yes" xml:space="preserve">
          <source>Fit label binarizer</source>
          <target state="translated">맞춤 라벨 이진 화기</target>
        </trans-unit>
        <trans-unit id="e7ecedc7848a8ef424a3d78854bd272e8154c780" translate="yes" xml:space="preserve">
          <source>Fit label binarizer and transform multi-class labels to binary labels.</source>
          <target state="translated">레이블 이진화기를 장착하고 다중 클래스 레이블을 이진 레이블로 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="c75cef45e92464b24b2d996a50f9aed44b9ef31e" translate="yes" xml:space="preserve">
          <source>Fit label encoder</source>
          <target state="translated">맞춤 라벨 인코더</target>
        </trans-unit>
        <trans-unit id="5de88deba19907fdf6f6eb3678730f7aacb3c312" translate="yes" xml:space="preserve">
          <source>Fit label encoder and return encoded labels</source>
          <target state="translated">라벨 인코더 맞춤 및 인코딩 된 라벨 반환</target>
        </trans-unit>
        <trans-unit id="cf615a74e4f9177f1a29f39484c75ecef454ecbe" translate="yes" xml:space="preserve">
          <source>Fit linear model with Passive Aggressive algorithm.</source>
          <target state="translated">Passive Aggressive 알고리즘으로 선형 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="def5054532c40485109a181ee65c06aba2df1b53" translate="yes" xml:space="preserve">
          <source>Fit linear model with Stochastic Gradient Descent.</source>
          <target state="translated">확률 적 경사 하강으로 선형 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="80dff5f041e36b3407c4f4e7528431510ae562cc" translate="yes" xml:space="preserve">
          <source>Fit linear model with coordinate descent</source>
          <target state="translated">좌표 하강으로 선형 모형 적합</target>
        </trans-unit>
        <trans-unit id="4a42bd8bc00e1e2eb46153b9c0e14ea2a3d30f42" translate="yes" xml:space="preserve">
          <source>Fit linear model.</source>
          <target state="translated">선형 모형에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="596c5db56f9a987de5f7691971fcf687d77673ec" translate="yes" xml:space="preserve">
          <source>Fit model to data.</source>
          <target state="translated">모델을 데이터에 적합</target>
        </trans-unit>
        <trans-unit id="7bad9b91d21434b63dea0481c80eaf84a7cbcb25" translate="yes" xml:space="preserve">
          <source>Fit model with coordinate descent.</source>
          <target state="translated">좌표 하강에 맞는 모델.</target>
        </trans-unit>
        <trans-unit id="8c69e1745d795a8f39d4e8e6661ab7afc8b0bcff" translate="yes" xml:space="preserve">
          <source>Fit regression model</source>
          <target state="translated">회귀 모델 적합</target>
        </trans-unit>
        <trans-unit id="dddcb769b9b69c71c174f417fcd2a333b19f55fd" translate="yes" xml:space="preserve">
          <source>Fit regression model with Bayesian Ridge Regression.</source>
          <target state="translated">베이지안 릿지 회귀를 사용하여 회귀 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="dda67be7d4db5d3187deb44446a3404222c185a6" translate="yes" xml:space="preserve">
          <source>Fit the ARDRegression model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 ARDRegression 모델을 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="b5a680830417593aa73dd7de7fcef69c6ed24e1b" translate="yes" xml:space="preserve">
          <source>Fit the EllipticEnvelope model.</source>
          <target state="translated">EllipticEnvelope 모델을 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="b52ba2a596ae6c033bcc7f3cf20ecd5c58d00c2f" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using EM</source>
          <target state="translated">EM을 사용하여 FactorAnalysis 모델을 X에 맞추기</target>
        </trans-unit>
        <trans-unit id="d35f22f8e1c64965b364c3173e1ec356759343b4" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using SVD based approach</source>
          <target state="translated">SVD 기반 접근 방식을 사용하여 FactorAnalysis 모델을 X에 맞추기</target>
        </trans-unit>
        <trans-unit id="6613ce48c67ddab345a10c60b764cb90b5d43667" translate="yes" xml:space="preserve">
          <source>Fit the Kernel Density model on the data.</source>
          <target state="translated">커널 밀도 모델을 데이터에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="18de2bdae7f6b4aabf66891dec1f775662960310" translate="yes" xml:space="preserve">
          <source>Fit the LSH forest on the data.</source>
          <target state="translated">데이터에 LSH 포리스트를 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="12a1c0d798db89b07ae159bda7700f6de8a414b1" translate="yes" xml:space="preserve">
          <source>Fit the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 Ledoit-Wolf 수축 공분산 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="7e835684e570bb3989f14c8abec4c77362174b02" translate="yes" xml:space="preserve">
          <source>Fit the NearestCentroid model according to the given training data.</source>
          <target state="translated">주어진 훈련 데이터에 따라 NearestCentroid 모델을 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="faef2120c4a0d719909dbc882126884ed71710cf" translate="yes" xml:space="preserve">
          <source>Fit the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 Oracle Approximating Shrinkage 공분산 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="f68da32a4110559ba3f4b03666c6374bbe1b67bc" translate="yes" xml:space="preserve">
          <source>Fit the OrdinalEncoder to X.</source>
          <target state="translated">OrdinalEncoder를 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="9efb3f60cfc062ba641dafe57455afe5be189469" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected</source>
          <target state="translated">RFE 모델을 맞추고 선택한 수를 자동으로 조정</target>
        </trans-unit>
        <trans-unit id="a1f31e1eab4986ee8b13885092428f58b3a203e9" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected features.</source>
          <target state="translated">RFE 모델을 맞추고 선택한 기능의 수를 자동으로 조정하십시오.</target>
        </trans-unit>
        <trans-unit id="9813edcd03393c0187279f71f7b7e90f266a7ca4" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected</source>
          <target state="translated">RFE 모델을 선택한 다음 선택한 추정값에 기본 추정값을 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="2e888db086bdb6861e2b6c27ddb4959896a661ee" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected features.</source>
          <target state="translated">RFE 모델을 선택한 다음 선택된 기능에 기본 추정기를 적용하십시오.</target>
        </trans-unit>
        <trans-unit id="3e8e54a78ebc1bc30d8f6371731aa34e571e1cb7" translate="yes" xml:space="preserve">
          <source>Fit the SVM model according to the given training data.</source>
          <target state="translated">주어진 훈련 데이터에 따라 SVM 모델을 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="e094bb2f16b3f77c4f0ded4998dd5acef469f424" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer only once.</source>
          <target state="translated">SelectFromModel 메타 변환기를 한 번만 설치하십시오.</target>
        </trans-unit>
        <trans-unit id="4f2b5174d874ffda16493a72eb6b7d9e49692dfa" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer.</source>
          <target state="translated">SelectFromModel 메타 변환기를 설치하십시오.</target>
        </trans-unit>
        <trans-unit id="48df38063d696d0aeb8b0988a010338565cac757" translate="yes" xml:space="preserve">
          <source>Fit the calibrated model</source>
          <target state="translated">보정 된 모델 맞추기</target>
        </trans-unit>
        <trans-unit id="c57e47c49821e6802e2a740c9051d8c66c744ebb" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features or affinity matrix, and return cluster labels.</source>
          <target state="translated">특성 또는 어피 니티 매트릭스에서 클러스터링을 피팅하고 클러스터 레이블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d9687ab7d6ecd0d65550bf660d242c20bd8d898b" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features, or affinity matrix.</source>
          <target state="translated">기능 또는 선호도 매트릭스에서 클러스터링을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="548ac10033a88a7f2f90c29b16d9b1eac7c9baf8" translate="yes" xml:space="preserve">
          <source>Fit the data from X, and returns the embedded coordinates</source>
          <target state="translated">X의 데이터를 맞추고 포함 된 좌표를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="08758549856b6a0e509ce1e713cae02d47e4bedc" translate="yes" xml:space="preserve">
          <source>Fit the estimator.</source>
          <target state="translated">추정량을 맞추십시오.</target>
        </trans-unit>
        <trans-unit id="855887273459477b0845aa915278722b2bfdc95e" translate="yes" xml:space="preserve">
          <source>Fit the estimators.</source>
          <target state="translated">추정값을 맞추십시오.</target>
        </trans-unit>
        <trans-unit id="78a5f0fa7f9e8e8876e6f8fdd879d2a72169691e" translate="yes" xml:space="preserve">
          <source>Fit the gradient boosting model.</source>
          <target state="translated">그라디언트 부스팅 모델을 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="59e531372ee14a17e072823da12da9c216d03637" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features or distance matrix, and return cluster labels.</source>
          <target state="translated">특징 또는 거리 행렬에서 계층 적 군집을 맞추고 군집 레이블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="5f5e34ffa1bdd98c15becac765f254aeb2f0a093" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features, or distance matrix.</source>
          <target state="translated">피처 또는 거리 행렬에서 계층 적 클러스터링을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="6951b3b9c1e77870a95debb989a3776ac4c5d6a1" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering on the data</source>
          <target state="translated">데이터에 계층 적 군집화</target>
        </trans-unit>
        <trans-unit id="5b5563854bcbf3fe8e972427b6550f84e6f5e44a" translate="yes" xml:space="preserve">
          <source>Fit the imputer on X.</source>
          <target state="translated">임 펄터를 X에 끼 웁니다.</target>
        </trans-unit>
        <trans-unit id="f79ad350ada74918a25b6a18b9c98a44219aea81" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer and transform the given label sets</source>
          <target state="translated">라벨 세트 이진화기를 장착하고 주어진 라벨 세트를 변형</target>
        </trans-unit>
        <trans-unit id="93ac61c2b8893a8a8dc44af4d06cf2252851b7b2" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;</source>
          <target state="translated">레이블 세트 이진화를 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;맞추고 classes_를&lt;/a&gt; 저장 합니다.</target>
        </trans-unit>
        <trans-unit id="aff4cb7658d810f05c33be44cd22feded7d4bab1" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;code&gt;classes_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;classes_&lt;/code&gt; 세트를 저장하는 레이블 세트 이진화 기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="8fe48671e323549fd93560341a4a3c7b625e4c7d" translate="yes" xml:space="preserve">
          <source>Fit the model</source>
          <target state="translated">모델 맞추기</target>
        </trans-unit>
        <trans-unit id="4a0fb954f184570eb54f89782d45ff6b62de8f93" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터와 파라미터에 따라 모델을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="226810036d48f43519ac6362c835a5ebb75ac273" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data.</source>
          <target state="translated">주어진 훈련 데이터에 따라 모델을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="12dc3b0f35bd8c187f3d6ad72da28e55416ad4ec" translate="yes" xml:space="preserve">
          <source>Fit the model and recover the sources from X.</source>
          <target state="translated">모델을 맞추고 X에서 소스를 복구하십시오.</target>
        </trans-unit>
        <trans-unit id="71e9ee1734e2bbf9ac4fe07ac1b6c03c04237b08" translate="yes" xml:space="preserve">
          <source>Fit the model and transform with the final estimator</source>
          <target state="translated">모델을 맞추고 최종 추정기로 변환</target>
        </trans-unit>
        <trans-unit id="6b8aa0f161bb8a77dc4775ed1ee1fcfc2c406c82" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X and transform X.</source>
          <target state="translated">X의 데이터에서 모형을 적합시키고 X를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="0aa179622924cc08ee70a31764d01ab49bbf6bcd" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X.</source>
          <target state="translated">X의 데이터에서 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="09a244ac4f08853db4f27fa0f56ca2d8ae157c91" translate="yes" xml:space="preserve">
          <source>Fit the model to X.</source>
          <target state="translated">모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="d8fc33348e2baabb167cc4df9e594abe384c1eb2" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target y.</source>
          <target state="translated">모델을 데이터 행렬 X에 맞추고 y를 목표로합니다.</target>
        </trans-unit>
        <trans-unit id="a08fe1d5397b99194d7ea288b13912038b863a22" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target(s) y.</source>
          <target state="translated">모델을 데이터 행렬 X 및 목표 y에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="5260da8ec9e88ebba64d6137961aeb8e84b7f391" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and targets Y.</source>
          <target state="translated">모델을 데이터 매트릭스 X에 맞추고 Y를 목표로합니다.</target>
        </trans-unit>
        <trans-unit id="45ba17915e2e3031d1582da2d277da7f4931a4e1" translate="yes" xml:space="preserve">
          <source>Fit the model to data.</source>
          <target state="translated">모델을 데이터에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="afd434d49f80c8d01132ca406d4c79b805654e96" translate="yes" xml:space="preserve">
          <source>Fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">모델을 데이터에 맞 춥니 다. 각 출력 변수에 대해 별도의 모델을 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="cfdc299b35aa8a48c0af501e27ae7cce65e8f528" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X which should contain a partial segment of the data.</source>
          <target state="translated">데이터의 일부 세그먼트를 포함해야하는 데이터 X에 모델을 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="b93160749fdae6a7fa042ecb31b3787d54faa056" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X.</source>
          <target state="translated">모델을 데이터 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="22b95bc22aacaf67cf593094b402a5ffdf9c9f1a" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data</source>
          <target state="translated">X를 훈련 데이터로 사용하여 모형 적합</target>
        </trans-unit>
        <trans-unit id="c8f2bc6ec471b46a95d660913e3db5351ccdd413" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data and y as target values</source>
          <target state="translated">X를 훈련 데이터로 사용하고 y를 목표 값으로 사용하여 모형을 적합</target>
        </trans-unit>
        <trans-unit id="67d84010e7f659892bf337ed29c5877ed8354203" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data.</source>
          <target state="translated">X를 훈련 데이터로 사용하여 모델을 적합시킵니다.</target>
        </trans-unit>
        <trans-unit id="3b6c346e2454cc9e9a61a017fe7c431d552c8c5b" translate="yes" xml:space="preserve">
          <source>Fit the model using X, y as training data.</source>
          <target state="translated">훈련 데이터로 X, y를 사용하여 모델을 적합시킵니다.</target>
        </trans-unit>
        <trans-unit id="5c7f073bf35ac36015511e86a701acdf9d1f18c6" translate="yes" xml:space="preserve">
          <source>Fit the model with X and apply the dimensionality reduction on X.</source>
          <target state="translated">X에 모형을 맞추고 X에 치수 축소를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ea7ce544f3a486e6f2a49538bf4fe64bf5a8afde" translate="yes" xml:space="preserve">
          <source>Fit the model with X, using minibatches of size batch_size.</source>
          <target state="translated">batch_size 크기의 미니 배치를 사용하여 모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="e7967bf329b3c1f757d75d92e374951929321ba7" translate="yes" xml:space="preserve">
          <source>Fit the model with X.</source>
          <target state="translated">모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="a5abbc09518b549e779e08db2e6e49f0ba32533d" translate="yes" xml:space="preserve">
          <source>Fit the random classifier.</source>
          <target state="translated">랜덤 분류기를 피팅하십시오.</target>
        </trans-unit>
        <trans-unit id="30f7575e3bac8aaa2852370b548ad3f04b290586" translate="yes" xml:space="preserve">
          <source>Fit the random regressor.</source>
          <target state="translated">랜덤 회귀 분석기를 적합합니다.</target>
        </trans-unit>
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">릿지 분류기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="1e4446f620ec40ca22b46eba2b6f8ce6be3c57e2" translate="yes" xml:space="preserve">
          <source>Fit the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 축소 공분산 모델을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">변압기를 X에 끼 웁니다.</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">ARD를 사용하여 회귀 모형의 가중치를 맞추십시오. 회귀 모형의 가중치는 가우스 분포에 있다고 가정합니다. 또한 람다 (가중 분포의 정밀도) 및 알파 (소음 분포의 정밀도)를 모수합니다. 추정은 반복 절차에 의해 수행됩니다 (증거 최대화).</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">데이터에 맞추고 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">X를 확인하여 변압기를 장착하십시오.</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">기본 견적에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">FastMCD 알고리즘으로 최소 공분산 결정자를 적합시킵니다.</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">모든 변환을 차례로 맞추고 데이터를 변환 한 다음 변환 된 데이터에 대해 최종 추정기로 fit_transform을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">GraphicalLasso 공분산 모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">GraphicalLasso 모델을 X에 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 Ledoit-Wolf 축소 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 학습 데이터 및 매개 변수에 따라 최대 우도 추정량 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 교육 데이터 및 매개 변수에 따라 Oracle Approximating Shrinkage 공분산 모델에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">추정기에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="3cea2475743e34d34ded723467b2a735e1203cfc" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return self.</source>
          <target state="translated">X에 대치자를 맞추고 자기를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="6bb471485459c492057aae581eaa0cf9c4592a1d" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return the transformed X.</source>
          <target state="translated">대치자를 X에 맞추고 변환 된 X를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="c13921c2e1a959ce7d05648d804f502aded299cf" translate="yes" xml:space="preserve">
          <source>Fits the model to the training set X and returns the labels.</source>
          <target state="translated">모델을 훈련 세트 X에 맞추고 레이블을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">주어진 훈련 데이터 및 매개 변수에 따라 수축 공분산 모형을 적합합니다.</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">선택적 매개 변수 fit_params를 사용하여 변환기를 X와 y에 맞추고 변환 된 버전의 X를 리턴합니다.</target>
        </trans-unit>
        <trans-unit id="06f59e2a32c90e6aa8aff9dfb100b6f03f2af9ff" translate="yes" xml:space="preserve">
          <source>Fitted classifier or a fitted &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; in which the last estimator is a classifier.</source>
          <target state="translated">마지막 추정자가 분류 자인 피팅 된 분류기 또는 피팅 된 &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e0dea44d64bdac3b47bf2c46576cad767f74d843" translate="yes" xml:space="preserve">
          <source>Fitted estimator.</source>
          <target state="translated">적합 추정량.</target>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">적합 회귀 기.</target>
        </trans-unit>
        <trans-unit id="f162f6bcf340dc521df64a1aae70440e61c389db" translate="yes" xml:space="preserve">
          <source>Fitted scaler.</source>
          <target state="translated">장착 된 스케일러.</target>
        </trans-unit>
        <trans-unit id="bccb0ee0062e477c480530940a0aca37551f6020" translate="yes" xml:space="preserve">
          <source>Fitted vectorizer.</source>
          <target state="translated">장착 된 벡터 라이저.</target>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">피팅 변압기는 계산 비용이 많이들 수 있습니다. 그것으로 &lt;code&gt;memory&lt;/code&gt; 파라미터 세트, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 호출 한 후 각 변압기를 캐시합니다 &lt;code&gt;fit&lt;/code&gt; . 이 기능은 파라미터와 입력 데이터가 동일한 경우 파이프 라인 내에서 적합한 변압기를 계산하지 않도록하는 데 사용됩니다. 일반적인 예는 변압기를 한 번만 장착하고 각 구성에 재사용 할 수있는 계통 검색의 경우입니다.</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">각 알파에 해당하는 교차 유효성 검사 값을 &lt;code&gt;cv_values_&lt;/code&gt; 특성에 저장해야하는지 여부를 나타내는 플래그입니다 (아래 참조). 이 플래그는 &lt;code&gt;cv=None&lt;/code&gt; 과만 호환됩니다 (즉, Generalized Cross-Validation 사용).</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">일반화 교차 검증을 수행 할 때 사용할 전략을 나타내는 플래그입니다. 옵션은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">밀도 추정에 적합한 평면 형상</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">작업자 프로세스와의 통신을위한 유연한 산 세척 제어.</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">플로 차트</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">작업자 프로세스와 메모리를 공유하기 위해 대형 어레이를 챙겨 내기 위해 풀에서 사용할 폴더입니다. None이면 순서대로 시도합니다.</target>
        </trans-unit>
        <trans-unit id="c09a9bf27e9aabfc7b35dc309da81eb816c5e989" translate="yes" xml:space="preserve">
          <source>Following special cases exist,</source>
          <target state="translated">다음과 같은 특별한 경우가 있습니다.</target>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">랜덤 한 구조 찾기의 알고리즘 4.3 : 근사 행렬 분해 구성을위한 확률 적 알고리즘 Halko, et al., 2009 (arXiv : 909) http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="061a7a165d75c1efa21f2a2a8a850415a95a5911" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="translated">무작위로 구조 찾기의 알고리즘 4.3을 따릅니다 : 근사 행렬 분해를 구성하기위한 확률 알고리즘 Halko, et al., 2009 (arXiv : 909) https://arxiv.org/pdf/0909.4061.pdf</target>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="871453ce5a358112246d8fa103eff55b515e95a2" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;(n_classes, n_features)&lt;/code&gt; and &lt;code&gt;(n_classes,)&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_classes&lt;/code&gt; &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">&quot;one-vs-rest&quot; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 의&lt;/a&gt; 경우 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 속성 은 각각 &lt;code&gt;(n_classes, n_features)&lt;/code&gt; 및 &lt;code&gt;(n_classes,)&lt;/code&gt; ) 모양을 갖습니다 . 계수의 각 행은 &lt;code&gt;n_classes&lt;/code&gt; &quot;one-vs-rest&quot;분류기 중 하나에 해당하며 &quot;one&quot;클래스의 순서로 인터셉트에 대해 유사합니다.</target>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">&quot;one-vs-rest&quot; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; 의&lt;/a&gt; 경우 &lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 속성 은 각각 &lt;code&gt;[n_class, n_features]&lt;/code&gt; 및 &lt;code&gt;[n_class]&lt;/code&gt; 의 모양을 갖습니다 . 계수의 각 행은 &lt;code&gt;n_class&lt;/code&gt; 많은&amp;ldquo;one-vs-rest&amp;rdquo;분류기 중 하나에 해당하며&amp;ldquo;one&amp;rdquo;클래스의 순서로 인터셉트에 대해 유사합니다.</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">추정기 또는 예측이 아닌 &lt;em&gt;샘플&lt;/em&gt; 간의 &quot;쌍별&quot;메트릭에 대해서는 &lt;a href=&quot;metrics#metrics&quot;&gt;쌍별 메트릭, 친화도 및 커널&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">들어 &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; (예 : 랜덤 포레스트, GBT, ExtraTrees 등) 나무의 나무의 수와 깊이는 가장 중요한 역할을한다. 지연 시간과 처리량은 트리 수에 따라 선형으로 확장되어야합니다. 이 경우 &lt;code&gt;n_estimators&lt;/code&gt; 의 n_estimators 매개 변수를 직접 사용 &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">들면 &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt; (예 올가미 ElasticNet, SGDClassifier / 회귀 변수, 릿지 및 RidgeClassifier, PassiveAggressiveClassifier / 회귀 변수, LinearSVC, 로지스틱 회귀 ...)의 예측시에 동일한 (내적)을 적용한 결정 함수이므로 지연 동일해야 .</target>
        </trans-unit>
        <trans-unit id="b1a84d28126765870388f5c6f8149674d42fd858" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, note that the output of the &lt;code&gt;estimators&lt;/code&gt; is controlled by the parameter &lt;code&gt;stack_method&lt;/code&gt; and it is called by each estimator. This parameter is either a string, being estimator method names, or &lt;code&gt;'auto'&lt;/code&gt; which will automatically identify an available method depending on the availability, tested in the order of preference: &lt;code&gt;predict_proba&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">들면 &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt; 상기 출력 참고 &lt;code&gt;estimators&lt;/code&gt; 파라미터에 의해 제어된다 &lt;code&gt;stack_method&lt;/code&gt; 하고 각 추정기에 의해 호출된다. 이 매개 변수는 추정기 메소드 이름 인 문자열이거나 선호도에 따라 사용 가능한 메소드를 자동으로 식별하는 &lt;code&gt;'auto'&lt;/code&gt; 이며, 선호도 순서에 따라 테스트됩니다 : &lt;code&gt;predict_proba&lt;/code&gt; , &lt;code&gt;decision_function&lt;/code&gt; 및 &lt;code&gt;predict&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d5bf64c1d60fc523cf9cf35c71f5018653089c51" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, when using &lt;code&gt;stack_method_='predict_proba'&lt;/code&gt;, the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear.</source>
          <target state="translated">용 &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt; , 사용시 &lt;code&gt;stack_method_='predict_proba'&lt;/code&gt; 문제 이진 분류 문제 때 첫 번째 열이 감소한다. 실제로 각 추정기에 의해 예측 된 두 확률 열은 완벽하게 공 선적입니다.</target>
        </trans-unit>
        <trans-unit id="6d0c4acadc1a3e244ad80f139e9b2149c5787665" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">용 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; (및 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; )을 NumPy와 배열에 복사되고 변환되는 바와 같이 전달 된 입력 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 내부 희소 데이터 표현 (배정도 바늘 및 비제로 성분 INT32 지수). 밀도가 높은 C 연속 배정 밀도 배열을 입력으로 복사하지 않고 대규모 선형 분류기를 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 대신 SGDClassifier 클래스 를 사용하는 것이 좋습니다 . 목적 함수는 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 모델 과 거의 동일하도록 구성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">용 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; (및 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; )을 NumPy와 배열 복사하여 liblinear 내부 희소 데이터 표현 (배정도 바늘 및 비제로 성분 INT32 인덱스)으로 변환되는 바와 같이 전달 된 입력. 밀도가 높은 numpy C 연속 배정 밀도 배열을 입력으로 복사하지 않고 대규모 선형 분류 기준을 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 려면 SGDClassifier 클래스를 대신 사용하는 것이 좋습니다 . 목적 함수는 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 모델 과 거의 동일하게 구성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2643eb1343d14a8cde9753d0546e2bd56743d7c1" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, the &amp;lsquo;recursion&amp;rsquo; method (used by default) will not account for the &lt;code&gt;init&lt;/code&gt; predictor of the boosting process. In practice, this will produce the same values as &amp;lsquo;brute&amp;rsquo; up to a constant offset in the target response, provided that &lt;code&gt;init&lt;/code&gt; is a constant estimator (which is the default). However, if &lt;code&gt;init&lt;/code&gt; is not a constant estimator, the partial dependence values are incorrect for &amp;lsquo;recursion&amp;rsquo; because the offset will be sample-dependent. It is preferable to use the &amp;lsquo;brute&amp;rsquo; method. Note that this only applies to &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, not to &lt;a href=&quot;sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">들어 &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; (기본적으로 사용)이 '재귀'방법은 고려하지 않습니다 &lt;code&gt;init&lt;/code&gt; 부스팅 프로세스의 예측. 실제로 이것은 &lt;code&gt;init&lt;/code&gt; 가 상수 추정기 (기본값) 인 경우 대상 응답의 상수 오프셋까지 'brute'와 동일한 값을 생성합니다 . 그러나 &lt;code&gt;init&lt;/code&gt; 가 상수 추정기가 아닌 경우 오프셋이 샘플에 따라 다르기 때문에 '재귀'에 대한 부분 종속성 값이 올바르지 않습니다. 'brute'방법을 사용하는 것이 좋습니다. 이것은 &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 에만 적용 되며&lt;a href=&quot;sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt; &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; &lt;/a&gt; 및&lt;a href=&quot;sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="f9c95b352fd6ddba1a98d2caa12d747735ff41c3" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;0 &amp;lt; power &amp;lt; 1&lt;/code&gt;, no distribution exists.</source>
          <target state="translated">를 들어 &lt;code&gt;0 &amp;lt; power &amp;lt; 1&lt;/code&gt; , 어떤 분포는 존재하지 않는다.</target>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">들어 &lt;code&gt;make_classification&lt;/code&gt; , 세 진 두 개의 다중 클래스 분류 데이터 세트는 정보 기능과 클래스 당 클러스터의 다른 번호로 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">가우시안 분포 데이터의 경우, 분포 모드까지 관측치 \ (x_i \)의 거리는 다음의 Mahalanobis 거리를 사용하여 계산할 수 있습니다. \ (d _ {(\ mu, \ Sigma)} (x_i) ^ 2 = (x_i- \ mu) '\ Sigma ^ {-1} (x_i-\ mu) \) 여기서 \ (\ mu \) 및 \ (\ Sigma \)는 기본 가우시안 분포의 위치 및 공분산입니다.</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">\ (k \) 클러스터의 경우 Calinski-Harabaz 점수 \ (s \)는 클러스터 간 분산 평균과 클러스터 내 분산의 비율로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">분류 모델의 경우 X의 각 샘플에 대해 예측 된 클래스가 반환됩니다. 회귀 모형의 경우 X를 기반으로 예측 된 값이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">다양한 스케일러, 변압기 및 노멀 라이저를 비교하려면 &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">완전한 확률 모델을 위해서는 잠재 변수 \ (h \)에 대한 사전 분포도 필요합니다. 가장 간단한 가정 (가우시안 분포의 훌륭한 특성에 기초)은 \ (h \ sim \ mathcal {N} (0, \ mathbf {I}) \)입니다. 이것은 \ (x \)의 한계 분포로 가우스를 산출합니다.</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">일정한 학습 속도를 위해 &lt;code&gt;learning_rate='constant'&lt;/code&gt; 를 사용하고 &lt;code&gt;eta0&lt;/code&gt; 을 사용하여 학습 속도를 지정하십시오.</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">사용 된 알고리즘의 구현 및 세부 사항에 대한 설명은</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">이러한 알고리즘에 대한 논의 및 비교는 &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;매니 폴드 모듈 페이지를 참조하십시오.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">여러 주제에서 생성 된 문서의 경우 모든 주제는 단어 모음을 생성 할 때 동일하게 가중치가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">최고의 biclusters의 경우, 가장 일반적인 문서 범주와 10 개의 가장 중요한 단어가 인쇄됩니다. 최고의 biclusters는 정규화 된 컷에 의해 결정됩니다. 가장 좋은 단어는 bicluster 내부와 외부의 합계를 비교하여 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt; 를 사용하여 사용자 지정 기능을 선택 하는 방법을 보여주는 전체 코드 예제는 FunctionTransformer 를 &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;사용하여 열 선택을&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">텍스트 분류 작업에서 코어 외 스케일링의 완전한 예 &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;는 텍스트 문서의 코어 외 분류를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">주어진 &lt;code&gt;n_components&lt;/code&gt; 값에 대해 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 는 종종 Nystroem 보다 정확도가 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 는 계산 비용이 저렴하지만 더 큰 피처 공간을보다 효율적으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">알파를 &lt;a href=&quot;linear_model#lasso&quot;&gt;적절히&lt;/a&gt; 선택 하기 위해 특정 조건을 충족 하는 경우 Lasso 는 관측치가 거의없는 정확한 0이 아닌 변수 세트를 완전히 복구 할 수 있습니다. 특히, 샘플 수는 &quot;충분히 많&quot;거나 L1 모델이 임의로 수행됩니다. 여기서 &quot;충분히 큰&quot;은 0이 아닌 계수의 수, 피처 수의 로그, 노이즈의 양, 0이 아닌 계수의 최소 절대 값 및 설계 행렬 X의 구조. 또한 설계 행렬은 상관되지 않은 특정 특성을 표시해야합니다.</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">N 클래스의 다중 레이블 분류 문제의 경우 N 이진 분류 자에 0과 N-1 사이의 정수가 할당됩니다. 이 정수는 체인의 모델 순서를 정의합니다. 그런 다음 각 분류자는 사용 가능한 교육 데이터와 모델에 더 낮은 번호가 할당 된 클래스의 실제 레이블에 맞습니다.</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">multi_class 문제의 경우 multi_class가 &quot;다항식&quot;으로 설정된 경우 softmax 함수를 사용하여 각 클래스의 예측 확률을 찾습니다. 그렇지 않으면 일대일 접근 방식을 사용하십시오. 즉, 로지스틱 함수를 사용하여 각 클래스가 양수라고 가정 할 때 각 클래스의 확률을 계산하십시오. 모든 클래스에서이 값을 정규화합니다.</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">멀티 클래스 문제의 경우 각 클래스의 하이퍼 파라미터는 모든 폴드와 클래스에서 병렬로 1 대 1을 수행하여 얻은 최고 점수를 사용하여 계산됩니다. 따라서 이것이 진정한 다항식 손실이 아닙니다.</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">루트로 들어가는 새로운 점의 경우, 가장 가까운 서브 클러스터와 병합되고 선형 합, 제곱합 및 해당 서브 클러스터의 샘플 수가 업데이트됩니다. 리프 노드의 속성이 업데이트 될 때까지 재귀 적으로 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="14657655d860195eca6994d15ac16d17936616a4" translate="yes" xml:space="preserve">
          <source>For a one-class model, +1 or -1 is returned.</source>
          <target state="translated">단일 클래스 모델의 경우 +1 또는 -1이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="49c44661048e1cd2101f0d885d98fc8331d85aee" translate="yes" xml:space="preserve">
          <source>For a set of data \(E\) of size \(n_E\) which has been clustered into \(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">\ (k \) 클러스터로 클러스터링 된 \ (n_E \) 크기의 데이터 세트 \ (E \)의 경우 Calinski-Harabasz 점수 \ (s \)는 클러스터 간 분산 비율로 정의됩니다. 평균 및 클러스터 내 분산 :</target>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">방법이 구체 데이터 세트에 적용되는 유사한 예 &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;는 잘린 구체의 매니 폴드 학습 방법을&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">방법이 S- 곡선 데이터 세트에 적용되는 유사한 예 &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;는 매니 폴드 학습 방법 비교를&lt;/a&gt; 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">적응 적으로 감소하는 학습 속도의 경우 &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; 를 사용하고 &lt;code&gt;eta0&lt;/code&gt; 을 사용하여 시작 학습 속도를 지정하십시오. 정지 기준에 도달하면 학습 속도는 5로 나눠지고 알고리즘은 중단되지 않습니다. 학습 속도가 1e-6 아래로 떨어지면 알고리즘이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">추정기가 효과적이기 위해서는 이웃 지점 사이의 거리가 문제에 따라 \ (d \) 값보다 작아야합니다. 한 차원에서 평균 \ (n \ sim 1 / d \) 포인트가 필요합니다. 위의 \ (k \)-NN 예와 관련하여, 데이터가 0에서 1 사이의 값을 갖는 하나의 특징과 \ (n \) 훈련 관측치에 의해 기술된다면, 새로운 데이터는 더 이상 멀지 않을 것입니다 \ (1 / n \). 따라서 클래스 간 기능 변동의 규모에 비해 \ (1 / n \)이 작 으면 가장 가까운 이웃 결정 규칙이 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">scikit-learn과 함께이 데이터 세트를 사용하는 예는 &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">예를 들어 &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.py를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">일반적으로 유니 코드 및 문자 인코딩에 대한 소개는 Joel Spolsky의 &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;절대 모든 소프트웨어 개발자가 유니 코드에 대해 알아야 할 절대적인&lt;/a&gt; 내용을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="28d13aab6e72bcde5f37b4278086b064720820de" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;https://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">일반적으로 유니 코드 및 문자 인코딩에 대한 소개는 Joel Spolsky의 &lt;a href=&quot;https://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;모든 소프트웨어 개발자가 유니 코드에 대해 알아야하는 절대 최소값을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">1 클래스 모델의 경우 +1 (inlier) 또는 -1 (outlier)이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">1 클래스 모델의 경우 +1 또는 -1이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">scikit-learn에서 사용 가능한 전략에 대한 개요는 &lt;a href=&quot;computing#scaling-strategies&quot;&gt;핵심 학습&lt;/a&gt; 자료 도 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">실제 레이블이 \ (y \ in \ {0,1 \} \)이고 확률 추정이 \ (p = \ operatorname {Pr} (y = 1) \) 인 이진 분류의 경우 샘플 당 로그 손실은 음수입니다. 실제 레이블이 지정된 분류기의 로그 가능성 :</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">이진 분류의 경우 \ (f (x) \)는 로지스틱 함수 \ (g (z) = 1 / (1 + e ^ {-z}) \)를 통과하여 0과 1 사이의 출력 값을 얻습니다. 0.5로 설정된 임계 값은 0.5보다 크거나 같은 출력 샘플을 양수 클래스에 할당하고 나머지는 음수 클래스에 할당합니다.</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">이진 문제의 경우 다음과 같이 진 음성, 오 탐지,가 음성 및 진 양성 카운트를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">&lt;code&gt;loss='deviance'&lt;/code&gt; 인 분류 의 경우 대상 응답은 logit (p)입니다.</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">로지스틱 손실이있는 분류의 경우, 평균 전략을 사용하는 SGD의 또 다른 변형이 SG (Stochastic Average Gradient) 알고리즘으로 제공되며 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 에서 솔버로 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">분류의 경우 링크 함수 이전의 회귀 점수로 생각할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">분류의 경우 &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) 또는 &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II) 와 함께 사용할 수 있습니다 . 회귀의 경우 &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) 또는 &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II) 와 함께 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">분류의 경우, 중요하지 않은 특징 추출 루틴은 새로운 / 보이지 않는 속성에 대처할 수 있지만 증분 학습자 자체는 새로운 / 보이지 않은 대상 클래스에 대처하지 못할 수 있습니다. 이 경우 , &lt;code&gt;classes=&lt;/code&gt; 매개 변수를 사용하여 가능한 모든 클래스를 첫 번째 &lt;code&gt;partial_fit&lt;/code&gt; 호출 로 전달해야합니다 .</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">분류 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;홍채&lt;/a&gt; 작업 에서와 같이 분류의 경우 선형 회귀는 의사 결정 경계에서 멀리 떨어진 데이터에 너무 많은 가중치를 부여하므로 올바른 접근 방식이 아닙니다. 선형 접근법은 S 자형 함수 또는 &lt;strong&gt;로지스틱&lt;/strong&gt; 함수 에 적합 합니다.</target>
        </trans-unit>
        <trans-unit id="cda870024c615048609a38e867ca66fe1aaab764" translate="yes" xml:space="preserve">
          <source>For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function.</source>
          <target state="translated">분류의 경우 대상 응답은 클래스 (이진 분류의 포지티브 클래스) 또는 결정 함수의 확률 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">: 분류 &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">비교를 위해, 임의의 코드북 (임의로 픽업 된 컬러)을 사용한 양자화 된 이미지가 또한 도시되어있다.</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">비교를 위해 MiniBatchKMeans를 사용하여 문서도 클러스터됩니다. biclusters에서 파생 된 문서 클러스터는 MiniBatchKMeans에서 찾은 클러스터보다 더 나은 V 측정을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="9da7ddf96abc62edc6f61720c2e84583fe9d6b7c" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">비교를 위해 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt; 의 출력도 추가합니다 . 충분한 훈련 샘플 (수천 개)이있는 경우 임의의 분포를 가우스로 강제 할 수 있습니다. 비모수 적 방법이기 때문에 모수 적 방법 (Box-Cox 및 Yeo-Johnson)보다 해석하기가 더 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">비교를 위해 &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; 의 출력도 추가합니다 . 충분한 훈련 샘플 (수천)이있는 경우 가우스로 임의의 분포를 강요 할 수 있습니다. 비모수 적 방법이므로 모수 적 방법 (Box-Cox 및 Yeo-Johnson)보다 해석하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">호환성을 위해이 메서드를 사용하는 사용자 코드는 형식 문제를 피하기 위해 &lt;code&gt;np.asarray&lt;/code&gt; 에서 해당 호출을 래핑해야 합니다.</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">위의 &lt;code&gt;C&lt;/code&gt; 와 같은 연속 모수 의 경우 랜덤 분포를 최대한 활용하려면 연속 분포를 지정하는 것이 중요합니다. 이런 식으로 &lt;code&gt;n_iter&lt;/code&gt; 를 늘리면 항상 더 정밀한 검색으로 이어집니다.</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">교차 검증을 위해 LassoCV 클래스에 의해 구현 된 좌표 하강 및 LassoLarsCV 클래스에 의해 구현 된 Lars (최소 각도 회귀)와 같은 올가미 경로를 계산하기 위해 2 가지 알고리즘과 함께 20 배의 알고리즘을 사용합니다. 두 알고리즘 모두 대략 동일한 결과를 제공합니다. 실행 속도와 수치 오류의 원인에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">메시지 문자열에 &quot;% (name) s&quot;가 있으면 사용자 지정 메시지의 경우 견적 자 이름으로 대체됩니다.</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">제공된 커널 함수의 정확한 수학적 공식과 &lt;code&gt;gamma&lt;/code&gt; , &lt;code&gt;coef0&lt;/code&gt; 및 &lt;code&gt;degree&lt;/code&gt; 가 서로 어떻게 영향을 미치는지에 대한 자세한 내용은 설명 설명서에서 해당 섹션을 참조하십시오 : &lt;a href=&quot;../svm#svm-kernels&quot;&gt;커널 함수&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="885e0e96d0439bc0a3432bfe3535963e5dbd93a5" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape (n_features, n_k), where &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to &lt;code&gt;V&lt;/code&gt;, the matrix of eigenvectors coming from the SVD of &lt;code&gt;Xk = U S Vt&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="translated">각 클래스 k에 대해 모양의 배열 (n_features, n_k), 여기서 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 가우스 분포의 회전, 즉 주축입니다. 이는 &lt;code&gt;Xk = U S Vt&lt;/code&gt; 의 SVD에서 나오는 고유 벡터의 행렬 인 &lt;code&gt;V&lt;/code&gt; 에 해당합니다. 여기서 &lt;code&gt;Xk&lt;/code&gt; 는 클래스 k의 샘플의 중심 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">각 클래스 k에 대해 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 인 형태 [n_features, n_k]의 배열은 가우스 분포의 회전, 즉 주축입니다.</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">각 클래스 k에 대해 모양 [n_k]의 배열. 여기에는 주축을 따라 가우스 분포의 스케일링, 즉 회전 좌표계의 분산이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">각 모델 클래스에 대해 관련 모델 매개 변수를 선택하여 모델 복잡성을 변경하고 계산 성능 (대기 시간)과 예측 전력 (MSE 또는 해밍 손실)에 미치는 영향을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="21f3c14e6817e5773b09bd85149800940dffa132" translate="yes" xml:space="preserve">
          <source>For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. It corresponds to &lt;code&gt;S^2 /
(n_samples - 1)&lt;/code&gt;, where &lt;code&gt;S&lt;/code&gt; is the diagonal matrix of singular values from the SVD of &lt;code&gt;Xk&lt;/code&gt;, where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="translated">각 클래스에 대해 기본 축을 따라 가우스 분포의 스케일링, 즉 회전 된 좌표계의 분산이 포함됩니다. 이것은에 대응 &lt;code&gt;S^2 / (n_samples - 1)&lt;/code&gt; , &lt;code&gt;S&lt;/code&gt; 는 의 SVD에서 특이 값의 대각 행렬이다 &lt;code&gt;Xk&lt;/code&gt; , &lt;code&gt;Xk&lt;/code&gt; 클래스 K에서 샘플 중심 행렬이다.</target>
        </trans-unit>
        <trans-unit id="93f968263bf15ebdfa5ca6b61b7631f18bb2dedd" translate="yes" xml:space="preserve">
          <source>For each class, gives the covariance matrix estimated using the samples of that class. The estimations are unbiased. Only present if &lt;code&gt;store_covariance&lt;/code&gt; is True.</source>
          <target state="translated">각 클래스에 대해 해당 클래스의 샘플을 사용하여 추정 된 공분산 행렬을 제공합니다. 추정치는 편향되지 않습니다. &lt;code&gt;store_covariance&lt;/code&gt; 가 True 인 경우에만 존재합니다 .</target>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">각 성분 k에 대해 최대 corr (Xk u, Yk v)를 최대화하는 가중치 u, v를 찾고 &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">각 성분 k에 대해 다음을 최적화하는 가중치 u, v를 찾으십시오.</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">각 성분 k에 대해 다음을 최적화하는 가중치 u, v를 찾으십시오. &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; , &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">X의 각 데이터 포인트 x와 앙상블의 각 트리에 대해 리프 x의 인덱스가 각 추정기에서 끝납니다.</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">X의 각 데이터 포인트 x와 앙상블의 각 트리에 대해 리프 x의 인덱스가 각 추정기에서 끝납니다. 이진 분류의 경우 n_classes는 1입니다.</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">X의 각 데이터 포인트 x와 포리스트의 각 트리에 대해 리프 x의 인덱스가 끝납니다.</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">X의 각 데이터 포인트 x에 대해 리프 x의 인덱스가 끝납니다. 리프는 &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; , 번호 매기기에 공백이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">각 데이터 세트에 대해 샘플의 15 %가 랜덤 균일 노이즈로 생성됩니다. 이 비율은 OneClassSVM의 nu 매개 변수와 다른 이상치 탐지 알고리즘의 오염 매개 변수에 주어진 값입니다. 특이 치 탐지에 사용될 때 새 데이터에 적용 할 예측 방법이 없으므로 LOF (Local Outlier Factor)를 제외하고 특이 치와 특이 치 간의 결정 경계가 검은 색으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">각 문서 &lt;code&gt;#i&lt;/code&gt; 에 대해 각 단어 &lt;code&gt;w&lt;/code&gt; 의 발생 횟수를 세고 &lt;code&gt;X[i, j]&lt;/code&gt; 에 기능 &lt;code&gt;#j&lt;/code&gt; 의 값으로 저장하십시오. 여기서 &lt;code&gt;j&lt;/code&gt; 는 단어 &lt;code&gt;w&lt;/code&gt; 의 색인입니다. 사전에서 .</target>
        </trans-unit>
        <trans-unit id="086df8ffb1b70734dd37b88cb050e6a142a873b8" translate="yes" xml:space="preserve">
          <source>For each document \(d \in D\), draw the topic proportions \(\theta_d \sim \mathrm{Dirichlet}(\alpha)\). \(\alpha\) corresponds to &lt;code&gt;doc_topic_prior&lt;/code&gt;.</source>
          <target state="translated">각 문서 \ (d \ in D \)에 대해 주제 비율 \ (\ theta_d \ sim \ mathrm {Dirichlet} (\ alpha) \)를 그립니다. \ (\ alpha \)는 &lt;code&gt;doc_topic_prior&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">각 문서 \ (d \)에 대해 \ (\ theta_d \ sim \ mathrm {Dirichlet} (\ alpha), \ : d = 1 ... D \)를 그립니다.</target>
        </trans-unit>
        <trans-unit id="6eb16a2ee99fe65f3a14878a1546f940e1e0bd2a" translate="yes" xml:space="preserve">
          <source>For each feature \(i\) in the training set \(X\), &lt;a href=&quot;generated/sklearn.naive_bayes.categoricalnb#sklearn.naive_bayes.CategoricalNB&quot;&gt;&lt;code&gt;CategoricalNB&lt;/code&gt;&lt;/a&gt; estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \(J = \{ 1, \dots, m \}\), with \(m\) as the number of samples.</source>
          <target state="translated">훈련 세트 \ (X \)의 각 기능 \ (i \)에 대해 &lt;a href=&quot;generated/sklearn.naive_bayes.categoricalnb#sklearn.naive_bayes.CategoricalNB&quot;&gt; &lt;code&gt;CategoricalNB&lt;/code&gt; &lt;/a&gt; 는 클래스 y에 조건이 지정된 X의 각 기능 i에 대한 범주 분포를 추정합니다. 샘플의 인덱스 세트는 \ (J = \ {1, \ dots, m \} \)로 정의되며 \ (m \)은 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="9366e353cbe2cfb64854c09c8a8adb8df13a11b1" translate="yes" xml:space="preserve">
          <source>For each feature \(j\) (column of \(D\)):</source>
          <target state="translated">각 기능 \ (j \) (\ (D \) 열) :</target>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">각 관측치에 대해 적합 모형에 따라이 값을 내부로 간주해야하는지 (+1 또는 -1)를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">의사 결정 트리는 각 쌍의 홍채 특징에 대해 훈련 샘플에서 추론 된 간단한 임계 값 규칙의 조합으로 이루어진 결정 경계를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="63ed3328d4a6b49da02c095fda15b05ff5f99ccf" translate="yes" xml:space="preserve">
          <source>For each repetition \(k\) in \({1, ..., K}\):</source>
          <target state="translated">\ ({1, ..., K} \)의 각 반복 \ (k \)에 대해 :</target>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">각 샘플에 대해 생성 프로세스는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d05dbfe66f302738b6b31b31becd47c7eec37764" translate="yes" xml:space="preserve">
          <source>For each topic \(k \in K\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta)\). This provides a distribution over the words, i.e. the probability of a word appearing in topic \(k\). \(\eta\) corresponds to &lt;code&gt;topic_word_prior&lt;/code&gt;.</source>
          <target state="translated">각 주제 \ (k \ in K \)에 대해 \ (\ beta_k \ sim \ mathrm {Dirichlet} (\ eta) \)를 그립니다. 이것은 단어에 대한 분포, 즉 주제 \ (k \)에 단어가 나타날 확률을 제공합니다. \ (\ eta \)는 &lt;code&gt;topic_word_prior&lt;/code&gt; 에 해당합니다 .</target>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">각 주제 \ (k \)에 대해 \ (\ beta_k \ sim \ mathrm {Dirichlet} (\ eta), \ : k = 1 ... K \)</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; 의 각 값에 대해 을 플롯합니다.</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; 에서 '목표'피처의 각 값에 대해 부분 의존 함수는 '보완'피처의 모든 가능한 값에 대한 트리 예측을 주 변화해야합니다. 의사 결정 트리에서이 기능은 교육 데이터를 참조하지 않고도 효율적으로 평가할 수 있습니다. 각 그리드 포인트에 대해 가중 트리 순회가 수행됩니다. 분할 노드에 '대상'기능이 포함 된 경우 해당 왼쪽 또는 오른쪽 분기가 따르고, 그렇지 않으면 두 분기가 따라 가며 각 분기에는 입력 된 훈련 샘플의 비율이 가중됩니다. 분기. 마지막으로, 부분 의존성은 모든 방문 된 잎의 가중 평균에 의해 주어진다. 트리 앙상블의 경우 각 개별 트리의 결과가 다시 평균화됩니다.</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">문서 \ (d \)의 각 단어 \ (i \)에 대해 :</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">효율성을 위해 한 쌍의 행 벡터 x와 y 사이의 유클리드 거리는 다음과 같이 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">여러 메트릭을 평가하려면 (고유 한) 문자열 목록을 제공하거나 이름을 키로, 콜 러블을 값으로 사용하여 받아쓰기를하십시오.</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">예를 들어 &lt;code&gt;average_precision&lt;/code&gt; 또는 roc 곡선 아래 면적은 이산 예측 만 사용하여 계산할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">예를 들어 &lt;code&gt;SVC&lt;/code&gt; 대신 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">예를 들어 &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; 계산 집약적 인 대안 은 &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">예를 들어, &lt;strong&gt;다항식 피처&lt;/strong&gt; 를 구성하여 간단한 선형 회귀를 확장 할 수 있습니다.&lt;strong&gt;&lt;/strong&gt; 계수에서 를 . 표준 선형 회귀 분석의 경우 2 차원 데이터에 대해 다음과 같은 모형이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8a4469c2cb53cff6560ff1210e7df829ee4e673a" translate="yes" xml:space="preserve">
          <source>For example, classification of the properties &amp;ldquo;type of fruit&amp;rdquo; and &amp;ldquo;colour&amp;rdquo; for a set of images of fruit. The property &amp;ldquo;type of fruit&amp;rdquo; has the possible classes: &amp;ldquo;apple&amp;rdquo;, &amp;ldquo;pear&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. The property &amp;ldquo;colour&amp;rdquo; has the possible classes: &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property.</source>
          <target state="translated">예를 들어, 과일 이미지 세트에 대한 속성 &quot;과일 유형&quot;및 &quot;색상&quot;의 분류. &quot;과일 유형&quot;속성에는 &quot;사과&quot;, &quot;배&quot;및 &quot;주황색&quot;과 같은 가능한 클래스가 있습니다. &quot;색상&quot;속성에는 &quot;녹색&quot;, &quot;빨간색&quot;, &quot;노란색&quot;및 &quot;주황색&quot;과 같은 가능한 클래스가 있습니다. 각 샘플은 과일의 이미지이고 레이블은 두 속성 모두에 대해 출력되며 각 레이블은 해당 속성의 가능한 클래스 중 하나입니다.</target>
        </trans-unit>
        <trans-unit id="794a77a9ad99cd614e0490463df18e6667fa3c9c" translate="yes" xml:space="preserve">
          <source>For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labelled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple.</source>
          <target state="translated">예를 들어, 과일 이미지 세트에서 추출한 특징을 사용하는 분류입니다. 여기서 각 이미지는 오렌지, 사과 또는 배일 수 있습니다. 각 이미지는 하나의 샘플이며 세 가지 가능한 클래스 중 하나로 레이블이 지정됩니다. 다중 클래스 분류는 각 샘플이 하나의 레이블에만 할당되어 있다고 가정합니다. 예를 들어 하나의 샘플은 배와 사과가 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">예를 들어, 각 점이 단지 하나의 숫자 (8 바이트) 인 경우, 가계도 \ (p \ sim 20 \) 차원에서 유효한 \ (k \)-NN 추정기는 현재 추정 크기보다 더 많은 훈련 데이터를 필요로합니다. 전체 인터넷 (&amp;plusmn; 1000 엑사 바이트 정도)</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">예를 들어, 랜덤 프로젝션에서이 경고는 대상 프로젝션 공간의 차원을 정량화하는 구성 요소의 수가 원래 소스 공간의 차원을 정량화하는 피처의 수보다 높으면 차원을 암시 할 때 발생합니다. 문제의 감소되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">예를 들어 여러 실험의 경우 &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt; 을 사용하여 기반으로 교차 검증을 만들 수 있습니다. 하나를 제외한 모든 실험의 샘플을 사용하여 트레이닝 세트를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">예를 들어, 다항식 Naive Bayes 분류기의 결과를 살펴보면 빠르게 학습하고 적절한 F 점수를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">예를 들어 &lt;code&gt;['words', 'wprds']&lt;/code&gt; 두 문서의 모음을 처리한다고 가정 해 봅시다 . 두 번째 문서에는 단어 'words'의 철자가 틀립니다. 간단한 단어 표현은이 두 가지 기능이 서로 다른 두 가지 매우 다른 문서로 간주합니다. 그러나 문자 2 그램 표현은 8 가지 기능 중 4 가지 기능에서 일치하는 문서를 찾을 수 있으므로 선호하는 분류자가 더 잘 결정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a387381a97998b7238fd2bc704165170ce740115" translate="yes" xml:space="preserve">
          <source>For example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and direction would be output for each sample.</source>
          <target state="translated">예를 들어 특정 위치에서 얻은 데이터를 사용하여 풍속과 풍향을 모두도 단위로 예측합니다. 각 샘플은 한 위치에서 얻은 데이터이며 각 샘플에 대해 풍속과 방향이 모두 출력됩니다.</target>
        </trans-unit>
        <trans-unit id="0256c851cb28fec27d5dcefe2d74c45aece1b249" translate="yes" xml:space="preserve">
          <source>For example, prediction of the topics relevant to a text document or video. The document or video may be about one of &amp;lsquo;religion&amp;rsquo;, &amp;lsquo;politics&amp;rsquo;, &amp;lsquo;finance&amp;rsquo; or &amp;lsquo;education&amp;rsquo;, several of the topic classes or all of the topic classes.</source>
          <target state="translated">예를 들어, 텍스트 문서 또는 비디오와 관련된 주제 예측. 문서 또는 비디오는 '종교', '정치', '금융'또는 '교육'중 하나, 여러 주제 수업 또는 모든 주제 수업에 관한 것일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">예를 들어 시퀀스 분류기 (예 : 청커)를 훈련하기 위해 보완 태그로 사용하려는 품사 (PoS) 태그를 추출하는 첫 번째 알고리즘이 있다고 가정합니다. 다음 구절은 '고양이가 매트에 앉았다'는 문장에서 '토'라는 단어 주위에 추출 된 특징의 창일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5b46799167b8bb61c43e949f07a2335c6f7cd1fa" translate="yes" xml:space="preserve">
          <source>For example, the distance between &lt;code&gt;[3, na, na, 6]&lt;/code&gt; and &lt;code&gt;[1, na, 4, 5]&lt;/code&gt; is:</source>
          <target state="translated">예를 들어 &lt;code&gt;[3, na, na, 6]&lt;/code&gt; 과 &lt;code&gt;[1, na, 4, 5]&lt;/code&gt; 사이의 거리는 다음 과 같습니다.</target>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">예를 들어 다음 스 니펫은 &lt;code&gt;chardet&lt;/code&gt; (scikit-learn과 함께 제공되지 않고 별도로 설치해야 함)을 사용하여 세 개의 텍스트 인코딩을 계산합니다. 그런 다음 텍스트를 벡터화하고 학습 한 어휘를 인쇄합니다. 출력은 여기에 표시되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">예를 들어이 경고는 사용자가</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">예를 들어, 마우스 뇌에서 유전자 발현의 데이터 세트를 다운로드하려면 :</target>
        </trans-unit>
        <trans-unit id="0fb165a7680e316154f88ea288da16adb651003b" translate="yes" xml:space="preserve">
          <source>For example, to use &lt;code&gt;n_jobs&lt;/code&gt; greater than 1 in the example below, &lt;code&gt;custom_scoring_function&lt;/code&gt; function is saved in a user-created module (&lt;code&gt;custom_scorer_module.py&lt;/code&gt;) and imported:</source>
          <target state="translated">예를 들어 아래 예에서 1보다 큰 &lt;code&gt;n_jobs&lt;/code&gt; 를 사용하려면 &lt;code&gt;custom_scoring_function&lt;/code&gt; 함수를 사용자가 만든 모듈 ( &lt;code&gt;custom_scorer_module.py&lt;/code&gt; )에 저장 하고 가져 옵니다 .</target>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">예를 들어, &lt;code&gt;counts&lt;/code&gt; 배열 의 첫 번째 문서에서 첫 번째 항의 tf-idf 를 다음과 같이 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">예를 들어, 부울 기능을 처리 할 때 모든 \ (n \)에 대해 \ (x_i ^ n = x_i \)이므로 쓸모가 없습니다. 그러나 \ (x_i x_j \)는 두 부울의 연결을 나타냅니다. 이런 식으로 선형 분류기로 XOR 문제를 해결할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">예를 들어, 유효성 검증 세트를 사용하는 경우 유효성 검증 세트의 &lt;code&gt;test_fold&lt;/code&gt; 모든 샘플에 대해 test_fold 를 0으로 설정하고 다른 모든 샘플에 대해서는 -1로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">사용 방법에 대한 예는 아래 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="717701439dd44ea572c3fc98c89180aa00806795" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="translated">편향-분산 분해에 대한 자세한 내용은 섹션 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">바이어스-분산 분해에 대한 자세한 내용은 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]의&lt;/a&gt; 7.3 단원을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">자세한 내용은&amp;ldquo;t-SNE를 효과적으로 사용하는 방법&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; 은 다양한 매개 변수의 영향에 대한 자세한 설명과 이러한 효과를 탐색하기위한 대화식 그림을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="0482a5c448a6c0f244b48ce337c6748a1bc4ca45" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;https://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">자세한 내용은 &quot;t-SNE를 효과적으로 사용하는 방법&quot; &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;https://distill.pub/2016/misread-tsne/&lt;/a&gt; 에서 다양한 매개 변수의 효과에 대한 좋은 토론과 이러한 효과를 탐색 할 수있는 대화 형 플롯을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">손으로 쓴 숫자 인식과 같이 흰색 배경에서 픽셀 값을 흑도로 해석 할 수있는 그레이 스케일 이미지 데이터의 경우 Bernoulli Restricted Boltzmann 머신 모델 ( &lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt; )은 효과적인 비선형 피쳐 추출을 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0f8240ee0365c34e0de439585e58b6dfcdcc2ed3" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear features, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">많은 공 선적 특징이있는 고차원 데이터 세트의 경우 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 가 가장 선호됩니다. 그러나 &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수 의 더 관련성있는 값을 탐색하는 이점이 있으며, 샘플 수가 특성 수에 비해 매우 적 으면 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 보다 빠른 경우가 많습니다 .</target>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">공 선형 회귀자가 많은 고차원 데이터 세트의 경우 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 가 가장 선호됩니다. 그러나 &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt; 는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수 의 관련 값을 더 많이 탐색 할 수 있다는 장점이 있으며, 샘플 수가 기능 수에 비해 매우 적 으면 종종 &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt; 보다 빠릅니다 .</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">NIST 전처리 루틴에 대한 정보는 MD Garris, JL Blue, GT Candela, DL Dimmick, J. Geist, PJ Grother, SA Janet 및 CL Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">예를 들어, 전자 메일과 같은 10,000 개의 짧은 텍스트 문서 모음은 총 100,000 개의 고유 단어 순서의 크기를 가진 어휘를 사용하지만 각 문서는 100에서 1000 개의 고유 단어를 개별적으로 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">예를 들어, 학습 벡터 알고리즘의 목적 함수에 사용되는 많은 요소 (예 : Support Vector Machine의 RBF 커널 또는 선형 모델의 L1 및 L2 정규화 기)는 모든 기능이 0을 중심으로하고 동일한 순서로 분산되어 있다고 가정합니다. 특징이 다른 것보다 큰 차수의 변동을 갖는 경우, 목적 함수를 지배하고 추정기가 예상대로 다른 특징에서 올바르게 학습하지 못할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">예를 들어 아래 주어진 테이블</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">예를 들어, 그룹은 샘플을 수집 한 연도 일 수 있으므로 시간 기반 분할에 대한 교차 검증을 허용합니다.</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">예를 들어, 이너 데이터가 가우스 분포라고 가정하면, 이너 위치와 공분산은 강력한 방식으로 (즉, 이상치의 영향을받지 않음) 추정합니다. 이 추정으로부터 얻은 마할 라 노비스 거리는 외곽의 척도를 도출하는 데 사용됩니다. 이 전략은 아래에 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">예를 들어, 모양 행렬 &lt;code&gt;(10, 10)&lt;/code&gt; 주어지면 3 개의 행과 2 개의 열이있는 하나의 가능한 bicluster는 모양의 행렬 &lt;code&gt;(3, 2)&lt;/code&gt; 유도합니다 .</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">예를 들어, \ (p \) 특이 벡터가 계산 된 경우 \ (q &amp;lt;p \)에서 설명한대로 최적의 \ (q \)를 찾습니다. \ (U \)를 \ (q \) 가장 왼쪽에있는 특이한 단일 벡터 열과 오른쪽에 대해 \ (V \)가있는 행렬로 둡니다. 행을 분할하기 위해 \ (A \)의 행은 \ (q \) 차원 공간으로 투영됩니다 : \ (A * V \). 이 \ (m \ times q \) 행렬의 \ (m \) 행을 샘플로 취급하고 k- 평균을 사용하여 클러스터링하면 행 레이블이 생성됩니다. 마찬가지로 열을 \ (A ^ {\ top} * U \)에 투영하고이 \ (n \ times q \) 행렬을 클러스터하면 열 레이블이 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">예를 들어, 얼굴 인식을 위해 64x64 픽셀 회색 레벨 그림으로 작업하는 경우 데이터의 차원은 4096이며 이러한 넓은 데이터에 대해 RBF 지원 벡터 시스템을 학습하는 속도가 느립니다. 또한 우리는 인간 얼굴의 모든 그림이 다소 비슷하게 보이기 때문에 데이터의 고유 차원이 4096보다 훨씬 낮다는 것을 알고 있습니다. 샘플은 훨씬 낮은 치수의 매니 폴드 (예 : 약 200)에 놓여 있습니다. PCA 알고리즘을 사용하면 차원을 줄이고 동시에 설명 된 분산을 대부분 보존하면서 데이터를 선형으로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">예를 들어, 숫자 데이터 세트의 경우, &lt;code&gt;digits.data&lt;/code&gt; 는 숫자 샘플을 분류하는 데 사용할 수있는 기능에 대한 액세스를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">예를 들어, 아래 예에서 의사 결정 트리는 데이터에서 학습하여 if-then-else 결정 규칙 세트로 사인 곡선을 근사화합니다. 나무가 깊을수록 의사 결정 규칙이 더 복잡하고 모델이 더 적합합니다.</target>
        </trans-unit>
        <trans-unit id="a42ba327206d2f6a371d1c7b16518c8946340f21" translate="yes" xml:space="preserve">
          <source>For instance, let&amp;rsquo;s compare the two predictions 1.0 and 100 that are both 50% of their corresponding true value.</source>
          <target state="translated">예를 들어, 해당 참 값의 50 % 인 두 예측 1.0과 100을 비교해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">예를 들어, 학습 벡터 알고리즘의 목적 함수에 사용되는 많은 요소 (예 : Support Vector Machines의 RBF 커널 또는 선형 모델의 l1 및 l2 정규화 기)는 모든 기능이 0을 중심으로 동일한 순서로 분산되어 있다고 가정합니다. 특징이 다른 것보다 큰 차수의 변동을 갖는 경우, 목적 함수를 지배하고 추정기가 예상대로 다른 특징에서 올바르게 학습하지 못할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">예를 들어, 다음은 Olivetti 데이터 세트에서 16 개의 샘플 인물 사진 (0.0을 중심으로)입니다. 오른쪽에는 처음 16 개의 특이 벡터가 초상화로 재구성되었습니다. 크기가 \ (n_ {samples} = 400 \)이고 \ (n_ {features} = 64 \ times 64 = 4096 \) 인 데이터 집합의 상위 16 개 특이 벡터 만 필요하므로 계산 시간은 1 초 미만입니다.</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">예를 들어, 샘플에 대해 \ (\ chi ^ 2 \) 테스트를 수행하여 다음과 같은 두 가지 최상의 기능 만 검색 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">예를 들어 :</target>
        </trans-unit>
        <trans-unit id="c017c696c4eba476debcc2637355a4ef09f7c0a3" translate="yes" xml:space="preserve">
          <source>For int/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">int / None 입력의 경우 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="560fc78b966a9c766c4d5505bc2466cc24122eb7" translate="yes" xml:space="preserve">
          <source>For int/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">int / None 입력의 경우 추정기가 분류기이고 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스이면 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 멀티 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용되고 그렇지 않으면 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 경우 &lt;code&gt;y&lt;/code&gt; 는 바이너리도 멀티 클래스도 아닌, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; 가&lt;/a&gt; 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 추정기가 분류 자이거나 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스가 아닌 경우 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 분류자가 True이고 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 추정기가 분류 자이고 &lt;code&gt;y&lt;/code&gt; 가 이진 또는 다중 클래스 인 경우 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ec46cd7e35a119deeb6479ced2aa91071495e898" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, &lt;code&gt;StratifiedKFold&lt;/code&gt; is used. In all other cases, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">정수 / 없음 입력의 경우 추정기가 분류기이고 y가 이진 또는 다중 클래스이면 &lt;code&gt;StratifiedKFold&lt;/code&gt; 가 사용됩니다. 다른 모든 경우에는 &lt;code&gt;KFold&lt;/code&gt; 가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">중간 값의 경우 두 번째 그림에서 &lt;code&gt;C&lt;/code&gt; 와 &lt;code&gt;gamma&lt;/code&gt; 의 대각선에서 좋은 모델을 찾을 수 있음을 알 수 있습니다 . 부드러운 점 ( &lt;code&gt;gamma&lt;/code&gt; 값이 낮을수록 )은 각 점을 올바르게 분류하는 중요성 (더 큰 &lt;code&gt;C&lt;/code&gt; 값) 을 증가시킴으로써 더 복잡한 모델을 만들 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 (n_samples_test, n_samples_train)입니다.</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 [n_samples_test, n_samples_train]입니다.</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">대규모 데이터 세트의 경우 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 사용을 고려할 수도 있습니다. '로그'손실과 함께 .</target>
        </trans-unit>
        <trans-unit id="3b24421d17b9758d0c0e99f847aff320121bf350" translate="yes" xml:space="preserve">
          <source>For large datasets, similar (but not identical) results can be obtained via &lt;a href=&quot;https://hdbscan.readthedocs.io&quot;&gt;HDBSCAN&lt;/a&gt;. The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain &lt;em&gt;n&lt;/em&gt; (as opposed to &lt;em&gt;n^2&lt;/em&gt;) memory scaling; however, tuning of the &lt;code&gt;max_eps&lt;/code&gt; parameter will likely need to be used to give a solution in a reasonable amount of wall time.</source>
          <target state="translated">대규모 데이터 세트의 경우 &lt;a href=&quot;https://hdbscan.readthedocs.io&quot;&gt;HDBSCAN을&lt;/a&gt; 통해 유사한 (동일하지 않은) 결과를 얻을 수 있습니다 . HDBSCAN 구현은 다중 스레드이며, 메모리 확장이 더 나빠지는 대신 OPTICS보다 알고리즘 런타임 복잡성이 더 좋습니다. HDBSCAN을 사용하여 시스템 메모리를 소모하는 매우 큰 데이터 세트의 경우 OPTICS는 &lt;em&gt;n&lt;/em&gt; ( &lt;em&gt;n ^ 2&lt;/em&gt; ) 메모리 스케일링을 유지합니다. 그러나 &lt;code&gt;max_eps&lt;/code&gt; 매개 변수의 조정은 적절한 양의 벽 시간 내에 솔루션을 제공하는 데 사용되어야합니다.</target>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">SVM을 포함한 많은 추정기의 경우, 각 기능에 대해 단위 표준 편차가있는 데이터 세트를 갖는 것이 좋은 예측을 얻는 데 중요합니다.</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">단일 출력 작업의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">자세한 내용은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani (2004)&amp;ldquo;최소 각 회귀 분석&amp;rdquo;, 통계 분석 (토론 포함), 407-499를 참조하십시오. ( &lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="d5e463f9f0eea6808a42462cec939570f71a16a6" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">자세한 내용은 Bradley Efron, Trevor Hastie, Iain Johnstone 및 Robert Tibshirani (2004) &quot;Least Angle Regression&quot;, Annals of Statistics (토론 포함), 407-499를 참조하십시오. ( &lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">자세한 정보는 &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;계층 적 클러스터링을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">사용법에 대한 자세한 내용은 사용 &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;설명서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">다중 클래스 분류의 경우 &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt; 는 AdaBoost-SAMME 및 AdaBoost-SAMME.R을 구현합니다. &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">멀티 클래스 분류의 경우 n_class 분류기는 일대일 접근 방식으로 훈련됩니다. 구체적으로 이는 Ridge에서 다변량 응답 지원을 활용하여 구현됩니다.</target>
        </trans-unit>
        <trans-unit id="f04898b2d6925a5dec9ef02339647623f6f19f8d" translate="yes" xml:space="preserve">
          <source>For multi-class classification, you need to set the class label for which the PDPs should be created via the &lt;code&gt;target&lt;/code&gt; argument:</source>
          <target state="translated">다중 클래스 분류의 경우 &lt;code&gt;target&lt;/code&gt; 인수 를 통해 PDP를 만들어야하는 클래스 레이블을 설정해야합니다 .</target>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">다중 클래스 모델의 경우 &lt;code&gt;label&lt;/code&gt; 인수 를 통해 PDP를 작성해야하는 클래스 레이블을 설정해야합니다 .</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">다중 메트릭 평가의 경우, 모든 스코어러에 대한 점수는 위에 표시된 &lt;code&gt;'_score'&lt;/code&gt; 대신 해당 스코어러 이름 ( &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; )으로 끝나는 키 의 &lt;code&gt;cv_results_&lt;/code&gt; dict에서 사용할 수 있습니다 . ( 'split0_test_precision', 'mean_train_precision'등)</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">다중 메트릭 평가의 경우이 특성은 &lt;code&gt;scoring&lt;/code&gt; 키를 스코어러 호출 가능에 맵핑하는 유효성 검증 된 스코어링 dict를 보유합니다 .</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">다중 메트릭 평가의 경우이 속성은 &lt;code&gt;refit&lt;/code&gt; 가 지정된 경우에만 존재 합니다.</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">다중 메트릭 평가를 들어,이 경우 사용할 수 없습니다 &lt;code&gt;refit&lt;/code&gt; 입니다 &lt;code&gt;False&lt;/code&gt; . 자세한 내용은 &lt;code&gt;refit&lt;/code&gt; 매개 변수를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">다중 메트릭 평가의 경우, &lt;code&gt;refit&lt;/code&gt; 가 지정된 경우에만 나타납니다 .</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">다중 출력 작업의 경우 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">다중 출력의 경우 y의 각 열의 가중치가 곱해집니다.</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">&quot;음수 클래스&quot;를 사용하는 멀티 클래스 분류의 경우 일부 레이블을 제외 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="dedd3af803ae0305b6a99c040827201493989ff1" translate="yes" xml:space="preserve">
          <source>For multiclass classification, K trees (for K classes) are built at each of the \(M\) iterations. The probability that \(x_i\) belongs to class k is modeled as a softmax of the \(F_{M,k}(x_i)\) values.</source>
          <target state="translated">다중 클래스 분류의 경우 K 트리 (K 클래스 용)는 \ (M \) 반복마다 빌드됩니다. \ (x_i \)가 클래스 k에 속할 확률은 \ (F_ {M, k} (x_i) \) 값의 소프트 맥스로 모델링됩니다.</target>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">멀티 클래스 문제의 경우 'newton-cg', 'sag', 'saga'및 'lbfgs'만 다항 손실을 처리합니다. 'liblinear'는 1 대 휴식 계획으로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">인스턴스 당 여러 레이블의 경우 &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="36264d57db60ea0d287310ae67879ef2207922af" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a &lt;code&gt;str&lt;/code&gt; denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">다중 메트릭 평가의 경우 마지막에 추정량을 다시 맞추기위한 최상의 매개 변수를 찾는 데 사용되는 스코어러를 나타내는 &lt;code&gt;str&lt;/code&gt; 이어야합니다 .</target>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">다중 메트릭 평가의 경우, 이는 스코어러가 끝에 추정기를 다시 시작하기위한 최상의 매개 변수를 찾는 데 사용됨을 나타내는 문자열이어야합니다.</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">다중 메트릭 평가의 경우, 이는 마지막에 추정기를 다시 시작하기위한 최상의 매개 변수를 찾는 데 사용되는 득점자를 나타내는 문자열이어야합니다.</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">승수 업데이트 ( 'mu') 솔버의 경우, Frobenius 규범 (0.5 * || X-WH || _Fro ^ 2)은 beta_loss 매개 변수를 변경하여 다른 베타-분산 손실로 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">n_components == 'mle'의 경우이 클래스는 &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt; 방법을 사용합니다 . NIPS에서, 598-604 페이지</target>
        </trans-unit>
        <trans-unit id="bd307d11754e6296ef567a0e60c357948ac810da" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;em&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/em&gt;</source>
          <target state="translated">n_components == 'mle'의 경우이 클래스는 &lt;em&gt;Minka, TP&amp;ldquo;PCA에 대한 차원의 자동 선택&amp;rdquo;&lt;/em&gt; 방법을 사용합니다 &lt;em&gt;. NIPS에서, pp. 598-604&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">스파 &lt;code&gt;coef_&lt;/code&gt; 아닌 모델의 경우, 즉 coef_에 0이 많지 않은 경우 실제로 메모리 사용량 이 &lt;em&gt;증가&lt;/em&gt; 할 수 있으므로이 방법을주의해서 사용하십시오. 일반적으로 &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; 으로 계산할 수있는 0 요소의 수는 50 % 이상이어야 상당한 이점을 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ffa9a81349b558a734852a9e93a9e01e72e14f97" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; &lt;a href=&quot;#yat2016&quot; id=&quot;id14&quot;&gt;[YAT2016]&lt;/a&gt;. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">정규화 된 상호 정보 및 조정 된 상호 정보의 경우 정규화 값은 일반적으로 각 클러스터링의 엔트로피에 대한 &lt;em&gt;일반화 된&lt;/em&gt; 평균입니다. 다양한 일반화 된 수단이 존재하며 하나를 다른 수단보다 선호하는 확고한 규칙은 없습니다. 결정은 주로 필드별로 이루어집니다. 예를 들어, 커뮤니티 감지에서 산술 평균이 가장 일반적입니다. 각 정규화 방법은 &quot;정 성적으로 유사한 동작&quot;을 제공합니다 &lt;a href=&quot;#yat2016&quot; id=&quot;id14&quot;&gt;[YAT2016]&lt;/a&gt; . 구현에서 이것은 &lt;code&gt;average_method&lt;/code&gt; 매개 변수에 의해 제어됩니다 .</target>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">정규화 된 상호 정보 및 조정 된 상호 정보의 경우, 정규화 값은 일반적으로 각 군집의 엔트로피의 일부 &lt;em&gt;일반화 된&lt;/em&gt; 평균이다. 다양한 일반화 된 수단이 존재하며, 하나를 다른 것보다 선호하기위한 확실한 규칙이 존재하지 않습니다. 결정은 주로 필드별로 이루어집니다. 예를 들어, 커뮤니티 감지에서 산술 평균이 가장 일반적입니다. 각 정규화 방법은 &quot;정 성적으로 유사한 행동&quot;[YAT2016]을 제공합니다. 우리의 구현에서 이것은 &lt;code&gt;average_method&lt;/code&gt; 매개 변수에 의해 제어됩니다 .</target>
        </trans-unit>
        <trans-unit id="8d22dd702102471017e490a2b7bc8b22b9add5e5" translate="yes" xml:space="preserve">
          <source>For now &amp;ldquo;auto&amp;rdquo; (kept for backward compatibiliy) chooses &amp;ldquo;elkan&amp;rdquo; but it might change in the future for a better heuristic.</source>
          <target state="translated">지금은 &quot;auto&quot;(이전 버전과의 호환성을 위해 유지됨)는 &quot;elkan&quot;을 선택하지만 향후 더 나은 휴리스틱을 위해 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">지금은 추정기를 블랙 박스로 간주합니다.</target>
        </trans-unit>
        <trans-unit id="436dc589cc421427188d0cca81de34e1e73f3c7d" translate="yes" xml:space="preserve">
          <source>For one sample, given the vector of continuous ground-truth values for each target \(y \in \mathbb{R}^{M}\), where \(M\) is the number of outputs, and the prediction \(\hat{y}\), which induces the ranking function \(f\), the DCG score is</source>
          <target state="translated">하나의 샘플에 대해 각 목표 \ (y \ in \ mathbb {R} ^ {M} \)에 대한 연속 실측 값 벡터가 주어지면 \ (M \)은 출력 수이고 예측값은 \ ( \ hat {y} \), 순위 함수 \ (f \)를 유도하면 DCG 점수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="09d6f289aa89a27e5d33a2c2e001f7d32a001ce5" translate="yes" xml:space="preserve">
          <source>For our dataset, again the model is not very predictive.</source>
          <target state="translated">데이터 세트의 경우에도 모델은 예측 성이 낮습니다.</target>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">모수 추정의 사후 분포는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">회귀의 경우 기본 학습 속도 일정은 다음과 같이 주어진 역 스케일링 ( &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; )입니다.</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">제곱 손실과 l2 페널티로 회귀하는 경우 평균 전략을 사용하는 SGD의 또 다른 변형이 SG (Stochastic Average Gradient) 알고리즘으로 제공되며 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 에서 솔버로 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">회귀를 위해 &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt; 는 AdaBoost.R2를 구현합니다 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">회귀를 위해 MLP는 제곱 오류 손실 기능을 사용합니다. 로 쓴</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">회귀 : &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">scikit-learn 버전 0.14.1 이전의 경우, 밀도가 높은 np.matrix 인스턴스를 반환하여 return_as = np.ndarray를 처리했습니다. 앞으로 np.ndarray는 예상대로 np.ndarray를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">간단한 변환을 위해 Transformer 객체 대신 변환 및 역 매핑을 정의하는 함수 쌍을 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">간단하게하기 위해 위의 방정식은 단일 교육 예를 위해 작성되었습니다. 가중치에 대한 구배는 상기의 것에 대응하는 2 개의 용어로 형성된다. 그것들은 일반적으로 각각의 부호 때문에 양의 기울기와 음의 기울기로 알려져 있습니다. 이 구현에서, 기울기는 샘플의 미니 배치에 대해 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">스코어링 매개 변수가 문자열, 호출 가능 또는 없음 인 단일 메트릭 평가의 경우 키는- &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">작은 데이터 세트 (\ (N \) 30 미만)의 경우 \ (\ log (N) \)는 \ (N \)와 비슷하며 무차별 알고리즘은 트리 기반 방식보다 더 효율적일 수 있습니다. &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 와 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; 는&lt;/a&gt; 모두 &lt;em&gt;리프 크기&lt;/em&gt; 매개 변수 를 제공 하여이 문제를 해결합니다 . 이는 쿼리가 무차별 대입으로 전환되는 샘플 수를 제어합니다. 이를 통해 두 알고리즘 모두 작은 \ (N \)에 대한 무차별 계산의 효율성에 접근 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">작은 데이터 세트의 경우 'liblinear'가 좋은 선택이지만 'sag'와 'saga'는 큰 데이터의 경우 더 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">일부 응용 분야의 경우, 전통적인 접근 방식에서는 예제, 기능 (또는 둘 다) 및 / 또는 처리 속도가 어려울 수 있습니다. 이 경우 scikit-learn에는 시스템 확장을 고려할 수있는 여러 가지 옵션이 있습니다.</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">일부 애플리케이션의 경우 추정기의 성능 (주로 예측 시간의 대기 시간 및 처리량)이 중요합니다. 교육 처리량을 고려하는 것도 흥미로울 수 있지만 프로덕션 환경 (종종 오프라인에서 발생하는 경우)에서는 덜 중요합니다.</target>
        </trans-unit>
        <trans-unit id="7c42c316b39e1433fd7efc7ca18424a81519f374" translate="yes" xml:space="preserve">
          <source>For some business applications, we are interested in the ability of the model to rank the riskiest from the safest policyholders, irrespective of the absolute value of the prediction. In this case, the model evaluation would cast the problem as a ranking problem rather than a regression problem.</source>
          <target state="translated">일부 비즈니스 애플리케이션의 경우 예측의 절대 값에 관계없이 가장 안전한 보험 계약자 중에서 가장 위험한 순위를 매기는 모델의 기능에 관심이 있습니다. 이 경우 모델 평가는 문제를 회귀 문제가 아닌 순위 문제로 던집니다.</target>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">일부 데이터 집합의 경우 사전 정의 된 데이터 분할을 교육 및 유효성 검사 접음 또는 여러 교차 유효성 검사 접음으로 이미 존재합니다. 사용 &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; 를&lt;/a&gt; 이 하이퍼 파라미터를 검색 할 때 이러한 주름 등을 사용하는 것이 가능하다.</target>
        </trans-unit>
        <trans-unit id="694369dc091b24e34e3df33be9eef2601d0ef6d1" translate="yes" xml:space="preserve">
          <source>For some losses, e.g. the least absolute deviation (LAD) where the gradients are \(\pm 1\), the values predicted by a fitted \(h_m\) are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree \(h_m\) are modified once the tree is fitted, such that the leaves values minimize the loss \(L_m\). The update is loss-dependent: for the LAD loss, the value of a leaf is updated to the median of the samples in that leaf.</source>
          <target state="translated">기울기가 \ (\ pm 1 \) 인 최소 절대 편차 (LAD)와 같은 일부 손실의 경우 피팅 된 \ (h_m \)에 의해 예측 된 값이 충분히 정확하지 않습니다. 트리는 정수 값만 출력 할 수 있습니다. 결과적으로, 나무가 장착되면 나무의 잎 값 \ (h_m \)이 수정되어 잎 값이 손실 \ (L_m \)을 최소화합니다. 업데이트는 손실에 따라 달라집니다. LAD 손실의 경우 리프 값이 해당 리프에있는 샘플의 중앙값으로 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">이미지, 비디오 및 오디오와 같은 기타 데이터의 경우 다음을 참조 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">스파 스 입력의 경우 데이터는 효율적인 Cython 루틴에 공급되기 전에 &lt;strong&gt;압축 스파 스 행 표현&lt;/strong&gt; ( &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 참조 ) 으로 &lt;strong&gt;변환됩니다&lt;/strong&gt; . 불필요한 메모리 복사를 피하려면 업스트림에서 CSR 표현을 선택하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">스파 스 입력의 경우 데이터가 &lt;strong&gt;압축 스파 스 행 표현으로 변환됩니다&lt;/strong&gt; ( &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 참조 ). 불필요한 메모리 복사를 피하려면 업스트림에서 CSR 표현을 선택하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">속도 및 공간 효율성을 위해 &lt;code&gt;scikit-learn&lt;/code&gt; 은 &lt;code&gt;target_names&lt;/code&gt; 목록 의 범주 이름 색인에 해당하는 정수의 배열로 대상 속성을로드 합니다. 각 샘플의 범주 정수 ID는 &lt;code&gt;target&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">속도를 높이기 위해 모든 실제 작업은 copy_predict (libsvm_helper.c) 함수의 C 레벨에서 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">데이터 세트의 명시적인 도메인 별 계층화에 따라 데이터를 분할합니다.</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">데이터 세트의 명시적이고 도메인 별 계층화에 따라 데이터를 분할합니다.</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">svd_solver == 'arpack'에 대해서는 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">svd_solver == 'randomized'의 경우 &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; 또한 &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c853f7e3d4c36b0f8076402fc9583125688d75a1" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;em&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/em&gt; and also &lt;em&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/em&gt;</source>
          <target state="translated">svd_solver == 'randomized'에 대해서는 &lt;em&gt;Halko, N., Martinsson, PG 및 Tropp, JA (2011)를 참조하십시오. &quot;무작위 구조 찾기 : 대략적인 행렬 분해를 구성하기위한 확률 알고리즘&quot;. SIAM 검토, 53 (2), 217-288.&lt;/em&gt; 또한 &lt;em&gt;Martinsson, PG, Rokhlin, V. 및 Tygert, M. (2011). &amp;ldquo;행렬 분해를위한 무작위 알고리즘&amp;rdquo;. 응용 및 전산 고조파 분석, 30 (1), 47-68.&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">'liblinear'의 경우 'sag'및 'lbfgs'솔버는 verbose를 자세한 양수로 설정합니다.</target>
        </trans-unit>
        <trans-unit id="e0f6f55d7894824895d15fbf0dd2debb3188c403" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;&lt;/a&gt; was used to influence the number of support vectors.</source>
          <target state="translated">비선형 커널을 사용하는 &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt; 알고리즘 제품군의 경우 지연 시간은 지원 벡터 수와 관련 이 있습니다 ( 적을 수록 빠름). 지연 시간과 처리량은 SVC 또는 SVR 모델의 지원 벡터 수에 따라 (점근 적으로) 선형 적으로 증가해야합니다. 커널은 또한 지원 벡터 당 한 번씩 입력 벡터의 투영을 계산하는 데 사용되기 때문에 지연 시간에 영향을줍니다. 다음 그래프에서 &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;nu&lt;/code&gt; 매개 변수를 사용하여 지원 벡터 수에 영향을 미쳤습니다.</target>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">비선형 커널을 사용하는 &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt; 알고리즘 제품군의 경우 지연 시간은 지원 벡터 수에 따라 달라집니다 (더 빠를수록 빠름). 지연 시간과 처리량은 SVC 또는 SVR 모델의 지원 벡터 수에 따라 (무증상) 선형으로 증가해야합니다. 커널은 지원 벡터 당 한 번 입력 벡터의 투영을 계산하는 데 사용되므로 대기 시간에 영향을줍니다. 다음 그래프에서 &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; 의 &lt;code&gt;nu&lt;/code&gt; 매개 변수 는 지원 벡터 수에 영향을주기 위해 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">를 들어 &lt;code&gt;l2&lt;/code&gt; 페널티 경우, 최상의 결과는 케이스에서 온다 &lt;code&gt;C&lt;/code&gt; 가 조절되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="b64515e541acdc2619277e9fede8598b267eca89" translate="yes" xml:space="preserve">
          <source>For the coefficient analysis, scaling is not needed this time.</source>
          <target state="translated">계수 분석을 위해 이번에는 스케일링이 필요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="73523e969cb224887344b79725c4bd74783daf1f" translate="yes" xml:space="preserve">
          <source>For the grid of &lt;code&gt;Cs&lt;/code&gt; values and &lt;code&gt;l1_ratios&lt;/code&gt; values, the best hyperparameter is selected by the cross-validator &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt;, but it can be changed using the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; parameter. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers can warm-start the coefficients (see &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;Glossary&lt;/a&gt;).</source>
          <target state="translated">&lt;code&gt;Cs&lt;/code&gt; 값과 &lt;code&gt;l1_ratios&lt;/code&gt; 값 의 그리드의 경우 교차 검증 자 &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; 가 최상의 하이퍼 파라미터를 선택 하지만 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; 매개 변수를 사용하여 변경할 수 있습니다 . 'newton-cg', 'sag', 'saga'및 'lbfgs'솔버는 계수를 웜 스타트 할 수 있습니다 ( &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;용어집&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">Cs 값의 그리드 (기본적으로 1e-4와 1e4 사이의 로그 스케일에서 10 개의 값으로 설정 됨)의 경우 교차 검증 자 StratifiedKFold에서 최상의 하이퍼 파라미터를 선택하지만 cv 매개 변수를 사용하여 변경할 수 있습니다. newton-cg 및 lbfgs 솔버의 경우 경로를 따라 웜 스타트합니다. 즉, 현재 맞춤의 초기 계수가 이전 맞춤의 수렴 후 얻은 계수 인 것으로 추측하므로 고차원 밀도의 경우 더 빠릅니다. 데이터.</target>
        </trans-unit>
        <trans-unit id="a2e465567e94e51f5ccdac035f7ba7d95a9eeb84" translate="yes" xml:space="preserve">
          <source>For the lbfgs solver set verbose to any positive number for verbosity.</source>
          <target state="translated">lbfgs 솔버의 경우 자세한 정보를 양수로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">liblinear 및 lbfgs 솔버의 경우 verbose를 자세한 양수로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="181a8f355e9076e45b2bb33dd2324e0459310c3b" translate="yes" xml:space="preserve">
          <source>For the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="translated">선형 사례의 경우 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 구현에 의해 &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt; 에서 사용되는 알고리즘 은 &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 기반 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 대응 알고리즘 보다 훨씬 효율적이며 수백만 개의 샘플 및 / 또는 기능으로 거의 선형으로 확장 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">가장 일반적인 사용 사례의 경우 &lt;code&gt;scoring&lt;/code&gt; 매개 변수를 사용하여 채점자 개체를 지정할 수 있습니다 . 아래 표는 가능한 모든 값을 보여줍니다. 모든 채점자 개체 &lt;strong&gt;는 반환 값이 높을수록 반환 값이 낮을수록 좋습니다&lt;/strong&gt; . 이와 같은 모델 데이터 사이의 거리를 측정하는 측정 항목 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; 은&lt;/a&gt; 메트릭의 부정 값을 반환 neg_mean_squared_error로서 가능하다.</target>
        </trans-unit>
        <trans-unit id="d789fec5338f15a1f2a15098b90094ce0875aa16" translate="yes" xml:space="preserve">
          <source>For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data.</source>
          <target state="translated">순진한 Bayes의 경우 검증 점수와 훈련 점수 모두 훈련 세트의 크기가 증가함에 따라 매우 낮은 값으로 수렴됩니다. 따라서 우리는 더 많은 훈련 데이터로 인해 많은 이점을 얻지 못할 것입니다.</target>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">&lt;code&gt;coef_&lt;/code&gt; 및 &lt;code&gt;intercept_&lt;/code&gt; 라는 이름의 근거 , 즉 선형 분류 자로서 순진 Bayes에 대해서는 J. Rennie et al. (2003), 순진한 Bayes 텍스트 분류기, ICML의 잘못된 가정을 다루기.</target>
        </trans-unit>
        <trans-unit id="1a1a7cda3c63aae62fc5939e46e880b718f959b0" translate="yes" xml:space="preserve">
          <source>For the remainder of this example, we remove the last element in &lt;code&gt;clfs&lt;/code&gt; and &lt;code&gt;ccp_alphas&lt;/code&gt;, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.</source>
          <target state="translated">이 예제의 나머지 부분에서는 노드가 하나 뿐인 사소한 트리이므로 &lt;code&gt;clfs&lt;/code&gt; 및 &lt;code&gt;ccp_alphas&lt;/code&gt; 의 마지막 요소를 제거 합니다. 여기서 우리는 알파가 증가함에 따라 노드 수와 트리 깊이가 감소 함을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">두 데이터 세트 사이에서 가장 가까운 이웃을 찾는 간단한 작업을 위해 &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; 내의 비 감독 알고리즘을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0a1bdaba8df32e218fd25f1e35b96e97a68bb33" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable using &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">이 데이터의 경우 &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt; 를 사용하여 &lt;code&gt;'city'&lt;/code&gt; 열을 범주 형 변수로 인코딩 하고 &lt;code&gt;'title'&lt;/code&gt; 열에 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 를 적용 할 수 있습니다 . 동일한 열에서 여러 특성 추출 방법을 사용할 수 있으므로 각 변환기에 고유 한 이름 (예 : &lt;code&gt;'city_category'&lt;/code&gt; 및 &lt;code&gt;'title_bow'&lt;/code&gt; )을 지정 합니다. 기본적으로 나머지 등급 열은 무시됩니다 ( &lt;code&gt;remainder='drop'&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">이 데이터의 경우 &lt;code&gt;'city'&lt;/code&gt; 열을 범주 형 변수로 인코딩하고 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 를 &lt;code&gt;'title'&lt;/code&gt; 열에 적용 할 수 있습니다 . 동일한 열에서 여러 기능 추출 방법을 사용할 수 있으므로 각 변환기에 고유 한 이름 ( &lt;code&gt;'city_category'&lt;/code&gt; 및 &lt;code&gt;'title_bow'&lt;/code&gt; )을 부여 합니다. 기본적으로 나머지 등급 열은 무시됩니다 ( &lt;code&gt;remainder='drop'&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">이 예에서는 각각 103 개의 특징과 14 개의 가능한 레이블이있는 2417 개의 데이터 포인트를 포함 하는 &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;효모&lt;/a&gt; 데이터 세트를 사용합니다 . 각 데이터 포인트에는 하나 이상의 레이블이 있습니다. 기준으로서 우리는 먼저 14 개의 레이블 각각에 대해 로지스틱 회귀 분류기를 훈련시킵니다. 이러한 분류기의 성능을 평가하기 위해 보류 테스트 세트를 예측하고 &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard 유사성 점수를&lt;/a&gt; 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="ccd3bf49df3c855961b0d8c881b499df98dfa5eb" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;https://www.openml.org/d/40597&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard score&lt;/a&gt; for each sample.</source>
          <target state="translated">이 예에서는 각각 103 개의 기능과 14 개의 가능한 레이블이있는 2417 개의 데이터 포인트가 포함 된 &lt;a href=&quot;https://www.openml.org/d/40597&quot;&gt;효모&lt;/a&gt; 데이터 세트를 사용합니다 . 각 데이터 포인트에는 하나 이상의 레이블이 있습니다. 기준선으로 먼저 14 개 레이블 각각에 대해 로지스틱 회귀 분류기를 훈련합니다. 이러한 분류기의 성능을 평가하기 위해 홀드 아웃 테스트 세트에서 예측하고 각 샘플에 대한 &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard 점수&lt;/a&gt; 를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="50c8fe3ea12a1b37824eecf96be6564acb86b697" translate="yes" xml:space="preserve">
          <source>For this example, the impurity-based and permutation methods identify the same 2 strongly predictive features but not in the same order. The third most predictive feature, &amp;ldquo;bp&amp;rdquo;, is also the same for the 2 methods. The remaining features are less predictive and the error bars of the permutation plot show that they overlap with 0.</source>
          <target state="translated">이 예에서 불순물 기반 및 순열 방법은 동일한 2 개의 강력한 예측 기능을 식별하지만 순서는 동일하지 않습니다. 세 번째로 가장 예측 가능한 기능인 &quot;bp&quot;는 두 가지 방법에서도 동일합니다. 나머지 특징은 덜 예측 적이며 순열 플롯의 오차 막대는 0과 겹치는 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a29265ef3f92733677c4dbea10c38e7e64e8fed9" translate="yes" xml:space="preserve">
          <source>For this example, we load a blood transfusion service center data set from &lt;code&gt;OpenML &amp;lt;https://www.openml.org/d/1464&amp;gt;&lt;/code&gt;. This is a binary classification problem where the target is whether an individual donated blood. Then the data is split into a train and test dataset and a logistic regression is fitted wtih the train dataset.</source>
          <target state="translated">이 예에서는 &lt;code&gt;OpenML &amp;lt;https://www.openml.org/d/1464&amp;gt;&lt;/code&gt; 에서 수혈 서비스 센터 데이터 세트를로드합니다 . 이것은 개인이 헌혈했는지 여부를 대상으로하는 이진 분류 문제입니다. 그런 다음 데이터가 기차 및 테스트 데이터 세트로 분할되고 로지스틱 회귀가 기차 데이터 세트에 맞춰집니다.</target>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">이 방법에서, M은 밀집 행렬, 희소 행렬 또는 일반 선형 연산자 일 수있다. 경고 : ARPACK은 일부 문제로 인해 불안정 할 수 있습니다. 결과를 확인하기 위해 여러 개의 임의의 씨앗을 사용하는 것이 가장 좋습니다.</target>
        </trans-unit>
        <trans-unit id="08c8de94919880222510697f43131d88196a6fc1" translate="yes" xml:space="preserve">
          <source>For this particular pattern of missing values we see that &lt;a href=&quot;../../modules/generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;sklearn.ensemble.ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; give the best results.</source>
          <target state="translated">이 특정 패턴의 결 측값에 대해 &lt;a href=&quot;../../modules/generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;sklearn.ensemble.ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt; &lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt; &lt;/a&gt; 가 최상의 결과를 제공 한다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">이를 위해 추정기는 '연결성'행렬을 사용하여 어떤 샘플이 연결되는지 제공합니다.</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">이러한 이유로 20 개의 뉴스 그룹 데이터를로드하는 함수는 &lt;strong&gt;remove&lt;/strong&gt; 라는 매개 변수를 제공하여 각 파일에서 어떤 종류의 정보를 제거할지 알려줍니다. &lt;strong&gt;remove&lt;/strong&gt; 는 &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; 하위 집합을 포함하는 튜플이어야하며 헤더, 서명 블록 및 인용 블록을 각각 제거하도록 지시합니다.</target>
        </trans-unit>
        <trans-unit id="60d82b78a5294ae2dc0ada0318b904b11e85c403" translate="yes" xml:space="preserve">
          <source>For two clusters, SpectralClustering solves a convex relaxation of the &lt;a href=&quot;https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image.</source>
          <target state="translated">두 군집의 경우 SpectralClustering 은 유사성 그래프 에서 &lt;a href=&quot;https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;정규화 된 절단&lt;/a&gt; 문제 의 볼록 이완을 해결합니다 . 즉, 각 군집 내부 모서리의 가중치에 비해 모서리 절단의 가중치가 작도록 그래프를 두 개로 절단합니다. 이 기준은 그래프 정점이 픽셀이고 유사성 그래프 가장자리의 가중치가 이미지의 기울기 함수를 사용하여 계산되는 이미지 작업시 특히 흥미 롭습니다.</target>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">두 군집의 경우 유사성 그래프 에서 &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;정규화 된 절단&lt;/a&gt; 문제 의 볼록한 이완 문제를 해결합니다. 그래프를 2 개로 절단하여 절단면의 무게가 각 군집 내부의 가장자리의 무게에 비해 작습니다. 이 기준은 이미지 작업시 특히 흥미 롭습니다. 그래프 정점은 픽셀이며 유사성 그래프의 가장자리는 이미지의 기울기 함수입니다.</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">여러 가지 이유로, 많은 실제 데이터 세트에는 종종 공백, NaN 또는 기타 자리 표시 자로 인코딩 된 결 측값이 포함됩니다. 그러나 이러한 데이터 세트는 배열의 모든 값이 숫자이고 모두 의미를 가지고 있다고 가정하는 scikit-learn 추정기와 호환되지 않습니다. 불완전한 데이터 세트를 사용하는 기본 전략은 결 측값이 포함 된 전체 행 및 / 또는 열을 삭제하는 것입니다. 그러나 이것은 가치가없는 데이터를 잃어 버릴 수 있습니다 (불완전하지만). 더 나은 전략은 결 측값을 대치하는 것입니다. 즉 데이터의 알려진 부분에서이를 유추하는 것입니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;대치에 대한 공통 용어 및 API 요소 용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e2303c3ef75d0f928a4dd5ec4517d95f82d7d02b" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">여러 가지 이유로 많은 실제 데이터 세트에는 종종 공백, NaN 또는 기타 자리 표시 자로 인코딩 된 결 측값이 포함되어 있습니다. 그러나 이러한 데이터 세트는 배열의 모든 값이 숫자이고 모두 의미를 갖고 있다고 가정하는 scikit-learn 추정기와 호환되지 않습니다. 불완전한 데이터 세트를 사용하는 기본 전략은 누락 된 값이 포함 된 전체 행 및 / 또는 열을 삭제하는 것입니다. 그러나 이것은 (불완전하더라도) 가치가있을 수있는 데이터 손실의 대가로 발생합니다. 더 나은 전략은 누락 된 값을 대치하는 것입니다. 즉, 데이터의 알려진 부분에서 유추하는 것입니다. 대치에 대한 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;용어집 및 API 요소&lt;/a&gt; 항목을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">시각화 목적으로 (t-SNE의 주요 사용 사례) Barnes-Hut 방법을 사용하는 것이 좋습니다. 정확한 t-SNE 방법은 더 높은 차원 공간에서 임베딩의 이론적 속성을 확인하는 데 유용하지만 계산 제약으로 인해 작은 데이터 세트로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">시각화를 위해, bicluster가 주어지면, 데이터 행렬의 행과 열은 bicluster를 연속적으로 만들기 위해 재 배열 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">시각화를 위해 2D 캔버스에 다른 기호를 배치해야합니다. 이를 위해 &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold 학습&lt;/a&gt; 기술을 사용 하여 2D 임베딩을 검색합니다.</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;working_memory&lt;/code&gt; 를 줄임으로써 행 단위로 강제 작성 :</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina, M. et al., PARVUS-데이터 탐색, 분류 및 상관 관계를위한 확장 가능한 패키지. 16ga Genoa, Brigata Salerno를 통한 제약 및 식품 분석 및 기술 연구소.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 평균 정밀도는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 범위는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">공식적으로,지면 진실 레이블 \ (y \ in \ left \ {0, 1 \ right \} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \) 및 점수의 이진 표시기 행렬 각 레이블 \ (\ hat {f} \ in \ mathbb {R} ^ {n_ \ text {samples} \ times n_ \ text {labels}} \)와 연관된 순위 손실은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="c45b835372dc3641934de6a0e36f8d5df72bc091" translate="yes" xml:space="preserve">
          <source>Format specification for values in confusion matrix. If &lt;code&gt;None&lt;/code&gt;, the format specification is &amp;lsquo;d&amp;rsquo; or &amp;lsquo;.2g&amp;rsquo; whichever is shorter.</source>
          <target state="translated">혼동 행렬의 값에 대한 형식 사양입니다. 경우 &lt;code&gt;None&lt;/code&gt; , 포맷 사양 중 짧은 '.2g'D '또는이다.</target>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">지정된 함수에 의해 주어진 선호도 매트릭스를 형성하고 스펙트럼 분해를 해당 그래프 라플라시안에 적용합니다. 결과 변환은 각 데이터 포인트에 대한 고유 벡터의 값으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">다행히도 &lt;strong&gt;X에서 대부분의 값은 0이 될 것&lt;/strong&gt; 입니다. 주어진 문서에 대해 수천 개 미만의 별개의 단어가 사용되기 때문입니다. 이러한 이유로 우리는 단어 모음이 일반적으로 &lt;strong&gt;고차원 희소 데이터 세트&lt;/strong&gt; 라고 말합니다 . 특징 벡터의 0이 아닌 부분 만 메모리에 저장함으로써 많은 메모리를 절약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e399cc71dd11205217c77d4c7f1a914e719b462c" translate="yes" xml:space="preserve">
          <source>Frequency model &amp;ndash; Poisson distribution</source>
          <target state="translated">주파수 모델 &amp;ndash; 포아송 분포</target>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">프로젝트와 기여에 대해 자주 묻는 질문.</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman et al., &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&quot;그래픽 올가미를 사용한 희소 역공 분산 추정&quot;&lt;/a&gt; , Biostatistics 9, pp 432, 2008</target>
        </trans-unit>
        <trans-unit id="563c92aa7d72e63e351914d407eb4e8a1bed4188" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;https://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman et al, &lt;a href=&quot;https://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&quot;그래픽 올가미를 사용한 희소 역 공분산 추정&quot;&lt;/a&gt; , Biostatistics 9, pp 432, 2008</target>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">Friedman, &quot;스토리지 그라디언트 부스팅&quot;, 1999</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">Friedman, 확률 적 그라디언트 부스팅, 1999</target>
        </trans-unit>
        <trans-unit id="910211d4464f03643fe19d7b724011f366eafec5" translate="yes" xml:space="preserve">
          <source>Friedmann, Jerome H., 2007, &lt;a href=&quot;https://statweb.stanford.edu/~jhf/ftp/stobst.pdf&quot;&gt;&amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">Friedmann, Jerome H., 2007, &lt;a href=&quot;https://statweb.stanford.edu/~jhf/ftp/stobst.pdf&quot;&gt;&quot;확률 적 그라디언트 부스팅&quot;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">트레이닝 데이터 &lt;code&gt;X&lt;/code&gt; 와 적합 모델로부터의 재구성 된 데이터 &lt;code&gt;WH&lt;/code&gt; 사이의 매트릭스 차이 또는 베타-분산의 프로 베니 우스 표준 .</target>
        </trans-unit>
        <trans-unit id="c51f06e97e3098b2be5170a19f440afe2d031cf5" translate="yes" xml:space="preserve">
          <source>From features with fewest missing values to most.</source>
          <target state="translated">결 측값이 가장 적은 특성부터 가장 많은 특성까지.</target>
        </trans-unit>
        <trans-unit id="42591b8a9cd574128a8414edcfb65dcf2e26248d" translate="yes" xml:space="preserve">
          <source>From features with most missing values to fewest.</source>
          <target state="translated">결 측값이 가장 많은 특성부터 가장 적은 특성까지.</target>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">이미지에서</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">발생에서 빈도까지</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">scikit-learn에서 : [ 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']. 이 메트릭은 희소 행렬 입력을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="e036581a0079e3a00bbe6bddf83817f4c4f30e31" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs. [&amp;lsquo;nan_euclidean&amp;rsquo;] but it does not yet support sparse matrices.</source>
          <target state="translated">scikit-learn에서 : [ 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']. 이러한 메트릭은 희소 행렬 입력을 지원합니다. [ 'nan_euclidean']하지만 아직 희소 행렬을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">scipy.spatial.distance에서 : [ 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'minkowski', 'rogerstanimoto ','russellrao ','seuclidean ','sokalmichener ','sokalsneath ','sqeuclidean ','yule '] 이러한 메트릭에 대한 자세한 내용은 scipy.spatial.distance 설명서를 참조하십시오. 이 메트릭은 희소 행렬 입력을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">텍스트에서</target>
        </trans-unit>
        <trans-unit id="b8b69c633940e44df1e4f7cf5b91c3ed0ce89b65" translate="yes" xml:space="preserve">
          <source>From the Wikipedia page for Discounted Cumulative Gain:</source>
          <target state="translated">할인 된 누적 이득에 대한 Wikipedia 페이지에서 :</target>
        </trans-unit>
        <trans-unit id="eea6e746f349894e725f780b7f19777ab0b59905" translate="yes" xml:space="preserve">
          <source>From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; for more details.</source>
          <target state="translated">위의 공식에서 LDA가 선형 결정 표면을 가지고 있음이 분명합니다. QDA의 경우 가우시안의 공분산 행렬 \ (\ Sigma_k \)에 대한 가정이 없으므로 2 차 결정 표면으로 이어집니다. 자세한 내용은 &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">구현 관점에서 이것은 예측 객체로 감싸 진 평범한 최소 제곱 (scipy.linalg.lstsq)입니다.</target>
        </trans-unit>
        <trans-unit id="704f1c6d00ec317ee90c0b6672883e3dd205ae68" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;a href=&quot;../../modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">프로그래밍 관점에서 보면 scikit-learn의 온라인 API를 사용하여 매우 큰 데이터 세트를 청크로 처리하는 방법을 보여주기 때문에 흥미 롭습니다. 진행 방법은 한 번에 이미지를로드하고이 이미지에서 무작위로 50 개의 패치를 추출하는 것입니다. 500 개의 패치 (이미지 10 개 사용)를 누적 하면 온라인 KMeans 개체 MiniBatchKMeans 의 &lt;a href=&quot;../../modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt; 메서드를 실행합니다 .</target>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">프로그래밍 관점에서 볼 때 scikit-learn의 온라인 API를 사용하여 청크별로 매우 큰 데이터 세트를 처리하는 방법을 보여주기 때문에 흥미 롭습니다. 진행 방법은 한 번에 이미지를로드하고이 이미지에서 무작위로 50 개의 패치를 추출하는 것입니다. 10 개의 이미지를 사용 하여이 패치 중 500 개가 누적되면 온라인 KMeans 객체 MiniBatchKMeans 의 &lt;code&gt;partial_fit&lt;/code&gt; 메소드를 실행합니다 .</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값) 또는 점수가있는 단일 배열을 반환하는 함수입니다. 기본값은 f_classif입니다 (아래 참조). 기본 기능은 분류 작업에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값)을 반환하는 함수입니다. 기본값은 f_classif입니다 (아래 참조). 기본 기능은 분류 작업에서만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">두 개의 배열 X와 y를 가져와 한 쌍의 배열 (점수, p 값)을 반환하는 함수입니다. '백분위 수'또는 'kbest'모드의 경우 단일 배열 점수를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 전에 &lt;code&gt;y&lt;/code&gt; 에 적용하는 기능 . &lt;code&gt;transformer&lt;/code&gt; 와 동시에 설정할 수 없습니다 . 이 함수는 2 차원 배열을 반환해야합니다. 경우 &lt;code&gt;func&lt;/code&gt; 없습니다 &lt;code&gt;None&lt;/code&gt; , 사용되는 함수는 식별 기능이 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">회귀 예측에 적용하는 기능. &lt;code&gt;transformer&lt;/code&gt; 와 동시에 설정할 수 없습니다 . 이 함수는 2 차원 배열을 반환해야합니다. 역함수는 예측을 원래 훈련 레이블의 동일한 공간으로 되 돌리는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">데이터 센터링 및 스케일링 데이터의 중요성에 대한 자세한 내용은이 FAQ에서 확인할 수 있습니다. 데이터를 &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;표준화 / 표준화 / 조정해야합니까?&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">'whiten = True'를 사용하여 피처에서 선형 상관 관계를 추가로 제거합니다.</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">또한이 모델은 샘플이 둘 이상의 클래스에 속할 수있는 &lt;a href=&quot;multiclass#multiclass&quot;&gt;다중 레이블 분류&lt;/a&gt; 를 지원합니다 . 각 클래스에 대해 원시 출력은 로지스틱 함수를 통과합니다. &lt;code&gt;0.5&lt;/code&gt; 보다 크거나 같은 값 은 &lt;code&gt;1&lt;/code&gt; 로 반올림되고 , 그렇지 않으면 &lt;code&gt;0&lt;/code&gt; 으로 반올림됩니다 . 샘플의 예측 출력에서 ​​값이 &lt;code&gt;1&lt;/code&gt; 인 인덱스 는 해당 샘플의 할당 된 클래스를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">또한 &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt; 는 &lt;strong&gt;대칭입니다&lt;/strong&gt; . 인수를 바꾸어도 점수가 변경되지 않습니다. 따라서 &lt;strong&gt;합의 조치&lt;/strong&gt; 로 사용될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f148ccef557451e9c13ec83bfface59d01588547" translate="yes" xml:space="preserve">
          <source>Furthermore, impurity-based feature importance for trees are &lt;strong&gt;strongly biased&lt;/strong&gt; and &lt;strong&gt;favor high cardinality features&lt;/strong&gt; (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.</source>
          <target state="translated">또한, 트리에 대한 불순물 기반 기능 중요성은 &lt;strong&gt;강하게 편향되어&lt;/strong&gt; 있으며 가능한 범주 수가 적은 이진 기능 또는 범주 형 변수와 같은 낮은 카디널리티 기능보다 &lt;strong&gt;높은 카디널리티 기능&lt;/strong&gt; (일반적으로 숫자 기능)을 &lt;strong&gt;선호&lt;/strong&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">또한 처리 된 예제 수에 따라 다른 알고리즘의 성능이 발전한 것을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">또한 기본 매개 변수 &lt;code&gt;smooth_idf=True&lt;/code&gt; 는 컬렉션의 모든 항을 정확히 한 번만 포함하는 추가 문서가있는 것처럼 분자와 분모에&amp;ldquo;1&amp;rdquo;을 추가하여 0 나누기를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">또한 tf 및 idf를 계산하는 데 사용되는 공식은 다음과 같이 IR에 사용 된 SMART 표기법에 해당하는 매개 변수 설정에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="f576c03970023ff0ede275b819158dc78e4bd414" translate="yes" xml:space="preserve">
          <source>Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.</source>
          <target state="translated">또한, 랜덤 포레스트의 불순물 기반 기능 중요성은 훈련 데이터 세트에서 파생 된 통계에서 계산되기 때문에 어려움을 겪습니다. 모델에 사용할 수있는 용량이있는 한 대상 변수를 예측하지 못하는 기능의 경우에도 중요도가 높을 수 있습니다. 과적 합합니다.</target>
        </trans-unit>
        <trans-unit id="a28e3ed3e4426c3b74ba7f5c5c797d3018edc64c" translate="yes" xml:space="preserve">
          <source>Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size &lt;code&gt;max_features&lt;/code&gt;. (See the &lt;a href=&quot;#random-forest-parameters&quot;&gt;parameter tuning guidelines&lt;/a&gt; for more details).</source>
          <target state="translated">또한 트리를 구성하는 동안 각 노드를 분할 할 때 모든 입력 특성 또는 &lt;code&gt;max_features&lt;/code&gt; 크기의 임의 하위 집합에서 최상의 분할을 찾습니다 . (자세한 내용은 &lt;a href=&quot;#random-forest-parameters&quot;&gt;매개 변수 조정 지침&lt;/a&gt; 을 참조하십시오).</target>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;확률로 표현 된 예측 확인&lt;/a&gt; , 월별 날씨 검토 78.1 (1950)</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G. Celeux, M. El Anbari, J.-M. Marin, CP Robert,&amp;ldquo;회귀 규칙 화 : 정보가 부족한 상황에서 베이지안과 잦은 방법 비교&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="623c67fa2cbbcf37d713401c722eceec0b680645" translate="yes" xml:space="preserve">
          <source>G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5, Section 5.4.4, pp. 252-253.</source>
          <target state="translated">G. Golub 및 C. Van Loan. 행렬 계산, 제 3 판, 5 장, 섹션 5.4.4, pp. 252-253.</target>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;통계 학습 소개&lt;/a&gt; , Springer 2013.</target>
        </trans-unit>
        <trans-unit id="3cb2eea23f004b3e761d528ddc6b2e3e79458d85" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;https://www-bcf.usc.edu/~gareth/ISL/&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;https://www-bcf.usc.edu/~gareth/ISL/&quot;&gt;통계 학습 소개&lt;/a&gt; , Springer 2013.</target>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. Louppe 및 P. Geurts, &quot;임의 패치에 대한 앙상블&quot;, 데이터베이스의 기계 학습 및 지식 발견, 346-361, 2012.</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G. Louppe,&amp;ldquo;임의의 숲 이해 : 이론에서 실제까지&amp;rdquo;, 박사 학위 논문, Liege of Liege, 2014.</target>
        </trans-unit>
        <trans-unit id="80030b72580197a6197c00418acf9f08511c2c49" translate="yes" xml:space="preserve">
          <source>G. Ridgeway, &amp;ldquo;Generalized Boosted Models: A guide to the gbm package&amp;rdquo;, 2007</source>
          <target state="translated">G. Ridgeway, &amp;ldquo;Generalized Boosted Models: A guide to the gbm package&amp;rdquo;, 2007</target>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP Box와 DR Cox,&amp;ldquo;변형 분석&amp;rdquo;, Royal Statistical Society B, 26, 211-252 (1964).</target>
        </trans-unit>
        <trans-unit id="83c6052410f7be2971558c8f2b162b661b4a734b" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB는 순방향 단계적 방식으로 적층 모델을 구축합니다. 회귀 트리는 이항 또는 다항 이탈도 손실 함수의 음의 기울기에 적합합니다. 이진 분류는 단일 회귀 트리 만 유도되는 특수한 경우입니다.</target>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB는 단계별 단계 방식으로 덧셈 모델을 만듭니다. 임의의 차별화 가능한 손실 함수를 최적화 할 수 있습니다. 각 단계에서 &lt;code&gt;n_classes_&lt;/code&gt; 회귀 트리는 이항 또는 다항 이탈 손실 함수의 음의 구배에 적합합니다. 이진 분류는 단일 회귀 트리 만 유도되는 특별한 경우입니다.</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GB는 단계별 단계 방식으로 덧셈 모델을 만듭니다. 임의의 차별화 가능한 손실 함수를 최적화 할 수 있습니다. 각 단계에서 회귀 트리는 주어진 손실 함수의 음의 구배에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">GBRT는 다음 형식의 추가 모델을 고려합니다.</target>
        </trans-unit>
        <trans-unit id="af90cc1188550654bd22990a09c9155ebaa04680" translate="yes" xml:space="preserve">
          <source>GBRT regressors are additive models whose prediction \(y_i\) for a given input \(x_i\) is of the following form:</source>
          <target state="translated">GBRT 회귀 변수는 주어진 입력 \ (x_i \)에 대한 예측 \ (y_i \)이 다음 형식 인 가법 모델입니다.</target>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">GMM 공분산</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G [i, j]는 그래프를 따라 점 i에서 점 j까지의 최단 거리를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">스핑크스 갤러리에 의해 생성 된 갤러리</target>
        </trans-unit>
        <trans-unit id="cba508b12182b68f501d6af46c4f03f8fc5d2473" translate="yes" xml:space="preserve">
          <source>Gamma</source>
          <target state="translated">Gamma</target>
        </trans-unit>
        <trans-unit id="927ef8e7f274c04e9c8836a36352812cc37bb26c" translate="yes" xml:space="preserve">
          <source>Gamma deviance is equivalent to the Tweedie deviance with the power parameter &lt;code&gt;power=2&lt;/code&gt;. It is invariant to scaling of the target variable, and measures relative errors.</source>
          <target state="translated">감마 편차는 전력 매개 변수 &lt;code&gt;power=2&lt;/code&gt; 를 사용하는 Tweedie 편차와 같습니다 . 대상 변수의 스케일링에 불변하며 상대 오차를 측정합니다.</target>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">RBF, 라플라시안, 다항식, 지수 chi2 및 시그 모이 드 커널에 대한 감마 매개 변수. 기본값의 해석은 커널에 맡겨져 있습니다. sklearn.metrics.pairwise 설명서를 참조하십시오. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">rbf, poly 및 sigmoid 커널의 감마 매개 변수. 다른 커널에서는 무시됩니다. 기본적으로 0.1입니다.</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">게이츠, GW (1972)&amp;ldquo;가장 가까운 이웃 규칙 감소&amp;rdquo;. 정보 이론에 관한 IEEE 거래, 1972 년 5 월, 431-433.</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">가우스 혼합 모델 타원체</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">가우스 혼합 모델 선택</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">가우스 혼합 모델 사인 곡선</target>
        </trans-unit>
        <trans-unit id="b8fb995e81cb89650fea0baec9d6ae8f98a9538f" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Models</source>
          <target state="translated">가우스 혼합 모델</target>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">가우스 혼합물.</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">가우스 나이브 베이 즈 (GaussianNB)</target>
        </trans-unit>
        <trans-unit id="d854cefb413c2902ad18940be1c741ae3117e7e6" translate="yes" xml:space="preserve">
          <source>Gaussian Process for Machine Learning</source>
          <target state="translated">기계 학습을위한 가우스 프로세스</target>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">가우스 프로세스 회귀 : 기본 입문 예</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">가우스 커널 ( &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">가우스 혼합 모델은 다양한 추론에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">군집에 유용한 가우시안 혼합 모델은 혼합 모델 전용 &lt;a href=&quot;mixture#mixture&quot;&gt;설명서의 다른 장에&lt;/a&gt; 설명되어 있습니다. KMeans는 성 분당 공분산이 동일한 가우스 혼합 모델의 특수 사례로 볼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">가우스 혼합물</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">Laplace 근사를 기반으로 한 가우스 프로세스 분류 (GPC).</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">홍채 데이터 세트의 가우스 프로세스 분류 (GPC)</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">Mauna Loa CO2 데이터에 대한 가우시안 프로세스 회귀 (GPR).</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">잡음 수준 추정 기능이있는 가우시안 프로세스 회귀 (GPR)</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">가우스 프로세스 회귀 (GPR).</target>
        </trans-unit>
        <trans-unit id="81d5ab12411c6f249a3ae9ff3884b17e3d00399b" translate="yes" xml:space="preserve">
          <source>Gaussian processes on discrete data structures</source>
          <target state="translated">이산 데이터 구조에 대한 가우스 프로세스</target>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayes는 확률을 0 또는 1로 푸시하는 경향이 있습니다 (히스토그램의 카운트 참고). 이는 클래스에 따라 기능이 조건부로 독립적이라는 가정을하기 때문입니다.이 데이터 세트에는 2 개의 중복 기능이 포함되어 있지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">일반 KDD 구조 :</target>
        </trans-unit>
        <trans-unit id="082e84b7a80940ab38b8fafffced3896fbf61a5f" translate="yes" xml:space="preserve">
          <source>General examples about classification algorithms.</source>
          <target state="translated">분류 알고리즘에 대한 일반적인 예.</target>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">범용, 균일 한 클러스터 크기, 평평한 형상, 클러스터가 너무 많지 않음</target>
        </trans-unit>
        <trans-unit id="5a99200d3c187d0fcefb7b4df6803366dc2748df" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Gamma distribution.</source>
          <target state="translated">감마 분포가있는 일반화 선형 모형.</target>
        </trans-unit>
        <trans-unit id="0ed4c66ad535ba7380d74741129413d4f8c145bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Poisson distribution.</source>
          <target state="translated">포아송 분포가있는 일반화 선형 모형.</target>
        </trans-unit>
        <trans-unit id="d682c681b1fa1783371317722a00ec63b80aa77c" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Tweedie distribution.</source>
          <target state="translated">Tweedie 분포를 사용하는 일반화 선형 모형.</target>
        </trans-unit>
        <trans-unit id="b17d9222a12b9513aac695dd37d7bdc64c218d77" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models</source>
          <target state="translated">일반화 선형 모형</target>
        </trans-unit>
        <trans-unit id="bcbd479b250088e4214e337494acb2a2758516bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models (GLM) extend linear models in two ways &lt;a href=&quot;#id33&quot; id=&quot;id31&quot;&gt;10&lt;/a&gt;. First, the predicted values \(\hat{y}\) are linked to a linear combination of the input variables \(X\) via an inverse link function \(h\) as</source>
          <target state="translated">일반화 선형 모델 (GLM)은 두 가지 방법으로 선형 모델을 확장합니다 &lt;a href=&quot;#id33&quot; id=&quot;id31&quot;&gt;10&lt;/a&gt; . 첫째, 예측값 \ (\ hat {y} \)은 다음과 같이 역 연결 함수 \ (h \)를 통해 입력 변수 \ (X \)의 선형 조합에 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="050c76b497e038e0c5a06ed24dce47a6958dfb02" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models, and Poisson loss for gradient boosting</source>
          <target state="translated">일반화 된 선형 모델 및 기울기 부스팅을위한 포아송 손실</target>
        </trans-unit>
        <trans-unit id="194ee7e5ec30d094070f5f72a72c8597376dc276" translate="yes" xml:space="preserve">
          <source>Generalized linear models (GLM) for regression</source>
          <target state="translated">회귀를위한 일반화 선형 모델 (GLM)</target>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">일반적으로 모델의 복잡성이 증가하면 예측력과 대기 시간이 증가합니다. 예측력을 높이는 것은 일반적으로 흥미롭지 만 많은 응용 프로그램에서 예측 대기 시간을 너무 많이 늘리지 않는 것이 좋습니다. 우리는 이제 여러 가지 감독 모델의이 아이디어를 검토 할 것입니다.</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">선택적 감소로 청크로 거리 매트릭스 청크 생성</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">종 모양의 특이 값으로 대부분 낮은 순위의 행렬 생성</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">지정된 차수 이하의 차수를 가진 모든 다항식 조합으로 구성된 새 피처 행렬을 생성합니다. 예를 들어, 입력 샘플이 2 차원이고 [a, b] 형태 인 경우, 차수 -2 다항식 특징은 [1, a, b, a ^ 2, ab, b ^ 2]입니다.</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">임의의 다중 레이블 분류 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">임의의 n- 클래스 분류 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">드문 상관없는 설계로 랜덤 회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">랜덤 회귀 문제를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">임의의 대칭 양수 한정 행렬을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">사전 요소의 희소 조합으로 신호를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">희소 랜덤 프로젝션 매트릭스 생성</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">희소 대칭 한정 양수 행렬을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">스위스 롤 데이터 세트를 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">S 곡선 데이터 셋을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">바이커 스터링을 위해 블록 바둑판 구조로 배열을 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">바이 블러스터 링을 위해 일정한 블록 대각선 구조로 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">각 입력 데이터 포인트에 대해 교차 검증 된 추정값 생성</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">데이터 세트를 생성하십시오. 알고리즘의 확장 성을 볼 수있을만큼 큰 크기를 선택하지만 너무 긴 실행 시간을 피하기에는 너무 크지 않습니다</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">등방성 가우스 및 라벨 샘플을 Quantile로 생성</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">클러스터링을위한 등방성 가우스 블롭을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">X에 대한 결 측값 표시기를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">다항식 및 교호 작용 피처를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">피팅 된 가우스 분포에서 랜덤 표본을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">모델에서 임의의 샘플을 생성하십시오.</target>
        </trans-unit>
        <trans-unit id="077b466863e9f097ef6d30c373ea7fea91f90736" translate="yes" xml:space="preserve">
          <source>Generate test sets such that all contain the same distribution of classes, or as close as possible.</source>
          <target state="translated">모두 동일한 클래스 분포를 포함하거나 가능한 한 가깝도록 테스트 세트를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="7afab3e6555db4edb28194e580e8ac980040a53c" translate="yes" xml:space="preserve">
          <source>Generate test sets where the smallest and largest differ by at most one sample.</source>
          <target state="translated">최대 하나의 샘플에서 가장 작은 것과 가장 큰 것이 다른 테스트 세트를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 1&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 2&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">&quot;Friedman # 3&quot;회귀 문제 생성</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">Hastie et al.에서 사용 된 이진 분류에 대한 데이터를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">Hastie et al.에서 사용 된 이진 분류에 대한 데이터를 생성합니다. 2009, 실시 예 10.2.</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">사전 정의 된 분할을 기반으로 기차 / 테스트 지수를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">랜덤 순열을 기반으로 기차 / 테스트 지수를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">주어진 분포에서 샘플링 된 모수에 대한 생성기.</target>
        </trans-unit>
        <trans-unit id="6d76c76581c79bfcc7307e6698c50d3852025179" translate="yes" xml:space="preserve">
          <source>Generator that yields (estimator, check) tuples. Returned when &lt;code&gt;generate_only=True&lt;/code&gt;.</source>
          <target state="translated">(추정기, 검사) 튜플을 생성하는 생성기. &lt;code&gt;generate_only=True&lt;/code&gt; 일 때 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">n까지 올라가는 n_packs 슬라이스를 생성하는 생성기</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">형상 (메트릭 사용)</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">조지 W 부시</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">게르하르트 슈뢰더</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">sklearn에서 모든 견적 목록을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">선택한 피처의 마스크 또는 정수 인덱스를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">문자열에서 득점자 확보</target>
        </trans-unit>
        <trans-unit id="892dda63b5e110479cdb36e62f1ec2fd9807071b" translate="yes" xml:space="preserve">
          <source>Get a scorer from string.</source>
          <target state="translated">문자열에서 득점자를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="9077d712fdd7764174aa9d64af32e58e63a20fd2" translate="yes" xml:space="preserve">
          <source>Get data and node arrays.</source>
          <target state="translated">데이터 및 노드 배열을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">모든 변압기에서 기능 이름을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="29212f8ab4fb514c62f70555a85d5fbb976ec617" translate="yes" xml:space="preserve">
          <source>Get number of calls.</source>
          <target state="translated">통화 수를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">이 추정기에 대한 모수를 얻으십시오.</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">이 커널의 매개 변수를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">진단 목적으로 교차 검증의 각 분할에서 예측을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">문자열 식별자에서 주어진 거리 메트릭을 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="d1f7f6e0092ef00a29852a7f857762682fb899cd" translate="yes" xml:space="preserve">
          <source>Get the parameters of an estimator from the ensemble.</source>
          <target state="translated">앙상블에서 추정기의 매개 변수를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">VotingClassifier의 매개 변수 가져 오기</target>
        </trans-unit>
        <trans-unit id="44fa9d84cdb2287aa5766955eab26611c0998b04" translate="yes" xml:space="preserve">
          <source>Get tree status.</source>
          <target state="translated">트리 상태를 가져옵니다.</target>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">정밀 행렬의 게터.</target>
        </trans-unit>
        <trans-unit id="24670d1cd19283e4b5f2e1096ab423493375ec8f" translate="yes" xml:space="preserve">
          <source>Gibbs sampling from visible and hidden layers.</source>
          <target state="translated">보이는 레이어와 숨겨진 레이어에서 Gibbs 샘플링.</target>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">2 개의 다변량 코 발리 2 차원 데이터 세트, X 및 Y가 주어지면, PLS는 '공분산의 방향', 즉 두 데이터 세트 간의 가장 공유 된 분산을 설명하는 각 데이터 세트의 구성 요소를 추출합니다. 이것은 &lt;strong&gt;산점도 매트릭스&lt;/strong&gt; 디스플레이 에서 명백하다 : 데이터 세트 X 및 데이터 세트 Y의 성분 1은 최대로 상관된다 (점은 첫 번째 대각선 주위에있다). 두 데이터 세트의 구성 요소 2에서도 마찬가지입니다. 그러나 다른 구성 요소에 대한 데이터 세트 간의 상관 관계는 약합니다. 포인트 클라우드는 매우 구형입니다.</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">반복 \ (t \)에 대한 후보 중심 \ (x_i \)이 주어지면 후보는 다음 방정식에 따라 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">두 가지 기능이있는 데이터 세트가 제공되면 인코더가 기능 당 고유 한 값을 찾고 데이터를 이진 one-hot 인코딩으로 변환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">두 개의 특징을 가진 데이터 세트가 주어지면, 우리는 인코더가 특징 당 고유 값을 찾고 데이터를 서수 인코딩으로 변환 할 수있게합니다.</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">\ ((x_1, y_1), (x_2, y_2), \ ldots, (x_n, y_n) \) 일련의 학습 예제가 제공됩니다. 여기서 \ (x_i \ in \ mathbf {R} ^ n \) 및 \ (y_i \ \ {0, 1 \} \)에서 하나의 숨겨진 레이어 하나의 숨겨진 뉴런 MLP는 \ (f (x) = W_2 g (W_1 ^ T x + b_1) + b_2 \) 함수를 학습합니다. 여기서 \ (W_1 \ in \ mathbf {R} ^ m \) 및 \ (W_2, b_1, b_2 \ in \ mathbf {R} \)은 모델 매개 변수입니다. \ (W_1, W_2 \)는 각각 입력 레이어와 숨겨진 레이어의 가중치를 나타냅니다. \ (b_1, b_2 \)는 각각 숨겨진 레이어와 출력 레이어에 추가 된 바이어스를 나타냅니다. \ (g (\ cdot) : R \ rightarrow R \)는 활성화 함수이며 기본적으로 쌍곡선으로 설정됩니다. 다음과 같이 주어진다.</target>
        </trans-unit>
        <trans-unit id="c774496cc27fa29850b7be4385a4e837807fe19c" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \mathcal{R}\) (\(y_i \in {-1, 1}\) for classification), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions for binary classification, we simply look at the sign of \(f(x)\). To find the model parameters, we minimize the regularized training error given by</source>
          <target state="translated">일련의 학습 예제 \ ((x_1, y_1), \ ldots, (x_n, y_n) \)가 주어지면 \ (x_i \ in \ mathbf {R} ^ m \) 및 \ (y_i \ in \ mathcal {R} \) (\ (y_i \ in {-1, 1} \)), 우리의 목표는 모델 매개 변수 \ (w \ in \ mathbf {R} ^ m \) 및 가로 채기 \ (b \ in \ mathbf {R} \). 이진 분류에 대한 예측을하기 위해 우리는 단순히 \ (f (x) \)의 부호를 살펴 봅니다. 모델 매개 변수를 찾기 위해 다음과 같은 정규화 된 학습 오류를 최소화합니다.</target>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">\ (x_i \ in \ mathbf {R} ^ m \) 및 \ (y_i \ in \ {-1, 1 \} \), 우리의 목표는 모델 매개 변수 \ (w \ in \ mathbf {R} ^ m \)를 가진 선형 채점 함수 \ (f (x) = w ^ T x + b \)를 배우고 가로채는 것입니다. (b \ in \ mathbf {R} \)입니다. 예측을하기 위해 간단히 \ (f (x) \)의 부호를 봅니다. 모델 매개 변수를 찾는 일반적인 선택은 다음과 같이 정규화 된 훈련 오류를 최소화하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">예외, 예외를 발생시킬 수있는 호출 가능 및 메시지 문자열이 제공되면 올바른 예외가 발생하고 메시지가 발생한 오류의 하위 문자열인지 테스트합니다. 예외 중에 발생 된 특정 메시지가 올바른지 테스트하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">특징들 (예를 들어, 선형 모델의 계수들)에 가중치들을 할당하는 외부 추정기가 주어지면, 재귀 특징 제거 ( &lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt; )는 더 작고 더 작은 특징들의 세트를 재귀 적으로 고려함으로써 특징들을 선택하는 것이다. 먼저 추정기는 초기 기능 세트에 대해 &lt;code&gt;coef_&lt;/code&gt; 하고 각 기능의 중요성은 coef_ 속성 또는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 얻습니다 . 그런 다음, 가장 중요한 기능은 현재 기능 세트에서 정리됩니다.이 절차는 선택할 원하는 기능 수가 결국 도달 할 때까지 정리 된 세트에서 반복적으로 반복됩니다.</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">피처에 가중치를 할당하는 외부 추정기 (예 : 선형 모델의 계수)를 고려할 때, 재귀 피처 제거 (RFE)의 목표는 더 작고 더 작은 피처 세트를 재귀 적으로 고려하여 피처를 선택하는 것입니다. 먼저 추정기는 초기 기능 세트에 대해 &lt;code&gt;coef_&lt;/code&gt; 하고 각 기능의 중요성은 coef_ 속성 또는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 얻습니다 . 그런 다음 가장 중요한 기능이 현재 기능 세트에서 제거됩니다. 선택할 수있는 피처 수에 도달 할 때까지 제거 된 세트에서 해당 절차를 반복적으로 반복합니다.</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">충분한 시간이 주어지면 K-means는 항상 수렴되지만 이는 로컬 최소값 일 수 있습니다. 이것은 중심의 초기화에 크게 의존합니다. 결과적으로 계산은 종종 중심의 초기화가 다른 여러 번 수행됩니다. 이 문제를 해결하는 한 가지 방법은 scikit-learn에서 구현 된 k-means ++ 초기화 체계입니다 ( &lt;code&gt;init='k-means++'&lt;/code&gt; 매개 변수 사용). 이것은 중심에서 (일반적으로) 서로 떨어져 있도록 초기화하며, 참조에 표시된 것처럼 무작위 초기화보다 결과가 더 좋습니다.</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">우리는 조리개의 3 종류가 있다고 알고 있지만 레이블을 분류 학자에 액세스하지 않은 경우, 홍채 데이터 집합을 감안할 때 : 우리는 시도 할 수 &lt;strong&gt;클러스터링 작업을&lt;/strong&gt; : 잘 분리 된 그룹이라고에 관찰을 분할 &lt;em&gt;클러스터&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">클래스 지정이 지상 진리의 지식을 감안할 때 &lt;code&gt;labels_true&lt;/code&gt; 과 같은 샘플 우리의 클러스터링 알고리즘 할당 &lt;code&gt;labels_pred&lt;/code&gt; 을 의 &lt;strong&gt;상호 정보&lt;/strong&gt; 함수입니다 측정 &lt;strong&gt;계약&lt;/strong&gt; 순열을 무시하고 두 과제. 이 측정의 두 가지 정규화 된 버전 인 &lt;strong&gt;NMI (Normalized Mutual Information)&lt;/strong&gt; 와 &lt;strong&gt;AMI (Adjusted Mutual Information)가&lt;/strong&gt; 있습니다. NMI는 종종 문헌에 사용되는 반면, AMI는 최근에 제안되었으며 &lt;strong&gt;우연히 정규화되었습니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">클래스 지정이 지상 진리의 지식을 감안할 때 &lt;code&gt;labels_true&lt;/code&gt; 과 같은 샘플 우리의 클러스터링 알고리즘 할당 &lt;code&gt;labels_pred&lt;/code&gt; 의 &lt;strong&gt;조정 랜드 지수&lt;/strong&gt; 를 측정하는 기능입니다 &lt;strong&gt;유사성&lt;/strong&gt; 두 과제, 무시 순열과 &lt;strong&gt;기회의 정상화와 함께&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">샘플의 기본 진리 클래스 할당에 대한 지식이 주어지면 조건부 엔트로피 분석을 사용하여 직관적 인 메트릭을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">이러한 특이 벡터를 고려할 때, 이들은 조각 별 상수 벡터로 가장 잘 추정 할 수있는 순위에 따라 순위가 매겨집니다. 각 벡터에 대한 근사는 1 차원 k- 평균을 사용하여 찾아 유클리드 거리를 사용하여 점수를 매 깁니다. 가장 좋은 왼쪽 및 오른쪽 특이 벡터의 일부가 선택됩니다. 다음으로, 데이터는이 단일 벡터의 가장 좋은 부분 집합에 투영되고 클러스터됩니다.</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">학습 벡터 \ (x_i \ in R ^ n \), i = 1,&amp;hellip;, l 및 레이블 벡터 \ (y \ in R ^ l \)가 주어지면 의사 결정 트리는 공간이 재귀 적으로 분할되어 샘플이 동일한 레이블이 함께 그룹화됩니다.</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">주어진 훈련 벡터 \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n 및 벡터 \ (y \ in \ mathbb {R} ^ n \) \ (\ varepsilon \)- SVR은 다음과 같은 초기 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">주어진 훈련 벡터 \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n, 두 클래스, 벡터 \ (y \ in \ {1, -1 \} ^ n \) SVC는 다음과 같은 초기 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="e43c2f871d10fa4875c4f15e109aeb5faf94fb18" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), our goal is to find \(w \in \mathbb{R}^p\) and \(b \in \mathbb{R}\) such that the prediction given by \(\text{sign} (w^T\phi(x) + b)\) is correct for most samples.</source>
          <target state="translated">주어진 훈련 벡터 \ (x_i \ in \ mathbb {R} ^ p \), i = 1,&amp;hellip;, n, 두 클래스 및 벡터 \ (y \ in \ {1, -1 \} ^ n \) , 우리의 목표는 \ (\ text {sign} (w ^ T \ phi (x) + b) \)는 대부분의 샘플에 맞습니다.</target>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">(복잡한) 샘플링 포인트 수를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ac4e9c94eac5d688eef08c9122f5d38187b8f922" translate="yes" xml:space="preserve">
          <source>Global min and max average predictions, such that all plots will have the same scale and y limits. &lt;code&gt;pdp_lim[1]&lt;/code&gt; is the global min and max for single partial dependence curves. &lt;code&gt;pdp_lim[2]&lt;/code&gt; is the global min and max for two-way partial dependence curves.</source>
          <target state="translated">모든 플롯이 동일한 스케일 및 y 한계를 갖도록 전역 최소 및 최대 평균 예측. &lt;code&gt;pdp_lim[1]&lt;/code&gt; 은 단일 부분 종속성 곡선에 대한 전역 최소 및 최대입니다. &lt;code&gt;pdp_lim[2]&lt;/code&gt; 는 양방향 부분 종속성 곡선에 대한 전역 최소 및 최대입니다.</target>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">전역 구조는 명시 적으로 보존되지 않습니다. 이 문제는 PCA로 포인트를 초기화하면 ( &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; 사용) 완화됩니다 .</target>
        </trans-unit>
        <trans-unit id="01649050ef673ff19d8d011219103526d0d7370d" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using &lt;code&gt;init='pca'&lt;/code&gt;).</source>
          <target state="translated">전역 구조는 명시 적으로 보존되지 않습니다. 이 문제는 PCA로 포인트를 초기화함으로써 완화됩니다 ( &lt;code&gt;init='pca'&lt;/code&gt; 사용 ).</target>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot, Xavier 및 Yoshua Bengio. &amp;ldquo;의 어려움을 이해</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">Golub과 C. Van Loan. 매트릭스 계산, 3 판, 5 장,</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin, (2004). K- 카테고리 상관 계수에 의한 두 K- 카테고리 할당 비교</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">그라디언트 부스팅 대역 외 추정</target>
        </trans-unit>
        <trans-unit id="ff01958eb0f121764b2210794dcbe435f29dcc7a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Regression Trees for Poisson regression</source>
          <target state="translated">포아송 회귀를위한 경사 부스팅 회귀 트리</target>
        </trans-unit>
        <trans-unit id="9396c57fff04d750ce06a05cfd3c756b4f971532" translate="yes" xml:space="preserve">
          <source>Gradient Boosting also gives the possibility to fit the trees with a Poisson loss (with an implicit log-link function) instead of the default least-squares loss. Here we only fit trees with the Poisson loss to keep this example concise.</source>
          <target state="translated">Gradient Boosting은 또한 기본 최소 제곱 손실 대신 포아송 손실 (암시 적 로그 링크 함수 사용)로 트리를 맞출 가능성을 제공합니다. 여기서는이 예제를 간결하게 유지하기 위해 Poisson 손실이있는 나무 만 맞 춥니 다.</target>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">그라디언트 부스팅은 가장 가파른 하강을 통해이 최소화 문제를 수치 적으로 해결하려고 시도합니다. 가장 가파른 하강 방향은 현재 모델 \ (F_ {m-1} \)에서 평가 된 손실 함수의 음의 구배입니다.</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">분류를위한 그라디언트 부스팅.</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">회귀에 대한 그라디언트 부스팅.</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">그라디언트 부스팅 회귀</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">그라디언트 부스팅 정규화</target>
        </trans-unit>
        <trans-unit id="a558a9ccdbbb397deb97e7223684a95578fb2ba7" translate="yes" xml:space="preserve">
          <source>Gradient boosting for classification is very similar to the regression case. However, the sum of the trees \(F_M(x_i) = \sum_m h_m(x_i)\) is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values.</source>
          <target state="translated">분류를위한 기울기 부스팅은 회귀 사례와 매우 유사합니다. 그러나 트리의 합계 \ (F_M (x_i) = \ sum_m h_m (x_i) \)는 예측과 동일하지 않습니다. 트리가 연속 값을 예측하기 때문에 클래스가 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">그라디언트 부스팅은 몇 가지 약한 학습자 (회귀 트리)를 결합하여 반복적 인 방식으로 강력한 단일 모델을 생성하는 조립 기술입니다.</target>
        </trans-unit>
        <trans-unit id="692996b3838fb57cde4b100ea1ec7f66fff47afe" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="translated">세타 위치에서 커널 하이퍼 파라미터에 대한 로그 한계 가능성의 기울기입니다. &lt;code&gt;eval_gradient&lt;/code&gt; 가 True 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">위치 세타에서의 커널 하이퍼 파라미터에 대한 로그-마진 가능성의 구배. eval_gradient가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="e64c4914bb8a27678b7a6969455bd717adc62d09" translate="yes" xml:space="preserve">
          <source>GradientBoostingRegressor</source>
          <target state="translated">GradientBoostingRegressor</target>
        </trans-unit>
        <trans-unit id="e2fb5831cb5dd547c1703af3319394a3f8535468" translate="yes" xml:space="preserve">
          <source>Gram = np.dot(X.T * X).</source>
          <target state="translated">그램 = np.dot (XT * X).</target>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">그램 직교 매칭 추구 (OMP)</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">입력 데이터의 그램 행렬 : XT * X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">그래프 거리 (예 : 가장 가까운 이웃 그래프)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">픽셀 간 연결 그래프</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">픽셀 간 그라디언트 연결 그래프</target>
        </trans-unit>
        <trans-unit id="a291a5c559f789dd92fe83af065257b966fe9953" translate="yes" xml:space="preserve">
          <source>Graph where A[i, j] is assigned the weight of edge that connects i to j. The matrix is of CSR format.</source>
          <target state="translated">A [i, j]에 i를 j에 연결하는 간선의 가중치가 할당 된 그래프입니다. 매트릭스는 CSR 형식입니다.</target>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">교차 검증에 사용되는 C 그리드.</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">각각에 대해 별개의 수의 값을 갖는 매개 변수 그리드.</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 일부로 분류기의 하이퍼 파라미터와 함께 &lt;code&gt;ColumnTransformer&lt;/code&gt; 객체에 정의 된 다양한 전처리 단계에서 그리드 검색을 수행 할 수도 있습니다 . 우리는 숫자 전처리의 imputer 전략 및 사용하여 로지스틱 회귀의 정규화 매개 변수를 모두 검색합니다 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">그리드 검색 및 교차 검증 추정기</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCV는 &quot;적합&quot;및 &quot;점수&quot;방법을 구현합니다. 또한 &quot;추정&quot;, &quot;predict_proba&quot;, &quot;decision_function&quot;, &quot;transform&quot;및 &quot;inverse_transform&quot;이 사용 된 추정기에서 구현 된 경우 구현합니다.</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas, 이오 아니스 카타 키스. 다중 레이블 분류 : 개요. 국제 데이터웨어 하우징 및 저널, 3 (3), 1-13, 2007 년 7 월 -9 월.</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">n_samples 샘플에 대한 정확한 사실 (올바른) 레이블.</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">진실 (정확한) 라벨.</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">진실 (올바른) 목표 값.</target>
        </trans-unit>
        <trans-unit id="b29893e6134ceb0ae63100b750ea64cd6227af16" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt; 0.</source>
          <target state="translated">Ground Truth (올바른) 목표 값. y_true&amp;gt; 0이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="8dcf4476037b86142a6c723cc6d58c74ce6fa30f" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt;= 0.</source>
          <target state="translated">Ground Truth (올바른) 목표 값. y_true&amp;gt; = 0이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">참조로 사용할지면 진실 클래스 레이블</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">데이터 세트를 기차 / 테스트 세트로 분할하는 동안 사용 된 샘플의 레이블을 그룹화합니다.</target>
        </trans-unit>
        <trans-unit id="da0d044e30ddccc2bad0f6e17da06a788ea2385a" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a &amp;ldquo;Group&amp;rdquo; &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; instance (e.g., &lt;a href=&quot;sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold&quot;&gt;&lt;code&gt;GroupKFold&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">데이터 세트를 학습 / 테스트 세트로 분할하는 동안 사용 된 샘플의 라벨을 그룹화합니다. &quot;Group&quot; &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; 인스턴스 와 함께 만 사용됩니다 (예 : &lt;a href=&quot;sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold&quot;&gt; &lt;code&gt;GroupKFold&lt;/code&gt; &lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">데이터 세트를 기차 / 테스트 세트로 분할하는 동안 사용 된 샘플의 레이블을 그룹화합니다. 다른 '파라미터'는 생략 할 수 있지만이 '그룹'파라미터는 항상 스플릿 수를 계산하기 위해 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; 가 있는 트리를 가장 먼저 성장 시킵니다 . 최상의 노드는 불순물의 상대적 감소로 정의됩니다. None 인 경우 무제한의 리프 노드.</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">&lt;code&gt;max_leaf_nodes&lt;/code&gt; 로 나무를 가장 먼저 성장 시키십시오 . 최상의 노드는 불순물의 상대적 감소로 정의됩니다. None 인 경우 무제한의 리프 노드.</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">Guide</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">Guyon, I., Weston, J., Barnhill, S. 및 Vapnik, V.,&amp;ldquo;서포트 벡터 머신을 사용한 암 분류를위한 유전자 선택&amp;rdquo;, Mach. 배우십시오., 46 (1-3), 389&amp;ndash;422, 2002.</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;나이브 베이 즈의 최적 성. &lt;/a&gt;Proc. 플레어.</target>
        </trans-unit>
        <trans-unit id="ce3bde746d403806636c8d155597ef91ca7e1f03" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">H. Zhang (2004). &lt;a href=&quot;https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;Naive Bayes의 최적 성. &lt;/a&gt;Proc. FLAIRS.</target>
        </trans-unit>
        <trans-unit id="de489f31c1f185d4a81f0399ead4e066a95b91be" translate="yes" xml:space="preserve">
          <source>HTML representation of &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; HTML 표현</target>
        </trans-unit>
        <trans-unit id="d930a6037b9120a42017959402d9dc27dd6bf69c" translate="yes" xml:space="preserve">
          <source>HTML representation of estimator.</source>
          <target state="translated">추정 자의 HTML 표현.</target>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">하가이 아티 아스. (2000). &amp;ldquo;그래픽 모델을위한 변형 베이지안 프레임 워크&amp;rdquo;. 신경 정보 처리 시스템의 발전 12.</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">할 키디, 마리아; 바티스타 키스, 야 니스; Vazirgiannis, Michalis (2001). &amp;ldquo;클러스터링 유효성 검사 기술&amp;rdquo;Journal of Intelligent Information Systems, 17 (2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi : 10.1023 / A : 1012801612483&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="959a6f4c185bd74a74d43205ed3cc9281eca4d45" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;https://doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">할 키디, 마리아; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo;Journal of Intelligent Information Systems, 17 (2-3), 107-145. &lt;a href=&quot;https://doi.org/10.1023/A:1012801612483&quot;&gt;doi : 10.1023 / A : 1012801612483&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="9757089e5251a143827d61ff72e389c7fd386869" translate="yes" xml:space="preserve">
          <source>Hand, D.J. and Till, R.J., (2001). &lt;a href=&quot;http://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;A simple generalisation of the area under the ROC curve for multiple class classification problems.&lt;/a&gt; Machine learning, 45(2), pp.171-186.</source>
          <target state="translated">Hand, DJ and Till, RJ, (2001). &lt;a href=&quot;http://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;다중 클래스 분류 문제에 대한 ROC 곡선 아래 영역의 단순 일반화. &lt;/a&gt;기계 학습, 45 (2), pp.171-186.</target>
        </trans-unit>
        <trans-unit id="88b6ef37ba2f9ba619bbf453c13dce1665119f21" translate="yes" xml:space="preserve">
          <source>Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.</source>
          <target state="translated">손, DJ, Till, RJ (2001). 다중 클래스 분류 문제에 대한 ROC 곡선 아래 영역의 간단한 일반화. 기계 학습, 45 (2), 171-186.</target>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">출력 파일의 핸들 또는 이름 경우 &lt;code&gt;None&lt;/code&gt; , 결과는 문자열로 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="528b68d16981ccbe32f7d51bc822d75e077c8b80" translate="yes" xml:space="preserve">
          <source>Handling Multicollinear Features</source>
          <target state="translated">다중 공선 피쳐 처리</target>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">백엔드를 선택하기 어려운 제약 조건 'sharedmem'으로 설정하면 사용자가 parallel_backend를 사용하여 스레드가 아닌 기반 백엔드를 요청한 경우에도 선택한 백엔드는 단일 호스트 및 스레드 기반이됩니다.</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">솔버 내 반복에 대한 하드 제한 또는 제한이없는 경우 -1</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">해리슨, D.와 루빈 펠트, DL</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">트리의 해시 함수 g (p, x)는 데이터 세트와 동일한 차원을 갖는 임의로 생성 된 32 개의 부동 소수점 배열로 구성된 배열입니다. 이 배열은 GaussianRandomProjectionHash 객체에 저장되며 &lt;code&gt;components_&lt;/code&gt; 속성 에서 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">완전 랜덤 트리를 사용한 해싱 기능 변환</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizer는 상태 비 저장 모델이므로 IDF 가중치를 제공하지 않습니다 (맞춤 방법은 아무 것도 수행하지 않음). IDF 가중치가 필요한 경우 출력을 TfidfTransformer 인스턴스에 파이프 라이닝하여 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizer는 충돌 가능성이있는 고정 된 차원 공간으로 단어 어커런스를 해시합니다. 단어 카운트 벡터는 각각 1- 규모 (유클리드 단위-볼에 투영 됨)와 동일한 l2-norm을 갖도록 정규화되는데, 이는 k- 평균이 고차원 공간에서 작동하는 데 중요한 것으로 보인다.</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie, R. Tibshirani 및 J. Friedman,&amp;ldquo;통계 학습의 요소 Ed. 2&amp;rdquo;, Springer, 2009.</target>
        </trans-unit>
        <trans-unit id="9803f456bd9f04731b6843d220c5ae89fa289aa2" translate="yes" xml:space="preserve">
          <source>Haussler, D. (1999). Convolution kernels on discrete structures (Vol. 646). Technical report, Department of Computer Science, University of California at Santa Cruz.</source>
          <target state="translated">Haussler, D. (1999). 이산 구조에 대한 컨볼 루션 커널 (Vol. 646). 산타 크루즈에있는 캘리포니아 대학교 컴퓨터 과학과 기술 보고서.</target>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">상기 봐 가지고 &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;해싱 벡터화&lt;/a&gt; 에 메모리 효율적인 대안으로 &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">사용하여 한 번 봐 가지고 &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;밖으로의 핵심 분류&lt;/a&gt; 컴퓨터 메인 메모리에 맞지 않을 데이터로부터 배울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">그는 Kaiming 등 &amp;ldquo;정류기 심층 분석 : 인간 수준을 능가</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">읽을 수있는 병렬 매핑을위한 도우미 클래스</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">예외에서 발생한 메시지를 테스트하는 도우미 기능</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">따라서 입력 공간에 64 개의 피처 만있는 숫자 데이터 세트에 임의의 투영을 사용하는 것은 의미가 없습니다.이 경우 치수 축소를 허용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">따라서 훈련 코퍼스에서 볼 수 없었던 단어는 나중에 transform 메소드 호출에서 완전히 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">따라서 없음의 경우 결과는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">여기서 &lt;code&gt;func&lt;/code&gt; 은 두 개의 1 차원 numpy 배열을 가져 와서 거리를 반환하는 함수입니다. BallTree 내에서 사용하려면 거리가 실제 측정 항목이어야합니다. 즉, 다음 속성을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">여기서 사인 함수는 0에 가까운 값에 대해 차수 3의 다항식에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">다음은 이진 클래스 문제에서 svm 분류기와 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수를 사용하는 방법을 보여주는 작은 예제입니다 .</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">다음은 가우스 랜덤 프로젝션 변압기를 사용하는 방법을 보여주는 작은 발췌문입니다.</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">다음은 희소 랜덤 프로젝션 변압기를 사용하는 방법을 보여주는 작은 발췌문입니다.</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">이 자습서를 마치면 scikit-learn 직관을 향상시키는 데 도움이되는 몇 가지 제안이 있습니다.</target>
        </trans-unit>
        <trans-unit id="3dda6ea8d57e795e10f1bb02b3e190ba1eee1ee3" translate="yes" xml:space="preserve">
          <source>Here are some examples demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input.</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 다중 레이블 표시기 행렬 입력 문제에서 각 클래스에 대한 재현율 (또는 민감도), 특이성, 폴 아웃 및 실패율을 계산 하는 몇 가지 예 입니다.</target>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">표준 컬럼 데이터를 scikit-learn이 사용할 수있는 형식으로로드하는 몇 가지 권장 방법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">이진 분류의 작은 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">다음은 다양한 작업을위한 증분 추정기 목록입니다.</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">다음은 쿼드 코어 머신에서 실행 한 샘플 출력입니다.</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">이 목표를 달성하기 위해 설계된 시스템의 스케치는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt; 함수 의 사용법을 보여주는 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예는 다음과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="1875c837ecf1df623da5e8546dcb195bb9e83c64" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt;&lt;code&gt;max_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt; &lt;code&gt;max_error&lt;/code&gt; &lt;/a&gt; 함수 사용의 간단한 예입니다 .</target>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 함수 사용법의 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">이 함수의 사용법에 대한 작은 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">이 함수를 사용하는 간단한 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">사용 예는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">다음은 이러한 혼동 행렬을 시각적으로 나타낸 것입니다 (이 그림은 &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;혼동 행렬&lt;/a&gt; 예 에서 나옴 ).</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다. 참고 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; 이&lt;/a&gt; 클래스 또는 그룹의 영향을받지 않습니다.</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">교차 유효성 검사 동작의 시각화는 다음과 같습니다. 참고 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 이&lt;/a&gt; 클래스 또는 그룹의 영향을받지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">다음은 멀티 클래스 문제에서 svm 분류기와 &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수를 사용하는 방법을 보여주는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="01baca5f1060ff615b57707b27b89349c3831f22" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; input:</source>
          <target state="translated">다음은 &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt; 클래스&lt;/a&gt; 입력 과 함께 multilabel_confusion_matrix 함수 의 사용을 보여주는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="ac06b69a6bbd9ae081c446d18662dd3517d588fd" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel-indicator-matrix&quot;&gt;multilabel indicator matrix&lt;/a&gt; input:</source>
          <target state="translated">여기서 사용 보여주는 예이다 &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; 를&lt;/a&gt; 가진 함수 &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel-indicator-matrix&quot;&gt;다중 레벨 표시기 매트릭스&lt;/a&gt; 입력 :</target>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">단일 메트릭을 사용하는 &lt;code&gt;cross_validate&lt;/code&gt; 의 예는 다음과 같습니다 .</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">다음은 다양한 각도의 다항식 특징을 사용하여이 아이디어를 1 차원 데이터에 적용하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">다음은 사용자 지정 채점자를 작성하고 &lt;code&gt;greater_is_better&lt;/code&gt; 매개 변수 를 사용하는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="3be41bccb12847b90804b0be88468f33593d4dc5" translate="yes" xml:space="preserve">
          <source>Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">다음은 두 개의 불균형 클래스의 50 개 샘플을 사용하여 데이터 세트에 대한 계층화 된 3 중 교차 검증의 예입니다. 각 클래스의 샘플 수를 표시하고 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 와 비교 합니다.</target>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">다음은 장난감 데이터 매트릭스를 &lt;code&gt;[0, 1]&lt;/code&gt; 범위 로 스케일링하는 예입니다 .</target>
        </trans-unit>
        <trans-unit id="04b3257d3ad37f9ca0ccbd79a367325c6e1ed5f4" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">다음은 &lt;code&gt;elasticnet&lt;/code&gt; 패널티 와 함께 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt; 를 사용하는 예 입니다. 정규화 강도는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수에 의해 전역 적으로 제어됩니다 . 충분히 높은으로 &lt;code&gt;alpha&lt;/code&gt; 한 다음 높일 수 &lt;code&gt;l1_ratio&lt;/code&gt; 의 파라미터 &lt;code&gt;elasticnet&lt;/code&gt; 을 모델 계수 희소성 다양한 레벨을 적용. 여기에서 희소성이 높을수록 모델 복잡도가 적다고 해석됩니다.이를 완전히 설명하는 데 더 적은 계수가 필요하기 때문입니다. 물론 희소 내적은 0이 아닌 계수의 수에 대략적으로 비례하는 시간이 걸리기 때문에 희소성은 예측 시간에 영향을 미칩니다.</target>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">다음은 &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; 를 &lt;code&gt;elasticnet&lt;/code&gt; 패널티 와 함께 사용하는 예 입니다. 정규화 강도는 &lt;code&gt;alpha&lt;/code&gt; 매개 변수에 의해 전체적으로 제어됩니다 . 충분히 높은 &lt;code&gt;alpha&lt;/code&gt; , 모델 계수에서 다양한 레벨의 희소성을 강제하기 위해 &lt;code&gt;elasticnet&lt;/code&gt; 의 &lt;code&gt;l1_ratio&lt;/code&gt; 파라미터 를 증가시킬 수 있습니다 . 여기서 희소성이 클수록 모델을 완전히 설명하기 위해 더 적은 계수가 필요하므로 모델 복잡성이 줄어 듭니다. 물론 희소 도트 곱은 0이 아닌 계수의 수에 대략 비례하는 시간이 걸리기 때문에 희소성이 차례로 예측 시간에 영향을 미칩니다.</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">예를 들면 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">이 스케일러에서 이전 예제의 장난감 데이터를 사용하는 방법은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">다음은 &lt;code&gt;sparsify()&lt;/code&gt; 메서드 사용을 보여주는 샘플 코드입니다 .</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">다음은 입력 희소성을 테스트하는 샘플 코드입니다.</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">다음은 자동 모델 선택을위한 AIC (Akaike Information Criterion) 또는 BIC (Bayesian Information Criterion)의 이점이있는 모델 목록입니다.</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">이러한 모델 목록은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="757c8807092bec583b3c00400f122f638dd1b02a" translate="yes" xml:space="preserve">
          <source>Here one can observe that the train accuracy is very high (the forest model has enough capacity to completely memorize the training set) but it can still generalize well enough to the test set thanks to the built-in bagging of random forests.</source>
          <target state="translated">여기에서 열차 정확도가 매우 높지만 (포리스트 모델은 훈련 세트를 완전히 기억할 수있는 충분한 용량을 가지고 있음) 랜덤 포레스트의 내장 배깅 덕분에 테스트 세트에 충분히 일반화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="15f3441e24a8e858a11c375d5c2fee3bc8aa09ba" translate="yes" xml:space="preserve">
          <source>Here our goal goal is to predict the expected value, i.e. the mean, of the total claim amount per exposure unit also referred to as the pure premium.</source>
          <target state="translated">여기서 우리의 목표는 순수 프리미엄이라고도하는 노출 단위당 총 청구 금액의 예상 가치, 즉 평균을 예측하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="785dc58756e0e128c51d97262d985997dfc75263" translate="yes" xml:space="preserve">
          <source>Here the &lt;code&gt;transform&lt;/code&gt; operation returns \(LX^T\), therefore its time complexity equals &lt;code&gt;n_components * n_features * n_samples_test&lt;/code&gt;. There is no added space complexity in the operation.</source>
          <target state="translated">여기서 &lt;code&gt;transform&lt;/code&gt; 연산은 \ (LX ^ T \)를 반환하므로 시간 복잡도는 &lt;code&gt;n_components * n_features * n_samples_test&lt;/code&gt; 와 같습니다 . 작업에 추가 된 공간 복잡성이 없습니다.</target>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">여기에서 scikit-learn으로 구현 된 Martinsson의 Randomized SVD 알고리즘 덕분에 계산이 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">여기서 정규화 매개 변수 C에 대한 우리의 선택이 최선이 아니기 때문에 결과가 좋지 않을 수 있습니다. 실제 애플리케이션에서이 매개 변수는 일반적으로 &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;추정기의 하이퍼 파라미터 튜닝을&lt;/a&gt; 사용하여 선택됩니다 .</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">여기서는 SA1 솔버를 선택하는데, 이는 매끄럽고 드문 드문 한 l1 페널티로 로지스틱 회귀 손실을 효율적으로 최적화 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="15d9d9f74de48f0968b756664a2f90e7435e3e3c" translate="yes" xml:space="preserve">
          <source>Here we choose the liblinear solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">여기서 우리는 l1 페널티를 유발하는 평활하지 않은 희소성으로 로지스틱 회귀 손실을 효율적으로 최적화 할 수있는 liblinear 솔버를 선택합니다.</target>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">여기서 우리는 3 가지 접근법을 비교합니다 :</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">여기 우리는 Dirichlet 프로세스 혼합물에 변형 추론 알고리즘을 설명합니다. Dirichlet 프로세스는 &lt;em&gt;무한한 무제한의 파티션&lt;/em&gt; 으로 &lt;em&gt;클러스터링&lt;/em&gt; 에 대한 사전 확률 분포입니다 . 변형 기술을 사용하여 유한 가우시안 혼합 모델과 비교하여 추론 시간에 거의 페널티없이 가우시안 혼합 모델에이 사전 구조를 통합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">여기서 우리는 MNIST 숫자 분류 작업의 부분 집합에 L1 페널티를 갖는 다항 로지스틱 회귀 분석에 적합합니다. 우리는이를 위해 SAGA 알고리즘을 사용합니다. 이것은 샘플 수가 피처 수보다 훨씬 많을 때 빠르며 l1 페널티의 경우와 같이 부드럽 지 않은 목적 함수를 미세하게 최적화 할 수있는 솔버입니다. 테스트 정확도는&amp;gt; 0.8에 도달하는 반면 가중치 벡터는 &lt;em&gt;희박&lt;/em&gt; 하게 유지 되므로 더 쉽게 &lt;em&gt;해석&lt;/em&gt; 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">여기 에서 위에서 본 것처럼 &lt;code&gt;kernel='gaussian'&lt;/code&gt; 을 사용했습니다. 수학적으로 커널은 긍정적 인 함수 \ (K (x; h) \)이며 대역폭 매개 변수 \ (h \)에 의해 제어됩니다. 이 커널 형식이 주어지면 점 그룹 \ (x_i; i = 1 \ cdots N \) 내의 점 \ (y \)에서의 밀도 추정값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">여기서는 자유도가 낮은 2 명의 학생 T 인 비 가우시안 프로세스 (왼쪽 상단 그림)를 사용하여 독립적 인 소스를 시뮬레이션합니다. 우리는 그것들을 혼합하여 관찰을 만듭니다 (오른쪽 상단 그림). 이 원시 관측 공간에서 PCA로 식별되는 방향은 주황색 벡터로 표시됩니다. PCA 벡터 (왼쪽 아래)에 해당하는 분산으로 미백 한 후 PCA 공간의 신호를 나타냅니다. ICA를 실행하면이 공간에서 가장 비 가우시안 방향 (오른쪽 아래)을 식별하기 위해 회전을 찾는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="72e463d9d9e7402af61cb976f70a41a655f66554" translate="yes" xml:space="preserve">
          <source>Here we use the caching property of pipelines to cache the nearest neighbors graph between multiple fits of KNeighborsClassifier. The first call is slow since it computes the neighbors graph, while subsequent call are faster as they do not need to recompute the graph. Here the durations are small since the dataset is small, but the gain can be more substantial when the dataset grows larger, or when the grid of parameter to search is large.</source>
          <target state="translated">여기서는 파이프 라인의 캐싱 속성을 사용하여 KNeighborsClassifier의 다중 맞춤 사이에서 가장 가까운 이웃 그래프를 캐시합니다. 첫 번째 호출은 이웃 그래프를 계산하기 때문에 느리고 후속 호출은 그래프를 다시 계산할 필요가 없기 때문에 더 빠릅니다. 여기서 데이터 세트가 작기 때문에 기간은 짧지 만 데이터 세트가 커지거나 검색 할 매개 변수 그리드가 클 때 이득이 더 클 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">여기서는 정보가없는 기능의 가중치를 0으로 자르는 l1 희소성을 사용합니다. 목표가 각 클래스의 강력한 차별적 어휘를 추출하는 것이라면 좋습니다. 최고의 예측 정확도를 얻는 것이 목표라면, 희소성을 유발하지 않는 l2 페널티를 대신 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="0013bebf729b11b2c5dfc0313efdfb116ab3b0c5" translate="yes" xml:space="preserve">
          <source>Here we want to model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; conditionally on &lt;code&gt;X&lt;/code&gt; via a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="translated">여기서 우리 는 (스케일 된) Poisson 분포를 통해 &lt;code&gt;X&lt;/code&gt; 에서 조건부로 빈도 &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; 를 모델링하고 &lt;code&gt;Exposure&lt;/code&gt; 를 &lt;code&gt;sample_weight&lt;/code&gt; 로 사용 하려고 합니다.</target>
        </trans-unit>
        <trans-unit id="034b3c08e48de8757b6f1f86830d0869f87a8e39" translate="yes" xml:space="preserve">
          <source>Here, &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; is the parameter name of the nested estimator, in this case &lt;code&gt;base_estimator&lt;/code&gt;. If the meta-estimator is constructed as a collection of estimators as in &lt;code&gt;pipeline.Pipeline&lt;/code&gt;, then &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; refers to the name of the estimator, see &lt;a href=&quot;compose#pipeline-nested-parameters&quot;&gt;Nested parameters&lt;/a&gt;. In practice, there can be several levels of nesting:</source>
          <target state="translated">여기서 &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; 는 중첩 된 추정기의 매개 변수 이름입니다 (이 경우 &lt;code&gt;base_estimator&lt;/code&gt; ) . meta-estimator가 &lt;code&gt;pipeline.Pipeline&lt;/code&gt; 에서 와 같이 추정기 모음으로 구성된 경우 &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; 는 추정기 의 이름을 참조합니다. &lt;a href=&quot;compose#pipeline-nested-parameters&quot;&gt;Nested parameters를&lt;/a&gt; 참조하세요 . 실제로 여러 수준의 중첩이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">여기서 \ (\ alpha \ geq 0 \)은 수축량을 제어하는 ​​복잡성 매개 변수입니다. \ (\ alpha \)의 값이 클수록 수축량이 커지므로 계수가 공선성에 대해보다 강력 해집니다.</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">여기서 분류자는 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; 를&lt;/a&gt; 사용하여 &lt;code&gt;y&lt;/code&gt; 의 2 차원 이진 레이블 표현에서 &lt;code&gt;fit()&lt;/code&gt; 입니다 . 이 경우 &lt;code&gt;predict()&lt;/code&gt; 는 해당 다중 레이블 예측을 나타내는 2d 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">여기서 기본 커널 &lt;code&gt;rbf&lt;/code&gt; 는 추정기가 구성된 후 &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt; 를 통해 먼저 &lt;code&gt;linear&lt;/code&gt; 변경되고 추정기 를 다시 맞추고 두 번째 예측을하기 위해 &lt;code&gt;rbf&lt;/code&gt; 로 다시 변경됩니다 .</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">여기에서 &lt;code&gt;iris.target&lt;/code&gt; (정수 배열)이 &lt;code&gt;fit&lt;/code&gt; 에서 사용 되었으므로 첫 번째 &lt;code&gt;predict()&lt;/code&gt; 는 정수 배열을 반환합니다 . &lt;code&gt;iris.target_names&lt;/code&gt; 가 적합하기 때문에 두 번째 &lt;code&gt;predict()&lt;/code&gt; 는 문자열 배열을 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">여기에서 표본의 수는 차원의 수보다 약간 크므로 경험적 공분산은 여전히 ​​되돌릴 수 없습니다. 그러나 관측치가 서로 밀접하게 연관되어 있기 때문에 경험적 공분산 행렬은 조건이 좋지 않으며 결과적으로 그 역수 &amp;ndash; 경험적 정밀 행렬 &amp;ndash;은 실제와는 거리가 멀다.</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">여기에서 예측 된 클래스 레이블은 평균 확률이 가장 높으므로 2입니다.</target>
        </trans-unit>
        <trans-unit id="2ac251de8487bd51d665dc484131dc49cb351bf8" translate="yes" xml:space="preserve">
          <source>Here, the scores for the test data call for caution as they are significantly worse than for the training data indicating an overfit despite the strong regularization.</source>
          <target state="translated">여기에서 테스트 데이터의 점수는 강력한 정규화에도 불구하고 과적 합을 나타내는 훈련 데이터보다 현저히 나쁘기 때문에주의가 필요합니다.</target>
        </trans-unit>
        <trans-unit id="e4ef491b953b35546ea7174e484d0bf1fa4f7b9b" translate="yes" xml:space="preserve">
          <source>Here, we are penalizing samples whose prediction is at least \(\varepsilon\) away from their true target. These samples penalize the objective by \(\zeta_i\) or \(\zeta_i^*\), depending on whether their predictions lie above or below the \(\varepsilon\) tube.</source>
          <target state="translated">여기서 우리는 예측이 실제 목표에서 적어도 \ (\ varepsilon \) 떨어진 샘플에 벌점을 부과합니다. 이러한 샘플은 예측이 \ (\ varepsilon \) 튜브 위 또는 아래에 있는지 여부에 따라 \ (\ zeta_i \) 또는 \ (\ zeta_i ^ * \)에 의해 목표에 페널티를줍니다.</target>
        </trans-unit>
        <trans-unit id="3799f87043c16356967f90e71e555e0bd9e10d17" translate="yes" xml:space="preserve">
          <source>Here, we combine 3 learners (linear and non-linear) and use a ridge regressor to combine their outputs together.</source>
          <target state="translated">여기에서는 3 명의 학습자 (선형 및 비선형)를 결합하고 능선 회귀 분석기를 사용하여 출력을 함께 결합합니다.</target>
        </trans-unit>
        <trans-unit id="1a1c9125aac07396a0a83a0358b1364478df4bbe" translate="yes" xml:space="preserve">
          <source>Here, we plot the partial dependence curves for a single feature, &amp;ldquo;age&amp;rdquo;, on the same axes. In this case, &lt;code&gt;tree_disp.axes_&lt;/code&gt; is passed into the second plot function.</source>
          <target state="translated">여기서는 동일한 축에 단일 특성 &quot;연령&quot;에 대한 부분 의존성 곡선을 플로팅합니다. 이 경우 &lt;code&gt;tree_disp.axes_&lt;/code&gt; 가 두 번째 플롯 함수로 전달됩니다.</target>
        </trans-unit>
        <trans-unit id="69bdd751bbdbd7bb5746658b628d98b5240af47b" translate="yes" xml:space="preserve">
          <source>Here, we used the default hyperparameters for the gradient boosting model without any preprocessing as tree-based models are naturally robust to monotonic transformations of numerical features.</source>
          <target state="translated">여기에서는 트리 기반 모델이 숫자 특성의 단조로운 변환에 자연스럽게 강하기 때문에 사전 처리없이 기울기 부스팅 모델에 대한 기본 하이퍼 파라미터를 사용했습니다.</target>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">Hessian Eigenmapping (Hessian 기반 LLE : HLLE이라고도 함)은 LLE의 정규화 문제를 해결하는 또 다른 방법입니다. 그것은 각 지역에서 헤 시안 기반의 이차 형태를 중심으로 회전하며, 이는 국소 적으로 선형 인 구조를 회복하는데 사용됩니다. 다른 구현에서는 데이터 크기에 &lt;code&gt;sklearn&lt;/code&gt; 확장 성이 좋지 않지만 sklearn 은 약간의 알고리즘 개선을 구현하여 비용이 작은 출력 차원의 다른 LLE 변형과 비교할 수 있습니다. HLLE는 키워드 &lt;code&gt;method = 'hessian'&lt;/code&gt; &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 함수 또는 객체 지향 대응 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 함수로 수행 할 수 있습니다 . 그것은 필요 &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4b22ade72c6627b1b562254b0df9c6b4d814d7ac" translate="yes" xml:space="preserve">
          <source>Hidden Activation sampled from the model distribution, where batch_size in the number of examples per minibatch and n_components is the number of hidden units.</source>
          <target state="translated">모델 분포에서 샘플링 된 Hidden Activation. 여기서 minibatch 및 n_components 당 예제 수의 batch_size는 은닉 유닛의 수입니다.</target>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">계층 적 응집 클러스터링 : 워드</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">계층 적 클러스터링은 중첩 클러스터를 병합하거나 분할하여 중첩 클러스터를 구축하는 일반적인 클러스터링 알고리즘 제품군입니다. 이 클러스터 계층은 트리 (또는 덴드로 그램)로 표시됩니다. 트리의 루트는 모든 샘플을 수집하는 고유 한 클러스터이며, 나뭇잎은 하나의 샘플 만있는 클러스터입니다. 자세한 내용은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia 페이지&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">계층 적 클러스터링 : 구조적 및 비 구조적 와드</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">고차원 데이터 세트는 시각화하기가 매우 어려울 수 있습니다. 2 차원 또는 3 차원의 데이터는 데이터의 고유 한 구조를 보여주기 위해 플로팅 될 수 있지만 동등한 고차원 플롯은 훨씬 덜 직관적입니다. 데이터 세트의 구조를 시각화하려면 차원을 어느 정도 줄여야합니다.</target>
        </trans-unit>
        <trans-unit id="769f5c4cc60f755dfe8d93fd7b194f0c3a9b7156" translate="yes" xml:space="preserve">
          <source>Hinge (soft-margin): equivalent to Support Vector Classification. \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\).</source>
          <target state="translated">힌지 (소프트 마진) : 지원 벡터 분류와 동일합니다. \ (L (y_i, f (x_i)) = \ max (0, 1-y_i f (x_i)) \).</target>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">경첩 : (소프트 마진) 지원 벡터 머신.</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">힌튼, 제프리 E.</target>
        </trans-unit>
        <trans-unit id="04790fed22fa2f8c9274791cf963a575a884ed64" translate="yes" xml:space="preserve">
          <source>Hispanic</source>
          <target state="translated">Hispanic</target>
        </trans-unit>
        <trans-unit id="d6215d83f5c22f8bb7c1095fa0298c0dd2d51f9d" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Classification Tree.</source>
          <target state="translated">히스토그램 기반 그라디언트 부스팅 분류 트리.</target>
        </trans-unit>
        <trans-unit id="772913778b88215dfb5f0c612cecbcae2c4b1a8f" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Regression Tree.</source>
          <target state="translated">히스토그램 기반 그라디언트 부스팅 회귀 트리.</target>
        </trans-unit>
        <trans-unit id="b8cd925555526484cde7ba65174f600e33250f6b" translate="yes" xml:space="preserve">
          <source>Hochreiter, Bodenhofer, et. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: factor analysis for bicluster acquisition&lt;/a&gt;.</source>
          <target state="translated">Hochreiter, Bodenhofer 등 . 등, 2010 년 &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;의 Fabia : bicluster 획득을위한 요인 분석&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cbd8a9628e732e150b1ccedb68c9c6ad4fe27c2c" translate="yes" xml:space="preserve">
          <source>Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the empirical log probability of categories given the respective feature and class, &lt;code&gt;P(x_i|y)&lt;/code&gt;.</source>
          <target state="translated">각 기능에 대한 모양 배열 (각 기능의 n_classes, n_categories)을 보유합니다. 각 배열은 각 특성 및 클래스 &lt;code&gt;P(x_i|y)&lt;/code&gt; 주어진 범주의 경험적 로그 확률을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="0265e0dec8e2ff0952abf481f5094201e14a30d8" translate="yes" xml:space="preserve">
          <source>Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the number of samples encountered for each class and category of the specific feature.</source>
          <target state="translated">각 기능에 대한 모양 배열 (각 기능의 n_classes, n_categories)을 보유합니다. 각 배열은 특정 기능의 각 클래스 및 범주에 대해 발견 된 샘플 수를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="7a5c3ac7ac58dcde82acc59c23c0dce0d31966d1" translate="yes" xml:space="preserve">
          <source>Holds the label for each class.</source>
          <target state="translated">각 클래스의 레이블을 보유합니다.</target>
        </trans-unit>
        <trans-unit id="4b7dceb5fe5f7e92199815a2b66fef8fdf05dc27" translate="yes" xml:space="preserve">
          <source>Homogeneity and completeness scores are formally given by:</source>
          <target state="translated">균질성과 완전성 점수는 공식적으로 다음과 같이 제공됩니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
